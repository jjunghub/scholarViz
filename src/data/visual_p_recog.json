[{"p_id":98304,"title":"RCK: accurate and efficient inference of sequence- and structure-based protein-RNA binding models from RNAcompete data","abstract":"Motivation: Protein-RNA interactions, which play vital roles in many processes, are mediated through both RNA sequence and structure. CLIP-based methods, which measure protein-RNA binding in vivo, suffer from experimental noise and systematic biases, whereas in vitro experiments capture a clearer signal of protein RNA-binding. Among them, RNAcompete provides binding affinities of a specific protein to more than 240 000 unstructured RNA probes in one experiment. The computational challenge is to infer RNA structure-and sequence-based binding models from these data. The state-of-the-art in sequence models, Deepbind, does not model structural preferences. RNAcontext models both sequence and structure preferences, but is outperformed by GraphProt. Unfortunately, GraphProt cannot detect structural preferences from RNAcompete data due to the unstructured nature of the data, as noted by its developers, nor can it be tractably run on the full RNACompete dataset.","keywords_author":null,"keywords_other":["SITES","MOTIFS","YB-1","RECOGNITION","SPECIFICITIES","OPTIMIZATION"],"max_cite":7.0,"pub_year":2016.0,"sources":"['wos']","rawkeys":["specificities","recognition","yb-1","motifs","optimization","sites"],"tags":["recognition","yb-1","specificity","motifs","optimization","sites"]},{"p_id":1,"title":"A fast learning algorithm for deep belief nets","abstract":"We show how to use \u201ccomplementary priors\u201d to eliminate the explaining-away effects thatmake inference difficult in densely connected belief nets that have many hidden layers. Using complementary priors, we derive a fast, greedy algorithm that can learn deep, directed belief networks one layer at a time, provided the top two layers form an undirected associative memory. The fast, greedy algorithm is used to initialize a slower learning procedure that fine-tunes the weights using a contrastive version of thewake-sleep algorithm. After fine-tuning, a networkwith three hidden layers forms a very good generative model of the joint distribution of handwritten digit images and their labels. This generative model gives better digit classification than the best discriminative learning algorithms. The low-dimensional manifolds on which the digits lie are modeled by long ravines in the free-energy landscape of the top-level associative memory, and it is easy to explore these ravines by using the directed connections to displaywhat the associativememory has in mind.","keywords_author":null,"keywords_other":["NETWORKS","RECOGNITION","MACHINES"],"max_cite":4965.0,"pub_year":2006.0,"sources":"['ieee', 'scp', 'wos']","rawkeys":["networks","recognition","machines"],"tags":["networks","machine","recognition"]},{"p_id":98310,"title":"Genome-wide prediction of minor-groove electrostatic potential enables biophysical modeling of protein-DNA binding","abstract":"Protein-DNA binding is a fundamental component of gene regulatory processes, but it is still not completely understood how proteins recognize their target sites in the genome. Besides hydrogen bonding in the major groove (base readout), proteins recognize minor-groove geometry using positively charged amino acids (shape readout). The underlying mechanism of DNA shape readout involves the correlation between minor-groove width and electrostatic potential (EP). To probe this biophysical effect directly, rather than using minor-groove width as an indirect measure for shape readout, we developed a methodology, DNAphi, for predicting EP in the minor groove and confirmed the direct role of EP in protein-DNA binding using massive sequencing data. The DNAphi method uses a sliding-window approach to mine results from non-linear Poisson-Boltzmann (NLPB) calculations on DNA structures derived from all-atom Monte Carlo simulations. We validated this approach, which only requires nucleotide sequence as input, based on direct comparison with NLPB calculations for available crystal structures. Using statistical machine-learning approaches, we showed that adding EP as a biophysical feature can improve the predictive power of quantitative binding specificity models across 27 transcription factor families. High-throughput prediction of EP offers a novel way to integrate biophysical and genomic studies of protein-DNA binding.","keywords_author":null,"keywords_other":["CRYSTAL-STRUCTURE","B\/HLH\/Z DOMAIN","TRANSCRIPTION FACTORS","NUCLEIC-ACIDS","RECOGNITION","SPECIFICITY","SHAPE","FIS PROTEIN","STRUCTURAL-ANALYSIS","B-DNA"],"max_cite":5.0,"pub_year":2017.0,"sources":"['wos']","rawkeys":["recognition","b-dna","specificity","transcription factors","nucleic-acids","structural-analysis","fis protein","shape","b\/hlh\/z domain","crystal-structure"],"tags":["recognition","b-dna","specificity","transcription factors","fis protein","shape","structural analysis","b\/hlh\/z domain","crystal-structure","nucleic acid"]},{"p_id":98316,"title":"Identification of Human Lineage-Specific Transcriptional Coregulators Enabled by a Glossary of Binding Modules and Tunable Genomic Backgrounds","abstract":"Transcription factors (TFs) control cellular processes by binding specific DNA motifs to modulate gene expression. Motif enrichment analysis of regulatory regions can identify direct and indirect TF binding sites. Here, we created a glossary of 108 non-redundant TF-8mer \"modules'' of shared specificity for 671 metazoan TFs from publicly available and new universal protein binding microarray data. Analysis of 239 ENCODE TF chromatin immunoprecipitation sequencingdatasets and associated RNA sequencing profiles suggest the 8mer modules are more precise than position weight matrices in identifying indirect binding motifs and their associated tethering TFs. We also developed GENRE (genomically equivalent negative regions), a tunable tool for construction of matched genomic background sequences for analysis of regulatory regions. GENRE outperformed four state-of-the-art approaches to background sequence construction. We used our TF-8mer glossary and GENRE in the analysis of the indirect binding motifs for the co-occurrence of tethering factors, suggesting novel TF-TF interactions. We anticipate that these tools will aid in elucidating tissue-specific gene-regulatory programs.","keywords_author":null,"keywords_other":["FACTOR SEQUENCE SPECIFICITY","INFORMATION","LIVER DEVELOPMENT","EXPRESSION","DETERMINANTS","PROTEIN-DNA INTERACTIONS","CELL-DIFFERENTIATION","ENHANCER-BINDING","RECOGNITION","MICROARRAYS"],"max_cite":2.0,"pub_year":2017.0,"sources":"['wos']","rawkeys":["recognition","microarrays","cell-differentiation","expression","factor sequence specificity","information","determinants","enhancer-binding","protein-dna interactions","liver development"],"tags":["recognition","cell differentiation","microarray","expression","factor sequence specificity","information","determinants","enhancer-binding","protein-dna interactions","liver development"]},{"p_id":98319,"title":"Accurate and sensitive quantification of protein-DNA binding affinity","abstract":"Transcription factors (TFs) control gene expression by binding to genomic DNA in a sequence-specific manner. Mutations in TF binding sites are increasingly found to be associated with human disease, yet we currently lack robust methods to predict these sites. Here, we developed a versatile maximum likelihood framework named No Read Left Behind (NRLB) that infers a biophysical model of protein-DNA recognition across the full affinity range from a library of in vitro selected DNA binding sites. NRLB predicts human Max homodimer binding in near-perfect agreement with existing low-throughput measurements. It can capture the specificity of the p53 tetramer and distinguish multiple binding modes within a single sample. Additionally, we confirm that newly identified low-affinity enhancer binding sites are functional in vivo, and that their contribution to gene expression matches their predicted affinity. Our results establish a powerful paradigm for identifying protein binding sites and interpreting gene regulatory sequences in eukaryotic genomes.","keywords_author":["transcription factors","SELEX","computational modeling","low-affinity binding sites","enhancer assays"],"keywords_other":["HOX SPECIFICITY","SYSTEMS-APPROACH","IN-VIVO","TRANSCRIPTION FACTORS","SITE","GENOME","RECOGNITION","P53","ENHANCERS","FAMILY"],"max_cite":0.0,"pub_year":2018.0,"sources":"['wos']","rawkeys":["computational modeling","systems-approach","recognition","transcription factors","in-vivo","enhancer assays","family","p53","selex","low-affinity binding sites","enhancers","site","genome","hox specificity"],"tags":["computational modeling","recognition","enhancement","transcription factors","in-vivo","enhancer assays","family","sites","system approach","low-affinity binding sites","p53","genomics","selex","hox specificity"]},{"p_id":98322,"title":"Bayesian Markov models consistently outperform PWMs at predicting motifs in nucleotide sequences","abstract":"Position weight matrices PWMs) are the standard model for DNA and RNA regulatory motifs. In PWMs nucleotide probabilities are independent of nucleotides at other positions. Models that account for dependencies need many parameters and are prone to overfitting. We have developed a Bayesian approach for motif discovery using Markov models in which conditional probabilities of order k - 1 act as priors for those of order k. This Bayesian Markov model BaMM) training automatically adapts model complexity to the amount of available data. We also derive an EM algorithm for de-novo discovery of enriched motifs. For transcription factor binding, BaMMs achieve significantly P = 1\/16) higher cross-validated partial AUC than PWMs in 97% of 446 ChIP-seq ENCODE datasets and improve performance by 36% on average. BaMMs also learn complex multipartite motifs, improving predictions of transcription start sites, polyadenylation sites, bacterial pause sites, and RNA binding sites by 26-101%. BaMMs never performed worse than PWMs. These robust improvements argue in favour of generally replacing PWMs by BaMMs.","keywords_author":null,"keywords_other":["SITES MODELS","START SITES","POSITIONAL WEIGHT MATRICES","IN-VIVO","IDENTIFICATION","PROTEIN-DNA INTERACTIONS","RECOGNITION","OPEN-ACCESS DATABASE","TRANSCRIPTION FACTOR-BINDING","SPECIFICITY"],"max_cite":9.0,"pub_year":2016.0,"sources":"['wos']","rawkeys":["positional weight matrices","identification","recognition","specificity","in-vivo","sites models","open-access database","transcription factor-binding","protein-dna interactions","start sites"],"tags":["positional weight matrices","identification","recognition","specificity","in-vivo","sites models","open-access database","transcription factor-binding","protein-dna interactions","start sites"]},{"p_id":98328,"title":"A survey of human pose estimation: The body parts parsing based methods","abstract":"Estimating human pose from videos and image sequences is not only an important computer vision problem, but also plays very critical role in many real-world applications. Main challenges for human pose estimation are variation of body poses, complicated background and depth ambiguities. To solve these problems, considerable research efforts have been devoted to the related fields. In this survey, we focus our attention on the recent advances in vision-based human pose estimation. We first present a general framework of human pose estimation, and then go through the latest technical progress on each stage. Finally, we discuss the limitations of the existing approaches and foresee the future directions to be explored. (C) 2015 Elsevier Inc. All rights reserved.","keywords_author":["Human pose estimation","Articulated object detection","Survey","Body parts parsing","Motion capture","Feature Extraction","Appearance models","Structure models"],"keywords_other":["FLEXIBLE MIXTURES","RECOGNITION","DEPTH IMAGES","HUMAN MOTION","SHAPE","PICTORIAL STRUCTURES"],"max_cite":8.0,"pub_year":2015.0,"sources":"['ieee', 'wos']","rawkeys":["human pose estimation","recognition","human motion","survey","structure models","appearance models","flexible mixtures","body parts parsing","pictorial structures","motion capture","shape","depth images","feature extraction","articulated object detection"],"tags":["appearance modeling","human pose estimations","recognition","depth image","survey","flexible mixtures","human motions","body parts parsing","pictorial structures","motion capture","shape","feature extraction","articulated object detection","structural models"]},{"p_id":27,"title":"Deep Convolutional Neural Networks for Computer-Aided Detection: CNN Architectures, Dataset Characteristics and Transfer Learning","abstract":"Remarkable progress has been made in image recognition, primarily due to the availability of large-scale annotated datasets and deep convolutional neural networks (CNNs). CNNs enable learning data-driven, highly representative, hierarchical image features from sufficient training data. However, obtaining datasets as comprehensively annotated as ImageNet in the medical imaging domain remains a challenge. There are currently three major techniques that successfully employ CNNs to medical image classification: training the CNN from scratch, using off-the-shelf pre-trained CNN features, and conducting unsupervised CNN pre-training with supervised fine-tuning. Another effective method is transfer learning, i.e., fine-tuning CNN models pre-trained from natural image dataset to medical image tasks. In this paper, we exploit three important, but previously understudied factors of employing deep convolutional neural networks to computer-aided detection problems. We first explore and evaluate different CNN architectures. The studied models contain 5 thousand to 160 million parameters, and vary in numbers of layers. We then evaluate the influence of dataset scale and spatial image context on performance. Finally, we examine when and why transfer learning from pre-trained ImageNet (via fine-tuning) can be useful. We study two specific computer-aided detection (CADe) problems, namely thoraco-abdominal lymph node (LN) detection and interstitial lung disease (ILD) classification. We achieve the state-of-the-art performance on the mediastinal LN detection, and report the first five-fold cross-validation classification results on predicting axial CT slices with ILD categories. Our extensive empirical evaluation, CNN model analysis and valuable insights can be extended to the design of high performance CAD systems for other medical imaging tasks.","keywords_author":["Biomedical imaging","computer aided diagnosis","image analysis","machine learning","neural networks","Biomedical imaging","computer aided diagnosis","image analysis","machine learning","neural networks","Biomedical imaging","computer aided diagnosis","image analysis","machine learning","neural networks"],"keywords_other":["thoraco-abdominal lymph node detection","Humans","Reproducibility of Results","NETS","high performance CAD systems","unsupervised CNN pretraining","dataset characteristics","Empirical evaluations","computerised tomography","image classification","OBJECT DETECTION","Lung Diseases, Interstitial","IMAGES","highly representative hierarchical image features","Computational modeling","transfer learning","medical imaging domain","five-fold cross-validation classification","Computed tomography","DATABASE","pretrained imagenet","CNN model analysis","computer-aided detection problems","state-of-the-art performance","medical image classification","Solid modeling","Interstitial lung disease","medical image tasks","Computer aided detection","State-of-the-art performance","CT DATA","Biomedical imaging","image representation","spatial image context","learning (artificial intelligence)","FEATURES","LYMPH-NODES","Diagnosis, Computer-Assisted","off-the-shelf pretrained CNN features","Lymph nodes","Databases, Factual","interstitial lung disease classification","supervised fine-tuning","RECOGNITION","medical image processing","Lymph Nodes","reviews","axial CT slices","learning data-driven","CNN architectures","neurophysiology","Diseases","natural image dataset","fine-tuning CNN models","Neural Networks (Computer)","lung","computer-aided detection","deep convolutional neural networks","Classification results","Image Interpretation, Computer-Assisted","diseases","Lungs","SEGMENTATION","Convolutional neural network","image recognition","mediastinal LN detection","Abdominal lymph nodes"],"max_cite":323.0,"pub_year":2016.0,"sources":"['wos', 'scp', 'ieee']","rawkeys":["lung diseases","thoraco-abdominal lymph node detection","databases","interstitial lung disease","high performance cad systems","ct data","dataset characteristics","mediastinal ln detection","computed tomography","axial ct slices","convolutional neural network","lymph nodes","computerised tomography","computer aided diagnosis","fine-tuning cnn models","image classification","highly representative hierarchical image features","image interpretation","segmentation","features","transfer learning","medical imaging domain","five-fold cross-validation classification","machine learning","solid modeling","pretrained imagenet","computer-aided detection problems","object detection","state-of-the-art performance","medical image classification","image representation","medical image tasks","computational modeling","diagnosis","off-the-shelf pretrained cnn features","learning (artificial intelligence)","images","neural networks (computer)","recognition","neural networks","spatial image context","computer aided detection","reproducibility of results","humans","database","interstitial lung disease classification","medical image processing","reviews","supervised fine-tuning","empirical evaluations","lymph-nodes","nets","learning data-driven","factual","neurophysiology","biomedical imaging","classification results","image analysis","interstitial","natural image dataset","lung","computer-assisted","computer-aided detection","deep convolutional neural networks","cnn architectures","diseases","unsupervised cnn pretraining","image recognition","abdominal lymph nodes","lungs","cnn model analysis"],"tags":["lung diseases","thoraco-abdominal lymph node detection","databases","interstitial lung disease","high performance cad systems","ct data","dataset characteristics","mediastinal ln detection","computed tomography","axial ct slices","convolutional neural network","lymph nodes","computerised tomography","fine-tuning cnn models","image classification","highly representative hierarchical image features","image interpretation","segmentation","features","transfer learning","computer-aided diagnosis","medical imaging domain","five-fold cross-validation classification","machine learning","solid modeling","pretrained imagenet","computer-aided detection problems","object detection","review","state-of-the-art performance","medical image classification","image representation","medical image tasks","computational modeling","diagnosis","off-the-shelf pretrained cnn features","recognition","images","spatial image context","neural networks","disease","reproducibility of results","humans","medical image processing","interstitial lung disease classification","supervised fine-tuning","empirical evaluations","nets","learning data-driven","factual","neurophysiology","biomedical imaging","classification results","image analysis","interstitial","natural image dataset","lung","computer-assisted","unsupervised cnn pretraining","image recognition","abdominal lymph nodes","cnn model analysis","cnn architecture"]},{"p_id":32817,"title":"Is the maximal margin hyperplane special in a feature space?","abstract":"Recent developments in Support Vector Machines (SVM) generalize the Linear Support Vector Machines (L-SVM) to learn non-linear separating surfaces by applying a feature mapping first. We construct an example in this paper to show that any Separating Hyperplane (SH), f, in any feature space can be mapped to a maximal-margin SH (mmSH), f 0, in another feature space of the same dimension such that f and f 0 give exactly the same separating surface in the original input space. Then the question is: which feature space's maximal margin hyperplane gives the best generalization guarantee?","keywords_author":["Machine learning","Supervised learning","Support Vector Machines (SVM)"],"keywords_other":["Kernel method","Support vector machines (SVM)","Generalization","Input spaces"],"max_cite":3.0,"pub_year":2001.0,"sources":"['scp']","rawkeys":["supervised learning","support vector machines (svm)","input spaces","machine learning","generalization","kernel method"],"tags":["supervised learning","recognition","machine learning","kernel methods","input space"]},{"p_id":52,"title":"Vehicle Detection in Satellite Images by Hybrid Deep Convolutional Neural Networks","abstract":"Detecting small objects such as vehicles in satellite images is a difficult problem. Many features (such as histogram of oriented gradient, local binary pattern, scale-invariant feature transform, etc.) have been used to improve the performance of object detection, but mostly in simple environments such as those on roads. Kembhavi et al. proposed that no satisfactory accuracy has been achieved in complex environments such as the City of San Francisco. Deep convolutional neural networks (DNNs) can learn rich features from the training data automatically and has achieved state-of-the-art performance in many image classification databases. Though the DNN has shown robustness to distortion, it only extracts features of the same scale, and hence is insufficient to tolerate large-scale variance of object. In this letter, we present a hybrid DNN (HDNN), by dividing the maps of the last convolutional layer and the max-pooling layer of DNN into multiple blocks of variable receptive field sizes or max-pooling field sizes, to enable the HDNN to extract variable-scale features. Comparative experimental results indicate that our proposed HDNN significantly outperforms the traditional DNN on vehicle detection.","keywords_author":["Deep convolutional neural networks (DNNs)","hybrid DNNs (HDNNs)","remote sensing","vehicle detection","Deep convolutional neural networks (DNNs)","hybrid DNNs (HDNNs)","remote sensing","vehicle detection"],"keywords_other":["remote sensing","satellite images","Satellites","hybrid DNNs (HDNNs)","variable-scale feature extraction","Feature extraction","Vehicles","vehicles","max-pooling layer","Training","Object detection","vehicle detection","RECOGNITION","hybrid Deep convolutional neural networks","max-pooling field sizes","Deep convolutional neural networks (DNNs)","neural nets","Remote sensing","variable receptive field sizes","CLASSIFICATION","Vehicle detection","feature extraction"],"max_cite":139.0,"pub_year":2014.0,"sources":"['ieee', 'wos']","rawkeys":["variable receptive field sizes","recognition","remote sensing","satellites","variable-scale feature extraction","training","satellite images","vehicle detection","vehicles","classification","feature extraction","max-pooling field sizes","max-pooling layer","object detection","neural nets","hybrid dnns (hdnns)","deep convolutional neural networks (dnns)","hybrid deep convolutional neural networks"],"tags":["variable receptive field sizes","recognition","remote sensing","satellites","variable-scale feature extraction","neural networks","training","satellite images","vehicle detection","vehicles","classification","feature extraction","convolutional neural network","max-pooling field sizes","max-pooling layer","object detection","hybrid dnns (hdnns)","hybrid deep convolutional neural networks"]},{"p_id":98358,"title":"When silent letters say more than a thousand words: An implementation and evaluation of CDP plus plus in French","abstract":"Cross-language comparisons can provide important constraints on our understanding of how people read aloud. French is an interesting case because it differs from most other writing systems in that it uses a large number of multi-letter vowel graphemes and consonants that are systematically silent (i.e., do not map to any lexical phonology; e.g., (Top). Here, we developed a French version of the Connectionist Dual Process Model of Reading Aloud (CDP++) that can handle multisyllabic stimuli (up to three syllables) and has a large-scale lexicon of more than 100,000 words. We tested the model on extant data and an additional experiment examining the reading aloud of nonwords with potentially silent letters. The results from the extant data showed that the model was able to capture a number of important psycholinguistic effects in the literature and explained between 52% and 67% of the item-specific variance in two large databases. The results of the silent-letter experiment showed that, contrary to what would be predicted on the basis of lexical database statistics, people generally pronounce \"silent\" consonants in nonwords. We show that the French CDP++ model faithfully predicted this effect because it implements a linear mapping between orthography and phonology. These findings highlight the theoretical and practical significance of using computational models to help determine the processes and representations that underlie skilled reading. (C) 2014 Elsevier Inc. All rights reserved.","keywords_author":["Reading aloud","Connectionist modelling","Connectionist Dual Process Model","French"],"keywords_other":["LEXICAL DECISION","RESOLVING NEIGHBORHOOD CONFLICTS","READING ALOUD","ITEM-LEVEL","ORTHOGRAPHIC SYLLABLE STRUCTURE","MODEL","RECOGNITION","ACQUIRED DYSGRAPHIA","REGULARITY","SIMILARITY"],"max_cite":13.0,"pub_year":2014.0,"sources":"['wos']","rawkeys":["connectionist modelling","lexical decision","model","orthographic syllable structure","acquired dysgraphia","recognition","similarity","regularity","resolving neighborhood conflicts","item-level","connectionist dual process model","reading aloud","french"],"tags":["connectionist models","lexical decision","orthographic syllable structure","model","recognition","acquired dysgraphia","similarity","resolving neighborhood conflicts","item-level","connectionist dual process model","reading aloud","french","regularization"]},{"p_id":41018,"title":"Comparative approaches to same\/different abstract-concept learning","abstract":"\u00a9 2017, Psychonomic Society, Inc.Martinho and Kacelnik (2016) imprinted newly hatched ducklings (Anas platyrhynchos domestica) with a moving pair of either same or different objects, and following only one session, the ducklings accurately transferred the same\/different relationship to novel object pairs that maintained the training relationship. This rapid learning and transfer of the concepts same and different far outstrips the more gradual learning of these basic concepts by animals in associative-learning tasks in which reinforcement is given for correct responses.","keywords_author":["Comparative cognition","Concept learning"],"keywords_other":["Concept Formation","Animals","Transfer (Psychology)","Psychology, Comparative","Association Learning","Imprinting (Psychology)"],"max_cite":1.0,"pub_year":2017.0,"sources":"['scp']","rawkeys":["transfer (psychology)","comparative cognition","comparative","concept formation","imprinting (psychology)","concept learning","association learning","psychology","animals"],"tags":["associative learning","recognition","comparative cognition","comparative","concept formation","concept learning","animals"]},{"p_id":73792,"title":"A Neural Network Approach to Intention Modeling for User-Adapted Conversational Agents","abstract":"Spoken dialogue systems have been proposed to enable a more natural and intuitive interaction with the environment and human-computer interfaces. In this contribution, we present a framework based on neural networks that allows modeling of the user's intention during the dialogue and uses this prediction to dynamically adapt the dialogue model of the system taking into consideration the user's needs and preferences. We have evaluated our proposal to develop a user-adapted spoken dialogue system that facilitates tourist information and services and provide a detailed discussion of the positive influence of our proposal in the success of the interaction, the information and services provided, and the quality perceived by the users.","keywords_author":null,"keywords_other":["INFORMATION","FEATURES","STRATEGIES","DESIGN","SYSTEM","RECOGNITION","SPEECH","DIALOGUE MANAGEMENT"],"max_cite":1.0,"pub_year":2016.0,"sources":"['wos']","rawkeys":["design","recognition","features","system","strategies","information","speech","dialogue management"],"tags":["design","recognition","features","system","strategies","information","speech","dialogue management"]},{"p_id":69,"title":"Spectral-Spatial Classification of Hyperspectral Data Based on Deep Belief Network","abstract":"Hyperspectral data classification is a hot topic in remote sensing community. In recent years, significant effort has been focused on this issue. However, most of the methods extract the features of original data in a shallow manner. In this paper, we introduce a deep learning approach into hyperspectral image classification. A new feature extraction (FE) and image classification framework are proposed for hyperspectral data analysis based on deep belief network (DBN). First, we verify the eligibility of restricted Boltzmann machine (RBM) and DBN by the following spectral information-based classification. Then, we propose a novel deep architecture, which combines the spectral-spatial FE and classification together to get high classification accuracy. The framework is a hybrid of principal component analysis (PCA), hierarchical learning-based FE, and logistic regression (LR). Experimental results with hyperspectral data indicate that the classifier provide competitive solution with the state-of-the-art methods. In addition, this paper reveals that deep learning system has huge potential for hyperspectral data classification.","keywords_author":["Deep belief network (DBN)","deep learning","feature extraction (FE)","hyperspectral data classification","logistic regression (LR)","restricted Boltzmann machine (RBM)","Support vector machine (SVM)","Deep belief network (DBN)","deep learning","feature extraction (FE)","hyperspectral data classification","logistic regression (LR)","restricted Boltzmann machine (RBM)","support vector machine (SVM)","Deep belief network (DBN)","deep learning","feature extraction (FE)","hyperspectral data classification","logistic regression (LR)","restricted Boltzmann machine (RBM)","support vector machine (SVM)"],"keywords_other":["restricted Boltzmann machine","remote sensing","Support vector machines","ALGORITHM","RBM","regression analysis","spectral-spatial hyperspectral data classification","Logistic regressions","deep learning approach","LR","image classification","Deep belief network (DBN)","MORPHOLOGICAL PROFILES","restricted Boltzmann machine (RBM)","Deep learning","Feature extraction","FEATURE-SELECTION","Iron","spectral-spatial FE","hyperspectral imaging","data analysis","deep belief network","DIMENSIONALITY REDUCTION","UNIVERSAL APPROXIMATORS","hyperspectral data classification","Restricted boltzmann machine","principal component analysis","hyperspectral data analysis","logistic regression","learning (artificial intelligence)","deep learning","Training","SUPPORT VECTOR MACHINES","Vectors","RECOGNITION","support vector machine (SVM)","DBN","belief networks","URBAN","REMOTE-SENSING IMAGES","Hyperspectral imaging","Hyperspectral data classification","hyperspectral image classification","spectral information-based classification","feature extraction","feature extraction (FE)","Boltzmann machines","hierarchical learning-based FE","SPARSE REPRESENTATION","logistic regression (LR)","PCA"],"max_cite":162.0,"pub_year":2015.0,"sources":"['scp', 'wos', 'ieee']","rawkeys":["remote sensing","logistic regression (lr)","regression analysis","restricted boltzmann machine (rbm)","feature-selection","spectral-spatial fe","spectral-spatial hyperspectral data classification","deep learning approach","iron","image classification","dimensionality reduction","feature extraction (fe)","rbm","dbn","lr","morphological profiles","vectors","hyperspectral imaging","logistic regressions","remote-sensing images","deep belief network","universal approximators","hyperspectral data classification","algorithm","principal component analysis","hyperspectral data analysis","learning (artificial intelligence)","logistic regression","deep belief network (dbn)","deep learning","recognition","boltzmann machines","sparse representation","training","urban","support vector machine (svm)","belief networks","pca","restricted boltzmann machine","hyperspectral image classification","spectral information-based classification","feature extraction","hierarchical learning-based fe","support vector machines","data analysis"],"tags":["remote sensing","regression analysis","spectral-spatial fe","spectral-spatial hyperspectral data classification","iron","deep learning approach","image classification","dimensionality reduction","machine learning","morphological profiles","vectors","feature selection","hyperspectral imaging","logistic regressions","data analysis","universal approximators","algorithms","remote sensing images","hyperspectral data classification","principal component analysis","hyperspectral data analysis","recognition","urban","boltzmann machines","sparse representation","training","belief networks","restricted boltzmann machine","hyperspectral image classification","spectral information-based classification","feature extraction","hierarchical learning-based fe","deep belief networks"]},{"p_id":96,"title":"Background Prior-Based Salient Object Detection via Deep Reconstruction Residual","abstract":"Detection of salient objects from images is gaining increasing research interest in recent years as it can substantially facilitate a wide range of content-based multimedia applications. Based on the assumption that foreground salient regions are distinctive within a certain context, most conventional approaches rely on a number of hand-designed features and their distinctiveness is measured using local or global contrast. Although these approaches have been shown to be effective in dealing with simple images, their limited capability may cause difficulties when dealing with more complicated images. This paper proposes a novel framework for saliency detection by first modeling the background and then separating salient objects from the background. We develop stacked denoising autoencoders with deep learning architectures to model the background where latent patterns are explored and more powerful representations of data are learned in an unsupervised and bottom-up manner. Afterward, we formulate the separation of salient objects from the background as a problem of measuring reconstruction residuals of deep autoencoders. Comprehensive evaluations of three benchmark datasets and comparisons with nine state-of-the-art algorithms demonstrate the superiority of this paper.","keywords_author":["deep reconstruction residual","salient object detection","stacked denoising autoencoder","Background prior","deep reconstruction residual","salient object detection","stacked denoising autoencoder (SDAE)","salient object detection","stacked denoising autoencoder","deep reconstruction residual"],"keywords_other":["REPRESENTATIONS","Noise reduction","Encoding","Salient object detection","Background prior","Feature extraction","global contrast","object detection","Auto encoders","deep reconstruction residual","learning (artificial intelligence)","stacked denoising autoencoders","Robustness","AUTOENCODERS","State-of-the-art algorithms","Training","Object detection","stacked denoising autoencoder (SDAE)","Research interests","RECOGNITION","image denoising","image coding","CUES","Content-based multimedia","local contrast","Image reconstruction","Comprehensive evaluation","image reconstruction","salient object detection","CONTRAST","NETWORKS","FRAMEWORK","data representation","foreground salient regions","deep learning architectures","multimedia computing","feature extraction","REGION DETECTION","background prior-based salient object detection","Conventional approach","content-based multimedia applications"],"max_cite":122.0,"pub_year":2015.0,"sources":"['ieee', 'scp', 'wos']","rawkeys":["content-based multimedia","robustness","region detection","cues","contrast","conventional approach","encoding","stacked denoising autoencoder (sdae)","global contrast","object detection","deep reconstruction residual","noise reduction","learning (artificial intelligence)","recognition","stacked denoising autoencoders","framework","background prior","training","image denoising","networks","image coding","research interests","local contrast","image reconstruction","state-of-the-art algorithms","salient object detection","data representation","comprehensive evaluation","auto encoders","deep learning architectures","foreground salient regions","autoencoders","representations","multimedia computing","feature extraction","background prior-based salient object detection","stacked denoising autoencoder","content-based multimedia applications"],"tags":["content-based multimedia","robustness","region detection","cues","contrast","conventional approach","encoding","data representations","machine learning","object detection","deep reconstruction residual","noise reduction","recognition","framework","background prior","training","global contrasts","image denoising","networks","image coding","research interests","local contrast","image reconstruction","state-of-the-art algorithms","salient object detection","comprehensive evaluation","auto encoders","deep learning architectures","foreground salient regions","representation","multimedia computing","feature extraction","background prior-based salient object detection","stacked denoising autoencoder","content-based multimedia applications"]},{"p_id":108,"title":"Lung Pattern Classification for Interstitial Lung Diseases Using a Deep Convolutional Neural Network","abstract":"Automated tissue characterization is one of the most crucial components of a computer aided diagnosis (CAD) system for interstitial lung diseases (ILDs). Although much research has been conducted in this field, the problem remains challenging. Deep learning techniques have recently achieved impressive results in a variety of computer vision problems, raising expectations that they might be applied in other domains, such as medical image analysis. In this paper, we propose and evaluate a convolutional neural network (CNN), designed for the classification of ILD patterns. The proposed network consists of 5 convolutional layers with 2 \u00d7 2 kernels and LeakyReLU activations, followed by average pooling with size equal to the size of the final feature maps and three dense layers. The last dense layer has 7 outputs, equivalent to the classes considered: healthy, ground glass opacity (GGO), micronodules, consolidation, reticulation, honeycombing and a combination of GGO\/reticulation. To train and evaluate the CNN, we used a dataset of 14696 image patches, derived by 120 CT scans from different scanners and hospitals. To the best of our knowledge, this is the first deep CNN designed for the specific problem. A comparative analysis proved the effectiveness of the proposed CNN against previous methods in a challenging dataset. The classification performance ( ~ 85.5%) demonstrated the potential of CNNs in analyzing lung patterns. Future work includes, extending the CNN to three-dimensional data provided by CT volume scans and integrating the proposed method into a CAD system that aims to provide differential diagnosis for ILDs as a supportive tool for radiologists.","keywords_author":["Convolutional neural networks","interstitial lung diseases","texture classification","Convolutional neural networks","interstitial lung diseases","texture classification","Convolutional neural networks","interstitial lung diseases","texture classification"],"keywords_other":["Design automation","ILD pattern classification","Humans","micronodules","CT volume scans","computer vision problems","Neural networks","Lung","computerised tomography","image classification","Lung Diseases, Interstitial","Feature extraction","Computer Aided Diagnosis(CAD)","convolution","Computed tomography","deep learning techniques","ground glass opacity","Convolution","Texture classification","RESOLUTION COMPUTED-TOMOGRAPHY","honeycombing","Interstitial lung disease","Three-dimensional data","computer aided diagnosis system","Algorithms","learning (artificial intelligence)","deep convolutional neural network","medical image processing","feature maps","RECOGNITION","interstitial lung diseases","Computer vision problems","biological tissues","medical image analysis","neural nets","automated tissue characterization","Diseases","Classification performance","Neural Networks (Computer)","lung","Tomography, X-Ray Computed","lung pattern classification","Image Interpretation, Computer-Assisted","consolidation","Lungs","reticulation","Convolutional neural network","feature extraction","TISSUE-ANALYSIS","diseases","Differential diagnosis"],"max_cite":111.0,"pub_year":2016.0,"sources":"['wos', 'scp', 'ieee']","rawkeys":["lung diseases","interstitial lung disease","micronodules","three-dimensional data","computer vision problems","ct volume scans","computed tomography","convolutional neural network","computerised tomography","computer aided diagnosis(cad)","resolution computed-tomography","image classification","tomography","ild pattern classification","image interpretation","tissue-analysis","convolution","deep learning techniques","ground glass opacity","algorithms","honeycombing","texture classification","learning (artificial intelligence)","classification performance","convolutional neural networks","neural networks (computer)","neural networks","recognition","x-ray computed","deep convolutional neural network","humans","medical image processing","feature maps","interstitial lung diseases","biological tissues","medical image analysis","neural nets","automated tissue characterization","interstitial","lung","computer-assisted","lung pattern classification","consolidation","reticulation","feature extraction","differential diagnosis","design automation","lungs","diseases","computer aided diagnosis system"],"tags":["lung diseases","interstitial lung disease","micronodules","three-dimensional data","computer vision problems","ct volume scans","computed tomography","convolutional neural network","ground-glass opacity","computerised tomography","resolution computed-tomography","image classification","tomography","feature map","ild pattern classification","image interpretation","computer-aided diagnosis","convolution","tissue-analysis","machine learning","deep learning techniques","algorithms","honeycombing","texture classification","computer aided diagnosis systems","recognition","classification performance","neural networks","x-ray computed","disease","humans","medical image processing","biological tissues","medical image analysis","automated tissue characterization","interstitial","lung","computer-assisted","lung pattern classification","consolidation","reticulation","feature extraction","differential diagnosis","design automation"]},{"p_id":16501,"title":"A statistical language modeling approach to online deception detection","abstract":"Online deception is disrupting our dally life, organizational process, and even national security. Existing approaches to online deception detection follow a traditional paradigm by using a set of cues as antecedents for deception detection, which may be hindered by ineffective cue identification. Motivated by the strength of statistical language models (SLMs) in capturing the dependency of words in text without explicit feature extraction, we developed SLMs to detect online deception. We also addressed the data-sparsity problem in building SLMs in general and in deception detection in specific using smoothing and vocabulary pruning techniques. The developed SLMs were evaluated empirically with diverse data sets. The results showed that the proposed SLM approach to deception detection outperformed a state-of-the-art text categorization method, as well as traditional feature-based methods. \u00a9 2008 IEEE.","keywords_author":["Classification","Knowledge management applications","Language models","Machine learning","Security","Text mining"],"keywords_other":["Organizational processes","Pruning techniques","National securities","Diverse data sets","Statistical language modeling","deception detection","Strength (IGC: D5\/D6)","General (CO)","Text categorization (TC)","Feature-based methods"],"max_cite":36.0,"pub_year":2008.0,"sources":"['scp']","rawkeys":["text mining","national securities","machine learning","general (co)","organizational processes","security","statistical language modeling","deception detection","classification","text categorization (tc)","diverse data sets","knowledge management applications","feature-based methods","strength (igc: d5\/d6)","language models","pruning techniques"],"tags":["recognition","text mining","machine learning","national security","organizational processes","security","statistical language modeling","deception detection","language model","classification","strength","diverse data sets","knowledge management applications","text categorization","feature-based method","pruning techniques"]},{"p_id":73847,"title":"Multi-Level Discriminative Dictionary Learning With Application to Large Scale Image Classification","abstract":"The sparse coding technique has shown flexibility and capability in image representation and analysis. It is a powerful tool in many visual applications. Some recent work has shown that incorporating the properties of task (such as discrimination for classification task) into dictionary learning is effective for improving the accuracy. However, the traditional supervised dictionary learning methods suffer from high computation complexity when dealing with large number of categories, making them less satisfactory in large scale applications. In this paper, we propose a novel multi-level discriminative dictionary learning method and apply it to large scale image classification. Our method takes advantage of hierarchical category correlation to encode multi-level discriminative information. Each internal node of the category hierarchy is associated with a discriminative dictionary and a classification model. The dictionaries at different layers are learnt to capture the information of different scales. Moreover, each node at lower layers also inherits the dictionary of its parent, so that the categories at lower layers can be described with multi-scale information. The learning of dictionaries and associated classification models is jointly conducted by minimizing an overall tree loss. The experimental results on challenging data sets demonstrate that our approach achieves excellent accuracy and competitive computation cost compared with other sparse coding methods for large scale image classification.","keywords_author":["Sparse coding","discriminative dictionary learning","hierarchical method","large scale classification"],"keywords_other":["SPARSE REPRESENTATION","RECOGNITION","ALGORITHMS","K-SVD"],"max_cite":12.0,"pub_year":2015.0,"sources":"['wos']","rawkeys":["hierarchical method","recognition","large scale classification","sparse representation","algorithms","discriminative dictionary learning","sparse coding","k-svd"],"tags":["hierarchical method","recognition","sparse representation","large scale classifications","algorithms","discriminative dictionary learning","sparse coding","k-svd"]},{"p_id":120,"title":"Learning Deep and Wide: A Spectral Method for Learning Deep Networks","abstract":"Building intelligent systems that are capable of extracting high-level representations from high-dimensional sensory data lies at the core of solving many computer vision-related tasks. We propose the multispectral neural networks (MSNN) to learn features from multicolumn deep neural networks and embed the penultimate hierarchical discriminative manifolds into a compact representation. The low-dimensional embedding explores the complementary property of different views wherein the distribution of each view is sufficiently smooth and hence achieves robustness, given few labeled training data. Our experiments show that spectrally embedding several deep neural networks can explore the optimum output from the multicolumn networks and consistently decrease the error rate compared with a single deep network.","keywords_author":["Deep networks","multispectral embedding","representation learning","Deep networks","multispectral embedding","representation learning."],"keywords_other":["computer vision-related task","penultimate hierarchical discriminative manifolds","error rate","Humans","spectral method","MSNN","compact representation","multicolumn deep neural network","Neural networks","Feature extraction","Artificial Intelligence","error statistics","multispectral embedding","learning (artificial intelligence)","Noise","building intelligent system","Training","Deep networks","Laplace equations","Databases, Factual","representation learning","RECOGNITION","low-dimensional embedding","Error analysis","neural nets","Data models","multispectral neural networks","Pattern Recognition, Visual","Neural Networks (Computer)","deep neural networks","CLASSIFICATION","learning deep networks","multicolumn networks","high-dimensional sensory data"],"max_cite":57.0,"pub_year":2014.0,"sources":"['wos', 'ieee']","rawkeys":["computer vision-related task","error rate","penultimate hierarchical discriminative manifolds","databases","spectral method","deep networks","compact representation","error analysis","multicolumn deep neural network","classification","msnn","error statistics","visual","multispectral embedding","learning (artificial intelligence)","neural networks (computer)","noise","recognition","neural networks","building intelligent system","training","humans","representation learning","low-dimensional embedding","factual","neural nets","multispectral neural networks","artificial intelligence","deep neural networks","laplace equations","learning deep networks","data models","feature extraction","multicolumn networks","pattern recognition","high-dimensional sensory data"],"tags":["computer vision-related task","error rate","penultimate hierarchical discriminative manifolds","databases","deep networks","compact representation","error analysis","multicolumn deep neural network","classification","convolutional neural network","visualization","machine learning","msnn","error statistics","multispectral embedding","spectral methods","recognition","noise","neural networks","building intelligent system","training","low dimensional embedding","humans","representation learning","factual","multispectral neural networks","laplace equations","learning deep networks","data models","feature extraction","multicolumn networks","pattern recognition","high-dimensional sensory data"]},{"p_id":73849,"title":"Cascade shallow CNN structure for face verification and identification","abstract":"Face recognition is a long-standing challenging topic in computer science, especially on insufficient datasets. The obstacle also lies in the balance of speed and accuracy. Recently, many algorithms claim that they have obtained great performance with high accuracy, but they are not enough for real-time application. In this work, a novel fast and accurate solution is proposed to deal with the face recognition problem on the small training set. Based on face alignment, we present two methods to extract features. One is a combination of several kinds of human designed feature descriptors applied on patches partitioned according to facial landmarks. The other one is a cascade classifier based on shallow convolutional neural networks. Both methods can represent the face as a set of feature vectors, which can be dealt with SVM or a boosting verification algorithm in this work. In the experiments, the proposed framework has achieved great performance for face recognition and verification with high speed and high accuracy, based on the public available datasets such as the Labeled Face in the Wild dataset and the AT&T database of faces. (C) 2016 Elsevier B.V. All rights reserved.","keywords_author":["Face recognition","Cascade convolutional network","Local features","Sub-region division"],"keywords_other":["IMAGE","REPRESENTATION","NETWORKS","MODEL","ALIGNMENT","RECOGNITION","3D OBJECT RETRIEVAL","RECONSTRUCTION"],"max_cite":2.0,"pub_year":2016.0,"sources":"['ieee', 'wos']","rawkeys":["recognition","model","cascade convolutional network","local features","representation","reconstruction","image","3d object retrieval","face recognition","networks","sub-region division","alignment"],"tags":["recognition","images","model","local feature","cascade convolutional network","representation","reconstruction","networks","3d object retrieval","face recognition","sub-region division","alignment"]},{"p_id":57471,"title":"Reset-free Trial-and-Error Learning for Robot Damage Recovery","abstract":"The high probability of hardware failures prevents many advanced robots (e.g., legged robots) from being confidently deployed in real-world situations (e.g., post-disaster rescue). Instead of attempting to diagnose the failures, robots could adapt by trial-and-error in order to be able to complete their tasks. In this situation, damage recovery can be seen as a Reinforcement Learning (RL) problem. However, the best RL algorithms for robotics require the robot and the environment to be reset to an initial state after each episode, that is, the robot is not learning autonomously. In addition, most of the RL methods for robotics do not scale well with complex robots (e.g., walking robots) and either cannot be used at all or take too long to converge to a solution (e.g., hours of learning). In this paper, we introduce a novel learning algorithm called \"Reset-free Trial-and-Error\" (RTE) that (1) breaks the complexity by pre-generating hundreds of possible behaviors with a dynamics simulator of the intact robot, and (2) allows complex robots to quickly recover from damage while completing their tasks and taking the environment into account. We evaluate our algorithm on a simulated wheeled robot, a simulated six-legged robot, and a real six-legged walking robot that are damaged in several ways (e.g., a missing leg, a shortened leg, faulty motor, etc.) and whose objective is to reach a sequence of targets in an arena. Our experiments show that the robots can recover most of their locomotion abilities in an environment with obstacles, and without any human intervention. (C) 2017 The Authors. Published by Elsevier B.V. This is an open access article under the CC BY license (http:\/\/creativecommons.orgilicenses\/by\/4.0\/).","keywords_author":["Robot damage recovery","Autonomous systems","Robotics","Trial-and-Error learning","Reinforcement learning"],"keywords_other":["DIVERSITY","OPTIMIZATION","REINFORCEMENT"],"max_cite":0.0,"pub_year":2018.0,"sources":"['wos']","rawkeys":["robot damage recovery","diversity","autonomous systems","reinforcement","trial-and-error learning","robotics","optimization","reinforcement learning"],"tags":["robot damage recovery","recognition","diversity","autonomous systems","trial-and-error learning","robotics","optimization","reinforcement learning"]},{"p_id":57472,"title":"Extreme Trust Region Policy Optimization for Active Object Recognition","abstract":"In this brief, we develop a deep reinforcement learning method to actively recognize objects by choosing a sequence of actions for an active camera that helps to discriminate between the objects. The method is realized using trust region policy optimization, in which the policy is realized by an extreme learning machine and, therefore, leads to efficient optimization algorithm. The experimental results on the publicly available data set show the advantages of the developed extreme trust region optimization method.","keywords_author":["Active object recognition","extreme learning machines (ELMs)","trust region policy optimization (TRPO)"],"keywords_other":["LEARNING-MACHINE","REINFORCEMENT","VIEW","CLASSIFICATION","ELM"],"max_cite":0.0,"pub_year":2018.0,"sources":"['ieee', 'wos']","rawkeys":["trust region policy optimization (trpo)","reinforcement","learning-machine","view","extreme learning machines (elms)","classification","elm","active object recognition"],"tags":["views","recognition","trust region policy optimization (trpo)","classification","learning machines","extreme learning machine","active object recognition"]},{"p_id":57475,"title":"Grandmother cells and localist representations: a review of current thinking","abstract":"There is now a large literature in neuroscience highlighting how some neurons respond highly selectively to high-level information (e.g. cells that respond to specific faces) and a growing literature in psychology and computer science showing that artificial neural networks often learn highly selective representations. Nevertheless, the vast majority of neuroscientists reject grandmother cell theories out of hand, and many psychologists reject localist models based on neuroscience. In this review, I detail some of the conceptual confusions regarding grandmother cells that have contributed to this state of affairs, and review the literature of single-unit recording studies in artificial neural networks that may provide insights into why some neurons respond in a highly selective manner. I then briefly review the contributions from leading theorists in psychology and neuroscience. My hope this special issue contributes to a more productive debate on an important issue that has often been characterised by misunderstandings between disciplines.","keywords_author":["Grandmother cells","localist representations","distributed representations","neural networks","deep networks"],"keywords_other":["DEEP","NEURONS","VISUAL-CORTEX","MODEL","NEURAL-NETWORKS","RECOGNITION","CONNECTIONISM","MEDIAL TEMPORAL-LOBE","MEMORY","BRAIN"],"max_cite":1.0,"pub_year":2017.0,"sources":"['wos']","rawkeys":["connectionism","neural-networks","grandmother cells","model","memory","recognition","neural networks","deep","visual-cortex","brain","distributed representations","localist representations","neurons","deep networks","medial temporal-lobe"],"tags":["connectionism","grandmother cells","model","memory","recognition","neural networks","deep","visual-cortex","brain","distributed representation","localist representations","neurons","deep networks","medial temporal-lobe"]},{"p_id":57477,"title":"Deep recurrent neural network reveals a hierarchy of process memory during dynamic natural vision","abstract":"The human visual cortex extracts both spatial and temporal visual features to support perception and guide behavior. Deep convolutional neural networks (CNNs) provide a computational framework to model cortical representation and organization for spatial visual processing, but unable to explain how the brain processes temporal information. To overcome this limitation, we extended a CNN by adding recurrent connections to different layers of the CNN to allow spatial representations to be remembered and accumulated over time. The extended model, or the recurrent neural network (RNN), embodied a hierarchical and distributed model of process memory as an integral part of visual processing. Unlike the CNN, the RNN learned spatiotemporal features from videos to enable action recognition. The RNN better predicted cortical responses to natural movie stimuli than the CNN, at all visual areas, especially those along the dorsal stream. As a fully observable model of visual processing, the RNN also revealed a cortical hierarchy of temporal receptive window, dynamics of process memory, and spatiotemporal representations. These results support the hypothesis of process memory, and demonstrate the potential of using the RNN for in-depth computational understanding of dynamic natural vision.","keywords_author":["deep learning","natural vision","neural encoding","process memory","recurrent neural network","temporal receptive window"],"keywords_other":["INFORMATION","ACTIVATION","VISUAL-CORTEX","REPRESENTATIONS","ATTENTION","SYSTEM","RECOGNITION","OBJECT","MODELS","CORTICAL DYNAMICS"],"max_cite":1.0,"pub_year":2018.0,"sources":"['ieee', 'wos']","rawkeys":["recognition","visual-cortex","deep learning","object","neural encoding","recurrent neural network","process memory","attention","system","cortical dynamics","representations","natural vision","information","models","activation","temporal receptive window"],"tags":["recognition","model","visual-cortex","neural networks","machine learning","neural encoding","objects","process memory","attention","representation","cortical dynamics","system","natural vision","information","activation","temporal receptive window"]},{"p_id":57482,"title":"Probabilistic Models and Generative Neural Networks: Towards an Unified Framework for Modeling Normal and Impaired Neurocognitive Functions","abstract":"Connectionist models can be characterized within the more general framework of probabilistic graphical models, which allow to efficiently describe complex statistical distributions involving a large number of interacting variables. This integration allows building more realistic computational models of cognitive functions, which more faithfully reflect the underlying neural mechanisms at the same time providing a useful bridge to higher-level descriptions in terms of Bayesian computations. Here we discuss a powerful class of graphical models that can be implemented as stochastic, generative neural networks. These models overcome many limitations associated with classic connectionist models, for example by exploiting unsupervised learning in hierarchical architectures (deep networks) and by taking into account top-down, predictive processing supported by feedback loops. We review some recent cognitive models based on generative networks, and we point out promising research directions to investigate neuropsychological disorders within this approach. Though further efforts are required in order to fill the gap between structured Bayesian models and more realistic, biophysical models of neuronal dynamics, we argue that generative neural networks have the potential to bridge these levels of analysis, thereby improving our understanding of the neural bases of cognition and of pathologies caused by brain damage.","keywords_author":["connectionist modeling","unsupervised learning","deep neural networks","probabilistic generative models","computational neuropsychology"],"keywords_other":["NEUROSCIENCE","BRAIN NETWORKS","VISUAL-CORTEX","REPRESENTATIONS","ATTENTION","COMPLEX NETWORKS","INSIGHTS","RECOGNITION","COGNITION","BAYESIAN-INFERENCE"],"max_cite":9.0,"pub_year":2016.0,"sources":"['wos']","rawkeys":["insights","neuroscience","recognition","visual-cortex","deep neural networks","bayesian-inference","attention","representations","computational neuropsychology","unsupervised learning","complex networks","connectionist modeling","brain networks","cognition","probabilistic generative models"],"tags":["insights","connectionist models","neuroscience","recognition","bayesian inference","visual-cortex","representation","attention","computational neuropsychology","unsupervised learning","convolutional neural network","complex networks","brain networks","cognition","probabilistic generative model"]},{"p_id":57486,"title":"Social is special: A normative framework for teaching with and learning from evaluative feedback","abstract":"Humans often attempt to influence one another's behavior using rewards and punishments. How does this work? Psychologists have often assumed that \"evaluative feedback\" influences behavior via standard learning mechanisms that learn from environmental contingencies. On this view, teaching with evaluative feedback involves leveraging learning systems designed to maximize an organism's positive outcomes. Yet, despite its parsimony, programs of research predicated on this assumption, such as ones in developmental psychology, animal behavior, and human-robot interaction, have had limited success. We offer an explanation by analyzing the logic of evaluative feedback and show that specialized learning mechanisms are uniquely favored in the case of evaluative feedback from a social partner. Specifically, evaluative feedback works best when it is treated as communicating information about the value of an action rather than as a form of reward to be maximized. This account suggests that human learning from evaluative feedback depends on inferences about communicative intent, goals and other mental states much like learning from other sources, such as demonstration, observation and instruction. Because these abilities are especially developed in humans, the present account also explains why evaluative feedback is far more widespread in humans than non-human animals. (C) 2017 Elsevier B.V. All rights reserved.","keywords_author":["Reward","Punishment","Theory of mind","Social learning","Evaluative feedback","Teaching"],"keywords_other":["REINFORCEMENT","INFANTS SELECTION","PUNISHMENT","MATERNAL ENCOURAGEMENT","PEDAGOGICAL CUES","RATIONAL IMITATION","CHILD COMPLIANCE","PRESCHOOLERS","COOPERATION","EVOLUTION"],"max_cite":2.0,"pub_year":2017.0,"sources":"['wos']","rawkeys":["cooperation","pedagogical cues","teaching","preschoolers","theory of mind","reinforcement","evolution","reward","social learning","maternal encouragement","child compliance","punishment","evaluative feedback","infants selection","rational imitation"],"tags":["cooperation","pedagogical cues","recognition","teaching","theory of mind","preschool","biological","reward","social learning","maternal encouragement","child compliance","punishment","evaluative feedback","infants selection","rational imitation"]},{"p_id":57501,"title":"Efficient Actor-Critic Algorithm with Hierarchical Model Learning and Planning","abstract":"To improve the convergence rate and the sample efficiency, two efficient learning methods AC-HMLP and RAC-HMLP (AC-HMLP with l(2)-regularization) are proposed by combining actor-critic algorithm with hierarchical model learning and planning. The hierarchical models consisting of the local and the global models, which are learned at the same time during learning of the value function and the policy, are approximated by local linear regression (LLR) and linear function approximation (LFA), respectively. Both the local model and the global model are applied to generate samples for planning; the former is used only if the state-prediction error does not surpass the threshold at each time step, while the latter is utilized at the end of each episode. The purpose of taking both models is to improve the sample efficiency and accelerate the convergence rate of the whole algorithm through fully utilizing the local and global information. Experimentally, AC-HMLP and RAC-HMLP are compared with three representative algorithms on two Reinforcement Learning (RL) benchmark problems. The results demonstrate that they perform best in terms of convergence rate and sample efficiency.","keywords_author":null,"keywords_other":["REINFORCEMENT","CONTINUOUS-TIME","NONLINEAR-SYSTEMS"],"max_cite":0.0,"pub_year":2016.0,"sources":"['wos']","rawkeys":["continuous-time","reinforcement","nonlinear-systems"],"tags":["nonlinear systems","continuous time","recognition"]},{"p_id":57505,"title":"Hybrid-augmented intelligence: collaboration and cognition","abstract":"The long-term goal of artificial intelligence (AI) is to make machines learn and think like human beings. Due to the high levels of uncertainty and vulnerability in human life and the open-ended nature of problems that humans are facing, no matter how intelligent machines are, they are unable to completely replace humans. Therefore, it is necessary to introduce human cognitive capabilities or human-like cognitive models into AI systems to develop a new form of AI, that is, hybrid-augmented intelligence. This form of AI or machine intelligence is a feasible and important developing model. Hybrid-augmented intelligence can be divided into two basic models: one is human-in-the-loop augmented intelligence with human-computer collaboration, and the other is cognitive computing based augmented intelligence, in which a cognitive model is embedded in the machine learning system. This survey describes a basic framework for human-computer collaborative hybrid-augmented intelligence, and the basic elements of hybrid-augmented intelligence based on cognitive computing. These elements include intuitive reasoning, causal models, evolution of memory and knowledge, especially the role and basic principles of intuitive reasoning for complex problem solving, and the cognitive learning framework for visual scene understanding based on memory and reasoning. Several typical applications of hybrid-augmented intelligence in related fields are given.","keywords_author":["Human-machine collaboration","Hybrid-augmented intelligence","Cognitive computing","Intuitive reasoning","Causal model","Cognitive mapping","Visual scene understanding","Self-driving cars"],"keywords_other":["IBM WATSON","HEALTH","UNMANNED AERIAL VEHICLE","ARTIFICIAL-INTELLIGENCE","SYSTEMS","BIG DATA","RECURRENT NEURAL-NETWORKS","CAUSALITY","RECOGNITION","MEMORY"],"max_cite":1.0,"pub_year":2017.0,"sources":"['wos']","rawkeys":["unmanned aerial vehicle","hybrid-augmented intelligence","recognition","recurrent neural-networks","memory","big data","systems","visual scene understanding","causality","causal model","self-driving cars","cognitive mapping","human-machine collaboration","ibm watson","intuitive reasoning","artificial-intelligence","health","cognitive computing"],"tags":["unmanned aerial vehicle","hybrid-augmented intelligence","cloud computing","recognition","memory","big data","neural networks","visual scene understanding","causality","machine learning","causal model","self-driving cars","system","cognitive maps","human-machine collaboration","ibm watson","intuitive reasoning","health"]},{"p_id":57510,"title":"Approximate Value Iteration Based on Numerical Quadrature","abstract":"Learning control policies has become an appealing alternative to the derivation of control laws based on classic control theory. Value iteration approaches have proven an outstanding flexibility, while maintaining high data efficiency when combined with probabilistic models to eliminate model bias. However, a major difficulty for these methods is that the state and action spaces must typically be discretized and often the value function update is analytically intractable. In this letter, we propose a projection based approximate value iteration approach, that employs numerical quadrature for the value function update step. It can handle continuous state and action spaces and noisy measurements of the system dynamics while learning globally optimal control from scratch. In addition, the proposed approximation technique allows for upper bounds on the approximation error, which can be used to guarantee convergence of the proposed approach to an optimal policy under some assumptions. Empirical evaluations on the mountain benchmark problem show the efficiency of the proposed approach and support our theoretical results.","keywords_author":["Optimization and optimal control","probability and statistical methods"],"keywords_other":["TIME","REINFORCEMENT","GRIDS","GAUSSIAN-PROCESSES"],"max_cite":0.0,"pub_year":2018.0,"sources":"['wos']","rawkeys":["gaussian-processes","grids","reinforcement","probability and statistical methods","time","optimization and optimal control"],"tags":["recognition","probability and statistical methods","time","optimization and optimal control","gaussian processes","grid"]},{"p_id":16559,"title":"Social Recommendation with Cross-Domain Transferable Knowledge","abstract":"\u00a9 2015 IEEE. Recommender systems can suffer from data sparsity and cold start issues. However, social networks, which enable users to build relationships and create different types of items, present an unprecedented opportunity to alleviate these issues. In this paper, we represent a social network as a star-structured hybrid graph centered on a social domain, which connects with other item domains. With this innovative representation, useful knowledge from an auxiliary domain can be transferred through the social domain to a target domain. Various factors of item transferability, including popularity and behavioral consistency, are determined. We propose a novel Hybrid Random Walk (HRW) method, which incorporates such factors, to select transferable items in auxiliary domains, bridge cross-domain knowledge with the social domain, and accurately predict user-item links in a target domain. Extensive experiments on a real social dataset demonstrate that HRW significantly outperforms existing approaches.","keywords_author":["cross-domain","random walk","Social recommendation","star-structured graph","transferability"],"keywords_other":["Random Walk","transferability","Cross-domain","Structured graphs","Social recommendation"],"max_cite":36.0,"pub_year":2015.0,"sources":"['scp']","rawkeys":["transferability","star-structured graph","cross-domain","structured graphs","random walk","social recommendation"],"tags":["recognition","star-structured graph","cross-domain","random walk","social recommendation","structural graph"]},{"p_id":82097,"title":"SHISS: Supervised hashing with informative set selection","abstract":"In recent years, there has been increasing demand for social security and identity authentication, which leads to the booming of many biometrics involved large-scale problems such as recognition, retrieval, and identification. In this case, traditional models are infeasible due to the limited capability for handling large-scale data. Therefore, hashing technique is becoming prevalent due to its low storage cost and fast query speed. Recently, researchers have shown that supervised hashing can achieve higher accuracy than unsupervised hashing by incorporating tag or label information of data for learning hashing function. However, existing supervised methods treat all training examples equally, ignoring the different impacts of various training examples on the learning process. As a result, their performance is not satisfactory under some practical situations. As an improvement, this paper proposes a new method called \"Supervised Hashing with Informative Set Selection\" (SHISS), which assumes that different training examples have different influence on the learning process, and their usage should follow a logic way during optimization. In particular, we propose two criteria, certainty and diversity, to evaluate the informativeness of the subsets of training examples and encourage the more informative subsets to be learned ahead of the less informative ones. The experiments on two typical image retrieval datasets and one face dataset demonstrate that the proposed SHISS obtains higher mean average precision and shows faster convergence rate compared with the state-of-the-art hashing methods. (C) 2017 Elsevier B.V. All rights reserved.","keywords_author":["Biometrics","Image retrieval","Supervised hashing","Information set selection"],"keywords_other":["IMAGE","REPRESENTATION","SCENE","RETRIEVAL","RECOGNITION"],"max_cite":0.0,"pub_year":2017.0,"sources":"['ieee', 'wos']","rawkeys":["supervised hashing","recognition","information set selection","image retrieval","representation","scene","image","biometrics","retrieval"],"tags":["supervised hashing","recognition","images","information set selection","image retrieval","representation","scene","biometrics","retrieval"]},{"p_id":82098,"title":"Image to Video Person Re-Identification by Learning Heterogeneous Dictionary Pair With Feature Projection Matrix","abstract":"Person re-identification plays an important role in video surveillance and forensics applications. In many cases, person re-identification needs to be conducted between image and video clip, e.g., re-identifying a suspect from large quantities of pedestrian videos given a single image of the suspect. We call re-identification in this scenario as image to video person re-identification (IVPR). In practice, image and video are usually represented with different features, and there usually exist large variations between frames within each video. These factors make matching between image and video become a very challenging task. In this paper, we propose a joint feature projection matrix and heterogeneous dictionary pair learning (PHDL) approach for IVPR. Specifically, the PHDL jointly learns an intra-video projection matrix and a pair of heterogeneous image and video dictionaries. With the learned projection matrix, the influence caused by the variations within each video on the matching can be reduced. With the learned dictionary pair, the heterogeneous image and video features can be transformed into coding coefficients with the same dimension, such that the matching can be conducted by using the coding coefficients. Furthermore, to ensure that the obtained coding coefficients own favorable discriminability, the PHDL designs a point-to-set coefficient discriminant term. To make better use of the complementary spatial-temporal and visual appearance information contained in pedestrian video data, we further propose a multi-view PHDL approach, which can fuse different video information effectively in the dictionary learning process. Experiments on four publicly available person sequence data sets demonstrate the effectiveness of the proposed approaches.","keywords_author":["Person re-identification","image to video person re-identification","heterogeneous dictionary pair learning","feature projection matrix","multi-view learning"],"keywords_other":["REPRESENTATION","RANKING","CLASSIFICATION","POINT","RECOGNITION","COUPLED DICTIONARY"],"max_cite":0.0,"pub_year":2018.0,"sources":"['ieee', 'wos']","rawkeys":["multi-view learning","recognition","coupled dictionary","heterogeneous dictionary pair learning","representation","person re-identification","image to video person re-identification","classification","ranking","point","feature projection matrix"],"tags":["multi-view learning","recognition","coupled dictionary","standards","heterogeneous dictionary pair learning","representation","person re-identification","image to video person re-identification","classification","point","feature projection matrix"]},{"p_id":82099,"title":"Supervised Discrete Hashing With Relaxation","abstract":"Data-dependent hashing has recently attracted attention due to being able to support efficient retrieval and storage of high-dimensional data, such as documents, images, and videos. In this paper, we propose a novel learning-based hashing method called \"supervised discrete hashing with relaxation\" (SDHR) based on \"supervised discrete hashing\" (SDH). SDH uses ordinary least squares regression and traditional zero-one matrix encoding of class label information as the regression target (code words), thus fixing the regression target. In SDHR, the regression target is instead optimized. The optimized regression target matrix satisfies a large margin constraint for correct classification of each example. Compared with SDH, which uses the traditional zero-one matrix, SDHR utilizes the learned regression target matrix and, therefore, more accurately measures the classification error of the regression model and is more flexible. As expected, SDHR generally outperforms SDH. Experimental results on two large-scale image data sets (CIFAR-10 and MNIST) and a large-scale and challenging face data set (FRGC) demonstrate the effectiveness and efficiency of SDHR.","keywords_author":["Data-dependent hashing","least squares regression","supervised discrete hashing (SDH)","supervised discrete hashing with relaxation (SDHR)"],"keywords_other":["LEARNING BINARY-CODES","PROCRUSTEAN APPROACH","SCENE","RECOGNITION","ITERATIVE QUANTIZATION","IMAGE RETRIEVAL"],"max_cite":4.0,"pub_year":2018.0,"sources":"['ieee', 'wos']","rawkeys":["recognition","learning binary-codes","least squares regression","supervised discrete hashing with relaxation (sdhr)","image retrieval","data-dependent hashing","procrustean approach","iterative quantization","scene","supervised discrete hashing (sdh)"],"tags":["recognition","learning binary-codes","supervised discrete hashing with relaxation (sdhr)","image retrieval","data-dependent hashing","procrustean approach","iterative quantization","scene","supervised discrete hashing (sdh)","least-squares regression"]},{"p_id":8372,"title":"Machine-learning-based coadaptive calibration for Brain-computer interfaces","abstract":"Brain-computer interfaces (BCIs) allow users to control a computer application by brain activity as acquired (e.g., by EEG). In our classic machine learning approach to BCIs, the participants undertake a calibration measurement without feedback to acquire data to train the BCI system. After the training, the user can control a BCI and improve the operation through some type of feedback. However, not all BCI users are able to perform sufficiently well during feedback operation. In fact, a nonnegligible portion of participants (estimated 15%-30%) cannot control the system (a BCI illiteracy problem, generic to all motor-imagery-based BCIs). We hypothesize that one main difficulty for a BCI user is the transition from offline calibration to online feedback. In this work, we investigate adaptive machine learning methods to eliminate offline calibration and analyze the performance of 11 volunteers in a BCI based on the modulation of sensorimotor rhythms. We present an adaptation scheme that individually guides the user. It starts with a subjectindependent classifier that evolves to a subject-optimized state-of-the-art classifier within one session while the user interacts continuously. These initial runs use supervised techniques for robust coadaptive learning of user and machine. Subsequent runs use unsupervised adaptation to track the features' drift during the session and provide an unbiased measure of BCI performance. Using this approach, without any offline calibration, six users, including one novice, obtained good performance after 3 to 6 minutes of adaptation. More important, this novel guided learning also allows participants with BCI illiteracy to gain significant control with the BCI in less than 60 minutes. In addition, one volunteer without sensorimotor idle rhythm peak at the beginning of the BCI experiment developed it during the course of the session and used voluntary modulation of its amplitude to control the feedback application. \u00a9 2011 Massachusetts Institute of Technology.","keywords_author":null,"keywords_other":["Feedback, Psychological","Algorithms","Adaptation, Physiological","Humans","Electroencephalography","Neuronal Plasticity","Brain","Adaptation, Psychological","Brain-Computer Interfaces","Artificial Intelligence","Calibration"],"max_cite":102.0,"pub_year":2011.0,"sources":"['scp', 'wos']","rawkeys":["artificial intelligence","brain-computer interfaces","calibration","physiological","humans","brain","psychological","neuronal plasticity","feedback","algorithms","adaptation","electroencephalography"],"tags":["brain-computer interfaces","recognition","calibration","physiology","machine learning","eeg","humans","brain","feedback","algorithms","adaptation","neuronal plasticity"]},{"p_id":57525,"title":"A new history experience replay design for model-free adaptive dynamic programming","abstract":"An adaptive dynamic programming (ADP) controller is a powerful control technique that has been investigated, designed and tested in a wide range of applications for solving optimal control problems in complex systems. The performance of the ADP controller is usually obtained by long training periods because the data usage efficiency is low as it discards the samples once used. History experience, also known as experience replay, is a powerful technique showing potential to accelerate the training process of learning and control. However, the existing design of history experience cannot be directly used for the model free ADP design, because the existing work focuses on the forward temporal difference (TD) information (e.g., state-action pair). This information is between the current time step and the future time step and will need a model network for future information prediction. This paper proposes a new history experience replay design to avoid the usage of the model network or identifier of the system environment. Specifically, we designed the experience tuple with one step backward state-action information and the TD can be achieved by a previous time step and a current time step. In addition, a systematic approach is proposed to integrate history experience in both the critic and action networks of the ADP controller design. The proposed approach is tested for two case studies: a cart-pole balancing task and a triple-link pendulum balancing task. For fair comparison, we set the same initial starting states and initial weight parameters for both approaches under the same simulation environment. The statistical results show that the proposed approach can improve the required average number of trials to succeed as well as the success rate. In general, the proposed approach improved the required average trial to succeed by 26.5% for cart-pole and 43% for triple-link balancing tasks. (C) 2017 Elsevier B.V. All rights reserved.","keywords_author":["Adaptive dynamic programming (ADP)","Model-free design","History experience","Reinforcement learning (RL)","Optimal control"],"keywords_other":["REINFORCEMENT"],"max_cite":0.0,"pub_year":2017.0,"sources":"['wos']","rawkeys":["adaptive dynamic programming (adp)","history experience","reinforcement","reinforcement learning (rl)","optimal control","model-free design"],"tags":["recognition","history experience","reinforcement learning","adaptive dynamic programming","optimal control","model-free design"]},{"p_id":82102,"title":"Clothes Co-Parsing Via Joint Image Segmentation and Labeling With Application to Clothing Retrieval","abstract":"This paper aims at developing an integrated system for clothing co-parsing (CCP), in order to jointly parse a set of clothing images (unsegmented but annotated with tags) into semantic configurations. A novel data-driven system consisting of two phases of inference is proposed. The first phase, referred as \"image cosegmentation,\" iterates to extract consistent regions on images and jointly refines the regions over all images by employing the exemplar-SVM technique [1]. In the second phase (i.e., \"region colabeling\"), we construct a multiimage graphical model by taking the segmented regions as vertices, and incorporating several contexts of clothing configuration (e.g., item locations and mutual interactions). The joint label assignment can be solved using the efficient Graph Cuts algorithm. In addition to evaluate our framework on the Fashionista dataset [2], we construct a dataset called the SYSU-Clothes dataset consisting of 2098 high-resolution street fashion photos to demonstrate the performance of our system. We achieve 90.29%\/88.23% segmentation accuracy and 65.52%\/63.89% recognition rate on the Fashionista and the SYSU-Clothes datasets, respectively, which are superior compared with the previous methods. Furthermore, we apply our method on a challenging task, i.e., cross-domain clothing retrieval: given user photo depicting a clothing image, retrieving the same clothing items from online shopping stores based on the fine-grained parsing results.","keywords_author":["Clothes recognition","fashion understanding","human-centric computing","image parsing"],"keywords_other":["REPRESENTATION","CONTEXT","RECOGNITION","COLOR"],"max_cite":10.0,"pub_year":2016.0,"sources":"['ieee', 'wos']","rawkeys":["clothes recognition","image parsing","recognition","representation","fashion understanding","human-centric computing","color","context"],"tags":["clothes recognition","image parsing","recognition","representation","fashion understanding","human-centric computing","color","context"]},{"p_id":82100,"title":"Vehicle Re-Identification by Deep Hidden Multi-View Inference","abstract":"Vehicle re-identification (re-ID) is an area that has received far less attention in the computer vision community than the prevalent person re-ID. Possible reasons for this slow progress are the lack of appropriate research data and the special 3D structure of a vehicle. Previous works have generally focused on some specific views (e.g., front); but, these methods are less effective in realistic scenarios, where vehicles usually appear in arbitrary views to cameras. In this paper, we focus on the uncertainty of vehicle viewpoint in re-ID, proposing two end-to-end deep architectures: the Spatially Concatenated ConvNet and convolutional neural network (CNN)-LSTM bi-directional loop. Our models exploit the great advantages of the CNN and long short-term memory (LSTM) to learn transformations across different viewpoints of vehicles. Thus, a multi-view vehicle representation containing all viewpoints' information can be inferred from the only one input view, and then used for learning to measure distance. To verify our models, we also introduce a Toy Car RE-ID data set with images from multiple viewpoints of 200 vehicles. We evaluate our proposed methods on the Toy Car RE-ID data set and the public Multi-View Car, VehicleID, and VeRi data sets. Experimental results illustrate that our models achieve consistent improvements over the state-of-the-art vehicle re-ID approaches.","keywords_author":["Vehicle re-identification","multi-view","spatially concatenated ConvNet","CNN-LSTM bi-directional loop"],"keywords_other":["ROAD","RECOGNITION","PERSON REIDENTIFICATION","CLASSIFICATION"],"max_cite":0.0,"pub_year":2018.0,"sources":"['ieee', 'wos']","rawkeys":["recognition","cnn-lstm bi-directional loop","road","spatially concatenated convnet","classification","multi-view","person reidentification","vehicle re-identification"],"tags":["recognition","cnn-lstm bi-directional loop","roads","spatially concatenated convnet","person re-identification","classification","vehicle re-identification","multi-views"]},{"p_id":82108,"title":"Topic driven multimodal similarity learning with multi-view voted convolutional features","abstract":"Similarity (and distance metric) learning plays a very important role in many artificial intelligence tasks aiming at quantifying the relevance between objects. We address the challenge of learning complex relation patterns from data objects exhibiting heterogeneous properties, and develop an effective multi view multimodal similarity learning model with much improved learning performance and model interpretability. The proposed method firstly computes multi-view convolutional features to achieve improved object representation, then analyses the similarities between objects by operating over multiple hidden relation types (modalities), and finally fine-tunes the entire model variables via back -propagating a ranking loss to the convolutional layers. We develop a topic-driven initialization scheme, so that each learned relation type can be interpreted as a representative of semantic topics of the objects. To improve model interpretability and generalization, sparsity is imposed over these hidden relations. The proposed method is evaluated by solving the image retrieval task using challenging image datasets, and is compared with seven state-of-the-art algorithms in the field. Experimental results demonstrate significant performance improvement of the proposed method over the competing ones. (C) 2017 Elsevier Ltd. All rights reserved.","keywords_author":["Convolutional auto-encoder","Representation learning","Multi-view learning","Multimodal similarity learning"],"keywords_other":["REGRESSION","SEARCH","PERSON REIDENTIFICATION","CLASSIFICATION","SCALE IMAGE RETRIEVAL","NEURAL-NETWORKS","RECOGNITION"],"max_cite":0.0,"pub_year":2017.0,"sources":"['ieee', 'wos']","rawkeys":["neural-networks","multi-view learning","recognition","search","scale image retrieval","convolutional auto-encoder","representation learning","classification","person reidentification","regression","multimodal similarity learning"],"tags":["multi-view learning","recognition","search","neural networks","scale image retrieval","person re-identification","representation learning","classification","convolutional autoencoder","regression","multimodal similarity learning"]},{"p_id":82109,"title":"Fast Supervised Discrete Hashing","abstract":"Learning-based hashing algorithms are \"hot topics\" because they can greatly increase the scale at which existing methods operate. In this paper, we propose a new learning-based hashing method called \"fast supervised discrete hashing\" (FSDH) based on \"supervised discrete hashing\" (SDH). Regressing the training examples (or hash code) to the corresponding class labels is widely used in ordinary least squares regression. Rather than adopting this method, FSDH uses a very simple yet effective regression of the class labels of training examples to the corresponding hash code to accelerate the algorithm. To the best of our knowledge, this strategy has not previously been used for hashing. Traditional SDH decomposes the optimization into three sub-problems, with the most critical sub-problem - discrete optimization for binary hash codes - solved using iterative discrete cyclic coordinate descent (DCC), which is time-consuming. However, FSDH has a closed-form solution and only requires a single rather than iterative hash code-solving step, which is highly efficient. Furthermore, FSDH is usually faster than SDH for solving the projection matrix for least squares regression, making FSDH generally faster than SDH. For example, our results show that FSDH is about 12-times faster than SDH when the number of hashing bits is 128 on the CIFAR-10 data base, and FSDH is about 151-times faster than FastHash when the number of hashing bits is 64 on the MNIST data-base. Our experimental results show that FSDH is not only fast, but also outperforms other comparative methods.","keywords_author":["Fast supervised discrete hashing","supervised discrete hashing","learning-based hashing","least squares regression"],"keywords_other":["LEARNING BINARY-CODES","PROCRUSTEAN APPROACH","REPRESENTATION","SCENE","RECOGNITION","ITERATIVE QUANTIZATION","IMAGE RETRIEVAL"],"max_cite":2.0,"pub_year":2018.0,"sources":"['ieee', 'wos']","rawkeys":["recognition","supervised discrete hashing","learning binary-codes","least squares regression","image retrieval","representation","procrustean approach","iterative quantization","scene","learning-based hashing","fast supervised discrete hashing"],"tags":["learning based hashing","recognition","learning binary-codes","image retrieval","representation","procrustean approach","iterative quantization","scene","supervised discrete hashing (sdh)","least-squares regression","fast supervised discrete hashing"]},{"p_id":82112,"title":"Attention driven multi-modal similarity learning","abstract":"To learn a function for measuring similarity or relevance between objects is an important machine learning task, referred to as similarity learning. Conventional methods are usually insufficient for processing complex patterns, while more sophisticated methods produce results supported by parameters and mathematical operations that are hard to interpret. To improve both model robustness and interpretability, we propose a novel attention driven multi-modal algorithm, which learns a distributed similarity score over different relation modalities and develops an interaction-oriented dynamic attention mechanism to selectively focus on salient patches of objects of interest. Neural networks are used to generate a set of high-level representation vectors for both the entire object and its segmented patches. Multi-view local neighboring structures between objects are encoded in the high-level object representation through an unsupervised pre-training procedure. By initializing the relation embeddings with object cluster centers, each relation modality can be reasonably interpreted as a semantic topic. A layer-wise training scheme based on a mixture of unsupervised and supervised training is proposed to improve generalization. The effectiveness of the proposed method and its superior performance compared against state-of-the-art algorithms are demonstrated through evaluations based on different image retrieval tasks. (C) 2017 Elsevier Inc. All rights reserved.","keywords_author":["Multi-modal similarity","Attention mechanism","Representation learning","Multi-view","Neural network"],"keywords_other":["FEATURES","REPRESENTATION","NETWORKS","SEARCH","PERSON REIDENTIFICATION","CLASSIFICATION","SCALE IMAGE RETRIEVAL","RECOGNITION"],"max_cite":0.0,"pub_year":2017.0,"sources":"['ieee', 'wos']","rawkeys":["neural network","recognition","search","features","scale image retrieval","representation","representation learning","networks","attention mechanism","multi-view","classification","person reidentification","multi-modal similarity"],"tags":["recognition","search","features","neural networks","attention mechanisms","representation","scale image retrieval","person re-identification","representation learning","networks","classification","multi-modal similarity","multi-views"]},{"p_id":82113,"title":"Cross-Domain Shoe Retrieval With a Semantic Hierarchy of Attribute Classification Network","abstract":"Cross-domain shoe image retrieval is a challenging problem, because the query photo from the street domain (daily life scenario) and the reference photo in the online domain (online shop images) have significant visual differences due to the viewpoint and scale variation, self-occlusion, and cluttered background. This paper proposes the semantic hierarchy of attribute convolutional neural network (SHOE-CNN) with a three-level feature representation for discriminative shoe feature expression and efficient retrieval. The SHOE-CNN with its newly designed loss function systematically merges semantic attributes of closer visual appearances to prevent shoe images with the obvious visual differences being confused with each other; the features extracted from image, region, and part levels effectively match the shoe images across different domains. We collect a large-scale shoe data set composed of 14341 street domain and 12652 corresponding online domain images with fine-grained attributes to train our network and evaluate our system. The top-20 retrieval accuracy improves significantly over the solution with the pre-trained CNN features.","keywords_author":["Semantic hierarchy","cross-domain","instance retrieval","convolutional neural network"],"keywords_other":["FEATURES","RANKING","IMAGE SIMILARITY","RECOGNITION","SHAPE"],"max_cite":0.0,"pub_year":2017.0,"sources":"['ieee', 'wos']","rawkeys":["instance retrieval","recognition","semantic hierarchy","features","cross-domain","shape","convolutional neural network","ranking","image similarity"],"tags":["instance retrieval","recognition","semantic hierarchies","features","standards","cross-domain","shape","convolutional neural network","image similarity"]},{"p_id":32964,"title":"Automatic stress-relieving music recommendation system based on photoplethysmography-derived heart rate variability analysis","abstract":"\u00a9 2014 IEEE. This paper presents an automatic stress-relieving music recommendation system (ASMRS) for individual music listeners. The ASMRS uses a portable, wireless photoplethysmography module with a finger-type sensor, and a program that translates heartbeat signals from the sensor to the stress index. The sympathovagal balance index (SVI) was calculated from heart rate variability to assess the user's stress levels while listening to music. Twenty-two healthy volunteers participated in the experiment. The results have shown that the participants' SVI values are highly correlated with their prespecified music preferences. The sensitivity and specificity of the favorable music classification also improved as the number of music repetitions increased to 20 times. Based on the SVI values, the system automatically recommends favorable music lists to relieve stress for individuals.","keywords_author":null,"keywords_other":["Adolescent","Heart Rate","Male","Photoplethysmography","User-Computer Interface","Vagus Nerve","Humans","Stress, Psychological","ROC Curve","Music Therapy","Music","Automation","Female"],"max_cite":3.0,"pub_year":2014.0,"sources":"['scp']","rawkeys":["adolescent","male","female","photoplethysmography","stress","humans","automation","psychological","roc curve","user-computer interface","vagus nerve","music therapy","heart rate","music"],"tags":["adolescent","recognition","automated","male","female","photoplethysmography","stress","humans","roc curve","user-computer interface","vagus nerve","music therapy","heart rate","music"]},{"p_id":202,"title":"Deep Learning for Acoustic Modeling in Parametric Speech Generation","abstract":"Hidden Markov models (HMMs) and Gaussian mixture models (GMMs) are the two most common types of acoustic models used in statistical parametric approaches for generating low-level speech waveforms from high-level symbolic inputs via intermediate acoustic feature sequences. However, these models have their limitations in representing complex, nonlinear relationships between the speech generation inputs and the acoustic features. Inspired by the intrinsically hierarchical process of human speech production and by the successful application of deep neural networks (DNNs) to automatic speech recognition (ASR), deep learning techniques have also been applied successfully to speech generation, as reported in recent literature. This article systematically reviews these emerging speech generation approaches, with the dual goal of helping readers gain a better understanding of the existing techniques as well as stimulating new work in the burgeoning area of deep learning for parametric speech generation.","keywords_author":null,"keywords_other":["human speech production","automatic speech recognition","Vocoders","ALGORITHM","high-level symbolic inputs","REPRESENTATIONS","speech recognition","Acoustic signal detection","Speech processing","acoustic models","DNN","ASR","Speech synthesis","ENHANCEMENT","BELIEF NETWORKS","parametric speech generation","acoustic features","EXPERTS","mixture models","acoustic signal processing","hidden Markov models","deep learning","VOICE CONVERSION","Gaussian processes","HMM","RECOGNITION","low-level speech waveforms","Hidden Markov models","GMM","neural nets","Gaussian mixture models","SYNTHESIS SYSTEM","deep neural networks","statistical parametric approach","burgeoning area","NEURAL-NETWORKS","acoustic modeling","Speech recognition","intermediate acoustic feature sequences"],"max_cite":46.0,"pub_year":2015.0,"sources":"['wos', 'ieee']","rawkeys":["human speech production","automatic speech recognition","high-level symbolic inputs","speech recognition","hmm","acoustic models","gaussian mixture models","parametric speech generation","neural-networks","enhancement","acoustic features","dnn","mixture models","acoustic signal processing","algorithm","recognition","vocoders","deep learning","experts","asr","acoustic signal detection","low-level speech waveforms","belief networks","neural nets","hidden markov models","synthesis system","deep neural networks","speech processing","gmm","burgeoning area","acoustic modeling","representations","speech synthesis","statistical parametric approach","gaussian processes","intermediate acoustic feature sequences","voice conversion"],"tags":["human speech production","automatic speech recognition","high-level symbolic inputs","speech recognition","convolutional neural network","parametric speech generation","enhancement","machine learning","acoustic features","mixture models","acoustic signal processing","algorithms","recognition","vocoders","neural networks","experts","acoustic signal detection","low-level speech waveforms","belief networks","gaussian mixture model","hidden markov models","synthesis system","acoustic model","representation","speech processing","statistical parametric approach","burgeoning area","speech synthesis","gaussian processes","intermediate acoustic feature sequences","voice conversion"]},{"p_id":82131,"title":"Fusion of multiple channel features for person re-identification","abstract":"Person re-identification plays an important role for automatic search of a person's presence in a surveillance video, and feature representation is a critical and fundamental problem for person re-identification. Besides, an reliable feature representation should effectively adapt to the changes of illumination, pose, viewpoint, etc. In this paper, we propose an effective feature representation called fusion of multiple channel features (FMCF) which captures different low-level features from multiple channels of HSV color space, considering the characteristics of different color channels and fusing color, texture and correlation of spatial structure. Furthermore, it takes advantage of an overlapping strategy to eliminate contrast of local cells in an image. In addition, we apply the simple weight distance metric to measure the similarity of different images, rather than metric learning which relies on a specific feature and requires more computing resources. Finally, we apply the proposed method of FMCF on the i-LIDS Multiple-Camera Tracking Scenario(MCTS) and CUHK-01 person re-identification datasets, and the experimental results demonstrate that it is more robust to the variation of visual appearance. (C) 2016 Elsevier B.V. All rights reserved.","keywords_author":["Person re-identification","Multiple channel","Feature fusion"],"keywords_other":["CLASSIFICATION","COLOR","RECOGNITION","HISTOGRAM","IMAGE RETRIEVAL","PATTERNS"],"max_cite":2.0,"pub_year":2016.0,"sources":"['ieee', 'wos']","rawkeys":["recognition","multiple channel","image retrieval","patterns","person re-identification","histogram","color","classification","feature fusion"],"tags":["recognition","image retrieval","histograms","patterns","person re-identification","color","classification","multiple channels","feature fusion"]},{"p_id":214,"title":"Learning Deep Representation for Face Alignment with Auxiliary Attributes","abstract":"In this study, we show that landmark detection or face alignment task is not a single and independent problem. Instead, its robustness can be greatly improved with auxiliary information. Specifically, we jointly optimize landmark detection together with the recognition of heterogeneous but subtly correlated facial attributes, such as gender, expression, and appearance attributes. This is non-trivial since different attribute inference tasks have different learning difficulties and convergence rates. To address this problem, we formulate a novel tasks-constrained deep model, which not only learns the inter-task correlation but also employs dynamic task coefficients to facilitate the optimization convergence when learning multiple complex tasks. Extensive evaluations show that the proposed task-constrained learning (i) outperforms existing face alignment methods, especially in dealing with faces with severe occlusion and pose variation, and (ii) reduces model complexity drastically compared to the state-of-the-art methods based on cascaded deep model.","keywords_author":["convolutional network","deep learning","Face Alignment","face landmark detection","Face Alignment","face landmark detection","deep learning","convolutional network","Face Alignment","Face Landmark Detection","Deep Learning","Convolutional Network"],"keywords_other":["convergence","pose estimation","REGRESSION","occlusion","task-constrained learning","Face Alignment","Glass","auxiliary attributes","cascaded deep model","Correlation","face landmark detection","Convergence","Deep learning","Convolutional networks","Face alignment","Face landmarks","optimization convergence","face recognition","learning deep representation","object detection","attribute inference task","Face","Learning difficulties","convolutional network","image representation","Covariance matrices","face alignment","dynamic task coefficients","learning (artificial intelligence)","State-of-the-art methods","correlation methods","deep learning","Training","pose variation","RECOGNITION","WILD","landmark detection","auxiliary information","optimisation","Gaussian distribution","task-constrained deep model","MODELS","facial attributes","Auxiliary information","Optimization convergence","intertask correlation"],"max_cite":68.0,"pub_year":2016.0,"sources":"['ieee', 'scp', 'wos']","rawkeys":["convergence","pose estimation","occlusion","state-of-the-art methods","task-constrained learning","auxiliary attributes","cascaded deep model","face landmark detection","covariance matrices","convolutional networks","optimization convergence","face recognition","learning deep representation","models","attribute inference task","gaussian distribution","object detection","regression","convolutional network","image representation","dynamic task coefficients","face alignment","learning (artificial intelligence)","correlation methods","recognition","deep learning","glass","training","face landmarks","learning difficulties","pose variation","landmark detection","face","correlation","wild","auxiliary information","optimisation","task-constrained deep model","facial attributes","intertask correlation"],"tags":["pose estimation","state-of-the-art methods","occlusion","task-constrained learning","auxiliary attributes","cascaded deep model","convolutional neural network","covariance matrices","machine learning","optimization convergence","face recognition","mathematics","learning deep representation","attribute inference task","gaussian distribution","object detection","regression","image representation","dynamic task coefficients","face alignment","recognition","correlation methods","glass","training","face landmarks","learning difficulties","pose variation","landmark detection","face","correlation","wild","auxiliary information","face landmarks detection","model","optimisation","task-constrained deep model","facial attributes","intertask correlation"]},{"p_id":217,"title":"Latent Hierarchical Model of Temporal Structure for Complex Activity Classification","abstract":"Modeling the temporal structure of sub-activities is an important yet challenging problem in complex activity classification. This paper proposes a latent hierarchical model (LHM) to describe the decomposition of complex activity into sub-activities in a hierarchical way. The LHM has a tree-structure, where each node corresponds to a video segment (sub-activity) at certain temporal scale. The starting and ending time points of each sub-activity are represented by two latent variables, which are automatically determined during the inference process. We formulate the training problem of the LHM in a latent kernelized SVM framework and develop an efficient cascade inference method to speed up classification. The advantages of our methods come from: 1) LHM models the complex activity with a deep structure, which is decomposed into sub-activities in a coarse-to-fine manner and 2) the starting and ending time points of each segment are adaptively determined to deal with the temporal displacement and duration variation of sub-activity. We conduct experiments on three datasets: 1) the KTH; 2) the Hollywood2; and 3) the Olympic Sports. The experimental results show the effectiveness of the LHM in complex activity classification. With dense features, our LHM achieves the state-of-the-art performance on the Hollywood2 dataset and the Olympic Sports dataset.","keywords_author":["Activity classification","hierarchical model","deep structure","latent learning","cascade inference","Activity classification","hierarchical model","deep structure","latent learning","cascade inference"],"keywords_other":["tree-structure","COMPUTER VISION","inference mechanisms","DENSE","Mathematical model","latent kernelized SVM framework","inference process","LHM","latent hierarchical model","temporal structure modeling","image classification","Computational modeling","IMAGE CLASSIFICATION","Olympic Sports","Hollywood2","KTH","Inference algorithms","Training","video signal processing","SUPPORT VECTOR MACHINES","RECOGNITION","complex activity classification","Hidden Markov models","training problem","temporal displacement","Adaptation models","video segment","complex activity decomposition","cascade inference method","support vector machines","sport","Support vector machines"],"max_cite":36.0,"pub_year":2013.0,"sources":"['wos', 'ieee']","rawkeys":["tree-structure","hollywood2","inference mechanisms","hierarchical model","adaptation models","latent learning","inference process","latent hierarchical model","dense","image classification","temporal structure modeling","inference algorithms","lhm","deep structure","olympic sports","computational modeling","mathematical model","recognition","training","video signal processing","activity classification","complex activity classification","computer vision","training problem","temporal displacement","complex activity decomposition","hidden markov models","video segment","cascade inference method","cascade inference","latent kernelized svm framework","support vector machines","sport","kth"],"tags":["hollywood2","inference mechanisms","inference algorithm","video segmentation","hierarchical model","adaptation models","latent learning","inference process","latent hierarchical model","dense","image classification","temporal structure modeling","machine learning","lhm","deep structure","olympic sports","computational modeling","mathematical model","recognition","training problems","training","video signal processing","complex activity classification","tree structures","activity classifications","computer vision","temporal displacement","complex activity decomposition","hidden markov models","cascade inference method","sports","cascade inference","latent kernelized svm framework","kth"]},{"p_id":82137,"title":"Robust face detection using local CNN and SVM based on kernel combination","abstract":"One key challenge of face detection is the large appearance variations due to some real-world factors, such as viewpoint, extreme illuminations and expression changes, which lead to the large intra-class variations and making the detection algorithm is not robust enough. In this paper, we propose a locality sensitive support vector machine using kernel combination (LS-KC-SVM) algorithm to solve the above two problems. First, we employ the locality-sensitive SVM (LSSVM) to construct a local model on each local region, which can handle the classification task easier due to smaller within-class variation. Second, motivated by the idea that local features are more robust compared with global features, we use multiple local CNNs to jointly learn local facial features because of the powerful strength of CNN learning characteristic. In order to use this property of local features effectively, we apply the global and local kernels to the features and introduce the combination kernel to the LSSVM. Extensive experiments demonstrate the robustness and efficiency of our algorithm by comparing it with several popular face detection algorithms on the widely used CMU+MIT dataset and FDDB dataset. (C) 2016 Elsevier B.V. All rights reserved.","keywords_author":["Face detection","Convolutional neural network","Kernel combination","Support vector machine","Local classifier"],"keywords_other":["SCALE IMAGE SEARCH","FEATURES","SUPPORT VECTOR MACHINE","OPTIMUM-PATH FOREST","CLASSIFICATION","RECOGNITION"],"max_cite":8.0,"pub_year":2016.0,"sources":"['ieee', 'wos']","rawkeys":["kernel combination","recognition","features","scale image search","face detection","classification","convolutional neural network","optimum-path forest","support vector machine","local classifier"],"tags":["kernel combination","recognition","features","scale image search","optimum-path forests","machine learning","face detection","classification","convolutional neural network","local classifier"]},{"p_id":82138,"title":"Person re-ID while Crossing Different Cameras: Combination of Salient-Gaussian Weighted BossaNova and Fisher Vector Encodings","abstract":"Person re-identification (re-ID) is a challenging task in the camera surveillance field, since it addresses the problem of re-identifying people across multiple non-overlapping cameras. Most of existing approaches have been concentrated on: 1) achieving a robust and effective feature representation; and 2) enforcing discriminative metric learning to predict if two images represent the same identity. In this context, we present a new approach for person re-ID built upon multi-level descriptors. This is achieved by combining three complementary representations: salient-Gaussian Fisher Vector (SGFV) encoding method, salient-Gaussian BossaNova (SGBN) histogram encoding method and deep Convolutional Neural Network (CNN) features. The two first methods adapt the histogram encoding framework to the person re-ID task. This is achieved by integrating the pedestrian saliency map and the spatial location information, in the histogram encoding process. On one hand, human saliency is reliable and distinctive in the person re-ID task, since it can model the uniqueness of the identity. On the other hand, localizing a person in the image can effectively discard noisy background information. Finally, one of the most advanced metric learning in person re-ID: the Cross-view Quadratic Discriminant Analysis (XQDA) is applied on the top of the resulting description. The proposed method yields promising person re-ID results on two challenging image-based person re-ID benchmarks: CUHK03 and Market-1501.","keywords_author":["Person re-identification","histogram encoding","fisher vector","BossaNova","Convolutional Neural Network (CNN)","salient weight","Gaussian weight"],"keywords_other":["RECOGNITION","REIDENTIFICATION","CLASSIFICATION"],"max_cite":0.0,"pub_year":2017.0,"sources":"['wos']","rawkeys":["bossanova","recognition","gaussian weight","salient weight","reidentification","person re-identification","convolutional neural network (cnn)","fisher vector","classification","histogram encoding"],"tags":["bossanova","recognition","gaussian weight","salient weight","person re-identification","classification","convolutional neural network","fisher vectors","histogram encoding","re identifications"]},{"p_id":24802,"title":"Instagram photos reveal predictive markers of depression","abstract":"\u00a9 2017, The Author(s).Using Instagram data from 166 individuals, we applied machine learning tools to successfully identify markers of depression. Statistical features were computationally extracted from 43,950 participant Instagram photos, using color analysis, metadata components, and algorithmic face detection. Resulting models outperformed general practitioners\u2019 average unassisted diagnostic success rate for depression. These results held even when the analysis was restricted to posts made before depressed individuals were first diagnosed. Human ratings of photo attributes (happy, sad, etc.) were weaker predictors of depression, and were uncorrelated with computationally-generated features. These results suggest new avenues for early screening and detection of mental illness.","keywords_author":["computational social science","depression","machine learning","psychology","social media"],"keywords_other":["Statistical features","Computational social science","Mental illness","Social media","General practitioners","depression","psychology","Applied machine learning"],"max_cite":8.0,"pub_year":2017.0,"sources":"['scp']","rawkeys":["statistical features","social media","machine learning","general practitioners","applied machine learning","depression","psychology","mental illness","computational social science"],"tags":["recognition","statistical features","social media","machine learning","general practitioners","applied machine learning","depression","mental illness","computational social science"]},{"p_id":49381,"title":"Multi-layer local energy patterns for texture representation and classification","abstract":"\u00a9 2016, Springer-Verlag Berlin Heidelberg.Motivated by the recent success of deep networks in providing effective and abstract image representations, in this paper, a multi-layer architecture called the multi-layer local energy patterns (ML-LEP) is proposed for texture representation and classification. The proposed approach follows a multi-layer convolutional neural network paradigm and is built upon the single-layer local energy pattern (LEP) approach, a statistical histogram-based method for texture representation. An important aspect of the proposed multi-layer method compared to other deep convolutional architectures is bypassing the computationally expensive learning stage using fixed filters. As such, the proposed training-free network circumvents the need for large data for learning system parameters. An extensive investigation is carried out to determine the merits of different nonlinear operators in the proposed architecture. For this purpose, different nonlinearities including an energy-based nonlinearity, the absolute operator as well as the rectifier functions are extensively investigated and compared against each other. Extensive experiments conducted on three challenging databases of KTH-TIPS, KTH-TIPS2-a and the UIUC indicate that the extension of the LEP method to the multi-layer LEP is effective and leads to better performance. Moreover, the proposed ML-LEP approach is compared to several other well-known descriptors in the field, achieving the best reported performance on the KTH-TIPS and the KTH-TIPS2-a databases despite being training-free.","keywords_author":["Convolutional neural networks","Deep learning","Multi-layer local energy patterns","Texture representation","Deep learning","Convolutional neural networks","Texture representation","Multi-layer local energy patterns"],"keywords_other":["Deep learning","Multi-layer architectures","FEATURES","LBP","Statistical histograms","Image representations","Texture representation","RECOGNITION","Convolutional neural network","SCALE","DESCRIPTOR","Local energy patterns","Proposed architectures","IMAGES"],"max_cite":0.0,"pub_year":2016.0,"sources":"['ieee', 'scp', 'wos']","rawkeys":["multi-layer architectures","recognition","convolutional neural networks","images","features","deep learning","multi-layer local energy patterns","proposed architectures","statistical histograms","local energy patterns","convolutional neural network","descriptor","texture representation","scale","lbp","image representations"],"tags":["multi-layer architectures","recognition","images","multi-layer local energy patterns","features","proposed architectures","machine learning","statistical histograms","local energy patterns","convolutional neural network","local binary patterns","descriptors","texture representation","scale","image representation"]},{"p_id":231,"title":"Change Detection in Synthetic Aperture Radar Images Based on Deep Neural Networks","abstract":"This paper presents a novel change detection approach for synthetic aperture radar images based on deep learning. The approach accomplishes the detection of the changed and unchanged areas by designing a deep neural network. The main guideline is to produce a change detection map directly from two images with the trained deep neural network. The method can omit the process of generating a difference image (DI) that shows difference degrees between multitemporal synthetic aperture radar images. Thus, it can avoid the effect of the DI on the change detection results. The learning algorithm for deep architectures includes unsupervised feature learning and supervised fine-tuning to complete classification. The unsupervised feature learning aims at learning the representation of the relationships between the two images. In addition, the supervised fine-tuning aims at learning the concepts of the changed and unchanged pixels. Experiments on real data sets and theoretical analysis indicate the advantages, feasibility, and potential of the proposed method. Moreover, based on the results achieved by various traditional algorithms, respectively, deep learning can further improve the detection performance.","keywords_author":["Deep learning","image change detection","neural network","synthetic aperture radar (SAR)","Deep learning","image change detection","neural network","synthetic aperture radar (SAR)","Deep learning","image change detection","neural network","synthetic aperture radar (SAR)."],"keywords_other":["neural network","Synthetic aperture radar","Joints","Complete classification","NETS","UNSUPERVISED CHANGE DETECTION","ALGORITHM","MULTITEMPORAL SAR IMAGES","Neural networks","image change detection","Difference degrees","Deep learning","Difference images","Algorithm design and analysis","Deep architectures","unsupervised feature learning algorithm","change detection map","Detection performance","object detection","multitemporal synthetic aperture radar image","Deep neural networks","Multitemporal synthetic aperture radar images","Noise","synthetic aperture radar","Training","change detection approach","Unsupervised feature learning","MODEL","RECOGNITION","supervised fine-tuning","synthetic aperture radar (SAR)","unsupervised learning","neural nets","deep neural networks","deep architecture","feature extraction","Change detection algorithms"],"max_cite":60.0,"pub_year":2016.0,"sources":"['ieee', 'scp', 'wos']","rawkeys":["neural network","change detection algorithms","complete classification","synthetic aperture radar (sar)","difference images","image change detection","change detection map","detection performance","unsupervised feature learning algorithm","multitemporal synthetic aperture radar images","difference degrees","object detection","multitemporal synthetic aperture radar image","joints","algorithm","recognition","noise","deep learning","neural networks","change detection approach","synthetic aperture radar","deep architectures","training","supervised fine-tuning","unsupervised learning","nets","algorithm design and analysis","neural nets","unsupervised feature learning","model","multitemporal sar images","deep neural networks","unsupervised change detection","deep architecture","feature extraction"],"tags":["change detection algorithms","complete classification","difference images","convolutional neural network","image change detection","change detection map","detection performance","machine learning","difference degrees","multitemporal synthetic aperture radar images","object detection","algorithms","joints","recognition","noise","neural networks","synthetic aperture radar","change detection approach","training","deep architectures","supervised fine-tuning","unsupervised learning","nets","algorithm design and analysis","unsupervised feature learning","model","multitemporal sar images","unsupervised feature learning algorithms","unsupervised change detection","feature extraction"]},{"p_id":33004,"title":"Discrimination of oil slicks and lookalikes in polarimetric SAR images using CNN","abstract":"\u00a9 2017 by the authors. Licensee MDPI, Basel, Switzerland.Oil slicks and lookalikes (e.g., plant oil and oil emulsion) all appear as dark areas in polarimetric Synthetic Aperture Radar (SAR) images and are highly heterogeneous, so it is very difficult to use a single feature that can allow classification of dark objects in polarimetric SAR images as oil slicks or lookalikes. We established multi-feature fusion to support the discrimination of oil slicks and lookalikes. In the paper, simple discrimination analysis is used to rationalize a preferred features subset. The features analyzed include entropy, alpha, and Single-bounce Eigenvalue Relative Difference (SERD) in the C-band polarimetric mode. We also propose a novel SAR image discrimination method for oil slicks and lookalikes based on Convolutional Neural Network (CNN). The regions of interest are selected as the training and testing samples for CNN on the three kinds of polarimetric feature images. The proposed method is applied to a training data set of 5400 samples, including 1800 crude oil, 1800 plant oil, and 1800 oil emulsion samples. In the end, the effectiveness of the method is demonstrated through the analysis of some experimental results. The classification accuracy obtained using 900 samples of test data is 91.33%. It is here observed that the proposed method not only can accurately identify the dark spots on SAR images but also verify the ability of the proposed algorithm to classify unstructured features.","keywords_author":["Convolutional Neural Network (CNN)","Feature fusion","Lookalikes","Oil slicks","Pattern recognition","Synthetic Aperture Radar (SAR)","Synthetic Aperture Radar (SAR)","pattern recognition","oil slicks","lookalikes","feature fusion","Convolutional Neural Network (CNN)"],"keywords_other":["DEEP CONVOLUTIONAL NETWORKS","Feature fusion","FEATURES","Single bounce eigenvalue relative differences","Lookalikes","SYNTHETIC-APERTURE RADAR","Discrimination analysis","CLASSIFICATION","SPILL DETECTION","RECOGNITION","Convolutional neural network","NEURAL-NETWORK","Oil slicks","Polarimetric synthetic aperture radars","Classification accuracy"],"max_cite":3.0,"pub_year":2017.0,"sources":"['ieee', 'scp', 'wos']","rawkeys":["deep convolutional networks","polarimetric synthetic aperture radars","recognition","features","oil slicks","classification accuracy","lookalikes","synthetic aperture radar (sar)","convolutional neural network (cnn)","discrimination analysis","classification","convolutional neural network","neural-network","pattern recognition","spill detection","feature fusion","single bounce eigenvalue relative differences","synthetic-aperture radar"],"tags":["polarimetric synthetic aperture radars","recognition","features","denoising autoencoder","neural networks","classification accuracy","lookalikes","oil slicks","synthetic aperture radar","classification","convolutional neural network","pattern recognition","spill detection","feature fusion","single bounce eigenvalue relative differences"]},{"p_id":16627,"title":"3D object retrieval with stacked local convolutional autoencoder","abstract":"\u00a9 2014 Elsevier B.V.The success of object recognition and retrieval is largely determined by data representation. A good feature descriptor can detect the high-level abstraction of objects, which contains much discriminative information. In this paper, a novel 3D object retrieval method is proposed based on stacked local convolutional autoencoder (SLCAE). In this approach, the greedy layerwise strategy is applied to train SLCAE, and gradient descent method is used for training each layer. After the processing of training, the representations of input data can be obtained, regarded as the features of 3D objects. The experiments are conducted on three publicly available 3D object datasets, and the results demonstrate that the proposed method can greatly improve 3D object retrieval performance, compared with several state-of-the-art methods.","keywords_author":["3D object retrieval","Object representation","Stacked local convolutional autoencoder","Object representation","Stacked local convolutional autoencoder","3D object retrieval"],"keywords_other":["MODEL RETRIEVAL","High-level abstraction","Data representations","Object representations","State-of-the-art methods","GRAPH","REPRESENTATION","FRAMEWORK","MACHINE","SEARCH","SYSTEM","RECOGNITION","Auto encoders","SHAPE DESCRIPTOR","Gradient Descent method","Feature descriptors","SIMILARITY","3D object retrieval"],"max_cite":35.0,"pub_year":2015.0,"sources":"['ieee', 'scp', 'wos']","rawkeys":["state-of-the-art methods","search","data representations","machine","system","gradient descent method","3d object retrieval","stacked local convolutional autoencoder","recognition","high-level abstraction","feature descriptors","model retrieval","framework","object representation","graph","shape descriptor","similarity","auto encoders","representation","object representations"],"tags":["state-of-the-art methods","search","data representations","machine","system","gradient descent method","3d object retrieval","stacked local convolutional autoencoder","recognition","high-level abstraction","feature descriptors","model retrieval","framework","graph","shape descriptors","similarity","auto encoders","representation","object representations"]},{"p_id":33016,"title":"A Neuromorphic Person Re-Identification Framework for Video Surveillance","abstract":"\u00a9 2017 IEEE. This paper presents a neuromorphic person re-identification (NPReId) framework to establish the correspondence among individuals observed across two disjoint camera views. The proposed framework comprises three modules (observation, cognition, and contemplation), inspired by the form-and-color-and-depth (FACADE) theory model of object recognition system. In the observation module, a semantic partitioning scheme is introduced to segment a pedestrian into several logical parts, and an exhaustive set of experiments have been carried out to select the best possible complementary feature cues. In the cognition module, an unsupervised procedure is suggested to partition the gallery candidates into multiple consensus clusters with high intra-cluster and low inter-cluster similarity. A supervised classifier is then deployed to learn the relationship between each gallery candidate and its associated cluster, which is subsequently used to identify a set of inlier consensus clusters. This module also includes weighing of contribution of each feature channel toward defining a consensus cluster. Finally, in the contemplation module, the contributory weights are employed in a correlation-based similarity measure to find the corresponding match within the inlier set. The proposed framework is compared with several state-of-the-art methods on three challenging data sets: VIPeR, iLIDS-VID, and CUHK01. The experimental results, with respect to recognition rates, demonstrate that the proposed framework can obtain superior performance as compared with the counterparts. The proposed framework, along with its low-rank bound property, further establishes its suitability in practical scenarios through yielding high cluster hit rate with low database penetration.","keywords_author":["consensus clustering","person re-identification","recognition","Surveillance"],"keywords_other":["Consensus clustering","recognition","State-of-the-art methods","Person re identifications","Object recognition systems","Semantic partitioning","Complementary features","Supervised classifiers"],"max_cite":3.0,"pub_year":2017.0,"sources":"['scp']","rawkeys":["state-of-the-art methods","recognition","person re identifications","object recognition systems","complementary features","person re-identification","consensus clustering","supervised classifiers","surveillance","semantic partitioning"],"tags":["state-of-the-art methods","recognition","object recognition systems","complementary features","person re-identification","consensus clustering","supervised classifiers","surveillance","semantic partitioning"]},{"p_id":261,"title":"Learning Speaker-Specific Characteristics With a Deep Neural Architecture","abstract":"Speech signals convey various yet mixed information ranging from linguistic to speaker-specific information. However, most of acoustic representations characterize all different kinds of information as whole, which could hinder either a speech or a speaker recognition (SR) system from producing a better performance. In this paper, we propose a novel deep neural architecture (DNA) especially for learning speaker-specific characteristics from mel-frequency cepstral coefficients, an acoustic representation commonly used in both speech recognition and SR, which results in a speaker-specific overcomplete representation. In order to learn intrinsic speaker-specific characteristics, we come up with an objective function consisting of contrastive losses in terms of speaker similarity\/dissimilarity and data reconstruction losses used as regularization to normalize the interference of non-speaker-related information. Moreover, we employ a hybrid learning strategy for learning parameters of the deep neural networks: i.e., local yet greedy layerwise unsupervised pretraining for initialization and global supervised learning for the ultimate discriminative goal. With four Linguistic Data Consortium (LDC) benchmarks and two non-English corpora, we demonstrate that our overcomplete representation is robust in characterizing various speakers, no matter whether their utterances have been used in training our DNA, and highly insensitive to text and languages spoken. Extensive comparative studies suggest that our approach yields favorite results in speaker verification and segmentation. Finally, we discuss several issues concerning our proposed approach.","keywords_author":["Deep neural architecture","hybrid learning strategy","overcomplete representation","speaker comparison","speaker segmentation","speaker verification","speaker-specific characteristics","Deep neural architecture","hybrid learning strategy","overcomplete representation","speaker comparison","speaker segmentation","speaker verification","speaker-specific characteristics"],"keywords_other":["deep neural architecture","supervised learning","acoustic representation","Humans","speaker segmentation","speech signal","nonspeaker-related information","natural languages","Strontium","IDENTIFICATION","Language","Speech processing","data reconstruction loss","hybrid learning strategy","Discrimination Learning","Speech Recognition Software","Computer Systems","mel-frequency cepstral coefficient","speaker similarity","speaker verification","Speech","neural net architecture","Artificial Intelligence","learning (artificial intelligence)","Algorithms","Models, Neurological","Learning systems","speaker-specific characteristics learning","cepstral analysis","RECOGNITION","DNA","languages spoken","Neurons","Normal Distribution","VERIFICATION","Neural Networks (Computer)","learning speaker-specific characteristics","NETWORKS","speaker recognition","SEGMENTATION","Speech recognition","MODELS","linguistic data consortium benchmark","nonEnglish corpora","speaker recognition system"],"max_cite":36.0,"pub_year":2011.0,"sources":"['ieee', 'wos']","rawkeys":["deep neural architecture","supervised learning","identification","acoustic representation","speaker comparison","speaker segmentation","nonspeaker-related information","natural languages","speech signal","speech recognition","verification","learning systems","data reconstruction loss","hybrid learning strategy","language","speaker-specific characteristics","mel-frequency cepstral coefficient","segmentation","speaker similarity","strontium","speaker verification","neural net architecture","models","speech","computer systems","algorithms","neurological","neurons","overcomplete representation","learning (artificial intelligence)","neural networks (computer)","recognition","speaker-specific characteristics learning","dna","cepstral analysis","humans","networks","languages spoken","artificial intelligence","nonenglish corpora","normal distribution","discrimination learning","learning speaker-specific characteristics","speech recognition software","speech processing","speaker recognition","linguistic data consortium benchmark","speaker recognition system"],"tags":["deep neural architecture","supervised learning","identification","acoustic representation","speaker comparison","speaker segmentation","nonspeaker-related information","natural languages","speech recognition","verification","speech signals","learning systems","data reconstruction loss","hybrid learning strategy","language","speaker-specific characteristics","segmentation","computational system","machine learning","speaker similarity","speaker verification","strontium","neural net architecture","speech","algorithms","neurological","neurons","recognition","neural networks","speaker-specific characteristics learning","dna","cepstral analysis","humans","mel-frequency cepstral coefficients","networks","languages spoken","over-complete representations","nonenglish corpora","normal distribution","model","learning speaker-specific characteristics","speech recognition software","speech processing","speaker recognition systems","speaker recognition","discriminative learning","linguistic data consortium benchmark"]},{"p_id":114963,"title":"Classification of red blood cell shapes in flow using outlier tolerant machine learning","abstract":"The manual evaluation, classification and counting of biological objects demands for an enormous expenditure of time and subjective human input may be a source of error. Investigating the shape of red blood cells (RBCs) in microcapillary Poiseuille flow, we overcome this drawback by introducing a convolutional neural regression network for an automatic, outlier tolerant shape classification. From our experiments we expect two stable geometries: the so-called 'slipper' and 'croissant' shapes depending on the prevailing flow conditions and the cell-intrinsic parameters. Whereas croissants mostly occur at low shear rates, slippers evolve at higher flow velocities. With our method, we are able to find the transition point between both 'phases' of stable shapes which is of high interest to ensuing theoretical studies and numerical simulations. Using statistically based thresholds, from our data, we obtain so-called phase diagrams which are compared to manual evaluations. Prospectively, our concept allows us to perform objective analyses of measurements for a variety of flow conditions and to receive comparable results. Moreover, the proposed procedure enables unbiased studies on the influence of drugs on flow properties of single RBCs and the resulting macroscopic change of the flow behavior of whole blood.","keywords_author":null,"keywords_other":["RECOGNITION","CAPILLARIES"],"max_cite":0.0,"pub_year":2018.0,"sources":"['wos']","rawkeys":["recognition","capillaries"],"tags":["recognition","capillaries"]},{"p_id":292,"title":"Robust visual tracking via convolutional networks without training","abstract":"Deep networks have been successfully applied to visual tracking by learning a generic representation offline from numerous training images. However, the offline training is time-consuming and the learned generic representation may be less discriminative for tracking specific objects. In this paper, we present that, even without offline training with a large amount of auxiliary data, simple two-layer convolutional networks can be powerful enough to learn robust representations for visual tracking. In the first frame, we extract a set of normalized patches from the target region as fixed filters, which integrate a series of adaptive contextual filters surrounding the target to define a set of feature maps in the subsequent frames. These maps measure similarities between each filter and useful local intensity patterns across the target, thereby encoding its local structural information. Furthermore, all the maps together form a global representation, via which the inner geometric layout of the target is also preserved. A simple soft shrinkage method that suppresses noisy values below an adaptive threshold is employed to de-noise the global representation. Our convolutional networks have a lightweight structure and perform favorably against several state-of-the-art methods on the recent tracking benchmark data set with 50 challenging videos.","keywords_author":["Convolutional Networks","Deep learning","Visual tracking","Visual tracking","convolutional networks","deep learning","Visual tracking","Convolutional Networks","Deep learning"],"keywords_other":["Global representation","SPARSE APPEARANCE MODEL","image filtering","Visual tracking","Visual Tracking","encoding","Computer architecture","Deep learning","Convolutional networks","adaptive threshold","adaptive contextual filter","Generic representation","auxiliary data","Structural information","Feature extraction","convolution","convolutional networks","Visualization","noise suppression","CORTEX","OBJECT TRACKING","intensity pattern","offline training","convolutional network","image representation","learning (artificial intelligence)","State-of-the-art methods","Robustness","deep learning","Training","Layout","deep network","RECOGNITION","image denoising","robust visual tracking","learned generic representation","SELECTION","Target tracking","NEURAL-NETWORKS","soft shrinkage method","Adaptive thresholds"],"max_cite":103.0,"pub_year":2016.0,"sources":"['scp', 'wos', 'ieee']","rawkeys":["state-of-the-art methods","robustness","image filtering","layout","encoding","visualization","neural-networks","adaptive threshold","adaptive contextual filter","auxiliary data","convolution","generic representation","target tracking","convolutional networks","noise suppression","intensity pattern","convolutional network","computer architecture","image representation","offline training","adaptive thresholds","learning (artificial intelligence)","object tracking","recognition","deep learning","training","deep network","image denoising","structural information","robust visual tracking","sparse appearance model","selection","learned generic representation","visual tracking","soft shrinkage method","global representation","feature extraction","cortex"],"tags":["state-of-the-art methods","robustness","image filtering","layout","convolutional neural network","encoding","visualization","adaptive contextual filter","auxiliary data","convolution","generic representation","machine learning","target tracking","noise suppression","intensity pattern","offline training","computer architecture","image representation","adaptive thresholds","recognition","object tracking","neural networks","training","image denoising","structural information","robust visual tracking","sparse appearance model","selection","learned generic representation","visual tracking","soft shrinkage method","global representation","feature extraction","cortex","deep networks"]},{"p_id":65831,"title":"A computer vision for animal ecology","abstract":"1. A central goal of animal ecology is to observe species in the natural world. The cost and challenge of data collection often limit the breadth and scope of ecological study. Ecologists often use image capture to bolster data collection in time and space. However, the ability to process these images remains a bottleneck.","keywords_author":["automation","camera traps","ecological monitoring","images","unmanned aerial vehicles"],"keywords_other":["SATELLITE IMAGERY","AERIAL IMAGES","CLASSIFICATION","IDENTIFICATION","SYSTEM","RECOGNITION","BEHAVIOR","ABUNDANCE","CAMOUFLAGE","APPEARANCE"],"max_cite":3.0,"pub_year":2018.0,"sources":"['wos']","rawkeys":["identification","images","recognition","unmanned aerial vehicles","camera traps","system","appearance","automation","camouflage","classification","satellite imagery","aerial images","ecological monitoring","abundance","behavior"],"tags":["unmanned aerial vehicle","identification","images","automated","recognition","camera traps","system","appearance","camouflage","classification","satellite imagery","aerial images","ecological monitoring","abundance","behavior"]},{"p_id":297,"title":"Intelligent query answering by knowledge discovery techniques","abstract":"Knowledge discovery facilitates querying database knowledge and intelligent query answering in database systems. We investigate the application of discovered knowledge, concept hierarchies, and knowledge discovery tools for intelligent query answering in database systems. A knowledge-rich data model is constructed to incorporate discovered knowledge and knowledge discovery tools. Queries are classified into data queries and knowledge queries. Both types of queries can be answered directly by simple retrieval or intelligently by analyzing the intent of query and providing generalized, neighborhood or associated information using stored or discovered knowledge. Techniques have been developed for intelligent query answering using discovered knowledge and\/or knowledge discovery tools, which includes generalization, data summarization, concept clustering, rule discovery, query rewriting, deduction, lazy evaluation, application of multiple-layered databases, etc. Our study shows that knowledge discovery substantially broadens the spectrum of intelligent query answering and may have deep implications on query answering in data- and knowledge-base systems.","keywords_author":null,"keywords_other":["lazy evaluation","Computer Society","data structures","query rewriting","data queries","Data mining","knowledge acquisition","deduction","Relational databases","generalization","Machine learning","deductive databases","query processing","Database systems","knowledge discovery techniques","knowledge-rich data model","knowledge discovery tools","Data models","multiple-layered databases","knowledge queries","database knowledge","Application software","data summarization","Information retrieval","Prototypes","concept hierarchies","Deductive databases","concept clustering","rule discovery","intelligent query answering"],"max_cite":22.0,"pub_year":1996.0,"sources":"['ieee']","rawkeys":["lazy evaluation","data structures","query rewriting","data queries","application software","deduction","knowledge acquisition","machine learning","generalization","prototypes","deductive databases","relational databases","data mining","query processing","knowledge discovery techniques","information retrieval","knowledge-rich data model","knowledge discovery tools","computer society","knowledge queries","database knowledge","multiple-layered databases","data summarization","database systems","concept hierarchies","concept clustering","data models","rule discovery","intelligent query answering"],"tags":["lazy evaluation","query rewritings","relational database","data structures","data summarizations","data queries","application software","deduction","knowledge acquisition","machine learning","prototypes","deductive databases","data mining","recognition","query processing","knowledge discovery techniques","information retrieval","knowledge-rich data model","knowledge query","knowledge discovery tools","computer society","multiple-layered databases","database knowledge","database systems","concept hierarchies","concept clustering","data models","rule discovery","intelligent query answering"]},{"p_id":8497,"title":"Deep learning for computational biology","abstract":"\u00a9 2016 The Authors. Published under the terms of the CC BY 4.0 licenseTechnological advances in genomics and imaging have led to an explosion of molecular and cellular profiling data from large numbers of samples. This rapid increase in biological data dimension and acquisition rate is challenging conventional analysis strategies. Modern machine learning methods, such as deep learning, promise to leverage very large data sets for finding hidden structure within them, and for making accurate predictions. In this review, we discuss applications of this new breed of analysis approaches in regulatory genomics and cellular imaging. We provide background of what deep learning is, and the settings in which it can be successfully applied to derive biological insights. In addition to presenting specific applications and providing tips for practical use, we also highlight possible pitfalls and limitations to guide computational biologists when and how to make the most use of this new technology.","keywords_author":["cellular imaging","computational biology","deep learning","machine learning","regulatory genomics","cellular imaging","computational biology","deep learning","machine learning","regulatory genomics"],"keywords_other":["RNA","Genomics","Humans","Models, Genetic","POPULATION","ALGORITHM","Machine Learning","NEURAL-NETWORKS","GENE-EXPRESSION VARIATION","RECOGNITION","SECONDARY STRUCTURE PREDICTION","DISCOVERY","PERCEPTRON","PROTEIN","Computational Biology"],"max_cite":97.0,"pub_year":2016.0,"sources":"['ieee', 'scp', 'wos']","rawkeys":["perceptron","secondary structure prediction","neural-networks","regulatory genomics","gene-expression variation","machine learning","rna","models","genomics","algorithm","recognition","deep learning","humans","population","cellular imaging","discovery","genetic","computational biology","protein"],"tags":["regulatory genomics","recognition","model","cellular imaging","discovery","neural networks","genetic","computational biology","machine learning","gene-expression variation","humans","perceptron","population","proteins","rna","secondary structure prediction","algorithms","genomics"]},{"p_id":106803,"title":"Isomap and Deep Belief Network-Based Machine Health Combined Assessment Model","abstract":"This paper presents a novel combined assessment model (CAM) for machine health assessment, in which 38 original features of the vibration signal were extracted from time domain analysis, frequency domain analysis, and wavelet packet transform (WPT), following which the nonlinear global algorithm Isomap was adopted for dimensionality reduction and extraction of the more representative features. Next, the acquired low-dimensional features array is input into the well trained deep belief network (DBN) model to evaluate the performance status of the bearing. Finally,after the bearing accelerated degradation data from Cincinnati University were investigated for further research, through the comparison experiments with two other popular dimensionality reduction methods (principal component analysis (PCA) and Laplacian Eigenmaps) and two other intelligent assessment algorithms (hidden Markov model (HMM) and back-propagation neural network (BPNN)), the proposed CAM has been proved to be more sensitive to the incipient fault and more effective for the evaluation of bearing performance degradation.","keywords_author":["Isomap","dimensionality reduction","deep belief network (DBN)","machine health","combined assessment model (CAM)"],"keywords_other":["DAMAGE","DIMENSIONALITY","TRANSFORM","FAULT-DIAGNOSIS","REDUCTION","FREQUENCY","RECOGNITION","SIGNALS"],"max_cite":0.0,"pub_year":2016.0,"sources":"['wos']","rawkeys":["damage","machine health","dimensionality","reduction","isomap","recognition","signals","deep belief network (dbn)","frequency","combined assessment model (cam)","fault-diagnosis","transform","dimensionality reduction"],"tags":["damage","machine health","dimensionality","reduction","isomap","recognition","signals","frequency","combined assessment model (cam)","transform","deep belief networks","fault diagnosis","dimensionality reduction"]},{"p_id":8501,"title":"An unobtrusive behavioral model of \"gross national happiness\"","abstract":"I analyze the use of emotion words for approximately 100 million Facebook users since September of 2007. \"Gross national happiness\" is operationalized as a standardized difference between the use of positive and negative words, aggregated across days, and present a graph of this metric. I begin to validate this metric by showing that positive and negative word use in status updates covaries with self-reported satisfaction with life (convergent validity), and also note that the graph shows peaks and valleys on days that are culturally and emotionally significant (face validity). I discuss the development and computation of this metric, argue that this metric and graph serves as a representation of the overall emotional health of the nation, and discuss the importance of tracking such metrics. \u00a9 2010 ACM.","keywords_author":["emotion","facebook","gross national happiness","psychology","quantitative methods","statistics"],"keywords_other":["Quantitative method","facebook","Behavioral model","Convergent validity"],"max_cite":97.0,"pub_year":2010.0,"sources":"['scp']","rawkeys":["gross national happiness","statistics","facebook","emotion","convergent validity","quantitative methods","behavioral model","psychology","quantitative method"],"tags":["gross national happiness","statistics","facebook","recognition","emotion","convergence validity","behavior model","quantitative method"]},{"p_id":106806,"title":"Intelligent Fault Diagnosis of Rotary Machinery Based on Unsupervised Multiscale Representation Learning","abstract":"The performance of traditional vibration based fault diagnosis methods greatly depends on those handcrafted features extracted using signal processing algorithms, which require significant amounts of domain knowledge and human labor, and do not generalize well to new diagnosis domains. Recently, unsupervised representation learning provides an alternative promising solution to feature extraction in traditional fault diagnosis due to its superior learning ability from unlabeled data. Given that vibration signals usually contain multiple temporal structures, this paper proposes a multiscale representation learning (MSRL) framework to learn useful features directly from raw vibration signals, with the aim to capture rich and complementary fault pattern information at different scales. In our proposed approach, a coarse-grained procedure is first employed to obtain multiple scale signals from an original vibration signal. Then, sparse filtering, a newly developed unsupervised learning algorithm, is applied to automatically learn useful features from each scale signal, respectively, and then the learned features at each scale to be concatenated one by one to obtain multiscale representations. Finally, the multiscale representations are fed into a supervised classifier to achieve diagnosis results. Our proposed approach is evaluated using two different case studies: motor bearing and wind turbine gearbox fault diagnosis. Experimental results show that the proposed MSRL approach can take full advantages of the availability of unlabeled data to learn discriminative features and achieved better performance with higher accuracy and stability compared to the traditional approaches.","keywords_author":["Intelligent fault diagnosis","Vibration signals","Unsupervised feature learning","Sparse filtering","Multiscale feature extraction"],"keywords_other":["FUZZY INFERENCE","ROTATING MACHINERY","LOCAL MEAN DECOMPOSITION","EXTRACTION","PROGNOSTICS","RECOGNITION","ENTROPY","ROLLER-BEARINGS","DEEP NEURAL-NETWORKS","EMPIRICAL MODE DECOMPOSITION"],"max_cite":0.0,"pub_year":2017.0,"sources":"['ieee', 'wos']","rawkeys":["local mean decomposition","recognition","prognostics","unsupervised feature learning","deep neural-networks","sparse filtering","vibration signals","multiscale feature extraction","intelligent fault diagnosis","entropy","roller-bearings","rotating machinery","extraction","fuzzy inference","empirical mode decomposition"],"tags":["local mean decomposition","recognition","prognostics","unsupervised feature learning","vibration signal","sparse filtering","convolutional neural network","multiscale feature extraction","intelligent fault diagnosis","entropy","roller-bearings","rotating machinery","extraction","fuzzy inference","empirical mode decomposition"]},{"p_id":312,"title":"Target Classification Using the Deep Convolutional Networks for SAR Images","abstract":"The algorithm of synthetic aperture radar automatic target recognition (SAR-ATR) is generally composed of the extraction of a set of features that transform the raw input into a representation, followed by a trainable classifier. The feature extractor is often hand designed with domain knowledge and can significantly impact the classification accuracy. By automatically learning hierarchies of features from massive training data, deep convolutional networks (ConvNets) recently have obtained state-of-the-art results in many computer vision and speech recognition tasks. However, when ConvNets was directly applied to SAR-ATR, it yielded severe overfitting due to limited training images. To reduce the number of free parameters, we present a new all-convolutional networks (A-ConvNets), which only consists of sparsely connected layers, without fully connected layers being used. Experimental results on the Moving and Stationary Target Acquisition and Recognition (MSTAR) benchmark data set illustrate that A-ConvNets can achieve an average accuracy of 99% on classification of ten-class targets and is significantly superior to the traditional ConvNets on the classification of target configuration and version variants.","keywords_author":["Automatic target recognition (ATR)","deep convolutional networks (ConvNets)","deep learning","Synthetic aperture radar (SAR)","Automatic target recognition (ATR)","deep convolutional networks (ConvNets)","deep learning","synthetic aperture radar (SAR)","Automatic target recognition (ATR)","deep convolutional networks (ConvNets)","deep learning","synthetic aperture radar (SAR)"],"keywords_other":["Synthetic aperture radar","target configuration classification","Moving and Stationary Target Acquisition and Recognition","deep convolutional networks (ConvNets)","geophysical image processing","Automatic target recognition (ATR)","Target Classification","SAR images","Feature extractor","Stationary targets","image classification","Target recognition","Computer architecture","sparsely connected layers","radar imaging","Convolutional networks","Feature extraction","Domain knowledge","Convolution","target classification","feature automatically learning hierarchies","ten-class target classification","remote sensing by radar","synthetic aperture radar automatic target recognition","Synthetic aperture radar automatic target recognition","speech recognition tasks","Training data","Target configurations","deep learning","synthetic aperture radar","Training","MODEL","MSTAR benchmark data set","RECOGNITION","synthetic aperture radar (SAR)","PERFORMANCE","computer vision","deep convolutional networks","REPRESENTATION","NEURAL-NETWORKS","massive training data","raw input","SAR-ATR","Classification accuracy"],"max_cite":86.0,"pub_year":2016.0,"sources":"['scp', 'wos', 'ieee']","rawkeys":["deep convolutional networks (convnets)","target configuration classification","performance","geophysical image processing","synthetic aperture radar (sar)","mstar benchmark data set","image classification","training data","sparsely connected layers","neural-networks","radar imaging","convolution","convolutional networks","automatic target recognition (atr)","target configurations","target classification","sar-atr","feature automatically learning hierarchies","remote sensing by radar","ten-class target classification","computer architecture","synthetic aperture radar automatic target recognition","moving and stationary target acquisition and recognition","recognition","speech recognition tasks","stationary targets","deep learning","synthetic aperture radar","training","computer vision","feature extractor","sar images","deep convolutional networks","model","classification accuracy","representation","massive training data","raw input","feature extraction","domain knowledge","target recognition"],"tags":["target configuration classification","performance","geophysical image processing","mstar benchmark data set","convolutional neural network","image classification","training data","sparsely connected layers","radar imaging","convolution","machine learning","automatic target recognition (atr)","target configurations","target classification","feature automatically learning hierarchies","remote sensing by radar","ten-class target classification","computer architecture","synthetic aperture radar automatic target recognition","moving and stationary target acquisition and recognition","recognition","speech recognition tasks","stationary targets","neural networks","synthetic aperture radar","training","computer vision","feature extractor","sar atr","sar images","model","classification accuracy","representation","massive training data","raw input","feature extraction","domain knowledge","target recognition"]},{"p_id":321,"title":"Marginal Space Deep Learning: Efficient Architecture for Volumetric Image Parsing","abstract":"Robust and fast solutions for anatomical object detection and segmentation support the entire clinical workflow from diagnosis, patient stratification, therapy planning, intervention and follow-up. Current state-of-the-art techniques for parsing volumetric medical image data are typically based on machine learning methods that exploit large annotated image databases. Two main challenges need to be addressed, these are the efficiency in scanning high-dimensional parametric spaces and the need for representative image features which require significant efforts of manual engineering. We propose a pipeline for object detection and segmentation in the context of volumetric image parsing, solving a two-step learning problem: anatomical pose estimation and boundary delineation. For this task we introduce Marginal Space Deep Learning (MSDL), a novel framework exploiting both the strengths of efficient object parametrization in hierarchical marginal spaces and the automated feature design of Deep Learning (DL) network architectures. In the 3D context, the application of deep learning systems is limited by the very high complexity of the parametrization. More specifically 9 parameters are necessary to describe a restricted affine transformation in 3D, resulting in a prohibitive amount of billions of scanning hypotheses. The mechanism of marginal space learning provides excellent run-time performance by learning classifiers in clustered, high-probability regions in spaces of gradually increasing dimensionality. To further increase computational efficiency and robustness, in our system we learn sparse adaptive data sampling patterns that automatically capture the structure of the input. Given the object localization, we propose a DL-based active shape model to estimate the non-rigid object boundary. Experimental results are presented on the aortic valve in ultrasound using an extensive dataset of 2891 volumes from 869 patients, showing significant improvements of up to 45.2% over the state-of-the-art. To our knowledge, this is the first successful demonstration of the DL potential to detection and segmentation in full 3D data with parametrized representations.","keywords_author":["Deep learning","image parsing","marginal space learning","sparse representations","three-dimensional (3D) object detection and segmentation","Deep learning","image parsing","marginal space learning","sparse representations","three-dimensional (3D) object detection and segmentation","Deep learning","image parsing","marginal space learning","sparse representations","three-dimensional (3D) object detection and segmentation"],"keywords_other":["Image Processing, Computer-Assisted","automated feature design","clinical workflow","ultrasound","deep learning systems","Humans","AUTOMATIC SEGMENTATION","therapy planning","volumetric medical image data parsing","Machine Learning","biomedical ultrasonics","run-time performance","image classification","image sampling","Deep learning","Feature extraction","anatomical pose estimation","Pattern Recognition, Automated","nonrigid object boundary","hierarchical marginal spaces","boundary delineation","Machine learning","Sparse representation","ULTRASOUND IMAGES","annotated image databases","restricted affine transformation","Context","two-step learning problem","representative image features","learning classifiers","anatomical object detection","scanning hypotheses","Three-dimensional (3-D) object detection","ultrasonic imaging","diagnosis","Aortic Valve","3D context","Algorithms","DL-based active shape model","Image parsing","FEATURES","full 3D data segmentation","Echocardiography, Transesophageal","Marginal space learning","learning (artificial intelligence)","Robustness","computational efficiency","Shape","Databases, Factual","object localization","parametrized representations","RECOGNITION","medical image processing","extensive dataset","scanning high-dimensional parametric spaces","probability","sparse adaptive data sampling patterns","full 3D data detection","image segmentation","machine learning methods","segmentation support","clustered high-probability regions","Neural Networks (Computer)","marginal space deep learning","patient stratification","Three-dimensional displays","NEURAL-NETWORKS","Image segmentation","aortic valve","MODELS","deep learning network architectures","feature extraction","pattern clustering","object parametrization","CARDIAC CT"],"max_cite":30.0,"pub_year":2016.0,"sources":"['ieee', 'scp', 'wos']","rawkeys":["neural-networks","annotated image databases","models","scanning hypotheses","neural networks (computer)","humans","medical image processing","shape","3d context","deep learning network architectures","pattern recognition","automated feature design","ultrasound","volumetric medical image data parsing","sparse representations","biomedical ultrasonics","image classification","image sampling","automatic segmentation","cardiac ct","run-time performance","transesophageal","dl-based active shape model","nonrigid object boundary","hierarchical marginal spaces","ultrasound images","algorithms","anatomical object detection","sparse representation","parametrized representations","image segmentation","segmentation support","clustered high-probability regions","computer-assisted","patient stratification","image processing","three-dimensional displays","feature extraction","three-dimensional (3d) object detection and segmentation","pattern clustering","clinical workflow","therapy planning","three-dimensional (3-d) object detection","anatomical pose estimation","full 3d data detection","full 3d data segmentation","machine learning","restricted affine transformation","learning classifiers","two-step learning problem","echocardiography","deep learning","aortic valve","context","robustness","automated","databases","deep learning systems","image parsing","features","boundary delineation","marginal space learning","representative image features","ultrasonic imaging","diagnosis","learning (artificial intelligence)","recognition","computational efficiency","object localization","extensive dataset","scanning high-dimensional parametric spaces","factual","probability","machine learning methods","marginal space deep learning","object parametrization","sparse adaptive data sampling patterns"],"tags":["automated feature design","clinical workflow","robustness","ultrasound","automated","databases","therapy planning","three-dimensional (3-d) object detection","volumetric medical image data parsing","biomedical ultrasonics","image classification","image sampling","automatic segmentation","cardiac ct","run-time performance","transesophageal","image parsing","dl-based active shape model","features","anatomical pose estimation","full 3d data detection","full 3d data segmentation","machine learning","hierarchical marginal spaces","nonrigid object boundary","boundary delineation","annotated image databases","marginal space learning","restricted affine transformation","algorithms","learning classifiers","representative image features","two-step learning problem","anatomical object detection","scanning hypotheses","ultrasonic imaging","diagnosis","echocardiography","recognition","neural networks","sparse representation","humans","medical image processing","object localization","parametrized representations","deep learning system","deep learning network architectures","extensive dataset","factual","probability","scanning high-dimensional parametric spaces","computationally efficient","ultrasound images","image segmentation","machine learning methods","segmentation support","clustered high-probability regions","model","computer-assisted","marginal space deep learning","image processing","patient stratification","pattern recognition","shape","three-dimensional displays","3d context","aortic valve","context","feature extraction","three-dimensional (3d) object detection and segmentation","pattern clustering","object parametrization","sparse adaptive data sampling patterns"]},{"p_id":65864,"title":"Extensive exploration of comprehensive vehicle attributes using D-CNN with weighted multi-attribute strategy","abstract":"As a classical machine learning method, multi-task learning (MTL) has been widely applied in computer vision technology. Due to deep convolutional neural network (D-CNN) having strong ability of feature representation, the combination of MTL and D-CNN has attracted much attention from researchers recently. However, this kind of combination has rarely been explored in the field of vehicle analysis. The authors propose a D-CNN enhanced with weighted multi-attribute strategy for extensive exploration of comprehensive vehicle attributes over surveillance images. Specifically, regarding to recognising vehicle model and make\/manufacturer, several related attributes as auxiliary tasks are incorporated in the training process of D-CNN structure. The proposed strategy focuses more on the main task compared with traditional MTL methods, which has assigned different weights for the main task and auxiliary tasks rather than treating all involved tasks equally. To the extent of their knowledge, this is the first report relating to the combination of D-CNN and weighted MTL for exploration of comprehensive vehicle attributes. The following experiments will show that the proposed approach outperforms the state-of-the-art method for the vehicle recognition and improves the accuracy rate by about 10% for the analysis of other vehicle attributes on the recently public CompCars dataset.","keywords_author":["object recognition","feedforward neural nets","learning (artificial intelligence)","comprehensive vehicle attributes","D-CNN","weighted multiattribute strategy","deep convolutional neural network","surveillance images","vehicle model recognition","make recognition","MTL methods","multitask learning","CompCars vehicle dataset","manufacturer recognition"],"keywords_other":["FEATURES","NETWORKS","CLASSIFICATION","MODEL","RECOGNITION"],"max_cite":0.0,"pub_year":2018.0,"sources":"['ieee', 'wos']","rawkeys":["compcars vehicle dataset","classification","features","d-cnn","mtl methods","multitask learning","feedforward neural nets","learning (artificial intelligence)","recognition","make recognition","deep convolutional neural network","networks","weighted multiattribute strategy","model","object recognition","surveillance images","comprehensive vehicle attributes","manufacturer recognition","vehicle model recognition"],"tags":["compcars vehicle dataset","recognition","model","features","make recognition","machine learning","object recognition","mtl methods","multitask learning","networks","classification","comprehensive vehicle attributes","convolutional neural network","surveillance images","weighted multiattribute strategy","manufacturer recognition","feedforward neural nets","vehicle model recognition"]},{"p_id":16718,"title":"Where do features come from?","abstract":"It is possible to learn multiple layers of non-linear features by backpropagating error derivatives through a feedforward neural network. This is a very effective learning procedure when there is a huge amount of labeled training data, but for many learning tasks very few labeled examples are available. In an effort to overcome the need for labeled data, several different generative models were developed that learned interesting features by modeling the higher order statistical structure of a set of input vectors. One of these generative models, the restricted Boltzmann machine (RBM), has no connections between its hidden units and this makes perceptual inference and learning much simpler. More significantly, after a layer of hidden features has been learned, the activities of these features can be used as training data for another RBM. By applying this idea recursively, it is possible to learn a deep hierarchy of progressively more complicated features without requiring any labeled data. This deep hierarchy can then be treated as a feedforward neural network which can be discriminatively fine-tuned using backpropagation. Using a stack of RBMs to initialize the weights of a feedforward neural network allows backpropagation to work effectively in much deeper networks and it leads to much better generalization. A stack of RBMs can also be used to initialize a deep Boltzmann machine that has many hidden layers. Combining this initialization method with a new method for fine-tuning the weights finally leads to the first efficient way of training Boltzmann machines with many hidden layers and millions of weights. \u00a9 2013 Cognitive Science Society, Inc.","keywords_author":["Backpropagation","Boltzmann machines","Contrastive divergence","Deep learning","Distributed representations","Learning features","Learning graphical models","Variational learning","Backpropagation","Boltzmann machines","Learning features","Learning graphical models","Distributed representations","Deep learning","Variational learning","Contrastive divergence"],"keywords_other":["Neural Networks (Computer)","Learning","Models, Neurological","Humans","SLEEP","REPRESENTATIONS","NEURAL-NETWORKS","RECOGNITION","Artificial Intelligence","TIME","Computer Simulation","MODELS","BELIEF NETWORKS","EM ALGORITHM"],"max_cite":34.0,"pub_year":2014.0,"sources":"['ieee', 'scp', 'wos']","rawkeys":["computer simulation","em algorithm","neural-networks","contrastive divergence","time","variational learning","models","neurological","learning features","neural networks (computer)","recognition","deep learning","learning graphical models","boltzmann machines","learning","humans","belief networks","sleep","artificial intelligence","distributed representations","representations","backpropagation"],"tags":["learned features","computer simulation","distributed representation","em algorithm","contrastive divergence","machine learning","time","neurological","recognition","neural networks","learning graphical models","boltzmann machines","humans","belief networks","sleep","model","representation","variation learning","backpropagation"]},{"p_id":337,"title":"A deep ensemble learning method for monaural speech separation","abstract":"Monaural speech separation is a fundamental problem in robust speech processing. Recently, deep neural network (DNN)-based speech separation methods, which predict either clean speech or an ideal time-frequency mask, have demonstrated remarkable performance improvement. However, a single DNN with a given window length does not leverage contextual information sufficiently, and the differences between the two optimization objectives are not well understood. In this paper, we propose a deep ensemble method, named multicontext networks, to address monaural speech separation. The first multicontext network averages the outputs of multiple DNNs whose inputs employ different window lengths. The second multicontext network is a stack of multiple DNNs. Each DNN in a module of the stack takes the concatenation of original acoustic features and expansion of the soft output of the lower module as its input, and predicts the ratio mask of the target speaker; the DNNs in the same module employ different contexts. We have conducted extensive experiments with three speech corpora. The results demonstrate the effectiveness of the proposed method. We have also compared the two optimization objectives systematically and found that predicting the ideal time-frequency mask is more efficient in utilizing clean training speech, while predicting clean speech is less sensitive to SNR variations.","keywords_author":["Deep neural networks","Ensemble learning","Mapping-based separation","Masking-based separation","Monaural speech separation","Multicontext networks","Deep neural networks","ensemble learning","mapping-based separation","masking-based separation","monaural speech separation","multicontext networks","Deep neural networks","ensemble learning","mapping-based separation","masking-based separation","monaural speech separation","multi-context networks"],"keywords_other":["contextual information","MASKS","deep ensemble learning method","window length","Acoustic features","robust speech processing","Speech processing","DNN","Monaural speech separation","Ensemble methods","target speaker","Optimization","Speech","SEGREGATION","Speech separation","speech separation methods","multicontext networks","Context","Deep neural networks","Training speech","learning (artificial intelligence)","Signal to noise ratio","Contextual information","Training","NOISE","monaural speech separation","Robust speech processing","RECOGNITION","mapping-based separation","neural nets","masking-based separation","ideal time frequency mask","deep neural network","optimisation","Ensemble learning","speech processing","NEURAL-NETWORKS","optimization objectives","ensemble learning","Acoustics","speech corpora"],"max_cite":33.0,"pub_year":2016.0,"sources":"['ieee', 'scp', 'wos']","rawkeys":["contextual information","deep ensemble learning method","ensemble methods","window length","robust speech processing","multi-context networks","acoustics","training speech","target speaker","neural-networks","acoustic features","dnn","multicontext networks","optimization","speech","segregation","speech separation methods","speech separation","learning (artificial intelligence)","noise","recognition","signal to noise ratio","training","monaural speech separation","mapping-based separation","neural nets","masking-based separation","ideal time frequency mask","deep neural network","optimisation","deep neural networks","speech processing","optimization objectives","context","ensemble learning","masks","speech corpora"],"tags":["contextual information","deep ensemble learning method","ensemble methods","window length","robust speech processing","convolutional neural network","acoustics","training speech","target speaker","masking","machine learning","acoustic features","speech separation methods","multicontext networks","optimization","speech","segregation","speech separation","recognition","noise","signal to noise ratio","neural networks","training","monaural speech separation","mapping-based separation","masking-based separation","ideal time frequency mask","optimisation","speech processing","optimization objectives","context","ensemble learning","speech corpora"]},{"p_id":41308,"title":"Deductive generalization in a default logic setting","abstract":"\u00a9 Springer-Verlag Berlin Heidelberg 1993. In this paper, we study how several patterns of deductive generalization from positive and negative examples can be relaxed to handle forms of defeasible reasoning, using default logic as a case study. We compare the resulting paradigms and establish the logical conditions under which they can take place.","keywords_author":["Generalization","Machine learning","Nonmonotonic logics"],"keywords_other":["Default logic","Nonmonotonic logic","Negative examples","Defeasible reasoning","Generalization"],"max_cite":1.0,"pub_year":1993.0,"sources":"['scp']","rawkeys":["negative examples","machine learning","generalization","default logic","nonmonotonic logic","nonmonotonic logics","defeasible reasoning"],"tags":["recognition","negative examples","machine learning","default logic","nonmonotonic logic","defeasible reasoning"]},{"p_id":353,"title":"Deep Learning of Part-Based Representation of Data Using Sparse Autoencoders With Nonnegativity Constraints","abstract":"We demonstrate a new deep learning autoencoder network, trained by a nonnegativity constraint algorithm (nonnegativity-constrained autoencoder), that learns features that show part-based representation of data. The learning algorithm is based on constraining negative weights. The performance of the algorithm is assessed based on decomposing data into parts and its prediction performance is tested on three standard image data sets and one text data set. The results indicate that the nonnegativity constraint forces the autoencoder to learn features that amount to a part-based representation of data, while improving sparsity and reconstruction quality in comparison with the traditional sparse autoencoder and nonnegative matrix factorization. It is also shown that this newly acquired representation improves the prediction performance of a deep neural network.","keywords_author":["Autoencoder","deep architectures","feature learning","nonnegativity constraints","part-based representation","Autoencoder","deep architectures","feature learning","nonnegativity constraints","part-based representation"],"keywords_other":["Artificial neural networks","nonnegative matrix factorization","ALGORITHM","Encoding","data structures","data decomposition","Feature extraction","image data sets","Machine learning","deep learning autoencoder network","nonnegativity constraint algorithm","data handling","learning (artificial intelligence)","nonnegativity constraints","Training","Cost function","RECOGNITION","MATRIX FACTORIZATION","data part-based representation","text data set","learning algorithm","OBJECTS","nonnegativity-constrained autoencoder","Image reconstruction","deep neural network","reconstruction quality","constraint handling","NEURAL-NETWORKS","constraining negative weights","sparse autoencoders"],"max_cite":33.0,"pub_year":2016.0,"sources":"['wos', 'ieee']","rawkeys":["nonnegative matrix factorization","autoencoder","matrix factorization","data structures","data decomposition","feature learning","encoding","neural-networks","image data sets","machine learning","deep learning autoencoder network","data handling","nonnegativity constraint algorithm","algorithm","learning (artificial intelligence)","recognition","nonnegativity constraints","training","deep architectures","objects","cost function","data part-based representation","text data set","learning algorithm","nonnegativity-constrained autoencoder","image reconstruction","part-based representation","deep neural network","reconstruction quality","constraint handling","artificial neural networks","feature extraction","constraining negative weights","sparse autoencoders"],"tags":["nonnegative matrix factorization","data structures","convolutional neural network","data decomposition","feature learning","encoding","machine learning","deep learning autoencoder network","algorithms","data handling","nonnegativity constraint algorithm","image datasets","recognition","neural networks","nonnegativity constraints","training","deep architectures","objects","cost function","data part-based representation","text data set","learning algorithm","nonnegativity-constrained autoencoder","image reconstruction","part-based representation","reconstruction quality","constraint handling","auto encoders","stacked autoencoders","feature extraction","constraining negative weights"]},{"p_id":98658,"title":"Handwritten digit segmentation: Is it still necessary?","abstract":"Over the last decades, a great deal of research has been devoted to handwritten digit segmentation. Algorithms based on different features extracted from the background, foreground, and contour of images have been proposed, with those achieving the best results usually relying on a heavy set of heuristics and over-segmentation. Here, the challenge lies in finding a good set of heuristics to reduce the number of segmentation hypotheses. Independently of the heuristic over-segmentation strategy adopted, all algorithms used show their limitations when faced with complex cases such as overlapping digits. In this work, we postulate that handwritten digit segmentation can be successfully replaced by a set of classifiers trained to predict the size of the string and classify them without any segmentation. To support our position, we trained four Convolutional Neural Networks (CNN) on data generated synthetically and validated the proposed method on two well-known databases, namely, the Touching Pairs Dataset and NIST SD19. Our experimental results show that the CNN classifiers can handle complex cases of touching digits more efficiently than all segmentation algorithms available in the literature. (C) 2018 Elsevier Ltd. All rights reserved.","keywords_author":null,"keywords_other":["RECOGNITION","NUMERAL STRINGS"],"max_cite":0.0,"pub_year":2018.0,"sources":"['ieee', 'wos']","rawkeys":["numeral strings","recognition"],"tags":["numeral strings","recognition"]},{"p_id":356,"title":"Text-Attentional Convolutional Neural Network for Scene Text Detection","abstract":"Recent deep learning models have demonstrated strong capabilities for classifying text and non-text components in natural images. They extract a high-level feature globally computed from a whole image component (patch), where the cluttered background information may dominate true text features in the deep representation. This leads to less discriminative power and poorer robustness. In this paper, we present a new system for scene text detection by proposing a novel text-attentional convolutional neural network (Text-CNN) that particularly focuses on extracting text-related regions and features from the image components. We develop a new learning mechanism to train the Text-CNN with multi-level and rich supervised information, including text region mask, character label, and binary text\/non-text information. The rich supervision information enables the Text-CNN with a strong capability for discriminating ambiguous texts, and also increases its robustness against complicated background components. The training process is formulated as a multi-task learning problem, where low-level supervised information greatly facilitates the main task of text\/non-text classification. In addition, a powerful low-level detector called contrast-enhancement maximally stable extremal regions (MSERs) is developed, which extends the widely used MSERs by enhancing intensity contrast between text patterns and background. This allows it to detect highly challenging text patterns, resulting in a higher recall. Our approach achieved promising results on the ICDAR 2013 data set, with an F-measure of 0.82, substantially improving the state-of-the-art results.","keywords_author":["convolutional neural networks","Maximally stable extremal regions","multi-level supervised information","multi-task learning","text detector","Maximally stable extremal regions","text detector","convolutional neural networks","multi-level supervised information","multi-task learning","Maximally Stable Extremal Regions","text detector","convolutional neural networks","multi-level supervised information","multi-task learning"],"keywords_other":["ambiguous texts","Humans","Contrast Enhancement","text detection","binary text\/non-text information","scene text detection","text region mask","Text recognition","Neural networks","Discriminative power","READING TEXT","Maximally Stable Extremal Regions","text-attentional convolutional neural network","Computational modeling","contrast-enhancement maximally stable extremal regions","Multilevels","text detector","Feature extraction","NATURAL IMAGES","multi-level supervised information","multi-task learning","Algorithms","convolutional neural networks","Robustness","Training","RECOGNITION","deep learning models","image enhancement","STABLE EXTREMAL REGIONS","neural nets","Background components","Detectors","LOCALIZATION","Neural Networks (Computer)","Multitask learning","ICDAR 2013 data set","Natural Language Processing","Maximally stable extremal regions","character label","Cluttered backgrounds","Convolutional neural network"],"max_cite":51.0,"pub_year":2016.0,"sources":"['ieee', 'scp', 'wos']","rawkeys":["robustness","ambiguous texts","localization","text detection","binary text\/non-text information","contrast enhancement","scene text detection","text region mask","convolutional neural network","icdar 2013 data set","text-attentional convolutional neural network","contrast-enhancement maximally stable extremal regions","text detector","multitask learning","multilevels","algorithms","natural images","multi-level supervised information","stable extremal regions","computational modeling","multi-task learning","neural networks (computer)","convolutional neural networks","maximally stable extremal regions","reading text","neural networks","recognition","training","humans","text recognition","background components","image enhancement","deep learning models","neural nets","cluttered backgrounds","detectors","discriminative power","natural language processing","character label","feature extraction"],"tags":["robustness","ambiguous texts","localization","text detection","binary text\/non-text information","contrast enhancement","scene text detection","text region mask","convolutional neural network","text-attentional convolutional neural network","contrast-enhancement maximally stable extremal regions","text detector","machine learning","multitask learning","algorithms","natural images","multi-level supervised information","stable extremal regions","computational modeling","recognition","maximally stable extremal regions","deep learning model","icdar2013 datasets","neural networks","reading text","training","humans","text recognition","background components","image enhancement","cluttered backgrounds","detectors","discriminative power","natural language processing","character label","feature extraction"]},{"p_id":33134,"title":"A spectral graph wavelet approach for nonrigid 3D shape retrieval","abstract":"\u00a9 2016 Elsevier B.V. In this paper, we propose a spectral graph wavelet approach for 3D shape retrieval using the bag-of-features paradigm. In an effort to capture both local and global characteristics of a 3D shape, we present a three-step feature description framework. Local descriptors are first extracted via the spectral graph wavelet transform having the Mexican hat wavelet as a generating kernel. Then, mid-level features are obtained by embedding local descriptors into the visual vocabulary space using the soft-assignment coding step of the bag-of-features model. A global descriptor is subsequently constructed by aggregating mid-level features weighted by a geodesic exponential kernel, resulting in a matrix representation that describes the frequency of appearance of nearby codewords in the vocabulary. Then, we compare the global descriptor of a query to all global descriptors of the shapes in the dataset using a dissimilarity measure and find the closest shape. Experimental results on two standard 3D shape benchmarks demonstrate the effectiveness of the proposed shape retrieval approach in comparison with state-of-the-art methods.","keywords_author":["Bag-of-features","Geodesic kernel","Shape retrieval","Spectral graph wavelet","Shape retrieval","Spectral graph wavelet","Geodesic kernel","Bag-of-features"],"keywords_other":["Geodesic kernel","Matrix representation","Frequency of appearance","OBJECT RETRIEVAL","State-of-the-art methods","Shape retrieval","Bag of features","Dissimilarity measures","RECOGNITION","Graph wavelets"],"max_cite":3.0,"pub_year":2016.0,"sources":"['ieee', 'scp', 'wos']","rawkeys":["state-of-the-art methods","bag of features","bag-of-features","dissimilarity measures","object retrieval","recognition","shape retrieval","spectral graph wavelet","matrix representation","geodesic kernel","frequency of appearance","graph wavelets"],"tags":["state-of-the-art methods","bag-of-features","dissimilarity measures","object retrieval","recognition","shape retrieval","matrix representation","geodesic kernel","spectral graph wavelets","frequency of appearance","graph wavelets"]},{"p_id":380,"title":"Combining Generative and Discriminative Representation Learning for Lung CT Analysis With Convolutional Restricted Boltzmann Machines","abstract":"The choice of features greatly influences the performance of a tissue classification system. Despite this, many systems are built with standard, predefined filter banks that are not optimized for that particular application. Representation learning methods such as restricted Boltzmann machines may outperform these standard filter banks because they learn a feature description directly from the training data. Like many other representation learning methods, restricted Boltzmann machines are unsupervised and are trained with a generative learning objective; this allows them to learn representations from unlabeled data, but does not necessarily produce features that are optimal for classification. In this paper we propose the convolutional classification restricted Boltzmann machine, which combines a generative and a discriminative learning objective. This allows it to learn filters that are good both for describing the training data and for classification. We present experiments with feature learning for lung texture classification and airway detection in CT images. In both applications, a combination of learning objectives outperformed purely discriminative or generative learning, increasing, for instance, the lung tissue classification accuracy by 1 to 8 percentage points. This shows that discriminative learning can help an otherwise unsupervised feature learner to learn filters that are optimized for classification.","keywords_author":["Deep learning","lung","machine learning","neural network","pattern recognition and classification","representation learning","restricted Boltzmann machine","segmentation","X-ray imaging and computed tomography","Deep learning","lung","machine learning","neural network","pattern recognition and classification","representation learning","restricted Boltzmann machine","segmentation","X-ray imaging and computed tomography","Deep learning","lung","machine learning","neural network","pattern recognition and classification","representation learning","restricted Boltzmann machine","segmentation","X-ray imaging and computed tomography"],"keywords_other":["Image Processing, Computer-Assisted","Humans","image filtering","lung CT analysis","Machine Learning","image texture","Neural networks","Lung","NEURAL-NETWORK","computerised tomography","image classification","training data","Deep learning","generative representation learning","Feature extraction","Computed tomography","airway detection","discriminative representation learning","tissue classification system","pneumodynamics","convolutional restricted Boltzmann machines","Restricted boltzmann machine","Algorithms","Training data","Learning systems","lung texture classification","medical image processing","representation learning","Xray imaging","RECOGNITION","biological tissues","Pattern recognition and classification","standard predefined filter banks","unlabeled data representations","lung","Neural Networks (Computer)","Tomography, X-Ray Computed","lung tissue classification accuracy","feature description","CLASSIFICATION","channel bank filters","Standards","Lungs","generative learning objective","feature extraction","Boltzmann machines"],"max_cite":22.0,"pub_year":2016.0,"sources":"['scp', 'wos', 'ieee']","rawkeys":["neural network","image filtering","classification","computed tomography","image texture","computerised tomography","learning systems","image classification","training data","xray imaging","tomography","generative representation learning","segmentation","machine learning","airway detection","discriminative representation learning","convolutional restricted boltzmann machines","neural-network","algorithms","tissue classification system","pneumodynamics","neural networks (computer)","recognition","deep learning","neural networks","boltzmann machines","x-ray computed","humans","lung texture classification","medical image processing","lung ct analysis","representation learning","biological tissues","restricted boltzmann machine","standard predefined filter banks","x-ray imaging and computed tomography","unlabeled data representations","lung","computer-assisted","image processing","lung tissue classification accuracy","standards","channel bank filters","feature description","generative learning objective","feature extraction","lungs","pattern recognition and classification"],"tags":["image filtering","classification","computed tomography","image texture","computerised tomography","learning systems","image classification","training data","tomography","generative representation learning","segmentation","machine learning","airway detection","discriminative representation learning","algorithms","tissue classification system","pneumodynamics","recognition","neural networks","x-ray computed","boltzmann machines","humans","lung texture classification","medical image processing","lung ct analysis","representation learning","biological tissues","restricted boltzmann machine","x-ray imaging","standard predefined filter banks","x-ray imaging and computed tomography","unlabeled data representations","lung","computer-assisted","image processing","lung tissue classification accuracy","standards","channel bank filters","crbm","feature description","generative learning object","feature extraction","pattern recognition and classification"]},{"p_id":384,"title":"A 1 TOPS\/W analog deep machine-learning engine with floating-gate storage in 0.13 \u03bcm CMOS","abstract":"An analog implementation of a deep machine-learning system for efficient feature extraction is presented in this work. It features online unsupervised trainability and non-volatile floating-gate analog storage. It utilizes a massively parallel reconfigurable current-mode analog architecture to realize efficient computation, and leverages algorithm-level feedback to provide robustness to circuit imperfections in analog signal processing. A 3-layer, 7-node analog deep machine-learning engine was fabricated in a 0.13 \u03bcm standard CMOS process, occupying 0.36 mm 2 active area. At a processing speed of 8300 input vectors per second, it consumes 11.4 \u03bcW from the 3 V supply, achieving 1\u00d710 12 operation per second per Watt of peak energy efficiency. Measurement demonstrates real-time cluster analysis, and feature extraction for pattern recognition with 8-fold dimension reduction with an accuracy comparable to the floating-point software simulation baseline.","keywords_author":["Analog signal processing","current mode arithmetic","deep machine learning","floating gate","neuromorphic engineering","translinear circuits","Analog signal processing","current mode arithmetic","deep machine learning","floating gate","neuromorphic engineering","translinear circuits","Analog signal processing","current mode arithmetic","deep machine learning","floating gate","neuromorphic engineering","translinear circuits"],"keywords_other":["CLASSIFIER","massively parallel reconfigurable current-mode analog architecture","standard CMOS process","CMOS digital integrated circuits","Computer architecture","voltage 3 V","Tunneling","Feature extraction","deep machine-learning engine","real-time systems","INVERSION","nonvolatile floating-gate analog storage","analog signal processing","random-access storage","online unsupervised trainability","power 11.4 muW","Floating gates","floating-point software simulation baseline","Training","Learning systems","real-time cluster analysis","RECOGNITION","unsupervised learning","algorithm-level feedback","8-fold dimension reduction","Translinear circuits","Logic gates","MOS-TRANSISTORS","Neuromorphic engineering","CIRCUIT","PROCESSOR","Analog signal processing","Current mode","feature extraction","pattern recognition","Engines","size 0.13 mum","pattern clustering"],"max_cite":27.0,"pub_year":2015.0,"sources":"['ieee', 'scp', 'wos']","rawkeys":["cmos digital integrated circuits","processor","massively parallel reconfigurable current-mode analog architecture","learning systems","logic gates","translinear circuits","deep machine learning","floating gate","mos-transistors","deep machine-learning engine","circuit","neuromorphic engineering","tunneling","real-time systems","current mode arithmetic","computer architecture","analog signal processing","nonvolatile floating-gate analog storage","random-access storage","online unsupervised trainability","power 11.4 muw","current mode","floating gates","recognition","voltage 3 v","floating-point software simulation baseline","training","real-time cluster analysis","classifier","algorithm-level feedback","unsupervised learning","8-fold dimension reduction","standard cmos process","engines","feature extraction","pattern recognition","inversion","size 0.13 mum","pattern clustering"],"tags":["cmos digital integrated circuits","processor","tunnel","massively parallel reconfigurable current-mode analog architecture","learning systems","logic gates","translinear circuits","deep machine learning","mos-transistors","deep machine-learning engine","neuromorphic engineering","real-time systems","current mode arithmetic","computer architecture","analog signal processing","nonvolatile floating-gate analog storage","random-access storage","online unsupervised trainability","power 11.4 muw","current mode","floating gates","recognition","voltage 3 v","floating-point software simulation baseline","training","real-time cluster analysis","classifier","algorithm-level feedback","unsupervised learning","8-fold dimension reduction","standard cmos process","engines","circuits","feature extraction","pattern recognition","inversion","size 0.13 mum","pattern clustering"]},{"p_id":65922,"title":"Enhancing performance of restricted Boltzmann machines via log-sum regularization","abstract":"Restricted Boltzmann machines (RBMs) are often used as building blocks to construct a deep belief network. By optimizing several RBMs, the deep networks can be trained quickly to achieve good performance on the tasks of interest. To further improve the performance of data representation, many researches focus on incorporating sparsity into RBMs. In this paper, we propose a novel sparse RBM model, referred to as LogSumRBM. Instead of constraining the expected activation of every hidden unit to the same low level of sparsity as done in [27], we explicitly encourage the hidden units to be sparse through adding a log-sum norm constraint on the totality of the hidden units' activation probabilities. In this approach, we do not need to keep the \"firing rate\" of each hidden unit at a certain level that is set beforehand, and therefore the level of sparsity corresponding to each hidden unit can be automatically learnt based on the task at hand. Some experiments conducted on several image data sets of different scales show that LogSumRBM learns sparser and more discriminative representations compared with the related state-of-the-art models, and stacking two LogSumRBMs learns more significant features which mimic computations in the cortical hierarchy. Meanwhile, LogSumRBM can also be used to pre-train deep networks, and achieve better classification performance. (C) 2014 Elsevier B.V. All rights reserved.","keywords_author":["Restricted Boltzmann machine","Sparsity","Log-sum regularization","Deep belief network","Feature learning"],"keywords_other":["REPRESENTATION","ALGORITHM","VISUAL-CORTEX","STIMULI","NEURAL-NETWORKS","PYRAMIDAL NEURONS","MINIMIZATION","RECOGNITION","SPARSE","GRADIENT"],"max_cite":3.0,"pub_year":2014.0,"sources":"['ieee', 'wos']","rawkeys":["neural-networks","algorithm","log-sum regularization","recognition","visual-cortex","minimization","sparsity","representation","gradient","sparse","deep belief network","feature learning","stimuli","pyramidal neurons","restricted boltzmann machine"],"tags":["log-sum regularization","recognition","visual-cortex","minimization","neural networks","pyramidal neurons","sparsity","representation","gradient","sparse","algorithms","feature learning","stimuli","deep belief networks","restricted boltzmann machine"]},{"p_id":65930,"title":"Sparse coding extreme learning machine for classification","abstract":"As one of supervised learning algorithms, extreme learning machine (ELM) has been proposed for training single-hidden-layer feedforward neural networks and shown great generalization performance. ELM randomly assigns the weights and biases between input and hidden layers and only learns the weights between hidden and output layers. Physiological research has shown that neurons at the same layer are laterally inhibited to each other such that outputs of each layer are sparse. However, it is difficult for ELM to accommodate the lateral inhibition by directly using random feature mapping. Therefore, this paper proposes a sparse coding ELM (ScELM) algorithm, which can map the input feature vector into a sparse representation. In this proposed ScELM algorithm, an unsupervised way is used for sparse coding and dictionary is randomly assigned rather than learned. Gradient projection based method is used for the sparse coding. The output weights are trained through the same supervised way as ELM. Experimental results on the benchmark datasets have shown that this proposed ScELM algorithm can outperform other state-of-the-art methods in terms of classification accuracy. (C) 2017 Elsevier B.V. All rights reserved.","keywords_author":["Sparse coding","Extreme learning machine","Gradient projection"],"keywords_other":["REPRESENTATION","APPROXIMATION","NEURAL-NETWORKS","RECOGNITION","MINIMIZATION","DICTIONARIES"],"max_cite":3.0,"pub_year":2017.0,"sources":"['wos']","rawkeys":["neural-networks","gradient projection","recognition","minimization","representation","approximation","extreme learning machine","dictionaries","sparse coding"],"tags":["gradient projection","recognition","minimization","neural networks","representation","approximation","extreme learning machine","dictionaries","sparse coding"]},{"p_id":98701,"title":"Predictive modeling of geometric shapes of different objects using image processing and an artificial neural network","abstract":"In this study, an artificial neural network model was developed to predict the geometric shapes of different objects using image processing. These objects with various sizes and shapes (circle, square, triangle, and rectangle) were used for the experimental process. In order to extract the features of these geometric shapes, morphological features, including the area, perimeter, compactness, elongation, rectangularity, and roundness, were applied. For the artificial neural network modeling, the standard back-propagation algorithm was found to be the optimum choice for training the model. In the building of the network structure, five different learning algorithms were used: the Levenberg-Marquardt, the quasi-Newton back propagation, the scaled conjugate gradient, the resilient back propagation, and the conjugate gradient back propagation. The best result was obtained by 6-5-1 network architectures with single hidden layers for the geometric shapes. After artificial neural network training, the correlation coefficients (R-2) of the geometric shape values for training and testing data were very close to 1. Similarly, the root-mean-square error and mean error percentage values for the training and testing data were less than 0.9% and 0.004%, respectively. These results demonstrated that the artificial neural network is an admissible model for the estimation of geometric shapes using image processing.","keywords_author":["Geometric shape","image processing","artificial neural network","learning algorithms","predict"],"keywords_other":["COMPACTNESS","BRAIN IMAGES","REPRESENTATION","PERIMETER","CLASSIFICATION","RETRIEVAL","RECOGNITION","PERFORMANCE","3-D OBJECTS"],"max_cite":0.0,"pub_year":2017.0,"sources":"['wos']","rawkeys":["perimeter","recognition","performance","image processing","geometric shape","representation","learning algorithms","classification","3-d objects","predict","retrieval","compactness","brain images","artificial neural network"],"tags":["perimeter","3d object","recognition","performance","neural networks","image processing","geometric shape","prediction","representation","brain imaging","classification","retrieval","compactness","learning algorithm"]},{"p_id":65935,"title":"An overview on Restricted Boltzmann Machines","abstract":"The Restricted Boltzmann Machine (RBM) has aroused wide interest in machine learning fields during the past decade. This review aims to report the recent developments in theoretical research and applications of the RBM. We first give an overview of the general RBM from the theoretical perspective, including stochastic approximation methods, stochastic gradient methods, and preventing overfitting methods. And then this review focuses on the RBM variants which further improve the learning ability of the RBM under general or specific applications. The RBM has recently been extended for representational learning, document modeling, multi-label learning, weakly supervised learning and many other tasks. The RBM and RBM variants provide powerful tools for representing dependency in the data, and they can be used as the basic building blocks to create deep networks. Apart from the Deep Belief Network (DBN) and the Deep Boltzmann Machine (DBM), the RBM can also be combined with the Convolutional Neural Network (CNN) to create deep networks. This review provides a comprehensive view of these advances in the RBM together with its future perspectives. (C) 2017 Elsevier B.V. All rights reserved.","keywords_author":["Restricted Boltzmann Machine","Classification","Representational learning","Deep networks"],"keywords_other":["REGRESSION","REPRESENTATIONS","DISCRETE","GRAPHICAL MODELS","NEURAL-NETWORKS","RECOGNITION","OPTIMIZATION","BELIEF NETWORKS","DISTRIBUTIONS"],"max_cite":0.0,"pub_year":2017.0,"sources":"['ieee', 'wos']","rawkeys":["neural-networks","recognition","discrete","representational learning","distributions","representations","classification","graphical models","belief networks","optimization","regression","deep networks","restricted boltzmann machine"],"tags":["recognition","neural networks","representation","discretization","distributions","representation learning","classification","graphical model","belief networks","optimization","regression","deep networks","restricted boltzmann machine"]},{"p_id":401,"title":"Describing Multimedia Content Using Attention-Based Encoder-Decoder Networks","abstract":"Whereas deep neural networks were first mostly used for classification tasks, they are rapidly expanding in the realm of structured output problems, where the observed target is composed of multiple random variables that have a rich joint distribution, given the input. In this paper we focus on the case where the input also has a rich structure and the input and output structures are somehow related. We describe systems that learn to attend to different places in the input, for each element of the output, for a variety of tasks: machine translation, image caption generation, video clip description, and speech recognition. All these systems are based on a shared set of building blocks: gated recurrent neural networks and convolutional neural networks, along with trained attention mechanisms. We report on experimental results with these systems, showing impressively good performance and the advantage of the attention mechanism.","keywords_author":["Attention mechanism","deep learning","recurrent neural networks","Attention mechanism","deep learning","recurrent neural networks","Attention mechanism","deep learning","recurrent neural networks"],"keywords_other":["Classification tasks","machine translation","speech recognition","Mathematical model","Multimedia contents","Machine translations","Computational modeling","Deep learning","multimedia content","recurrent neural nets","Joint distributions","Context modeling","attention-based encoder-decoder networks","Context","Deep neural networks","video clip description","Attention mechanisms","learning (artificial intelligence)","convolutional neural networks","multimedia systems","image caption generation","RECOGNITION","gated recurrent neural networks","deep neural networks","Recurrent neural networks","Convolutional neural network","Decoding"],"max_cite":47.0,"pub_year":2015.0,"sources":"['ieee', 'scp', 'wos']","rawkeys":["machine translation","speech recognition","classification tasks","recurrent neural networks","multimedia contents","attention mechanism","context modeling","convolutional neural network","multimedia content","recurrent neural nets","attention mechanisms","attention-based encoder-decoder networks","machine translations","video clip description","computational modeling","mathematical model","joint distributions","learning (artificial intelligence)","convolutional neural networks","recognition","deep learning","multimedia systems","image caption generation","gated recurrent neural networks","deep neural networks","context","decoding"],"tags":["speech recognition","classification tasks","multimedia contents","context modeling","convolutional neural network","recurrent neural nets","attention mechanisms","machine learning","attention-based encoder-decoder networks","machine translations","video clip description","computational modeling","mathematical model","joint distributions","recognition","neural networks","multimedia systems","image caption generation","gated recurrent neural network","context","decoding"]},{"p_id":65939,"title":"Spike-Timing-Dependent Construction","abstract":"Spike-timing-dependent construction (STDC) is the production of new spiking neurons and connections in a simulated neural network in response to neuron activity. Following the discovery of spike-timing-dependent plasticity (STDP), significant effort has gone into the modeling and simulation of adaptation in spiking neural networks (SNNs). Limitations in computational power imposed by network topology, however, constrain learning capabilities through connection weight modification alone. Constructive algorithms produce new neurons and connections, allowing automatic structural responses for applications of unknown complexity and nonstationary solutions. A conceptual analogy is developed and extended to theoretical conditions for modeling synaptic plasticity as network construction. Generalizing past constructive algorithms, we propose a framework for the design of novel constructive SNNs and demonstrate its application in the development of simulations for the validation of developed theory. Potential directions of future research and applications of STDC for biological modeling and machine learning are also discussed.","keywords_author":null,"keywords_other":["ADULT NEUROGENESIS","REINFORCEMENT","COMPUTATIONAL POWER","NEURONS","MODEL","NEURAL-NETWORKS","ROBOTS","LEARNING ALGORITHM","VISUAL-PATTERN RECOGNITION","SYNAPTIC PLASTICITY"],"max_cite":1.0,"pub_year":2013.0,"sources":"['ieee', 'wos']","rawkeys":["adult neurogenesis","neural-networks","model","synaptic plasticity","robots","reinforcement","learning algorithm","visual-pattern recognition","neurons","computational power"],"tags":["adult neurogenesis","visual pattern recognition","recognition","model","synaptic plasticity","neural networks","robotics","learning algorithm","neurons","computational power"]},{"p_id":406,"title":"Frontiers of Affect-Aware Learning Technologies","abstract":"Affect-aware technologies are moving the frontiers of how we understand, support, and optimize student learning. The authors explore five areas that exemplify cutting-edge research in the burgeoning field. These include intelligent tutoring systems that detect and respond to students' affective states and sometimes synthesize affect; the strategic induction of confusion as a means to stimulate deep learning; techniques to increase student engagement and reflection; systems that support the development of prosocial behaviors, resilience, and other aspects that contribute to students' well-being; and sample projects that highlight how these new ideas can be taken from laboratories into real-world classrooms.","keywords_author":["advanced learning technologies","affective computing","affect and learning","human-computer interaction"],"keywords_other":["prosocial behavior","student learning","student engagement","Learning systems","Research and development","Education","student affective state","intelligent tutoring systems","student reflection","affect synthesis","resilience","real-world classroom","student well-being","affect-aware learning technologies","psychology","Artificial intelligence"],"max_cite":21.0,"pub_year":2012.0,"sources":"['ieee']","rawkeys":["affective computing","resilience","learning systems","affect and learning","affect-aware learning technologies","student well-being","education","research and development","student engagement","human-computer interaction","affect synthesis","real-world classroom","psychology","student learning","student affective state","prosocial behavior","artificial intelligence","intelligent tutoring systems","student reflection","advanced learning technologies"],"tags":["affective computing","resilience","learning systems","affect and learning","affect-aware learning technologies","student well-being","education","research and development","student engagement","machine learning","human-computer interaction","affect synthesis","real-world classroom","student learning","recognition","student affective state","prosocial behavior","intelligent transportation systems","student reflection","advanced learning technologies"]},{"p_id":33177,"title":"A practical tutorial on autoencoders for nonlinear feature fusion: Taxonomy, models, software and guidelines","abstract":"\u00a9 2017 Elsevier B.V.Many of the existing machine learning algorithms, both supervised and unsupervised, depend on the quality of the input characteristics to generate a good model. The amount of these variables is also important, since performance tends to decline as the input dimensionality increases, hence the interest in using feature fusion techniques, able to produce feature sets that are more compact and higher level. A plethora of procedures to fuse original variables for producing new ones has been developed in the past decades. The most basic ones use linear combinations of the original variables, such as PCA (Principal Component Analysis) and LDA (Linear Discriminant Analysis), while others find manifold embeddings of lower dimensionality based on non-linear combinations, such as Isomap or LLE (Linear Locally Embedding) techniques. More recently, autoencoders (AEs) have emerged as an alternative to manifold learning for conducting nonlinear feature fusion. Dozens of AE models have been proposed lately, each with its own specific traits. Although many of them can be used to generate reduced feature sets through the fusion of the original ones, there also AEs designed with other applications in mind. The goal of this paper is to provide the reader with a broad view of what an AE is, how they are used for feature fusion, a taxonomy gathering a broad range of models, and how they relate to other classical techniques. In addition, a set of didactic guidelines on how to choose the proper AE for a given task is supplied, together with a discussion of the software tools available. Finally, two case studies illustrate the usage of AEs with datasets of handwritten digits and breast cancer.","keywords_author":["Autoencoders","Deep learning","Feature extraction","Feature fusion","Machine learning","Representation learning","Autoencoders","Feature fusion","Feature extraction","Representation learning","Deep learning","Machine learning"],"keywords_other":["Classical techniques","SINGULAR VALUE DECOMPOSITION","Feature fusion","Representation learning","INFORMATION","PRINCIPAL COMPONENT ANALYSIS","Nonlinear feature fusion","CLASSIFICATION","REPRESENTATIONS","NEURAL-NETWORKS","PCA (principal component analysis)","MULTIPLE MEASUREMENTS","LEARNING ALGORITHM","RECOGNITION","DIMENSIONALITY REDUCTION","Linear discriminant analysis","Linear combinations","Autoencoders"],"max_cite":2.0,"pub_year":2018.0,"sources":"['ieee', 'scp', 'wos']","rawkeys":["linear combinations","nonlinear feature fusion","classification","dimensionality reduction","neural-networks","multiple measurements","linear discriminant analysis","pca (principal component analysis)","machine learning","feature fusion","classical techniques","principal component analysis","recognition","deep learning","representation learning","learning algorithm","singular value decomposition","autoencoders","representations","feature extraction","information"],"tags":["linear combinations","nonlinear feature fusion","classification","dimensionality reduction","multiple measurements","linear discriminant analysis","machine learning","feature fusion","classical techniques","principal component analysis","recognition","neural networks","representation learning","learning algorithm","singular value decomposition","auto encoders","representation","feature extraction","information"]},{"p_id":33183,"title":"Modeling Task fMRI Data Via Deep Convolutional Autoencoder","abstract":"\u00a9 1982-2012 IEEE. Task-based functional magnetic resonance imaging (tfMRI) has been widely used to study functional brain networks under task performance. Modeling tfMRI data is challenging due to at least two problems: the lack of the ground truth of underlying neural activity and the highly complex intrinsic structure of tfMRI data. To better understand brain networks based on fMRI data, data-driven approaches have been proposed, for instance, independent component analysis (ICA) and sparse dictionary learning (SDL). However, both ICA and SDL only build shallow models, and they are under the strong assumption that original fMRI signal could be linearly decomposed into time series components with their corresponding spatial maps. As growing evidence shows that human brain function is hierarchically organized, new approaches that can infer and model the hierarchical structure of brain networks are widely called for. Recently, deep convolutional neural network (CNN) has drawn much attention, in that deep CNN has proven to be a powerful method for learning high-level and mid-level abstractions from low-level raw data. Inspired by the power of deep CNN, in this paper, we developed a new neural network structure based on CNN, called deep convolutional auto-encoder (DCAE), in order to take the advantages of both data-driven approach and CNN's hierarchical feature abstraction ability for the purpose of learning mid-level and high-level features from complex, large-scale tfMRI time series in an unsupervised manner. The DCAE has been applied and tested on the publicly available human connectome project tfMRI data sets, and promising results are achieved.","keywords_author":["CNN","deep learning","Task fMRI","unsupervised","Task fMRI","CNN","deep learning","unsupervised"],"keywords_other":["RESTING-STATE FMRI","Independent component analyses (ICA)","Task fMRI","NEUROIMAGING INITIATIVE ADNI","REPRESENTATION","Hierarchical structures","NETWORKS","Human brain functions","unsupervised","Neural network structures","RECOGNITION","FUNCTIONAL ARCHITECTURE","CORTEX","ALZHEIMERS-DISEASE","DYNAMICS","Deep convolutional neural networks","Hierarchical features"],"max_cite":2.0,"pub_year":2018.0,"sources":"['ieee', 'scp', 'wos']","rawkeys":["hierarchical features","task fmri","functional architecture","hierarchical structures","recognition","deep learning","cnn","unsupervised","networks","neural network structures","human brain functions","neuroimaging initiative adni","alzheimers-disease","deep convolutional neural networks","representation","resting-state fmri","independent component analyses (ica)","dynamics","cortex"],"tags":["functional architecture","hierarchical structures","recognition","alzheimers-disease","machine learning","hierarchical features","representation","resting-state fmri","networks","independent component analyses (ica)","task fmri","convolutional neural network","dynamics","neural network structures","cortex","human brain functions","neuroimaging initiative adni"]},{"p_id":445,"title":"Deep Networks are Effective Encoders of Periodicity","abstract":"We present a comparative theoretical analysis of representation in artificial neural networks with two extreme architectures, a shallow wide network and a deep narrow network, devised to maximally decouple their representative power due to layer width and network depth. We show that, given a specific activation function, models with comparable VC-dimension are required to guarantee zero error modeling of real functions over a binary input. However, functions that exhibit repeating patterns can be encoded much more efficiently in the deep representation, resulting in significant reduction in complexity. This paper provides some initial theoretical evidence of when and how depth can be extremely effective.","keywords_author":["Deep architectures","universal approximation","Deep architectures","universal approximation."],"keywords_other":["Biological neural networks","zero error modeling","BELIEF NETWORKS","Computer architecture","Computational modeling","computational complexity","Deep architectures","shallow wide network","complexity reduction","UNIVERSAL APPROXIMATORS","activation function","deep narrow network","BOLTZMANN MACHINES","universal approximation","DIMENSION","Vectors","RECOGNITION","Complexity theory","neural nets","Neurons","Function approximation","periodicity encoders","transfer functions","VC-dimension","artificial neural networks"],"max_cite":17.0,"pub_year":2014.0,"sources":"['wos', 'ieee']","rawkeys":["zero error modeling","computational complexity","function approximation","vectors","shallow wide network","complexity reduction","universal approximators","activation function","biological neural networks","neurons","computer architecture","computational modeling","deep narrow network","complexity theory","recognition","boltzmann machines","deep architectures","universal approximation","dimension","belief networks","neural nets","periodicity encoders","transfer functions","artificial neural networks","vc-dimension"],"tags":["zero error modeling","dimensions","computational complexity","function approximation","vectors","shallow wide network","complexity reduction","universal approximators","biological neural networks","neurons","computer architecture","computational modeling","deep narrow network","complexity theory","recognition","neural networks","boltzmann machines","deep architectures","belief networks","periodicity encoders","transfer functions","activation functions","vc dimension"]},{"p_id":106960,"title":"Toward an Instructionally Oriented Theory of Example-Based Learning","abstract":"Learning from examples is a very effective means of initial cognitive skill acquisition. There is an enormous body of research on the specifics of this learning method. This article presents an instructionally oriented theory of example-based learning that integrates theoretical assumptions and findings from three research areas: learning from worked examples, observational learning, and analogical reasoning. This theory has descriptive and prescriptive elements. The descriptive subtheory deals with (a) the relevance and effectiveness of examples, (b) phases of skill acquisition, and (c) learning processes. The prescriptive subtheory proposes instructional principles that make full exploitation of the potential of example-based learning possible.","keywords_author":["Worked example","Observational learning","Analogical reasoning","Transfer","Skill acquisition"],"keywords_other":["PROBLEM-SOLVING TRANSFER","RELATIONAL INTEGRATION","EARLIER PROBLEMS","COGNITIVE SKILL ACQUISITION","WORKING-MEMORY","ANALOGICAL TRANSFER","SELF-EXPLAINING EXAMPLES","WORD-PROBLEMS","INCORRECT EXAMPLES","WORKED-OUT EXAMPLES"],"max_cite":76.0,"pub_year":2014.0,"sources":"['wos']","rawkeys":["transfer","relational integration","observational learning","self-explaining examples","earlier problems","working-memory","cognitive skill acquisition","worked example","skill acquisition","worked-out examples","analogical reasoning","word-problems","incorrect examples","analogical transfer","problem-solving transfer"],"tags":["recognition","relational integration","observational learning","self-explaining examples","worked examples","earlier problems","working-memory","cognitive skill acquisition","skill acquisition","worked-out examples","analogical reasoning","word-problems","incorrect examples","analogical transfer","problem-solving transfer"]},{"p_id":33234,"title":"Parallel deep solutions for image retrieval from imbalanced medical imaging archives","abstract":"\u00a9 2017 Elsevier B.V. Learning and extracting representative features along with similarity measurements in high dimensional feature spaces is a critical task. Moreover, the problem of how to bridge the semantic gap, between the low-level information captured by a machine learning model and the high-level one interpreted by a human operator, is still a practical challenge, especially in medicine. In medical applications, retrieving similar images from archives of past cases can be immensely beneficial in diagnostic imaging. However, large and balanced datasets may not be available for many reasons. Exploring the ways of using deep networks, for classification to retrieval, to fill this semantic gap was a key question for this research. In this work, we propose a parallel deep solution approach based on convolutional neural networks followed by a local search using LBP, HOG and Radon features. The IRMA dataset, from ImageCLEF initiative, containing 14,400 X-ray images, is employed to validate the proposed scheme. With a total IRMA error of 165.55, the performance of our scheme surpasses the dictionary approach and many other learning methods applied on the same dataset.","keywords_author":["CBIR","Content-based image retrieval","Deep learning","HOG","LBP","Medical imaging","Radon","Content-based image retrieval","CBIR","Medical imaging","Deep learning","LBP","HOG","Radon"],"keywords_other":["Balanced datasets","Diagnostic imaging","Similarity measurements","FRAMEWORK","Machine learning models","CLASSIFICATION","CBIR","RECOGNITION","High-dimensional feature space","Convolutional neural network","CATEGORIZATION","PATTERNS","Content based image retrieval"],"max_cite":2.0,"pub_year":2018.0,"sources":"['ieee', 'scp', 'wos']","rawkeys":["machine learning models","patterns","content based image retrieval","classification","cbir","convolutional neural network","high-dimensional feature space","radon","similarity measurements","content-based image retrieval","lbp","recognition","deep learning","framework","medical imaging","categorization","balanced datasets","hog","diagnostic imaging"],"tags":["recognition","framework","machine learning models","medical imaging","machine learning","histogram of oriented gradients","patterns","balanced datasets","categorization","radon","similarity measure","classification","convolutional neural network","high-dimensional feature space","local binary patterns","diagnostic imaging","content-based image retrieval"]},{"p_id":8666,"title":"NeC4.5: Neural ensemble based C4.5","abstract":"Decision tree is with good comprehensibility while neural network ensemble is with strong generalization ability. In this paper, these merits are integrated into a novel decision tree algorithm NeC4.5. This algorithm trains a neural network ensemble at first. Then, the trained ensemble is employed to generate a new training set through replacing the desired class labels of the original training examples with those output from the trained ensemble. Some extra training examples are also generated from the trained ensemble and added to the new training set. Finally, a C4.5 decision tree is grown from the new training set. Since its learning results are decision trees, the comprehensibility of NeC4.5 is better than that of neural network ensemble. Moreover, experiments show that the generalization ability of NeC4.5 decision trees can be better than that of C4.5 decision trees.","keywords_author":["Comprehensibility","Decision tree","Ensemble learning","Generalization","Machine learning","Neural network ensemble","Neural networks"],"keywords_other":["Comprehensibility","Decision tree","Ensemble learning","Neural network ensemble","Generalization"],"max_cite":93.0,"pub_year":2004.0,"sources":"['scp']","rawkeys":["neural network ensemble","neural networks","comprehensibility","machine learning","generalization","decision tree","ensemble learning"],"tags":["comprehension","recognition","neural networks","machine learning","ensemble learning","neural network ensembles","decision trees"]},{"p_id":25062,"title":"Good recognition is non-metric","abstract":"Recognition is the fundamental task of visual cognition, yet how to formalize the general recognition problem for computer vision remains an open issue. The problem is sometimes reduced to the simplest case of recognizing matching pairs, often structured to allow for metric constraints. However, visual recognition is broader than just pair-matching: what we learn and how we learn it has important implications for effective algorithms. In this review paper, we reconsider the assumption of recognition as a pair-matching test, and introduce a new formal definition that captures the broader context of the problem. Through a meta-analysis and an experimental assessment of the top algorithms on popular data sets, we gain a sense of how often metric properties are violated by recognition algorithms. By studying these violations, useful insights come to light: we make the case for local distances and systems that leverage outside information to solve the general recognition problem. \u00a9 2014 Elsevier Ltd.","keywords_author":["Computer vision","Face recognition","Machine learning","Metric learning","Object recognition","Recognition"],"keywords_other":["Visual recognition","Recognition","Effective algorithms","Recognition algorithm","Metric learning","Experimental assessment","Metric properties","Formal definition"],"max_cite":8.0,"pub_year":2014.0,"sources":"['scp']","rawkeys":["metric properties","visual recognition","recognition","formal definition","recognition algorithm","effective algorithms","machine learning","object recognition","face recognition","experimental assessment","computer vision","metric learning"],"tags":["metric properties","visual recognition","recognition","formal definition","recognition algorithm","effective algorithms","machine learning","object recognition","face recognition","experimental assessment","computer vision","metric learning"]},{"p_id":33265,"title":"Multi-modal feature fusion for geographic image annotation","abstract":"\u00a9 2017This paper presents a multi-modal feature fusion based framework to improve the geographic image annotation. To achieve effective representations of geographic images, the method leverages a low-to-high learning flow for both the deep and shallow modality features. It first extracts low-level features for each input image pixel, such as shallow modality features (SIFT, Color, and LBP) and deep modality features (CNNs). It then constructs mid-level features for each superpixel from low-level features. Finally it harvests high-level features from mid-level features by using deep belief networks (DBN). It uses a restricted Boltzmann machine (RBM) to mine deep correlations between high-level features from both shallow and deep modalities to achieve a final representation for geographic images. Comprehensive experiments show that this feature fusion based method achieves much better performances compared to traditional methods.","keywords_author":["Convolutional neural networks (CNNs)","Deep learning","Geographic image annotation","Multi-modal feature fusion","Convolutional neural networks (CNNs)","Deep learning","Geographic image annotation","Multi-modal feature fusion"],"keywords_other":["Restricted boltzmann machine","Feature fusion","REPRESENTATION","High-level features","Low-level features","Mid-level features","CLASSIFICATION","MODEL","RECOGNITION","KEYPOINTS","Geographic images","Convolutional neural network","SATELLITE IMAGES","VEHICLE DETECTION","Deep belief network (DBN)","CONVOLUTIONAL NEURAL-NETWORKS"],"max_cite":2.0,"pub_year":2018.0,"sources":"['ieee', 'scp', 'wos']","rawkeys":["satellite images","classification","convolutional neural-networks","convolutional neural network","convolutional neural networks (cnns)","multi-modal feature fusion","feature fusion","high-level features","recognition","deep belief network (dbn)","deep learning","vehicle detection","restricted boltzmann machine","model","keypoints","geographic image annotation","representation","geographic images","low-level features","mid-level features"],"tags":["high-level features","mid-level features","model","recognition","keypoints","machine learning","geographic image annotation","feature fusion","geographic images","multi-modal feature fusion","representation","satellite images","vehicle detection","classification","convolutional neural network","deep belief networks","restricted boltzmann machine","low-level features"]},{"p_id":509,"title":"Dependability in a Multi-tenant Multi-framework Deep Learning as-a-Service Platform","abstract":"Deep learning (DL), a form of machine learning, is becoming increasingly popular in several application domains. As a result, cloud-based Deep Learning as a Service (DLaaS) platforms have become an essential infrastructure in many organizations. These systems accept, schedule, manage and execute DL training jobs at scale. This paper explores dependability in the context of a DLaaS platform used in IBM. We begin by explaining how DL training workloads are different, and what features ensure dependability in this context. We then describe the architecture, design and implementation of a cloud-based orchestration system for DL training. We show how this system has been architected with dependability in mind while also being horizontally scalable, elastic, flexible and efficient. We also present an initial empirical evaluation of the overheads introduced by our platform, and discuss tradeoffs between efficiency and dependability.","keywords_author":["dependability","deep learning","cloud platforms","lifecycle management","orchestration"],"keywords_other":["Containers","Noise measurement","Training","Machine learning","Computer crashes","Checkpointing","Hardware"],"max_cite":null,"pub_year":2018.0,"sources":"['ieee']","rawkeys":["cloud platforms","deep learning","training","machine learning","containers","orchestration","computer crashes","hardware","lifecycle management","checkpointing","dependability","noise measurement"],"tags":["cloud platforms","recognition","life-cycle management","training","machine learning","containers","orchestration","computer crashes","checkpoint","hardware","noise measurement"]},{"p_id":33287,"title":"Weed detection in soybean crops using ConvNets","abstract":"\u00a9 2017 Elsevier B.V.Weeds are undesirable plants that grow in agricultural crops, such as soybean crops, competing for elements such as sunlight and water, causing losses to crop yields. The objective of this work was to use Convolutional Neural Networks (ConvNets or CNNs) to perform weed detection in soybean crop images and classify these weeds among grass and broadleaf, aiming to apply the specific herbicide to weed detected. For this purpose, a soybean plantation was carried out in Campo Grande, Mato Grosso do Sul, Brazil, and the Phantom DJI 3 Professional drone was used to capture a large number of crop images. With these photographs, an image database was created containing over fifteen thousand images of the soil, soybean, broadleaf and grass weeds. The Convolutional Neural Networks used in this work represent a Deep Learning architecture that has achieved remarkable success in image recognition. For the training of Neural Network the CaffeNet architecture was used. Available in Caffe software, it consists of a replication of the well known AlexNet, network which won the ImageNet Large Scale Visual Recognition Challenge 2012 (ILSVRC2012). A software was also developed, Pynovis\u00e3o, which through the use of the superpixel segmentation algorithm SLIC, was used to build a robust image dataset and classify images using the model trained by Caffe software. In order to compare the results of ConvNets, Support Vector Machines, AdaBoost and Random Forests were used in conjunction with a collection of shape, color and texture feature extraction techniques. As a result, this work achieved above 98% accuracy using ConvNets in the detection of broadleaf and grass weeds in relation to soil and soybean, with an accuracy average between all images above 99%.","keywords_author":["Computer vision","Deep Learning","Weed detection","Deep Learning","Weed detection","Computer vision"],"keywords_other":["Visual recognition","Weed detection","Agricultural crops","Learning architectures","Color and texture features","CLASSIFICATION","RECOGNITION","Convolutional neural network","Superpixel segmentations","Image datasets","IMAGES"],"max_cite":2.0,"pub_year":2017.0,"sources":"['ieee', 'scp', 'wos']","rawkeys":["learning architectures","visual recognition","image datasets","images","recognition","deep learning","color and texture features","weed detection","agricultural crops","superpixel segmentations","classification","convolutional neural network","computer vision"],"tags":["learning architectures","visual recognition","image datasets","images","recognition","color and texture features","machine learning","weed detection","agricultural crops","superpixel segmentations","classification","convolutional neural network","computer vision"]},{"p_id":115207,"title":"Adaptive neuro-heuristic hybrid model for fruit peel defects detection","abstract":"Fusion of machine learning methods benefits in decision support systems. A composition of approaches gives a possibility to use the most efficient features composed into one solution. In this article we would like to present an approach to the development of adaptive method based on fusion of proposed novel neural architecture and heuristic search into one co-working solution. We propose a developed neural network architecture that adapts to processed input co-working with heuristic method used to precisely detect areas of interest. Input images are first decomposed into segments. This is to make processing easier, since in smaller images (decomposed segments) developed Adaptive Artificial Neural Network (AANN) processes less information what makes numerical calculations more precise. For each segment a descriptor vector is composed to be presented to the proposed AANN architecture. Evaluation is run adaptively, where the developed AANN adapts to inputs and their features by composed architecture. After evaluation, selected segments are forwarded to heuristic search, which detects areas of interest. As a result the system returns the image with pixels located over peel damages. Presented experimental research results on the developed solution are discussed and compared with other commonly used methods to validate the efficacy and the impact of the proposed fusion in the system structure and training process on classification results. (C) 2017 Elsevier Ltd. All rights reserved.","keywords_author":["Neural networks","Adaptive systems","Automated decision support","Heuristic methods","Image processing"],"keywords_other":["CUCKOO SEARCH ALGORITHM","SPIKING NEURONS","FEATURES","NETWORKS","VISION","CLASSIFICATION","APPLES","SYSTEM","RECOGNITION","SEGMENTATION"],"max_cite":1.0,"pub_year":2018.0,"sources":"['wos']","rawkeys":["heuristic methods","recognition","segmentation","features","image processing","neural networks","automated decision support","system","vision","spiking neurons","networks","classification","apples","adaptive systems","cuckoo search algorithm"],"tags":["heuristic methods","recognition","cuckoo search algorithms","features","image processing","neural networks","automated decision support","segmentation","system","vision","spiking neurons","networks","classification","apples","adaptive systems"]},{"p_id":33327,"title":"Learning Aerial Image Segmentation from Online Maps","abstract":"\u00a9 1980-2012 IEEE. This paper deals with semantic segmentation of high-resolution (aerial) images where a semantic class label is assigned to each pixel via supervised classification as a basis for automatic map generation. Recently, deep convolutional neural networks (CNNs) have shown impressive performance and have quickly become the de-facto standard for semantic segmentation, with the added benefit that task-specific feature design is no longer necessary. However, a major downside of deep learning methods is that they are extremely data hungry, thus aggravating the perennial bottleneck of supervised classification, to obtain enough annotated training data. On the other hand, it has been observed that they are rather robust against noise in the training labels. This opens up the intriguing possibility to avoid annotating huge amounts of training data, and instead train the classifier from existing legacy data or crowd-sourced maps that can exhibit high levels of noise. The question addressed in this paper is: can training with large-scale publicly available labels replace a substantial part of the manual labeling effort and still achieve sufficient performance? Such data will inevitably contain a significant portion of errors, but in return virtually unlimited quantities of it are available in larger parts of the world. We adapt a state-of-the-art CNN architecture for semantic segmentation of buildings and roads in aerial images, and compare its performance when using different training data sets, ranging from manually labeled pixel-accurate ground truth of the same city to automatic training data derived from OpenStreetMap data from distant locations. We report our results that indicate that satisfying performance can be obtained with significantly less manual annotation effort, by exploiting noisy large-scale training data.","keywords_author":["Crowdsourcing","image classification","machine learning","neural networks","supervised learning","terrain mapping","urban areas","Crowdsourcing","image classification","machine learning","neural networks","supervised learning","terrain mapping","urban areas"],"keywords_other":["Manuals","Terrain mapping","Training data","POINT-PROCESSES","FEATURES","EXTRACTION","Urban areas","CLASSIFICATION","RECOGNITION","OPENSTREETMAP","3D","Roads","TEXTURE","ROAD NETWORKS","OBJECT DETECTION"],"max_cite":2.0,"pub_year":2017.0,"sources":"['ieee', 'scp', 'wos']","rawkeys":["supervised learning","3d","texture","classification","image classification","training data","extraction","urban areas","terrain mapping","features","roads","machine learning","crowdsourcing","object detection","manuals","recognition","neural networks","road networks","openstreetmap","point-processes"],"tags":["supervised learning","texture","classification","image classification","training data","extraction","urban areas","terrain mapping","features","roads","machine learning","crowdsourcing","object detection","manuals","point process","recognition","neural networks","openstreetmap (osm)","road network","three-dimensional"]},{"p_id":16948,"title":"Activities of daily living and quality of life across different stages of dementia: A UK study","abstract":"\u00a9 2014 Taylor & Francis.Objectives: People with dementia (PwD) require an increasing degree of assistance with activities of daily living (ADLs), and dependency may negatively impact on their well-being. However, it remains unclear which activities are impaired at each stage of dementia and to what extent this is associated with variations in quality of life (QoL) across the different stages, which were the two objectives of this study.Methods: The sample comprised 122 PwD, and their carers, either living at home or recently admitted to long-term care. Measures of cognition and QoL were completed by the PwD and proxy measures of psychopathology, depression, ADLs and QoL were recorded. Using frequency, correlation and multiple regression analysis, data were analysed for the number of ADL impairments across mild, moderate and severe dementia and for the factors impacting on QoL.Results: ADL performance deteriorates differently for individual activities, with some ADLs showing impairment in mild dementia, including dressing, whereas others only deteriorate later on, including feeding. This decline may be seen in the degree to which carers perceive ADLs to explain the QoL of the PwD, with more ADLs associated with QoL in severe dementia. Results of the regression analysis showed that total ADL performance however was only impacting on QoL in moderate dementia.Conclusion: Knowledge about performance deterioration in different ADLs has implications for designing interventions to address specific activities at different stages of the disease. Furthermore, findings suggest that different factors are important to consider when trying to improve or maintain QoL at different stages.","keywords_author":["activities of daily living","carers","dementia","depression","quality of life"],"keywords_other":["Humans","Neuropsychological Tests","Depression","Aged, 80 and over","Europe","Aged","Cross-Sectional Studies","Caregivers","Dementia","Female","Psychiatric Status Rating Scales","Activities of Daily Living","Dependency (Psychology)","Comorbidity","Quality of Life","Severity of Illness Index","Male","Socioeconomic Factors","Questionnaires"],"max_cite":33.0,"pub_year":2015.0,"sources":"['scp']","rawkeys":["psychiatric status rating scales","aged","depression","neuropsychological tests","comorbidity","dementia","dependency (psychology)","socioeconomic factors","severity of illness index","questionnaires","humans","caregivers","80 and over","activities of daily living","cross-sectional studies","quality of life","male","europe","carers","female"],"tags":["psychiatric status rating scales","aged","depression","neuropsychological tests","comorbidity","dementia","socioeconomic factors","severity of illness index","recognition","humans","caregivers","80 and over","activities of daily living","cross-sectional studies","quality of life","male","europe","carers","questionnaire","female"]},{"p_id":33334,"title":"A deep neural network for real-time detection of falling humans in naturally occurring scenes","abstract":"\u00a9 2017 Elsevier B.V. We introduce a novel approach to the problem of human fall detection in naturally occurring scenes. This is important because falling incidents cause thousands of deaths every year and vision-based approaches offer a promising and effective way to detect falls. To address this challenging issue, we regard it as an example of action detection and propose to also locate its temporal extent. We achieve this by exploiting the effectiveness of deep networks. In the training stage, the trimmed video clips of four phases (standing, falling, fallen and not moving) in a fall are converted to four categories of so-called dynamic image to train a deep ConvNet that scores and predicts the label of each dynamic image. In the testing stage, a set of sub-videos is generated using a sliding window on an untrimmed video that converts it to multiple dynamic images. Based on the predicted label of each dynamic image by the trained deep ConvNet, the videos are classified as falling or not by a \u201cstanding watch\u201d for a situation consisting of the four sequential phases. In order to localize the temporal extent of the event, we propose a difference score method (DSM) based on adjacent dynamic images in the temporal sequence. We collect a new dataset, called the YouTube Fall Dataset (YTFD), which contains 430 falling incidents and 176 normal activities and use it to learn the deep network to detect falling humans. We perform experiments on datasets of varying complexity: Le2i fall detection dataset, multiple cameras fall dataset, high quality fall simulation dataset and our own YouTube Fall Dataset. The results demonstrate the effectiveness and efficiency of our approach.","keywords_author":["Action detection","Convolutional neural network","Deep learning","Dynamic image","Fall detection","Temporal location","Fall detection","Action detection","Temporal location","Dynamic image","Convolutional neural network","Deep learning"],"keywords_other":["Dynamic images","SIMULATED DATA","Real-time detection","Vision-based approaches","Human fall detection","ENVIRONMENTS","RECOGNITION","Convolutional neural network","Fall detection","MOTION","SURVEILLANCE","CAMERA","Effectiveness and efficiencies","Naturally occurring"],"max_cite":2.0,"pub_year":2017.0,"sources":"['ieee', 'scp', 'wos']","rawkeys":["surveillance","recognition","human fall detection","deep learning","effectiveness and efficiencies","motion","action detection","dynamic image","simulated data","vision-based approaches","fall detection","camera","environments","real-time detection","temporal location","convolutional neural network","naturally occurring","dynamic images"],"tags":["surveillance","recognition","human fall detection","effectiveness and efficiencies","motion","action detection","machine learning","simulated data","vision-based approaches","cameras","fall detection","real-time detection","temporal location","convolutional neural network","environment","naturally occurring","dynamic images"]},{"p_id":25146,"title":"Fast dependency-aware feature selection in very-high-dimensional pattern recognition","abstract":"The paper addresses the problem of making dependency-aware feature selection feasible in pattern recognition problems of very high dimensionality. The idea of individually best ranking is generalized to evaluate the contextual quality of each feature in a series of randomly generated feature subsets. Each random subset is evaluated by a criterion function of arbitrary choice (permitting functions of high complexity). Eventually, the novel dependency-aware feature rank is computed, expressing the average benefit of including a feature into feature subsets. The method is efficient and generalizes well especially in very-high-dimensional problems, where traditional context-aware feature selection methods fail due to prohibitive computational complexity or to over-fitting. The method is shown well capable of over-performing the commonly applied individual ranking which ignores important contextual information contained in data. \u00a9 2011 IEEE.","keywords_author":["classification","feature selection","generalization","high dimensionality","machine learning","over-fitting","pattern recognition","ranking","stability"],"keywords_other":["High dimensionality","Overfitting","Machine-learning","generalization","ranking"],"max_cite":8.0,"pub_year":2011.0,"sources":"['scp']","rawkeys":["machine learning","generalization","high dimensionality","feature selection","classification","stability","ranking","machine-learning","pattern recognition","overfitting","over-fitting"],"tags":["recognition","standards","machine learning","high-dimensional","feature selection","stability","classification","pattern recognition","overfitting"]},{"p_id":573,"title":"A deep learning method for recognizing elevated mature strawberries","abstract":"Strawberry picking by machines confronts a complex environment where a targe may be sheltered by leaves or overlap with each other. Also, it is a challenge for machines to recognize mature strawberries among those in different maturity. This work presents a fast recognition method for elevated mature strawberries by the approach of deep learning. It uses an Ostu algorithm to separate targets from background and then the resulted effective image areas designated by the minimum external rectangular marking method are used to train CaffeNet for automatic target recognition. For comparison, we also design a SVM classifer that uses HOG gradient direction feature and H component of the color feature of the mature strawberries. The experimental results show that the average recognition rate of mature strawberries by CaffeNet can reach 95%, higher than that by SVM by 11%.","keywords_author":["Deep learning","HOG feature","H component","Ostu algorithm","Recognition","Strawberry picking","SVM"],"keywords_other":["Feature extraction","Training","Image color analysis","Machine learning","Kernel","Support vector machines","Target recognition"],"max_cite":null,"pub_year":2018.0,"sources":"['ieee']","rawkeys":["recognition","strawberry picking","deep learning","training","machine learning","image color analysis","svm","kernel","h component","hog feature","feature extraction","support vector machines","ostu algorithm","target recognition"],"tags":["recognition","strawberry picking","training","machine learning","image color analysis","kernel","h component","hog feature","feature extraction","ostu algorithm","target recognition"]},{"p_id":49734,"title":"Fault Diagnosis in Highly Dependable Medical Wearable Systems","abstract":"\u00a9 2016, Springer Science+Business Media New York.High levels of dependability are required to promote the adherence by public and medical communities to wearable medical devices. The study presented herein addresses fault detection and diagnosis in these systems. The main objective resides on correctly classifying the captured physiological signals, in order to distinguish whether the actual cause of a detected anomaly is a wearer health condition or a system functional flaw. Data fusion techniques, namely fuzzy logic, artificial neural networks, decision trees and naive Bayes classifiers are employed to process the captured data to increase the trust levels with which diagnostics are made. Concerning the wearer condition, additional information is provided after classifying the set of signals into normal or abnormal (e.g., arrhythmia, tachycardia and bradycardia). As for the monitoring system, once an abnormal situation is detected in its operation or in the sensors, a set of tests is run to check if actually the wearer shows a degradation of his health condition or if the system is reporting erroneous values. Selected features from the vital signals and from quantities that characterize the system performance serve as inputs to the data fusion algorithms for Patient and System Status diagnosis purposes. The algorithms performance was evaluated based on their sensitivity, specificity and accuracy. Based on these criteria the naive Bayes classifier presented the best performance.","keywords_author":["Dependability","Fault detection","Machine learning","Wearable medical systems"],"keywords_other":["Naive Bayes classifiers","Wearable medical devices","Medical systems","Physiological signals","Data fusion algorithm","Data fusion technique","Fault detection and diagnosis","Dependability"],"max_cite":0.0,"pub_year":2016.0,"sources":"['scp']","rawkeys":["wearable medical devices","data fusion algorithm","physiological signals","machine learning","fault detection","fault detection and diagnosis","naive bayes classifiers","wearable medical systems","medical systems","dependability","data fusion technique"],"tags":["wearable medical devices","recognition","data fusion algorithm","physiological signals","machine learning","fault detection","fault detection and diagnosis","naive bayes classifiers","wearable medical systems","medical systems","data fusion technique"]},{"p_id":33367,"title":"A survey on data quality for dependable monitoring in wireless sensor networks","abstract":"\u00a9 2017 by the authors. Licensee MDPI, Basel, Switzerland. Wireless sensor networks are being increasingly used in several application areas, particularly to collect data and monitor physical processes. Non-functional requirements, like reliability, security or availability, are often important and must be accounted for in the application development. For that purpose, there is a large body of knowledge on dependability techniques for distributed systems, which provide a good basis to understand how to satisfy these non-functional requirements of WSN-based monitoring applications. Given the data-centric nature of monitoring applications, it is of particular importance to ensure that data are reliable or, more generically, that they have the necessary quality. In this survey, we look into the problem of ensuring the desired quality of data for dependable monitoring using WSNs. We take a dependability-oriented perspective, reviewing the possible impairments to dependability and the prominent existing solutions to solve or mitigate these impairments. Despite the variety of components that may form a WSN-based monitoring system, we give particular attention to understanding which faults can affect sensors, how they can affect the quality of the information and how this quality can be improved and quantified.","keywords_author":["Data quality","Dependability","Machine learning","Monitoring","Sensor fusion","Wireless sensor networks"],"keywords_other":["Application development","Monitoring applications","Data quality","Non-functional requirements","Distributed systems","Sensor fusion","Body of knowledge","Dependability"],"max_cite":2.0,"pub_year":2017.0,"sources":"['scp']","rawkeys":["body of knowledge","non-functional requirements","monitoring applications","data quality","machine learning","distributed systems","wireless sensor networks","sensor fusion","application development","dependability","monitoring"],"tags":["recognition","body of knowledge","monitoring applications","data quality","machine learning","application developers","distributed systems","wireless sensor networks","sensor fusion","non-functional requirements","monitoring"]},{"p_id":33380,"title":"Spatiotemporal Joint Mitosis Detection Using CNN-LSTM Network in Time-Lapse Phase Contrast Microscopy Images","abstract":"\u00a9 2017 IEEE.We present an approach to jointly detect mitotic events spatially and temporally in time-lapse phase contrast microscopy images. In particular, we combine a convolutional neural network (CNN) and a long short-term memory (LSTM) network to detect mitotic events in patch sequences. The CNN-LSTM network can be trained end-to-end to simultaneously learn convolutional features within each frame and temporal dynamics between frames, without hand-crafted visual or temporal feature design. Owing to the LSTM layer, this approach is able to detect mitotic events in patch sequences of variable length, as well as making use of longer context information among frames in the sequences. To the best of our knowledge, this is the first work to detect mitosis using deep learning in both spatial and temporal domains. Experiments have shown that the CNN-LSTM network can be trained efficiently, and we evaluate this design by applying the network to original raw microscopy image sequences to locate mitotic events both spatially and temporally. The data with which we validate the proposed method include C3H10 mesenchymal and C2C12 myoblastic stem cell populations. Our approach achieved the F score of 98.72% on the C2C12 data set, and the F score of 96.5% on the C3H10 data set. The results on both data sets outperform the traditional graph model-based approaches by a large margin, both in terms of detection accuracy and frame localization accuracy. Furthermore, we have developed a framework to aid humans in annotating mitosis with high efficiency and accuracy in raw phase contrast microscopy images based on the joint detection results using the proposed method. Under this framework, expert level annotations can be obtained in raw phase contrast microscopy image sequences, and the annotations have shown to further improve the training performance of the CNN-LSTM network.","keywords_author":["Biomedical imaging","computer vision","machine learning","mitosis detection","stem cell","Biomedical imaging","computer vision","mitosis detection","machine learning","stem cell"],"keywords_other":["Localization accuracy","Biomedical imaging","Spatiotemporal phenomena","Mitosis detections","Phase-contrast microscopy images","STEM-CELL POPULATIONS","SEQUENCES","RECOGNITION","Convolutional neural network","Myoblastic stem cells","Context information","TRACKING"],"max_cite":2.0,"pub_year":2017.0,"sources":"['ieee', 'scp', 'wos']","rawkeys":["phase-contrast microscopy images","recognition","spatiotemporal phenomena","stem-cell populations","myoblastic stem cells","machine learning","mitosis detections","stem cell","localization accuracy","tracking","sequences","convolutional neural network","computer vision","mitosis detection","biomedical imaging","context information"],"tags":["phase-contrast microscopy images","recognition","spatiotemporal phenomena","stem-cell populations","myoblastic stem cells","machine learning","stem-cells","tracking","localization accuracy","sequence","convolutional neural network","computer vision","mitosis detection","biomedical imaging","context information"]},{"p_id":33387,"title":"Dependency and AMR embeddings for drug-drug interaction extraction from biomedical literature","abstract":"\u00a9 2017 ACM. Drug-drug interaction (DDI) is an unexpected change in a drug's effect on the human body when the drug and a second drug are co-prescribed and taken together. As many DDIs are frequently reported in biomedical literature, it is important to mine DDI information from literature to keep DDI knowledge up to date. One of the SemEval challenges in the year 2011 and 2013 was designed to tackle the task where the best system achieved an F1 score of 0.80. In this paper, we propose to utilize dependency embeddings and Abstract Meaning Representation (AMR) embeddings as features for extracting DDIs. Our contribution is two-fold. First, we employed dependency embeddings, previously shown effective for sentence classification, for DDI extraction. The dependency embeddings incorporated structural syntactic contexts into the embeddings, which were not present in the conventional word embeddings. Second, we proposed a novel syntactic embedding approach using AMR. AMR aims to abstract away from syntactic idiosyncrasies and attempts to capture only the core meaning of a sentence, which could potentially improve DDI extraction from sentences. Two classifiers (Support Vector Machine and Random Forest) taking these embedding features as input were evaluated on the DDIExtraction 2013 challenge corpus. The experimental results show the effectiveness of dependency and AMR embeddings in the DDI extraction task. The best performance was obtained by combining word, dependency and AMR embeddings (F1 score=0.84).","keywords_author":["Abstract Meaning Representation","Deep learning","Dependency","Drug-drug interaction","Embeddings"],"keywords_other":["Biomedical literature","F1 scores","Random forests","Drug-drug interactions","Sentence classifications","Human bodies","Dependency","Embeddings"],"max_cite":2.0,"pub_year":2017.0,"sources":"['scp', 'wos']","rawkeys":["embeddings","abstract meaning representation","deep learning","drug-drug interactions","dependency","f1 scores","random forests","sentence classifications","drug-drug interaction","human bodies","biomedical literature"],"tags":["embeddings","recognition","abstract meaning representation","drug-drug interactions","machine learning","f1 scores","random forests","sentence classifications","human bodies","biomedical literature"]},{"p_id":8816,"title":"Rotation-invariant convolutional neural networks for galaxy morphology prediction","abstract":"\u00a9 2015 The Authors Published by Oxford University Press on behalf of the Royal Astronomical Society.Measuring the morphological parameters of galaxies is a key requirement for studying their formation and evolution. Surveys such as the Sloan Digital Sky Survey have resulted in the availability of very large collections of images, which have permitted population-wide analyses of galaxy morphology. Morphological analysis has traditionally been carried out mostly via visual inspection by trained experts, which is time consuming and does not scale to large (\u227310<sup>4<\/sup>) numbers of images. Although attempts have been made to build automated classification systems, these have not been able to achieve the desired level of accuracy. The Galaxy Zoo project successfully applied a crowdsourcing strategy, inviting online users to classify images by answering a series of questions. Unfortunately, even this approach does not scale well enough to keep up with the increasing availability of galaxy images. We present a deep neural network model for galaxy morphology classification which exploits translational and rotational symmetry. It was developed in the context of the Galaxy Challenge, an international competition to build the best model for morphology classification based on annotated images from the Galaxy Zoo project. For images with high agreement among the Galaxy Zoo participants, our model is able to reproduce their consensus with near-perfect accuracy (>99 per cent) for most questions. Confident model predictions are highly accurate, which makes the model suitable for filtering large collections of images and forwarding challenging images to experts for manual annotation. This approach greatly reduces the experts' workload without affecting accuracy. The application of these algorithms to larger sets of training data will be critical for analysing results from future surveys such as the Large Synoptic Survey Telescope.","keywords_author":["Catalogues","Galaxies: general","Methods: data analysis","Techniques: image processing","methods: data analysis","techniques: image processing","catalogues","galaxies: general"],"keywords_other":["STAR","ZOO","EXTRACTION","CLASSIFICATION","SAMPLE","ESTIMATING PHOTOMETRIC REDSHIFTS","RECOGNITION","DEPENDENCE","DIGITAL-SKY-SURVEY","FRACTION"],"max_cite":88.0,"pub_year":2015.0,"sources":"['ieee', 'scp', 'wos']","rawkeys":["star","estimating photometric redshifts","recognition","digital-sky-survey","zoo","methods: data analysis","sample","fraction","classification","galaxies: general","dependence","catalogues","extraction","techniques: image processing"],"tags":["estimating photometric redshifts","recognition","stars","zoo","sampling","fractionizing","classification","galaxies: general","methods: data analysis","catalogues","extraction","digital sky survey","techniques: image processing"]},{"p_id":82569,"title":"Driving Maneuver Classification: A Comparison of Feature Extraction Methods","abstract":"Driving maneuver classification has received increasing attention in recent years. Early work focused on car-based sensor systems, but recently the use of smartphone-based sensors has been increasingly favored. For a driving maneuver classification system, feature extraction often plays an important role. Previous studies have proposed various feature extraction methods for classifying driving maneuvers, however, a direct comparison of feature extraction methods using various data sets is missing. In this paper, we systematically compare three window-based feature extraction methods for driving maneuver classification: statistical values and automatically extracted features using principal component analysis and stacked sparse auto-encoders. Specifically, all sensor information from each data set is first segmented into windowed signals after preprocessing. Then, the three feature extraction methods are applied to those windowed signals. Finally, extracted features are fed into a random forest classifier. Maneuver classification performance is evaluated on three different data sets, demonstrating weighted classification F1-scores of 68.56%, 80.87%, and 87.26%. For all three data sets, statistical features achieve the best performance.","keywords_author":["Driving maneuver classification","feature extraction","random forest","Kalman filter"],"keywords_other":["RECOGNITION"],"max_cite":0.0,"pub_year":2018.0,"sources":"['wos']","rawkeys":["recognition","kalman filter","driving maneuver classification","feature extraction","random forest"],"tags":["recognition","random forests","kalman filter","driving maneuver classification","feature extraction"]},{"p_id":656,"title":"Emotion stress detection using EEG signal and deep learning technologies","abstract":"Brainwave reflects the change in electrical potential resulting from the conjunction between the thousands of brain neurons. A neuron can receive signals from other neurons and starts off cyclic discharge reaction when sufficient energy is accumulated. That is also the reason why people persistently emit brainwaves. According to experts from Laboratory of Brain Recognition and Behavior, Michigan University, long-term multitask operation results in the lack of efficiency and in filtering out irrelevant signals leads to the distraction of paying attention of the irrelevant message rather than work-related information. As a result, one would have problems in the transition from one job to another. However, for some people rely on their brain to deal with many things and it may lead to fatigue. Therefore, we did this experiment and tried to figure out the most efficient way to soothe the spiritual pressure and calm the mind down. We utilize deep learning as learning method to predict user's stress feeling through listening to the music. Through above research, by listening to music or create the atmosphere of a music background also with an artistic performance could provide not only psychological treatment effect but also improve the ability of the person to focus.","keywords_author":["Brainwaves","Emotion Stress Detection","EEG","Deep Learning"],"keywords_other":["brainwave","Mathematical model","brain neurons","brain","Music","electrical potential","Laboratory of Brain Recognition and Behavior","work-related information","emotion stress detection","EEG signal","Electroencephalography","Psychology","Machine learning","psychology","Michigan University","learning (artificial intelligence)","Stress","deep learning","medical signal processing","neurophysiology","electroencephalography","Neurons","music background","spiritual pressure","patient treatment","emotion recognition","feature extraction","cyclic discharge reaction","music"],"max_cite":null,"pub_year":2018.0,"sources":"['ieee']","rawkeys":["brainwave","brain neurons","brain","electrical potential","work-related information","emotion stress detection","stress","machine learning","psychology","neurons","mathematical model","eeg signal","learning (artificial intelligence)","deep learning","eeg","medical signal processing","neurophysiology","electroencephalography","music background","laboratory of brain recognition and behavior","spiritual pressure","patient treatment","emotion recognition","brainwaves","feature extraction","cyclic discharge reaction","michigan university","music"],"tags":["brainwave","brain neurons","brain","eeg signals","electrical potential","work-related information","emotion stress detection","stress","machine learning","neurons","mathematical model","recognition","eeg","medical signal processing","neurophysiology","music background","laboratory of brain recognition and behavior","spiritual pressure","patient treatment","emotion recognition","feature extraction","cyclic discharge reaction","michigan university","music"]},{"p_id":66220,"title":"An Approximation of the Error Backpropagation Algorithm in a Predictive Coding Network with Local Hebbian Synaptic Plasticity","abstract":"To efficiently learn from feedback, cortical networks need to update synaptic weights on multiple levels of cortical hierarchy. An effective and well-known algorithm for computing such changes in synaptic weights is the error backpropagation algorithm. However, in this algorithm, the change in synaptic weights is a complex function of weights and activities of neurons not directly connected with the synapse being modified, whereas the changes in biological synapses are determined only by the activity of presynaptic and postsynaptic neurons. Several models have been proposed that approximate the backpropagation algorithm with local synaptic plasticity, but these models require complex external control over the network or relatively complex plasticity rules. Here we show that a network developed in the predictive coding framework can efficiently perform supervised learning fully autonomously, employing only simple local Hebbian plasticity. Furthermore, for certain parameters, the weight change in the predictive coding model converges to that of the backpropagation algorithm. This suggests that it is possible for cortical networks with simple Hebbian synaptic plasticity to implement efficient learning algorithms in which synapses in areas on multiple levels of hierarchy are modified to minimize the error on the output.","keywords_author":null,"keywords_other":["REINFORCEMENT","HIPPOCAMPUS","FEEDFORWARD","NEURAL-NETWORKS","RECOGNITION","MOUSE VISUAL-CORTEX","LEARNING ALGORITHM","FREE-ENERGY","MODELS","EXPECTATIONS"],"max_cite":2.0,"pub_year":2017.0,"sources":"['ieee', 'wos']","rawkeys":["neural-networks","recognition","mouse visual-cortex","feedforward","reinforcement","expectations","models","learning algorithm","hippocampus","free-energy"],"tags":["recognition","model","mouse visual-cortex","neural networks","expectation","learning algorithm","free energy","feed-forward","hippocampus"]},{"p_id":66222,"title":"Spatial features of synaptic adaptation affecting learning performance","abstract":"Recent studies have proposed that the diffusion of messenger molecules, such as monoamines, can mediate the plastic adaptation of synapses in supervised learning of neural networks. Based on these findings we developed a model for neural learning, where the signal for plastic adaptation is assumed to propagate through the extracellular space. We investigate the conditions allowing learning of Boolean rules in a neural network. Even fully excitatory networks show very good learning performances. Moreover, the investigation of the plastic adaptation features optimizing the performance suggests that learning is very sensitive to the extent of the plastic adaptation and the spatial range of synaptic connections.","keywords_author":null,"keywords_other":["TRANSMISSION","REINFORCEMENT","PLASTICITY","ERRORS","VISUAL-CORTEX","NEURAL-NETWORKS","SYNAPSES"],"max_cite":1.0,"pub_year":2017.0,"sources":"['wos']","rawkeys":["neural-networks","errors","synapses","visual-cortex","reinforcement","transmission","plasticity"],"tags":["error","recognition","synapses","visual-cortex","neural networks","transmission","plasticity"]},{"p_id":58030,"title":"Encrypted image classification based on multilayer extreme learning machine","abstract":"Nowadays, numerous corporations (such as Google, Baidu, etc.) require an efficient and effective search algorithm to crawl out the images with queried objects from databases. Moreover, privacy protection is a significant issue such that confidential images must be encrypted in corporations. Nevertheless, decrypting and then classifying millions of encrypted images becomes a heavy burden to computation. In this paper, we proposed an encrypted image classification framework based on multi-layer extreme learning machine that is able to directly classify encrypted images without decryption. Experiments were conducted on popular handwritten digits and letters databases. Results demonstrate that the proposed framework is secure, efficient and accurate for classifying encrypted images.","keywords_author":["Encrypted image classification","Privacy preservation","Multi layer extreme learning machine","ELM auto encoder"],"keywords_other":["CLASSIFIERS","RECOGNITION","NETWORK"],"max_cite":1.0,"pub_year":2017.0,"sources":"['wos']","rawkeys":["network","recognition","classifiers","multi layer extreme learning machine","elm auto encoder","encrypted image classification","privacy preservation"],"tags":["recognition","privacy preserving","elm autoencoder","networks","classifier","multi layer extreme learning machine","encrypted image classification"]},{"p_id":66224,"title":"Reward-based training of recurrent neural networks for cognitive and value-based tasks","abstract":"Trained neural network models, which exhibit many features observed in neural recordings from behaving animals and whose activity and connectivity can be fully analyzed, may provide insights into neural mechanisms. In contrast to commonly used methods for supervised learning from graded error signals, however, animals learn from reward feedback on definite actions through reinforcement learning. Reward maximization is particularly relevant when the optimal behavior depends on an animal's internal judgment of confidence or subjective preferences. Here, we describe reward-based training of recurrent neural networks in which a value network guides learning by using the selected actions and activity of the policy network to predict future reward. We show that such models capture both behavioral and electrophysiological findings from well-known experimental paradigms. Our results provide a unified framework for investigating diverse cognitive and value-based computations, including a role for value representation that is essential for learning, but not executing, a task.","keywords_author":null,"keywords_other":["ORBITOFRONTAL CORTEX","REINFORCEMENT","SPIKING NEURONS","POLICY GRADIENTS","PREFRONTAL CORTEX","DECISION-MAKING","DYNAMIC PERTURBATION","BASAL GANGLIA","GRADIENT ESTIMATION","PARIETAL CORTEX"],"max_cite":8.0,"pub_year":2017.0,"sources":"['wos']","rawkeys":["basal ganglia","decision-making","parietal cortex","prefrontal cortex","reinforcement","spiking neurons","gradient estimation","policy gradients","dynamic perturbation","orbitofrontal cortex"],"tags":["decision making","basal ganglia","recognition","parietal cortex","policy gradient","prefrontal cortex","spiking neurons","gradient estimation","dynamic perturbation","orbitofrontal cortex"]},{"p_id":58038,"title":"Representability of algebraic topology for biomolecules in machine learning based scoring and virtual screening","abstract":"This work introduces a number of algebraic topology approaches, including multi-component persistent homology, multi-level persistent homology, and electrostatic persistence for the representation, characterization, and description of small molecules and biomolecular complexes. In contrast to the conventional persistent homology, multi-component persistent homology retains critical chemical and biological information during the topological simplification of biomolecular geometric complexity. Multi-level persistent homology enables a tailored topological description of inter-and\/ or intra-molecular interactions of interest. Electrostatic persistence incorporates partial charge information into topological invariants. These topological methods are paired with Wasserstein distance to characterize similarities between molecules and are further integrated with a variety of machine learning algorithms, including k-nearest neighbors, ensemble of trees, and deep convolutional neural networks, to manifest their descriptive and predictive powers for protein-ligand binding analysis and virtual screening of small molecules. Extensive numerical experiments involving 4,414 protein- ligand complexes from the PDBBind database and 128,374 ligand-target and decoytarget pairs in the DUD database are performed to test respectively the scoring power and the discriminatory power of the proposed topological learning strategies. It is demonstrated that the present topological learning outperforms other existing methods in protein-ligand binding affinity prediction and ligand-decoy discrimination.","keywords_author":null,"keywords_other":["BINDING-AFFINITY PREDICTION","DOCKING","ACCURACY","TIME-SERIES","PERSISTENT HOMOLOGY","RANDOM FOREST","NEURAL-NETWORKS","RECOGNITION","DRUG-LIKE MOLECULES","SURFACE"],"max_cite":3.0,"pub_year":2018.0,"sources":"['ieee', 'wos']","rawkeys":["accuracy","neural-networks","docking","recognition","time-series","persistent homology","surface","binding-affinity prediction","drug-like molecules","random forest"],"tags":["accuracy","surfaces","docking","recognition","persistent homology","neural networks","random forests","binding-affinity prediction","drug-like molecules","time series"]},{"p_id":17081,"title":"Systems approaches to innovation in crop protection. A systematic literature review","abstract":"The objective of this paper is to explore the extent to which systems approaches to innovation are reflected in the crop protection literature and how such approaches are used. A systematic literature review is conducted to study the relation between crop protection and systems approaches to innovation in 107 publications. The analysis of the crop protection literature demonstrates that only a small fraction is systems-oriented as compared to the bulk of publications with a technology-oriented approach. The analysis of agricultural innovations systems literature shows that, although crop protection is addressed, the potential of this systems approach remains largely unexplored for crop protection innovation. A large share of the publications included in this review focus on cropping or farming 'systems' while 'innovation' often equals the development, transfer, adoption and diffusion of crop protection technologies at farm level. There is relatively little attention for the institutional and political dimensions of crop protection and the interactions between farm, regional and national levels in crop protection systems. The traditional division of roles and responsibilities of researchers as innovators, extension personnel as disseminators, and farmers as end-users, is challenged only to a limited extent. The majority of publications discusses ways to optimise existing features of crop protection systems, without exploring more structural transformations that may be required to enhance the resilience of crop protection systems. \u00a9 2013 Elsevier Ltd.","keywords_author":["Agricultural innovation systems (AIS)","Agricultural knowledge and information systems (AKIS)","Development, transfer, adoption and dissemination or diffusion of technology","Farming systems research (FSR)"],"keywords_other":null,"max_cite":32.0,"pub_year":2014.0,"sources":"['scp']","rawkeys":["transfer","agricultural knowledge and information systems (akis)","agricultural innovation systems (ais)","development","adoption and dissemination or diffusion of technology","farming systems research (fsr)"],"tags":["recognition","agricultural knowledge and information systems (akis)","agricultural innovation systems (ais)","development","adoption and dissemination or diffusion of technology","farming systems research (fsr)"]},{"p_id":58047,"title":"Implementation of multilayer perceptron network with highly uniform passive memristive crossbar circuits","abstract":"The progress in the field of neural computation hinges on the use of hardware more efficient than the conventional microprocessors. Recent works have shown that mixed-signal integrated memristive circuits, especially their passive (OT1R) variety, may increase the neuromorphic network performance dramatically, leaving far behind their digital counterparts. The major obstacle, however, is immature memristor technology so that only limited functionality has been reported. Here we demonstrate operation of one-hidden layer perceptron classifier entirely in the mixed-signal integrated hardware, comprised of two passive 20 x 20 metal-oxide memristive crossbar arrays, board-integrated with discrete conventional components. The demonstrated network, whose hardware complexity is almost 10x higher as compared to previously reported functional classifier circuits based on passive memristive crossbars, achieves classification fidelity within 3% of that obtained in simulations, when using ex-situ training. The successful demonstration was facilitated by improvements in fabrication technology of memristors, specifically by lowering variations in their I-V characteristics.","keywords_author":null,"keywords_other":["DEVICE","SYSTEMS","CLASSIFICATION","NEURAL-NETWORKS","RECOGNITION","ELECTRONIC SYNAPSES","ANALOG","MEMORY"],"max_cite":0.0,"pub_year":2018.0,"sources":"['wos']","rawkeys":["neural-networks","device","recognition","analog","memory","electronic synapses","classification","systems"],"tags":["recognition","memory","devices","neural networks","system","analogy","electronic synapses","classification"]},{"p_id":58048,"title":"Dynamics of Learning in MLP: Natural Gradient and Singularity Revisited","abstract":"The dynamics of supervised learning play a main role in deep learning, which takes place in the parameter space of a multilayer perceptron (MLP). We review the history of supervised stochastic gradient learning, focusing on its singular structure and natural gradient. The parameter space includes singular regions in which parameters are not identifiable. One of our results is a full exploration of the dynamical behaviors of stochastic gradient learning in an elementary singular network. The bad news is its pathological nature, in which part of the singular region becomes an attractor and another part a repulser at the same time, forming a Milnor attractor. A learning trajectory is attracted by the attractor region, staying in it for a long time, before it escapes the singular region through the repulser region. This is typical of plateau phenomena in learning. We demonstrate the strange topology of a singular region by introducing blow-down coordinates, which are useful for analyzing the natural gradient dynamics. We confirm that the natural gradient dynamics are free of critical slowdown. The second main result is the good news: the interactions of elementary singular networks eliminate the attractor part and the Milnor-type attractors disappear. This explainswhy large-scale networks do not suffer from serious critical slowdowns due to singularities. We finally show that the unit-wise natural gradient is effective for learning in spite of its low computational cost.","keywords_author":null,"keywords_other":["RIEMANNIAN METRICS","NEURAL-NETWORKS","RECOGNITION","ALGORITHMS","MULTILAYER PERCEPTRONS"],"max_cite":1.0,"pub_year":2018.0,"sources":"['wos']","rawkeys":["neural-networks","multilayer perceptrons","recognition","riemannian metrics","algorithms"],"tags":["recognition","neural networks","riemannian metrics","multi layer perceptron","algorithms"]},{"p_id":66243,"title":"A deep stochastic weight assignment network and its application to chess playing","abstract":"Chinese chess is an ancient game in which the chess situation is the information ensemble of chess pieces' spatial locations and interrelations. The situation evaluation plays an extremely important role in the policy decisions of Chinese chess game. However, the situation evaluation is too complex for human to cover every detail with naked eyes. The deep stochastic weight assignment network (DSWAN) proposed in this paper to classify the situations in advantages and disadvantages can solve the above problem. DSWAN is a type of multi-layer perception (MLP) which is divided into two main components: unsupervised feature extractor formed by multi-layer auto encoder and supervised classifier trained by stochastic weight assignment network (SWAN). We summarize a series of chess situation features by gathering specialized knowledges of Chinese chess, and these features are proved valid to estimate the situation. Another highlight of this paper is that the auto encoder is constrained by L-1\/2 regularization. By doing so, it can bring more sparsity to data set, make situation features more representative and ease the overfitting trouble of SWAN. (C) 2017 Elsevier Inc. All rights reserved.","keywords_author":["Chinese chess game","Chess situation analysis","Deep stochastic weight assignment network (DSWAN)","Auto encoder","L-1\/2 regularization","Stochastic weight assignment network (SWAN)"],"keywords_other":["FEEDFORWARD NETWORKS","REGULARIZATION","NEURAL-NETWORKS","RECOGNITION","EXTREME LEARNING-MACHINE"],"max_cite":1.0,"pub_year":2017.0,"sources":"['ieee', 'wos']","rawkeys":["neural-networks","recognition","chinese chess game","auto encoder","deep stochastic weight assignment network (dswan)","l-1\/2 regularization","chess situation analysis","stochastic weight assignment network (swan)","feedforward networks","extreme learning-machine","regularization"],"tags":["recognition","neural networks","chinese chess game","auto encoders","feed-forward network","deep stochastic weight assignment network (dswan)","l-1\/2 regularization","chess situation analysis","stochastic weight assignment network (swan)","extreme learning machine","regularization"]},{"p_id":58059,"title":"Continuous Online Sequence Learning with an Unsupervised Neural Network Model","abstract":"The ability to recognize and predict temporal sequences of sensory inputs is vital for survival in natural environments. Based on many known properties of cortical neurons, hierarchical temporal memory (HTM) sequence memory recently has been proposed as a theoretical framework for sequence learning in the cortex. In this letter, we analyze properties of HTM sequence memory and apply it to sequence learning and prediction problems with streaming data. We show the model is able to continuously learn a large number of variable order temporal sequences using an unsupervised Hebbian-like learning rule. The sparse temporal codes formed by the model can robustly handle branching temporal sequences by maintaining multiple predictions until there is sufficient disambiguating evidence. We compare the HTM sequence memory with other sequence learning algorithms, including statistical methodsautoregressive integrated moving average; feedforward neural networkstime delay neural network and online sequential extreme learning machine; and recurrent neural networkslong short-term memory and echo-state networks on sequence prediction problems with both artificial and real-world data. The HTM model achieves comparable accuracy to other state-of-the-art algorithms. The model also exhibits properties that are critical for sequence learning, including continuous online learning, the ability to handle multiple predictions and branching sequences with high-order statistics, robustness to sensor noise and fault tolerance, and good performance without task-specific hyperparameter tuning. Therefore, the HTM sequence memory not only advances our understanding of how the brain may solve the sequence learning problem but is also applicable to real-world sequence learning problems from continuous data streams.","keywords_author":null,"keywords_other":["COMPETITION","MACHINE","SYSTEMS","NEURONS","AUDITORY-CORTEX","TIME-SERIES PREDICTION","DENDRITES","RECOGNITION","DATA STREAMS","PRIMARY VISUAL-CORTEX"],"max_cite":8.0,"pub_year":2016.0,"sources":"['wos']","rawkeys":["dendrites","auditory-cortex","recognition","competition","machine","primary visual-cortex","time-series prediction","systems","neurons","data streams"],"tags":["dendrites","auditory-cortex","recognition","time series prediction","data stream","competition","machine","primary visual-cortex","system","neurons"]},{"p_id":58062,"title":"Seven neurons memorizing sequences of alphabetical images via spike-timing dependent plasticity","abstract":"An artificial neural network, such as a Boltzmann machine, can be trained with the Hebb rule so that it stores static patterns and retrieves a particular pattern when an associated cue is presented to it. Such a network, however, cannot effectively deal with dynamic patterns in the manner of living creatures. Here, we design a dynamic Boltzmann machine (DyBM) and a learning rule that has some of the properties of spike-timing dependent plasticity (STDP), which has been postulated for biological neural networks. We train a DyBM consisting of only seven neurons in a way that it memorizes the sequence of the bitmap patterns in an alphabetical image \"SCIENCE\" and its reverse sequence and retrieves either sequence when a partial sequence is presented as a cue. The DyBM is to STDP as the Boltzmann machine is to the Hebb rule.","keywords_author":null,"keywords_other":["REINFORCEMENT","LEARNING RULE","DENTATE AREA","NEURAL-NETWORKS","PERFORANT PATH","TIME","LONG-LASTING POTENTIATION","SYNAPTIC-TRANSMISSION","SHORT-TERM-MEMORY","RABBIT FOLLOWING STIMULATION"],"max_cite":4.0,"pub_year":2015.0,"sources":"['wos']","rawkeys":["learning rule","neural-networks","synaptic-transmission","short-term-memory","reinforcement","long-lasting potentiation","time","rabbit following stimulation","perforant path","dentate area"],"tags":["recognition","synaptic-transmission","neural networks","short-term-memory","long-lasting potentiation","time","rabbit following stimulation","perforant path","dentate area","learning rules"]},{"p_id":58077,"title":"An Event-Driven Classifier for Spiking Neural Networks Fed with Synthetic or Dynamic Vision Sensor Data","abstract":"This paper introduces a novel methodology for training an event-driven classifier within a Spiking Neural Network (SNN) System capable of yielding good classification results when using both synthetic input data and real data captured from Dynamic Vision Sensor (DVS) chips. The proposed supervised method uses the spiking activity provided by an arbitrary topology of prior SNN layers to build histograms and train the classifier in the frame domain using the stochastic gradient descent algorithm. In addition, this approach can cope with leaky integrate-and-fire neuron models within the SNN, a desirable feature for real-world SNN applications, where neural activation must fade away after some time in the absence of inputs. Consequently, this way of building histograms captures the dynamics of spikes immediately before the classifier. We tested our method on the MNIST data set using different synthetic encodings and real DVS sensory data sets such as N-MNIST, MNIST-DVS, and Poker-DVS using the same network topology and feature maps. We demonstrate the effectiveness of our approach by achieving the highest classification accuracy reported on the N-MNIST (97.77%) and Poker-DVS (100%) real DVS data sets to date with a spiking convolutional network. Moreover, by using the proposed method we were able to retrain the output layer of a previously reported spiking neural network and increase its performance by 2%, suggesting that the proposed classifier can be used as the output layer in works where features are extracted using unsupervised spike-based learning methods. In addition, we also analyze SNN performance figures such as total event activity and network latencies, which are relevant for eventual hardware implementations. In summary, the paper aggregates unsupervised-trained SNNs with a supervised-trained SNN classifier, combining and applying them to heterogeneous sets of benchmarks, both synthetic and from real DVS chips.","keywords_author":["spiking neural networks","supervised learning","event driven processing","DVS sensors","convolutional neural networks","fully connected neural networks","neuromorphic"],"keywords_other":["SYSTEMS","LARGE-SCALE MODEL","RECOGNITION","CONVNETS","SPINNAKER"],"max_cite":4.0,"pub_year":2017.0,"sources":"['ieee', 'wos']","rawkeys":["large-scale model","supervised learning","recognition","convolutional neural networks","spiking neural networks","fully connected neural networks","dvs sensors","spinnaker","event driven processing","convnets","neuromorphic","systems"],"tags":["large-scale model","supervised learning","recognition","spiking neural networks","spinnaker","dvs sensors","fully connected neural network","event driven processing","system","neuromorphic","convolutional neural network"]},{"p_id":33501,"title":"Face Verification via Class Sparsity Based Supervised Encoding","abstract":"\u00a9 2016 IEEE. Autoencoders are deep learning architectures that learn feature representation by minimizing the reconstruction error. Using an autoencoder as baseline, this paper presents a novel formulation for a class sparsity based supervised encoder, termed as CSSE. We postulate that features from the same class will have a common sparsity pattern\/support in the latent space. Therefore, in the formulation of the autoencoder, a supervision penalty is introduced as a joint-sparsity promoting l2,1-norm. The formulation of CSSE is derived for a single hidden layer and it is applied for multiple hidden layers using a greedy layer-by-layer learning approach. The proposed CSSE approach is applied for learning face representation and verification experiments are performed on the LFW and PaSC face databases. The experiments show that the proposed approach yields improved results compared to autoencoders and comparable results with state-of-the-art face recognition algorithms.","keywords_author":["autoencoders","deep learning","Face verification","supervised feature learning","Face verification","deep learning","supervised feature learning","autoencoders"],"keywords_other":["Face recognition algorithms","REPRESENTATION","AUTOENCODERS","Face representations","SYSTEMS","Learning architectures","Feature representation","RECOGNITION","ALGORITHMS","RECOVERY","Feature learning","Reconstruction error","Face Verification","Autoencoders"],"max_cite":2.0,"pub_year":2017.0,"sources":"['ieee', 'scp', 'wos']","rawkeys":["learning architectures","recognition","systems","deep learning","face recognition algorithms","representation","recovery","face verification","autoencoders","supervised feature learning","algorithms","feature learning","feature representation","reconstruction error","face representations"],"tags":["learning architectures","recognition","face recognition algorithms","auto encoders","machine learning","recovery","representation","face verification","system","supervised feature learning","algorithms","feature learning","feature representation","reconstruction error","face representations"]},{"p_id":58090,"title":"Deep Belief Networks for Quantitative Analysis of a Gold Immunochromatographic Strip","abstract":"Gold immunochromatographic strip (GICS) has become a popular membrane-based diagnostic tool in a variety of settings due to its sensitivity, simplicity and rapidness. This paper aimed to develop a framework of automatic image inspection to further improve the sensitivity as well as the quantitative performance of the GICS systems. As one of the latest methodologies in machine learning, the deep belief network (DBN) is applied, for the first time, to quantitative analysis of GICS images with hope to segment the test and control lines with a high accuracy. It is remarkable that the exploited DBN is capable of simultaneously learning three proposed features including intensity, distance and difference to distinguish the test and control lines from the region of interest that are obtained by preprocessing the GICS images. Several indices are proposed to evaluate the proposed method. The experiment results show the feasibility and effectiveness of the DBN in the sense that it provides a robust image processing methodology for quantitative analysis of GICS.","keywords_author":["Gold immunochromatographic strip","Deep belief networks (DBNs)","Restricted Boltzmann machine (RBM)","Quantitative analysis","Image segmentation"],"keywords_other":["LATERAL FLOW IMMUNOASSAY","ALGORITHM","MODEL","NEURAL-NETWORKS","RECOGNITION","STOCHASTIC-SYSTEMS","DELAYS","PSO","ASSAY","FORMAT"],"max_cite":37.0,"pub_year":2016.0,"sources":"['ieee', 'wos']","rawkeys":["neural-networks","algorithm","pso","recognition","format","model","gold immunochromatographic strip","delays","stochastic-systems","restricted boltzmann machine (rbm)","deep belief networks (dbns)","assay","lateral flow immunoassay","quantitative analysis","image segmentation"],"tags":["recognition","format","model","gold immunochromatographic strip","neural networks","stochastic systems","delays","assay","algorithms","restricted boltzmann machine","lateral flow immunoassay","deep belief networks","quantitative analysis","image segmentation","particle swarm optimization"]},{"p_id":17152,"title":"Improving tRNAscan-SE Annotation Results via Ensemble Classifiers","abstract":"\u00a9 2015 WILEY-VCH Verlag GmbH & Co. KGaA, Weinheim. tRNAScan-SE is a tRNA detection program that is widely used for tRNA annotation; however, the false positive rate of tRNAScan-SE is unacceptable for large sequences. Here, we used a machine learning method to try to improve the tRNAScan-SE results. A new predictor, tRNA-Predict, was designed. We obtained real and pseudo-tRNA sequences as training data sets using tRNAScan-SE and constructed three different tRNA feature sets. We then set up an ensemble classifier, LibMutil, to predict tRNAs from the training data. The positive data set of 623 tRNA sequences was obtained from tRNAdb 2009 and the negative data set was the false positive tRNAs predicted by tRNAscan-SE. Our in silico experiments revealed a prediction accuracy rate of 95.1 % for tRNA-Predict using 10-fold cross-validation. tRNA-Predict was developed to distinguish functional tRNAs from pseudo-tRNAs rather than to predict tRNAs from a genome-wide scan. However, tRNA-Predict can work with the output of tRNAscan-SE, which is a genome-wide scanning method, to improve the tRNAscan-SE annotation results. The tRNA-Predict web server is accessible at http:\/\/datamining.xmu.edu.cn\/\u223cgjs\/tRNA-Predict.","keywords_author":["annotation","ensemble classifier","machine learning","tRNA","tRNAscan-SE"],"keywords_other":["Molecular Sequence Annotation","RNA, Transfer","Databases, Nucleic Acid","Sequence Analysis, DNA","Software"],"max_cite":31.0,"pub_year":2015.0,"sources":"['scp']","rawkeys":["transfer","ensemble classifier","databases","machine learning","dna","rna","trna","trnascan-se","software","annotation","sequence analysis","nucleic acid","molecular sequence annotation"],"tags":["recognition","databases","dna","machine learning","rna","trna","trnascan-se","software","ensemble classifiers","annotation","sequence analysis","nucleic acid","molecular sequence annotation"]},{"p_id":58126,"title":"In Defense of Locality-Sensitive Hashing","abstract":"Hashing-based semantic similarity search is becoming increasingly important for building large-scale content-based retrieval system. The state-of-the-art supervised hashing techniques use flexible two-step strategy to learn hash functions. The first step learns binary codes for training data by solving binary optimization problems with millions of variables, thus usually requiring intensive computations. Despite simplicity and efficiency, locality-sensitive hashing (LSH) has never been recognized as a good way to generate such codes due to its poor performance in traditional approximate neighbor search. We claim in this paper that the true merit of LSH lies in transforming the semantic labels to obtain the binary codes, resulting in an effective and efficient two-step hashing framework. Specifically, we developed the locality-sensitive two-step hashing (LS-TSH) that generates the binary codes through LSH rather than any complex optimization technique. Theoretically, with proper assumption, LS-TSH is actually a useful LSH scheme, so that it preserves the label-based semantic similarity and possesses sublinear query complexity for hash lookup. Experimentally, LS-TSH could obtain comparable retrieval accuracy with state of the arts with two to three orders of magnitudes faster training speed.","keywords_author":["Locality-sensitive hashing (LSH)","semantic similarity search","two-step hashing"],"keywords_other":["KERNEL METHODS","MULTICLASS","REGRESSION","REPRESENTATION","QUANTIZATION","CLASSIFICATION","CODES","NEAREST-NEIGHBOR","RECOGNITION","IMAGE RETRIEVAL"],"max_cite":0.0,"pub_year":2018.0,"sources":"['ieee', 'wos']","rawkeys":["locality-sensitive hashing (lsh)","nearest-neighbor","recognition","multiclass","codes","image retrieval","representation","classification","semantic similarity search","kernel methods","quantization","regression","two-step hashing"],"tags":["signals","multi-class","recognition","neural networks","codes","image retrieval","representation","locality sensitive hashing","classification","semantic similarity search","kernel methods","regression","two-step hashing"]},{"p_id":58129,"title":"Classifying Discriminative Features for Blur Detection","abstract":"Blur detection in a single image is challenging especially when the blur is spatially-varying. Developing discriminative blur features is an open problem. In this paper, we propose a new kernel-specific feature vector consisting of the information of a blur kernel and the information of an image patch. Specifically, the kernel specific-feature is composed of the multiplication of the variance of filtered kernel and the variance of filtered patch gradients. The feature origins from a blur-classification theorem and its discrimination can also be intuitively explained. To make the kernel-specific features useful for real applications, we build a pool of kernels consisting of motion-blur kernels, defocus-blur (out-of-focus) kernels, and their combinations. By extracting such features followed by the classifiers, the proposed algorithm outperforms the state-of-the-art blur detection method. Experimental results on public databases demonstrate the effectiveness of the proposed method.","keywords_author":["Blur detection","classifier","feature extraction","motion blur","support vector machine (SVM)"],"keywords_other":["SVM","SPACE","RECOGNITION","SURVEILLANCE","OBJECT DETECTION","SINGLE IMAGE"],"max_cite":9.0,"pub_year":2016.0,"sources":"['ieee', 'wos']","rawkeys":["single image","motion blur","recognition","space","support vector machine (svm)","svm","blur detection","classifier","object detection","feature extraction","surveillance"],"tags":["single images","motion blur","recognition","space","machine learning","blur detection","classifier","object detection","feature extraction","surveillance"]},{"p_id":789,"title":"An Uphill Safety Controller with Deep Learning-Based Ramp Detection for Intelligent Wheelchairs","abstract":"In a society with aging population, the demand for electric wheelchairs is growing with the advancement of automation. However, many accidents have occurred due to the misjudgment of the slope angle and wheelchair speed while the wheelchair is traveling on ramps. This research employs the light electronic assistance pal compact motor package to reduce the weight and size of conventional electric wheelchairs. The modular design of proposed uphill controller and ramp detection functions allows users to easily select and incorporate only the functions they need. This paper proposes a ramp detection model implemented using the deep learning algorithm with CNN-4 structure to analyze depth image data. The model's recognition time of each video frame is 11 times faster than that of the AlexNet and GoogleNet. The uphill safety controller is designed as an adaptive network-based fuzzy inference system with Q-learning. The safe speed is automatically calculated according to the angle obtained from slope classification and revised in real-time during the slope driving to prevent the user from moving towards the dangerous ramp or rolling back due to inadequate speed. The accuracy of ramp detection is further increased by 5% to 97.1% due to assistance from the voting system processing and the gyroscope output data. The 5\u00b0 ramp experiment of our uphill controller with ramp classification takes 20 s to complete the slope driving which is 23% faster than the controller without ramp detection. The energy consumption is also one half less than the experiment without uphill detection.","keywords_author":["adaptive network-based fuzzy inference system (ANFIS)","Command and control systems","deep learning","intelligent wheelchair","learning","Q-learning","ramp classification","Command and control systems","learning","intelligent wheelchair","deep learning","adaptive network-based fuzzy inference system (ANFIS)","Q-learning","ramp classification","Command and control systems","learning","intelligent wheelchair","deep learning","adaptive network-based fuzzy inference system (ANFIS)","Q-learning","ramp classification"],"keywords_other":["Safety controller","Learning","energy consumption","ramp classification","light electronic assistance pal compact motor package","INFERENCE","Gyroscopes","conventional electric wheelchairs","Adaptive network based fuzzy inference system","TRACKING","uphill detection","depth image data","Electric wheelchair","slope classification","power engineering computing","Cameras","Machine learning","real-time systems","Q-learning","object detection","Intelligent wheelchair","adaptive network-based fuzzy inference system","Safety","deep learning algorithm","learning (artificial intelligence)","Detection functions","fuzzy reasoning","Training","real-time","Databases","Wheelchairs","RECOGNITION","FUZZY CONTROLLER","CNN-4 structure","electric vehicles","time 20.0 s","handicapped aids","AlexNet","control engineering computing","wheelchair speed","slope driving","ENVIRONMENTS","wheelchairs","ramp detection model","Electronic assistance","uphill safety controller","voting system processing","slope angle","GoogleNet"],"max_cite":0.0,"pub_year":2018.0,"sources":"['scp', 'wos', 'ieee']","rawkeys":["energy consumption","databases","ramp classification","intelligent wheelchair","q-learning","cnn-4 structure","light electronic assistance pal compact motor package","adaptive network based fuzzy inference system","conventional electric wheelchairs","detection functions","googlenet","uphill detection","alexnet","depth image data","power engineering computing","slope classification","machine learning","safety","cameras","environments","real-time systems","object detection","inference","adaptive network-based fuzzy inference system","deep learning algorithm","adaptive network-based fuzzy inference system (anfis)","learning (artificial intelligence)","recognition","fuzzy reasoning","deep learning","real-time","training","learning","fuzzy controller","electric vehicles","handicapped aids","time 20.0 s","safety controller","control engineering computing","command and control systems","slope driving","gyroscopes","wheelchair speed","tracking","wheelchairs","ramp detection model","uphill safety controller","voting system processing","slope angle","electronic assistance","electric wheelchair"],"tags":["energy consumption","databases","ramp classification","intelligent wheelchair","q-learning","cnn-4 structure","light electronic assistance pal compact motor package","environment","fuzzy control","conventional electric wheelchairs","adaptive neuro-fuzzy inference system","detection functions","googlenet","uphill detection","alexnet","depth image data","power engineering computing","slope classification","machine learning","safety","cameras","real-time systems","face recognition","inference","object detection","gyroscope","deep learning algorithm","recognition","training","slope angles","electric vehicles","handicapped aids","time 20.0 s","control engineering computing","command and control systems","slope driving","wheelchair speed","real time","tracking","wheelchairs","ramp detection model","uphill safety controller","voting system processing","electronic assistance","electric wheelchair","safety controls"]},{"p_id":82710,"title":"Feature Weight Driven Interactive Mutual Information Modeling for Heterogeneous Bio-Signal Fusion to Estimate Mental Workload","abstract":"Many people suffer from high mental workload which may threaten human health and cause serious accidents. Mental workload estimation is especially important for particular people such as pilots, soldiers, crew and surgeons to guarantee the safety and security. Different physiological signals have been used to estimate mental workload based on the n-back task which is capable of inducing different mental workload levels. This paper explores a feature weight driven signal fusion method and proposes interactive mutual information modeling (IMIM) to increase the mental workload classification accuracy. We used EEG and ECG signals to validate the effectiveness of the proposed method for heterogeneous bio-signal fusion. The experiment of mental workload estimation consisted of signal recording, artifact removal, feature extraction, feature weight calculation, and classification. Ten subjects were invited to take part in easy, medium and hard tasks for the collection of EEG and ECG signals in different mental workload levels. Therefore, heterogeneous physiological signals of different mental workload states were available for classification. Experiments reveal that ECG can be utilized as a supplement of EEG to optimize the fusion model and improve mental workload estimation. Classification results show that the proposed bio-signal fusion method IMIM can increase the classification accuracy in both feature level and classifier level fusion. This study indicates that multi-modal signal fusion is promising to identify the mental workload levels and the fusion strategy has potential application of mental workload estimation in cognitive activities during daily life.","keywords_author":["mental workload","signal fusion","n-back task","mutual information","heterogeneous bio-signals"],"keywords_other":["EEG","SENSOR NETWORKS","VARIABILITY","FATIGUE","COGNITIVE WORKLOAD","FEATURE-SELECTION","IDENTIFICATION","RECOGNITION","TIME","FUNCTIONAL-STATE CLASSIFICATION"],"max_cite":0.0,"pub_year":2017.0,"sources":"['wos']","rawkeys":["heterogeneous bio-signals","mental workload","identification","recognition","n-back task","mutual information","variability","eeg","time","feature-selection","cognitive workload","fatigue","functional-state classification","sensor networks","signal fusion"],"tags":["heterogeneous bio-signals","mental workload","identification","recognition","signal fusion","n-back task","mutual information","variability","eeg","time","feature selection","fatigue","functional-state classification","sensor networks","cognitive workloads"]},{"p_id":58141,"title":"Stereoscopic image quality assessment method based on binocular combination saliency model","abstract":"The objective quality assessment of stereoscopic images plays an important role in three-dimensional (3D) technologies. In this paper, we propose an effective method to evaluate the quality of stereoscopic images that are afflicted by symmetric distortions. The major technical contribution of this paper is that the binocular combination behaviors and human 3D visual saliency characteristics are both considered. In particular, a new 3D saliency map is developed, which not only greatly reduces the computational complexity by avoiding calculation of the depth information, but also assigns appropriate weights to the image contents. Experimental results indicate that the proposed metric not only significantly outperforms conventional 2D quality metrics, but also achieves higher performance than the existing 3D quality assessment models. (C) 2016 Elsevier B.V. All rights reserved.","keywords_author":["Binocular vision","Visual attention","Image quality","Human visual system"],"keywords_other":["RIVALRY","INFORMATION","DEPENDS","ENERGY","SYSTEM","FREQUENCY","INTEGRATION","SUBJECTIVE EVALUATION","STATISTICS","SIMILARITY"],"max_cite":25.0,"pub_year":2016.0,"sources":"['ieee', 'wos']","rawkeys":["statistics","integration","depends","similarity","frequency","system","energy","human visual system","information","image quality","subjective evaluation","visual attention","binocular vision","rivalry"],"tags":["statistics","integration","recognition","similarity","frequency","system","energy","human visual system","information","image quality","subjective evaluations","visual attention","binocular vision","rivalry"]},{"p_id":82737,"title":"Multi-Modal Detection and Mapping of Static and Dynamic Obstacles in Agriculture for Process Evaluation","abstract":"Today, agricultural vehicles are available that can automatically perform tasks such as weed detection and spraying, mowing, and sowing while being steered automatically. However, for such systems to be fully autonomous and self-driven, not only their specific agricultural tasks must be automated. An accurate and robust perception system automatically detecting and avoiding all obstacles must also be realized to ensure safety of humans, animals, and other surroundings. In this paper, we present a multi-modal obstacle and environment detection and recognition approach for process evaluation in agricultural fields. The proposed pipeline detects and maps static and dynamic obstacles globally, while providing process-relevant information along the traversed trajectory. Detection algorithms are introduced for a variety of sensor technologies, including range sensors (lidar and radar) and cameras (stereo and thermal). Detection information is mapped globally into semantical occupancy grid maps and fused across all sensors with late fusion, resulting in accurate traversability assessment and semantical mapping of process-relevant categories (e.g., crop, ground, and obstacles). Finally, a decoding step uses a Hidden Markov model to extract relevant process-specific parameters along the trajectory of the vehicle, thus informing a potential control system of unexpected structures in the planned path. The method is evaluated on a public dataset for multi-modal obstacle detection in agricultural fields. Results show that a combination of multiple sensor modalities increases detection performance and that different fusion strategies must be applied between algorithms detecting similar and dissimilar classes.","keywords_author":["occupancy grid maps","mapping and localization","obstacle detection","precision agriculture","sensor fusion","multi-modal perception","inverse sensor models","process evaluation"],"keywords_other":["STEREO VISION","ROBOT","OPERATIONS","SYSTEM","RECOGNITION","NAVIGATION","VEHICLES","TERRAIN CLASSIFICATION"],"max_cite":0.0,"pub_year":2018.0,"sources":"['wos']","rawkeys":["precision agriculture","recognition","operations","navigation","robot","process evaluation","mapping and localization","stereo vision","system","occupancy grid maps","inverse sensor models","terrain classification","sensor fusion","vehicles","obstacle detection","multi-modal perception"],"tags":["precision agriculture","recognition","operations","navigation","process evaluation","mapping and localization","occupancy grid map","stereo vision","system","robotics","terrain classification","inverse sensor models","vehicles","sensor fusion","obstacle detection","multi-modal perception"]},{"p_id":865,"title":"A Survey of Deep Learning: Platforms, Applications and Emerging Research Trends","abstract":"Deep learning has exploded in the public consciousness, primarily as predictive and analytical products suffuse our world, in the form of numerous human-centered smart-world systems, including targeted advertisements, natural language assistants and interpreters, and prototype self-driving vehicle systems. Yet to most, the underlying mechanisms that enable such human-centered smart products remain obscure. In contrast, researchers across disciplines have been incorporating deep learning into their research to solve problems that could not have been approached before. In this paper, we seek to provide a thorough investigation of deep learning in its applications and mechanisms. Specifically, as a categorical collection of state of the art in deep learning research, we hope to provide a broad reference for those seeking a primer on deep learning and its various implementations, platforms, algorithms, and uses in a variety of smart-world systems. Furthermore, we hope to outline recent key advancements in the technology, and provide insight into areas, in which deep learning can improve investigation, as well as highlight new areas of research that have yet to see the application of deep learning, but could nonetheless benefit immensely. We hope this survey provides a valuable reference for new deep learning practitioners, as well as those seeking to innovate in the application of deep learning.","keywords_author":["cyber-physical systems","deep learning","emergent applications","Human-centered smart systems","Internet of Things","networking","neural networks","platform","security","survey","Human-centered smart systems","deep learning","platform","neural networks","emergent applications","Internet of Things","cyber-physical systems","survey","networking","security","Human-centered smart systems","deep learning","platform","neural networks","emergent applications","Internet of Things","cyber-physical systems","survey","networking","security"],"keywords_other":["GO","ALGORITHM","Neural networks","GAME","SENSOR DATA","Computational modeling","Smart System","SYSTEMS","Machine learning","Networking and Security","INTERNET","THINGS","human-centered smart-world systems","learning (artificial intelligence)","Training","Learning systems","RECOGNITION","Task analysis","Neurons","Computational model","deep learning research","CLASSIFICATION","NEURAL-NETWORKS","Platform"],"max_cite":0.0,"pub_year":2018.0,"sources":"['scp', 'wos', 'ieee']","rawkeys":["task analysis","smart system","networking","internet of things","classification","learning systems","internet","neural-networks","game","sensor data","survey","things","machine learning","neurons","computational modeling","algorithm","human-centered smart-world systems","learning (artificial intelligence)","recognition","emergent applications","deep learning","neural networks","training","human-centered smart systems","security","platform","computational model","cyber-physical systems","networking and security","go","deep learning research","systems"],"tags":["task analysis","smart system","emerging applications","classification","learning systems","internet","internet of things (iot)","sensor data","survey","things","machine learning","system","algorithms","neurons","computational modeling","human-centered smart-world systems","recognition","neural networks","training","human-centered smart systems","security","platform","networks","games","cyber-physical systems","networking and security","go","deep learning research"]},{"p_id":922,"title":"A greedy deep learning method for medical disease analysis","abstract":"This paper proposes a new deep learning method, the greedy deep weighted dictionary learning for mobile multimedia for medical diseases analysis. Based on the traditional dictionary learning methods, which neglects the relationship between the sample and the dictionary atom, we propose the weighted mechanism to connect the sample with the dictionary atom in this paper. Meanwhile, the traditional dictionary learning method is prone to cause over-fitting for patient classification of the limited training data set. Therefore, this paper adopts l 2 -norm regularization constraint, which realizes the limitation of the model space, and enhances the generalization ability of the model and avoids over-fitting to some extent. Compared with the previous shallow dictionary learning, this paper proposed the greedy deep dictionary learning. We adopt the thinking of layer by layer training to increase the hidden layer, so that the local information between the layer and the layer can be trained to maintain their own characteristics, reduce the risk of overfitting and make sure that each layer of the network is convergent, which improves the accuracy of training and learning. With the development of Internet of Things and the soundness of healthcare monitoring system, the method proposed have better reliability in the field of mobile multimedia for healthcare. The results show that the learning method has a good effect on the classification of mobile multimedia for medical diseases, and the accuracy, sensitivity, and specificity of the classification have good performance, which may provide guidance for the diagnosis of disease in wisdom medical.","keywords_author":["Deep learning","Dictionary learning","Machine learning","Medical big data","Mobile multimedia","Patient classification","Medical big data","machine learning","mobile multimedia","deep learning","dictionary learning","patient classification","Medical big data","machine learning","mobile multimedia","deep learning","dictionary learning","patient classification"],"keywords_other":["health care","greedy deep weighted dictionary","ALZHEIMERS-DISEASE","dictionary atom","Medical diagnostic imaging","disease diagnosis","healthcare","mobile multimedia","medical diagnostic computing","FEATURE-SELECTION","Dictionaries","mobile computing","medical diseases analysis","Machine learning","l2-norm regularization constraint","greedy deep dictionary learning","traditional dictionary learning method","learning (artificial intelligence)","Training","greedy algorithms","Big Data","multimedia systems","medical disease analysis","RECOGNITION","DICTIONARY","Internet of Things","Generalization ability","pattern classification","Mobile multimedia","Healthcare monitoring systems","Dictionary learning","Medical services","shallow dictionary learning","patient classification","CLASSIFICATION","Sensitivity and specificity","SPARSE REPRESENTATION","greedy deep learning method","Internet of Things (IOT)","diseases"],"max_cite":0.0,"pub_year":2018.0,"sources":"['scp', 'wos', 'ieee']","rawkeys":["health care","greedy deep weighted dictionary","big data","healthcare monitoring systems","internet of things","feature-selection","dictionary","classification","medical services","internet of things (iot)","dictionary atom","disease diagnosis","healthcare","medical diagnostic computing","mobile multimedia","machine learning","medical diagnostic imaging","medical big data","medical diseases analysis","mobile computing","l2-norm regularization constraint","greedy deep dictionary learning","sensitivity and specificity","traditional dictionary learning method","learning (artificial intelligence)","recognition","deep learning","training","greedy algorithms","sparse representation","multimedia systems","medical disease analysis","dictionary learning","pattern classification","dictionaries","generalization ability","alzheimers-disease","shallow dictionary learning","patient classification","greedy deep learning method","diseases"],"tags":["health care","greedy deep weighted dictionary","big data","healthcare monitoring systems","classification","medical services","dictionary atoms","internet of things (iot)","healthcare","medical diagnostic computing","mobile multimedia","machine learning","medical diagnostic imaging","medical big data","mobile computing","feature selection","greedy deep learning method","l2-norm regularization constraint","greedy deep dictionary learning","sensitivity and specificity","traditional dictionary learning method","recognition","training","disease","greedy algorithms","sparse representation","multimedia systems","medical disease analysis","dictionary learning","pattern classification","dictionaries","generalization ability","alzheimers-disease","shallow dictionary learning","patient classification","disease diagnosis"]},{"p_id":927,"title":"Intelligent Map Reader: A Framework for Topographic Map Understanding With Deep Learning and Gazetteer","abstract":"Text features in topographic maps are important for helping users to locate the area that a map covers and to understand the map's content. Previous works on the optical detection of map text from topographic maps have used geometric features, the Hough transform, and segmentation. However, these approaches still face challenges when detecting map text in complicated contexts, especially when the map text is touching other map features, such as contours or geographical features. Thus, state-of-the-art techniques for map text and feature recognition and manual interpretation and correction are always required to produce accurate results when optically converting topographic maps into a readable format. This paper proposes a methodological framework called the intelligent map reader that enables the automatic and accurate optical understanding of the content of a topographic map using deep learning techniques in combination with a gazetteer. The intelligent map reader framework includes the detection of map text via deep learning, the separation of text units via graph-based segmentation and clustering, optical character recognition (OCR) via an OCR engine, and digital-gazetteer-based map content understanding. Experimental results validate the efficiency and robustness of our proposed methodology for map text recognition and map content understanding. We expect the proposed intelligent map reader to contribute to various applications in the GeoAI field.","keywords_author":["Optical character recognition","deep convolutional neural network","map feature detection","gazetteer","topographic map understanding","Optical character recognition","deep convolutional neural network","map feature detection","gazetteer","topographic map understanding"],"keywords_other":["TEXT","map text recognition","Google","Text recognition","geographic information systems","PAPER MAPS","OCR engine","Feature extraction","topographic map understanding","text features","ENGINEERING DRAWINGS","deep learning techniques","Machine learning","feature recognition","intelligent map reader framework","Proposals","THE-NATIONAL-MAP","optical character recognition","learning (artificial intelligence)","FEATURES","Training","Hough transform","RECOGNITION","Hough transforms","Optical character recognition software","geographical features","SEMANTIC WEB","cartography","image segmentation","digital-gazetteer-based map content understanding","geometric features","EXTRACTION","LINE","SEGMENTATION","text analysis","graph-based segmentation","feature extraction","map features","optical detection"],"max_cite":0.0,"pub_year":2018.0,"sources":"['ieee', 'wos']","rawkeys":["optical detection","hough transform","map text recognition","text","semantic web","extraction","google","segmentation","features","topographic map understanding","text features","machine learning","deep learning techniques","feature recognition","intelligent map reader framework","proposals","optical character recognition","engineering drawings","learning (artificial intelligence)","ocr engine","line","recognition","training","deep convolutional neural network","text recognition","hough transforms","geographical features","image segmentation","cartography","digital-gazetteer-based map content understanding","the-national-map","geometric features","optical character recognition software","gazetteer","text analysis","paper maps","feature extraction","graph-based segmentation","map features","geographic information systems","map feature detection"],"tags":["ocr engines","hough transform","map text recognition","text","convolutional neural network","semantic web","extraction","google","segmentation","features","topographic map understanding","engineering drawing","machine learning","deep learning techniques","feature recognition","intelligent map reader framework","proposals","optical character recognition","gis","recognition","line","training","geometric feature","text feature","text recognition","geographical features","image segmentation","cartography","digital-gazetteer-based map content understanding","the-national-map","optical character recognition software","gazetteer","text analysis","paper maps","feature extraction","graph-based segmentation","map features","optical detection","map feature detection"]},{"p_id":82863,"title":"A deep boosting based approach for capturing the sequence binding preferences of RNA-binding proteins from high-throughput CLIP-seq data","abstract":"Characterizing the binding behaviors of RNA-binding proteins (RBPs) is important for understanding their functional roles in gene expression regulation. However, current high-throughput experimental methods for identifying RBP targets, such as CLIP-seq and RNAcompete, usually suffer from the false negative issue. Here, we develop a deep boosting based machine learning approach, called DeBooster, to accurately model the binding sequence preferences and identify the corresponding binding targets of RBPs from CLIP-seq data. Comprehensive validation tests have shown that DeBooster can outperform other state-of-the-art approaches in RBP target prediction. In addition, we have demonstrated that DeBooster may provide new insights into understanding the regulatory functions of RBPs, including the binding effects of the RNA helicase MOV10 on mRNA degradation, the potentially different ADAR1 binding behaviors related to its editing activity, as well as the antagonizing effect of RBP binding on miRNA repression. Moreover, DeBooster may provide an effective index to investigate the effect of pathogenic mutations in RBP binding sites, especially those related to splicing events. We expect that DeBooster will be widely applied to analyze large-scale CLIP-seq experimental data and can provide a practically useful tool for novel biological discoveries in understanding the regulatory mechanisms of RBPs. The source code of DeBooster can be downloaded from http:\/\/github.com\/dongfanghong\/deepboost.","keywords_author":null,"keywords_other":["WIDE ANALYSIS","MOTIF","CARCINOMA","IDENTIFICATION","MESSENGER-RNAS","INSIGHTS","RECOGNITION","SPECIFICITIES","CANCER","GENE-EXPRESSION"],"max_cite":0.0,"pub_year":2017.0,"sources":"['wos']","rawkeys":["specificities","insights","carcinoma","identification","recognition","cancer","messenger-rnas","motif","gene-expression","wide analysis"],"tags":["insights","carcinoma","identification","recognition","cancer","specificity","messenger-rnas","motifs","wide analysis","gene expression"]},{"p_id":82866,"title":"Template-Based Modeling of Protein-RNA Interactions","abstract":"Protein-RNA complexes formed by specific recognition between RNA and RNA-binding proteins play an important role in biological processes. More than a thousand of such proteins in human are curated and many novel RNA-binding proteins are to be discovered. Due to limitations of experimental approaches, computational techniques are needed for characterization of protein-RNA interactions. Although much progress has been made, adequate methodologies reliably providing atomic resolution structural details are still lacking. Although protein-RNA free docking approaches proved to be useful, in general, the template-based approaches provide higher quality of predictions. Templates are key to building a high quality model. Sequence\/structure relationships were studied based on a representative set of binary protein-RNA complexes from PDB. Several approaches were tested for pairwise target\/template alignment. The analysis revealed a transition point between random and correct binding modes. The results showed that structural alignment is better than sequence alignment in identifying good templates, suitable for generating protein-RNA complexes close to the native structure, and outperforms free docking, successfully predicting complexes where the free docking fails, including cases of significant conformational change upon binding. A template-based protein-RNA interaction modeling protocol PRIME was developed and benchmarked on a representative set of complexes.","keywords_author":null,"keywords_other":["STRUCTURE ALIGNMENT","DOCKING","SITES","BINDING PROTEINS","SEQUENCE INFORMATION","RECOGNITION","LONG NONCODING RNAS","COMPLEXES","STRUCTURAL INFORMATION","STRUCTURE PREDICTION"],"max_cite":0.0,"pub_year":2016.0,"sources":"['wos']","rawkeys":["long noncoding rnas","structure prediction","recognition","docking","sequence information","sites","structural information","structure alignment","complexes","binding proteins"],"tags":["long noncoding rnas","structure prediction","recognition","docking","sequence informations","complexity","structural information","structural alignment","sites","binding proteins"]},{"p_id":949,"title":"Research on Deep Learning Techniques in Breaking Text-Based Captchas and Designing Image-Based Captcha","abstract":"The ability of hackers to infiltrate computer systems using computer attack programs and bots led to the development of Captchas or Completely Automated Public Turing Tests to Tell Computers and Humans Apart. The text Captcha is the most popular Captcha scheme given its ease of construction and user friendliness. However, the next generation of hackers and programmers has decreased the expected security of these mechanisms, leaving websites open to attack. Text Captchas are still widely used, because it is believed that the attack speeds are slow, typically two to five seconds per image, and this is not seen as a critical threat. In this paper, we introduce a simple, generic, and fast attack on text Captchas that effectively challenges that supposition. With deep learning techniques, our attack demonstrates a high success rate in breaking the Roman-character-based text Captchas deployed by the top 50 most popular international websites and three Chinese Captchas that use a larger character set. These targeted schemes cover almost all existing resistance mechanisms, demonstrating that our attack techniques are also applicable to other existing Captchas. Does this work then spell the beginning of the end for text-based Captcha? We believe so. A novel image-based Captcha named Style Area Captcha (SACaptcha) is proposed in this paper, which is based on semantic information understanding, pixel-level segmentation, and deep learning techniques. Having demonstrated that text Captchas are no longer secure, we hope that our proposal shows promise in the development of image-based Captchas using deep learning techniques.","keywords_author":["Captcha","convolutional neural network","deep learning","image-based","security","text-based","Captcha","text-based","security","deep learning","convolutional neural network","image-based","Captcha","text-based","security","deep learning","convolutional neural network","image-based"],"keywords_other":["Character recognition","websites","Text Captchas","computer attack programs","Chinese Captchas","bots","deep learning techniques","popular Captcha scheme","Machine learning","Web sites","Completely Automated Public Turing Tests","learning (artificial intelligence)","existing Captchas","security","invasive software","RECOGNITION","Image-based","text-based","CAPTCHAs","image segmentation","SACaptcha","text Captcha","Resistance","Computer security","user friendliness","human computer interaction","text analysis","Style Area Captcha","Convolutional neural network","Image segmentation"],"max_cite":0.0,"pub_year":2018.0,"sources":"['scp', 'wos', 'ieee']","rawkeys":["web sites","websites","computer security","computer attack programs","convolutional neural network","completely automated public turing tests","chinese captchas","bots","machine learning","deep learning techniques","popular captcha scheme","captchas","learning (artificial intelligence)","recognition","deep learning","image-based","sacaptcha","security","character recognition","invasive software","text-based","image segmentation","text captcha","style area captcha","text captchas","existing captchas","user friendliness","human computer interaction","captcha","text analysis","resistance"],"tags":["web sites","computer security","computer attack programs","convolutional neural network","completely automated public turing tests","chinese captchas","bots","machine learning","human-computer interaction","deep learning techniques","website","popular captcha scheme","captchas","recognition","image-based","sacaptcha","security","character recognition","invasive software","text-based","image segmentation","text captcha","style area captcha","existing captchas","user friendliness","text analysis","resistance"]},{"p_id":74681,"title":"Heterogeneous domain adaptation network based on autoencoder","abstract":"Heterogeneous domain adaptation is a more challenging problem than homogeneous domain adaptation. The transfer effect is not ideally caused by shallow structure which cannot adequately describe the probability distribution and obtain more effective features. In this paper, we propose a heterogeneous domain adaptation network based on autoencoder, in which two sets of autoencoder networks are used to project the source-domain and target-domain data to a shared feature space to obtain more abstractive feature representations. In the last feature and classification layer, the marginal and conditional distributions can be matched by empirical maximum mean discrepancy metric to reduce distribution difference. To preserve the consistency of geometric structure and label information, a manifold alignment term based on labels is introduced. The classification performance can be improved further by making full use of label information of both domains. The experimental results of 16 cross-domain transfer tasks verify that HDANA outperforms several state-of-the-art methods. (C) 2017 Elsevier Inc. All rights reserved.","keywords_author":["Heterogeneous domain adaptation","Autoencoder","Maximum mean discrepancy","Manifold alignment"],"keywords_other":["RECOGNITION","KERNEL","FEATURES"],"max_cite":0.0,"pub_year":2018.0,"sources":"['wos']","rawkeys":["heterogeneous domain adaptation","recognition","maximum mean discrepancy","features","autoencoder","kernel","manifold alignment"],"tags":["heterogeneous domain adaptation","recognition","maximum mean discrepancy","features","auto encoders","kernel","manifold alignment"]},{"p_id":74684,"title":"Domain class consistency based transfer learning for image classification across domains","abstract":"Distribution mismatch between the modeling data and the query data is a known domain adaptation issue in machine learning. To this end, in this paper, we propose a l(2,1)-norm based discriminative robust kernel transfer learning (DKTL) method for high-level recognition tasks. The key idea is to realize robust domain transfer by simultaneously integrating domain-class-consistency (DCC) metric based discriminative subspace learning, kernel learning in reproduced kernel Hilbert space, and representation learning between source and target domain. The DCC metric includes two properties: domain-consistency used to measure the between-domain distribution discrepancy and class-consistency used to measure the within-domain class separability. The essential objective of the proposed transfer learning method is to maximize the DCC metric, which is equivalently to minimize the domain-class-inconsistency (DCIC), such that domain distribution mismatch and class inseparability are well formulated and unified simultaneously. The merits of the proposed method include (1) the robust sparse coding selects a few valuable source data with noises (outliers) removed during knowledge transfer, and (2) the proposed DCC metric can pursue more discriminative subspaces of different domains. As a result, the maximum class separability is also well guaranteed. Extensive experiments on a number of visual datasets demonstrate the superiority of the proposed method over other state-of-the-art domain adaptation and transfer learning methods. (C) 2017 Elsevier Inc. All rights reserved.","keywords_author":["Transfer learning","Representation learning","Subspace learning","Kernel learning","Domain adaptation"],"keywords_other":["ADAPTATION","RECOGNITION","FRAMEWORK","REGULARIZATION"],"max_cite":3.0,"pub_year":2017.0,"sources":"['ieee', 'wos']","rawkeys":["recognition","transfer learning","framework","representation learning","domain adaptation","subspace learning","kernel learning","adaptation","regularization"],"tags":["recognition","denoising autoencoder","framework","transfer learning","representation learning","subspace learning","kernel learning","adaptation","regularization"]},{"p_id":25536,"title":"Recognition of the part of growth of flue-cured tobacco leaves based on support vector machine","abstract":"It is the most important in flue-cured tobacco leaves grading to recognize the parts of tobacco plants where the flue-cured tobacco leaves grown. In this paper, the image processing and analysis are applied in extracting the feature parameters of the tobacco leaves quality, the fuzzy statistics and comprehensive judgment techniques are applied to judge the group membership of tobacco leaf samples preparatory to the further recognition by means of support vector machine (SVM). The samples which are wrongly classified form the working set of SVM, the rest samples make up the non-working set. By this method, a passel of flue-cured tobacco leaves from Qujing area, Yunnan province are classified into 3 groups according to the recognition of the grow parts of tobacco plants. The result indicates that near 95% of samples in the SVM grouping are consistent with those in the expert grouping. \u00a9 2008 IEEE.","keywords_author":["Features extraction","Recognition","Support vector machine","Tobacco leave"],"keywords_other":["Working set","Recognition","Fuzzy statistics","Support vector machine","Yunnan province","Flue-cured tobacco","Tobacco plants","Feature parameters","Image processing and analysis","Features extraction","Tobacco leave","Group memberships","Tobacco leaves","Control and automation"],"max_cite":8.0,"pub_year":2008.0,"sources":"['scp']","rawkeys":["control and automation","recognition","features extraction","working set","tobacco leave","feature parameters","image processing and analysis","support vector machine","tobacco leaves","yunnan province","fuzzy statistics","flue-cured tobacco","group memberships","tobacco plants"],"tags":["control and automation","recognition","working set","machine learning","tobacco leave","feature parameters","image processing and analysis","feature extraction","yunnan province","fuzzy statistics","flue-cured tobacco","group memberships","tobacco plants"]},{"p_id":50125,"title":"Areas of life visualisation: Growing data-reliance","abstract":"\u00a9 Springer International Publishing AG 2016. This paper presents a framework to mine and identify the areas of life and the way they are perceived, understood cognitively, and effectively using visualisation and machine learning. We provide an overview of the network of users including their activity and connections as well as zoom and details on demand of each individual areas of life. This research identifies the factors of each area of life which are significant on the user\u2019s social media profile in relation to information associated with each user such as time and location, including dynamic social behaviours. It aims to identify the key psychological factors and salient behaviours in order to find out the psychological factors of the user, and other overheads that can be portrayed in an image.","keywords_author":["Areas of life","Data visualisation","Dynamic social behaviours","Machine learning","Psychology","Social networks","Twitter"],"keywords_other":["Twitter","Psychology","Social media","Areas of life","Psychological factors","Social behaviour","On demands"],"max_cite":0.0,"pub_year":2016.0,"sources":"['scp']","rawkeys":["dynamic social behaviours","social behaviour","social media","machine learning","social networks","psychological factors","areas of life","on demands","psychology","data visualisation","twitter"],"tags":["recognition","dynamic social behaviours","social behaviour","social media","machine learning","social networks","psychological factors","areas of life","on demands","data visualisation","twitter"]},{"p_id":74703,"title":"Improving Smart Home Security: Integrating Logical Sensing Into Smart Home","abstract":"This paper explains various security issues in the existing home automation systems and proposes the use of logic-based security algorithms to improve home security. This paper classifies natural access points to a home as primary and secondary access points depending on their use. Logic-based sensing is implemented by identifying normal user behavior at these access points and requesting user verification when necessary. User position is also considered when various access points changed states. Moreover, the algorithm also verifies the legitimacy of a fire alarm by measuring the change in temperature, humidity, and carbon monoxide levels, thus defending against manipulative attackers. The experiment conducted in this paper used a combination of sensors, microcontrollers, Raspberry Pi and ZigBee communication to identify user behavior at various access points and implement the logical sensing algorithm. In the experiment, the proposed logical sensing algorithm was successfully implemented for a month in a studio apartment. During the course of the experiment, the algorithm was able to detect all the state changes of the primary and secondary access points and also successfully verified user identity 55 times generating 14 warnings and 5 alarms.","keywords_author":["Home automation","smart homes","wireless sensor networks","access control","ZigBee"],"keywords_other":["PROTOCOL","CHALLENGES","IMPLEMENTATION","NETWORKS","ATTACKS","ALGORITHM","DESIGN","RECOGNITION","CONTEXT-AWARE MIDDLEWARE","AUTOMATION SYSTEM"],"max_cite":6.0,"pub_year":2017.0,"sources":"['wos']","rawkeys":["algorithm","design","recognition","smart homes","home automation","automation system","challenges","networks","wireless sensor networks","zigbee","attacks","implementation","access control","context-aware middleware","protocol"],"tags":["design","context aware middleware","recognition","smart homes","automated systems","home automation","challenges","networks","zigbee","biometrics","attacks","algorithms","implementation","protocols","wireless sensor networks"]},{"p_id":25557,"title":"Memristive neural network for on-line learning and tracking with brain-inspired spike timing dependent plasticity","abstract":"\u00a9 2017 The Author(s). Brain-inspired computation can revolutionize information technology by introducing machines capable of recognizing patterns (images, speech, video) and interacting with the external world in a cognitive, humanlike way. Achieving this goal requires first to gain a detailed understanding of the brain operation, and second to identify a scalable microelectronic technology capable of reproducing some of the inherent functions of the human brain, such as the high synaptic connectivity (~104) and the peculiar time-dependent synaptic plasticity. Here we demonstrate unsupervised learning and tracking in a spiking neural network with memristive synapses, where synaptic weights are updated via brain-inspired spike timing dependent plasticity (STDP). The synaptic conductance is updated by the local time-dependent superposition of pre-and post-synaptic spikes within a hybrid one-transistor\/one-resistor (1T1R) memristive synapse. Only 2 synaptic states, namely the low resistance state (LRS) and the high resistance state (HRS), are sufficient to learn and recognize patterns. Unsupervised learning of a static pattern and tracking of a dynamic pattern of up to 4 \u00d7 4 pixels are demonstrated, paving the way for intelligent hardware technology with up-scaled memristive neural networks.","keywords_author":null,"keywords_other":["COMPUTATION","RESISTIVE-SWITCHING MEMORY","RULE","SYSTEMS","VISUAL-CORTEX","RECOGNITION","SYNAPTIC DEVICE","METAL-OXIDE MEMRISTORS","SYNAPSES","ARCHITECTURE"],"max_cite":8.0,"pub_year":2017.0,"sources":"['scp', 'wos']","rawkeys":["recognition","synapses","visual-cortex","rule","resistive-switching memory","metal-oxide memristors","computation","systems","architecture","synaptic device"],"tags":["rules","recognition","synapses","visual-cortex","neural networks","system","metal-oxide memristors","resistive switching memory","architecture","synaptic device"]},{"p_id":9176,"title":"Mastering the game of Go without human knowledge","abstract":"\u00a9 2017 Macmillan Publishers Limited, part of Springer Nature. All rights reserved. A long-standing goal of artificial intelligence is an algorithm that learns, tabula rasa, superhuman proficiency in challenging domains. Recently, AlphaGo became the first program to defeat a world champion in the game of Go. The tree search in AlphaGo evaluated positions and selected moves using deep neural networks. These neural networks were trained by supervised learning from human expert moves, and by reinforcement learning from self-play. Here we introduce an algorithm based solely on reinforcement learning, without human data, guidance or domain knowledge beyond game rules. AlphaGo becomes its own teacher: a neural network is trained to predict AlphaGo's own move selections and also the winner of AlphaGo's games. This neural network improves the strength of the tree search, resulting in higher quality move selection and stronger self-play in the next iteration. Starting tabula rasa, our new program AlphaGo Zero achieved superhuman performance, winning 100-0 against the previously published, champion-defeating AlphaGo.","keywords_author":null,"keywords_other":["Neural Networks (Computer)","Supervised Machine Learning","Humans","Unsupervised Machine Learning","Reinforcement (Psychology)","Games, Recreational","Software"],"max_cite":79.0,"pub_year":2017.0,"sources":"['ieee', 'scp', 'wos']","rawkeys":["unsupervised machine learning","neural networks (computer)","reinforcement (psychology)","humans","software","recreational","games","supervised machine learning"],"tags":["unsupervised machine learning","recognition","neural networks","humans","software","recreational","games","supervised machine learning"]},{"p_id":41950,"title":"Application of the convolutional neural network to design an algorithm for recognition of tower lighthouses","abstract":"\u00a9 2017 Concern CSRI Elektropribor, JSC. The paper considers the main features of the convolutional neural network architecture and their application to design the algorithm for recognition of tower lighthouse images. Tower lighthouses are supposed to have red and white horizontal bands. Their images can be received from some electronic video devices. A brief review of the definitions used in this subject area, such as layers and layer types: Convolution layer, pooling layer, and fully-connected layer, is given. The process of training the neural network and the most popular technologies for developing convolution neural networks are described. A recognition algorithm has been designed, which proved to be correct in about 69-73% of the test cases.","keywords_author":["convolutional neural network","recognition","tower lighthouse"],"keywords_other":["recognition","Recognition algorithm","Video devices","Convolutional neural network","Test case","Convolution neural network","As layers"],"max_cite":1.0,"pub_year":2017.0,"sources":"['scp']","rawkeys":["recognition","convolution neural network","test case","video devices","convolutional neural network","recognition algorithm","tower lighthouse","as layers"],"tags":["recognition","test case","video devices","convolutional neural network","recognition algorithm","tower lighthouse","as layers"]},{"p_id":74723,"title":"Assessment of PD severity in gas-insulated switchgear with an SSAE","abstract":"Scientific partial discharge (PD) severity evaluation is highly important to the safe operation of gas-insulated switchgear. However, describing PD severity with only a few statistical features such as discharge time and discharge amplitude is unreliable. Hence, a deep-learning neural network model called stacked sparse auto-encoder (SSAE) is proposed to realise feature extraction from the middle layer with a small number of nodes. The output feature that is almost similar to the input PD information is produced in the model. The features extracted from PD data are then fed into a soft-max classifier to be classified into one of four defined PD severity states. In addition, unsupervised greedy layer-wise pre-training and supervised fine-tuning are utilised to train the SSAE network during evaluation. Results of testing and simulation analysis show that the features extracted by the SSAE model effectively characterise PD severity. The performance of the SSAE model, which possesses an average assessment accuracy of up to 92.2%, is better than that of the support vector machine algorithm based on statistical features. According to the tested number of SSAE layers and features and the training sample size, the SSAE model possesses good expansibility and can be useful in practical applications.","keywords_author":["partial discharge measurement","gas insulated switchgear","statistical analysis","neural nets","learning (artificial intelligence)","encoding","feature extraction","computerised instrumentation","PD severity assessment","gas-insulated switchgear","SSAE","partial discharge severity assessment","discharge time","discharge amplitude","deep-learning neural network model","stacked sparse autoencoder","feature extraction","soft-max classifier","unsupervised greedy layer-wise pre-training method","supervised fine-tuning method","support vector machine algorithm"],"keywords_other":["DIAGNOSIS","CLASSIFICATION","NEURAL-NETWORKS","RECOGNITION","METALLIC PARTICLES","SF6","GIS","PARTIAL DISCHARGE","SURFACE"],"max_cite":0.0,"pub_year":2017.0,"sources":"['ieee', 'wos']","rawkeys":["partial discharge severity assessment","pd severity assessment","support vector machine algorithm","deep-learning neural network model","discharge amplitude","statistical analysis","classification","encoding","gas insulated switchgear","neural-networks","metallic particles","partial discharge","partial discharge measurement","gas-insulated switchgear","supervised fine-tuning method","diagnosis","gis","learning (artificial intelligence)","recognition","sf6","soft-max classifier","discharge time","computerised instrumentation","neural nets","stacked sparse autoencoder","surface","unsupervised greedy layer-wise pre-training method","feature extraction","ssae"],"tags":["partial discharge severity assessment","pd severity assessment","support vector machine algorithm","deep-learning neural network model","surfaces","discharge amplitude","statistical analysis","classification","encoding","gas insulated switchgear","metallic particles","machine learning","partial discharge measurement","supervised fine-tuning method","diagnosis","gis","parkinson's disease","recognition","sf6","neural networks","discharge time","computerised instrumentation","stacked sparse autoencoder","unsupervised greedy layer-wise pre-training method","softmax classifier","feature extraction"]},{"p_id":58342,"title":"Visual feature coding based on heterogeneous structure fusion for image classification","abstract":"The relationship between visual words and local feature (words structure) or the distribution among images (images structure) is important in feature encoding to approximate the intrinsically discriminative structure of images in the Bag-of-Words (BoW) model. However, in recently most methods, the intrinsic invariance in intra-class images is difficultly captured using words structure or images structure for large variability image classification. To overcome this limitation, we propose a local visual feature coding based on heterogeneous structure fusion (LVFC-HSF) that explores the nonlinear relationship between words structure and images structure in feature space, as follows. First, we utilize high-order topology to describe the dependence of the visual words, and use the distance measurement based on the local feature to represent the distribution of images. Then, we construct the unitedly optimal framework according to the relevance between words structure and images structure to solve the projection matrix of local feature and the Weight coefficient, which can exploit the nonlinear relationship of heterogeneous structure to balance their interaction. Finally, we adopt the improving fisher kemel(IFK) to fit the distribution of the projected features for obtaining the image feature. The experimental results on ORL, 15 Scenes, Caltech 101 and Caltech 256 demonstrate that heterogeneous structure fusion significantly enhances the intrinsic structure construction, and consequently improves the classification performance in these data sets. (C) 2016 Elsevier B.V. All rights reserved.","keywords_author":["Words structure","Images structure","Heterogeneous structure fusion","Image classification"],"keywords_other":["NETWORKS","RECOGNITION","CODEBOOK"],"max_cite":3.0,"pub_year":2017.0,"sources":"['ieee', 'wos']","rawkeys":["words structure","recognition","networks","images structure","codebook","heterogeneous structure fusion","image classification"],"tags":["codebooks","recognition","image structures","networks","heterogeneous structure fusion","words structure","image classification"]},{"p_id":82919,"title":"Morphological path filtering at the region scale for efficient and robust road network extraction from satellite imagery","abstract":"Roads are important elements in geographic information systems and remote sensing applications. Their automatic extraction is challenging when only aerial or satellite images are used. Recently, some promising attempts have been made with (incomplete) path opening\/closing, morphological filters able to deal with curvilinear structures. We propose here to apply morphological path filters not on pixels directly but rather on regions representing road segments, in order to improve both efficiency and robustness. The overall process is organized in two steps: first we map road segments by rectangular areas made of similar content, before we connect such segments into paths of segments or polylines using region-based path filtering. Robustness to occlusion is ensured through the adaptation of the incomplete path filtering strategy to the region scale, while better discrimination between road segments and other objects is achieved through an hit-or-miss transform that exploits background knowledge. Experiments conducted on several satellite images illustrate the interest of the proposed approach, and shows it outperforms pixelwise detection. (C) 2016 Elsevier B.V. All rights reserved.","keywords_author":["Mathematical morphology","Path filtering","Hit-or-miss transform","Road extraction","Remote sensing"],"keywords_other":["REMOTE-SENSING IMAGES","MATHEMATICAL MORPHOLOGY","OPENINGS","RECOGNITION","CLOSINGS"],"max_cite":1.0,"pub_year":2016.0,"sources":"['wos']","rawkeys":["recognition","remote sensing","path filtering","remote-sensing images","openings","road extraction","hit-or-miss transform","mathematical morphology","closings"],"tags":["recognition","remote sensing","closeness","path filtering","openings","remote sensing images","road extraction","hit-or-miss transform","mathematical morphology"]},{"p_id":58348,"title":"Joint Attributes and Event Analysis for Multimedia Event Detection","abstract":"Semantic attributes have been increasingly used the past few years for multimedia event detection (MED) with promising results. The motivation is that multimedia events generally consist of lower level components such as objects, scenes, and actions. By characterizing multimedia event videos with semantic attributes, one could exploit more informative cues for improved detection results. Much existing work obtains semantic attributes from images, which may be suboptimal for video analysis since these image-inferred attributes do not carry dynamic information that is essential for videos. To address this issue, we propose to learn semantic attributes from external videos using their semantic labels. We name them video attributes in this paper. In contrast with multimedia event videos, these external videos depict lower level contents such as objects, scenes, and actions. To harness video attributes, we propose an algorithm established on a correlation vector that correlates them to a target event. Consequently, we could incorporate video attributes latently as extra information into the event detector learnt from multimedia event videos in a joint framework. To validate our method, we perform experiments on the real-world large-scale TRECVID MED 2013 and 2014 data sets and compare our method with several state-of-the-art algorithms. The experiments show that our method is advantageous for MED.","keywords_author":["Correlation uncovering","multimedia event detection (MED)","video attributes"],"keywords_other":["RECOGNITION","FEATURES"],"max_cite":1.0,"pub_year":2018.0,"sources":"['ieee', 'wos']","rawkeys":["recognition","features","multimedia event detection (med)","video attributes","correlation uncovering"],"tags":["recognition","features","video attributes","multimedia event detections","correlation uncovering"]},{"p_id":82926,"title":"MRF-based segmentation and unsupervised classification for building and road detection in peri-urban areas of high -resolution satellite images","abstract":"We present in this article a new method on unsupervised semantic parsing and structure recognition in peri-urban areas using satellite images. The automatic \"building\" and \"road\" detection is based on regions extracted by an unsupervised segmentation method. We propose a novel segmentation algorithm based on a Marlcov random field model and we give an extensive data analysis for determining relevant features for the classification problem. The novelty of the segmentation algorithm lies on the class -driven vector data quantization and clustering and the estimation of the likelihoods given the resulting clusters. We have evaluated the reachability of a good classification rate using the Random Forest method. We found that, with a limited number of features, among them some new defined in this article, we can obtain good classification performance. Our main contribution lies again on the data analysis and the estimation of likelihoods. Finally, we propose a new method for completing the road network exploiting its connectivity, and the local and global properties of the road network. (C) 2016 International Society for Photogrammetry and Remote Sensing, Inc. (ISPRS). Published by Elsevier B.V. All rights reserved.","keywords_author":["Building\/road extraction","Satellite images","Image segmentation","Feature analysis","Random Forest","Unsupervised classification"],"keywords_other":["REMOTE-SENSING IMAGES","MEAN SHIFT","GRAPH CUTS","AUTOMATED DETECTION","AERIAL IMAGERY","URBAN-AREA","NETWORK EXTRACTION","ENVIRONMENTS","RECOGNITION","MAN-MADE OBJECTS"],"max_cite":8.0,"pub_year":2016.0,"sources":"['wos']","rawkeys":["urban-area","aerial imagery","recognition","unsupervised classification","automated detection","network extraction","feature analysis","building\/road extraction","man-made objects","environments","mean shift","remote-sensing images","satellite images","graph cuts","random forest","image segmentation"],"tags":["urban areas","aerial imagery","man made objects","recognition","automated detection","network extraction","unsupervised classification","feature analysis","building\/road extraction","multiple sclerosis","random forests","satellite images","environment","remote sensing images","graph cuts","image segmentation"]},{"p_id":99313,"title":"Predicting malignant nodules by fusing deep features with classical radiomics features","abstract":"Lung cancer has a high incidence and mortality rate. Early detection and diagnosis of lung cancers is best achieved with low-dose computed tomography (CT). Classical radiomics features extracted from lung CT images have been shown as able to predict cancer incidence and prognosis. With the advancement of deep learning and convolutional neural networks (CNNs), deep features can be identified to analyze lung CTs for prognosis prediction and diagnosis. Due to a limited number of available images in the medical field, the transfer learning concept can be helpful. Using subsets of participants from the National Lung Screening Trial (NLST), we utilized a transfer learning approach to differentiate lung cancer nodules versus positive controls. We experimented with three different pretrained CNNs for extracting deep features and used five different classifiers. Experiments were also conducted with deep features from different color channels of a pretrained CNN. Selected deep features were combined with radiomics features. A CNN was designed and trained. Combinations of features from pretrained, CNNs trained on NLST data, and classical radiomics were used to build classifiers. The best accuracy (76.79%) was obtained using feature combinations. An area under the receiver operating characteristic curve of 0.87 was obtained using a CNN trained on an augmented NLST data cohort. (c) 2018 Society of Photo-Optical Instrumentation Engineers (SPIE)","keywords_author":["nonsmall cell lung cancer","National Lung Screening Trial","convolutional neural network","transfer learning","deep features","radiomics"],"keywords_other":["NETWORKS","CHARACTERISTIC ROC CURVE","RECOGNITION"],"max_cite":0.0,"pub_year":2018.0,"sources":"['wos']","rawkeys":["characteristic roc curve","recognition","transfer learning","networks","radiomics","convolutional neural network","deep features","national lung screening trial","nonsmall cell lung cancer"],"tags":["characteristic roc curve","recognition","transfer learning","networks","radiomics","convolutional neural network","deep features","national lung screening trial","nonsmall cell lung cancer"]},{"p_id":1011,"title":"Speech Quality Assessment Over Lossy Transmission Channels Using Deep Belief Networks","abstract":"Nowadays, there are several telephone services based on IP networks. However, the networks can present many disturbances, such as packet loss rate (PLR), which is one of the most impairing network factors. An impaired speech communication affects the users' quality of experience; hence, the assessment of speech quality is relevant to the telephone operators. Therefore, the determination of a methodology to predict a speech quality with a higher accuracy in telephone services is relevant. In this context, this letter introduces a novel nonintrusive speech quality classifier (SQC) model based on deep belief networks (DBN), in which the support vector machine with radial basis function kernel is the classifier applied in DBN, in order to identify four speech quality classes. A speech database was built, based on unimpaired speech files of public databases, in which different PLR models and values are applied, and a standardized intrusive method is used to calculate the index quality of each file. Results show that SQC largely overcomes the results obtained by ITU-T Recommendation P.563. Also, subjective tests are performed to validate the SQC performance, and it reached an accuracy of 95% on speech quality classification. Furthermore, a solution architecture is introduced, demonstrating the usefulness and flexibility of the proposed SQC.","keywords_author":["Deep belief networks (DBN)","machine learning","packet loss rate (PLR)","speech quality assessment","Deep belief networks (DBN)","machine learning","packet loss rate (PLR)","speech quality assessment"],"keywords_other":["P.563 ITU-T recommendation","radial basis function networks","IP networks","speech database","SQC","speech recognition","Packet loss","support vector machine","nonintrusive speech quality classifier model","user quality of experience","speech quality classes","speech quality classification","standardized intrusive method","Speech","index quality","radial basis function kernel","telephone services","unimpaired speech files","public databases","Quality assessment","PLR model","speech quality assessment","lossy transmission channels","Indexes","RECOGNITION","Hidden Markov models","DBN","belief networks","packet loss rate","telephone operators","speech coding","impaired speech communication","speech processing","NEURAL-NETWORKS","Speech recognition","feature extraction","support vector machines","deep belief networks"],"max_cite":0.0,"pub_year":2017.0,"sources":"['wos', 'ieee']","rawkeys":["radial basis function networks","speech database","speech recognition","deep belief networks","support vector machine","nonintrusive speech quality classifier model","neural-networks","user quality of experience","speech quality classes","speech quality classification","dbn","standardized intrusive method","machine learning","index quality","radial basis function kernel","speech","telephone services","unimpaired speech files","public databases","p.563 itu-t recommendation","lossy transmission channels","recognition","speech quality assessment","indexes","deep belief networks (dbn)","packet loss","belief networks","packet loss rate","plr model","telephone operators","hidden markov models","speech coding","packet loss rate (plr)","impaired speech communication","quality assessment","speech processing","sqc","feature extraction","support vector machines","ip networks"],"tags":["speech database","speech recognition","deep belief networks","radial basis function neural networks","nonintrusive speech quality classifier model","user quality of experience","speech quality classes","speech quality classification","standardized intrusive method","machine learning","index","index quality","speech","telephone services","unimpaired speech files","p.563 itu-t recommendation","lossy transmission channels","recognition","speech quality assessment","neural networks","public database","information retrieval","packet loss","belief networks","packet loss rate","plr model","radial basis functions","telephone operators","hidden markov models","speech coding","impaired speech communication","speech processing","sqc","feature extraction","ip networks"]},{"p_id":74743,"title":"Deep Feature Fusion for Iris and Periocular Biometrics on Mobile Devices","abstract":"The quality of iris images on mobile devices is significantly degraded due to hardware limitations and less constrained environments. Traditional iris recognition methods cannot achieve high identification rate using these low- quality images. To enhance the performance of mobile identification, we develop a deep feature fusion network that exploits the complementary information presented in iris and periocular regions. The proposed method first applies maxout units into the convolutional neural networks (CNNs) to generate a compact representation for each modality and then fuses the discriminative features of two modalities through a weighted concatenation. The parameters of convolutional filters and fusion weights are simultaneously learned to optimize the joint representation of iris and periocular biometrics. To promote the iris recognition research on mobile devices under near-infrared (NIR) illumination, we publicly release the CASIA-Iris-Mobile-V1.0 database, which in total includes 11 000 NIR iris images of both eyes from 630 Asians. It is the largest NIR mobile iris database as far as we know. On the newly built CASIA-Iris-M1-S3 data set, the proposed method achieves 0.60% equal error rate and 2.32% false non-match rate at false match rate = 10(-5), which are obviously better than unimodal biometrics as well as traditional fusion methods. Moreover, the proposed model requires much fewer storage spaces and computational resources than general CNNs.","keywords_author":["Iris recognition","periocular recognition","deep feature fusion","adaptive weights","mobile devices"],"keywords_other":["RECOGNITION","AUTHENTICATION","FACE"],"max_cite":0.0,"pub_year":2018.0,"sources":"['wos']","rawkeys":["recognition","authentication","deep feature fusion","adaptive weights","mobile devices","iris recognition","periocular recognition","face"],"tags":["recognition","authentication","deep feature fusion","adaptive weights","mobile devices","iris recognition","periocular recognition","face"]},{"p_id":25608,"title":"Multitalker Speech Separation With Utterance-Level Permutation Invariant Training of Deep Recurrent Neural Networks","abstract":"\u00a9 2017 IEEE.In this paper, we propose the utterance-level permutation invariant training (uPIT) technique. uPIT is a practically applicable, end-to-end, deep-learning-based solution for speaker independent multitalker speech separation. Specifically, uPIT extends the recently proposed permutation invariant training (PIT) technique with an utterance-level cost function, hence eliminating the need for solving an additional permutation problem during inference, which is otherwise required by frame-level PIT. We achieve this using recurrent neural networks (RNNs) that, during training, minimize the utterance-level separation error, hence forcing separated frames belonging to the same speaker to be aligned to the same output stream. In practice, this allows RNNs, trained with uPIT, to separate multitalker mixed speech without any prior knowledge of signal duration, number of speakers, speaker identity, or gender. We evaluated uPIT on the WSJ0 and Danish two- and three-talker mixed-speech separation tasks and found that uPIT outperforms techniques based on nonnegative matrix factorization and computational auditory scene analysis, and compares favorably with deep clustering, and the deep attractor network. Furthermore, we found that models trained with uPIT generalize well to unseen speakers and languages. Finally, we found that a single model, trained with uPIT, can handle both two-speaker, and three-speaker speech mixtures.","keywords_author":["CNN","cocktail party problem","deep learning","DNN","LSTM","permutation invariant training","speech separation","CNN","cocktail party problem","deep learning","DNN","LSTM","permutation invariant training","speech separation"],"keywords_other":["Computational model","NOISE","Time frequency analysis","LSTM","Speech separation","INTELLIGIBILITY","RECOGNITION","COCHANNEL SPEECH","ENHANCEMENT","Cocktail party problems"],"max_cite":7.0,"pub_year":2017.0,"sources":"['ieee', 'scp', 'wos']","rawkeys":["cochannel speech","permutation invariant training","lstm","noise","enhancement","recognition","deep learning","speech separation","intelligibility","cnn","cocktail party problems","dnn","time frequency analysis","cocktail party problem","computational model"],"tags":["cochannel speech","computational modeling","permutation invariant training","intelligence","noise","enhancement","recognition","long short-term memory","machine learning","time-frequency analysis","convolutional neural network","cocktail party problem","speech separation"]},{"p_id":1034,"title":"Visual Representation and Classification by Learning Group Sparse Deep Stacking Network","abstract":"Deep stacking networks (DSNs) have been successfully applied in classification tasks. Its architecture builds upon blocks of simplified neural network modules (SNNM). The hidden units are assumed to be independent in the SNNM module. However, this assumption prevents SNNM from learning the local dependencies between hidden units to better capture the information in the input data for the classification task. In addition, the hidden representations of input data in each class can be expectantly split into a group in real-world classification applications. Therefore, we propose two kinds of group sparse SNNM modules by mixing $l_{1}$ -norm and $l_{2}$ -norm. The first module learns the local dependencies among hidden units by dividing them into non-overlapping groups. The second module splits the representations of samples in different classes into separate groups to cluster the samples in each class. A group sparse DSN (GS-DSN) is constructed by stacking the group sparse SNNM modules. Experimental results further verify that our GS-DSN model outperforms the relevant classification methods. Particularly, GS-DSN achieves the state-of-the-art performance (99.1%) on 15-Scene.","keywords_author":["Deep learning","stacking network","sparse representation","image classification","Deep learning","stacking network","sparse representation","image classification"],"keywords_other":["REGRESSION","Training","SYSTEMS","Biological neural networks","Dictionaries","Encoding","IMAGE CLASSIFICATION","RECOGNITION","DICTIONARY","Stacking"],"max_cite":0.0,"pub_year":2017.0,"sources":"['ieee', 'wos']","rawkeys":["recognition","deep learning","training","sparse representation","stacking","stacking network","dictionary","systems","biological neural networks","regression","image classification","dictionaries","encoding"],"tags":["recognition","training","sparse representation","machine learning","stacking","stacking network","system","biological neural networks","regression","image classification","dictionaries","encoding"]},{"p_id":1041,"title":"Hybrid adversarial sample crafting for black-box evasion attack","abstract":"Machine learning has been broadly applied in different applications due to its satisfying performance. In security-related applications, e.g. facial recognition and fingerprint identification, an adversary misleads the pattern recognition system on purpose by manipulating training or test samples. Black-box attack is one of adversarial attacks which camouflages unseen samples to evade the detection with limited information on the system. Recently, there are a few studies on the robustness of machine learning, especially deep learning, under black-box attack. In this study, we investigate the security of the stacked autoencoder, which is one of the most famous models in deep learning against black-box attack. A training dataset generation method is proposed for a substitute model in black-box attack. By enquiring labels of samples from the target classifier, we train a model to approximate the target classifier. To have a better approximation to the decision boundary of the target classifier, a hybrid data generation method is adopted to increase the number of the training data. We generate new samples by combining both near and far away the decision boundary of the substitute model. The experiment results suggest that our proposed method downgrades the classifier in terms of accuracy more significantly than the existing one within less label query times. It shows the proposed method is more efficient to train a substitute model to approximate target classifier.","keywords_author":["Adversary learning","Black-box attack","Deep learning","Evasion attack","Machine learning","Transferability","Deep learning","Adversary learning","Machine learning","Transferability","Evasion attack","Black-box attack"],"keywords_other":["Fingerprint identification","black-box evasion attack","Facial recognition","Transferability","encoding","Adversary learning","machine learning","target classifier approximation","hybrid adversarial sample crafting","learning (artificial intelligence)","deep learning","hybrid data generation","approximation theory","sampling methods","Decision boundary","Limited information","Evasion attack","security of data","stacked autoencoder","Black boxes"],"max_cite":0.0,"pub_year":2017.0,"sources":"['scp', 'ieee']","rawkeys":["black-box evasion attack","adversary learning","encoding","black-box attack","transferability","fingerprint identification","machine learning","target classifier approximation","facial recognition","hybrid adversarial sample crafting","evasion attack","learning (artificial intelligence)","decision boundary","deep learning","hybrid data generation","approximation theory","sampling methods","limited information","black boxes","security of data","stacked autoencoder"],"tags":["black-box evasion attack","encoding","black-box attack","adversarial learning","machine learning","fingerprint identification","target classifier approximation","facial recognition","hybrid adversarial sample crafting","recognition","evasion attacks","decision boundary","hybrid data generation","approximation theory","sampling methods","limited information","black boxes","stacked autoencoders","security of data"]},{"p_id":115747,"title":"An FPGA-Based Hardware Accelerator for Traffic Sign Detection","abstract":"Traffic sign detection plays an important role in a number of practical applications, such as intelligent driver assistance and roadway inventory management. In order to process the large amount of data from either real-time videos or large off-line databases, a high-throughput traffic sign detection system is required. In this paper, we propose an FPGA-based hardware accelerator for traffic sign detection based on cascade classifiers. To maximize the throughput and power efficiency, we propose several novel ideas, including: 1) rearranged numerical operations; 2) shared image storage; 3) adaptive workload distribution; and 4) fast image block integration. The proposed design is evaluated on a Xilinx ZC706 board. When processing high-definition (1080p) video, it achieves the throughput of 126 frames\/s and the energy efficiency of 0.041 J\/frame.","keywords_author":["Accelerator","cascade classifier","energy efficiency","FPGA","traffic sign detection"],"keywords_other":["RECOGNITION","CLASSIFICATION","FEATURES","OBJECT DETECTION"],"max_cite":2.0,"pub_year":2017.0,"sources":"['wos']","rawkeys":["recognition","features","fpga","cascade classifier","classification","object detection","traffic sign detection","energy efficiency","accelerator"],"tags":["recognition","features","acceleration","fpga","classification","object detection","traffic sign detection","energy efficiency","cascade classifiers"]},{"p_id":58412,"title":"Collaborative representation-based classification of microarray gene expression data","abstract":"Microarray technology is important to simultaneously express multiple genes over a number of time points. Multiple classifier models, such as sparse representation (SR)-based method, have been developed to classify microarray gene expression data. These methods allocate the gene data points to different clusters. In this paper, we propose a novel collaborative representation (CR)-based classification with regularized least square to classify gene data. First, the CR codes a testing sample as a sparse linear combination of all training samples and then classifies the testing sample by evaluating which class leads to the minimum representation error. This CR-based classification approach is remarkably less complex than traditional classification methods but leads to very competitive classification results. In addition, compressive sensing approach is adopted to project the high-dimensional gene expression dataset to a lower-dimensional space which nearly contains the whole information. This compression without loss is beneficial to reduce the computational load. Experiments to detect subtypes of diseases, such as leukemia and autism spectrum disorders, are performed by analyzing the gene expression. The results show that the proposed CR-based algorithm exhibits significantly higher stability and accuracy than the traditional classifiers, such as support vector machine algorithm.","keywords_author":null,"keywords_other":["SELECTION","PREDICTION","SPECTRUM DISORDERS","CANCER-DIAGNOSIS","MODEL","RECOGNITION","SIGNATURES"],"max_cite":0.0,"pub_year":2017.0,"sources":"['ieee', 'wos']","rawkeys":["recognition","model","cancer-diagnosis","signatures","prediction","spectrum disorders","selection"],"tags":["recognition","model","cancer diagnosis","prediction","spectrum disorders","signature","selection"]},{"p_id":25694,"title":"Scene Parsing with Integration of Parametric and Non-Parametric Models","abstract":"\u00a9 1992-2012 IEEE.We adopt convolutional neural networks (CNNs) to be our parametric model to learn discriminative features and classifiers for local patch classification. Based on the occurrence frequency distribution of classes, an ensemble of CNNs (CNN-Ensemble) are learned, in which each CNN component focuses on learning different and complementary visual patterns. The local beliefs of pixels are output by CNN-Ensemble. Considering that visually similar pixels are indistinguishable under local context, we leverage the global scene semantics to alleviate the local ambiguity. The global scene constraint is mathematically achieved by adding a global energy term to the labeling energy function, and it is practically estimated in a non-parametric framework. A large margin-based CNN metric learning method is also proposed for better global belief estimation. In the end, the integration of local and global beliefs gives rise to the class likelihood of pixels, based on which maximum marginal inference is performed to generate the label prediction maps. Even without any post-processing, we achieve the state-of-the-art results on the challenging SiftFlow and Barcelona benchmarks.","keywords_author":["CNN-Ensemble","Convolution Neural Network","Deep Learning","Global Scene Constraint","Local Ambiguity","Scene Parsing","Scene parsing","convolution neural network","CNN-ensemble","global scene constraint","local ambiguity","deep learning"],"keywords_other":["Deep learning","Local Ambiguity","RECOGNITION","CNN-Ensemble","Scene constraints","Convolution neural network","IMAGE FEATURES","Scene Parsing"],"max_cite":7.0,"pub_year":2016.0,"sources":"['ieee', 'scp', 'wos']","rawkeys":["image features","recognition","local ambiguity","convolution neural network","deep learning","scene parsing","global scene constraint","cnn-ensemble","scene constraints"],"tags":["image features","recognition","local ambiguity","scene parsing","machine learning","convolutional neural network","global scene constraint","cnn-ensemble","scene constraints"]},{"p_id":33888,"title":"Complex wavelet based quality assessment for AS-OCT images with application to Angle Closure Glaucoma diagnosis","abstract":"\u00a9 2016 Elsevier Ireland Ltd.Background and objectives: Angle closure disease in the eye can be detected using time-domain Anterior Segment Optical Coherence Tomography (AS-OCT). The Anterior Chamber (AC) characteristics can be quantified from AS-OCT image, which is dependent on the image quality at the image acquisition stage. To date, to the best of our knowledge there are no objective or automated subjective measurements to assess the quality of AS-OCT images. Methods: To address AS-OCT image quality assessment issue, we define a method for objective assessment of AS-OCT images using complex wavelet based local binary pattern features. These features are pooled using the Na\u00efve Bayes classifier to obtain the final quality parameter. To evaluate the proposed method, a subjective assessment has been performed by clinical AS-OCT experts, who graded the quality of AS-OCT images on a scale of good, fair, and poor. This was done based on the ability to identify the AC structures including the position of the scleral spur. Results: We compared the results of the proposed objective assessment with the subjective assessments. From this comparison, it is validated that the proposed objective assessment has the ability of differentiating the good and fair quality AS-OCT images for glaucoma diagnosis from the poor quality AS-OCT images. Conclusions: This proposed algorithm is an automated approach to evaluate the AS-OCT images with the intention for collecting of high quality data for further medical diagnosis. Our proposed quality index has the ability of automatic objective and quantitative assessment of AS-OCT image quality and this quality index is similar to glaucoma specialist.","keywords_author":["Angle closure glaucoma","Complex wavelets","Image quality assessment","Local binary pattern","Machine learning","Optical coherence tomography","Optical coherence tomography","Angle closure glaucoma","Complex wavelets","Local binary pattern","Image quality assessment","Machine learning"],"keywords_other":["Anterior segment optical coherence tomographies","Humans","Image quality assessment","Angle-closure glaucoma","Subjective measurements","NATURAL SCENE STATISTICS","OPTICAL COHERENCE TOMOGRAPHY","FEATURE-SELECTION","MECHANISM","Glaucoma, Angle-Closure","Subjective assessments","Complex wavelets","LOCAL BINARY PATTERNS","RECOGNITION","Tomography, Optical Coherence","Quantitative assessments","INFORMATION","CLASSIFICATION","Local binary patterns"],"max_cite":2.0,"pub_year":2016.0,"sources":"['ieee', 'scp', 'wos']","rawkeys":["glaucoma","feature-selection","classification","optical coherence","tomography","subjective measurements","optical coherence tomography","machine learning","image quality assessment","anterior segment optical coherence tomographies","local binary pattern","recognition","mechanism","complex wavelets","angle-closure glaucoma","humans","natural scene statistics","angle closure glaucoma","quantitative assessments","angle-closure","information","local binary patterns","subjective assessments"],"tags":["glaucoma","classification","optical coherence","tomography","subjective measurements","optical coherence tomography","machine learning","feature selection","image quality assessment","anterior segment optical coherence tomographies","recognition","mechanisms","complex wavelets","angle-closure glaucoma","humans","natural scene statistics","quantitative assessments","angle-closure","information","local binary patterns","subjective assessments"]},{"p_id":1159,"title":"Input Layer Regularization of Multilayer Feedforward Neural Networks","abstract":"Multilayer feedforward neural networks (MFNNs) have been widely used for classification or approximation of nonlinear mappings described by a data set consisting of input and output samples. In many MFNN applications, a common compressive sensing task is to find the redundant dimensions of the input data. The aim of a regularization technique presented in this paper is to eliminate the redundant dimensions and to achieve compression of the input layer. It is achieved by introducing an L1 or L1\/2 regularizer to the input layer weights training. As a comparison, in the existing references, a regularization method is usually applied to the hidden layer for a better representation of the dataset and sparsification of the network. Gradient-descent method is used for solving the resulting optimization problem. Numerical experiments including a simulated approximation problem and three classification problems (Monk, Sonar, and the MNIST data set) have been used to illustrate the algorithm.","keywords_author":["Multilayer feedforward neural network","autoencoder","compressive sensing","regularization of input layer","L-1 and L-1\/2 regularization","Multilayer feedforward neural network","autoencoder","compressive sensing","regularization of input layer","L\u2081 and L1\/2 regularization"],"keywords_other":["ALGORITHM","input layer regularization","dataset representation","CONVERGENCE","SMOOTHING L-1\/2 REGULARIZATION","gradient methods","compressive sensing","Feedforward neural networks","gradient-descent method","feedforward neural nets","Training","ATOMIC DECOMPOSITION","Cost function","RECOGNITION","compressed sensing","pattern classification","SELECTION","Nonhomogeneous media","MFNN","STEM","GRADIENT-METHOD","multilayer feedforward neural networks","Tools","LASSO","classification problems"],"max_cite":0.0,"pub_year":2017.0,"sources":"['wos', 'ieee']","rawkeys":["convergence","autoencoder","dataset representation","input layer regularization","gradient-method","regularization of input layer","nonhomogeneous media","stem","tools","smoothing l-1\/2 regularization","gradient methods","compressive sensing","feedforward neural networks","gradient-descent method","feedforward neural nets","algorithm","recognition","multilayer feedforward neural network","atomic decomposition","training","lasso","cost function","l\u2081 and l1\/2 regularization","compressed sensing","selection","pattern classification","mfnn","l-1 and l-1\/2 regularization","multilayer feedforward neural networks","classification problems"],"tags":["dataset representation","input layer regularization","regularization of input layer","nonhomogeneous media","stem","smoothing l-1\/2 regularization","tool","gradient methods","gradient descent method","compressive sensing","mathematics","algorithms","feedforward neural nets","recognition","atomic decomposition","neural networks","training","lasso","cost function","l\u2081 and l1\/2 regularization","selection","pattern classification","mfnn","auto encoders","l-1 and l-1\/2 regularization","multilayer feedforward neural networks","classification problems"]},{"p_id":1164,"title":"Discriminative Deep Metric Learning for Face and Kinship Verification","abstract":"This paper presents a new discriminative deep metric learning (DDML) method for face and kinship verification in wild conditions. While metric learning has achieved reasonably good performance in face and kinship verification, most existing metric learning methods aim to learn a single Mahalanobis distance metric to maximize the inter-class variations and minimize the intra-class variations, which cannot capture the nonlinear manifold where face images usually lie on. To address this, we propose a DDML method to train a deep neural network to learn a set of hierarchical nonlinear transformations to project face pairs into the same latent feature space, under which the distance of each positive pair is reduced and that of each negative pair is enlarged. To better use the commonality of multiple feature descriptors to make all the features more robust for face and kinship verification, we develop a discriminative deep multi-metric learning method to jointly learn multiple neural networks, under which the correlation of different features of each sample is maximized, and the distance of each positive pair is reduced and that of each negative pair is enlarged. Extensive experimental results show that our proposed methods achieve the acceptable results in both face and kinship verification.","keywords_author":["deep learning","deep metric learning","Face verification","kinship verification","multi-feature learning","Face verification","kinship verification","deep learning","deep metric learning","multi-feature learning","Face verification","kinship verification","deep learning","deep metric learning","multi-feature learning"],"keywords_other":["Measurement","Mahalanobis distances","Multi features","kinship verification","multiple feature descriptors","Neural networks","discriminative deep metric learning","DDML method","Feature extraction","Multiple neural networks","wild conditions","project face pairs","Machine learning","face recognition","Face","learning (artificial intelligence)","FEATURES","Training","Learning systems","Metric learning","RECOGNITION","WILD","face images","interclass variations","neural nets","Face Verification","correlation","Multiple feature descriptors","multiple neural networks","IDENTITY","Mahalanobis distance metric","deep neural network","latent feature space","hierarchical nonlinear transformations","face verification","Non-linear transformations"],"max_cite":5.0,"pub_year":2017.0,"sources":"['ieee', 'scp', 'wos']","rawkeys":["kinship verification","multiple feature descriptors","learning systems","discriminative deep metric learning","deep metric learning","features","multi features","machine learning","mahalanobis distance metric","wild conditions","project face pairs","face recognition","metric learning","ddml method","measurement","learning (artificial intelligence)","recognition","deep learning","multi-feature learning","neural networks","training","face images","interclass variations","face","neural nets","non-linear transformations","correlation","wild","multiple neural networks","deep neural network","identity","hierarchical nonlinear transformations","latent feature space","face verification","mahalanobis distances","feature extraction"],"tags":["kinship verification","interclass variation","multiple feature descriptors","convolutional neural network","learning systems","discriminative deep metric learning","deep metric learning","features","multi features","machine learning","mahalanobis distance metric","wild conditions","project face pairs","face recognition","metric learning","ddml method","measurement","recognition","multi-feature learning","neural networks","training","face images","face","non-linear transformations","correlation","wild","multiple neural networks","identity","hierarchical nonlinear transformations","latent feature space","face verification","mahalanobis distances","feature extraction"]},{"p_id":42134,"title":"Extroverts tweet differently from introverts in Weibo","abstract":"\u00a9 2018, The Author(s). As dominant factors driving human actions, personalities can be excellent indicators to predict the offline and online behavior of individuals. However, because of the great expense and inevitable subjectivity in questionnaires and surveys, it is challenging for conventional studies to explore the connection between personality and behavior and to gain insight in the context of a large number of individuals. Considering the increasingly important role of online social media in daily communications, we argue that the footprints of massive numbers of individuals, such as tweets on Weibo, can be used as a proxy to infer personality and further understand its function in shaping online human behavior. In this study, a map from self-reports of personalities to online profiles of 293 active users on Weibo is established to train a competent machine learning model, which then successfully identifies more than 7000 users as extroverts or introverts. Systematic comparison from the perspectives of tempo-spatial patterns, online activities, emotional expressions and attitudes to virtual honors show that extroverts indeed behave differently from introverts on Weibo. Our findings provide solid evidence to justify the methodology of employing machine learning to objectively study the personalities of a massive number of individuals and shed light on applications of probing personalities and corresponding behaviors solely through online profiles.","keywords_author":["Extraversion","Machine learning","Personality","Social media"],"keywords_other":["Personality","Online social medias","Extraversion","Machine learning models","Social media","Spatial patterns","Emotional expressions","Online activities"],"max_cite":0.0,"pub_year":2018.0,"sources":"['scp']","rawkeys":["online activities","emotional expressions","machine learning models","social media","machine learning","online social medias","personality","extraversion","spatial patterns"],"tags":["recognition","online activities","emotional expressions","machine learning models","social media","machine learning","online social medias","spatial patterns","personalizations"]},{"p_id":9369,"title":"How deep learning extracts and learns leaf features for plant classification","abstract":"Plant identification systems developed by computer vision researchers have helped botanists to recognize and identify unknown plant species more rapidly. Hitherto, numerous studies have focused on procedures or algorithms that maximize the use of leaf databases for plant predictive modeling, but this results in leaf features which are liable to change with different leaf data and feature extraction techniques. In this paper, we learn useful leaf features directly from the raw representations of input data using Convolutional Neural Networks (CNN), and gain intuition of the chosen features based on a Deconvolutional Network (DN) approach. We report somewhat unexpected results: (1) different orders of venation are the best representative features compared to those of outline shape, and (2) we observe multi-level representation in leaf data, demonstrating the hierarchical transformation of features from lower-level to higher-level abstraction, corresponding to species classes. We show that these findings fit with the hierarchical botanical definitions of leaf characters. Through these findings, we gained insights into the design of new hybrid feature extraction models which are able to further improve the discriminative power of plant classification systems. The source code and models are available at: https:\/\/github.comics-chan\/Deep-Plant. (C) 2017 Elsevier Ltd. All rights reserved.","keywords_author":["Deep learning","Feature visualisation","Plant recognition","Plant recognition","Deep learning","Feature visualisation"],"keywords_other":["Plant recognition","AUTOMATIC CLASSIFICATION","LEAVES","VENATION PATTERNS","Hybrid-feature extraction","Plant classification","RECOGNITION","IMAGES","Higher-level abstraction","Convolutional neural network","SHAPE","Discriminative power","CLASSIFIERS","Plant identification systems","Feature extraction techniques","TEXTURE"],"max_cite":17.0,"pub_year":2017.0,"sources":"['scp', 'wos']","rawkeys":["hybrid-feature extraction","feature extraction techniques","recognition","images","leaves","deep learning","automatic classification","discriminative power","feature visualisation","plant recognition","classifiers","shape","plant identification systems","texture","convolutional neural network","plant classification","venation patterns","higher-level abstraction"],"tags":["hybrid-feature extraction","feature extraction techniques","recognition","images","leaves","feature visualisation","automatic classification","discriminative power","machine learning","plant recognition","shape","plant identification systems","classifier","texture","convolutional neural network","plant classification","venation patterns","higher-level abstraction"]},{"p_id":9370,"title":"Deep learning","abstract":"Deep learning is a branch of machine learning that tries to model high-level abstractions of data using multiple layers of neurons consisting of complex structures or non-liner transformations. With the increase of the amount of data and the power of computation, neural networks with more complex structures have attracted widespread attention and been applied to various fields. This paper provides an overview of deep learning in neural networks including popular architecture models and training algorithms.","keywords_author":["Deep learning","neural networks","training"],"keywords_other":["REPRESENTATION","NEURAL-NETWORKS","RECOGNITION","MODEL"],"max_cite":121.0,"pub_year":2016.0,"sources":"['ieee', 'scp', 'wos']","rawkeys":["neural-networks","recognition","model","deep learning","neural networks","training","representation"],"tags":["recognition","model","neural networks","training","representation","machine learning"]},{"p_id":1185,"title":"Deep Feature Fusion for VHR Remote Sensing Scene Classification","abstract":"The rapid development of remote sensing technology allows us to get images with high and very high resolution (VHR). VHR imagery scene classification has become an important and challenging problem. In this paper, we introduce a framework for VHR scene understanding. First, the pretrained visual geometry group network (VGG-Net) model is proposed as deep feature extractors to extract informative features from the original VHR images. Second, we select the fully connected layers constructed by VGG-Net in which each layer is regarded as separated feature descriptors. And then we combine between them to construct final representation of the VHR image scenes. Third, discriminant correlation analysis (DCA) is adopted as feature fusion strategy to further refine the original features extracting from VGG-Net, which allows a more efficient fusion approach with small cost than the traditional feature fusion strategies. We apply our approach to three challenging data sets: 1) UC MERCED data set that contains 21 different areal scene categories with submeter resolution; 2) WHU-RS data set that contains 19 challenging scene categories with various resolutions; and 3) the Aerial Image data set that has a number of 10 000 images within 30 challenging scene categories with various resolutions. The experimental results demonstrate that our proposed method outperforms the state-of-the-art approaches. Using feature fusion technique achieves a higher accuracy than solely using the raw deep features. Moreover, the proposed method based on DCA fusion produces good informative features to describe the images scene with much lower dimension.","keywords_author":["Discriminant correlation analysis (DCA)","features fusion","scene classification","unsupervised features learning","Discriminant correlation analysis (DCA)","features fusion","scene classification","unsupervised features learning","Discriminant correlation analysis (DCA)","features fusion","scene classification","unsupervised features learning"],"keywords_other":["remote sensing","geophysical image processing","Scene understanding","feature fusion technique","VGG-Net model","Correlation","Feature extractor","image classification","Correlation analysis","feature fusion strategy","Feature extraction","Visualization","Machine learning","IMAGE CLASSIFICATION","remote sensing technology","Principal component analysis","Feature descriptors","Very high resolution","image fusion","Remote sensing technology","deep feature fusion","Aerial Image dataset","RECOGNITION","Scene classification","UC MERCED dataset","visual geometry group network","feature extractor","Remote sensing","FEATURE-EXTRACTION","Image resolution","FEATURE-LEVEL FUSION","State-of-the-art approach","discriminant correlation analysis","VHR remote sensing scene classification","VHR image scenes","feature extraction","very high resolution imagery scene classification","VHR imagery scene classification","CONVOLUTIONAL NEURAL-NETWORKS"],"max_cite":13.0,"pub_year":2017.0,"sources":"['scp', 'wos', 'ieee']","rawkeys":["aerial image dataset","remote sensing","vgg-net model","geophysical image processing","feature fusion technique","scene classification","convolutional neural-networks","unsupervised features learning","image classification","visualization","correlation analysis","feature fusion strategy","machine learning","remote sensing technology","image resolution","principal component analysis","recognition","feature-level fusion","image fusion","feature descriptors","feature-extraction","deep feature fusion","vhr imagery scene classification","scene understanding","visual geometry group network","feature extractor","correlation","features fusion","vhr remote sensing scene classification","vhr image scenes","discriminant correlation analysis","feature extraction","very high resolution","state-of-the-art approach","very high resolution imagery scene classification","discriminant correlation analysis (dca)","uc merced dataset"],"tags":["aerial image dataset","remote sensing","vgg-net model","geophysical image processing","feature fusion technique","scene classification","convolutional neural network","uc merced data set","image classification","visualization","correlation analysis","feature fusion strategy","machine learning","remote sensing technology","feature level fusion","feature fusion","image resolution","principal component analysis","recognition","image fusion","vhr imagery scene classification","feature descriptors","deep feature fusion","scene understanding","visual geometry group network","feature extractor","correlation","unsupervised feature learning","vhr remote sensing scene classification","vhr image scenes","feature extraction","very high resolution","state-of-the-art approach","very high resolution imagery scene classification","discriminant correlation analysis (dca)"]},{"p_id":9397,"title":"Learning Deep Generative Models","abstract":"Building intelligent systems that are capable of extracting high-level representations from high-dimensional sensory data lies at the core of solving many artificial intelligence-related tasks, including object recognition, speech perception, and language understanding. Theoretical and biological arguments strongly suggest that building such systems requires models with deep architectures that involve many layers of nonlinear processing. In this article, we review several popular deep learning models, including deep belief networks and deep Boltzmann machines. We show that (a) these deep generative models, which contain many layers of latent variables and millions of parameters, can be learned efficiently, and (b) the learned high-level feature representations can be successfully applied in many application domains, including visual object recognition, information retrieval, classification, and regression tasks.","keywords_author":["deep learning","deep belief networks","deep Boltzmann machines","graphical models"],"keywords_other":["NEURAL-NETWORKS","RECOGNITION","BOLTZMANN MACHINES"],"max_cite":22.0,"pub_year":2015.0,"sources":"['ieee', 'wos']","rawkeys":["neural-networks","recognition","deep learning","boltzmann machines","deep boltzmann machines","graphical models","deep belief networks"],"tags":["recognition","neural networks","boltzmann machines","machine learning","deep boltzmann machines","graphical model","deep belief networks"]},{"p_id":9400,"title":"Deep learning in bioinformatics","abstract":"In the era of big data, transformation of biomedical big data into valuable knowledge has been one of the most important challenges in bioinformatics. Deep learning has advanced rapidly since the early 2000s and now demonstrates state-of-theart performance in various fields. Accordingly, application of deep learning in bioinformatics to gain insight from data has been emphasized in both academia and industry. Here, we review deep learning in bioinformatics, presenting examples of current research. To provide a useful and comprehensive perspective, we categorize research both by the bioinformatics domain (i.e. omics, biomedical imaging, biomedical signal processing) and deep learning architecture (i.e. deep neural networks, convolutional neural networks, recurrent neural networks, emergent architectures) and present brief descriptions of each study. Additionally, we discuss theoretical and practical issues of deep learning in bioinformatics and suggest future research directions. We believe that this review will provide valuable insights and serve as a starting point for researchers to apply deep learning approaches in their bioinformatics studies.","keywords_author":["bioinformatics","biomedical imaging","biomedical signal processing","deep learning","machine learning","neural network","omics","deep learning","neural network","machine learning","bioinformatics","omics","biomedical imaging","biomedical signal processing"],"keywords_other":["EEG","Neural Networks (Computer)","Humans","ALGORITHM","CLASSIFICATION","SEQUENCE","Machine Learning","RECURRENT NEURAL-NETWORKS","MODEL","RECOGNITION","SECONDARY STRUCTURE PREDICTION","BINDING","Computational Biology","CONVOLUTIONAL NETWORKS"],"max_cite":36.0,"pub_year":2017.0,"sources":"['ieee', 'scp', 'wos']","rawkeys":["neural network","biomedical signal processing","sequence","classification","secondary structure prediction","recurrent neural-networks","machine learning","convolutional networks","algorithm","neural networks (computer)","recognition","deep learning","eeg","humans","binding","biomedical imaging","omics","model","computational biology","bioinformatics"],"tags":["omics","recognition","model","biomedical signal processing","neural networks","computational biology","machine learning","eeg","humans","binding","sequence","classification","convolutional neural network","bioinformatics","algorithms","secondary structure prediction","biomedical imaging"]},{"p_id":33982,"title":"Fine-grained representation learning in convolutional autoencoders","abstract":"\u00a9 2016 SPIE and IS&T.Convolutional autoencoders (CAEs) have been widely used as unsupervised feature extractors for high-resolution images. As a key component in CAEs, pooling is a biologically inspired operation to achieve scale and shift invariances, and the pooled representation directly affects the CAEs' performance. Fine-grained pooling, which uses small and dense pooling regions, encodes fine-grained visual cues and enhances local characteristics. However, it tends to be sensitive to spatial rearrangements. In most previous works, pooled features were obtained by empirically modulating parameters in CAEs. We see the CAE as a whole and propose a fine-grained representation learning law to extract better fine-grained features. This representation learning law suggests two directions for improvement. First, we probabilistically evaluate the discrimination-invariance tradeoff with fine-grained granularity in the pooled feature maps, and suggest the proper filter scale in the convolutional layer and appropriate whitening parameters in preprocessing step. Second, pooling approaches are combined with the sparsity degree in pooling regions, and we propose the preferable pooling approach. Experimental results on two independent benchmark datasets demonstrate that our representation learning law could guide CAEs to extract better fine-grained features and performs better in multiclass classification task. This paper also provides guidance for selecting appropriate parameters to obtain better fine-grained representation in other convolutional neural networks.","keywords_author":["convolutional neural networks","deep learning","discrimination-invariance tradeoff","fine-grained representation","Pooling","sparse autoencoder","unsupervised learning","Pooling","fine-grained representation","convolutional neural networks","sparse autoencoder","discrimination-invariance tradeoff","unsupervised learning","deep learning"],"keywords_other":["Deep learning","Fine grained","POOLING OPERATIONS","discrimination-invariance tradeoff","NEURAL-NETWORKS","RECOGNITION","T-SNE","Convolutional neural network","Auto encoders","Pooling"],"max_cite":2.0,"pub_year":2016.0,"sources":"['scp', 'wos']","rawkeys":["neural-networks","recognition","convolutional neural networks","deep learning","t-sne","fine grained","discrimination-invariance tradeoff","auto encoders","pooling operations","unsupervised learning","convolutional neural network","pooling","fine-grained representation","sparse autoencoder"],"tags":["recognition","neural networks","t-sne","fine grained","discrimination-invariance tradeoff","auto encoders","machine learning","pooling operations","stacked autoencoders","unsupervised learning","convolutional neural network","pooling","fine-grained representation"]},{"p_id":25809,"title":"Objective-analytical measures of workload \u2013the third pillar of workload triangulation?","abstract":"\u00a9 Springer International Publishing Switzerland 2015. The ability to assess operator workload is important for dynamically allocating tasks in a way that allows efficient and effective goal completion. For over fifty years, human factors professionals have relied upon self-reported measures of workload. However, these subjective-empirical measures have limited use for real-time applications because they are often collected only at the completion of the activity. In contrast, objective-empirical measurements of workload, such as physiological data, can be recorded continuously, and provide frequently-updated information over the course of a trial. Linking the low-sample-rate subjective-empirical measurement to the high-sample-rate objective-empirical measurements poses a significant challenge. While the series of objective-empirical measurements could be down\u2013sampled or averaged over a longer time period to match the subjective-empirical sample rate, this process discards potentially relevant information, and may produce meaningless values for certain types of physiological data. This paper demonstrates the technique of using an objective-analytical measurement produced by mathematical models of workload to bridge the gap between subjective-empirical and objective-empirical measures. As a proof of concept, we predicted operator workload from physiological data using VACP, an objective-analytical measure, which was validated against NASA-TLX scores. Strong predictive results pave the way to use the objective-empirical measures in real-time augmentation (such as dynamic task allocation) to improve operator performance.","keywords_author":["IMPRINT","Machine learning","VACP","Workload measurement"],"keywords_other":["Real-time application","IMPRINT","VACP","Updated informations","Workload measurements","Operator performance","Empirical measurement","Dynamic task allocation"],"max_cite":7.0,"pub_year":2015.0,"sources":"['scp']","rawkeys":["workload measurement","imprint","operator performance","real-time application","machine learning","updated informations","empirical measurement","vacp","workload measurements","dynamic task allocation"],"tags":["recognition","operation performance","real-time application","machine learning","updated informations","empirical measurement","vacp","workload measurements","dynamic task allocation"]},{"p_id":9425,"title":"Understanding Deep Representations Learned in Modeling Users Likes","abstract":"Automatically understanding and discriminating different users' liking for an image is a challenging problem. This is because the relationship between image features (even semantic ones extracted by existing tools, viz., faces, objects, and so on) and users' likes is non-linear, influenced by several subtle factors. This paper presents a deep bi-modal knowledge representation of images based on their visual content and associated tags (text). A mapping step between the different levels of visual and textual representations allows for the transfer of semantic knowledge between the two modalities. Feature selection is applied before learning deep representation to identify the important features for a user to like an image. The proposed representation is shown to be effective in discriminating users based on images they like and also in recommending images that a given user likes, outperforming the state-of-the-art feature representations by similar to 15%- 20%. Beyond this test-set performance, an attempt is made to qualitatively understand the representations learned by the deep architecture used to model user likes.","keywords_author":["Deep representations","user likes","image recommendation","semantic structures"],"keywords_other":["IMAGE RETRIEVAL","RECOGNITION","POSE ESTIMATION","PERSONALITY"],"max_cite":4.0,"pub_year":2016.0,"sources":"['ieee', 'wos']","rawkeys":["pose estimation","deep representations","recognition","user likes","image retrieval","personality","semantic structures","image recommendation"],"tags":["pose estimation","deep representation","recognition","user likes","image retrieval","semantic structures","personalizations","image recommendation"]},{"p_id":9427,"title":"Designing for deeper learning in a blended computer science course for middle school students","abstract":"The focus of this research was to create and test an introductory computer science course for middle school. Titled \"Foundations for Advancing Computational Thinking\" (FACT), the course aims to prepare and motivate middle school learners for future engagement with algorithmic problem solving. FACT was also piloted as a seven-week course on Stanford's OpenEdX MOOC platform for blended in-class learning. Unique aspects of FACT include balanced pedagogical designs that address the cognitive, interpersonal, and intrapersonal aspects of \"deeper learning\"; a focus on pedagogical strategies for mediating and assessing for transfer from block-based to text-based programming; curricular materials for remedying misperceptions of computing; and \"systems of assessments\" (including formative and summative quizzes and tests, directed as well as open-ended programming assignments, and a transfer test) to get a comprehensive picture of students' deeper computational learning. Empirical investigations, accomplished over two iterations of a design-based research effort with students (aged 11- 14 years) in a public school, sought to examine student understanding of algorithmic constructs, and how well students transferred this learning from Scratch to text-based languages. Changes in student perceptions of computing as a discipline were measured. Results and mixed-method analyses revealed that students in both studies (1) achieved substantial learning gains in algorithmic thinking skills, (2) were able to transfer their learning from Scratch to a text-based programming context, and (3) achieved significant growth toward a more mature understanding of computing as a discipline. Factor analyses of prior computing experience, multivariate regression analyses, and qualitative analyses of student projects and artifact-based interviews were conducted to better understand the factors affecting learning outcomes. Prior computing experiences (as measured by a pretest) and math ability were found to be strong predictors of learning outcomes.","keywords_author":["blended learning","computational thinking","computer science education","curriculum design","deeper learning","design-based research","K-12 CS curriculum","middle school computer science","pedagogical content knowledge","perceptions of computing","preparation for future learning","transfer","computational thinking","computer science education","K-12 CS curriculum","middle school computer science","curriculum design","pedagogical content knowledge","design-based research","deeper learning","blended learning","transfer","preparation for future learning","perceptions of computing"],"keywords_other":null,"max_cite":22.0,"pub_year":2015.0,"sources":"['scp', 'wos']","rawkeys":["pedagogical content knowledge","transfer","computer science education","deeper learning","k-12 cs curriculum","computational thinking","perceptions of computing","middle school computer science","blended learning","curriculum design","design-based research","preparation for future learning"],"tags":["pedagogical content knowledge","recognition","computer science education","deeper learning","k-12 cs curriculum","computational thinking","perceptions of computing","middle school computer science","blended learning","curriculum design","design-based research","preparation for future learning"]},{"p_id":42203,"title":"Web traffic anomaly detection using C-LSTM neural networks","abstract":"\u00a9 2018 Elsevier LtdWeb traffic refers to the amount of data that is sent and received by people visiting online websites. Web traffic anomalies represent abnormal changes in time series traffic, and it is important to perform detection quickly and accurately for the efficient operation of complex computer networks systems. In this paper, we propose a C-LSTM neural network for effectively modeling the spatial and temporal information contained in traffic data, which is a one-dimensional time series signal. We also provide a method for automatically extracting robust features of spatial-temporal information from raw data. Experiments demonstrate that our C-LSTM method can extract more complex features by combining a convolutional neural network (CNN), long short-term memory (LSTM), and deep neural network (DNN). The CNN layer is used to reduce the frequency variation in spatial information; the LSTM layer is suitable for modeling time information; and the DNN layer is used to map data into a more separable space. Our C-LSTM method also achieves nearly perfect anomaly detection performance for web traffic data, even for very similar signals that were previously considered to be very difficult to classify. Finally, the C-LSTM method outperforms other state-of-the-art machine learning techniques on Yahoo's well-known Webscope S5 dataset, achieving an overall accuracy of 98.6% and recall of 89.7% on the test dataset.","keywords_author":["Anomaly detection","C-LSTM","Deep learning","Web traffic","Web traffic","Anomaly detection","Deep learning","C-LSTM"],"keywords_other":["Convolutional Neural Networks (CNN)","Anomaly detection","Computer networks systems","RECOGNITION","Temporal information","One-dimensional time series","Web traffic","Spatial informations","Machine learning techniques"],"max_cite":0.0,"pub_year":2018.0,"sources":"['ieee', 'scp', 'wos']","rawkeys":["computer networks systems","one-dimensional time series","recognition","web traffic","anomaly detection","deep learning","c-lstm","machine learning techniques","spatial informations","temporal information","convolutional neural networks (cnn)"],"tags":["computer networks systems","one-dimensional time series","recognition","web traffic","anomaly detection","c-lstm","machine learning","machine learning techniques","convolutional neural network","temporal information","spatial informations"]},{"p_id":42208,"title":"Text detection in natural scene images based on color prior guided MSER","abstract":"\u00a9 2018 Elsevier B.V. In this paper, we focus on text detection in natural scene images which is conducive to content-based wild image analysis and understanding. This task is still an open problem and usually includes two key issues: text candidate extraction and verification. For text candidate extraction, we introduce a color prior to guide the character candidate extraction by Maximally Stable Extremal Region (MSER). The principle of color prior acquirement is to obtain stroke-like textures with modified Stroke Width Transform (SWT), which is based on segmented edges. For text verification, the ideology of deep learning is adopted to distinguish text\/non-text candidates. To improve classification accuracy, the results of specific task CNNs are fused. The proposed framework is evaluated on the ICDAR 2013 Robust Reading Competition database. It achieves F-score at 85.87%, which are superior over several state-of-the-art text detection methods.","keywords_author":["Deep learning","Maximally stable extremal region","Stroke width transform","Text candidate extraction","Text detection","Text verification","Text detection","Text candidate extraction","Maximally stable extremal region","Stroke width transform","Text verification","Deep learning"],"keywords_other":["Stroke widths","LOCALIZATION","Text detection","Content-based","Text detection in natural scenes","VISION","State of the art","RECOGNITION","SEGMENTATION","ADABOOST","Specific tasks","Maximally Stable Extremal Regions","Classification accuracy"],"max_cite":0.0,"pub_year":2018.0,"sources":"['scp', 'wos']","rawkeys":["text verification","content-based","recognition","maximally stable extremal regions","localization","segmentation","deep learning","stroke widths","classification accuracy","maximally stable extremal region","specific tasks","state of the art","text detection","vision","stroke width transform","text candidate extraction","text detection in natural scenes","adaboost"],"tags":["text verification","content-based","recognition","maximally stable extremal regions","localization","segmentation","stroke widths","text detection","classification accuracy","specific tasks","machine learning","state of the art","vision","stroke width transform","text candidate extraction","text detection in natural scenes","adaboost"]},{"p_id":74986,"title":"Sickle cell disease diagnosis based on spatio-temporal cell dynamics analysis using 3D printed shearing digital holographic microscopy","abstract":"We present a spatio-temporal analysis of cell membrane fluctuations to distinguish healthy patients from patients with sickle cell disease. A video hologram containing either healthy red blood cells (h-RBCs) or sickle cell disease red blood cells (SCD-RBCs) was recorded using a low-cost, compact, 3D printed shearing interferometer. Reconstructions were created for each hologram frame (time steps), forming a spatio-temporal data cube. Features were extracted by computing the standard deviations and the mean of the height fluctuations over time and for every location on the cell membrane, resulting in two-dimensional standard deviation and mean maps, followed by taking the standard deviations of these maps. The optical flow algorithm was used to estimate the apparent motion fields between subsequent frames (reconstructions). The standard deviation of the magnitude of the optical flow vectors across all frames was then computed. In addition, seven morphological cell (spatial) features based on optical path length were extracted from the cells to further improve the classification accuracy. A random forest classifier was trained to perform cell identification to distinguish between SCD-RBCs and h-RBCs. To the best of our knowledge, this is the first report of machine learning assisted cell identification and diagnosis of sickle cell disease based on cell membrane fluctuations and morphology using both spatio-temporal and spatial analysis. (C) 2018 Optical Society of America under the terms of the OSA Open Access Publishing Agreement","keywords_author":null,"keywords_other":["COMPACT","RED-BLOOD-CELLS","RECOGNITION","IDENTIFICATION"],"max_cite":0.0,"pub_year":2018.0,"sources":"['wos']","rawkeys":["recognition","identification","compact","red-blood-cells"],"tags":["red blood cell","compactness","recognition","identification"]},{"p_id":17643,"title":"Design of a visual perception model with edge-adaptive Gabor filter and support vector machine for traffic sign detection","abstract":"Traffic sign detection is a useful application for driving assistance systems, and it is necessary to accurately detect traffic signs before they can be identified. Sometimes, however, it is difficult to detect traffic sign, which may be obscured by other objects or affected by illumination or lightning reflections. Most previous work on this topic has been based on region of interest analysis using the color information of traffic signs. Although this provides a simple way to segment signs, this approach is weak when a sign is affected by illumination or its own color information is distorted. To overcome this, this paper introduces a robust traffic detection framework for cluttered scenes or complex city views that does not use color information. Moreover, the proposed method can detect traffic sign in the night. We establish an edge-adaptive Gabor function, which is derived from human visual perception. It is an enhanced version of the original Gabor filter, and filters out unnecessary information to provide robust recognition. It decomposes the directional information of objects and reflects specific shapes of traffic signs. Once the extracted feature is obtained, a support vector machine detects the traffic sign. Applying scale-space theory, it is possible to resolve the scaling problem of the objects that we want to find. Our system shows robust performance in traffic sign detection, and experiments on real-world scenes confirmed its properties. \u00a9 2012 Elsevier Ltd. All rights reserved.","keywords_author":["Ensemble learning","Gabor filter","Generalization","Machine learning","Scale-space","Support vector machine","Traffic sign detection"],"keywords_other":["Real-world","Scale-space","Region-of-interest analysis","Generalization","Robust recognition","Color information","Scale-space theory","Traffic sign detection","Scaling problem","Gabor function","Human visual perception","Cluttered scenes","Ensemble learning","Visual perception models","And filters","Robust performance","Directional information","Traffic detection","Driving assistance systems"],"max_cite":28.0,"pub_year":2013.0,"sources":"['scp', 'wos']","rawkeys":["traffic detection","scaling problem","cluttered scenes","traffic sign detection","real-world","support vector machine","driving assistance systems","machine learning","human visual perception","generalization","region-of-interest analysis","and filters","color information","scale-space theory","robust performance","scale-space","robust recognition","gabor filter","directional information","visual perception models","ensemble learning","gabor function"],"tags":["traffic detection","scaling problem","cluttered scenes","traffic sign detection","real-world","driving assistance systems","machine learning","human visual perception","region-of-interest analysis","and filters","recognition","color information","scale-space theory","robust performance","scale-space","robust recognition","gabor filter","directional information","visual perception models","ensemble learning","gabor function"]},{"p_id":74988,"title":"Automatic multicell identification using a compact lensless single and double random phase encoding system","abstract":"We investigate the use of compact, lensless, single random phase encoding (SRPE) and double random phase encoding (DRPE) systems for automatic cell identification when multiple cells, either of the same or mixed classes, are in the field of view. A microscope glass slide containing the sample is inputted into the single or double random phase encoding system, which is then illuminated by a coherent or partially coherent light source generating a unique opto-biological signature (OBS) that is captured by an image sensor. Statistical features such as mean, standard deviation, skewness, kurtosis, entropy, and Pearson's correlation coefficient are extracted from the OBSs and used for cell identification with the random forest classifier. With the exception of the correlation coefficient, all features were extracted in both the spatial and frequency domains. Experiments are performed with single random phase encoding and double random phase encoding, and system analysis is presented to show the robustness and classification accuracy of the random phase encoding cell identification systems. The proposed systems are compact, as they are lensless and do not have spatial frequency bandwidth limitations due to the numerical aperture of a microscope objective lens. We demonstrate that cell identification is possible using both the SRPE and DRPE systems. While DRPE systems have been extensively used for image encryption, to the best of our knowledge, this is the first report on using DRPE for automated cell identification. (C) 2018 Optical Society of America","keywords_author":null,"keywords_other":["SUPERRESOLUTION","LIVING CELLS","MALARIA","VISUALIZATION","DIGITAL HOLOGRAPHIC MICROSCOPY","RECOGNITION","BIOLOGICAL MICROORGANISMS"],"max_cite":0.0,"pub_year":2018.0,"sources":"['wos']","rawkeys":["biological microorganisms","recognition","superresolution","living cells","malaria","visualization","digital holographic microscopy"],"tags":["biological microorganisms","recognition","sparse representation","living cells","malaria","visualization","digital holographic microscopy"]},{"p_id":17647,"title":"Improving generalization performance in co-evolutionary learning","abstract":"Recently, the generalization framework in co-evolutionary learning has been theoretically formulated and demonstrated in the context of game-playing. Generalization performance of a strategy (solution) is estimated using a collection of random test strategies (test cases) by taking the average game outcomes, with confidence bounds provided by Chebyshev's theorem. Chebyshev's bounds have the advantage that they hold for any distribution of game outcomes. However, such a distribution-free framework leads to unnecessarily loose confidence bounds. In this paper, we have taken advantage of the near-Gaussian nature of average game outcomes and provided tighter bounds based on parametric testing. This enables us to use small samples of test strategies to guide and improve the co-evolutionary search. We demonstrate our approach in a series of empirical studies involving the iterated prisoner's dilemma (IPD) and the more complex Othello game in a competitive co-evolutionary learning setting. The new approach is shown to improve on the classical co-evolutionary learning in that we obtain increasingly higher generalization performance using relatively small samples of test strategies. This is achieved without large performance fluctuations typical of the classical approach. The new approach also leads to faster co-evolutionary search where we can strictly control the condition (sample sizes) under which the speedup is achieved (not at the cost of weakening precision in the estimates). \u00a9 2011 IEEE.","keywords_author":["Co-evolutionary learning","evolutionary computation","generalization","iterated prisoner's dilemma","machine learning","Othello"],"keywords_other":["Evolutionary computations","Machine-learning","generalization","Othello","Co-evolutionary learning","Iterated prisoner's dilemma"],"max_cite":28.0,"pub_year":2012.0,"sources":"['scp']","rawkeys":["co-evolutionary learning","othello","machine learning","generalization","machine-learning","evolutionary computation","evolutionary computations","iterated prisoner's dilemma"],"tags":["co-evolutionary learning","recognition","othello","machine learning","evolutionary computation","iterated prisoner's dilemma"]},{"p_id":17651,"title":"Automation for underwater mine recognition: Current trends & future strategy","abstract":"The purpose of this paper is to define the vision and future strategy for advancing the use of automation in underwater mine recognition. The technical portion of this strategy is founded on the principle of adapting the automation in situ based on a highly variable environment \/ context and the occasional availability of the human operator. To frame this strategy, a survey of past and current algorithm development for underwater mine recognition is presented and includes a detailed description on adaptive algorithms. This discussion is motivated by illustrating the extreme variability in the underwater environment and that performance estimation techniques are now emerging that are capable of quantifying these variations in situ. It is the in situ linkage of performance estimation with adaptive recognition that forms one of the key technological enablers of this future strategy. The non-technical portion of this strategy is centered on enabling an effective human-machine team. Enabling this teaming relationship involves both gaining trust and establishing an overall support system that is amenable to such human-machine interactions. Aspects of trust include both individual trust and institutional trust, and a path for gaining both is discussed. Overall aspects of the support system are highlighted and include standards for data and interoperability, network-centric software architectures, and issues in proliferating knowledge that is learned in situ by multiple distributed algorithms. This paper concludes with an articulation of several important and timely research questions concerning automation for underwater mine recognition. \u00a9 2011 SPIE.","keywords_author":["Adaptive","Automation","Machine learning","Mine countermeasures","Mine warfare","Performance estimation","Recognition","Standards","Trust","Underwater perception"],"keywords_other":["Adaptive","Performance estimation","Recognition","Machine-learning","Mine countermeasures","Trust","Mine warfare","Underwater perception"],"max_cite":28.0,"pub_year":2011.0,"sources":"['scp']","rawkeys":["recognition","standards","machine learning","mine warfare","trust","automation","mine countermeasures","machine-learning","adaptive","performance estimation","underwater perception"],"tags":["recognition","automated","standards","machine learning","mine warfare","trust","mine countermeasures","underwater perception","adaptation","performance estimation"]},{"p_id":1277,"title":"Nonlinear Deep Kernel Learning for Image Annotation","abstract":"Multiple kernel learning (MKL) is a widely used technique for kernel design. Its principle consists in learning, for a given support vector classifier, the most suitable convex (or sparse) linear combination of standard elementary kernels. However, these combinations are shallow and often powerless to capture the actual similarity between highly semantic data, especially for challenging classification tasks, such as image annotation. In this paper, we redefine multiple kernels using deep multi-layer networks. In this new contribution, a deep multiple kernel is recursively defined as a multi-layered combination of nonlinear activation functions, each one involves a combination of several elementary or intermediate kernels, and results into a positive semi-definite deep kernel. We propose four different frameworks in order to learn the weights of these networks: supervised, unsupervised, kernel-based semi-supervised, and Laplacian-based semi-supervised. When plugged into support vector machines, the resulting deep kernel networks show clear gain, compared with several shallow kernels for the task of image annotation. Extensive experiments and analysis on the challenging ImageCLEF photo annotation benchmark, the COREL5k database, and the Banana data set validate the effectiveness of the proposed method.","keywords_author":["deep learning","image annotation","Multiple kernel learning","semi-supervised learning","Multiple kernel learning","image annotation","deep learning","semi-supervised learning","Multiple kernel learning","image annotation","deep learning","semi-supervised learning"],"keywords_other":["supervised learning","ImageCLEF photo annotation benchmark","Support vector machines","Classification tasks","Linear combinations","RETRIEVAL","Kernel","Multi-layer network","Support vector classifiers","kernel-based semi-supervised learning","Feature extraction","nonlinear deep kernel learning","Semi- supervised learning","Visualization","Nonlinear activation functions","FEATURES","Training","WORDS","image retrieval","MODEL","RECOGNITION","unsupervised learning","positive semi-definite deep kernel","COREL5k database","Image annotation","deep multilayer networks","Semantics","CLASSIFICATION","Standards","support vector classifier","Multiple Kernel Learning","support vector machines","Laplacian-based semi-supervised learning","PICTURES","image annotation","Banana data set"],"max_cite":6.0,"pub_year":2017.0,"sources":"['ieee', 'scp', 'wos']","rawkeys":["supervised learning","linear combinations","classification tasks","kernel","classification","visualization","kernel-based semi-supervised learning","features","nonlinear deep kernel learning","semi-supervised learning","support vector classifiers","nonlinear activation functions","words","recognition","semi- supervised learning","multi-layer network","deep learning","semantics","training","image retrieval","banana data set","unsupervised learning","laplacian-based semi-supervised learning","positive semi-definite deep kernel","corel5k database","model","deep multilayer networks","imageclef photo annotation benchmark","pictures","standards","multiple kernel learning","support vector classifier","feature extraction","retrieval","support vector machines","image annotation"],"tags":["supervised learning","linear combinations","classification tasks","kernel","classification","visualization","kernel-based semi-supervised learning","features","nonlinear deep kernel learning","machine learning","semi-supervised learning","support vector classifiers","nonlinear activation functions","words","recognition","multi-layer network","semantics","training","image retrieval","banana data set","unsupervised learning","laplacian-based semi-supervised learning","positive semi-definite deep kernel","corel5k database","model","deep multilayer networks","imageclef photo annotation benchmark","pictures","standards","multiple kernel learning","feature extraction","retrieval","image annotation"]},{"p_id":9469,"title":"Stock prediction using deep learning","abstract":"Stock market is considered chaotic, complex, volatile and dynamic. Undoubtedly, its prediction is one of the most challenging tasks in time series forecasting. Moreover existing Artificial Neural Network (ANN) approaches fail to provide encouraging results. Meanwhile advances in machine learning have presented favourable results for speech recognition, image classification and language processing. Methods applied in digital signal processing can be applied to stock data as both are time series. Similarly, learning outcome of this paper can be applied to speech time series data. Deep learning for stock prediction has been introduced in this paper and its performance is evaluated on Google stock price multimedia data (chart) from NASDAQ. The objective of this paper is to demonstrate that deep learning can improve stock market forecasting accuracy. For this, (2D)(2)PCA + Deep Neural Network (DNN) method is compared with state of the art method 2-Directional 2-Dimensional Principal Component Analysis (2D)(2)PCA + Radial Basis Function Neural Network (RBFNN). It is found that the proposed method is performing better than the existing method RBFNN with an improved accuracy of 4.8% for Hit Rate with a window size of 20. Also the results of the proposed model are compared with the Recurrent Neural Network (RNN) and it is found that the accuracy for Hit Rate is improved by 15.6%. The correlation coefficient between the actual and predicted return for DNN is 17.1% more than RBFNN and it is 43.4% better than RNN.","keywords_author":["(2D)2PCA","Deep Learning","Multimedia","Neural Network","Radial Basis Function Neural Network","Regularization","Stock Prediction","Deep Learning","Stock Prediction","Neural Network","(2D)(2)PCA","Radial Basis Function Neural Network","Regularization","Multimedia"],"keywords_other":["Deep learning","Stock predictions","Radial basis function neural networks","(2D)2PCA","Regularization","Multimedia","NEURAL-NETWORKS","RECOGNITION","TIME","UNIVERSAL APPROXIMATORS","BELIEF NETWORKS"],"max_cite":1.0,"pub_year":2017.0,"sources":"['ieee', 'scp', 'wos']","rawkeys":["neural-networks","neural network","recognition","(2d)(2)pca","(2d)2pca","deep learning","time","radial basis function neural network","radial basis function neural networks","stock prediction","universal approximators","multimedia","belief networks","regularization","stock predictions"],"tags":["principal component analysis","recognition","neural networks","(2d)2pca","machine learning","time","radial basis function neural networks","universal approximators","multimedia","belief networks","regularization","stock predictions"]},{"p_id":9471,"title":"Encoding Voxels with Deep Learning","abstract":null,"keywords_author":null,"keywords_other":["RECOGNITION","AREA","CORTEX"],"max_cite":3.0,"pub_year":2015.0,"sources":"['wos']","rawkeys":["area","cortex","recognition"],"tags":["area","cortex","recognition"]},{"p_id":34063,"title":"Innovative personality-based digital services","abstract":"Since the advent of social media, human-internet interaction has changed dramatically towards greater individual characteristic-based services. Since personality traits are the most stable behavioral dispositions of an individual, it is surprising that the digital service industry has not focused its attention on personality-based services. Consequently the value creation potential of personality-based services is currently largely ignored. That is why in this paper I demonstrate that social media data contains fruitful information about a user's personality which in turn can be used for novel personality-based services, e.g. in marketing and recruiting.","keywords_author":["Agreeableness","Big five","C5.0","Conscientiousness","Emotional stability","Extraversion","Five factor model","Machine learning","Online social networks","Openness to experience","Personality mining service","Predictive analytics","Random forest","Social media","Xing"],"keywords_other":["Emotional stabilities","Agreeableness","Random forests","Extraversion","Personality minings","On-line social networks","C5.0","Social media","Conscientiousness","Big five","Xing","Five-Factor Model","Openness to experience"],"max_cite":2.0,"pub_year":2016.0,"sources":"['scp']","rawkeys":["online social networks","conscientiousness","five-factor model","five factor model","random forests","big five","random forest","c5.0","emotional stabilities","xing","machine learning","personality mining service","emotional stability","personality minings","openness to experience","on-line social networks","social media","agreeableness","predictive analytics","extraversion"],"tags":["on-line social networks","c5.0","online social networks","recognition","conscientiousness","emotional stabilities","xing","agreeableness","predictive analytics","social media","five-factor model","machine learning","personality mining service","random forests","big five","personality minings","openness to experience"]},{"p_id":1308,"title":"Learning deep sharable and structural detectors for face alignment","abstract":"Face alignment aims at localizing multiple facial landmarks for a given facial image, which usually suffers from large variances of diverse facial expressions, aspect ratios and partial occlusions, especially when face images were captured in wild conditions. Conventional face alignment methods extract local features and then directly concatenate these features for global shape regression. Unlike these methods which cannot explicitly model the correlation of neighbouring landmarks and motivated by the fact that individual landmarks are usually correlated, we propose a deep sharable and structural detectors (DSSD) method for face alignment. To achieve this, we firstly develop a structural feature learning method to explicitly exploit the correlation of neighbouring landmarks, which learns to cover semantic information to disambiguate the neighbouring landmarks. Moreover, our model selectively learns a subset of sharable latent tasks across neighbouring landmarks under the paradigm of the multi-task learning framework, so that the redundancy information of the overlapped patches can be efficiently removed. To better improve the performance, we extend our DSSD to a recurrent DSSD (R-DSSD) architecture by integrating with the complementary information from multi-scale perspectives. Experimental results on the widely used benchmark datasets show that our methods achieve very competitive performance compared to the state-of-the-arts.","keywords_author":["biometrics","deep learning","Face alignment","Face alignment","deep learning","biometrics","Face alignment","deep learning","biometrics"],"keywords_other":["DSSD method","NETS","ALGORITHM","regression analysis","Correlation","Redundancy information","Facial Expressions","Face alignment","Feature extraction","Competitive performance","deep sharable and structural detectors method","Structural feature","face recognition","Face","Decision support systems","neighbouring landmarks","face alignment","learning (artificial intelligence)","structural feature learning method","Benchmark datasets","Shape","Partial occlusions","RECOGNITION","WILD","global shape regression","multitask learning framework","Detectors","Semantic information","Redundancy","feature extraction","MODELS"],"max_cite":4.0,"pub_year":2017.0,"sources":"['ieee', 'scp', 'wos']","rawkeys":["benchmark datasets","partial occlusions","regression analysis","decision support systems","competitive performance","redundancy information","deep sharable and structural detectors method","face recognition","models","dssd method","semantic information","neighbouring landmarks","algorithm","face alignment","learning (artificial intelligence)","recognition","structural feature","deep learning","structural feature learning method","facial expressions","global shape regression","biometrics","nets","multitask learning framework","face","correlation","wild","detectors","shape","redundancy","feature extraction"],"tags":["benchmark datasets","partial occlusions","regression analysis","decision support systems","competitive performance","machine learning","deep sharable and structural detectors method","face recognition","dssd method","algorithms","semantic information","neighbouring landmarks","face alignment","recognition","structural feature","structural feature learning method","facial expressions","global shape regression","biometrics","nets","multitask learning framework","face","correlation","redundant informations","wild","detectors","model","shape","redundancy","feature extraction"]},{"p_id":66847,"title":"Wiener filtering based speech enhancement with Weighted Denoising Auto-encoder and noise classification","abstract":"A novel speech enhancement method based on Weighted Denoising Auto-encoder (WDA) and noise classification is proposed in this paper. A weighted reconstruction loss function is introduced into the conventional Denoising Auto-encoder (DA), and the relationship between the power spectra of clean speech and noisy observation is described by WDA model. First, the sub-band power spectrum of clean speech is estimated by WDA model from the noisy observation. Then, the a priori SNR is estimated by the a Posteriori SNR Controlled Recursive Averaging (PCRA) approach. Finally, the clean speech is obtained by Wiener filter in frequency domain. In addition, in order to make the proposed method suitable for various kinds of noise conditions, a Gaussian Mixture Model (GMM) based noise classification method is employed. And the corresponding WDA model is used in the enhancement process. From the test results under ITU-T G.160, it is shown that, in comparison with the reference method which is the Wiener filtering method with decision-directed approach for SNR estimation, the WDA-based speech enhancement methods could achieve better objective speech quality, no matter whether the noise conditions are included in the training set or not. And the similar amount of noise reduction and SNR improvement can be obtained with smaller distortion on speech level. (C) 2014 Elsevier B.V. All rights reserved.","keywords_author":["Speech enhancement","Weighted Denoising Auto-encoder","SNR estimation","Wiener filter","Noise classification","Gaussian mixture model"],"keywords_other":["RECOGNITION"],"max_cite":40.0,"pub_year":2014.0,"sources":"['ieee', 'wos']","rawkeys":["recognition","noise classification","snr estimation","weighted denoising auto-encoder","speech enhancement","wiener filter","gaussian mixture model"],"tags":["recognition","noise classification","wiener filters","snr estimation","weighted denoising auto-encoder","speech enhancement","gaussian mixture model"]},{"p_id":1312,"title":"Deep Supervised and Contractive Neural Network for SAR Image Classification","abstract":"The classification of a synthetic aperture radar (SAR) image is a significant yet challenging task, due to the presence of speckle noises and the absence of effective feature representation. Inspired by deep learning technology, a novel deep supervised and contractive neural network (DSCNN) for SAR image classification is proposed to overcome these problems. In order to extract spatial features, a multiscale patch-based feature extraction model that consists of gray level-gradient co-occurrence matrix, Gabor, and histogram of oriented gradient descriptors is developed to obtain primitive features from the SAR image. Then, to get discriminative representation of initial features, the DSCNN network that comprises four layers of supervised and contractive autoencoders is proposed to optimize features for classification. The supervised penalty of the DSCNN can capture the relevant information between features and labels, and the contractive restriction aims to enhance the locally invariant and robustness of the encoding representation. Consequently, the DSCNN is able to produce effective representation of sample features and provide superb predictions of the class labels. Moreover, to restrain the influence of speckle noises, a graph-cut-based spatial regularization is adopted after classification to suppress misclassified pixels and smooth the results. Experiments on three SAR data sets demonstrate that the proposed method is able to yield superior classification performance compared with some related approaches.","keywords_author":["Contractive autoencoder (AE)","deep neural network (DNN)","supervised classification","synthetic aperture radar (SAR) image","Contractive autoencoder (AE)","deep neural network (DNN)","supervised classification","synthetic aperture radar (SAR) image"],"keywords_other":["Synthetic aperture radar","geophysical image processing","ALGORITHM","histogram of oriented gradient descriptors","synthetic aperture radar image","LAND CHANGE","image classification","gray level-gradient co-occurrence matrix","radar imaging","Feature extraction","spatial feature extraction","gradient methods","INTEGRATION","Machine learning","graph-cut-based spatial regularization","Gabor model","remote sensing by radar","learning (artificial intelligence)","Robustness","FEATURES","AUTOENCODERS","synthetic aperture radar","RECOGNITION","SAR image classification","Transforms","multiscale patch-based feature extraction model","neural nets","Speckle","matrix algebra","INFORMATION","REPRESENTATION","SYNTHETIC-APERTURE RADAR","novel deep supervised and contractive neural network","SEGMENTATION","feature extraction","DSCNN","Decoding"],"max_cite":10.0,"pub_year":2017.0,"sources":"['wos', 'ieee']","rawkeys":["robustness","geophysical image processing","land change","transforms","histogram of oriented gradient descriptors","synthetic aperture radar image","image classification","contractive autoencoder (ae)","gray level-gradient co-occurrence matrix","speckle","radar imaging","supervised classification","gabor model","features","segmentation","spatial feature extraction","machine learning","gradient methods","graph-cut-based spatial regularization","remote sensing by radar","deep neural network (dnn)","algorithm","synthetic aperture radar (sar) image","integration","learning (artificial intelligence)","recognition","synthetic aperture radar","multiscale patch-based feature extraction model","neural nets","dscnn","matrix algebra","novel deep supervised and contractive neural network","representation","sar image classification","autoencoders","feature extraction","information","decoding","synthetic-aperture radar"],"tags":["robustness","geophysical image processing","land change","histogram of oriented gradient descriptors","convolutional neural network","image classification","gray level-gradient co-occurrence matrix","speckle","radar imaging","supervised classification","gabor model","features","segmentation","spatial feature extraction","machine learning","gradient methods","graph-cut-based spatial regularization","algorithms","transform","remote sensing by radar","integration","recognition","neural networks","synthetic aperture radar","multiscale patch-based feature extraction model","synthetic aperture radar (sar) images","dscnn","matrix algebra","novel deep supervised and contractive neural network","auto encoders","representation","sar image classification","feature extraction","information","decoding"]},{"p_id":9509,"title":"Deep learning for biological image classification","abstract":"A number of industries use human inspection to visually classify the quality of their products and the raw materials used in the production process, this process could be done automatically through digital image processing. The industries are not always interested in the most accurate technique for a given problem, but most appropriate for the expected results, there must be a balance between accuracy and computational cost. This paper investigates the classification of the quality of wood boards based on their images. For such, it compares the use of deep learning, particularly Convolutional Neural Networks, with the combination of texture-based feature extraction techniques and traditional techniques: Decision tree induction algorithms, Neural Networks, Nearest neighbors and Support vector machines. Reported studies show that Deep Learning techniques applied to image processing tasks have achieved predictive performance superior to traditional classification techniques, mainly in high complex scenarios. One of the reasons pointed out is their embedded feature extraction mechanism. Deep Learning techniques directly identify and extract features, considered by them to be relevant, in a given image dataset. However, empirical results for the image data set have shown that the texture descriptor method proposed, regardless of the strategy employed is very competitive when compared with Convolutional Neural Network for all the performed experiments. The best performance of the texture descriptor method could be caused by the nature of the image dataset. Finally are pointed out some perspectives of futures developments with the application of Active learning and Semi supervised methods. (C) 2017 Elsevier Ltd. All rights reserved.","keywords_author":["Deep learning","Image classification","Machine learning","Wood classification","Wood classification","Deep learning","Image classification","Machine learning"],"keywords_other":["Classification technique","Semi-supervised method","Predictive performance","FEATURES","INSPECTION","RECOGNITION","Convolutional neural network","Traditional techniques","Feature extraction techniques","Decision tree induction","Extraction mechanisms","CONVOLUTIONAL NEURAL-NETWORKS"],"max_cite":7.0,"pub_year":2017.0,"sources":"['ieee', 'scp', 'wos']","rawkeys":["predictive performance","feature extraction techniques","recognition","semi-supervised method","features","deep learning","machine learning","inspection","wood classification","classification technique","convolutional neural-networks","convolutional neural network","traditional techniques","extraction mechanisms","image classification","decision tree induction"],"tags":["feature extraction techniques","traditional techniques","recognition","semi-supervised method","features","machine learning","inspection","wood classification","classification technique","convolutional neural network","prediction performance","extraction mechanisms","image classification","decision tree induction"]},{"p_id":9524,"title":"Deep learning with support vector data description","abstract":"One of the most critical problems for machine learning methods is overfitting. The overfitting problem is a phenomenon in which the accuracy of the model on unseen data is poor whereas the training accuracy is nearly perfect. This problem is particularly severe in complex models that have a large set of parameters. In this paper, we propose a deep learning neural network model that adopts the support vector data description (SVDD). The SVDD is a variant of the support vector machine, which has high generalization performance by acquiring a maximal margin in one-class classification problems. The proposed model strives to obtain the representational power of deep learning. Generalization performance is maintained using the SVDD. The experimental results showed that the proposed model can learn multiclass data without severe overfitting problems. (C) 2015 Elsevier B.V. All rights reserved.","keywords_author":["Deep learning","Generalization","Pattern recognition","Support vector data description","Support vector data description","Deep learning","Pattern recognition","Generalization"],"keywords_other":["Deep learning","BOLTZMANN MACHINES","One-class Classification","NETWORKS","CROSS-VALIDATION","Generalization performance","ALGORITHM","PARAMETER","Machine learning methods","Training accuracy","Support vector data description","Generalization","Over fitting problem"],"max_cite":20.0,"pub_year":2015.0,"sources":"['ieee', 'scp', 'wos']","rawkeys":["machine learning methods","algorithm","over fitting problem","support vector data description","deep learning","boltzmann machines","training accuracy","parameter","generalization","networks","cross-validation","pattern recognition","one-class classification","generalization performance"],"tags":["machine learning methods","over fitting problem","recognition","parameters","support vector data description","boltzmann machines","training accuracy","machine learning","pattern recognition","networks","algorithms","computer vision","one-class classification","generalization performance"]},{"p_id":34108,"title":"Object detection and localization using deep convolutional networks with softmax activation and multi-class log loss","abstract":"\u00a9 Springer International Publishing Switzerland 2016.We introduce a deep neural network that can be used to localize and detect a region of interest (ROI) in an image. We show how this network helped us extract ROIs when working on two separate problems: a whale recognition problem and a heart volume estimation problem. In the former problem, we used this network to localize the head of the whale while in the later we used it to localize the heart left ventricle from MRI images. Most localization networks regress a bounding box around the region of interest. Unlike these architecture, we treat the problem as a classification problem where each pixel in the image is a separate class. The network is trained on images along with masks which indicate where the object is in the image. We treat the problem as a multi-class classification. Therefore, the last layer has a softmax activation. Furthermore, during training, the mutli-class log loss is minimized just like any classification task.","keywords_author":["Artificial neural networks","Convolutional neural network","Deep learning","Detection","Image classification","Localization","Recognition"],"keywords_other":["Deep learning","Convolutional networks","Recognition","Multi-class classification","Classification tasks","Localization","Object detection and localizations","Convolutional neural network"],"max_cite":2.0,"pub_year":2016.0,"sources":"['scp', 'wos']","rawkeys":["object detection and localizations","recognition","localization","deep learning","convolutional networks","classification tasks","artificial neural networks","convolutional neural network","detection","image classification","multi-class classification"],"tags":["object detection and localizations","recognition","localization","neural networks","machine learning","classification tasks","detection","convolutional neural network","image classification","multi-class classification"]},{"p_id":9533,"title":"Maximum Entropy Learning with Deep Belief Networks","abstract":"Conventionally, the maximum likelihood (ML) criterion is applied to train a deep belief network (DBN). We present a maximum entropy (ME) learning algorithm for DBNs, designed specifically to handle limited training data. Maximizing only the entropy of parameters in the DBN allows more effective generalization capability, less bias towards data distributions, and robustness to over-fitting compared to ML learning. Results of text classification and object recognition tasks demonstrate ME-trained DBN outperforms ML-trained DBN when training data is limited.","keywords_author":["maximum entropy","machine learning","deep learning","deep belief networks","restricted Boltzmann machine","deep neural networks","low-resource tasks"],"keywords_other":["BOLTZMANN MACHINES","CORTEX","ALGORITHM","CONTRASTIVE DIVERGENCE","REPRESENTATIONS","NEURAL-NETWORKS","RECOGNITION","MODELS","INFORMATION-THEORY","GRADIENT"],"max_cite":2.0,"pub_year":2016.0,"sources":"['ieee', 'wos']","rawkeys":["neural-networks","algorithm","maximum entropy","recognition","contrastive divergence","deep learning","deep neural networks","boltzmann machines","information-theory","machine learning","gradient","representations","models","cortex","deep belief networks","restricted boltzmann machine","low-resource tasks"],"tags":["maximum entropy","information theory","model","contrastive divergence","recognition","neural networks","boltzmann machines","machine learning","representation","gradient","convolutional neural network","algorithms","cortex","deep belief networks","restricted boltzmann machine","low-resource tasks"]},{"p_id":1342,"title":"Deep Learning for Health Informatics","abstract":"With a massive influx of multimodality data, the role of data analytics in health informatics has grown rapidly in the last decade. This has also prompted increasing interests in the generation of analytical, data driven models based on machine learning in health informatics. Deep learning, a technique with its foundation in artificial neural networks, is emerging in recent years as a powerful tool for machine learning, promising to reshape the future of artificial intelligence. Rapid improvements in computational power, fast data storage, and parallelization have also contributed to the rapid uptake of the technology in addition to its predictive power and ability to generate automatically optimized high-level features and semantic interpretation from the input data. This article presents a comprehensive up-to-date review of research employing deep learning in health informatics, providing a critical analysis of the relative merit, and potential pitfalls of the technique as well as its future outlook. The paper mainly focuses on key applications of deep learning in the fields of translational bioinformatics, medical imaging, pervasive sensing, medical informatics, and public health.","keywords_author":["Bioinformatics","deep learning","health informatics","machine learning","medical imaging","public health","wearable devices","Bioinformatics","deep learning","health informatics","machine learning","medical imaging","public health","wearable devices","Bioinformatics","deep learning","health informatics","machine learning","medical imaging","public health","wearable devices"],"keywords_other":["Monitoring, Ambulatory","Artificial neural networks","Humans","parallelization","Medical Informatics","health informatics","Machine Learning","Biological neural networks","Semantic interpretation","Data-driven model","Computational power","DIAGNOSIS","Health informatics","High-level features","Critical analysis","machine learning","BIOINFORMATICS","Machine learning","Wearable devices","ARCHITECTURE","Biomedical imaging","learning (artificial intelligence)","deep learning","medical imaging","Training","MODEL","RECOGNITION","medical informatics","public health","neural nets","medical information systems","Neurons","Medical informatics","translational bioinformatics","artificial intelligence","FRAMEWORK","CLASSIFICATION","BIG DATA","multimodality data","SEGMENTATION","Informatics","bioinformatics","Public Health","pervasive sensing","Computational Biology","CONVOLUTIONAL NEURAL-NETWORKS"],"max_cite":51.0,"pub_year":2017.0,"sources":"['ieee', 'scp', 'wos']","rawkeys":["ambulatory","big data","parallelization","health informatics","classification","convolutional neural-networks","architecture","monitoring","wearable devices","critical analysis","segmentation","machine learning","semantic interpretation","biological neural networks","neurons","diagnosis","high-level features","learning (artificial intelligence)","recognition","deep learning","data-driven model","framework","medical imaging","training","humans","medical informatics","public health","neural nets","medical information systems","biomedical imaging","informatics","translational bioinformatics","artificial intelligence","model","computational biology","multimodality data","artificial neural networks","bioinformatics","pervasive sensing","computational power"],"tags":["ambulatory","big data","multimodal data","health informatics","classification","convolutional neural network","architecture","monitoring","wearable devices","critical analysis","segmentation","machine learning","parallelizations","semantic interpretation","biological neural networks","neurons","diagnosis","high-level features","recognition","neural networks","data-driven model","framework","medical imaging","training","humans","medical informatics","public health","medical information systems","biomedical imaging","informatics","translational bioinformatics","model","computational biology","bioinformatics","pervasive sensing","computational power"]},{"p_id":1346,"title":"Improving the Robustness of Neural Networks Using K-Support Norm Based Adversarial Training","abstract":"It is of significant importance for any classification and recognition system, which claims near or better than human performance to be immune to small perturbations in the dataset. Researchers found out that neural networks are not very robust to small perturbations and can easily be fooled to persistently misclassify by adding a particular class of noise in the test data. This, so-called adversarial noise severely deteriorates the performance of neural networks, which otherwise perform really well on unperturbed dataset. It has been recently proposed that neural networks can be made robust against adversarial noise by training them using the data corrupted with adversarial noise itself. Following this approach, in this paper, we propose a new mechanism to generate a powerful adversarial noise model based on K-support norm to train neural networks. We tested our approach on two benchmark datasets, namely the MNIST and STL-10, using muti-layer perceptron and convolutional neural networks. Experimental results demonstrate that neural networks trained with the proposed technique show significant improvement in robustness as compared to state-of-the-art techniques.","keywords_author":["K-Support norm","robutness","generalization","convolutional neural networks","adversarial"],"keywords_other":["multilayer perceptrons","classification system","k-support norm based adversarial training","convolutional neural networks","neural network training","learning (artificial intelligence)","Robustness","Training","Optimization","Biological neural networks","pattern classification","MNIST datasets","Mathematical model","recognition system","mutilayer perceptron","adversarial noise","STL-10 datasets","Support vector machines"],"max_cite":1.0,"pub_year":2016.0,"sources":"['ieee']","rawkeys":["k-support norm","multilayer perceptrons","robustness","k-support norm based adversarial training","stl-10 datasets","generalization","recognition system","optimization","biological neural networks","mnist datasets","mathematical model","classification system","learning (artificial intelligence)","convolutional neural networks","neural network training","training","mutilayer perceptron","pattern classification","adversarial","support vector machines","robutness","adversarial noise"],"tags":["k-support norm","robustness","k-support norm based adversarial training","convolutional neural network","stl-10 datasets","machine learning","optimization","biological neural networks","mathematical model","classification system","recognition","neural network training","training","mutilayer perceptron","pattern classification","adversarial","mnist dataset","robutness","multi layer perceptron","recognition systems","adversarial noise"]},{"p_id":9556,"title":"Convolutional deep-learning artificial neural networks","abstract":"This paper discusses the history of the appearance and development of the concept of convolutional artificial neural networks, which, because they use a learning technology based on back-propagation of the error signal, have become one of the most efficient tools of automatic image classification. Along with the possibilities of modern convolutional neural networks in the area of shape classification of objects, the features of their use in analyzing the information of other hierarchical levels have also been analyzed-from the classification of textures to the structural decomposition of images, based on the formation of attention zones. Convolutional networks are considered in close association with a description of their natural analogs, found in the neural ensembles of living visual systems. (C) 2015 Optical Society of America.","keywords_author":null,"keywords_other":["RECOGNITION","FEATURES"],"max_cite":3.0,"pub_year":2015.0,"sources":"['wos']","rawkeys":["recognition","features"],"tags":["recognition","features"]},{"p_id":9571,"title":"A review of deep machine learning","abstract":"The rapid increase of information and accessibility in recent years has activated a paradigm shift in algorithm design for artificial intelligence. Recently, Deep Learning (a surrogate of Machine Learning) have won several contests in pattern recognition and machine learning. This review comprehensively summarises relevant studies, much of it from prior state-of-the-art techniques. This paper also discusses the motivations and principles regarding learning algorithms for deep architectures.","keywords_author":["Boltzmann machine","Deep belief networks","Deep learning","Feature learning","Neural nets","Unsupervised learning","Deep learning","Deep belief networks","feature learning","unsupervised learning","Boltzmann Machine","neural nets"],"keywords_other":["Deep learning","BOLTZMANN MACHINES","Deep architectures","Paradigm shifts","ARCHITECTURES","ALGORITHM","Deep belief networks","NEURAL-NETWORKS","RECOGNITION","Boltzmann machines","Feature learning","State-of-the-art techniques","Algorithm design"],"max_cite":2.0,"pub_year":2016.0,"sources":"['scp', 'wos']","rawkeys":["algorithm design","algorithm","neural-networks","recognition","state-of-the-art techniques","architectures","deep learning","boltzmann machines","deep architectures","paradigm shifts","unsupervised learning","feature learning","neural nets","deep belief networks","boltzmann machine"],"tags":["algorithm design","recognition","neural networks","boltzmann machines","deep architectures","machine learning","paradigm shifts","unsupervised learning","algorithms","feature learning","state-of-the-art techniques","deep belief networks","architecture"]},{"p_id":1382,"title":"Mispronunciation Detection and Diagnosis in L2 English Speech Using Multidistribution Deep Neural Networks","abstract":"This paper investigates the use of multidistribution deep neural networks (DNNs) for mispronunciation detection and diagnosis (MDD), to circumvent the difficulties encountered in an existing approach based on extended recognition networks (ERNs). The ERNs leverage existing automatic speech recognition technology by constraining the search space via including the likely phonetic error patterns of the target words in addition to the canonical transcriptions. MDDs are achieved by comparing the recognized transcriptions with the canonical ones. Although this approach performs reasonably well, it has the following issues: 1) Learning the error patterns of the target words to generate the ERNs remains a challenging task. Phones or phone errors missing from the ERNs cannot be recognized even if we have well-trained acoustic models; and 2) acoustic models and phonological rules are trained independently, and hence, contextual information is lost. To address these issues, we propose an acoustic-graphemic-phonemic model (AGPM) using a multidistribution DNN, whose input features include acoustic features, as well as corresponding graphemes and canonical transcriptions (encoded as binary vectors). The AGPM can implicitly model both grapheme-to-likely-pronunciation and phoneme-to-likely-pronunciation conversions, which are integrated into acoustic modeling. With the AGPM, we develop a unified MDD framework, which works much like free-phone recognition. Experiments show that our method achieves a phone error rate (PER) of 11.1%. The false rejection rate (FRR), false acceptance rate (FAR), and diagnostic error rate (DER) for MDD are 4.6%, 30.5%, and 13.5%, respectively. It outperforms the ERN approach using DNNs as acoustic models, whose PER, FRR, FAR, and DER are 16.8%, 11.0%, 43.6%, and 32.3%, respectively.","keywords_author":["Deep neural networks","L2 English speech","mispronunciation detection","mispronunciation diagnosis","speech recognition","Deep neural networks","L2 English speech","mispronunciation detection","mispronunciation diagnosis","speech recognition"],"keywords_other":["Acoustics","DER","REPRESENTATIONS","ERN","multidistribution deep neural network","speech recognition","PER","PRONUNCIATION ERROR PATTERNS","search space","Neural networks","false rejection rate","likely phonetic error pattern","false acceptance rate","automatic speech recognition technology","mispronunciation detection and diagnosis","AGREEMENT","extended recognition network","word processing","Speech","Context modeling","L2 english speech","AGPM","phoneme-to-likely-pronunciation conversion","MARKOV-MODELS","target word","RECOGNITION","Hidden Markov models","grapheme-to-likely-pronunciation conversion","neural nets","multidistribution DNN","canonical transcription","diagnostic error rate","well-trained acoustic model","phone error rate","FAR","natural language processing","speech processing","voice activity detection","MDD","Speech recognition","TO-PHONEME CONVERSION","LANGUAGE","UNSUPERVISED DISCOVERY","acoustic-graphemic-phonemic model","FRR"],"max_cite":4.0,"pub_year":2016.0,"sources":"['wos', 'ieee']","rawkeys":["frr","markov-models","mdd","speech recognition","multidistribution deep neural network","agreement","context modeling","search space","ern","acoustics","language","agpm","false rejection rate","likely phonetic error pattern","false acceptance rate","automatic speech recognition technology","der","extended recognition network","l2 english speech","mispronunciation detection and diagnosis","word processing","speech","per","phoneme-to-likely-pronunciation conversion","target word","recognition","unsupervised discovery","neural networks","far","grapheme-to-likely-pronunciation conversion","mispronunciation detection","neural nets","canonical transcription","diagnostic error rate","hidden markov models","well-trained acoustic model","mispronunciation diagnosis","deep neural networks","phone error rate","natural language processing","speech processing","voice activity detection","representations","pronunciation error patterns","multidistribution dnn","acoustic-graphemic-phonemic model","to-phoneme conversion"],"tags":["target words","speech recognition","markov model","multidistribution deep neural network","agreement","context modeling","convolutional neural network","ern","acoustics","language","agpm","false rejection rate","likely phonetic error pattern","false acceptance rate","automatic speech recognition technology","der","extended recognition network","l2 english speech","mispronunciation detection and diagnosis","word processing","speech","per","phoneme-to-likely-pronunciation conversion","false alarm rate","recognition","unsupervised discovery","search spaces","neural networks","grapheme-to-likely-pronunciation conversion","mispronunciation detection","canonical transcription","diagnostic error rate","hidden markov models","well-trained acoustic model","mispronunciation diagnosis","phone error rate","natural language processing","representation","speech processing","voice activity detection","pronunciation error patterns","multidistribution dnn","major depressive disorder","acoustic-graphemic-phonemic model","to-phoneme conversion"]},{"p_id":9575,"title":"Deep unsupervised learning of visual similarities","abstract":"Exemplar learning of visual similarities in an unsupervised manner is a problem of paramount importance to computer vision. In this context, however, the recent breakthrough in deep learning could not yet unfold its full potential. With only a single positive sample, a great imbalance between one positive and many negatives, and unreliable relationships between most samples, training of Convolutional Neural networks is impaired. In this paper we use weak estimates of local similarities and propose a single optimization problem to extract batches of samples with mutually consistent relations. Conflicting relations are distributed over different batches and similar samples are grouped into compact groups. Learning visual similarities is then framed as a sequence of categorization tasks. The CNN then consolidates transitivity relations within and between groups and learns a single representation for all samples without the need for labels. The proposed unsupervised approach has shown competitive performance on detailed posture analysis and object classification. (C) 2018 Elsevier Ltd. All rights reserved.","keywords_author":["Deep learning","Human pose analysis","Object retrieval","Self-supervised learning","Visual similarity learning","Visual similarity learning","Deep learning","Self-supervised learning","Human pose analysis","Object retrieval"],"keywords_other":["Optimization problems","Object retrieval","SEARCH","Competitive performance","Human pose","Unsupervised approaches","RECOGNITION","Convolutional neural network","Visual similarity","Object classification"],"max_cite":0.0,"pub_year":2018.0,"sources":"['scp', 'wos']","rawkeys":["competitive performance","human pose analysis","object retrieval","recognition","search","unsupervised approaches","deep learning","visual similarity","optimization problems","visual similarity learning","object classification","convolutional neural network","self-supervised learning","human pose"],"tags":["competitive performance","human pose analysis","object retrieval","recognition","search","unsupervised approaches","visual similarity","optimization problems","machine learning","visual similarity learning","object classification","convolutional neural network","self-supervised learning","human pose"]},{"p_id":17776,"title":"Deepfruits: A fruit detection system using deep neural networks","abstract":"\u00a9 2016 by the authors; licensee MDPI, Basel, Switzerland.This paper presents a novel approach to fruit detection using deep convolutional neural networks. The aim is to build an accurate, fast and reliable fruit detection system, which is a vital element of an autonomous agricultural robotic platform; it is a key element for fruit yield estimation and automated harvesting. Recent work in deep neural networks has led to the development of a state-of-the-art object detector termed Faster Region-based CNN (Faster R-CNN). We adapt this model, through transfer learning, for the task of fruit detection using imagery obtained from two modalities: colour (RGB) and Near-Infrared (NIR). Early and late fusion methods are explored for combining the multi-modal (RGB and NIR) information. This leads to a novel multi-modal Faster R-CNN model, which achieves state-of-the-art results compared to prior work with the F1 score, which takes into account both precision and recall performances improving from 0.807 to 0.838 for the detection of sweet pepper. In addition to improved accuracy, this approach is also much quicker to deploy for new fruits, as it requires bounding box annotation rather than pixel-level annotation (annotating bounding boxes is approximately an order of magnitude quicker to perform). The model is retrained to perform the detection of seven fruits, with the entire process taking four hours to annotate and train the new model per fruit.","keywords_author":["Agricultural robotics","Deep convolutional neural network","Harvesting robots","Horticulture","Multi-modal","Rapid training","Real-time performance","Visual fruit detection","visual fruit detection","deep convolutional neural network","multi-modal","rapid training","real-time performance","harvesting robots","horticulture","agricultural robotics"],"keywords_other":["Image Processing, Computer-Assisted","CONVOLUTIONAL NETWORKS","Algorithms","Neural Networks (Computer)","Humans","Pattern Recognition, Automated","Horticulture","CLASSIFICATION","Fruit","RECOGNITION","Convolutional neural network","Real time performance","Robotics","SCALE","Multi-modal","Harvesting robot","Capsicum","Agricultural robotics"],"max_cite":28.0,"pub_year":2016.0,"sources":"['scp', 'wos']","rawkeys":["automated","agricultural robotics","classification","convolutional neural network","harvesting robot","visual fruit detection","horticulture","convolutional networks","algorithms","rapid training","neural networks (computer)","recognition","deep convolutional neural network","fruit","humans","multi-modal","real time performance","scale","real-time performance","capsicum","computer-assisted","image processing","harvesting robots","robotics","pattern recognition"],"tags":["automated","classification","convolutional neural network","harvesting robot","visual fruit detection","horticulture","agricultural robot","algorithms","rapid training","recognition","neural networks","fruit","humans","multi-modal","real time performance","scale","capsicum","computer-assisted","image processing","robotics","pattern recognition"]},{"p_id":9586,"title":"Age classification with deep learning face representation","abstract":"Automatic age estimation from facial images is challenging not only for computers, but also for humans in some cases. Therefore, coarse age groups such as children, teen age, adult and senior adult are considered in age classification, instead of evaluating specific age. In this paper, we propose an approach that provides a significant improvement in performance on benchmark databases and standard protocols for age classification. Our approach is based on deep learning techniques. We optimize the network architecture using the Deep IDentification-verification features, which are proved very efficient for face representation. After reducing the redundancy among the large number of output features, we apply different classifiers to classify the facial images to different age group with the final features. The experimental analysis shows that the proposed approach outperforms the reported state-of-the-arts on both constrained and unconstrained databases.","keywords_author":["Age classification","Deep learning","DeepID2","Face representation","Age classification","Deep learning","Face representation","DeepID2"],"keywords_other":["Benchmark database","INFORMATION","NETWORKS","Face representations","Experimental analysis","State of the art","DeepID2","Learning techniques","Standard protocols","RECOGNITION","MODELS","IMAGE RETRIEVAL","FACIAL IMAGES","PATTERNS","Age classification"],"max_cite":3.0,"pub_year":2017.0,"sources":"['ieee', 'scp', 'wos']","rawkeys":["age classification","recognition","deep learning","image retrieval","standard protocols","patterns","state of the art","deepid2","experimental analysis","learning techniques","networks","face representation","information","models","facial images","benchmark database","face representations"],"tags":["age classification","recognition","model","image retrieval","machine learning","patterns","standard protocols","state of the art","deepid2","experimental analysis","learning techniques","networks","information","facial images","benchmark database","face representations"]},{"p_id":9587,"title":"Geometric Deep Learning Going beyond Euclidean data","abstract":null,"keywords_author":null,"keywords_other":["GRAPHS","INVARIANT SCATTERING","REPRESENTATION","MODEL","MAPS","RECOGNITION","DIMENSIONALITY REDUCTION","DESCRIPTORS","SHAPES","CONVOLUTIONAL NETWORKS"],"max_cite":5.0,"pub_year":2017.0,"sources":"['ieee', 'wos']","rawkeys":["invariant scattering","graphs","model","recognition","representation","convolutional networks","shapes","descriptors","dimensionality reduction","maps"],"tags":["invariant scattering","recognition","model","graph","map","representation","shape","convolutional neural network","descriptors","dimensionality reduction"]},{"p_id":1404,"title":"Mid-Level Feature Representation via Sparse Autoencoder for Remotely Sensed Scene Classification","abstract":"Feature representation is a classic problem in the machine learning community due to the fact that different representations can entangle and hide more or less the different explanatory factors of variation behind the raw data. Especially for scene classification, its performance generally depends on the discriminative power of feature representation. Recently, unsupervised feature learning attracts tremendous attention because of its ability to learn feature representation automatically. However, reliable performance of feature representations by unsupervised learning always requires a large number of features and complex framework of mid-level feature representation. To alleviate such drawbacks, this paper presents a new framework of mid-level feature representation, which does not need learn many convolutional features during the unsupervised feature learning process, and has few parameter settings. In detail, the unsupervised feature learning method, sparse autoencoder, is employed to learn relatively small number of convolutional features from input dataset, and then extended features are extracted from the learned features by a multiple normalized difference features extraction method to compose a derivative feature set. At mid-level feature representation stage, in order to avoid poor performance of standard pooling technology in solving problems brought by rotation and translation of scene images, global feature descriptors (histogram moments, mean, variance, standard deviation) are utilized to build mid-level feature representations of images. For validation and comparison purposes, the proposed approach is evaluated via experiments with two challenging high-resolution remote sensing datasets. The results demonstrate that the approach is effective, and shows strong performance for remotely sensed scene classification.","keywords_author":["Global feature descriptors","scene classification","sparse autoencoder (SAE)","unsupervised feature learning","Global feature descriptors","scene classification","sparse autoencoder (SAE)","unsupervised feature learning"],"keywords_other":["mid-level feature representation","remote sensing","scene image rotation","image classification","Unsupervised learning","Computational modeling","scene image translation","Feature extraction","Visualization","unsupervised feature learning method","feature extraction method","Machine learning","LEARNING ALGORITHM","learning (artificial intelligence)","SENSING IMAGERY","remotely sensed scene classification","RECOGNITION","machine learning community","Remote sensing","sparse autoencoder","Semantics","standard pooling technology","NEURAL-NETWORKS","feature extraction","high-resolution remote sensing dataset"],"max_cite":4.0,"pub_year":2016.0,"sources":"['wos', 'ieee']","rawkeys":["mid-level feature representation","remote sensing","scene classification","scene image rotation","image classification","visualization","neural-networks","scene image translation","machine learning","feature extraction method","unsupervised feature learning method","sparse autoencoder (sae)","computational modeling","learning (artificial intelligence)","recognition","semantics","remotely sensed scene classification","machine learning community","unsupervised learning","global feature descriptors","learning algorithm","sparse autoencoder","unsupervised feature learning","standard pooling technology","feature extraction","sensing imagery","high-resolution remote sensing dataset"],"tags":["mid-level feature representation","remote sensing","feature extraction methods","scene classification","scene image rotation","sensed imagery","image classification","visualization","remote sensing scene classification","scene image translation","machine learning","unsupervised feature learning method","computational modeling","recognition","neural networks","semantics","machine learning communities","unsupervised learning","global feature descriptors","learning algorithm","unsupervised feature learning","standard pooling technology","stacked autoencoders","feature extraction","high-resolution remote sensing dataset"]},{"p_id":1406,"title":"A Convolutional Neural Network-Based Chinese Text Detection Algorithm via Text Structure Modeling","abstract":"Text detection in a natural environment plays an important role in many computer vision applications. While existing text detection methods are focused on English characters, there are strong application demands on text detection in other languages, such as Chinese. In this paper, we present a novel text detection algorithm for Chinese characters based on a specific designed convolutional neural network (CNN). The CNN contains a text structure component detector layer, a spatial pyramid layer, and a multi-input-layer deep belief network (DBN). The CNN is pre-trained via a convolutional sparse auto-encoder, specifically designed for extracting complex features from Chinese characters. In particular, the text structure component detectors enhance the accuracy and uniqueness of feature descriptors by extracting multiple text structure components in various ways. The spatial pyramid layer enhances the scale invariability of the CNN for detecting texts in multiple scales. Finally, the multi-input-layer DBN replaces the fully connected layers in the CNN to ensure features from multiple scales are comparable. A multilingual text detection dataset, in which texts in Chinese, English, and digits are labeled separately, is set up to evaluate the proposed text detection algorithm. The proposed algorithm shows a significant performance improvement over the baseline CNN algorithms. In addition the proposed algorithm is evaluated over a public multilingual benchmark and achieves state-of-the-art result under multiple languages. Furthermore, a simplified version of the proposed algorithm with only general components is evaluated on the ICDAR 2011 and 2013 datasets, showing comparable detection performance to the existing general text detection algorithms.","keywords_author":["Chinese text detection","convolutional neural network (CNN)","text structure detector","unsupervised learning","Chinese text detection","convolutional neural network (CNN)","text structure detector","unsupervised learning"],"keywords_other":["Detection algorithms","English","convolutional neural network","Neural networks","READING TEXT","Unsupervised learning","spatial pyramid layer","Feature extraction","multilingual text detection dataset","CNN","complex feature extraction","Machine learning","chinese text detection algorithm","text structure modeling","text structure component detector layer","convolutional sparse auto-encoder","general text detection algorithms","English characters","computer vision applications","multi-input-layer deep belief network","RECOGNITION","DBN","computer vision","neural nets","Detectors","Chinese","LOCALIZATION","multiple text structure components","Chinese characters","VIDEOS","Image edge detection","natural language processing","text analysis","NATURAL SCENE IMAGES"],"max_cite":3.0,"pub_year":2016.0,"sources":"['ieee', 'wos']","rawkeys":["detection algorithms","localization","english","convolutional neural network","spatial pyramid layer","dbn","multilingual text detection dataset","machine learning","complex feature extraction","chinese text detection algorithm","text structure modeling","text structure component detector layer","convolutional sparse auto-encoder","general text detection algorithms","natural scene images","chinese characters","image edge detection","recognition","reading text","neural networks","computer vision applications","multi-input-layer deep belief network","cnn","convolutional neural network (cnn)","unsupervised learning","computer vision","neural nets","chinese","chinese text detection","english characters","text structure detector","multiple text structure components","detectors","natural language processing","text analysis","videos","feature extraction"],"tags":["localization","english","convolutional neural network","spatial pyramid layer","multilingual text detection dataset","machine learning","complex feature extraction","video","chinese text detection algorithm","text structure modeling","text structure component detector layer","natural scene images","general text detection algorithms","chinese characters","image edge detection","recognition","reading text","neural networks","computer vision applications","multi-input-layer deep belief network","detection algorithm","unsupervised learning","computer vision","chinese","chinese text detection","english characters","convolutional sparse autoencoders","text structure detector","multiple text structure components","detectors","natural language processing","text analysis","feature extraction","deep belief networks"]},{"p_id":9598,"title":"Encryption scheme classification: a deep learning approach","abstract":"Encryption has an important role in protecting cyber assets. However the use of weak encryption algorithms is a vulnerability that may be exploited. When exploited, detecting this vulnerability from encrypted data is a very difficult task to undertake. This research explores the use of recent advancement in machine learning algorithms specifically deep learning algorithms to classify encryption schemes based on entropy measurements of encrypted data with no feature engineering. Past research works using various machine learning algorithms have failed to achieve good accuracy results in classification. The research entails applying popular encryption algorithms with block cipher modes over the image dataset from CIFAR10. Two ImageNet winning convolutional neural network deep learning models were used to perform the classification. Transfer learning and layer modification were applied to evaluate the classification effectiveness. This research concludes that deep learning algorithms can be used to perform such classification where other algorithms have failed.","keywords_author":["encryption classification","deep learning","artificial intelligence"],"keywords_other":["RECOGNITION"],"max_cite":0.0,"pub_year":2017.0,"sources":"['wos']","rawkeys":["deep learning","encryption classification","artificial intelligence","recognition"],"tags":["encryption classification","recognition","machine learning"]},{"p_id":1414,"title":"Multi-valued autoencoders for multi-valued neural networks","abstract":"In order to reduce data dimensions, autoencoders with neural networks have been proposed by Hinton et al. Autoencoders are composed of input, one hidden, and output layers, which tune weights and biases by a back propagation to minimize an error between inputs and outputs. The learned weights have input features, and can be applied to pretrainings of deep neural networks. However, these autoencoders have been developed for real-valued neural networks. In this study, we propose complex and quaternion autoencoders for complex and quaternion neural networks, respectively. In the complex-valued autoencoder, inputs, weights, biases and outputs of the real-valued autoencoder are extended to complex numbers. In the quaternion autoencoder, these parameters are extended to quaternion numbers. We show the learning abilities of the proposed methods using handwritten digit images. The results show that the proposed methods can recognize the images as the real-valued methods.","keywords_author":["autoencoder","complex-valued neural network","quaternion neural network","recognition"],"keywords_other":["Image recognition","Sections","Neural networks","number theory","handwritten character recognition","multivalued autoencoders","data dimension reduction","Machine learning","learning abilities","optical character recognition","complex-valued autoencoder","real-valued autoencoder","Quaternions","Training","handwritten digit images","complex numbers","neural nets","deep neural networks","error minimization","multivalued neural networks","Speech recognition","image recognition","quaternion autoencoders","backpropagation"],"max_cite":null,"pub_year":2016.0,"sources":"['ieee']","rawkeys":["quaternions","autoencoder","speech recognition","quaternion neural network","number theory","handwritten character recognition","multivalued autoencoders","complex-valued neural network","data dimension reduction","machine learning","learning abilities","optical character recognition","complex-valued autoencoder","real-valued autoencoder","recognition","neural networks","training","handwritten digit images","complex numbers","neural nets","deep neural networks","error minimization","multivalued neural networks","image recognition","quaternion autoencoders","sections","backpropagation"],"tags":["quaternion","speech recognition","convolutional neural network","quaternion neural network","handwritten character recognition","number theory","multivalued autoencoders","data dimension reduction","machine learning","learning abilities","optical character recognition","complex-valued autoencoder","real-valued autoencoder","complex-valued neural networks","recognition","neural networks","training","handwritten digit images","complex numbers","error minimization","auto encoders","multivalued neural networks","image recognition","quaternion autoencoders","sections","backpropagation"]},{"p_id":34205,"title":"EMG pattern classification by split and merge deep belief network","abstract":"\u00ef\u00bf\u00bd 2016 by the authors. In this paper; we introduce an enhanced electromyography (EMG) pattern recognition algorithm based on a split-and-merge deep belief network (SM-DBN). Generally, it is difficult to classify the EMG features because the EMG signal has nonlinear and time-varying characteristics. Therefore, various machine-learning methods have been applied in several previously published studies. A DBN is a fast greedy learning algorithm that can identify a fairly good set of weights rapidly-even in deep networks with a large number of parameters and many hidden layers. To reduce overfitting and to enhance performance, the adopted optimization method was based on genetic algorithms (GA). As a result, the performance of the SM-DBN was 12.06% higher than conventional DBN. Additionally, SM-DBN results in a short convergence time, thereby reducing the training epoch. It is thus efficient in reducing the risk of overfitting. It is verified that the optimization was improved using GA.","keywords_author":["Deep belief network","Deep learning","EMG pattern recognition","SM-DBN","Split and merge deep belief network","EMG pattern recognition","deep learning","deep belief network","split and merge deep belief network","SM-DBN"],"keywords_other":["MOVEMENTS","USER EXPERIENCE","SIGNAL","ALGORITHM","NEURAL-NETWORKS","RECOGNITION","SURFACE EMG"],"max_cite":2.0,"pub_year":2016.0,"sources":"['ieee', 'scp', 'wos']","rawkeys":["neural-networks","algorithm","recognition","emg pattern recognition","deep learning","split and merge deep belief network","movements","surface emg","deep belief network","sm-dbn","user experience","signal"],"tags":["signals","recognition","emg pattern recognition","neural networks","movement","split and merge deep belief network","machine learning","surface emg","sm-dbn","algorithms","user experience","deep belief networks"]},{"p_id":1438,"title":"Learning auditory neural representations for emotion recognition","abstract":"Auditory emotion recognition has become a very important topic in recent years. However, still after the development of some architectures and frameworks, generalization is a big problem. Our model examines the capability of deep neural networks to learn specific features for different kinds of auditory emotion recognition: speech and music-based recognition. We propose the use of a cross-channel architecture to improve the generalization aspects of complex auditory recognition by the integration of previously learned knowledge of specific representation into a high-level auditory descriptor. We evaluate our models using the SAVEE dataset, the GTZAN dataset and the EmotiW corpus, and show comparable results with state-of-the-art approaches.","keywords_author":null,"keywords_other":["EmotiW corpus","speech recognition","Microprocessors","GTZAN dataset","music-based recognition","Computer architecture","cross-channel architecture","Feature extraction","high-level auditory descriptor","generalization","Speech","Convolution","complex auditory recognition","auditory neural representations learning","SAVEE dataset","generalisation (artificial intelligence)","learning (artificial intelligence)","Training","neural nets","deep neural networks","Emotion recognition","emotion recognition","auditory emotion recognition"],"max_cite":3.0,"pub_year":2016.0,"sources":"['ieee']","rawkeys":["gtzan dataset","speech recognition","music-based recognition","cross-channel architecture","convolution","high-level auditory descriptor","generalization","speech","complex auditory recognition","auditory neural representations learning","computer architecture","savee dataset","generalisation (artificial intelligence)","learning (artificial intelligence)","microprocessors","training","neural nets","deep neural networks","emotiw corpus","emotion recognition","feature extraction","auditory emotion recognition"],"tags":["gtzan dataset","speech recognition","convolutional neural network","music-based recognition","cross-channel architecture","convolution","high-level auditory descriptor","machine learning","speech","complex auditory recognition","auditory neural representations learning","computer architecture","savee dataset","recognition","microprocessors","neural networks","training","emotiw corpus","emotion recognition","feature extraction","auditory emotion recognition"]},{"p_id":66979,"title":"Scene text detection via extremal region based double threshold convolutional network classification","abstract":"In this paper, we present a robust text detection approach in natural images which is based on region proposal mechanism. A powerful low-level detector named saliency enhanced-MSER extended from the widely-used MSER is proposed by incorporating saliency detection methods, which ensures a high recall rate. Given a natural image, character candidates are extracted from three channels in a perception-based illumination invariant color space by saliency-enhanced MSER algorithm. A discriminative convolutional neural network (CNN) is jointly trained with multi-level information including pixel-level and character-level information as character candidate classifier. Each image patch is classified as strong text, weak text and non-text by double threshold filtering instead of conventional one-step classification, leveraging confident scores obtained via CNN. To further prune non-text regions, we develop a recursive neighborhood search algorithm to track credible texts from weak text set. Finally, characters are grouped into text lines using heuristic features such as spatial location, size, color, and stroke width. We compare our approach with several state-of-the-art methods, and experiments show that our method achieves competitive performance on public datasets ICDAR 2011 and ICDAR 2013.","keywords_author":null,"keywords_other":["REAL-WORLD IMAGES","LOCALIZATION","FACE","REPRESENTATION","RECOGNITION","NATURAL IMAGES","READING TEXT"],"max_cite":0.0,"pub_year":2017.0,"sources":"['ieee', 'wos']","rawkeys":["real-world images","recognition","localization","reading text","representation","natural images","face"],"tags":["recognition","localization","reading text","real-world image","representation","natural images","face"]},{"p_id":1445,"title":"Bilevel Model-Based Discriminative Dictionary Learning for Recognition","abstract":"Most supervised dictionary learning methods optimize the combinations of reconstruction error, sparsity prior, and discriminative terms. Thus, the learnt dictionaries may not be optimal for recognition tasks. Also, the sparse codes learning models in the training and the testing phases are inconsistent. Besides, without utilizing the intrinsic data structure, many dictionary learning methods only employ the l0 or I1 norm to encode each datum independently, limiting the performance of the learnt dictionaries. We present a novel bilevel model-based discriminative dictionary learning method for recognition tasks. The upper level directly minimizes the classification error, while the lower level uses the sparsity term and the Laplacian term to characterize the intrinsic data structure. The lower level is subordinate to the upper level. Therefore, our model achieves an overall optimality for recognition in that the learnt dictionary is directly tailored for recognition. Moreover, the sparse codes learning models in the training and the testing phases can be the same. We further propose a novel method to solve our bilevel optimization problem. It first replaces the lower level with its Karush-Kuhn-Tucker conditions and then applies the alternating direction method of multipliers to solve the equivalent problem. Extensive experiments demonstrate the effectiveness and robustness of our method.","keywords_author":["Sparse representation","dictionary learning","bilevel optimization","recognition","alternating direction method"],"keywords_other":["bilevel optimization problem","supervised dictionary learning method","Computational modeling","Testing","recognition task","classification error minimization","Data structures","Dictionaries","bilevel model-based discriminative dictionary learning","sparse code learning model","learning (artificial intelligence)","intrinsic data structure","Training","Learning systems","Laplace equations","signal reconstruction","Karush-Kuhn-Tucker condition","optimisation","reconstruction error"],"max_cite":4.0,"pub_year":2016.0,"sources":"['ieee']","rawkeys":["bilevel optimization problem","supervised dictionary learning method","data structures","learning systems","bilevel optimization","recognition task","classification error minimization","karush-kuhn-tucker condition","bilevel model-based discriminative dictionary learning","computational modeling","sparse code learning model","learning (artificial intelligence)","intrinsic data structure","recognition","training","sparse representation","alternating direction method","dictionary learning","dictionaries","signal reconstruction","optimisation","testing","laplace equations","reconstruction error"],"tags":["supervised dictionary learning method","data structures","karush kuhn tucker condition","learning systems","recognition tasks","classification error minimization","machine learning","bilevel model-based discriminative dictionary learning","computational modeling","sparse code learning model","recognition","intrinsic data structure","training","sparse representation","alternating direction method","dictionary learning","dictionaries","signal reconstruction","optimisation","testing","bi-level optimization","laplace equations","bilevel optimization problems","reconstruction error"]},{"p_id":34222,"title":"Performance evaluation of different support vector machine kernels for face emotion recognition","abstract":"\u00a9 2015 IEEE.Face emotion recognition systems identify emotions expressed on the face without necessarily identifying the person involved, as in Face recognition. Support Vector Machine (SVM) has been shown to give better performance on other classification tasks but has not been applied to emotion recognition, especially with still face images. This research work analyses the performance of four different SVM kernels (Radial Basis Function, Linear Function, Quadratic Function and Polynomial Function) for face emotion recognition. A database of 714 face emotion images was created by capturing twice, seven facial expressions of 51 persons with a digital camera. Principal component analysis was used to extract distinctive features by reducing the dimensionality of each image from 571 \u00d7 800 pixels to four smaller dimensions; 50 \u00d7 50, 100 \u00d7 100, 150 \u00d7 150 and 200 \u00d7 200 pixels. The performance of four SVM kernels were evaluated for face emotion recognition with 476 training and 238 testing to recognise seven emotions; Fear, Anger, Disgust, Happiness, Sadness, Surprise and Neutral. The SVM multi-class classification scheme was used in the design of our experiments. Empirical results indicate that the Quadratic Function SVM kernel performs best for face emotion recognition with an average accuracy of 99.33%. Also, larger dimensions of the reduced image results in better performance accuracy though with increasing computation time. We intend to experiment on other classifiers for emotion recognition in our future work.","keywords_author":["artificial intelligence","biometrics","emotion","face","machine learning","recognition","SVM"],"keywords_other":["recognition","Face emotion recognition","Multi-class classification","emotion","Classification tasks","Radial basis functions","face","Polynomial functions"],"max_cite":2.0,"pub_year":2015.0,"sources":"['scp']","rawkeys":["artificial intelligence","recognition","face emotion recognition","emotion","machine learning","svm","classification tasks","biometrics","polynomial functions","face","multi-class classification","radial basis functions"],"tags":["recognition","face emotion recognition","emotion","machine learning","classification tasks","biometrics","polynomial functions","face","multi-class classification","radial basis functions"]},{"p_id":42414,"title":"DANoC: An efficient algorithm and hardware codesign of deep neural networks on chip","abstract":"\u00a9 2012 IEEE. Deep neural networks (NNs) are the state-of-the-art models for understanding the content of images and videos. However, implementing deep NNs in embedded systems is a challenging task, e.g., a typical deep belief network could exhaust gigabytes of memory and result in bandwidth and computational bottlenecks. To address this challenge, this paper presents an algorithm and hardware codesign for efficient deep neural computation. A hardware-oriented deep learning algorithm, named the deep adaptive network, is proposed to explore the sparsity of neural connections. By adaptively removing the majority of neural connections and robustly representing the reserved connections using binary integers, the proposed algorithm could save up to 99.9% memory utility and computational resources without undermining classification accuracy. An efficient sparse-mapping-memory-based hardware architecture is proposed to fully take advantage of the algorithmic optimization. Different from traditional Von Neumann architecture, the deep-adaptive network on chip (DANoC) brings communication and computation in close proximity to avoid power-hungry parameter transfers between on-board memory and on-chip computational units. Experiments over different image classification benchmarks show that the DANoC system achieves competitively high accuracy and efficiency comparing with the state-of-the-art approaches.","keywords_author":["Binary weights","deep belief network (DBN)","deep learning","embedded system","field-programmable gate array (FPGA)","sparse connections","Binary weights","deep belief network (DBN)","deep learning","embedded system","field-programmable gate array (FPGA)","sparse connections"],"keywords_other":["Computational resources","SELECTION","REGRESSION","Computational bottlenecks","Algorithm design and analysis","WEIGHTS","HYPERSPECTRAL IMAGE CLASSIFICATION","Binary weights","Algorithmic optimization","IMPLEMENTATIONS","State-of-the-art approach","RECOGNITION","Deep belief network (DBN)","Classification accuracy"],"max_cite":0.0,"pub_year":2018.0,"sources":"['ieee', 'scp', 'wos']","rawkeys":["computational bottlenecks","recognition","deep belief network (dbn)","deep learning","classification accuracy","selection","weights","hyperspectral image classification","computational resources","field-programmable gate array (fpga)","binary weights","embedded system","sparse connections","implementations","algorithm design and analysis","algorithmic optimization","regression","state-of-the-art approach"],"tags":["algorithm optimization","computational bottlenecks","recognition","classification accuracy","embedded systems","machine learning","fpga","hyperspectral image classification","weight","computational resources","binary weights","sparse connections","implementation","algorithm design and analysis","selection","regression","state-of-the-art approach","deep belief networks"]},{"p_id":1460,"title":"Class-Specific Kernel Discriminant Analysis Revisited: Further Analysis and Extensions","abstract":"In this paper, we revisit class-specific kernel discriminant analysis (KDA) formulation, which has been applied in various problems, such as human face verification and human action recognition. We show that the original optimization problem solved for the determination of class-specific discriminant projections is equivalent to a low-rank kernel regression (LRKR) problem using training data-independent target vectors. In addition, we show that the regularized version of class-specific KDA is equivalent to a regularized LRKR problem, exploiting the same targets. This analysis allows us to devise a novel fast solution. Furthermore, we devise novel incremental, approximate and deep (hierarchical) variants. The proposed methods are tested in human facial image and action video verification problems, where their effectiveness and efficiency is shown.","keywords_author":["Approximation","class-specific kernel discriminant analysis (CSKDA)","incremental learning","low-rank kernel regression (LRKR)","regularization","Approximation","class-specific kernel discriminant analysis (CSKDA)","incremental learning","low-rank kernel regression (LRKR)","regularization"],"keywords_other":["low-rank kernel regression problem","ALGORITHM","regression analysis","Kernel","regularized LRKR problem","human face verification","human facial image","human action recognition","action video verification problems","class-specific kernel discriminant analysis formulation","Optimization","face recognition","training data-independent target vectors","Principal component analysis","FACE VERIFICATION","Training data","class-specific KDA","FEATURES","Training","Learning systems","RECOGNITION","Data models","class-specific discriminant projections","CLASSIFICATION","original optimization problem"],"max_cite":1.0,"pub_year":2016.0,"sources":"['wos', 'ieee']","rawkeys":["low-rank kernel regression problem","incremental learning","regression analysis","approximation","kernel","class-specific kernel discriminant analysis (cskda)","classification","learning systems","human face verification","human facial image","human action recognition","training data","action video verification problems","class-specific kernel discriminant analysis formulation","features","face recognition","training data-independent target vectors","optimization","regularization","algorithm","principal component analysis","recognition","training","regularized lrkr problem","class-specific discriminant projections","class-specific kda","face verification","low-rank kernel regression (lrkr)","data models","original optimization problem"],"tags":["low-rank kernel regression problem","incremental learning","regression analysis","approximation","kernel","class-specific kernel discriminant analysis (cskda)","classification","learning systems","human face verification","human facial image","training data","action video verification problems","class-specific kernel discriminant analysis formulation","features","face recognition","training data-independent target vectors","optimization","algorithms","regularization","principal component analysis","recognition","training","human activity recognition","regularized lrkr problem","class-specific discriminant projections","class-specific kda","face verification","low-rank kernel regression (lrkr)","data models","original optimization problem"]},{"p_id":9661,"title":"Deep Learning for Drug-Induced Liver Injury","abstract":"Drug-induced liver injury (DILI) has been the single most frequent cause of safety-related drug marketing withdrawals for the past 50 years. Recently, deep learning (DL) has been successfully applied in many fields due to its exceptional and automatic learning ability. In this study, DILI prediction models were developed using DL architectures, and the best model trained on 475 drugs predicted an external validation set of 198 drugs with an accuracy of 86.9%, sensitivity of 82.5%, specificity of 92.9%, and area under the curve of 0.955, which is better than the performance of previously described DILI prediction models. Furthermore, with deep analysis, we also identified important molecular features that are related to DILI. Such DL models could improve the prediction of DILI risk in humans. The DL DILI prediction models are freely available at http:\/\/www.repharma.cn\/DILIserver\/DILI_home.php.","keywords_author":null,"keywords_other":["MOLECULES","Humans","ARCHITECTURES","ALGORITHM","Prediction model","Area under the curves","Deep learning","Molecular feature","Automatic-learning","Glycine","Drug-Induced Liver Injury","PROTEINS","ROC Curve","CHEMOINFORMATICS","Safety-Based Drug Withdrawals","Algorithms","Safety-Related","HEPATOTOXICITY","RECOGNITION","Software","Best model","PREDICTION","Liver injuries","Models, Biological","Risk Factors","NEURAL-NETWORKS","DESCRIPTORS"],"max_cite":43.0,"pub_year":2015.0,"sources":"['ieee', 'scp', 'wos']","rawkeys":["area under the curves","neural-networks","molecular feature","glycine","molecules","models","algorithms","descriptors","biological","roc curve","liver injuries","algorithm","recognition","safety-related","chemoinformatics","deep learning","humans","software","prediction model","hepatotoxicity","architectures","proteins","risk factors","drug-induced liver injury","prediction","automatic-learning","safety-based drug withdrawals","best model"],"tags":["predictive models","architecture","molecular feature","glycine","machine learning","molecules","algorithms","descriptors","biological","roc curve","liver injuries","recognition","safety-related","chemoinformatics","neural networks","humans","software","hepatotoxicity","model","risk factors","proteins","drug-induced liver injury","prediction","automatic-learning","safety-based drug withdrawals","best model"]},{"p_id":17858,"title":"DomNet: Protein domain boundary prediction using enhanced general regression network and new profiles","abstract":"The accurate and stable prediction of protein domain boundaries is an important avenue for the prediction of protein structure, function, evolution, and design. Recent research on protein domain boundary prediction has been mainly based on widely known machine learning techniques. In this paper, we propose a new machine learning based domain predictor namely, DomNet that can show a more accurate and stable predictive performance than the existing state-of-the-art models. The DomNet is trained using a novel compact domain profile, secondary structure, solvent accessibility information, and interdomain linker index to detect possible domain boundaries for a target sequence. The performance of the proposed model was compared to nine different machine learning models on the Benchmark_2 dataset in terms of accuracy, sensitivity, specificity, and correlation coefficient. The DomNet achieved the best performance with 71% accuracy for domain boundary identification in multidomains proteins. With the CASP7 benchmark dataset, it again demonstrated superior performance to contemporary domain boundary predictors such as DOMpro, DomPred, DomSSEA, DomCut, and DomainDiscovery. \u00a9 2008 IEEE.","keywords_author":["Domain boundary prediction","Domain linker index","Machine learning","Sequence encoding","Sequence profile"],"keywords_other":["ART models","Data sets","Correlation coefficient (CC)","superior performance","Multidomains","Protein structures","Regression (R2)","Benchmark dataset","Machine learning techniques","secondary structures","Protein domains","Inter domain","Target sequences","domain boundaries","machine-learning","solvent accessibility","Predictive performance","Evolution (CO)","General (CO)"],"max_cite":27.0,"pub_year":2008.0,"sources":"['scp']","rawkeys":["evolution (co)","data sets","domain linker index","superior performance","machine learning","art models","regression (r2)","multidomains","secondary structures","sequence encoding","machine learning techniques","sequence profile","domain boundaries","inter domain","machine-learning","target sequences","domain boundary prediction","solvent accessibility","predictive performance","benchmark dataset","protein domains","general (co)","protein structures","correlation coefficient (cc)"],"tags":["benchmark datasets","data sets","domain linker index","secondary structure","superior performance","machine learning","art models","biological","regression","prediction performance","sequence encoding","recognition","protein structure","machine learning techniques","sequence profile","domain boundaries","inter domain","target sequences","multi domains","solvent accessibility","cloud computing","protein domains","domain boundary prediction"]},{"p_id":1474,"title":"Deep Models for Engagement Assessment with Scarce Label Information","abstract":"Task engagement is delined as loadings on energetic arousal (affect), task motivation, and concentration (cognition) [1]. It is usually challenging and expensive to label cognitive state data, and traditional computational models trained with limited label information for engagement assessment do not perform well because of overlitting. In this paper, we proposed two deep models (i.e., a deep classilier and a deep autoencoder) for engagement assessment with scarce label information. We recruited 15 pilots to conduct a 4-h flight simulation from Seattle to Chicago and recorded their electroencephalograph (EEG) signals during the simulation. Experts carefully examined the EEG signals and labeled 20 min of the EEG data for each pilot. The EEG signals were preprocessed and power spectral features were extracted. The deep models were pretrained by the unlabeled data and were line-tuned by a different proportion of the labeled data (top 1%, 3%, 5%, 10%, 15%, and 20%) to learn new representations for engagement assessment. The models were then tested on the remaining labeled data. We compared performances of the new data representations with the original EEG features for engagement assessment. Experimental results show that the representations learned by the deep models yielded better accuracies for the six scenarios (77.09%, 80.45%, 83.32%, 85.74%, 85.78%, and 86.52%), based on different proportions of the labeled data for training, as compared with the corresponding accuracies (62.73%, 67.19%, 73.38%, 79.18%, 81.47%, and 84.92%) achieved by the original EEG features. Deep models are effective for engagement assessment especially when less label information was used for training.","keywords_author":["Deep learning","electroencephalography (EEG)","engagement assessment","scarce label information","Deep learning","electroencephalography (EEG)","engagement assessment","scarce label information","Deep learning","electroencephalography (EEG)","engagement assessment","scarce label information"],"keywords_other":["engagement assessment","Cognitive state","cognitive state data labeling","SYSTEM","scarce label information","Label information","Flight simulation","Brain models","deep classilier model","TASK","EEG signals","electroencephalograph signal","Feature extraction","EEG signal","Electroencephalography","Machine learning","ARTIFACTS","Different proportions","Data representations","Training","deep autoencoder model","RECOGNITION","power spectral feature extraction","energetic arousal","medical signal processing","NETWORK","electroencephalography","Data models","Computational model","Unlabeled data","Power spectral","task motivation","feature extraction","FUSION","cognition"],"max_cite":0.0,"pub_year":2017.0,"sources":"['scp', 'wos', 'ieee']","rawkeys":["engagement assessment","cognitive state data labeling","scarce label information","deep classilier model","eeg signals","task","data representations","power spectral","different proportions","electroencephalograph signal","brain models","flight simulation","machine learning","system","artifacts","fusion","network","eeg signal","recognition","deep learning","training","unlabeled data","deep autoencoder model","medical signal processing","power spectral feature extraction","energetic arousal","electroencephalography","computational model","label information","task motivation","data models","cognitive state","electroencephalography (eeg)","feature extraction","cognition"],"tags":["engagement assessment","cognitive state data labeling","scarce label information","deep classilier model","eeg signals","brain modeling","task","data representations","power spectral","different proportions","flight simulation","electroencephalographic signals","machine learning","system","artifacts","fusion","computational modeling","recognition","training","eeg","unlabeled data","deep autoencoder model","medical signal processing","networks","energetic arousal","power spectral feature extraction","label information","task motivation","data models","cognitive state","feature extraction","cognition"]},{"p_id":17860,"title":"Intelligent selection of application-specific garbage collectors","abstract":"Java program execution times vary greatly with different garbage collection algorithms. Until now, it has not been possible to determine the best GC algorithm for a particular program without exhaustively profiling that program for all available GC algorithms. This paper presents a new approach. We use machine learning techniques to build a prediction model that, given a single profile run of a previously unseen Java program, can predict a good GC algorithm for that program. We implement this technique in Jikes RVM and test it on several standard benchmark suites. Our technique achieves 5% speedup in overall execution time (averaged across all test programs for all heap sizes) compared with selecting the default GC algorithm in every trial. We present further experiments to show that an oracle predictor could achieve an average 17% speedup on the same experiments. In addition, we provide evidence to suggest that GC behaviour is sometimes independent of program inputs. These observations lead us to propose that intelligent selection of GC algorithms is suitably straightforward, efficient and effective to merit further exploration regarding its potential inclusion in the general Java software deployment process. Copyright \u00a9 2007 ACM.","keywords_author":["Application-specific garbage collection","Machine learning"],"keywords_other":["new approaches","Memory-management","Application-specific garbage collectors","Java software","International symposium","test programs","Benchmark suites","Garbage collection (GC)","prediction modeling","Overall execution","JAVA programs","General (CO)","Machine learning techniques"],"max_cite":27.0,"pub_year":2007.0,"sources":"['scp']","rawkeys":["java software","memory-management","new approaches","machine learning techniques","machine learning","general (co)","application-specific garbage collection","test programs","application-specific garbage collectors","java programs","international symposium","benchmark suites","prediction modeling","overall execution","garbage collection (gc)"],"tags":["java software","new approaches","recognition","predictive models","testing programs","machine learning techniques","machine learning","application-specific garbage collection","memory management","application-specific garbage collectors","international symposium","benchmark suites","garbage collection","overall execution","java program"]},{"p_id":83396,"title":"Predicting the helix-helix interactions from correlated residue mutations","abstract":"Helix-helix interactions are crucial in the structure assembly, stability and function of helix-rich proteins including many membrane proteins. In spite of remarkable progresses over the past decades, the accuracy of predicting protein structures from their amino acid sequences is still far from satisfaction. In this work, we focused on a simpler problem, the prediction of helix-helix interactions, the results of which could facilitate practical protein structure prediction by constraining the sampling space. Specifically, we started from the noisy 2D residue contact maps derived from correlated residue mutations, and utilized ridge detection to identify the characteristic residue contact patterns for helix-helix interactions. The ridge information as well as a few additional features were then fed into a machine learning model HHConPred to predict interactions between helix pairs. In an independent test, our method achieved an F-measure of approximate to 60% for predicting helix-helix interactions. Moreover, although the model was trained mainly using soluble proteins, it could be extended to membrane proteins with at least comparable performance relatively to previous approaches that were generated purely using membrane proteins. All data and source codes are available at or .","keywords_author":["helix-helix interactions","machine learning","ridge detection","residue contact map","protein structure prediction"],"keywords_other":["SELECTION","DATA-BANK","SECONDARY STRUCTURE","RECOGNITION","PROTEIN-STRUCTURE PREDICTION","CONTACT MAPS","MODELS","PATTERNS","MEMBRANE-PROTEINS","RECONSTRUCTION"],"max_cite":0.0,"pub_year":2017.0,"sources":"['wos']","rawkeys":["recognition","residue contact map","ridge detection","protein structure prediction","protein-structure prediction","machine learning","helix-helix interactions","patterns","contact maps","reconstruction","models","data-bank","selection","secondary structure","membrane-proteins"],"tags":["recognition","model","residue contact map","protein structure prediction","ridge detection","machine learning","helix-helix interactions","patterns","contact maps","reconstruction","data-bank","selection","secondary structure","membrane proteins"]},{"p_id":9670,"title":"Extreme learning machine with kernel model based on deep learning","abstract":"Extreme learning machine (ELM) proposed by Huang et al. is a learning algorithm for single-hidden layer feedforward neural networks (SLFNs). ELM has the advantage of fast learning speed and high efficiency, so it brought into public focus. Later someone developed regularized extreme learning machine (RELM) and extreme learning machine with kernel (KELM). But they are the single-hidden layer network structure, so they have deficient in feature extraction. Deep learning (DL) is a multilayer network structure, and it can extract the significant features by learning from a lower layer to a higher layer. As DL mostly uses the gradient descent method, it will spend too much time in the process of adjusting parameters. This paper proposed a novel model of convolutional extreme learning machine with kernel (CKELM) which was based on DL for solving problems-KELM is deficient in feature extraction, and DL spends too much time in the training process. In CKELM model, alternate convolutional layers and subsampling layers add to hidden layer of the original KELM so as to extract features and classify. The convolutional layer and subsampling layer do not use the gradient algorithm to adjust parameters because of some architectures yielded good performance with random weights. Finally, we took experiments on USPS and MNIST database. The accuracy of CKELM is higher than ELM, RELM and KELM, which proved the validity of the optimization model. To make the proposed approach more convincing, we compared with other ELM-based methods and other DL methods and also achieved satisfactory results.","keywords_author":["CKELM","Convolutional neural network (CNN)","Deep learning (DL)","Extreme learning machine (ELM)","Extreme learning machine (ELM)","Deep-learning (DL)","Convolutional neural network (CNN)","CKELM"],"keywords_other":["Deep learning","Hidden layer networks","FACE","Single-hidden layer feedforward neural networks","CKELM","Optimization modeling","NEURAL-NETWORKS","RECOGNITION","Convolutional neural network","Extreme learning machine","Gradient Descent method"],"max_cite":3.0,"pub_year":2017.0,"sources":"['scp', 'wos']","rawkeys":["neural-networks","extreme learning machine (elm)","recognition","deep learning","face","deep-learning (dl)","convolutional neural network (cnn)","hidden layer networks","gradient descent method","optimization modeling","convolutional neural network","single-hidden layer feedforward neural networks","extreme learning machine","ckelm","deep learning (dl)"],"tags":["recognition","neural networks","machine learning","optimization modeling","gradient descent method","convolutional neural network","single-hidden layer feedforward neural networks","face","ckelm","extreme learning machine","hidden layer networks"]},{"p_id":1484,"title":"Deep Transfer Metric Learning","abstract":"Conventional metric learning methods usually assume that the training and test samples are captured in similar scenarios so that their distributions are assumed to be the same. This assumption does not hold in many real visual recognition applications, especially when samples are captured across different data sets. In this paper, we propose a new deep transfer metric learning (DTML) method to learn a set of hierarchical nonlinear transformations for cross-domain visual recognition by transferring discriminative knowledge from the labeled source domain to the unlabeled target domain. Specifically, our DTML learns a deep metric network by maximizing the inter-class variations and minimizing the intra-class variations, and minimizing the distribution divergence between the source domain and the target domain at the top layer of the network. To better exploit the discriminative information from the source domain, we further develop a deeply supervised transfer metric learning (DSTML) method by including an additional objective on DTML, where the output of both the hidden layers and the top layer are optimized jointly. To preserve the local manifold of input data points in the metric space, we present two new methods, DTML with autoencoder regularization and DSTML with autoencoder regularization. Experimental results on face verification, person re-identification, and handwritten digit recognition validate the effectiveness of the proposed methods.","keywords_author":["Deep metric learning","deep transfer metric learning","transfer learning","face verification","person reidentification","Deep metric learning","deep transfer metric learning","transfer learning","face verification","person reidentification"],"keywords_other":["GENERAL FRAMEWORK","Measurement","deep transfer metric learning method","Neural networks","handwritten character recognition","DSTML-with-autoencoder regularization method","Manifolds","deep metric network","interclass variation maximization","handwritten digit recognition","deeply supervised transfer metric learning method","Visualization","Machine learning","distribution divergence minimization","biometrics (access control)","face recognition","unlabeled target domain","intraclass variation minimization","FACE VERIFICATION","learning (artificial intelligence)","Training","Learning systems","RECOGNITION","person reidentification","KERNEL","ADAPTATION","discriminative knowledge transfer","labeled source domain","CLASSIFICATION","hierarchical nonlinear transformations","face verification","cross-domain visual recognition","DTML-with-autoencoder regularization method"],"max_cite":10.0,"pub_year":2016.0,"sources":"['ieee', 'wos']","rawkeys":["deep transfer metric learning","manifolds","deep transfer metric learning method","kernel","classification","adaptation","handwritten character recognition","learning systems","visualization","deep metric network","interclass variation maximization","deep metric learning","handwritten digit recognition","transfer learning","deeply supervised transfer metric learning method","dtml-with-autoencoder regularization method","machine learning","distribution divergence minimization","biometrics (access control)","face recognition","unlabeled target domain","intraclass variation minimization","measurement","learning (artificial intelligence)","recognition","neural networks","training","person reidentification","discriminative knowledge transfer","labeled source domain","dstml-with-autoencoder regularization method","general framework","hierarchical nonlinear transformations","face verification","cross-domain visual recognition"],"tags":["deep transfer metric learning","person re-identification","manifolds","deep transfer metric learning method","kernel","classification","adaptation","handwritten character recognition","learning systems","visualization","deep metric network","interclass variation maximization","deep metric learning","handwritten digit recognition","transfer learning","deeply supervised transfer metric learning method","dtml-with-autoencoder regularization method","machine learning","distribution divergence minimization","face recognition","unlabeled target domain","intraclass variation minimization","measurement","recognition","neural networks","training","biometrics","discriminative knowledge transfer","labeled source domain","dstml-with-autoencoder regularization method","general framework","hierarchical nonlinear transformations","face verification","cross-domain visual recognition"]},{"p_id":91601,"title":"A New Approach to Track Multiple Vehicles With the Combination of Robust Detection and Two Classifiers","abstract":"It plays an important role to accurately track multiple vehicles in intelligent transportation, especially in intelligent vehicles. Due to complicated traffic environments it is difficult to track multiple vehicles accurately and robustly, especially when there are occlusions among vehicles. To alleviate these problems, a new approach is proposed to track multiple vehicles with the combination of robust detection and two classifiers. An improved ViBe algorithm is proposed for robust and accurate detection of multiple vehicles. It uses the gray-scale spatial information to build dictionary of pixel life length to make ghost shadows and object's residual shadows quickly blended into the samples of the background. The improved algorithm takes good post-processing method to restrain dynamic noise. In this paper, we also design a method using two classifiers to further attack the problem of failure to track vehicles with occlusions and interference. It classifies tracking rectangles with confidence values between two thresholds through combining local binary pattern with support vector machine (SVM) classifier and then using a convolutional neural network (CNN) classifier for the second time to remove the interference areas between vehicles and other moving objects. The two classifiers method has both time efficiency advantage of SVM and high accuracy advantage of CNN. Comparing with several existing methods, the qualitative and quantitative analysis of our experiment results showed that the proposed method not only effectively removed the ghost shadows, and improved the detection accuracy and real-time performance, but also was robust to deal with the occlusion of multiple vehicles in various traffic scenes.","keywords_author":["Multiple vehicles tracking","robust detection","detection accuracy","support vector machine (SVM)","convolutional neural network (CNN)","occlusion"],"keywords_other":["OPTICAL-FLOW","SVM","LBP","ALGORITHM","MODEL","RECOGNITION","OBJECTS","VISUAL TRACKING","IMAGES"],"max_cite":0.0,"pub_year":2018.0,"sources":"['wos']","rawkeys":["algorithm","occlusion","multiple vehicles tracking","optical-flow","images","model","recognition","detection accuracy","objects","convolutional neural network (cnn)","support vector machine (svm)","svm","visual tracking","robust detection","lbp"],"tags":["occlusion","multiple vehicles tracking","recognition","images","model","detection accuracy","machine learning","objects","visual tracking","local binary patterns","convolutional neural network","algorithms","robust detection","optical flow"]},{"p_id":91602,"title":"Automatic vehicle classification using linked visual words","abstract":"An improvement in the method of automatic vehicle classification is investigated. The challenges are to correctly classify vehicles regardless of changes in illumination, differences in points of view of the camera, and variations in the types of vehicles. Our proposed appearance-based feature extraction algorithm is called linked visual words (LVWs) and is based on the existing technique bag-of-visual word (BoVW) with the addition of spatial information to improve accuracy of classification. In addition, to prevent over-fitting due to a large number of LVWs, four common sampling techniques with LVWs are investigated. Our results suggest that the sampling of LVWs using TF-IDF with grouping improved the accuracy of classification for the test dataset. In summary, the proposed system is able to classify nine types of vehicles and work with surveillance cameras in real-world scenarios. The classification accuracy of the proposed system is 5.58% and 4.27% higher on average for three datasets when compared with BoVW + SVM and Lenet-5, respectively. (C) 2017 SPIE and IS&T","keywords_author":["linked visual word","vehicle classification","bag-of-visual word","intelligent transportation system"],"keywords_other":["PROFILES","BAG","SYSTEM","MODEL","RECOGNITION","NEURAL-NETWORK"],"max_cite":1.0,"pub_year":2017.0,"sources":"['wos']","rawkeys":["recognition","model","vehicle classification","bag-of-visual word","intelligent transportation system","bag","system","linked visual word","neural-network","profiles"],"tags":["recognition","model","neural networks","vehicle classification","system","intelligent transportation systems","linked visual word","bag-of-visual-words","bagging","profiles"]},{"p_id":9683,"title":"When Ensemble Learning Meets Deep Learning: a New Deep Support Vector Machine for Classification","abstract":"Recently, Deep Learning (DL) method has received a significant breakthrough in the data representation, whose success mainly depends on its deep structure. In this paper, we focus on the DL research based on Support Vector Machine (SVM), and first present an Ex-Adaboost learning strategy, and then propose a new Deep Support Vector Machine (called DeepSVM). Unlike other DL algorithms based on SVM, in each layer, Ex-Adaboost is applied to not only select SVMs with the minimal error rate and the highest diversity, but also to produce the weight for each feature. In this way, new training data is obtained. By stacking these SVMs into multiple layers following the same way, we finally acquire a new set of deep features that can greatly boost the classification performance. In the end, the training data represented by these new features is regarded as the input for a standard SVM classifier. In the experimental part, we offer these answers to the following questions: 1) is the deep structure of DeepSVM really useful for classification problem? 2) Does Ex-Adaboost work, and is it helpful for further improving on DeepSVM's performance with respect to the deep structure? 3) How much improvement in classification accuracy of DeepSVM, compared with other exist algorithms? (C) 2016 Elsevier B.V. All rights reserved.","keywords_author":["Deep architectures","Pattern recognition","Support vector machine","Pattern recognition","Deep architectures","Support vector machine"],"keywords_other":["Multiple layers","Classification performance","Data representations","Deep architectures","Adaboost learning","NETS","Ensemble learning","MODEL","SVM classifiers","NEURAL-NETWORKS","RECOGNITION","Classification accuracy"],"max_cite":6.0,"pub_year":2016.0,"sources":"['ieee', 'scp', 'wos']","rawkeys":["neural-networks","recognition","classification performance","data representations","model","classification accuracy","deep architectures","multiple layers","pattern recognition","svm classifiers","nets","ensemble learning","support vector machine","adaboost learning"],"tags":["recognition","classification performance","data representations","model","neural networks","classification accuracy","deep architectures","multiple layers","pattern recognition","machine learning","svm classifiers","nets","ensemble learning","adaboost learning"]},{"p_id":42452,"title":"Detection of a casting defect tracked by deep convolution neural network","abstract":"\u00a9 2018, Springer-Verlag London Ltd., part of Springer Nature. In order to relieve the problem of a false and missed detection of casting defects in X-ray detection, a robust detection method based on vision attention mechanism and deep learning of feature map is proposed. The ray images are used as input sequence, the false detection is eliminated by the intra-frame attention strategy, and the missed detection is excluded by the inter-frame deep convolution neural network (DCNN) strategy. In the intra-frame detection stage, the center-peripheral difference method is proposed to simulate the difference operation of biological vision; the suspicious defect area is directly detected according to the gradient threshold in this stage. In the inter-frame learning stage, the convolution neural network is established based on deep learning strategy to extract defect feature from a suspicious defect area; a deep learning feature vector is obtained in this stage. The similarity degree of the suspicious defect area is computed by a feature vector; a casting defect is tracked by the similarity matching of the suspicious defect in continuous frames; then, the false defects (such as noise) is excluded after defect tracking. The experimental results show that the false rate and missed rate for detection of casting defects are less than 4%, and the accuracy of the defect detection is more than 96%, which proves the robustness of the proposed method.","keywords_author":["Casting defect","Convolution neural network","Deep learning","Ray image","Casting defect","Deep learning","Ray image","Convolution neural network"],"keywords_other":["Ray image","Gradient thresholds","Biological visions","Similarity-matching","Similarity degree","RECOGNITION","Attention mechanisms","Convolution neural network","OPTIMIZATION","Casting defect"],"max_cite":0.0,"pub_year":2018.0,"sources":"['scp', 'wos']","rawkeys":["recognition","convolution neural network","deep learning","ray image","attention mechanisms","similarity degree","optimization","gradient thresholds","similarity-matching","biological visions","casting defect"],"tags":["recognition","biological vision","ray image","attention mechanisms","machine learning","similarity degree","convolutional neural network","optimization","gradient thresholds","similarity-matching","casting defect"]},{"p_id":1493,"title":"Unsupervised 3D Local Feature Learning by Circle Convolutional Restricted Boltzmann Machine","abstract":"Extracting local features from 3D shapes is an important and challenging task that usually requires carefully designed 3D shape descriptors. However, these descriptors are hand-crafted and require intensive human intervention with prior knowledge. To tackle this issue, we propose a novel deep learning model, namely circle convolutional restricted Boltzmann machine (CCRBM), for unsupervised 3D local feature learning. CCRBM is specially designed to learn from raw 3D representations. It effectively overcomes obstacles such as irregular vertex topology, orientation ambiguity on the 3D surface, and rigid or slightly non-rigid transformation invariance in the hierarchical learning of 3D data that cannot be resolved by the existing deep learning models. Specifically, by introducing the novel circle convolution, CCRBM holds a novel ring-like multi-layer structure to learn 3D local features in a structure preserving manner. Circle convolution convolves across 3D local regions via rotating a novel circular sector convolution window in a consistent circular direction. In the process of circle convolution, extra points are sampled in each 3D local region and projected onto the tangent plane of the center of the region. In this way, the projection distances in each sector window are employed to constitute a novel local raw 3D representation called projection distance distribution (PDD). In addition, to eliminate the initial location ambiguity of a sector window, the Fourier transform modulus is used to transform the PDD into the Fourier domain, which is then conveyed to CCRBM. Experiments using the learned local features are conducted on three aspects: global shape retrieval, partial shape retrieval, and shape correspondence. The experimental results show that the learned local features outperform other state-of-the-art 3D shape descriptors.","keywords_author":["3D shapes","Circle convolutional restricted Boltzmann machine","deep learning","fourier transform modulus","geometry processing","projection distance distribution","Circle convolutional restricted Boltzmann machine","deep learning","projection distance distribution","geometry processing","fourier transform modulus","3D shapes","Circle convolutional restricted Boltzmann machine","deep learning","projection distance distribution","geometry processing","fourier transform modulus","3D shapes"],"keywords_other":["Multilayer structures","Fourier transform modulus","Non-rigid transformation","Shape correspondences","Two dimensional displays","unsupervised 3D local feature learning","Feature extraction","convolution","SHAPE RETRIEVAL","Machine learning","global shape retrieval","CCRBM","local feature extraction","partial shape retrieval","Fourier transforms","Convolution","Projection distance","feedforward neural nets","novel ring-like multilayer structure","image representation","MODEL RETRIEVAL","Restricted boltzmann machine","deep learning model","3D local regions","Geometry processing","DIFFUSION","image retrieval","Shape","RECOGNITION","3-D OBJECT RETRIEVAL","circle convolutional restricted Boltzmann machine","unsupervised learning","novel circular sector convolution window","consistent circular direction","SIMILARITY","ROBUST","Hierarchical learning","REPRESENTATION","PDD","Three-dimensional displays","3-D shape","3D shape descriptors","NEURAL-NETWORKS","novel local raw 3D representation","shape correspondence","projection distance distribution","Boltzmann machines","feature extraction","DESCRIPTORS","Solid modeling"],"max_cite":4.0,"pub_year":2016.0,"sources":"['scp', 'wos', 'ieee']","rawkeys":["projection distance","neural-networks","pdd","non-rigid transformation","convolution","machine learning","3-d shape","solid modeling","global shape retrieval","fourier transforms","local feature extraction","partial shape retrieval","robust","unsupervised 3d local feature learning","descriptors","3-d object retrieval","feedforward neural nets","hierarchical learning","image representation","novel ring-like multilayer structure","3d local regions","recognition","deep learning model","deep learning","model retrieval","boltzmann machines","image retrieval","shape correspondences","ccrbm","3d shapes","novel local raw 3d representation","unsupervised learning","circle convolutional restricted boltzmann machine","novel circular sector convolution window","consistent circular direction","fourier transform modulus","multilayer structures","restricted boltzmann machine","3d shape descriptors","two dimensional displays","shape retrieval","similarity","representation","shape","three-dimensional displays","projection distance distribution","feature extraction","shape correspondence","geometry processing","diffusion"],"tags":["robustness","projection distance","pdd","non-rigid transformation","convolution","machine learning","3-d shape","solid modeling","global shape retrieval","3d object retrieval","local feature extraction","partial shape retrieval","unsupervised 3d local feature learning","descriptors","fourier transform","feedforward neural nets","hierarchical learning","image representation","novel ring-like multilayer structure","3d local regions","recognition","deep learning model","model retrieval","neural networks","boltzmann machines","image retrieval","ccrbm","novel local raw 3d representation","unsupervised learning","circle convolutional restricted boltzmann machine","novel circular sector convolution window","consistent circular direction","fourier transform modulus","multilayer structures","restricted boltzmann machine","3d shape descriptors","two dimensional displays","shape retrieval","similarity","representation","shape","three-dimensional displays","projection distance distribution","feature extraction","shape correspondence","geometry processing","diffusion"]},{"p_id":9706,"title":"Emotion and basal ganglia (II): What can we learn from subthalamic nucleus deep brain stimulation in Parkinson's disease?","abstract":"The subthalamic nucleus deep-brain stimulation Parkinson's disease patient model seems to represent a unique opportunity for studying the functional role of the basal ganglia and notably the subthalamic nucleus in human emotional processing. Indeed, in addition to constituting a therapeutic advance for severely disabled Parkinson's disease patients, deep brain stimulation is a technique, which selectively modulates the activity of focal structures targeted by surgery. There is growing evidence of a link between emotional impairments and deep-brain stimulation of the subthalamic nucleus. In this context, according to the definition of emotional processing exposed in the companion paper available in this issue, the aim of the present review will consist in providing a synopsis of the studies that investigated the emotional disturbances observed in subthalamic nucleus deep brain stimulation Parkinson's disease patients. This review leads to the conclusion that several emotional components would be disrupted after subthalamic nucleus deep brain stimulation in Parkinson's disease: subjective feeling, neurophysiological activation, and motor expression. Finally, after a description of the limitations of this study model, we discuss the functional role of the subthalamic nucleus (and the striato-thalamo-cortical circuits in which it is involved) in emotional processing. It seems reasonable to conclude that the striato-thalamo-cortical circuits are indeed involved in emotional processing and that the subthalamic nucleus plays a central in role the human emotional architecture. (C) 2012 Elsevier Masson SAS. All rights reserved.","keywords_author":["Subthalamic nucleus","Emotion","Theory of mind","Parkinson's disease","Deep brain stimulation"],"keywords_other":["RECOGNITION","REGION","FEAR","PET"],"max_cite":1.0,"pub_year":2012.0,"sources":"['wos']","rawkeys":["parkinson's disease","fear","subthalamic nucleus","recognition","theory of mind","emotion","pet","deep brain stimulation","region"],"tags":["parkinson's disease","fear","subthalamic nucleus","recognition","theory of mind","emotion","positron-emission-tomography","regions","deep brain stimulation"]},{"p_id":9718,"title":"A fused deep learning architecture for viewpoint classification of echocardiography","abstract":"This study extends the state of the art of deep learning convolutional neural network (CNN) to the classification of video images of echocardiography, aiming at assisting clinicians in diagnosis of heart diseases. Specifically, the architecture of neural networks is established by embracing hand-crafted features within a data-driven learning framework, incorporating both spatial and temporal information sustained by the video images of the moving heart and giving rise to two strands of two-dimensional convolutional neural network (CNN). In particular, the acceleration measurement along the time direction at each point is calculated using dense optical flow technique to represent temporal motion information. Subsequently, the fusion of both networks is conducted via linear integrations of the vectors of class scores obtained from each of the two networks. As a result, this architecture maintains the best classification results for eight viewpoint categories of echo videos with 92.1% accuracy rate whereas 89.5% is achieved using only single spatial CNN network. When concerning only three primary locations, 98% of accuracy rate is realised. In addition, comparisons with a number of well-known hand-engineered approaches are also performed, including 2D KAZE, 2D KAZE with Optical Flow, 3D KAZA, Optical Flow, 2D SIFT and 3D SIFT, which delivers accuracy rate of 89.4%, 84.3%, 87.9%, 79.4%, 83.8% and 73.8% respectively. (C) 2016 Elsevier B.V. All rights reserved.","keywords_author":["Classification architecture for echo video images","Convolutional neural network","Deep learning","Echocardiography","KAZE","SIFT","SURF","Deep leaming","Classification architecture for echo video images","Convolutional neural network","Echocardiography","KAZE","SIFT","SURF"],"keywords_other":["Deep learning","SIFT","REPRESENTATION","VIDEOS","KAZE FEATURES","RECOGNITION","Convolutional neural network","KAZE","SURF","Video image"],"max_cite":7.0,"pub_year":2017.0,"sources":"['ieee', 'scp', 'wos']","rawkeys":["echocardiography","recognition","sift","deep leaming","deep learning","representation","surf","kaze features","kaze","video image","convolutional neural network","videos","classification architecture for echo video images"],"tags":["echocardiography","recognition","deep leaming","machine learning","representation","surf","video","kaze features","kaze","video image","convolutional neural network","scale invariant feature transforms","classification architecture for echo video images"]},{"p_id":83447,"title":"Automatic fish species classification in underwater videos: exploiting pre-trained deep neural network models to compensate for limited labelled data","abstract":"There is a need for automatic systems that can reliably detect, track and classify fish and other marine species in underwater videos without human intervention. Conventional computer vision techniques do not perform well in underwater conditions where the background is complex and the shape and textural features of fish are subtle. Data-driven classification models like neural networks require a huge amount of labelled data, otherwise they tend to over-fit to the training data and fail on unseen test data which is not involved in training. We present a state-of-the-art computer vision method for fine-grained fish species classification based on deep learning techniques. A cross-layer pooling algorithm using a pre-trained Convolutional Neural Network as a generalized feature detector is proposed, thus avoiding the need for a large amount of training data. Classification on test data is performed by a SVM on the features computed through the proposed method, resulting in classification accuracy of 94.3% for fish species from typical underwater video imagery captured off the coast of Western Australia. This research advocates that the development of automated classification systems which can identify fish from underwater video imagery is feasible and a cost-effective alternative to manual identification by humans.","keywords_author":["deep learning","fish classification","fisheries management","neural networks","stock assessment","underwater video"],"keywords_other":["COMPUTER VISION","ASSEMBLAGES","PROTECTION","SYSTEM","RECOGNITION","REEF FISH","SHIFT"],"max_cite":1.0,"pub_year":2018.0,"sources":"['wos']","rawkeys":["fisheries management","recognition","deep learning","neural networks","reef fish","system","shift","fish classification","computer vision","stock assessment","underwater video","protection","assemblages"],"tags":["fisheries management","recognition","neural networks","reef fish","machine learning","system","shift","fish classification","computer vision","stock assessment","underwater video","protection","assemblages"]},{"p_id":1543,"title":"Learning Cascaded Deep Auto-Encoder Networks for Face Alignment","abstract":"In this paper, we propose a new cascaded deep auto-encoder networks (CDAN) approach for face alignment. Our framework consists of a global exemplar-based deep auto-encoder network (GEDAN) and a series of localized deep auto-encoder networks (LDAN) in a cascaded fashion. The global network takes a low-resolution holistic facial image as input and generates a preliminary facial landmark configuration. The following localized networks sample pose-indexed local features around current landmark positions, and refine the landmark positions with increasingly higher image resolutions. Our network architectures are designed to achieve greater robustness against pose variations as well as higher landmark estimation accuracy. Experimental results on three datasets show that the proposed approach achieves superior alignment accuracy with real-time speed.","keywords_author":["Auto-encoder","deep learning","Face alignment","Auto-encoder","deep learning","face alignment","Auto-encoder","deep learning","face alignment"],"keywords_other":["pose estimation","NETS","facial landmark configuration","LDAN","Pose variation","Face recognition","Deep learning","global exemplar-based deep auto-encoder network","Face alignment","Feature extraction","CDAN","GEDAN","Machine learning","face recognition","Auto encoders","image resolutions","Face","pose variations","image resolution","global network","landmark estimation accuracy","face alignment","pose-indexed local features","learning (artificial intelligence)","deep learning","Shape","landmark positions","WILD","RECOGNITION","Low resolution","Alignment accuracy","network architectures","cascaded deep auto-encoder networks","Real-time systems","low-resolution holistic facial image","Image resolution","REPRESENTATION","Facial landmark","ACTIVE APPEARANCE MODELS","localized deep auto-encoder networks","feature extraction","Global networks"],"max_cite":7.0,"pub_year":2016.0,"sources":"['ieee', 'scp', 'wos']","rawkeys":["active appearance models","pose estimation","auto-encoder","ldan","gedan","facial landmark configuration","global networks","low resolution","facial landmark","global exemplar-based deep auto-encoder network","cdan","machine learning","real-time systems","face recognition","image resolutions","pose variations","image resolution","global network","landmark estimation accuracy","face alignment","pose-indexed local features","learning (artificial intelligence)","recognition","deep learning","pose variation","landmark positions","alignment accuracy","nets","network architectures","face","cascaded deep auto-encoder networks","wild","low-resolution holistic facial image","auto encoders","representation","shape","localized deep auto-encoder networks","feature extraction"],"tags":["active appearance models","pose estimation","ldan","gedan","facial landmark configuration","global networks","low resolution","facial landmark","global exemplar-based deep auto-encoder network","cdan","machine learning","real-time systems","face recognition","image resolution","landmark estimation accuracy","face alignment","pose-indexed local features","recognition","pose variation","landmark positions","alignment accuracy","nets","face","cascaded deep auto-encoder networks","wild","low-resolution holistic facial image","network architecture","auto encoders","representation","shape","localized deep auto-encoder networks","feature extraction"]},{"p_id":9737,"title":"Text feature extraction based on deep learning: a review","abstract":"Selection of text feature item is a basic and important matter for text mining and information retrieval. Traditional methods of feature extraction require handcrafted features. To hand-design, an effective feature is a lengthy process, but aiming at new applications, deep learning enables to acquire new effective feature representation from training data. As a new feature extraction method, deep learning has made achievements in text mining. The major difference between deep learning and conventional methods is that deep learning automatically learns features from big data, instead of adopting handcrafted features, which mainly depends on priori knowledge of designers and is highly impossible to take the advantage of big data. Deep learning can automatically learn feature representation from big data, including millions of parameters. This thesis outlines the common methods used in text feature extraction first, and then expands frequently used deep learning methods in text feature extraction and its applications, and forecasts the application of deep learning in feature extraction.","keywords_author":["Deep learning","Feature extraction","Natural language processing","Text characteristic","Text mining","Deep learning","Feature extraction","Text characteristic","Natural language processing","Text mining"],"keywords_other":["Text mining","DIMENSION REDUCTION","ITS applications","Feature extraction methods","Learning methods","FEATURE-SELECTION","Conventional methods","CLASSIFICATION","ALGORITHM","Feature representation","RECOGNITION","NEURAL-NETWORK","Priori knowledge","Text characteristic"],"max_cite":1.0,"pub_year":2017.0,"sources":"['scp', 'wos']","rawkeys":["algorithm","recognition","text mining","feature extraction methods","deep learning","its applications","learning methods","text characteristic","natural language processing","priori knowledge","conventional methods","feature-selection","classification","feature extraction","neural-network","feature representation","dimension reduction"],"tags":["recognition","text mining","feature extraction methods","neural networks","its applications","learning methods","text characteristic","machine learning","natural language processing","conventional methods","priori knowledge","feature selection","classification","feature extraction","algorithms","feature representation","dimensionality reduction"]},{"p_id":9738,"title":"Prediction of human protein subcellular localization using deep learning","abstract":"Protein subcellular localization (PSL), as one of the most critical characteristics of human cells, plays an important role for understanding specific functions and biological processes in cells. Accurate prediction of protein subcellular localization is a fundamental and challenging problem, for which machine learning algorithms have been widely used. Traditionally, the performance of PSL prediction highly depends on handcrafted feature descriptors to represent proteins. In recent years, deep learning has emerged as a hot research topic in the field of machine learning, achieving outstanding success in learning high-level latent features within data samples. In this paper, to accurately predict protein subcellular locations, we propose a deep learning based predictor called DeepPSL by using Stacked Auto-Encoder (SAE) networks. In this predictor, we automatically learn high-level and abstract feature representations of proteins by exploring non-linear relations among diverse subcellular locations, addressing the problem of the need of handcrafted feature representations. Experimental results evaluated with three-fold cross validation show that the proposed DeepPSL outperforms traditional machine learning based methods. It is expected that DeepPSL, as the first predictor in the field of PSL prediction, has great potential to be a powerful computational method complementary to existing tools. (C) 2017 Elsevier Inc. All rights reserved.","keywords_author":["Deep learning","Feature representation","Protein subcellular localization","Protein subcellular localization","Feature representation","Deep learning"],"keywords_other":["AMINO-ACID-COMPOSITION","LOCATIONS","RNA","NETWORKS","Protein subcellular localization","Protein subcellular location","SUPPORT VECTOR MACHINES","SEQUENCE","WEB SERVER","Feature representation","Hot research topics","Three fold cross validation","Subcellular location","Accurate prediction","IDENTIFICATION","RECOGNITION","Feature descriptors","DNA"],"max_cite":3.0,"pub_year":2018.0,"sources":"['ieee', 'scp', 'wos']","rawkeys":["identification","sequence","amino-acid-composition","subcellular location","hot research topics","rna","three fold cross validation","web server","recognition","feature descriptors","deep learning","dna","networks","feature representation","locations","protein subcellular localization","support vector machines","accurate prediction","protein subcellular location"],"tags":["identification","recognition","subcellular location","feature descriptors","dna","machine learning","rna","three fold cross validation","feature representation","networks","protein subcellular localization","sequence","web server","amino-acid-composition","accurate prediction","protein subcellular location","location","hot research topics"]},{"p_id":9739,"title":"Deep Correlation Feature Learning for Face Verification in the Wild","abstract":"Convolutional neural networks (CNNs) commonly uses the softmax loss function as the supervision signal. In order to enhance the discriminative power of the deeply learned features, this letter proposes a new supervision signal, called correlation loss, for face verification task. Specifically, the correlation loss encourages the large correlation between the deep feature vectors and their corresponding weight vectors in softmax loss. With the joint supervision of softmax loss and correlation loss, the deep correlation feature learning (DCFL) network can learn the deep features with both the interclass separability and the intraclass compactness, which are highly discriminative for face verification. More importantly, by applying the weight vector of softmax function as the class prototype, the proposed correlation loss function is easy to be optimized during the backpropatation of CNN. Finally, the DCFL method achieves 99.55% and 96.06% face verification accuracy using a 64-layer ResNet on the labeled face in-the-Wild (LFW) and you-tube face (YTF) benchmark, respectively.","keywords_author":["Convolutional neural networks (CNNs)","deep learning","face verification","feature learning","softmax","Convolutional neural networks (CNNs)","deep learning","feature learning","face verification","softmax"],"keywords_other":["Corresponding weights","Softmax","REPRESENTATION","Correlation features","RECOGNITION","Convolutional neural network","Correlation loss","Discriminative power","Feature learning","Face Verification"],"max_cite":0.0,"pub_year":2017.0,"sources":"['ieee', 'scp', 'wos']","rawkeys":["recognition","deep learning","discriminative power","representation","correlation loss","face verification","softmax","correlation features","convolutional neural network","feature learning","convolutional neural networks (cnns)","corresponding weights"],"tags":["recognition","discriminative power","machine learning","representation","correlation loss","face verification","softmax","correlation features","convolutional neural network","feature learning","corresponding weights"]},{"p_id":1550,"title":"Layerwise Class-Aware Convolutional Neural Network","abstract":"The human vision system usually has a specifically activated area of neurons when recognizing a category of images. Inspired by this visual mechanism, we propose a layerwise class-aware convolutional neural network architecture to explicitly discover category-tailored neurons on intermediate hidden layers to improve the network learning ability. Instead of directly selecting activated neurons for different categories, we inversely suppress those neurons of intermediate layers irrelevant with the given target class to produce a class-specific subnetwork, which implicitly enhances the discriminability of hidden layer features due to the increase of the inter-class discrepancy on them. Together with the classifier of the top layer, we jointly learn this network by formulating the suppressor of hidden layers as a penalty term in the objective function. To address class-specific neuron suppression in each hidden layer, we also introduce a statistic method based on mutual information to dynamically and automatically update the suppressed neurons during the network training. Extensive experiments demonstrate that the proposed model is superior to the state-of-the-art models.","keywords_author":["Convolutional neural network (CNN)","deep learning","mutual information","object classification","Convolutional neural network (CNN)","deep learning","mutual information","object classification","Convolutional neural network (CNN)","deep learning","mutual information","object classification"],"keywords_other":["Convolutional codes","Biological neural networks","Statistic method","Human vision systems","Mutual informations","Computer architecture","Computational modeling","Network training","class-specific subnetwork","Machine learning","network learning ability","inter-class discrepancy","class-specific neuron suppression","learning (artificial intelligence)","hidden layer features","Training","Objective functions","network training","human vision system","RECOGNITION","neural nets","Mutual information","Convolutional neural network","Intermediate layers","Object classification","activated neurons","specifically activated area","visual mechanism","suppressed neurons","layerwise class-aware convolutional neural network architecture"],"max_cite":0.0,"pub_year":2017.0,"sources":"['scp', 'wos', 'ieee']","rawkeys":["convolutional neural network","mutual informations","class-specific subnetwork","machine learning","object classification","network learning ability","biological neural networks","computer architecture","computational modeling","inter-class discrepancy","class-specific neuron suppression","convolutional codes","learning (artificial intelligence)","hidden layer features","deep learning","recognition","training","convolutional neural network (cnn)","network training","human vision system","intermediate layers","neural nets","statistic method","mutual information","objective functions","human vision systems","activated neurons","visual mechanism","layerwise class-aware convolutional neural network architecture","specifically activated area","suppressed neurons"],"tags":["convolutional neural network","active neurons","class-specific subnetwork","machine learning","object classification","network learning ability","biological neural networks","computer architecture","computational modeling","inter-class discrepancy","class-specific neuron suppression","convolutional codes","recognition","hidden layer features","neural networks","training","network training","intermediate layers","statistical methods","mutual information","objective functions","human vision systems","visual mechanism","layerwise class-aware convolutional neural network architecture","specifically activated area","suppressed neurons"]},{"p_id":1552,"title":"Mesh Convolutional Restricted Boltzmann Machines for Unsupervised Learning of Features With Structure Preservation on 3-D Meshes","abstract":"Discriminative features of 3-D meshes are significant to many 3-D shape analysis tasks. However, handcrafted descriptors and traditional unsupervised 3-D feature learning methods suffer from several significant weaknesses: 1) the extensive human intervention is involved; 2) the local and global structure information of 3-D meshes cannot be preserved, which is in fact an important source of discriminability; 3) the irregular vertex topology and arbitrary resolution of 3-D meshes do not allow the direct application of the popular deep learning models; 4) the orientation is ambiguous on the mesh surface; and 5) the effect of rigid and nonrigid transformations on 3-D meshes cannot be eliminated. As a remedy, we propose a deep learning model with a novel irregular model structure, called mesh convolutional restricted Boltzmann machines (MCRBMs). MCRBM aims to simultaneously learn structure-preserving local and global features from a novel raw representation, local function energy distribution. In addition, multiple MCRBMs can be stacked into a deeper model, called mesh convolutional deep belief networks (MCDBNs). MCDBN employs a novel local structure preserving convolution (LSPC) strategy to convolve the geometry and the local structure learned by the lower MCRBM to the upper MCRBM. LSPC facilitates resolving the challenging issue of the orientation ambiguity on the mesh surface in MCDBN. Experiments using the proposed MCRBM and MCDBN were conducted on three common aspects: global shape retrieval, partial shape retrieval, and shape correspondence. Results show that the features learned by the proposed methods outperform the other state-of-the-art 3-D shape features.","keywords_author":["3-D mesh","Laplace-Beltrami operator","mesh convolutional deep belief networks (MCDBNs)","mesh convolutional restricted Boltzmann machines (MCRBMs)","3-D mesh","Laplace\u2013Beltrami operator","mesh convolutional deep belief networks (MCDBNs)","mesh convolutional restricted Boltzmann machines (MCRBMs)"],"keywords_other":["3D meshes","local function energy distribution","local structure information","novel local structure preserving convolution strategy","3D SHAPE RETRIEVAL","mesh convolutional restricted Boltzmann machines","Unsupervised learning","DEEP","Feature extraction","Machine learning","global shape retrieval","Convolution","partial shape retrieval","global structure information","MODEL RETRIEVAL","MCRBM","solid modelling","Shape","novel irregular model structure","RECOGNITION","unsupervised learning","deep learning models","OBJECT RETRIEVAL","NETWORKS","REPRESENTATION","SURFACES","mesh generation","shape correspondence","Boltzmann machines","DESCRIPTORS","Solid modeling"],"max_cite":6.0,"pub_year":2016.0,"sources":"['wos', 'ieee']","rawkeys":["surfaces","local function energy distribution","local structure information","novel local structure preserving convolution strategy","deep","object retrieval","laplace\u2013beltrami operator","mesh convolutional restricted boltzmann machines","convolution","3d shape retrieval","machine learning","solid modeling","global shape retrieval","3d meshes","partial shape retrieval","descriptors","global structure information","laplace-beltrami operator","recognition","model retrieval","boltzmann machines","solid modelling","novel irregular model structure","networks","unsupervised learning","mcrbm","deep learning models","representation","3-d mesh","mesh generation","shape","mesh convolutional deep belief networks (mcdbns)","shape correspondence","feature extraction","mesh convolutional restricted boltzmann machines (mcrbms)"],"tags":["surfaces","local function energy distribution","local structure information","novel local structure preserving convolution strategy","deep","object retrieval","laplace\u2013beltrami operator","convolution","3d shape retrieval","machine learning","solid modeling","global shape retrieval","3d meshes","partial shape retrieval","descriptors","global structure information","laplace-beltrami operator","recognition","deep learning model","model retrieval","boltzmann machines","novel irregular model structure","networks","unsupervised learning","mcrbm","representation","mesh generation","shape","mesh convolutional deep belief networks (mcdbns)","shape correspondence","feature extraction","mesh convolutional restricted boltzmann machines (mcrbms)"]},{"p_id":34330,"title":"Automatic personality assessment: A systematic review","abstract":"\u00a9 2015 IEEE.Personality Assessment is an emerging research area. In recent years, the interest of the scientific community towards personality assessment has grown incredibly. Personality is a psychological model that can be used to explain the wide variety of human behaviors with the help of individual characteristics. The applications of personality assessment range from behavior analysis to disease diagnosis, counseling, employee recruitment, social network analysis, security systems, mood prediction, and many others. Automatic personality assessment consists of the automatic classification of users' personality traits from the data such as video, speech and text. Despite growing number of works in personality assessment, it is still very difficult to say what the current state-of-the-art is. The objective of this survey paper is to discuss various approaches used for personality assessment and to present current state-of-art related to it. It also provides guidelines for further research.","keywords_author":["handwriting analysis","personality","personality assessmen","personality traits","psychology"],"keywords_other":["Personality traits","personality","personality assessmen","psychology","Handwriting analysis"],"max_cite":2.0,"pub_year":2016.0,"sources":"['scp']","rawkeys":["handwriting analysis","personality","personality assessmen","psychology","personality traits"],"tags":["handwriting analysis","recognition","personality assessmen","personalizations","personality traits"]},{"p_id":34332,"title":"A study of the relationship between narcissism, extraversion, drive for entertainment, and narcissistic behavior on social networking sites","abstract":"\u00a9 2016 Elsevier Ltd As social networking sites are gaining popularity, the rise in narcissistic culture on theses sites are also documented. Literature suggests users\u2019 personality traits may be important factors leading people to engage in narcissistic behaviors. Using a national sample of Chinese adults, this study investigated the relationship between narcissism, extraversion, drive for entertainment and narcissistic behaviors on a Chinese social networking site, Wechat Moment. We also examined whether demographic variables and online time played a role in that dynamic. Results revealed that narcissism predicted exhibitionistic behaviors on Wechat Moment, ratings of one's profile picture and using an image of oneself as profile picture over and above extraversion and drive for entertainment controlling for online time and demographic variables. Contrary to our hypothesis, narcissism did not predict higher frequency of Wechat Moment posting over and above drive for entertainment and extraversion. Different sub-factors of narcissism also contributed differently to the variance of narcissistic behavior on SNS.","keywords_author":["Drive for entertainment","Extraversion","Narcissism","Narcissistic behavior","Social networking site","Wechat moment"],"keywords_other":["Extraversion","Higher frequencies","Narcissistic behavior","Social networking sites","Narcissism","Personality traits","Demographic variables","Online time"],"max_cite":2.0,"pub_year":2017.0,"sources":"['scp']","rawkeys":["wechat moment","narcissistic behavior","social networking site","demographic variables","online time","extraversion","higher frequencies","social networking sites","narcissism","personality traits","drive for entertainment"],"tags":["wechat moment","recognition","narcissistic behavior","demographic variables","online time","higher frequencies","social networking sites","narcissism","personality traits","drive for entertainment"]},{"p_id":9764,"title":"Detecting silicone mask-based presentation attack via deep dictionary learning","abstract":"In movies, film stars portray another identity or obfuscate their identity with the help of silicone\/latex masks. Such realistic masks are now easily available and are used for entertainment purposes. However, their usage in criminal activities to deceive law enforcement and automatic face recognition systems is also plausible. Therefore, it is important to guard biometrics systems against such realistic presentation attacks. This paper introduces the first-of-its-kind silicone mask attack database which contains 130 real and attacked videos to facilitate research in developing presentation attack detection algorithms for this challenging scenario. Along with silicone mask, there are several other presentation attack instruments that are explored in literature. The next contribution of this research is a novel multilevel deep dictionary learning-based presentation attack detection algorithm that can discern different kinds of attacks. An efficient greedy layer by layer training approach is formulated to learn the deep dictionaries followed by SVM to classify an input sample as genuine or attacked. Experimental are performed on the proposed SMAD database, some samples with real world silicone mask attacks, and four existing presentation attack databases, namely, replay-attack, CASIA-FASD, 3DMAD, and UVAD. The results show that the proposed algorithm yields better performance compared with state-of-the-art algorithms, in both intra-database and cross-database experiments.","keywords_author":["Deep dictionary","Face recognition","Presentation attack detection","Silicone mask","Face recognition","silicone mask","presentation attack detection","deep dictionary"],"keywords_other":["Dictionary learning","Criminal activities","IMAGE","Input sample","IRIS","Automatic face recognition systems","Layer by layer","State-of-the-art algorithms","Replay attack","RECOGNITION","Attack detection","Attack database","FACE SPOOFING DETECTION"],"max_cite":11.0,"pub_year":2017.0,"sources":"['ieee', 'scp', 'wos']","rawkeys":["deep dictionary","automatic face recognition systems","iris","layer by layer","recognition","replay attack","face spoofing detection","state-of-the-art algorithms","image","attack database","criminal activities","dictionary learning","face recognition","silicone mask","input sample","attack detection","presentation attack detection"],"tags":["face spoofing detections","deep dictionary","automatic face recognition systems","images","iris","layer by layer","recognition","replay attack","state-of-the-art algorithms","attack database","criminal activities","dictionary learning","face recognition","silicone mask","input sample","attack detection","presentation attack detection"]},{"p_id":1578,"title":"Mobile big data analytics using deep learning and apache spark","abstract":"The proliferation of mobile devices, such as smartphones and Internet of Things gadgets, has resulted in the recent mobile big data era. Collecting mobile big data is unprofitable unless suitable analytics and learning methods are utilized to extract meaningful information and hidden patterns from data. This article presents an overview and brief tutorial on deep learning in mobile big data analytics and discusses a scalable learning framework over Apache Spark. Specifically, distributed deep learning is executed as an iterative MapReduce computing on many Spark workers. Each Spark worker learns a partial deep model on a partition of the overall mobile, and a master deep model is then built by averaging the parameters of all partial models. This Spark-based framework speeds up the learning of deep models consisting of many hidden layers and millions of parameters. We use a context-aware activity recognition application with a real-world dataset containing millions of samples to validate our framework and assess its speedup effectiveness.","keywords_author":null,"keywords_other":["BELIEF NETS","Apache Spark","Sensors","Computational modeling","mobile Big Data analytics","mobile computing","iterative methods","Machine learning","iterative MapReduce computing","context-aware activity recognition application","real-world dataset","Mobile communication","learning (artificial intelligence)","deep learning","Big Data","RECOGNITION","Sparks","Big data","Mobile handsets","parallel programming"],"max_cite":32.0,"pub_year":2016.0,"sources":"['ieee', 'scp', 'wos']","rawkeys":["apache spark","big data","sensors","iterative mapreduce computing","belief nets","machine learning","mobile computing","iterative methods","context-aware activity recognition application","real-world dataset","computational modeling","learning (artificial intelligence)","recognition","deep learning","sparks","mobile handsets","mobile big data analytics","mobile communication","parallel programming"],"tags":["computational modeling","apache spark","belief nets","recognition","big data","sensors","mobile communications","machine learning","mobile handsets","mobile big data analytics","iterative methods","mobile computing","real-world datasets","context-aware activity recognition application","spark","iterative mapreduce computing","parallel programming"]},{"p_id":116268,"title":"Applications of Automated Facial Coding in Media Measurement","abstract":"Facial coding has become a common tool in media measurement, with large companies (e.g., Unilever) using it to test all of their new video ad content. Facial reactions capture the in-the-moment response of an individual and these data complement self-report measures. Two advancements in affective computing have made measurement possible at scale: 1) computer vision algorithms are used to automatically code sign and message judgments based on facial muscle movements, 2) video data are collected by recording responses in everyday environments via the viewer's own webcam over the Internet. We present results of online facial coding studies of video ads, movie trailers, political content, and long-form TV shows. We explain how these data can be used in market research. Despite the ability to measure facial behavior in a scalable and quantifiable way, the interpretation of these data is still challenging without baselines and comparative measures. Over the past four years we have collected and coded over two million responses to everyday media content. Our huge dataset allows us to calculate reliable normative distributions of responses across different media types. We present these data and argue that this provides a context within which to interpret facial responses more accurately.","keywords_author":["Facial expressions","facial coding","emotion","media measurement","advertising","marketing","crowdsourcing"],"keywords_other":["ADS","ADVERTISEMENT","RESPONSES","RECOGNITION","SMILE","SCALE","COMMERCIALS","PATTERNS","SEX-DIFFERENCES"],"max_cite":0.0,"pub_year":2017.0,"sources":"['wos']","rawkeys":["smile","recognition","emotion","sex-differences","media measurement","facial expressions","crowdsourcing","patterns","marketing","responses","commercials","advertisement","ads","scale","facial coding","advertising"],"tags":["commercial","recognition","emotion","anomaly detection","media measurement","sex-differences","facial expressions","smiles","crowdsourcing","patterns","marketing","responses","scale","facial coding","advertising"]},{"p_id":83502,"title":"Dimensionality Reduction for Hyperspectral Data Based on Pairwise Constraint Discriminative Analysis and Nonnegative Sparse Divergence","abstract":"To improve the classification accuracy of unlabeled large-scale hyperspectral data, a dimensionality reduction algorithm based on pairwise constraint discriminant analysis and nonnegative sparse divergence (PCDA-NSD) is proposed by using the feature transfer learning technology. Different from labeled sample information that is relatively difficult to acquire, pairwise constraints are a kind of useful supervision information, which can be automatically acquired without artificial interference and thus can better avoid the selection of redundant and noisy samples. Therefore, the pairwise constraint discriminant analysis method is used to learn potential discriminant information of sample sets in the source and target domains. Consequently, positively correlated constraint samples in the source and target domains share one sub-space whereas positively and negatively correlated constraint samples are highly separated. Because hyperspectral data in the source and target domains often follow different distributions, a nonnegative sparse divergence is established to measure the divergence between different distributions, based on the nonnegative sparse representation method. Therefore, not only the computation load of the kernel matrix is reduced, but also the natural discriminant capacity is obtained. Experiments of a four-group hyperspectral data show that PCDA-NSD can reduce dimensionality of target data and improve classification accuracy and efficiency by adequate use of the information available in similar hyperspectral data.","keywords_author":["Dimensionality reduction","hyperspectral data","nonnegative sparse divergence","pairwise constraint","transfer learning"],"keywords_other":["NMF","SELECTION","MANIFOLD","LAND-COVER MAPS","SVM","SENSING IMAGE CLASSIFICATION","FRAMEWORK","PATCH ALIGNMENT","RECOGNITION","DOMAIN ADAPTATION"],"max_cite":1.0,"pub_year":2017.0,"sources":"['ieee', 'wos']","rawkeys":["land-cover maps","recognition","hyperspectral data","transfer learning","framework","nonnegative sparse divergence","selection","sensing image classification","svm","patch alignment","manifold","domain adaptation","pairwise constraint","nmf","dimensionality reduction"],"tags":["recognition","hyperspectral data","pairwise constraints","denoising autoencoder","framework","nonnegative sparse divergence","nonnegative matrix factorization","sensing image classification","machine learning","transfer learning","manifolds","patch alignment","selection","land cover mapping","dimensionality reduction"]},{"p_id":67119,"title":"Exploiting semantic similarity for named entity disambiguation in knowledge graphs","abstract":"With the increasing popularity of large scale Knowledge Graph (KG)s, many applications such as semantic analysis, search and question answering need to link entity mentions in texts to entities in KGs. Because of the polysemy problem in natural language, entity disambiguation is thus a key problem in current research. Existing disambiguation methods have considered entity prominence, context similarity and entity-entity relatedness to discriminate ambiguous entities, which are mainly working on document or paragraph level texts containing rich contextual information, and based on lexical matching for computing context similarity. When meeting short texts containing limited contextual information, such as web queries, questions and tweets, those conventional disambiguation methods are not good at handling single entity mention and measuring context similarity. In order to enhance the performance of disambiguation methods based on context similarity with such short texts, we propose SCSNED method for disambiguation based on semantic similarity between contextual words and informative words of entities in KGs. Specially, we exploit the effectiveness of both knowledge-based and corpus-based semantic similarity methods for entity disambiguation with SCSNED. Moreover, we propose a Category2Vec embedding model based on joint learning of word and category embedding, in order to compute word-category similarity for entity disambiguation. We show the effectiveness of these proposed methods with illustrative examples, and evaluate their effectiveness in a comparative experiment for entity disambiguation in real world web queries, questions and tweets. The experimental results have identified the effectiveness of different semantic similarity methods, and demonstrated the improvement of semantic similarity methods in SCSNED and Category2Vec over the conventional context similarity baseline. We further compare the proposed approaches with the state of the art entity disambiguation systems and show the performances of the proposed approaches are among the best performing systems. In addition, one important feature of the proposed approaches using semantic similarity, is the potential application on any existing KGs since they mainly use common features of entity descriptions and categories. Another contribution of the paper is an updated survey on background of entity disambiguation in KGs and semantic similarity methods. (C) 2018 The Authors. Published by Elsevier Ltd. This is an open access article under the CC BY-NC-ND license.","keywords_author":["Entity linking","Named entity disambiguation","Context similarity","Semantic similarity","Word embedding","Knowledge graph"],"keywords_other":["DBPEDIA","INFORMATION-CONTENT","NETS","BASE","CLASSIFICATION","LINKING","RECOGNITION","QUERIES","WORDNET","WIKIPEDIA"],"max_cite":0.0,"pub_year":2018.0,"sources":"['ieee', 'wos']","rawkeys":["queries","wikipedia","wordnet","recognition","knowledge graph","context similarity","dbpedia","named entity disambiguation","semantic similarity","linking","word embedding","classification","nets","entity linking","information-content","base"],"tags":["knowledge graphs","wordnet","recognition","information contents","context similarity","dbpedia","named entity disambiguation","semantic similarity","linking","word embedding","classification","nets","symptoms","entity linking","wikipedia","base"]},{"p_id":67118,"title":"GAR: An efficient and scalable graph-based activity regularization for semi-supervised learning","abstract":"In this paper, we propose a novel graph-based approach for semi-supervised learning problems, which considers an adaptive adjacency of the examples throughout the unsupervised portion of the training. Adjacency of the examples is inferred using the predictions of a neural network model which is first initialized by a supervised pretraining. These predictions are then updated according to a novel unsupervised objective which regularizes another adjacency, now linking the output nodes. Regularizing the adjacency of the output nodes, inferred from the predictions of the network, creates an easier optimization problem and ultimately provides that the predictions of the network turn into the optimal embedding. Ultimately, the proposed framework provides an effective and scalable graph-based solution which is natural to the operational mechanism of deep neural networks. Our results show comparable performance with state-of-the-art generative approaches for semi-supervised learning on an easier-to-train, low-cost framework. (C) 2018 Elsevier B.V. All rights reserved.","keywords_author":["Semi-supervised learning","Neural networks","Graph-based"],"keywords_other":["RECOGNITION","DIMENSIONALITY REDUCTION"],"max_cite":0.0,"pub_year":2018.0,"sources":"['wos']","rawkeys":["recognition","neural networks","graph-based","semi-supervised learning","dimensionality reduction"],"tags":["recognition","neural networks","graph-based","semi-supervised learning","dimensionality reduction"]},{"p_id":42547,"title":"PsyPhy: A Psychophysics Driven Evaluation Framework for Visual Recognition","abstract":"IEEE By providing substantial amounts of data and standardized evaluation protocols, datasets in computer vision have helped fuel advances across all areas of visual recognition. But even in light of breakthrough results on recent benchmarks, it is still fair to ask if our recognition algorithms are doing as well as we think they are. The vision sciences at large make use of a very different evaluation regime known as Visual Psychophysics to study visual perception. Psychophysics is the quantitative examination of the relationships between controlled stimuli and the behavioral responses they elicit in experimental test subjects. Instead of using summary statistics to gauge performance, psychophysics directs us to construct item-response curves made up of individual stimulus responses to find perceptual thresholds, thus allowing one to identify the exact point at which a subject can no longer reliably recognize the stimulus class. In this article, we introduce a comprehensive evaluation framework for visual recognition models that is underpinned by this methodology. Over millions of procedurally rendered 3D scenes and 2D images, we compare the performance of well-known convolutional neural networks. Our results bring into question recent claims of human-like performance, and provide a path forward for correcting newly surfaced algorithmic deficiencies.","keywords_author":["Computational modeling","Computer vision","Deep Learning","Evaluation","Machine learning","Neuroscience","Object Recognition","Observers","Psychology","Psychology","Task analysis","Visual Psychophysics","Visualization"],"keywords_other":["Neuroscience","Computational model","Psychology","Task analysis","Visual psychophysics","Observers","Evaluation"],"max_cite":0.0,"pub_year":2018.0,"sources":"['scp']","rawkeys":["computational modeling","task analysis","neuroscience","deep learning","machine learning","evaluation","object recognition","visual psychophysics","observers","computer vision","psychology","computational model","visualization"],"tags":["computational modeling","task analysis","neuroscience","recognition","machine learning","evaluation","object recognition","visual psychophysics","observers","computer vision","visualization"]},{"p_id":9784,"title":"Efficient and robust deep learning with Correntropy-induced loss function","abstract":"Deep learning systems aim at using hierarchical models to learning high-level features from low-level features. The progress in deep learning is great in recent years. The robustness of the learning systems with deep architectures is however rarely studied and needs further investigation. In particular, the mean square error (MSE), a commonly used optimization cost function in deep learning, is rather sensitive to outliers (or impulsive noises). Robust methods are needed to improve the learning performance and immunize the harmful influences caused by outliers which are pervasive in real-world data. In this paper, we propose an efficient and robust deep learning model based on stacked auto-encoders and Correntropy-induced loss function (CLF), called CLF-based stacked auto-encoders (CSAE). CLF as a nonlinear measure of similarity is robust to outliers and can approximate different norms (from to ) of data. Essentially, CLF is an MSE in reproducing kernel Hilbert space. Different from conventional stacked auto-encoders, which use, in general, the MSE as the reconstruction loss and KL divergence as the sparsity penalty term, the reconstruction loss and sparsity penalty term in CSAE are both built with CLF. The fine-tuning procedure in CSAE is also based on CLF, which can further enhance the learning performance. The excellent and robust performance of the proposed model is confirmed by simulation experiments on MNIST benchmark dataset.","keywords_author":["Correntropy","Deep learning","Stacked auto-encoders","Unsupervised feature learning","Deep learning","Correntropy","Stacked auto-encoders","Unsupervised feature learning"],"keywords_other":["Deep learning","High-level features","Correntropy","Benchmark datasets","Learning performance","CLASSIFICATION","Unsupervised feature learning","RECOGNITION","Reproducing Kernel Hilbert spaces","Auto encoders"],"max_cite":10.0,"pub_year":2016.0,"sources":"['ieee', 'scp', 'wos']","rawkeys":["benchmark datasets","high-level features","recognition","reproducing kernel hilbert spaces","unsupervised feature learning","deep learning","auto encoders","correntropy","classification","stacked auto-encoders","learning performance"],"tags":["benchmark datasets","high-level features","recognition","reproducing kernel hilbert spaces","unsupervised feature learning","auto encoders","machine learning","stacked autoencoders","correntropy","classification","learning performance"]},{"p_id":9785,"title":"AU-inspired Deep Networks for Facial Expression Feature Learning","abstract":"Most existing technologies for facial expression recognition utilize off-the-shelf feature extraction methods for classification. In this paper, aiming at learning better features specific for expression representation, we propose to construct a deep architecture, AU-inspired Deep Networks (AUDN), inspired by the psychological theory that expressions can be decomposed into multiple facial Action Units (AUs). To fully exploit this inspiration but avoid detecting AUs, we propose to automatically learn: (1) informative local appearance variation; (2) optimal way to combining local variation and (3) high level representation for final expression recognition. Accordingly, the proposed AUDN is composed of three sequential modules. Firstly, we build a convolutional layer and a max-pooling layer to learn the Micro-Action-Pattern (MAP) representation, which can explicitly depict local appearance variations caused by facial expressions. Secondly, feature grouping is applied to simulate larger receptive fields by combining correlated MAPs adaptively, aiming to generate more abstract mid-level semantics. Finally, a multi-layer learning process is employed in each receptive field respectively to construct group-wise sub-networks for higher-level representations. Experiments on three expression databases CK+, MMI and SFEW demonstrate that, by simply applying linear classifiers on the learned features, our method can achieve state-of-the-art results on all the databases, which validates the effectiveness of AUDN in both lab-controlled and wild environments. (C) 2015 Elsevier B.V. All rights reserved.","keywords_author":["Facial expression recognition","AU-inspired Deep Networks (AUDN)","Micro-Action-Pattern","Receptive field","Group-wise sub-network learning"],"keywords_other":["RECOGNITION","CLASSIFICATION"],"max_cite":43.0,"pub_year":2015.0,"sources":"['ieee', 'wos']","rawkeys":["facial expression recognition","micro-action-pattern","recognition","au-inspired deep networks (audn)","classification","receptive field","group-wise sub-network learning"],"tags":["facial expression recognition","micro-action-pattern","recognition","random forests","au-inspired deep networks (audn)","classification","group-wise sub-network learning"]},{"p_id":9787,"title":"Deep learning models to remix music for cochlear implant users","abstract":"The severe hearing loss problems that some people suffer can be treated by providing them with a surgically implanted electrical device called cochlear implant (CI). CI users struggle to perceive complex audio signals such as music; however, previous studies show that CI recipients find music more enjoyable when the vocals are enhanced with respect to the background music. In this manuscript source separation (SS) algorithms are used to remix pop songs by applying gain to the lead singing voice. This work uses deep convolutional auto-encoders, a deep recurrent neural network, a multilayer perceptron (MLP), and non-negative matrix factorization to be evaluated objectively and subjectively through two different perceptual experiments which involve normal hearing subjects and CI recipients. The evaluation assesses the relevance of the artifacts introduced by the SS algorithms considering their computation time, as this study aims at proposing one of the algorithms for real-time implementation. Results show that the MLP performs in a robust way throughout the tested data while providing levels of distortions and artifacts which are not perceived by CI users. Thus, an MLP is proposed to be implemented for real-time monaural audio SS to remix music for CI users. (C) 2018 Acoustical Society of America.","keywords_author":null,"keywords_other":["SPEECH ENHANCEMENT","PITCH","IMPROVE","NOISE","PERCEPTION","NEURAL-NETWORKS","RECOGNITION","AUDIO SOURCE SEPARATION"],"max_cite":0.0,"pub_year":2018.0,"sources":"['ieee', 'wos']","rawkeys":["neural-networks","audio source separation","recognition","noise","improve","perception","speech enhancement","pitch"],"tags":["audio source separation","recognition","noise","neural networks","improvement","speech enhancement","perceptions","pitch"]},{"p_id":17981,"title":"Predicting active users' personality based on micro-blogging behaviors","abstract":"Because of its richness and availability, micro-blogging has become an ideal platform for conducting psychological research. In this paper, we proposed to predict active users' personality traits through micro-blogging behaviors. 547 Chinese active users of micro-blogging participated in this study. Their personality traits were measured by the Big Five Inventory, and digital records of micro-blogging behaviors were collected via web crawlers. After extracting 845 micro-blogging behavioral features, we first trained classification models utilizing Support Vector Machine (SVM), differentiating participants with high and low scores on each dimension of the Big Five Inventory. The classification accuracy ranged from 84% to 92%. We also built regression models utilizing PaceRegression methods, predicting participants' scores on each dimension of the Big Five Inventory. The Pearson correlation coefficients between predicted scores and actual scores ranged from 0.48 to 0.54. Results indicated that active users' personality traits could be predicted by micro-blogging behaviors. \u00a9 2014 Li et al.","keywords_author":null,"keywords_other":["Male","Support Vector Machines","Young Adult","Humans","Reproducibility of Results","Personality Assessment","Questionnaires","Adult","Personality Inventory","Models, Psychological","Blogging","Female"],"max_cite":26.0,"pub_year":2014.0,"sources":"['scp']","rawkeys":["male","questionnaires","adult","reproducibility of results","humans","personality inventory","psychological","models","blogging","support vector machines","personality assessment","young adult","female"],"tags":["recognition","model","male","adult","reproducibility of results","humans","blogs","machine learning","questionnaire","personal inventories","young adult","personality assessment","female"]},{"p_id":83518,"title":"Unsupervised Data Driven Feature Extraction by Means of Mutual Information Maximization","abstract":"In Earth observations technical literature, several methods have been proposed and implemented to efficiently extract a proper set of features for classification and segmentation purposes. However, these architectures show drawbacks when the considered datasets are characterized by complex interactions among the samples, especially when they rely on strong assumptions on noise and label domains. In this paper, a new unsupervised approach for feature extraction, based on data driven discovery, is introduced for accurate classification of remotely sensed data. Specifically, the proposed architecture exploits mutual information maximization in order to retrieve the most relevant features with respect to information measures. Experimental results on real datasets showthat the proposed approach represents a valid framework for feature extraction from remote sensing images.","keywords_author":["Data driven discovery","feature extraction","information theory","mutual information maximization","remote sensing"],"keywords_other":["REMOTE-SENSING IMAGES","DATA CLASSIFICATION","SUN-INDUCED FLUORESCENCE","PHOTOSYNTHESIS","SUPPORT VECTOR MACHINES","BIG DATA","RETRIEVAL","RECOGNITION","AFRICA","SPARSE REPRESENTATION"],"max_cite":3.0,"pub_year":2017.0,"sources":"['ieee', 'wos']","rawkeys":["photosynthesis","africa","information theory","recognition","remote sensing","big data","data classification","sparse representation","remote-sensing images","sun-induced fluorescence","feature extraction","retrieval","support vector machines","mutual information maximization","data driven discovery"],"tags":["photosynthesis","africa","information theory","recognition","remote sensing","big data","data classification","sparse representation","machine learning","sun-induced fluorescence","feature extraction","retrieval","remote sensing images","mutual information maximization","data driven discovery"]},{"p_id":91710,"title":"Emotional textile image classification based on cross-domain convolutional sparse autoencoders with feature selection","abstract":"We aim to apply sparse autoencoder-based unsupervised feature learning to emotional semantic analysis for textile images. To tackle the problem of limited training data, we present a cross-domain feature learning scheme for emotional textile image classification using convolutional autoencoders. We further propose a correlation-analysis-based feature selection method for the weights learned by sparse autoencoders to reduce the number of features extracted from large size images. First, we randomly collect image patches on an unlabeled image dataset in the source domain and learn local features with a sparse autoencoder. We then conduct feature selection according to the correlation between different weight vectors corresponding to the autoencoder's hidden units. We finally adopt a convolutional neural network including a pooling layer to obtain global feature activations of textile images in the target domain and send these global feature vectors into logistic regression models for emotional image classification. The cross-domain unsupervised feature learning method achieves 65% to 78% average accuracy in the cross-validation experiments corresponding to eight emotional categories and performs better than conventional methods. Feature selection can reduce the computational cost of global feature extraction by about 50% while improving classification performance. (C) 2017 SPIE and IS&T","keywords_author":["textile image","emotional image classification","convolutional autoencoder","domain adaptation","feature selection"],"keywords_other":["TEXTURE CLASSIFICATION","PATTERN","EMPIRICAL MODE DECOMPOSITION","RECOGNITION"],"max_cite":1.0,"pub_year":2017.0,"sources":"['wos']","rawkeys":["pattern","recognition","domain adaptation","feature selection","textile image","convolutional autoencoder","emotional image classification","texture classification","empirical mode decomposition"],"tags":["recognition","denoising autoencoder","patterns","feature selection","textile image","convolutional autoencoder","emotional image classification","texture classification","empirical mode decomposition"]},{"p_id":91712,"title":"LMAE: A large margin Auto-Encoders for classification","abstract":"Auto-Encoders, as one representative deep learning method, has demonstrated to achieve superior performance in many applications. Hence, it is drawing more and more attentions and variants of Auto Encoders have been reported including Contractive Auto-Encoders, Denoising Auto-Encoders, Sparse Auto Encoders and Nonnegativity Constraints Auto-Encoders. Recently, a Discriminative Auto-Encoders is reported to improve the performance by considering the within class and between class information. In this paper, we propose the Large Margin Auto-Encoders (LMAE) to further boost the discriminability by enforcing different class samples to be large marginally distributed in hidden feature space. Particularly, we stack the single-layer LMAE to construct a deep neural network to learn proper features. And finally we put these features into a softmax classifier for classification. Extensive experiments are conducted on the MNIST dataset and the CIFAR-10 dataset for classification respectively. The experimental results demonstrate that the proposed LMAE outperforms the traditional Auto-Encoders algorithm. (C) 2017 Elsevier B.V. All rights reserved.","keywords_author":["Auto-Encoder","Large Margin","kNN","Classification"],"keywords_other":["IMAGE","REPRESENTATION","CONSTRAINTS","NEURAL-NETWORKS","RECOGNITION","HYPERGRAPH","RETRIEVAL","DENOISING AUTOENCODERS","HUMAN POSE RECOVERY"],"max_cite":1.0,"pub_year":2017.0,"sources":"['ieee', 'wos']","rawkeys":["neural-networks","auto-encoder","constraints","recognition","knn","representation","denoising autoencoders","image","classification","retrieval","human pose recovery","large margin","hypergraph"],"tags":["constraints","images","recognition","large margins","denoising autoencoder","neural networks","auto encoders","representation","classification","retrieval","human pose recovery","k-nearest neighbors","hypergraph"]},{"p_id":67133,"title":"Discriminative Deep Belief Network for Indoor Environment Classification Using Global Visual Features","abstract":"Indoor environment classification, also known as indoor environment recognition, is a highly appreciated perceptual ability in mobile robots. In this paper, we present a novel approach which is centered on biologically inspired methods for recognition and representation of indoor environments. First, global visual features are extracted by using the GIST descriptor, and then we use the subsequent features for training the discriminative deep belief network (DDBN) classifier. DDBN employs a new deep architecture which is based on restricted Boltzmann machines (RBMs) and the joint density model. The back-propagation technique is used over the entire classifier to fine-tune the weights for an optimum classification. The acquired experimental results validate our approach as it performs well both in the real-world and in synthetic datasets and outperforms the Convolution Neural Networks (ConvNets) in terms of computational efficiency.","keywords_author":["Indoor environment","GIST descriptor","Discriminative Deep Belief Network (DDBN)","Back-propagation technique"],"keywords_other":["SALIENCY","ATTENTION","MODEL","RECOGNITION","LEARNING ALGORITHM","SEGMENTATION","SCENE CLASSIFICATION","IMAGE FEATURES"],"max_cite":0.0,"pub_year":2018.0,"sources":"['ieee', 'wos']","rawkeys":["back-propagation technique","image features","recognition","model","segmentation","scene classification","attention","gist descriptor","discriminative deep belief network (ddbn)","saliency","indoor environment","learning algorithm"],"tags":["image features","recognition","model","segmentation","scene classification","attention","gist descriptor","discriminative deep belief network (ddbn)","backpropagation techniques","saliency","indoor environment","learning algorithm"]},{"p_id":67137,"title":"A new oscillating-error technique for classifiers","abstract":"This paper describes a new method for reducing the error in a classifier. It uses an error correction update that includes the very simple rule of either adding or subtracting the error adjustment, based on whether the variable value is currently larger or smaller than the desired value. While a traditional neuron would sum the inputs together and then apply a function to the total, this new method can change the function decision for each input value. This gives added flexibility to the convergence procedure, where through a series of transpositions, variables that are far away can continue towards the desired value, whereas variables that are originally much closer can oscillate from one side to the other. Tests show that the method can successfully classify some benchmark datasets. It can also work in a batch mode, with reduced training times and can be used as part of a neural network architecture. Some comparisons with an earlier wave shape paper are also made.","keywords_author":["classifier","oscillating error","transposition","matrix","neural network","cellular automata"],"keywords_other":["NETWORKS","RECOGNITION","ALGORITHM"],"max_cite":0.0,"pub_year":2017.0,"sources":"['wos']","rawkeys":["algorithm","neural network","recognition","matrix","transposition","cellular automata","networks","classifier","oscillating error"],"tags":["recognition","matrix","neural networks","transposition","cellular automata","networks","classifier","oscillating error","algorithms"]},{"p_id":67144,"title":"Multi-step-ahead host load prediction using autoencoder and echo state networks in cloud computing","abstract":"Cloud computing is a model for enabling ubiquitous, convenient, on-demand network access to a shared pool of configurable computing resources that can be rapidly provisioned and released with minimal management effort or service provider interaction. There are many proposals for resource management approaches for cloud infrastructures, but effective resource management is still a major challenge for the leading cloud infrastructure operators (e.g., Amazon, Microsoft, Google), because the details of the underlying workloads and the real-world operational demands are too complex. Among those proposals, accurate host load prediction is one of the most effective measures to address this challenge. In this paper, we proposed a new method for host load prediction, which uses an autoencoder as the pre-recurrent feature layer of the echo state networks. The aim of our proposed method is to predict the host load in the future interval based on Google cluster usage dataset. Experiments performed on Google load traces show that our proposed method achieves higher accuracy than the state-of-the-art methods.","keywords_author":["Host load prediction","Autoencoder","Echo state networks"],"keywords_other":["RECOGNITION","TIME","ENERGY"],"max_cite":0.0,"pub_year":2015.0,"sources":"['ieee', 'wos']","rawkeys":["recognition","autoencoder","time","echo state networks","energy","host load prediction"],"tags":["recognition","auto encoders","time","echo state networks","energy","host load prediction"]},{"p_id":108107,"title":"Efficient deep network for vision-based object detection in robotic applications","abstract":"Vision-based object detection is essential for a multitude of robotic applications. However, it is also a challenging job due to the diversity of the environments in which such applications are required to operate, and the strict constraints that apply to many robot systems in terms of run-time, power and space. To meet these special requirements of robotic applications, we propose an efficient deep network for vision-based object detection. More specifically, for a given image captured by a robot mount camera, we first introduce a novel proposal layer to efficiently generate potential object bounding-boxes. The proposal layer consists of efficient on-line convolutions and effective off-line optimization. Afterwards, we construct a robust detection layer which contains a multiple population genetic algorithm-based convolutional neural network (MPGA-based CNN) module and a TLD-based multi-frame fusion procedure. Unlike most deep learning based approaches, which rely on GPU, all of the on-line processes in our system are able to run efficiently without GPU support. We perform several experiments to validate each component of our proposed object detection approach and compare the approach with some recently published state-of-the-art object detection algorithms on widely used datasets. The experimental results demonstrate that the proposed network exhibits high efficiency and robustness in object detection tasks. (C) 2017 Elsevier B.V. All rights reserved.","keywords_author":["Deep network","Object detection","Computer vision","Robotic application","MPGA"],"keywords_other":["NEURAL-NETWORK","RECOGNITION","FEATURES"],"max_cite":3.0,"pub_year":2017.0,"sources":"['wos']","rawkeys":["recognition","features","mpga","deep network","neural-network","object detection","computer vision","robotic application"],"tags":["recognition","features","neural networks","mpga","object detection","robotic applications","computer vision","deep networks"]},{"p_id":9805,"title":"Deep learning with geodesic moments for 3D shape classification","abstract":"In this paper, we present a deep learning framework for efficient 3D shape classification using geodesic moments. Our approach inherits many useful properties from the geodesic distance, most notably the capture of the intrinsic geometric structure of 3D shapes and the invariance to isometric deformations. Moreover, we show the similarity between the convergent series of the geodesic moments and the inverse-square eigenvalues of the Laplace-Beltrami operator in the continuous setting. The proposed algorithm uses a two-layer stacked sparse autoencoder to learn deep features from geodesic moments by training the hidden layers individually in an unsupervised fashion, followed by a softmax classifier. Experimental results on three standard 3D shape benchmarks demonstrate superior performance of the proposed approach compared to existing methods. (c) 2017Elsevier B.V. All rights reserved.","keywords_author":["Deep learning","Geodesic moments","Laplace\u2013Beltrami","Shape classification","Stacked autoencoders","Geodesic moments","Deep learning","Laplace-Beltrami","Stacked autoencoders","Shape classification"],"keywords_other":["65D17","Laplace-Beltrami","NETWORKS","41A05","65D05","RETRIEVAL","RECOGNITION","SIGNATURE","Geodesic moments","41A10","Shape classification","Autoencoders"],"max_cite":1.0,"pub_year":2018.0,"sources":"['ieee', 'scp', 'wos']","rawkeys":["laplace\u2013beltrami","41a10","recognition","shape classification","deep learning","geodesic moments","laplace-beltrami","stacked autoencoders","41a05","signature","autoencoders","65d05","networks","65d17","retrieval"],"tags":["laplace\u2013beltrami","41a10","recognition","shape classification","geodesic moments","auto encoders","machine learning","laplace-beltrami","41a05","stacked autoencoders","signature","networks","65d05","65d17","retrieval"]},{"p_id":9809,"title":"AENet: Learning Deep Audio Features for Video Analysis","abstract":"We propose a new deep network for audio event recognition, called AENet. In contrast to speech, sounds coming from audio events may be produced by a wide variety of sources. Furthermore, distinguishing them often requires analyzing an extended time period due to the lack of clear subword units that are present in speech. In order to incorporate this long-time frequency structure of audio events, we introduce a convolutional neural network (CNN) operating on a large temporal input. In contrast to previous works, this allows us to train an audio event detection system end to end. The combination of our network architecture and a novel data augmentation outperforms previous methods for audio event detection by 16%. Furthermore, we perform transfer learning and show that our model learned generic audio features, similar to the way CNNs learn generic features on vision tasks. In video analysis, combining visual features and traditional audio features, such as mel frequency cepstral coefficients, typically only leads to marginal improvements. Instead, combining visual features with our AENet features, which can be computed efficiently on a GPU, leads to significant performance improvements on action recognition and video highlight detection. In video highlight detection, our audio features improve the performance by more than 8% over visual features alone.","keywords_author":["Convolutional neural network","audio feature","large audio event dataset","large input field","highlight detection"],"keywords_other":["RECOGNITION","CLASSIFICATION"],"max_cite":0.0,"pub_year":2018.0,"sources":"['ieee', 'wos']","rawkeys":["large input field","large audio event dataset","highlight detection","recognition","audio feature","classification","convolutional neural network"],"tags":["large input field","large audio event dataset","highlight detection","recognition","classification","audio features","convolutional neural network"]},{"p_id":58961,"title":"A Hierarchical Extractor-Based Visual Rail Surface Inspection System","abstract":"Rail inspection based on visual inspection system (VIS) has drawn much attention recently, since VIS is automatic, fast, nondestructive, and objective. However, visual rail inspection is still a challenge to accurately identify possible defects involving a large variety of visual appearances. This paper presents an automatic visual inspection system to address these difficulties. Rail surface images are first obtained by the system, and then fed into the proposed hierarchical inspection framework containing coarse extractor and fine extractor. Specifically, the coarse extractor handles this defect inspection problem by exploring characteristics of rail background instead of defect itself, and focuses on finding the background modes. The fine extractor, which integrates the longitudinal context information and transversal prior information, is presented to largely reduce the impacts of other irregulars. The proposed method is evaluated on labeled Type-I and Type-II rail surface discrete defects data sets and a practical rail line. The experimental results demonstrate that our method outperforms the state-of-the-art works.","keywords_author":["Rail inspection","machine vision","discrete surface defects","mean shift","coarse-to-fine"],"keywords_other":["DEFECT DETECTION","BOLTS","TRACK INSPECTION","CURRENT PULSED THERMOGRAPHY","MODEL","RECOGNITION","IMAGES","MAINTENANCE"],"max_cite":0.0,"pub_year":2017.0,"sources":"['wos']","rawkeys":["defect detection","coarse-to-fine","recognition","images","maintenance","model","track inspection","current pulsed thermography","machine vision","bolts","discrete surface defects","mean shift","rail inspection"],"tags":["defect detection","recognition","images","maintenance","model","track inspection","current pulsed thermography","multiple sclerosis","machine vision","bolts","discrete surface defects","coarse to fine","rail inspections"]},{"p_id":1619,"title":"Sparseness Analysis in the Pretraining of Deep Neural Networks","abstract":"A major progress in deep multilayer neural networks (DNNs) is the invention of various unsupervised pretraining methods to initialize network parameters which lead to good prediction accuracy. This paper presents the sparseness analysis on the hidden unit in the pretraining process. In particular, we use the L1-norm to measure sparseness and provide some sufficient conditions for that pretraining leads to sparseness with respect to the popular pretraining models- such as denoising autoencoders (DAEs) and restricted Boltzmann machines (RBMs). Our experimental results demonstrate that when the sufficient conditions are satisfied, the pretraining models lead to sparseness. Our experiments also reveal that when using the sigmoid activation functions, pretraining plays an important sparseness role in DNNs with sigmoid (Dsigm), and when using the rectifier linear unit (ReLU) activation functions, pretraining becomes less effective for DNNs with ReLU (Drelu). Luckily, Drelu can reach a higher recognition accuracy than DNNs with pretraining (DAEs and RBMs), as it can capture the main benefit (such as sparseness-encouraging) of pretraining in Dsigm. However, ReLU is not adapted to the different firing rates in biological neurons, because the firing rate actually changes along with the varying membrane resistances. To address this problem, we further propose a family of rectifier piecewise linear units (RePLUs) to fit the different firing rates. The experimental results show that the performance of RePLU is better than ReLU, and is comparable with those with some pretraining techniques, such as RBMs and DAEs.","keywords_author":["Activation function","deep neural networks","infomax principle","sparseness","unsupervised pretraining","Activation function","deep neural networks","infomax principle","sparseness","unsupervised pretraining"],"keywords_other":["biological neurons","firing rates","Dsigm","CONTRASTIVE DIVERGENCE","denoising autoencoders","RBM","Biological neural networks","DNN","sparseness analysis","rectifier piecewise linear units","unsupervised pretraining methods","rectifier linear unit","DAE","Biomembranes","ReLU activation functions","pretraining models","learning (artificial intelligence)","membrane resistances","Training","Learning systems","sparseness role","deep multilayer neural networks","RECOGNITION","Neurons","restricted Boltzmann machines","sigmoid activation functions","RePLU","Signal processing algorithms","Boltzmann machines"],"max_cite":9.0,"pub_year":2016.0,"sources":"['wos', 'ieee']","rawkeys":["biological neurons","firing rates","replu","dae","sparseness","biomembranes","denoising autoencoders","learning systems","signal processing algorithms","dsigm","sparseness analysis","rectifier piecewise linear units","contrastive divergence","rbm","relu activation functions","rectifier linear unit","unsupervised pretraining","dnn","unsupervised pretraining methods","activation function","biological neural networks","neurons","pretraining models","learning (artificial intelligence)","membrane resistances","recognition","boltzmann machines","training","sparseness role","deep multilayer neural networks","infomax principle","restricted boltzmann machines","sigmoid activation functions","deep neural networks"],"tags":["firing rates","replu","biomembranes","convolutional neural network","learning systems","signal processing algorithms","dsigm","pre-trained models","sparseness analysis","biological neuron","rectifier piecewise linear units","contrastive divergence","machine learning","unsupervised pretraining methods","unsupervised pretraining","biological neural networks","neurons","recognition","membrane resistances","relu activation function","denoising autoencoder","boltzmann machines","training","sparseness role","deep multilayer neural networks","infomax principle","restricted boltzmann machine","rectified linear units","sigmoid activation function","activation functions","sparse"]},{"p_id":58965,"title":"Automatic Visual Detection System of Railway Surface Defects With Curvature Filter and Improved Gaussian Mixture Model","abstract":"Rails are among the most important components of railway transportation, and real-time defects detection of the railway is an important and challenging task because of intensity inhomogeneity, low contrast, and noise. This paper presents an automatic railway visual detection system (RVDS) for surface defects and focuses on several key issues of RVDS. First, in view of challenges such as complex condition and orbital reflectance inequality, we put forward a region-of-interest detection region extraction algorithm by vertical projection and gray contrast algorithm. In addition, a curvature filter equipped with implicit computing and surface preserving power is studied to eliminate noise and keep only the details. Then, an improved fast and robust Gaussian mixture model based on Markov random field is established for accurate and rapid surface defect segmentation. Additionally, an expectation-maximization algorithm is applied to optimize the parameters. The experimental results demonstrate that the proposed method performs well with both noisy and railway images, which enables identification and segmentation of the defects from rail surface, achieving detection performance with 92% precision and 88.8% recall rate on average, and is robust compared with the related well-established approaches.","keywords_author":["Curvature filter","improved Gaussian mixture model (GMM)","Markov random field (MRF)","railway surface defect","visual detection"],"keywords_other":["SQUATS","INSPECTION SYSTEM","BOLTS","TRACK INSPECTION","CLASSIFICATION","RECOGNITION","IMAGE SEGMENTATION","FUSION","INSTRUMENT","MAINTENANCE"],"max_cite":0.0,"pub_year":2018.0,"sources":"['wos']","rawkeys":["instrument","recognition","maintenance","visual detection","markov random field (mrf)","track inspection","improved gaussian mixture model (gmm)","curvature filter","bolts","classification","railway surface defect","fusion","squats","inspection system","image segmentation"],"tags":["markov random fields","recognition","maintenance","visual detection","track inspection","improved gaussian mixture model (gmm)","curvature filter","bolts","instrumentation","classification","railway surface defect","fusion","squats","inspection system","image segmentation"]},{"p_id":9817,"title":"Classification of CT brain images based on deep learning networks","abstract":"While computerised tomography (CT) may have been the first imaging tool to study human brain, it has not yet been implemented into clinical decision making process for diagnosis of Alzheimer's disease (AD). On the other hand, with the nature of being prevalent, inexpensive and non-invasive, CT does present diagnostic features of AD to a great extent. This study explores the significance and impact on the application of the burgeoning deep learning techniques to the task of classification of CT brain images, in particular utilising convolutional neural network (CNN), aiming at providing supplementary information for the early diagnosis of Alzheimer's disease. Towards this end, three categories of CT images (N = 285) are clustered into three groups, which are AD, lesion (e.g. tumour) and normal ageing. In addition, considering the characteristics of this collection with larger thickness along the direction of depth (z) (similar to 3-5 mm), an advanced CNN architecture is established integrating both 2D and 3D CNN networks. The fusion of the two CNN networks is subsequently coordinated based on the average of Softmax scores obtained from both networks consolidating 2D images along spatial axial directions and 3D segmented blocks respectively. As a result, the classification accuracy rates rendered by this elaborated CNN architecture are 85.2%, 80% and 95.3% for classes of AD, lesion and normal respectively with an average of 87.6%. Additionally, this improved CNN network appears to outperform the others when in comparison with 2D version only of CNN network as well as a number of state of the art hand-crafted approaches. As a result, these approaches deliver accuracy rates in percentage of 86.3, 85.6 1.10, 86.3 1.04, 85.2 1.60, 83.1 0.35 for 2D CNN, 2D SIFT, 2D KAZE, 3D SIFT and 3D KAZE respectively. The two major contributions of the paper constitute a new 3-D approach while applying deep learning technique to extract signature information rooted in both 2D slices and 3D blocks of CT images and an elaborated hand-crated approach of 3D KAZE. (C) 2016 Elsevier Ireland Ltd. All rights reserved.","keywords_author":["3D CNN","Classification","Convolutional neural network","CT brain images","Deep learning","KAZE","Deep learning","Convolutional neural network","Classification","CT brain images","3D CNN","KAZE"],"keywords_other":["Deep learning","Brain images","Learning","FEATURES","Humans","Reproducibility of Results","Tomography, X-Ray Computed","Alzheimer Disease","DISEASE","Brain","RECOGNITION","Convolutional neural network","KAZE","3D CNN"],"max_cite":20.0,"pub_year":2017.0,"sources":"['ieee', 'scp', 'wos']","rawkeys":["tomography","3d cnn","recognition","features","deep learning","x-ray computed","disease","learning","reproducibility of results","humans","brain","alzheimer disease","classification","convolutional neural network","kaze","brain images","ct brain images"],"tags":["tomography","recognition","3d convolutional neural networks","alzheimers-disease","features","x-ray computed","disease","machine learning","reproducibility of results","humans","brain","brain imaging","classification","convolutional neural network","kaze","ct brain images"]},{"p_id":9818,"title":"Random synaptic feedback weights support error backpropagation for deep learning","abstract":"The brain processes information through multiple layers of neurons. This deep architecture is representationally powerful, but complicates learning because it is difficult to identify the responsible neurons when a mistake is made. In machine learning, the backpropagation algorithm assigns blame by multiplying error signals with all the synaptic weights on each neuron's axon and further downstream. However, this involves a precise, symmetric backward connectivity pattern, which is thought to be impossible in the brain. Here we demonstrate that this strong architectural constraint is not required for effective error propagation. We present a surprisingly simple mechanism that assigns blame by multiplying errors by even random synaptic weights. This mechanism can transmit teaching signals across multiple layers of neurons and performs as effectively as backpropagation on a variety of tasks. Our results help reopen questions about how the brain could use error signals and dispel long-held assumptions about algorithmic constraints on learning.","keywords_author":null,"keywords_other":["REINFORCEMENT","PLASTICITY","CEREBELLUM","SPIKING","NEURONS","REPRESENTATIONS","PREDICTION ERRORS","NEURAL-NETWORKS","RECOGNITION","PRIMARY VISUAL-CORTEX"],"max_cite":31.0,"pub_year":2016.0,"sources":"['scp', 'wos']","rawkeys":["neural-networks","recognition","cerebellum","prediction errors","primary visual-cortex","reinforcement","spiking","plasticity","representations","neurons"],"tags":["recognition","neural networks","cerebellum","prediction errors","primary visual-cortex","representation","spiking","plasticity","neurons"]},{"p_id":83548,"title":"Siamese neural network for intelligent information security control in multi-robot systems","abstract":"Anomaly detection of the robot system behavior is one of the important components of the information security control. In order to control robots equipped with many sensors it is difficult to apply the well-known Mahalanobis distance which allows us to analyze the current state of the sensors. Therefore, the Siamese neural network is proposed to intellectually support the security control. The Siamese network simplifies the anomaly detection of the robot system and realizes a non-linear analogue of the Mahalanobis distance. This peculiarity allows us to take into account complex data structures received from the robot sensors.","keywords_author":["multi-robot system","security control","anomaly detection","Siamese neural network","Mahalanobis distance","sensor"],"keywords_other":["SWARM ROBOTICS","RECOGNITION","SVM","FACE VERIFICATION"],"max_cite":0.0,"pub_year":2017.0,"sources":"['wos']","rawkeys":["siamese neural network","recognition","anomaly detection","mahalanobis distance","sensor","multi-robot system","face verification","svm","swarm robotics","security control"],"tags":["security controls","siamese neural network","recognition","anomaly detection","sensors","multi-robot systems","machine learning","face verification","swarm robotics","mahalanobis distances"]},{"p_id":9826,"title":"Deep multiple multilayer kernel learning in core vector machines","abstract":"Over the last few years, we have been witnessing a dramatic progress of deep learning in many real world applications. Deep learning concepts have been originated in the area of neural network and show a quantum leap in effective feature learning techniques such as auto-encoders, convolutional neural networks, recurrent neural networks etc. In the case of kernel machines, there are several attempts to model learning machines that mimic deep neural networks. In this direction, Multilayer Kernel Machines (MKMs) was an attempt to build a kernel machine architecture with multiple layers of feature extraction. It composed of many layers of kernel PCA based feature extraction units with support vector machine having arc-cosine kernel as the final classifier. The other approaches like Multiple Kernel Learning (MKL) and deep core vector machines solve the fixed kernel computation problem and scalability aspects of MKMs respectively. In addition to this, there are lot of avenues where the use of unsupervised MKL with both single and multilayer kernels in the multilayer feature extraction framework have to be evaluated. In this context, this paper attempts to build a scalable deep kernel machines with multiple layers of feature extraction. Each kernel PCA based feature extraction layer in this framework is modeled by the combination of both single and multilayer kernels in an unsupervised manner. Core vector machine with arc-cosine kernel is used as the final layer classifier which ensure the scalability in this model. The major contribution of this paper is a novel effort to build a deep structured kernel machine architecture similar to deep neural network architecture for classification. It opens up an extendable research avenue for researchers in deep learning based intelligent system leveraging the principles of kernel theory. Experiments show that the proposed method consistently improves the generalization performances of existing deep core vector machine. (C) 2017 Elsevier Ltd. All rights reserved.","keywords_author":["Deep kernel machines","Unsupervised MKL","Core vector machines","Scalability","Arc-cosine kernel","Multilayer kernel learning"],"keywords_other":["REPRESENTATION","RECOGNITION","NETWORKS"],"max_cite":0.0,"pub_year":2017.0,"sources":"['ieee', 'wos']","rawkeys":["scalability","core vector machines","recognition","arc-cosine kernel","representation","unsupervised mkl","networks","multilayer kernel learning","deep kernel machines"],"tags":["scalability","core vector machines","recognition","arc-cosine kernel","representation","unsupervised mkl","networks","multilayer kernel learning","deep kernel machines"]},{"p_id":83558,"title":"A monitoring method of semiconductor manufacturing processes using Internet of Things-based big data analysis","abstract":"This article proposes an intelligent monitoring system for semiconductor manufacturing equipment, which determines spec-in or spec-out for a wafer in process, using Internet of Things-based big data analysis. The proposed system consists of three phases: initialization, learning, and prediction in real time. The initialization sets the weights and the effective steps for all parameters of equipment to be monitored. The learning performs a clustering to assign similar patterns to the same class. The patterns consist of a multiple time-series produced by semiconductor manufacturing equipment and an after clean inspection measured by the corresponding tester. We modify the Line, Buzo, and Gray algorithm for classifying the time-series patterns. The modified Line, Buzo, and Gray algorithm outputs a reference model for every cluster. The prediction compares a time-series entered in real time with the reference model using statistical dynamic time warping to find the best matched pattern and then calculates a predicted after clean inspection by combining the measured after clean inspection, the dissimilarity, and the weights. Finally, it determines spec-in or spec-out for the wafer. We will present experimental results that show how the proposed system is applied on the data acquired from semiconductor etching equipment.","keywords_author":["Monitoring","learning","prediction","matched pattern","Internet of Things","reference model"],"keywords_other":["INFORMATION","CLASSIFICATION","SYSTEM","MODEL","RECOGNITION","SERVICES"],"max_cite":0.0,"pub_year":2017.0,"sources":"['wos']","rawkeys":["matched pattern","recognition","model","prediction","internet of things","learning","system","classification","information","services","reference model","monitoring"],"tags":["recognition","model","prediction","machine learning","system","classification","information","services","reference modeling","matching patterns","internet of things (iot)","monitoring"]},{"p_id":58988,"title":"Unsupervised-Restricted Deconvolutional Neural Network for Very High Resolution Remote-Sensing Image Classification","abstract":"As the acquisition of very high resolution (VHR) satellite images becomes easier owing to technological advancements, ever more stringent requirements are being imposed on automatic image interpretation. Moreover, per-pixel classification has become the focus of research interests in this regard. However, the efficient and effective processing and the interpretation of VHR satellite images remain a critical task. Convolutional neural networks (CNNs) have recently been applied to VHR satellite images with considerable success. However, the prevalent CNN models accept input data of fixed sizes and train the classifier using features extracted directly from the convolutional stages or the fully connected layers, which cannot yield pixel-to-pixel classifications. Moreover, training a CNN model requires large amounts of labeled reference data. These are challenging to obtain because per-pixel labeled VHR satellite images are not open access. In this paper, we propose a framework called the unsupervised-restricted deconvolutional neural network (URDNN). It can solve these problems by learning an end-to-end and pixel-to-pixel classification and handling a VHR classification using a fully convolutional network and a small number of labeled pixels. In URDNN, supervised learning is always under the restriction of unsupervised learning, which serves to constrain and aid supervised training in learning more generalized and abstract feature. To some degree, it will try to reduce the problems of overfitting and undertraining, which arise from the scarcity of labeled training data, and to gain better classification results using fewer training samples. It improves the generality of the classification model. We tested the proposed URDNN on images from the Geoeye and Quickbird sensors and obtained satisfactory results with the highest overall accuracy (OA) achieved as 0.977 and 0.989, respectively. Experiments showed that the combined effects of additional kernels and stages may have produced better results, and two-stage URDNN consistently produced a more stable result. We compared URDNN with four methods and found that with a small ratio of selected labeled data items, it yielded the highest and most stable results, whereas the accuracy values of the other methods quickly decreased. For some categories with fewer training pixels, accuracy for categories from other methods was considerably worse than that in URDNN, with the largest difference reaching almost 10%. Hence, the proposed URDNN can successfully handle the VHR image classification using a small number of labeled pixels. Furthermore, it is more effective than state-of-the-art methods.","keywords_author":["Convolutional neural networks (CNNs)","deconvolution","unsupervised learning and supervised learning","very high resolution (VHR) image per-pixel classification"],"keywords_other":["TRANSFORM","NDWI","EXTRACTION","RECOGNITION","VEGETATION","SCENE CLASSIFICATION"],"max_cite":3.0,"pub_year":2017.0,"sources":"['ieee', 'wos']","rawkeys":["very high resolution (vhr) image per-pixel classification","recognition","vegetation","deconvolution","scene classification","ndwi","unsupervised learning and supervised learning","transform","convolutional neural networks (cnns)","extraction"],"tags":["very high resolution (vhr) image per-pixel classification","recognition","vegetation","deconvolution","scene classification","ndwi","convolutional neural network","unsupervised learning and supervised learning","transform","extraction"]},{"p_id":58993,"title":"Spatial-Spectral Unsupervised Convolutional Sparse Auto-Encoder Classifier for Hyperspectral Imagery","abstract":"The traditional spatial-spectral classification methods applied to hyperspectral remote sensing imagery are conducted by combining the spatial information vector and the spectral information vector in a separate manner, which may cause information loss and concatenation deficiency between the spatial and spectral information. In addition, the traditional morphological-based spatial-spectral classification methods require the design of handcrafted features according to experience, which is far from automatic and lacks generalization ability. To automatically represent the spatial-spectral features around the central pixel within a spatial neighborhood window, a novel spatial-spectral feature classification method based on the unsupervised convolutional sparse auto-encoder (UCSAE) with a window-in-window strategy is proposed in this study. The UCSAE algorithm features a unique spatial-spectral feature extraction approach which is executed in two stages. The first stage represents the spatial-spectral features within a spatial neighborhood window on the basis of spatial-spectral feature extraction of sub-windows with a sparse auto-encoder (SAE). The second stage exploits the spatial-spectral feature representation with a convolution mechanism for the larger outer windows. The UCSAE algorithm was validated by two widely used hyperspectral imagery datasets (the Pavia University dataset and the Washington DC Mall dataset) obtaining accuracies of 90.03 percent and 96.88 percent, respectively, which are better results than those obtained by the traditional hyperspectral spatial-spectral classification approaches.","keywords_author":null,"keywords_other":["NETWORKS","RECOGNITION","INFORMATION","AUTOENCODERS"],"max_cite":7.0,"pub_year":2017.0,"sources":"['wos']","rawkeys":["networks","autoencoders","recognition","information"],"tags":["networks","recognition","auto encoders","information"]},{"p_id":1649,"title":"Driving posture recognition by convolutional neural networks","abstract":"Driver fatigue and inattention have long been recognised as the main contributing factors in traffic accidents. This study presents a novel system which applies convolutional neural network (CNN) to automatically learn and predict pre-defined driving postures. The main idea is to monitor driver hand position with discriminative information extracted to predict safe\/unsafe driving posture. In comparison to previous approaches, CNNs can automatically learn discriminative features directly from raw images. In the authors' works, a CNN model was first pre-trained by an unsupervised feature learning method called sparse filtering, and subsequently fine-tuned with classification. The approach was verified using the Southeast University driving posture dataset, which comprised of video clips covering four driving postures, including normal driving, responding to a cell phone call, eating, and smoking. Compared with other popular approaches with different image descriptors and classification methods, the authors' scheme achieves the best performance with an overall accuracy of 99.78%. To evaluate the effectiveness and generalisation performance in more realistic conditions, the method was further tested using other two specially designed datasets which takes into account of the poor illuminations and different road conditions, achieving an overall accuracy of 99.3 and 95.77%, respectively.","keywords_author":["pose estimation","image classification","feature extraction","driver information systems","feedforward neural nets","unsupervised learning","road accidents","road traffic","video signal processing","image filtering","object recognition","driving posture recognition","convolutional neural networks","driver fatigue","driver inattention","traffic accidents","intelligent driver assistance system development","embedded functionality","driver vigilance monitoring","driver hand position monitoring","automatic discriminative feature learning","unsupervised feature learning method","sparse filtering","Southeast University driving posture dataset","video clips","normal driving","cell phone call","eating","smoking","image descriptors","effectiveness performance","generalisation performance","road conditions","poor illuminations"],"keywords_other":["pose estimation","image filtering","KNOWLEDGE","driver information systems","SYSTEM","traffic accidents","road conditions","image classification","normal driving","embedded functionality","DRIVER","video clips","unsupervised feature learning method","poor illuminations","smoking","DEPENDENCIES","driver vigilance monitoring","eating","feedforward neural nets","effectiveness performance","convolutional neural networks","FEATURES","generalisation performance","video signal processing","road traffic","driver inattention","unsupervised learning","PREDICTION","intelligent driver assistance system development","image descriptors","TRANSFORM","Southeast University driving posture dataset","cell phone call","automatic discriminative feature learning","object recognition","BEHAVIOR","driving posture recognition","sparse filtering","feature extraction","road accidents","driver fatigue","driver hand position monitoring"],"max_cite":5.0,"pub_year":2016.0,"sources":"['ieee', 'wos']","rawkeys":["pose estimation","knowledge","southeast university driving posture dataset","image filtering","driver information systems","road conditions","traffic accidents","image classification","normal driving","embedded functionality","features","system","video clips","unsupervised feature learning method","poor illuminations","smoking","transform","driver vigilance monitoring","feedforward neural nets","eating","behavior","effectiveness performance","convolutional neural networks","generalisation performance","video signal processing","driver","road traffic","dependencies","driver inattention","unsupervised learning","intelligent driver assistance system development","image descriptors","prediction","cell phone call","automatic discriminative feature learning","object recognition","driving posture recognition","sparse filtering","feature extraction","road accidents","driver fatigue","driver hand position monitoring"],"tags":["pose estimation","knowledge","image filtering","driver information systems","convolutional neural network","traffic accidents","image classification","normal driving","embedded functionality","features","system","video clips","unsupervised feature learning method","smoking","transform","driver vigilance monitoring","drivers","poor illumination","feedforward neural nets","eating","behavior","recognition","generalisation performance","video signal processing","road condition","road traffic","driver inattention","unsupervised learning","southeast university driving-posture dataset","intelligent driver assistance system development","image descriptors","effective performance","prediction","cell phone call","automatic discriminative feature learning","object recognition","driving posture recognition","sparse filtering","feature extraction","road accidents","driver fatigue","driver hand position monitoring"]},{"p_id":42612,"title":"New trends on digitisation of complex engineering drawings","abstract":"\u00a9 2018 The Author(s) Engineering drawings are commonly used across different industries such as oil and gas, mechanical engineering and others. Digitising these drawings is becoming increasingly important. This is mainly due to the legacy of drawings and documents that may provide rich source of information for industries. Analysing these drawings often requires applying a set of digital image processing methods to detect and classify symbols and other components. Despite the recent significant advances in image processing, and in particular in deep neural networks, automatic analysis and processing of these engineering drawings is still far from being complete. This paper presents a general framework for complex engineering drawing digitisation. A thorough and critical review of relevant literature, methods and algorithms in machine learning and machine vision is presented. Real-life industrial scenario on how to contextualise the digitised information from specific type of these drawings, namely piping and instrumentation diagrams, is discussed in details. A discussion of how new trends on machine vision such as deep learning could be applied to this domain is presented with conclusions and suggestions for future research directions.","keywords_author":["Classification","Contextualisation","Convolutional neural networks","Deep learning","Digitisation","Engineering drawing","Feature extraction","Recognition","Segmentation"],"keywords_other":["Recognition","Contextualisation","Engineering drawing","Convolutional neural network","Digitisation"],"max_cite":0.0,"pub_year":2018.0,"sources":"['scp', 'ieee']","rawkeys":["recognition","convolutional neural networks","segmentation","deep learning","engineering drawing","classification","convolutional neural network","feature extraction","digitisation","contextualisation"],"tags":["recognition","segmentation","engineering drawing","machine learning","classification","convolutional neural network","feature extraction","digitisation","contextualisation"]},{"p_id":91765,"title":"Automated mitosis detection in histopathology based on non-gaussian modeling of complex wavelet coefficients","abstract":"To diagnose breast cancer, the number of mitotic cells present in histology sections is an important indicator for examining and grading biopsy specimen. This study aims at improving the accuracy of automated mitosis detection by characterizing mitotic cells in wavelet based multi-resolution representations via a non-Gaussian modeling method. The potential mitosis candidates were decomposed into multi-scale forms by an undecimated dual-tree complex wavelet transform. Two non-Gaussian models (the generalized Gaussian distribition (GGD) and the symmetric alpha-stable (SaS) distributions) were used to accurately model the heavy-tailed behavior of wavelet marginal distributions. The method was evaluated on two independent data cohorts, including the benchmark dataset (MITOS), via a support vector machine classifier. The quantitative results shOws that the bivariate SaS model achieved superior classification performance with the area under the curve valhe of 0.82 in comparison with 0.79 for bivariate GGD, 0.77 for univariate SaS, 0.72 for univariate GGD, and 0.59 for Gaussian model. Since both mitotic and non-mitotic cells appear as small objects with a large variety of shapes, characterization of mitosis is a hard problem. The inter-scale dependencies of wavelet coefficients allowing extraction of salient features within the cells that are more likely to appear at all different scales were captured by the bivariate non-Gaussian models, leading to more accurate detection results. The presented automated mitosis detection method might assist pathologists in enhancing the operational efficiency and productivity as well as improving diagnostic confidence.","keywords_author":["Histopathology","Breast cancer","Mitosis detection","Non-Gaussianmodel","Wavelet"],"keywords_other":["DENSITY","TRANSFORM","IMAGE-ANALYSIS","SUPPORT VECTOR MACHINE","CLASSIFICATION","DISTANCE","DEPENDENCY","SEGMENTATION","BREAST-CANCER HISTOPATHOLOGY"],"max_cite":0.0,"pub_year":2017.0,"sources":"['ieee', 'wos']","rawkeys":["non-gaussianmodel","segmentation","breast-cancer histopathology","dependency","histopathology","wavelet","distance","classification","transform","support vector machine","image-analysis","mitosis detection","density","breast cancer"],"tags":["non-gaussianmodel","recognition","segmentation","breast-cancer histopathology","image analysis","machine learning","histopathology","wavelet","distance","classification","transform","mitosis detection","density","breast cancer"]},{"p_id":9851,"title":"A deep semantic framework for multimodal representation learning","abstract":"Multimodal representation learning has gained increasing importance in various real-world multimedia applications. Most previous approaches focused on exploring inter-modal correlation by learning a common or intermediate space in a conventional way, e.g. Canonical Correlation Analysis (CCA). These works neglected the exploration of fusing multiple modalities at higher semantic level. In this paper, inspired by the success of deep networks in multimedia computing, we propose a novel unified deep neural framework for multimodal representation learning. To capture the high-level semantic correlations across modalities, we adopted deep learning feature as image representation and topic feature as text representation respectively. In joint model learning, a 5-layer neural network is designed and enforced with a supervised pre-training in the first 3 layers for intra-modal regularization. The extensive experiments on benchmark Wikipedia and MIR Flickr 25K datasets show that our approach achieves state-of-the-art results compare to both shallow and deep models in multimodal and cross-modal retrieval.","keywords_author":["Multimodal representation","Deep neural networks","Semantic feature","Cross-modal retrieval"],"keywords_other":["IMAGE RETRIEVAL","RECOGNITION","MODAL MULTIMEDIA RETRIEVAL","FEATURES"],"max_cite":6.0,"pub_year":2016.0,"sources":"['ieee', 'wos']","rawkeys":["modal multimedia retrieval","semantic feature","recognition","features","deep neural networks","multimodal representation","image retrieval","cross-modal retrieval"],"tags":["modal multimedia retrieval","recognition","features","image retrieval","semantic features","multimodal representations","convolutional neural network","cross-modal retrieval"]},{"p_id":83583,"title":"Dimensionality Reduction and Classification of Hyperspectral Images Using Ensemble Discriminative Local Metric Learning","abstract":"The high-dimensional data space of hyperspectral images (HSIs) often result in ill-conditioned formulations, which finally leads to many of the high-dimensional feature spaces being empty and the useful data existing primarily in a subspace. To avoid these problems, we use distance metric learning for dimensionality reduction. The goal of distance metric learning is to incorporate abundant discriminative information by reducing the dimensionality of the data. Considering that global metric learning is not appropriate for all training samples, this paper proposes an ensemble discriminative local metric learning (EDLML) algorithm for HSI analysis. The EDLML algorithm learns robust local metrics from both the training samples and the relative neighborhood of them and considers the different local discriminative distance metrics by dealing with the data region by region. It aims to learn a subspace to keep all the samples in the same class are as near as possible, while those from different classes are separated. The learned local metrics are then used to build an ensemble metric. Experiments on a number of different hyperspectral data sets confirm the effectiveness of the proposed EDLML algorithm compared with that of the other dimension reduction methods.","keywords_author":["Dimensionality reduction","ensemble learning","hyperspectral image (HSI) classification","local discriminative distance metrics"],"keywords_other":["SUBSPACE","TARGET DETECTION","DISTANCE METRICS","CLONAL SELECTION","RECOGNITION","SPARSE REPRESENTATION","FEATURE-EXTRACTION","PCA"],"max_cite":9.0,"pub_year":2017.0,"sources":"['ieee', 'wos']","rawkeys":["recognition","clonal selection","hyperspectral image (hsi) classification","feature-extraction","sparse representation","local discriminative distance metrics","distance metrics","target detection","ensemble learning","subspace","pca","dimensionality reduction"],"tags":["principal component analysis","recognition","clonal selection","sparse representation","hyperspectral image classification","local discriminative distance metrics","distance metrics","target detection","feature extraction","ensemble learning","subspace","dimensionality reduction"]},{"p_id":59008,"title":"Bag of Visual Words Model with Deep Spatial Features for Geographical Scene Classification","abstract":"With the popular use of geotagging images, more and more research efforts have been placed on geographical scene classification. In geographical scene classification, valid spatial feature selection can significantly boost the final performance. Bag of visual words (BoVW) can do well in selecting feature in geographical scene classification; nevertheless, it works effectively only if the provided feature extractor is well-matched. In this paper, we use convolutional neural networks (CNNs) for optimizing proposed feature extractor, so that it can learn more suitable visual vocabularies from the geotagging images. Our approach achieves better performance than BoVW as a tool for geographical scene classification, respectively, in three datasets which contain a variety of scene categories.","keywords_author":null,"keywords_other":["NETWORKS","RECOGNITION"],"max_cite":0.0,"pub_year":2017.0,"sources":"['ieee', 'wos']","rawkeys":["networks","recognition"],"tags":["networks","recognition"]},{"p_id":1670,"title":"Deep Neural Network for Structural Prediction and Lane Detection in Traffic Scene","abstract":"Hierarchical neural networks have been shown to be effective in learning representative image features and recognizing object classes. However, most existing networks combine the low\/middle level cues for classification without accounting for any spatial structures. For applications such as understanding a scene, how the visual cues are spatially distributed in an image becomes essential for successful analysis. This paper extends the framework of deep neural networks by accounting for the structural cues in the visual signals. In particular, two kinds of neural networks have been proposed. First, we develop a multitask deep convolutional network, which simultaneously detects the presence of the target and the geometric attributes (location and orientation) of the target with respect to the region of interest. Second, a recurrent neuron layer is adopted for structured visual detection. The recurrent neurons can deal with the spatial distribution of visible cues belonging to an object whose shape or structure is difficult to explicitly define. Both the networks are demonstrated by the practical task of detecting lane boundaries in traffic scenes. The multitask convolutional neural network provides auxiliary geometric information to help the subsequent modeling of the given lane structures. The recurrent neural network automatically detects lane boundaries, including those areas containing no marks, without any explicit prior knowledge or secondary modeling.","keywords_author":["Image recognition","pattern analysis","recurrent neural networks","Image recognition","pattern analysis","recurrent neural networks"],"keywords_other":["Image recognition","structural prediction","region of interest","ARCHITECTURES","ALGORITHM","Biological neural networks","edge detection","multitask convolutional neural network","TRACKING","recurrent neural nets","Feature extraction","LEARNING DEEP","lane boundary detection","Visualization","Convolution","structured visual detection","recurrent neural network","RECOGNITION","TIME","ADAPTIVE-BEHAVIOR","multitask deep convolutional network","Neurons","traffic scenes","Image analysis","geometric attributes","traffic scene","recurrent neuron layer"],"max_cite":20.0,"pub_year":2016.0,"sources":"['wos', 'ieee']","rawkeys":["structural prediction","region of interest","recurrent neural networks","edge detection","multitask convolutional neural network","visualization","recurrent neural nets","convolution","lane boundary detection","time","learning deep","biological neural networks","adaptive-behavior","neurons","structured visual detection","algorithm","recognition","recurrent neural network","multitask deep convolutional network","image analysis","architectures","traffic scenes","tracking","geometric attributes","feature extraction","image recognition","traffic scene","pattern analysis","recurrent neuron layer"],"tags":["region of interest","adaptive behavior","edge detection","multitask convolutional neural network","visualization","architecture","recurrent neural nets","convolution","lane boundary detection","time","learning deep","algorithms","biological neural networks","structured visual detection","neurons","structure prediction","recognition","neural networks","multitask deep convolutional network","image analysis","tracking","geometric attributes","feature extraction","image recognition","traffic scene","pattern analysis","recurrent neuron layer"]},{"p_id":9863,"title":"Automatic handgun detection alarm in videos using deep learning","abstract":"Current surveillance and control systems still require human supervision and intervention. This work presents a novel automatic handgun detection system in videos appropriate for both, surveillance and control purposes. We reformulate this detection problem into the problem of minimizing false positives and solve it by i) building the key training data-set guided by the results of a deep Convolutional Neural Networks (CNN) classifier and ii) assessing the best classification model under two approaches, the sliding window approach and region proposal approach. The most promising results are obtained by Faster R-CNN based model trained on our new database. The best detector shows a high potential even in low quality youtube videos and provides satisfactory results as automatic alarm system. Among 30 scenes, it successfully activates the alarm after five successive true positives in a time interval smaller than 0.2 s, in 27 scenes. We also define a new metric, Alarm Activation Time per Interval (AATpI), to assess the performance of a detection model as an automatic detection system in videos. (c) 2017 Elsevier B.V. All rights reserved.","keywords_author":["Alarm Activation Time per Interval","Classification","Convolutional Neural Networks","Deep learning","Detection","Faster R-CNN","VGG-16","Classification","Detection","Deep learning","Convolutional Neural Networks","Faster R-CNN","VGG-16","Alarm Activation Time per Interval"],"keywords_other":["Faster R-CNN","Automatic detection systems","Activation time","Classification models","VGG-16","RECOGNITION","Convolutional neural network","Detection problems","Training data sets"],"max_cite":2.0,"pub_year":2018.0,"sources":"['ieee', 'scp', 'wos']","rawkeys":["recognition","convolutional neural networks","deep learning","vgg-16","alarm activation time per interval","automatic detection systems","classification","convolutional neural network","detection","activation time","faster r-cnn","training data sets","detection problems","classification models"],"tags":["recognition","vgg-16","machine learning","alarm activation time per interval","automatic detection systems","classification","convolutional neural network","detection","training data sets","faster r-cnn","detection problems","classification models","activity time"]},{"p_id":26249,"title":"100 years of occupational safety research: From basic protections and work analysis to a multilevel view of workplace safety and risk","abstract":"\u00a9 2017 American Psychological Association. Starting with initiatives dating back to the mid-1800s, we provide a high-level review of the key trends and developments in the application of applied psychology to the field of occupational safety. Factory laws, basic worker compensation, and research on accident proneness comprised much of the early work. Thus, early research and practice very much focused on the individual worker, the design of their work, and their basic protection. Gradually and over time, the focus began to navigate further into the organizational context. One of the early efforts to broaden beyond the individual worker was a significant focus on safety-related training during the middle of the 20th century. Toward the latter years of the 20th century and continuing the move from the individual worker to the broader organizational context, there was a significant increase in leadership and organizational climate (safety climate) research. Ultimately, this resulted in the development of a multilevel model of safety culture\/climate. After discussing these trends, we identify key conclusions and opportunities for future research.","keywords_author":["History","Occupational health","Review","Safety"],"keywords_other":["Psychology, Industrial","History, 19th Century","Occupational Health","Humans","Research","History, 20th Century","History, 21st Century","Safety","Organizational Culture"],"max_cite":7.0,"pub_year":2017.0,"sources":"['scp']","rawkeys":["industrial","review","organizational culture","humans","safety","psychology","history","19th century","occupational health","21st century","20th century","research"],"tags":["industry","recognition","review","organizational culture","humans","safety","21st-century","history","19th century","occupational health","research","20th century"]},{"p_id":59017,"title":"Superpixel-based 3D deep neural networks for hyperspectral image classification","abstract":"This paper presents a novel hyperspectral image (HSI) classification method to effectively exploit the 3D spectral-spatial information via superpixel-based 3D deep neural networks (3D DNNs). Superpixel can represent the structure of HSI with adaptive sizes and shapes, and therefore, it is incorporated into 3D DNNs to improve the classification performance, especially for noisy classification and boundary misclassification. First, a spatial feature image via superpixel is constructed to increase the spectral-spatial similarity and diversity. Second, a 3D superpixel-based sample filling method is designed to solve the misclassification problem of boundaries. Third, a 3D recurrent convolutional networks (3D RCNNs) are designed to further exploit spatial continuity and suppress noisy prediction. Experimental results on real HSI datasets demonstrate the superiority of the proposed method over several well-known methods in both visual appearance and classification accuracy. (C) 2017 Elsevier Ltd. All rights reserved.","keywords_author":["Hyperspectral image classification","Superpixel","3D deep neural networks"],"keywords_other":["SPARSE REPRESENTATION","RECOGNITION","SPECTRAL-SPATIAL CLASSIFICATION","FEATURES"],"max_cite":2.0,"pub_year":2017.0,"sources":"['ieee', 'wos']","rawkeys":["recognition","3d deep neural networks","features","spectral-spatial classification","sparse representation","hyperspectral image classification","superpixel"],"tags":["recognition","3d deep neural networks","features","spectral-spatial classification","sparse representation","hyperspectral image classification","superpixel"]},{"p_id":9869,"title":"Classification of Power Quality Disturbances via Deep Learning","abstract":"In this paper, a deep learning-based method is introduced into the classification of power quality disturbances (PQDs). Stacked autoencoder, as a deep learning framework, is employed to extract high-level features of PQDs for classification. In this context, a previously unsolved issue regarding optimal features' selection for PQDs can be addressed. Besides, variances of signals and a particle swarm optimization algorithm are applied to assist classification of the PQDs. To sufficiently validate the effectiveness of the proposed classification scheme, a 10-fold cross-validation has been done. Experimental results indicate that the proposed approach is robust against noise and reaches a desirable classification accuracy rate.","keywords_author":["Deep learning","Power quality disturbances (PQDs)","Sparse autoencoder","Stacked autoencoder (SAE)","Deep learning","Power quality disturbances (PQDs)","Sparse autoencoder","Stacked autoencoder (SAE)"],"keywords_other":["Learning-based methods","AUTOMATIC CLASSIFICATION","S-TRANSFORM","10-fold cross-validation","REPRESENTATION","High-level features","Classification scheme","Power quality disturbances","Learning frameworks","NEURAL-NETWORKS","RECOGNITION","Auto encoders","Classification accuracy"],"max_cite":2.0,"pub_year":2017.0,"sources":"['ieee', 'scp', 'wos']","rawkeys":["neural-networks","learning-based methods","high-level features","recognition","10-fold cross-validation","automatic classification","classification accuracy","deep learning","auto encoders","representation","s-transform","learning frameworks","stacked autoencoder (sae)","power quality disturbances (pqds)","classification scheme","power quality disturbances","sparse autoencoder"],"tags":["learning-based methods","high-level features","recognition","10-fold cross-validation","automatic classification","classification accuracy","neural networks","auto encoders","machine learning","representation","s-transform","stacked autoencoders","learning frameworks","classification scheme","power quality disturbances"]},{"p_id":59024,"title":"Using convolutional features and a sparse autoencoder for land-use scene classification","abstract":"In this article, we propose a novel approach based on convolutional features and sparse autoencoder (AE) for scene-level land-use (LU) classification. This approach starts by generating an initial feature representation of the scenes under analysis from a deep convolutional neural network (CNN) pre-learned on a large amount of labelled data from an auxiliary domain. Then these convolutional features are fed as input to a sparse AE for learning a new suitable representation in an unsupervised manner. After this pre-training phase, we propose two different scenarios for building the classification system. In the first scenario, we add a softmax layer on the top of the AE encoding layer and then fine-tune the resulting network in a supervised manner using the target training images available at hand. Then we classify the test images based on the posterior probabilities provided by the softmax layer. In the second scenario, we view the classification problem from a reconstruction perspective. To this end we train several class-specific AEs (i.e. one AE per class) and then classify the test images based on the reconstruction error. Experimental results conducted on the University of California (UC) Merced and Banja-Luka LU public data sets confirm the superiority of the proposed approach compared to state-of-the-art methods.","keywords_author":null,"keywords_other":["DOMAIN","HYPERSPECTRAL DATA","RECOGNITION","DEEP NEURAL-NETWORKS","OBJECT DETECTION","IMAGES"],"max_cite":14.0,"pub_year":2016.0,"sources":"['ieee', 'wos']","rawkeys":["recognition","images","hyperspectral data","deep neural-networks","object detection","domain"],"tags":["recognition","images","hyperspectral data","object detection","convolutional neural network","domain"]},{"p_id":9874,"title":"Why Deep Learning Works: A Manifold Disentanglement Perspective","abstract":"Deep hierarchical representations of the data have been found out to provide better informative features for several machine learning applications. In addition, multilayer neural networks surprisingly tend to achieve better performance when they are subject to an unsupervised pretraining. The booming of deep learning motivates researchers to identify the factors that contribute to its success. One possible reason identified is the flattening of manifold-shaped data in higher layers of neural networks. However, it is not clear how to measure the flattening of such manifold-shaped data and what amount of flattening a deep neural network can achieve. For the first time, this paper provides quantitative evidence to validate the flattening hypothesis. To achieve this, we propose a few quantities for measuring manifold entanglement under certain assumptions and conduct experiments with both synthetic and real-world data. Our experimental results validate the proposition and lead to new insights on deep learning.","keywords_author":["Deep learning","disentanglement","manifold learning","unsupervised feature transformation","Deep learning","disentanglement","manifold learning","unsupervised feature transformation"],"keywords_other":["Deep learning","Real-world","DIMENSIONALITY","Pre-training","Hierarchical representation","NEURAL-NETWORKS","Deep neural networks","RECOGNITION","Machine learning applications","SUBSPACES"],"max_cite":16.0,"pub_year":2016.0,"sources":"['ieee', 'scp', 'wos']","rawkeys":["hierarchical representation","neural-networks","dimensionality","recognition","deep learning","deep neural networks","machine learning applications","pre-training","manifold learning","unsupervised feature transformation","real-world","subspaces","disentanglement"],"tags":["hierarchical representation","dimensionality","recognition","machine learning applications","neural networks","pre-training","machine learning","manifold learning","unsupervised feature transformation","convolutional neural network","real-world","subspace","disentanglement"]},{"p_id":9876,"title":"Deep super-class learning for long-tail distributed image classification","abstract":"Long-tail distribution is widespread in many practical applications, where most categories contain only a small number of samples. As sufficient instances cannot be obtained for describing the intra-class diversity of the minority classes, the separating hyperplanes learned by traditional machine learning methods are usually heavily skewed. Resampling techniques and cost-sensitive algorithms have been introduced to enhance the statistical power of the minority classes, but they cannot infer more reliable class boundaries beyond the description of samples in the training set. To address this issue, we cluster the original categories into super-class to produce a relatively balanced distribution in the super-class space. Moreover, the knowledge shared among categories belonging to a certain super-class can facilitate the generalization of the minority classes. However, existing super-class construction methods have some inherent disadvantages. Specifically, taxonomy-based methods suffer a gap between the semantic space and the feature space, and the performance of learning-based algorithms strongly depends on the features and data distribution. In this paper, we propose a deep super-class learning (DSCL) model to tackle the problem of long-tail distributed image classification. Motivated by the observation that classes belonging to the same super-class usually have more similar evaluations on the features than those belonging to different super classes, we design a block-structured sparse constraint and attach it on the top of a convolutional neural network. Thus, the proposed DSCL model can accomplish representation learning, classifier training, and super-class construction in a unified end-to-end learning procedure. We compared the proposed model with several super-class construction methods on two public image datasets. Experimental results show that the super-class construction strategy is effective for the long-tail distributed classification, and the DSCL model can achieve better results than the other methods. (C) 2018 Elsevier Ltd. All rights reserved.","keywords_author":["Block-structured sparsity","Deep learning","Long-tail distribution","Super-class construction","Super-class construction","Block-structured sparsity","Deep learning","Long-tail distribution"],"keywords_other":["Learning-based algorithms","SELECTION","EFFICIENT","Long-tail distribution","REGULARIZATION","Construction strategies","NEURAL-NETWORKS","Distributed classification","RECOGNITION","Convolutional neural network","Machine learning methods","TRAINING DATA","ANNOTATION","Cost-sensitive algorithm","Block structured"],"max_cite":0.0,"pub_year":2018.0,"sources":"['ieee', 'scp', 'wos']","rawkeys":["efficient","machine learning methods","neural-networks","super-class construction","recognition","learning-based algorithms","deep learning","long-tail distribution","selection","construction strategies","block-structured sparsity","block structured","convolutional neural network","cost-sensitive algorithm","distributed classification","annotation","training data","regularization"],"tags":["machine learning methods","super-class construction","recognition","learning-based algorithms","neural networks","machine learning","efficiency","construction strategies","long-tailed distributions","block structures","block-structured sparsity","convolutional neural network","cost-sensitive algorithm","distributed classification","selection","annotation","training data","regularization"]},{"p_id":9879,"title":"Learning Deep Spatio-Temporal Dependence for Semantic Video Segmentation","abstract":"Semantically labeling every pixel in a video is a very challenging task as video is an information-intensive media with complex spatio-temporal dependence. We present in this paper a novel deep convolutional network architecture, called deep spatio-temporal fully convolutional networks (DST-FCN), which leverages both spatial and temporal dependencies among pixels and voxels by training them in an end-to-end manner. Specifically, we introduce a two-stream network by learning the deep spatio-temporal dependence, in which a 2D FCN followed by the convolutional long short-term memory (ConvLSTM) is employed on the pixel level and a 3-D FCN is exploited on the voxel level. Our model differs from conventional FCN in that it not only extends FCN by adding ConvLSTM on the pixel level for exploring long-term dependence, but also proposes 3-D FCN to enable voxel level prediction. On two benchmarks of A2D and CamVid, our DST-FCN achieves superior results to state-of-the-art techniques. More remarkably, we obtain to-date the best reported results: 45.0% per-label accuracy on A2D and 68.8% mean IoU on CamVid.","keywords_author":["Semantic segmentation","fully convolutional networks","long-short term memory","video segmentation"],"keywords_other":["RECOGNITION"],"max_cite":0.0,"pub_year":2018.0,"sources":"['ieee', 'wos']","rawkeys":["recognition","video segmentation","fully convolutional networks","semantic segmentation","long-short term memory"],"tags":["recognition","long short-term memory","video segmentation","fully convolutional network","semantic segmentation"]},{"p_id":9895,"title":"Sunspot drawings handwritten character recognition method based on deep learning","abstract":"High accuracy scanned sunspot drawings handwritten characters recognition is an issue of critical importance to analyze sunspots movement and store them in the database. This paper presents a robust deep learning method for scanned sunspot drawings handwritten characters recognition. The convolution neural network (CNN) is one algorithm of deep learning which is truly successful in training of multi-layer network structure. CNN is used to train recognition model of handwritten character images which are extracted from the original sunspot drawings. We demonstrate the advantages of the proposed method on sunspot drawings provided by Chinese Academy Yunnan Observatory and obtain the daily full-disc sunspot numbers and sunspot areas from the sunspot drawings. The experimental results show that the proposed method achieves a high recognition accurate rate. (C) 2015 Elsevier B.V. All rights reserved.","keywords_author":["Sunspot drawings","Convolution neural network","Handwriting character","Recognition"],"keywords_other":["CATALOG"],"max_cite":1.0,"pub_year":2016.0,"sources":"['ieee', 'wos']","rawkeys":["recognition","convolution neural network","sunspot drawings","handwriting character","catalog"],"tags":["recognition","sunspot drawings","convolutional neural network","handwriting character","catalog"]},{"p_id":116393,"title":"An approach for chest tube detection in chest radiographs","abstract":null,"keywords_author":null,"keywords_other":["CONVOLUTIONAL NEURAL-NETWORK","IMAGE DATABASE","COMPUTER-AIDED DIAGNOSIS","LUNG NODULES","SYSTEM","RECOGNITION"],"max_cite":5.0,"pub_year":2014.0,"sources":"['wos']","rawkeys":["recognition","computer-aided diagnosis","system","lung nodules","convolutional neural-network","image database"],"tags":["recognition","computer-aided diagnosis","system","lung nodules","convolutional neural network","image database"]},{"p_id":9901,"title":"Updating the Silent Speech Challenge benchmark with deep learning","abstract":"The term \"Silent Speech Interface\" was introduced almost a decade ago to describe speech communication systems using only non-acoustic sensors, such as electromyography, ultrasound tongue imaging, or electromagnetic articulography. Although the use of specialized sensors in speech processing is challenging, silent speech research remains an active field that can often profit from new developments in traditional acoustic speech processing - for example recent advances in Deep Learning. After an overview of Silent Speech Interfaces and their special challenges, the article presents new results in which a 2010 benchmark study, called the Silent Speech Challenge, is updated with a Deep Learning strategy, using the same input features and decoding strategy as in the original Challenge article. A Word Error Rate of 6.4% is obtained with the new method, compared to the published benchmark value of 17.4%. Additional results comparing new auto-encoder-based features with the original features at reduced dimensionality, as well as decoding scenarios on two different language models, are also presented. The Silent Speech Challenge archive has furthermore been updated to contain both the original and the new auto-encoder features, in addition to the original raw data.","keywords_author":["Deep learning","Language model","Multimodal speech recognition","Silent speech interface","Silent speech interface","Multimodal speech recognition","Deep learning","Language model"],"keywords_other":["Electromagnetic articulography","Multi-modal speech recognition","Benchmark study","Decoding strategy","Non-acoustic sensors","Language model","SYSTEM","NEURAL-NETWORKS","RECOGNITION","Learning strategy","ULTRASOUND IMAGES","Silent speech interfaces","INTERFACE"],"max_cite":0.0,"pub_year":2018.0,"sources":"['ieee', 'scp', 'wos']","rawkeys":["neural-networks","electromagnetic articulography","non-acoustic sensors","recognition","deep learning","interface","multimodal speech recognition","system","decoding strategy","multi-modal speech recognition","benchmark study","language model","silent speech interface","ultrasound images","silent speech interfaces","learning strategy"],"tags":["electromagnetic articulography","non-acoustic sensors","recognition","neural networks","interface","multimodal speech recognition","machine learning","system","decoding strategy","multi-modal speech recognition","benchmark study","language model","ultrasound images","silent speech interfaces","learning strategy"]},{"p_id":108208,"title":"First- and Second-Order Methods for Online Convolutional Dictionary Learning","abstract":"Convolutional sparse representations are a form of sparse representation with a structured, translation-invariant dictionary. Most convolutional dictionary learning algorithms to date operate in batch mode, requiring simultaneous access to all training images during the learning process, which results in very high memory usage and severely limits the training data size that can be used. Very recently, however, a number of authors have considered the design of online convolutional dictionary learning algorithms that offer far better scaling of memory and computational cost with training set size than batch methods. This paper extends our prior work, improving a number of aspects of our previous algorithm; proposing an entirely new one, with better performance, that supports the inclusion of a spatial mask for learning from incomplete data; and providing a rigorous theoretical analysis of these methods.","keywords_author":["convolutional sparse coding","convolutional dictionary learning","online dictionary learning","stochastic gradient descent","recursive least squares"],"keywords_other":["IMAGE","RECURSIVE LEAST-SQUARES","NOISE","VISION","ALGORITHM","CONVERGENCE","RECOGNITION","RECOVERY","SPARSE REPRESENTATION"],"max_cite":0.0,"pub_year":2018.0,"sources":"['wos']","rawkeys":["convergence","algorithm","recognition","noise","online dictionary learning","recursive least-squares","sparse representation","convolutional sparse coding","recovery","vision","image","convolutional dictionary learning","stochastic gradient descent","recursive least squares"],"tags":["recognition","images","noise","online dictionary learning","sparse representation","convolutional sparse coding","recovery","vision","convolutional dictionary learning","recursive least square (rls)","mathematics","stochastic gradient descent","algorithms"]},{"p_id":9904,"title":"Deep unsupervised learning with consistent inference of latent representations","abstract":"Utilizing unlabeled data to train deep neural networks (DNNs) is a crucial but challenging task. In this paper, we propose an end-to-end approach to tackle this problem with consistent inference of latent representations. Specifically, each unlabeled data point is considered as a seed to generate a set of latent labeled data points by adding various random disturbances or transformations. Under the expectation maximization framework, DNNs can be trained in an unsupervised way by minimizing the distances between the data points with the same latent representations. Furthermore, several variants of our approach can be derived by applying regularized and sparse constraints during optimization. Theoretically, the convergence of the proposed method and its variants are fully analyzed. Experimental results show that the proposed approach can significantly improve the performance on various tasks, including image classification and clustering. Such results also indicate that our method can guide DNNs to learn more invariant feature representations in comparison with traditional unsupervised methods. (C) 2017 Elsevier Ltd. All rights reserved.","keywords_author":["Deep unsupervised learning","Consistent inference of latent representations"],"keywords_other":["ALGORITHM","AUTO-ENCODERS","CLASSIFICATION","NEURAL-NETWORKS","RECOGNITION"],"max_cite":0.0,"pub_year":2017.0,"sources":"['ieee', 'wos']","rawkeys":["neural-networks","algorithm","recognition","deep unsupervised learning","consistent inference of latent representations","classification","auto-encoders"],"tags":["recognition","neural networks","deep unsupervised learning","auto encoders","consistent inference of latent representations","classification","algorithms"]},{"p_id":9912,"title":"Modulation Format Identification in Coherent Receivers Using Deep Machine Learning","abstract":"We propose a novel technique for modulation format identification (MFI) in digital coherent receivers by applying deep neural network (DNN) based pattern recognition on signals' amplitude histograms obtained after constant modulus algorithm (CMA) equalization. Experimental results for three commonly-used modulation formats demonstrate MFI with an accuracy of 100% over a wide optical signal-to-noise ratio (OSNR) range. The effects of fiber nonlinearity on the performance of MFI technique are also investigated. The proposed technique is non-data-aided (NDA) and avoids any additional hardware on top of standard digital coherent receiver. Therefore, it is ideal for simple and cost-effective MFI in future heterogeneous optical networks.","keywords_author":["coherent detection","deep machine learning","Modulation format identification","Modulation format identification","coherent detection","deep machine learning"],"keywords_other":["Amplitude histograms","Heterogeneous optical networks","Optical signal to noise ratio","Constant modulus algorithms","Coherent detection","RECOGNITION","Modulation formats","Fiber nonlinearities","Deep neural networks"],"max_cite":18.0,"pub_year":2016.0,"sources":"['ieee', 'scp', 'wos']","rawkeys":["heterogeneous optical networks","fiber nonlinearities","deep machine learning","optical signal to noise ratio","recognition","deep neural networks","constant modulus algorithms","modulation format identification","modulation formats","coherent detection","amplitude histograms"],"tags":["heterogeneous optical networks","fiber nonlinearities","deep machine learning","optical signal to noise ratio","recognition","constant modulus algorithms","modulation format identification","modulation formats","coherent detection","convolutional neural network","amplitude histograms"]},{"p_id":26298,"title":"Abstract-concept learning of difference in pigeons","abstract":"\u00a9 2015, Springer-Verlag Berlin Heidelberg.Many species have demonstrated the capacity to learn abstract concepts. Recent studies have shown that the quantity of stimuli used during training plays a critical role in how subjects learn abstract concepts. As the number of stimuli available in the training set increases, so too does performance on novel combinations. The role of set size has been explored with learning the concept of matching and same\/different but not with learning the concept of difference. In the present study, pigeons were trained in a non-matching-to-sample task with an initial training set of three stimuli followed by transfer tests to novel stimuli. The training set was progressively doubled eight times with learning and transfer following each expansion. Transfer performance increased from chance level (50 %) at the smallest set size to a level equivalent to asymptotic training performance at the two largest training set sizes (384, 768). This progressive novel-stimulus transfer function of a non-matching (difference) rule is discussed in comparison with results from a similar experiment where pigeons were trained on a matching rule.","keywords_author":["Abstract-concept learning","Matching-to-sample","Non-matching","Oddity-from-sample","Set-size expansion"],"keywords_other":["Concept Formation","Pattern Recognition, Visual","Male","Animals","Transfer (Psychology)","Columbidae","Association Learning","Discrimination Learning"],"max_cite":7.0,"pub_year":2015.0,"sources":"['scp']","rawkeys":["transfer (psychology)","discrimination learning","male","columbidae","matching-to-sample","concept formation","abstract-concept learning","non-matching","pattern recognition","set-size expansion","oddity-from-sample","association learning","visual","animals"],"tags":["associative learning","recognition","male","columbidae","matching-to-sample","concept formation","abstract-concept learning","non-matching","set-size expansion","oddity-from-sample","discriminative learning","pattern recognition","visualization","animals"]},{"p_id":108224,"title":"Aircraft detection in remote sensing images based on saliency and convolution neural network","abstract":"New algorithms and architectures for the current industrial wireless sensor networks shall be explored to ensure the efficiency, robustness, and consistence in variable application environments which concern different issues, such as the smart grid, water supply, and gas monitoring. Object detection automatic in remote sensing images has always been a hot topic. Using the conventional deep convolution network based on region proposal for detection, there are many negative samples in the generated region proposal, which will affect the model detection precision and efficiency. Saliency uses the human visual attention mechanism to achieve the bottom-up object detection. Since replacing the selective search with saliency can greatly reduce the number of proposal areas, we will get some region of interests (RoIs) and their position information by using the saliency algorithm based on the background priori for the remote sensing image. And then, the position information is mapped to the feature vector of the whole image obtained by deep convolution neural network. Finally, the each RoI will be classified and fine-tuned bounding box. In this paper, our model is compared with Fast-RCNN that is the current state-of-the-art detection model. The mAP of our model reaches 99%, which is 12.4% higher than that of Fast-RCNN. In addition, we also study the effect of different iterations on model and find the model of 10,000 iterations already has a higher accuracy. Finally, we compare the results of different number of negative samples and find the detection accuracy is highest when the number of negative samples reaches 400.","keywords_author":["Remote sensing image","Detection","Saliency","Convolution neural network"],"keywords_other":["VISUAL-ATTENTION","TARGET DETECTION","CLOUD","FOURIER DESCRIPTORS","SYSTEMS","CNN","MODEL","RECOGNITION","REGION DETECTION","OBJECT DETECTION"],"max_cite":0.0,"pub_year":2018.0,"sources":"['wos']","rawkeys":["recognition","model","region detection","remote sensing image","convolution neural network","visual-attention","cnn","target detection","fourier descriptors","detection","cloud","object detection","saliency","systems"],"tags":["recognition","model","region detection","system","target detection","fourier descriptors","detection","cloud","convolutional neural network","object detection","remote sensing images","visual attention","saliency"]},{"p_id":108228,"title":"Distinct contributions of functional and deep neural network features to representational similarity of scenes in human brain and behavior","abstract":"Inherent correlations between visual and semantic features in real-world scenes make it difficult to determine how different scene properties contribute to neural representations. Here, we assessed the contributions of multiple properties to scene representation by partitioning the variance explained in human behavioral and brain measurements by three feature models whose inter-correlations were minimized a priori through stimulus preselection. Behavioral assessments of scene similarity reflected unique contributions from a functional feature model indicating potential actions in scenes as well as high-level visual features from a deep neural network (DNN). In contrast, similarity of cortical responses in scene-selective areas was uniquely explained by mid- and high-level DNN features only, while an object label model did not contribute uniquely to either domain. The striking dissociation between functional and DNN features in their contribution to behavioral and brain representations of scenes indicates that scene-selective cortex represents only a subset of behaviorally relevant scene information.","keywords_author":null,"keywords_other":["SELECTIVE CORTEX","PERCEPTION","COMPLEX","CONTEXT","RECOGNITION","REAL-WORLD SCENES","LEVEL VISUAL-CORTEX","PARAHIPPOCAMPAL PLACE AREA","REGIONS","CATEGORIES"],"max_cite":1.0,"pub_year":2018.0,"sources":"['wos']","rawkeys":["recognition","parahippocampal place area","selective cortex","categories","perception","level visual-cortex","real-world scenes","regions","context","complex"],"tags":["recognition","parahippocampal place area","selective cortex","complexity","categories","regions","level visual-cortex","perceptions","real-world scenes","context"]},{"p_id":108229,"title":"Optimization Methods for Large-Scale Machine Learning","abstract":"This paper provides a review and commentary on the past, present, and future of numerical optimization algorithms in the context of machine learning applications. Through case studies on text classification and the training of deep neural networks, we discuss how optimization problems arise in machine learning and what makes them challenging. A major theme of our study is that large-scale machine learning represents a distinctive setting in which the stochastic gradient (SG) method has traditionally played a central role while conventional gradient-based nonlinear optimization techniques typically falter. Based on this viewpoint, we present a comprehensive theory of a straightforward, yet versatile SG algorithm, discuss its practical behavior, and highlight opportunities for designing algorithms with improved performance. This leads to a discussion about the next generation of optimization methods for large-scale machine learning, including an investigation of two main streams of research on techniques that diminish noise in the stochastic directions and methods that make use of second-order derivative approximations.","keywords_author":["numerical optimization","machine learning","stochastic gradient methods","algorithm complexity analysis","noise reduction methods","second-order methods"],"keywords_other":["INFORMATION","QUASI-NEWTON METHODS","GRADIENT DESCENT","CONVERGENCE","INVERSE PROBLEMS","MINIMIZATION","RECOGNITION","BOUND-CONSTRAINED OPTIMIZATION","EM ALGORITHM","STOCHASTIC-APPROXIMATION"],"max_cite":0.0,"pub_year":2018.0,"sources":"['wos']","rawkeys":["convergence","stochastic-approximation","noise reduction methods","recognition","second-order methods","minimization","machine learning","stochastic gradient methods","bound-constrained optimization","inverse problems","numerical optimization","gradient descent","em algorithm","information","quasi-newton methods","algorithm complexity analysis"],"tags":["noise reduction methods","recognition","second-order methods","minimization","quasi-newton methods","machine learning","stochastic gradient methods","inverse problems","stochastic approximations","gradient descent","em algorithm","mathematics","information","numerical optimizations","bound constrained optimization","algorithm complexity analysis"]},{"p_id":9927,"title":"Group-aware deep feature learning for facial age estimation","abstract":"In this paper, we propose a group-aware deep feature learning (GA-DFL) approach for facial age estimation. Unlike most existing methods which utilize hand-crafted descriptors for face representation, our GA-DFL method learns a discriminative feature descriptor per image directly from raw pixels for face representation under the deep convolutional neural networks framework. Motivated by the fact that age labels are chronologically correlated and the facial aging datasets are usually lack of labeled data for each person in a long range of ages, we split ordinal ages into a set of discrete groups and learn deep feature transformations across age groups to project each face pair into the new feature space, where the intra-group variances of positive face pairs from the training set are minimized and the inter-group variances of negative face pairs are maximized, simultaneously. Moreover, we employ an overlapped coupled learning method to exploit the smoothness for adjacent age groups. To further enhance the discriminative capacity of face representation, we design a multi-path CNN approach to integrate the complementary information from multi-scale perspectives. Experimental results show that our approach achieves very competitive performance compared with most state-of-the-arts on three public face aging datasets that were captured under both controlled and uncontrolled environments.","keywords_author":["Biometrics","Deep learning","Facial age estimation","Feature learning","Facial age estimation","Deep learning","Feature learning","Biometrics"],"keywords_other":["Deep learning","REGRESSION","MANIFOLD","Feature transformations","NETS","FRAMEWORK","Competitive performance","Deep feature learning","RECOGNITION","Convolutional neural network","DIMENSIONALITY REDUCTION","Feature learning","Discriminative features","PATTERNS","Age estimation","IMAGES"],"max_cite":10.0,"pub_year":2017.0,"sources":"['ieee', 'scp', 'wos']","rawkeys":["competitive performance","recognition","images","deep learning","framework","patterns","age estimation","deep feature learning","discriminative features","biometrics","convolutional neural network","facial age estimation","feature learning","feature transformations","manifold","nets","regression","dimensionality reduction"],"tags":["competitive performance","recognition","images","framework","machine learning","patterns","age estimation","deep feature learning","manifolds","discriminative features","biometrics","convolutional neural network","facial age estimation","feature learning","feature transformations","nets","regression","dimensionality reduction"]},{"p_id":9929,"title":"Deep learning for plant identification using vein morphological patterns","abstract":"We propose using a deep convolutional neural network (CNN) for the problem of plant identification from leaf vein patterns. In particular, we consider classifying three different legume species: white bean, red bean and soybean. The introduction of a CNN avoids the use of handcrafted feature extractors as it is standard in state of the art pipeline. Furthermore, this deep learning approach significantly improves the accuracy of the referred pipeline. We also show that the reported accuracy is reached by increasing the model depth. Finally, by analyzing the resulting models with a simple visualization technique, we are able to unveil relevant vein patterns. (C) 2016 Elsevier B.V. All rights reserved.","keywords_author":["Automatic plant identification","Deep learning","Leaf vein image","Machine vision","Deep learning","Machine vision","Automatic plant identification","Leaf vein image"],"keywords_other":["Deep learning","FEATURES","State of the art","Visualization technique","NEURAL-NETWORKS","Leaf vein image","RECOGNITION","VENATION","Convolutional neural network","IMAGES","Morphological patterns","Feature extractor","Plant identification","CONVOLUTIONAL NETWORKS"],"max_cite":23.0,"pub_year":2016.0,"sources":"['scp', 'wos']","rawkeys":["neural-networks","recognition","images","features","deep learning","visualization technique","state of the art","convolutional networks","machine vision","automatic plant identification","leaf vein image","venation","convolutional neural network","morphological patterns","feature extractor","plant identification"],"tags":["leaf vein images","recognition","images","features","neural networks","visualization technique","state of the art","machine learning","machine vision","automatic plant identification","venation","convolutional neural network","morphological patterns","feature extractor","plant identification"]},{"p_id":108233,"title":"Sharpening of Hierarchical Visual Feature Representations of Blurred Images","abstract":"The robustness of the visual system lies in its ability to perceive degraded images. This is achieved through interacting bottom-up, recurrent, and top-down pathways that process the visual input in concordance with stored prior information. The interaction mechanism by which they integrate visual input and prior information is still enigmatic. We present a new approach using deep neural network (DNN) representation to reveal the effects of such integration on degraded visual inputs. We transformed measured human brain activity resulting from viewing blurred images to the hierarchical representation space derived from a feedforward DNN. Transformed representations were found to veer toward the original nonblurred image and away from the blurred stimulus image. This indicated deblurring or sharpening in the neural representation, and possibly in our perception. We anticipate these results will help unravel the interplay mechanism between bottom-up, recurrent, and top-down pathways, leading to more comprehensive models of vision.","keywords_author":["Decoding","Deep Neural Network","fMRI"],"keywords_other":["MECHANISMS","EXPECTATION","EXTRASTRIATE CORTEX","DIRECTED ATTENTION","RESPONSES","NEURAL-NETWORKS","RECOGNITION","TOP-DOWN FACILITATION","FMRI"],"max_cite":0.0,"pub_year":2018.0,"sources":"['wos']","rawkeys":["neural-networks","recognition","deep neural network","directed attention","mechanisms","extrastriate cortex","expectation","responses","fmri","decoding","top-down facilitation"],"tags":["recognition","directed attention","neural networks","mechanisms","extrastriate cortex","expectation","responses","fmri","convolutional neural network","decoding","top-down facilitation"]},{"p_id":108237,"title":"Source localization using deep neural networks in a shallow water environment","abstract":"Deep neural networks (DNNs) are advantageous for representing complex nonlinear relationships. This paper applies DNNs to source localization in a shallow water environment. Two methods are proposed to estimate the range and depth of a broadband source through different neural network architectures. The first adopts the classical two-stage scheme, in which feature extraction and DNN analysis are independent steps. The eigenvectors associated with the modal signal space are extracted as the input feature. Then, the time delay neural network is exploited to model the long term feature representation and constructs the regression model. The second concerns a convolutional neural network-feed-forward neural network (CNN-FNN) architecture, which trains the network directly by taking the raw multi-channel waveforms as input. The CNNs are expected to perform spatial filtering for multi-channel signals, in an operation analogous to time domain filters. The outputs of CNNs are summed as the input to FNN. Several experiments are conducted on the simulated and experimental data to evaluate the performance of the proposed methods. The results demonstrate that DNNs are effective for source localization in complex and varied water environments, especially when there is little precise environmental information. (C) 2018 Acoustical Society of America.","keywords_author":null,"keywords_other":["MATCHED-FIELD LOCALIZATION","CLASSIFICATION","ARRAY","PROCESSOR","MISMATCH","RECOGNITION","PROBABILISTIC APPROACH"],"max_cite":0.0,"pub_year":2018.0,"sources":"['wos']","rawkeys":["array","recognition","processor","mismatch","classification","probabilistic approach","matched-field localization"],"tags":["arrays","recognition","processor","mismatch","classification","matched-field localization","probabilistic approaches"]},{"p_id":9934,"title":"Transferring Pre-Trained Deep CNNs for Remote Scene Classification with General Features Learned from Linear PCA Network","abstract":"Deep convolutional neural networks (CNNs) have been widely used to obtain high-level representation in various computer vision tasks. However, in the field of remote sensing, there are not sufficient images to train a useful deep CNN. Instead, we tend to transfer successful pre-trained deep CNNs to remote sensing tasks. In the transferring process, generalization power of features in pre-trained deep CNNs plays the key role. In this paper, we propose two promising architectures to extract general features from pre-trained deep CNNs for remote scene classification. These two architectures suggest two directions for improvement. First, before the pre-trained deep CNNs, we design a linear PCA network (LPCANet) to synthesize spatial information of remote sensing images in each spectral channel. This design shortens the spatial distance of target and source datasets for pre-trained deep CNNs. Second, we introduce quaternion algebra to LPCANet, which further shortens the spectral distance between remote sensing images and images used to pre-train deep CNNs. With five well-known pre-trained deep CNNs, experimental results on three independent remote sensing datasets demonstrate that our proposed framework obtains state-of-the-art results without fine-tuning and feature fusing. This paper also provides baseline for transferring fresh pre-trained deep CNNs to other remote sensing tasks.","keywords_author":["convolutional neural network","remote scene classification","general feature","principle component analysis","deep learning"],"keywords_other":["SENSING IMAGERY","MODEL","IMAGE CLASSIFICATION","RECOGNITION","MECHANISM","FUSION"],"max_cite":4.0,"pub_year":2017.0,"sources":"['ieee', 'wos']","rawkeys":["recognition","model","remote scene classification","mechanism","deep learning","principle component analysis","convolutional neural network","fusion","image classification","general feature","sensing imagery"],"tags":["principal component analysis","recognition","model","remote scene classification","mechanisms","machine learning","sensed imagery","convolutional neural network","fusion","image classification","general feature"]},{"p_id":108238,"title":"A Real-Time Chinese Traffic Sign Detection Algorithm Based on Modified YOLOv2","abstract":"Traffic sign detection is an important task in traffic sign recognition systems. Chinese traffic signs have their unique features compared with traffic signs of other countries. Convolutional neural networks (CNNs) have achieved a breakthrough in computer vision tasks and made great success in traffic sign classification. In this paper, we present a Chinese traffic sign detection algorithm based on a deep convolutional network. To achieve real-time Chinese traffic sign detection, we propose an end-to-end convolutional network inspired by YOLOv2. In view of the characteristics of traffic signs, we take the multiple 1 x 1 convolutional layers in intermediate layers of the network and decrease the convolutional layers in top layers to reduce the computational complexity. For effectively detecting small traffic signs, we divide the input images into dense grids to obtain finer feature maps. Moreover, we expand the Chinese traffic sign dataset (CTSD) and improve the marker information, which is available online. All experimental results evaluated according to our expanded CTSD and German Traffic Sign Detection Benchmark (GTSDB) indicate that the proposed method is the faster and more robust. The fastest detection speed achieved was 0.017 s per image.","keywords_author":["object detection","CNNs","YOLOv2","Chinese traffic sign","CTSD","GTSDB"],"keywords_other":["NETWORKS","RECOGNITION"],"max_cite":0.0,"pub_year":2017.0,"sources":"['wos']","rawkeys":["cnns","recognition","gtsdb","ctsd","chinese traffic sign","networks","object detection","yolov2"],"tags":["recognition","gtsdb","yolo v2","ctsd","chinese traffic sign","networks","object detection","convolutional neural network"]},{"p_id":9936,"title":"NOISY IMAGE CLASSIFICATION USING HYBRID DEEP LEARNING METHODS","abstract":"In real-world scenario, image classification models degrade in performance as the images are corrupted with noise, while these models are trained with preprocessed data. Although deep neural networks (DNNs) are found efficient for image classification due to their deep layer-wise design to emulate latent features from data, they suffer from the same noise issue. Noise in image is common phenomena in real life scenarios and a number of studies have been conducted in the previous couple of decades with the intention to overcome the effect of noise in the image data. The aim of this study was to investigate the DNN-based better noisy image classification system. At first, the autoencoder (AE)-based denoising techniques were considered to reconstruct native image from the input noisy image. Then, convolutional neural network (CNN) is employed to classify the reconstructed image; as CNN was a prominent DNN method with the ability to preserve better representation of the internal structure of the image data. In the denoising step, a variety of existing AEs, named denoising autoencoder (DAE), convolutional denoising autoencoder (CDAE) and denoising variational autoencoder (DVAE) as well as two hybrid AEs (DAE-CDAE and DVAE-CDAE) were used. Therefore, this study considered five hybrid models for noisy image classification termed as: DAE-CNN, CDAE-CNN, DVAE-CNN, DAE-CDAE-CNN and DVAE-CDAE-CNN. The proposed hybrid classifiers were validated by experimenting over two benchmark datasets (i.e. MNIST and CIFAR-10) after corrupting them with noises of various proportions. These methods outperformed some of the existing eminent methods attaining satisfactory recognition accuracy even when the images were corrupted with 50% noise though these models were trained with 20% noise in the image. Among the proposed methods, DVAE-CDAE-CNN was found to be better than the others while classifying massive noisy images, and DVAE-CNN was the most appropriate for regular noise. The main significance of this work is the employment of the hybrid model with the complementary strengths of AEs and CNN in noisy image classification. AEs in the hybrid models enhanced the proficiency of CNN to classify highly noisy data even though trained with low level noise.","keywords_author":["Image denoising","CNN","denoising autoencoder","convolutional denoising autoencoder","variational denoising autoencoder","hybrid architecture"],"keywords_other":["CONVOLUTIONAL NEURAL-NETWORK","AUTOENCODERS","REPRESENTATIONS","RECOGNITION","SEGMENTATION","SPARSE"],"max_cite":0.0,"pub_year":2018.0,"sources":"['wos']","rawkeys":["recognition","segmentation","denoising autoencoder","cnn","autoencoders","image denoising","representations","sparse","variational denoising autoencoder","convolutional denoising autoencoder","convolutional neural-network","hybrid architecture"],"tags":["recognition","segmentation","denoising autoencoder","hybrid architectures","auto encoders","representation","image denoising","sparse","variational denoising autoencoder","convolutional neural network","convolutional denoising autoencoder"]},{"p_id":1750,"title":"Arabic handwritten characters recognition using Deep Belief Neural Networks","abstract":"In the handwriting recognition field, the deep learning is becoming the new trend thanks to their ability to deal with unlabeled raw data especially with the huge size of raw data available nowadays. In this paper, we investigate Deep Belief Neural Network (DBNN) for Arabic handwritten character\/word recognition. The proposed system takes the raw data as input and proceeds with a grasping layer-wise unsupervised learning algorithm. The approach was tested on two different databases. For the character level one, the results were promising with an error classification rate of 2.1% on the HACDB database. Unlike, the character level, the evaluation on the ADAB database to deal with word level shows an error rate which exceeds the 40%. Hence, the proposed DBNN structure is not already able to deal with high-level dimensional data and thus has to be improved.","keywords_author":["Arabic handwritten","DBNN","recognition","unsupervised training","DBNN","unsupervised training","Arabic handwritten","recognition"],"keywords_other":["Character recognition","ADAB database","error rate","DBNN structure","Arabic handwritten character\/word recognition","handwriting recognition field","unlabeled raw data","handwritten character recognition","Handwriting recognition","word level","Error classification","word processing","HACDB database","error statistics","DBNN","recognition","deep learning","Training","Character level","Databases","Shape","error classification rate","Arabic handwritten characters recognition","Error analysis","character level","Hand-written characters","grasping layer-wise unsupervised learning algorithm","high-level dimensional data","neural nets","Neurons","Arabic handwritten","deep belief neural networks","Unsupervised training"],"max_cite":7.0,"pub_year":2015.0,"sources":"['wos', 'scp', 'ieee']","rawkeys":["error rate","adab database","databases","dbnn","dbnn structure","error analysis","handwriting recognition field","unlabeled raw data","arabic handwritten character\/word recognition","handwritten character recognition","unsupervised training","arabic handwritten characters recognition","word level","word processing","error statistics","arabic handwritten","handwriting recognition","neurons","hand-written characters","recognition","deep learning","training","character recognition","error classification rate","character level","high-level dimensional data","grasping layer-wise unsupervised learning algorithm","neural nets","deep belief neural networks","shape","error classification","hacdb database"],"tags":["error rate","adab database","databases","dbnn structure","error analysis","handwriting recognition field","unlabeled raw data","arabic handwritten character\/word recognition","handwritten character recognition","unsupervised training","arabic handwritten characters recognition","word level","machine learning","word processing","error statistics","arabic handwritten","handwriting recognition","neurons","hand-written characters","recognition","neural networks","training","character recognition","error classification rate","character level","high-level dimensional data","grasping layer-wise unsupervised learning algorithm","deep belief neural networks","shape","error classification","hacdb database"]},{"p_id":1752,"title":"Hybrid Deep Learning for Face Verification","abstract":"This paper proposes a hybrid convolutional network (ConvNet)-Restricted Boltzmann Machine (RBM) model for face verification. A key contribution of this work is to learn high-level relational visual features with rich identity similarity information. The deep ConvNets in our model start by extracting local relational visual features from two face images in comparison, which are further processed through multiple layers to extract high-level and global relational features. To keep enough discriminative information, we use the last hidden layer neuron activations of the ConvNet as features for face verification instead of those of the output layer. To characterize face similarities from different aspects, we concatenate the features extracted from different face region pairs by different deep ConvNets. The resulting high-dimensional relational features are classified by an RBM for face verification. After pre-training each ConvNet and the RBM separately, the entire hybrid network is jointly optimized to further improve the accuracy. Various aspects of the ConvNet structures, relational features, and face verification classifiers are investigated. Our model achieves the state-of-the-art face verification performance on the challenging LFW dataset under both the unrestricted protocol and the setting when outside data is allowed to be used for training.","keywords_author":["Convolutional networks","deep learning","face recognition","Convolutional networks","deep learning","face recognition","Convolutional networks","deep learning","face recognition"],"keywords_other":["Sun","Similarity informations","DIMENSIONALITY","ALGORITHM","Face recognition","Computational modeling","Deep learning","High-dimensional","Convolutional networks","Feature extraction","Machine learning","Face","Relational features","Restricted boltzmann machine","RECOGNITION","WILD","Face Verification","Hidden layer neurons","Data models","IDENTITY","CLASSIFICATION","SPARSE REPRESENTATION"],"max_cite":14.0,"pub_year":2016.0,"sources":"['ieee', 'scp', 'wos']","rawkeys":["dimensionality","classification","machine learning","convolutional networks","face recognition","relational features","computational modeling","algorithm","recognition","deep learning","sparse representation","similarity informations","high-dimensional","face","sun","restricted boltzmann machine","wild","identity","face verification","data models","feature extraction","hidden layer neurons"],"tags":["dimensionality","classification","convolutional neural network","machine learning","face recognition","relational features","algorithms","computational modeling","recognition","sparse representation","similarity informations","high-dimensional","face","sun","restricted boltzmann machine","wild","identity","face verification","data models","feature extraction","hidden layer neurons"]},{"p_id":1755,"title":"Cross-domain and within-domain synaptic maintenance for autonomous development of visual areas","abstract":"Where-What Networks (WWNs) is a series of developmental networks for the recognition and attention of complex visual scenes. One of the most critical challenges of autonomous development is task non-specificity, namely, the network is meant to learn a variety of open-ended task skills without pre-defined tasks. Then how does a brain-like network develop skills for object relation that can generalize using implicit symbol-like rules? A preliminary scheme of uniform synaptic maintenance, which works across a neuron's sensory and motor domains, has been proposed in our WWN-9. In the new work here, we show that cross-domain and within-domain synaptic maintenance gains superior generalization than using the uniform synaptic maintenance scheme. This generalization enables the WWN to automatically discover symbol-like but implicit rules - detecting object groups from new combinations of object locations that were never observed. By \u201csymbol-like but implicit rules\u201d, we mean that the development program has no symbols and explicit rules, but symbol-like concepts (location, type) and implicit rule (two specific type objects must present concurrently - group) emerge as the firing patterns of the motor area and are used by the control. Moreover, the process of synaptic maintenance corresponds to the genesis (and adaptation) of cell connections and our model autonomously develops the Y area into two subarea, early area and later area, in charge of pattern recognition and symbolic reasoning respectively.","keywords_author":null,"keywords_other":["generalisation (artificial intelligence)","Neurons","inference mechanisms","symbolic reasoning","object recognition","generalization","Yttrium","WWN-9","Visualization","autonomous development","synaptic maintenance gain","Maintenance engineering","Brain modeling","Biological system modeling","pattern recognition","where-what network","uniform synaptic maintenance scheme"],"max_cite":null,"pub_year":2015.0,"sources":"['ieee']","rawkeys":["generalisation (artificial intelligence)","wwn-9","inference mechanisms","yttrium","symbolic reasoning","visualization","object recognition","generalization","biological system modeling","autonomous development","synaptic maintenance gain","brain modeling","pattern recognition","where-what network","neurons","maintenance engineering","uniform synaptic maintenance scheme"],"tags":["wwn-9","recognition","inference mechanisms","yttrium","symbolic reasoning","machine learning","visualization","object recognition","biological system modeling","where-what networks","autonomous development","synaptic maintenance gain","brain modeling","pattern recognition","neurons","maintenance engineering","uniform synaptic maintenance scheme"]},{"p_id":9956,"title":"Deep-Learning-Enabled On-Demand Design of Chiral Metamaterials","abstract":"Deep-learning framework has significantly impelled the development of modern machine learning technology by continuously pushing the limit of traditional recognition and processing of images, speech, and videos. In the meantime, it starts to penetrate other disciplines, such as biology, genetics, materials science, and physics. Here, we report a deep-learning-based model, comprising two bidirectional neural networks assembled by a partial stacking strategy, to automatically design and optimize three-dimensional chiral metamaterials with strong chiroptical responses at predesignated wavelengths. The model can help to discover the intricate, nonintuitive relationship between a metamaterial structure and its optical responses from a number of training examples, which circumvents the time-consuming, case-by-case numerical simulations in conventional metamaterial designs. This approach not only realizes the forward prediction of optical performance much more accurately and efficiently but also enables one to inversely retrieve designs from given requirements. Our results demonstrate that such a data-driven model can be applied as a very powerful tool in studying complicated light-matter interactions and accelerating the on-demand design of nanophotonic devices, systems, and architectures for real world applications.","keywords_author":["chirality","deep learning","metamaterial","neural network","on-demand design","deep learning","neural network","chirality","metamaterial","on-demand design"],"keywords_other":["SENSOR","Optical performance","SECONDARY STRUCTURE","Metamaterial structures","Systems and architectures","NEURAL-NETWORKS","Learning Based Models","Chiral metamaterials","Light-matter interactions","Nanophotonic devices","OPTICAL-RESPONSE","RECOGNITION","Bidirectional neural networks"],"max_cite":0.0,"pub_year":2018.0,"sources":"['scp', 'wos']","rawkeys":["neural-networks","chirality","neural network","chiral metamaterials","recognition","optical-response","deep learning","bidirectional neural networks","metamaterial structures","optical performance","sensor","light-matter interactions","secondary structure","metamaterial","systems and architectures","learning based models","on-demand design","nanophotonic devices"],"tags":["chirality","chiral metamaterials","recognition","light-matter interaction","optical-response","neural networks","bidirectional neural networks","metamaterial structures","optical performance","machine learning","sensors","secondary structure","metamaterial","systems and architectures","learning based models","on-demand design","nanophotonic devices"]},{"p_id":9965,"title":"Learning deep kernels in the space of dot product polynomials","abstract":"Recent literature has shown the merits of having deep representations in the context of neural networks. An emerging challenge in kernel learning is the definition of similar deep representations. In this paper, we propose a general methodology to define a hierarchy of base kernels with increasing expressiveness and combine them via multiple kernel learning (MKL) with the aim to generate overall deeper kernels. As a leading example, this methodology is applied to learning the kernel in the space of Dot-Product Polynomials (DPPs), that is a positive combination of homogeneous polynomial kernels (HPKs). We show theoretical properties about the expressiveness of HPKs that make their combination empirically very effective. This can also be seen as learning the coefficients of the Maclaurin expansion of any definite positive dot product kernel thus making our proposed method generally applicable. We empirically show the merits of our approach comparing the effectiveness of the kernel generated by our method against baseline kernels (including homogeneous and non homogeneous polynomials, RBF, etc...) and against another hierarchical approach on several benchmark datasets.","keywords_author":["Multiple kernel learning","Kernel learning","Deep kernel"],"keywords_other":["RECOGNITION"],"max_cite":1.0,"pub_year":2017.0,"sources":"['wos']","rawkeys":["multiple kernel learning","deep kernel","recognition","kernel learning"],"tags":["multiple kernel learning","deep kernel","recognition","kernel learning"]},{"p_id":9972,"title":"Local deep feature learning framework for 3D shape","abstract":"For 3D shape analysis, an effective and efficient feature is the key to popularize its applications in 3D domain. In this paper, we present a novel framework to learn and extract local deep feature (LDF), which encodes multiple low-level descriptors and provides high-discriminative representation of local region on 3D shape. The framework consists of four main steps. First, several basic descriptors are calculated and encapsulated to generate geometric bag-of-words in order to make full use of the various basic descriptors' properties. Then 3D mesh is down-sampled to hundreds of feature points for accelerating the model learning. Next, in order to preserve the local geometric information and establish the relationships among points in a local area, the geometric bag-of-words are encoded into local geodesic-aware bag-of-features (LGA-BoF). However, the resulting feature is redundant, which leads to low discriminative and efficiency. Therefore, in the final step, we use deep belief networks (DBNs) to learn a model, and use it to generate the LDF, which is high-discriminative and effective for 3D shape applications. 3D shape correspondence and symmetry detection experiments compared with related feature descriptors are carried out on several datasets and shape recognition is also conducted, validating the proposed local deep feature learning framework. (C) 2014 Elsevier Ltd. All rights reserved.","keywords_author":["Bag-of-words","Deep learning","Local deep feature","Shape correspondence","Shape recognition","Shape symmetry detection","Bag-of-words","Deep learning","Shape correspondence","Shape symmetry detection","Shape recognition","Local deep feature"],"keywords_other":["Deep learning","OBJECT RETRIEVAL","Local deep feature","Shape symmetry","RECOGNITION","Shape recognition","Shape correspondences","MODELS","Bag of words","DESCRIPTORS"],"max_cite":12.0,"pub_year":2015.0,"sources":"['scp', 'wos']","rawkeys":["bag of words","object retrieval","recognition","shape symmetry detection","deep learning","shape correspondences","local deep feature","shape correspondence","models","shape symmetry","descriptors","shape recognition","bag-of-words"],"tags":["bag of words","object retrieval","model","recognition","shape symmetry detection","machine learning","local deep feature","shape correspondence","shape symmetry","descriptors","shape recognition"]},{"p_id":59128,"title":"High-throughput time-stretch imaging flow cytometry for multi-class classification of phytoplankton","abstract":"Time-stretch imaging has been regarded as an attractive technique for high-throughput imaging flow cytometry primarily owing to its real-time, continuous ultrafast operation. Nevertheless, two key challenges remain: (1) sufficiently high time-stretch image resolution and contrast is needed for visualizing sub-cellular complexity of single cells, and (2) the ability to unravel the heterogeneity and complexity of the highly diverse population of cells - a central problem of single-cell analysis in life sciences - is required. We here demonstrate an optofluidic time-stretch imaging flow cytometer that enables these two features, in the context of high-throughput multi-class (up to 14 classes) phytoplantkton screening and classification. Based on the comprehensive feature extraction and selection procedures, we show that the intracellular texture\/morphology, which is revealed by high-resolution time-stretch imaging, plays a critical role of improving the accuracy of phytoplankton classification, as high as 94.7%, based on multi-class support vector machine (SVM). We also demonstrate that high-resolution time-stretch images, which allows exploitation of various feature domains, e.g. Fourier space, enables further sub-population identification - paving the way toward deeper learning and classification based on large-scale single-cell images. Not only applicable to biomedical diagnostic, this work is anticipated to find immediate applications in marine and biofuel research. (C) 2016 Optical Society of America","keywords_author":null,"keywords_other":["RECOGNITION"],"max_cite":9.0,"pub_year":2016.0,"sources":"['wos']","rawkeys":["recognition"],"tags":["recognition"]},{"p_id":9977,"title":"Improving galaxy morphologies for SDSS with Deep Learning","abstract":"We present a morphological catalogue for similar to 670 000 galaxies in the Sloan Digital Sky Survey in two flavours: T-type, related to the Hubble sequence, and Galaxy Zoo 2 (GZ2 hereafter) classification scheme. By combining accurate existing visual classification catalogues with machine learning, we provide the largest and most accurate morphological catalogue up to date. The classifications are obtained with Deep Learning algorithms using Convolutional Neural Networks (CNNs). We use two visual classification catalogues, GZ2 and Nair & Abraham (2010), for training CNNs with colour images in order to obtain T-types and a series of GZ2 type questions (disc\/features, edge-on galaxies, bar signature, bulge prominence, roundness, and mergers). We also provide an additional probability enabling a separation between pure elliptical (E) from SO, where the T-type model is not so efficient. For the T-type, our results show smaller offset and scatter than previous models trained with support vector machines. For the GZ2 type questions, our models have large accuracy (>97 per cent), precision and recall values (>90 per cent), when applied to a test sample with the same characteristics as the one used for training. The catalogue is publicly released with the paper.","keywords_author":["methods","observational catalogues galaxies","structure"],"keywords_other":["ZOO","DIGITAL SKY SURVEY","SEQUENCE","CLASSIFICATIONS","NEURAL-NETWORKS","CATALOG","DEPENDENCE","NEARBY","EVOLUTION","LIGHT PROFILE"],"max_cite":1.0,"pub_year":2018.0,"sources":"['wos']","rawkeys":["neural-networks","nearby","catalog","zoo","structure","methods","sequence","light profile","observational catalogues galaxies","evolution","dependence","digital sky survey","classifications"],"tags":["nearby","recognition","neural networks","zoo","connectivity","methods","sequence","classification","light profile","observational catalogues galaxies","biological","digital sky survey","catalog"]},{"p_id":9983,"title":"Wildfire: Approximate synchronization of parameters in distributed deep learning","abstract":"In distributed deep learning approaches, contributions to changes in the parameter values from multiple learners are gathered at periodic intervals and collectively used to update weights associated with the learning network. Gathering these contributions at a centralized location, as in common synchronous parameter server models, causes a bottleneck in two ways. First, the parameter server needs to wait until gradients from all learners have been received, and second, the traffic pattern of the gradients between the learners and the parameter server causes an imbalance in bandwidth utilization in most common networks. In this paper, we introduce a scheme called Wildfire, which communicates weights among subsets of parallel learners, each of which updates its model using only the information received from other learners in the subset. Different subsets of learners communicate at different times, allowing the learning to diffuse through the system. Wildfire reduces the communication overhead in deep learning by allowing learners to communicate directly among themselves rather than through a parameter server, and by limiting the time that learners need to wail before updating their models. We demonstrate the effectiveness of Wildfire on common deep learning benchmarks, using the IBM Rudra deep learning framework.","keywords_author":null,"keywords_other":["RECOGNITION"],"max_cite":1.0,"pub_year":2017.0,"sources":"['ieee', 'wos']","rawkeys":["recognition"],"tags":["recognition"]},{"p_id":67338,"title":"An Overview of Deep Generative Models","abstract":"As an important category of deep models, deep generative model has attracted more and more attention with the proposal of Deep Belief Networks (DBNs) and the fast greedy training algorithm based on restricted Boltzmann machines (RBMs). In the past few years, many different deep generative models are proposed and used in the area of Artificial Intelligence. In this paper, three important deep generative models including DBNs, deep autoencoder, and deep Boltzmann machine are reviewed. In addition, some successful applications of deep generative models in image processing, speech recognition and information retrieval are also introduced and analysed.","keywords_author":["Deep belief networks","Restricted boltzmann machine","Deep autoencoder","Deep generative model","Deep boltzmann machine"],"keywords_other":["RESTRICTED BOLTZMANN MACHINES","CONTRASTIVE DIVERGENCE","VISUAL-CORTEX","NEURAL-NETWORKS","RECOGNITION","BELIEF NETWORKS"],"max_cite":6.0,"pub_year":2015.0,"sources":"['ieee', 'wos']","rawkeys":["neural-networks","deep generative model","restricted boltzmann machines","recognition","visual-cortex","contrastive divergence","deep autoencoder","belief networks","deep boltzmann machine","deep belief networks","restricted boltzmann machine"],"tags":["deep generative model","recognition","visual-cortex","contrastive divergence","denoising autoencoder","neural networks","deep boltzmann machines","belief networks","deep belief networks","restricted boltzmann machine"]},{"p_id":10006,"title":"Imputation for transcription factor binding predictions based on deep learning","abstract":"Understanding the cell-specific binding patterns of transcription factors (TFs) is fundamental to studying gene regulatory networks in biological systems, for which ChIP-seq not only provides valuable data but is also considered as the gold standard. Despite tremendous efforts from the scientific community to conduct TF ChIP-seq experiments, the available data represent only a limited percentage of ChIP-seq experiments, considering all possible combinations of TFs and cell lines. In this study, we demonstrate a method for accurately predicting cell-specific TF binding for TF-cell line combinations based on only a small fraction (4%) of the combinations using available ChIP-seq data. The proposed model, termed TFImpute, is based on a deep neural network with a multi-task learning setting to borrow information across transcription factors and cell lines. Compared with existing methods, TFImpute achieves comparable accuracy on TF-cell line combinations with ChIP-seq data; moreover, TFImpute achieves better accuracy on TF-cell line combinations without ChIP-seq data. This approach can predict cell line specific enhancer activities in K562 and HepG2 cell lines, as measured by massively parallel reporter assays, and predicts the impact of SNPs on TF binding.","keywords_author":null,"keywords_other":["BIAS","REGULATORY MOTIFS","GENOME","IDENTIFICATION","SEQUENCE","REVEALS","DATABASE","RECOGNITION","ENHANCERS","PROTEIN-DNA BINDING"],"max_cite":9.0,"pub_year":2017.0,"sources":"['wos']","rawkeys":["identification","recognition","protein-dna binding","database","sequence","regulatory motifs","bias","enhancers","reveals","genome"],"tags":["identification","recognition","enhancement","databases","protein-dna binding","epidemiology","sequence","regulatory motifs","reveals","genomics"]},{"p_id":116510,"title":"A hybrid CNN feature model for pulmonary nodule malignancy risk differentiation","abstract":"The malignancy risk differentiation of pulmonary nodule is one of the most challenge tasks of computer-aided diagnosis (CADx). Most recently reported CADx methods or schemes based on texture and shape estimation have shown relatively satisfactory on differentiating the risk level of malignancy among the nodules detected in lung cancer screening. However, the existing CADx schemes tend to detect and analyze characteristics of pulmonary nodules from a statistical perspective according to local features only. Enlightened by the currently prevailing learning ability of convolutional neural network (CNN), which simulates human neural network for target recognition and our previously research on texture features, we present a hybrid model that takes into consideration of both global and local features for pulmonary nodule differentiation using the largest public database founded by the Lung Image Database Consortium and Image Database Resource Initiative (LIDC-IDRI). By comparing three types of CNN models in which two of them were newly proposed by us, we observed that the multi-channel CNN model yielded the best discrimination in capacity of differentiating malignancy risk of the nodules based on the projection of distributions of extracted features. Moreover, CADx scheme using the new multi-channel CNN model outperformed our previously developed CADx scheme using the 3D texture feature analysis method, which increased the computed area under a receiver operating characteristic curve (AUC) from 0.9441 to 0.9702.","keywords_author":["Convolutional neural network (CNN)","multi-channel CNN","texture","computer-aided diagnosis (CADx)","deep learning","pulmonary nodule differentiation"],"keywords_other":["MANAGEMENT","CLASSIFICATION","LUNG NODULES","LOCAL BINARY PATTERNS","HIGH-RESOLUTION CT","RECOGNITION","BENIGN","NEURAL-NETWORK","SCANS"],"max_cite":3.0,"pub_year":2017.0,"sources":"['ieee', 'wos']","rawkeys":["recognition","high-resolution ct","deep learning","pulmonary nodule differentiation","scans","lung nodules","convolutional neural network (cnn)","texture","classification","local binary patterns","neural-network","management","multi-channel cnn","benign","computer-aided diagnosis (cadx)"],"tags":["recognition","high-resolution ct","neural networks","computer-aided diagnosis","pulmonary nodule differentiation","scans","machine learning","lung nodules","texture","classification","convolutional neural network","local binary patterns","management","multi-channel cnn","benign"]},{"p_id":75561,"title":"Car Detection from Low-Altitude UAV Imagery with the Faster R-CNN","abstract":"UAV based trafficmonitoring holds distinct advantages over traditional traffic sensors, such as loop detectors, as UAVs have higher mobility, wider field of view, and less impact on the observed traffic. For traffic monitoring from UAV images, the essential but challenging task is vehicle detection. This paper extends the framework of Faster R-CNN for car detection from low-altitude UAV imagery captured over signalized intersections. Experimental results show that Faster R-CNN can achieve promising car detection results compared with othermethods. Our tests further demonstrate that Faster R-CNN is robust to illumination changes and cars' in-plane rotation. Besides, the detection speed of Faster R- CNN is insensitive to the detection load, that is, the number of detected cars in a frame; therefore, the detection speed is almost constant for each frame. In addition, our tests show that Faster R- CNN holds great potential for parking lot car detection. This paper tries to guide the readers to choose the best vehicle detection framework according to their applications. Future research will be focusing on expanding the current framework to detect other transportation modes such as buses, trucks, motorcycles, and bicycles.","keywords_author":null,"keywords_other":["CONVOLUTIONAL NEURAL-NETWORKS","SYSTEM","AIRBORNE VIDEO","RECOGNITION","VEHICLE DETECTION","OBJECT DETECTION"],"max_cite":0.0,"pub_year":2017.0,"sources":"['ieee', 'wos']","rawkeys":["recognition","system","vehicle detection","object detection","convolutional neural-networks","airborne video"],"tags":["recognition","system","vehicle detection","object detection","convolutional neural network","airborne video"]},{"p_id":116526,"title":"Integration of heterogeneous features for remote sensing scene classification","abstract":"Scene classification is one of the most important issues in remote sensing Q(RS) image processing. We find that features from different channels Q(shape, spectral, texture, etc.), levels Q(low-level and middle-level), or perspectives Q(local and global) could provide various properties for RS images, and then propose a heterogeneous feature framework to extract and integrate heterogeneous features with different types for RS scene classification. The proposed method is composed of three modules Q(1) heterogeneous features extraction, where three heterogeneous feature types, called DS-SURF-LLC, mean-Std-LLC, and MS-CLBP, are calculated, Q(2) heterogeneous features fusion, where the multiple kernel learning Q(MKL) is utilized to integrate the heterogeneous features, and Q(3) an MKL support vector machine classifier for RS scene classification. The proposed method is extensively evaluated on three challenging benchmark datasets Q(a 6-class dataset, a 12-class dataset, and a 21-class dataset), and the experimental results show that the proposed method leads to good classification performance. It produces good informative features to describe the RS image scenes. Moreover, the integration of heterogeneous features outperforms some state-of-the-art features on RS scene classification tasks. (C) 2018 Society of Photo-Optical Instrumentation Engineers (SPIE)","keywords_author":["remote sensing","scene classification","heterogeneous features","multiple kernel learning"],"keywords_other":["REPRESENTATION","FEATURE-SELECTION","MODEL","LOCAL BINARY PATTERNS","RECOGNITION","KEYPOINTS","SURF","SCALE","FUSION","IMAGES"],"max_cite":0.0,"pub_year":2018.0,"sources":"['wos']","rawkeys":["recognition","images","model","remote sensing","keypoints","representation","multiple kernel learning","scene classification","heterogeneous features","feature-selection","surf","local binary patterns","fusion","scale"],"tags":["recognition","images","model","remote sensing","keypoints","representation","multiple kernel learning","scene classification","heterogeneous features","surf","feature selection","local binary patterns","fusion","scale"]},{"p_id":10031,"title":"Applications of Deep Learning in Biological and Medical Data Analysis","abstract":"The rapid accumulation of biomedical data provided unprecedented opportunities for biology and clinical research, while it also made traditional data analysis technology face enormous challenges. In this paper, we reviewed recent studies on biomedical data using deep learning. We introduced several recommended deep learning models and summarized current applications of biological and medical data analysis using deep learning, including the general procedure, model construction and training process. Finally, we made a discussion on some issues in deep learning applications.","keywords_author":["deep learning","high-throughput omics","clinical medicine","data mining"],"keywords_other":["PREDICTION","SECONDARY STRUCTURE","FEATURES","FRAMEWORK","ARCHITECTURES","CLASSIFICATION","RECOGNITION","SEGMENTATION","NUCLEI","CONVOLUTIONAL NEURAL-NETWORKS"],"max_cite":3.0,"pub_year":2016.0,"sources":"['wos']","rawkeys":["data mining","recognition","architectures","features","deep learning","framework","segmentation","clinical medicine","prediction","classification","convolutional neural-networks","secondary structure","high-throughput omics","nuclei"],"tags":["data mining","recognition","segmentation","features","framework","clinical medicine","machine learning","prediction","classification","convolutional neural network","secondary structure","high-throughput omics","architecture","nuclei"]},{"p_id":42801,"title":"A scalable low-power reconfigurable accelerator for action-dependent heuristic dynamic programming","abstract":"\u00a9 2004-2012 IEEE. Adaptive dynamic programming (ADP) is an effective algorithm that has been successfully deployed in various control tasks. For many emerging applications where power consumption is a major design consideration, the conventional way of implementing ADP as software executing on a general-purpose processor is not sufficient. This paper proposes a scalable and low-power hardware architecture for implementing one of the most popular forms of ADP called action-dependent heuristic dynamic programming. Different from most machine-learning accelerators that mainly focus on the inference operation, the proposed architecture is also designed for energy-efficient learning, considering the highly iterative and interactive nature of the ADP algorithm. In addition, a virtual update technique is proposed to speed up the computation and to improve the energy efficiency of the accelerators. Two design examples are presented to demonstrate the proposed algorithm and architecture. Compared with the software approach running on a general-purpose processor, the accelerator operating at 175 MHz achieves 270 times improvement in computational time while consuming merely 25 mW power. Furthermore, it is demonstrated that the proposed virtual update algorithm can effectively boost the energy efficiency of the accelerator. Improvements up to 1.64 times are observed in the benchmark tasks employed.","keywords_author":["Action-dependent heuristic dynamic programming","Adaptive dynamic programming","Low-power accelerators","Machine learning","Neural networks","Adaptive dynamic programming","neural networks","low-power accelerators","action-dependent heuristic dynamic programming","machine learning"],"keywords_other":["FEEDBACK-CONTROL","REINFORCEMENT","Low Power","Algorithm design and analysis","Action dependent heuristic dynamic programming","ADAPTIVE CRITIC DESIGNS","Software algorithms","NEURAL-NETWORKS","Adaptive dynamic programming","INSECT","ARCHITECTURE"],"max_cite":0.0,"pub_year":2018.0,"sources":"['ieee', 'scp', 'wos']","rawkeys":["neural-networks","low-power accelerators","adaptive critic designs","action-dependent heuristic dynamic programming","neural networks","feedback-control","insect","machine learning","reinforcement","software algorithms","adaptive dynamic programming","algorithm design and analysis","action dependent heuristic dynamic programming","low power","architecture"],"tags":["low-power accelerators","recognition","neural networks","adc","insect","feedback control","machine learning","software algorithms","adaptive dynamic programming","algorithm design and analysis","action dependent heuristic dynamic programming","low power","architecture"]},{"p_id":75572,"title":"Cascaded face alignment via intimacy definition feature","abstract":"Recent years have witnessed the emerging popularity of regression-based face aligners, which directly learn mappings between facial appearance and shape-increment manifolds. We propose a randomforest based, cascaded regression model for face alignment by using a locally lightweight feature, namely intimacy definition feature. This feature is more discriminative than the pose-indexed feature, more efficient than the histogram of oriented gradients feature and the scale-invariant feature transform feature, and more compact than the local binary feature (LBF). Experimental validation of our algorithm shows that our approach achieves state-of-the-art performance when testing on some challenging datasets. Compared with the LBFbased algorithm, our method achieves about twice the speed, 20% improvement in terms of alignment accuracy and saves an order of magnitude on memory requirement. (C) 2017 SPIE and IS&T","keywords_author":["cascaded face alignment","random forest","intimacy definition feature"],"keywords_other":["RECOGNITION","LOCALIZATION","MODELS","REGRESSION"],"max_cite":0.0,"pub_year":2017.0,"sources":"['wos']","rawkeys":["intimacy definition feature","recognition","localization","models","cascaded face alignment","regression","random forest"],"tags":["intimacy definition feature","recognition","model","localization","random forests","cascaded face alignment","regression"]},{"p_id":10037,"title":"A complementary facial representation extracting method based on deep learning","abstract":"The identification and expression are two orthogonal properties of faces. But, few studies considered the two properties together. In this paper, the two properties are modeled in a unified framework. A pair of 18-layered Convolutional Deconvolutional Networks (Conv-Deconv) is proposed to learn a bidirectional mapping between the emotional expressions and the neutral expressions. One network extracts the complementary facial representations (i.e. identification representations and emotional representations) from emotional faces. The other network reconstructs the original faces from the extracted representations. Two networks are mutually inverse functions. Based on the framework, the networks are extended for various tasks, including face generation, face interpolation, facial expression recognition, and face verification. A new facial expression dataset called Large-scale Synthesized Facial Expression Dataset (LSFED) is presented. The dataset contains 105,000 emotional faces of 15,000 subjects synthesized by computer graphics program. Its distorted version (LSFED-D) is also presented to increase the difficulty and mimic real-world conditions. Good experiment results are obtained after evaluating our method on the synthesized clean LSFED dataset, the synthesized distorted LSFED-D dataset, and the real-world RaFD dataset. (C) 2018 Elsevier B.V. All rights reserved.","keywords_author":["Complementary facial representation","Deep learning","Facial expression","Complementary facial representation","Facial expression","Deep learning"],"keywords_other":["Orthogonal property","SALIENT","Facial expression recognition","Emotional expressions","Unified framework","RECOGNITION","Emotional representations","Facial Expressions","Complementary facial representation","OBJECT DETECTION","Bidirectional mapping"],"max_cite":0.0,"pub_year":2018.0,"sources":"['scp', 'wos']","rawkeys":["facial expression recognition","emotional representations","recognition","deep learning","emotional expressions","salient","facial expressions","bidirectional mapping","unified framework","object detection","orthogonal property","facial expression","complementary facial representation"],"tags":["facial expression recognition","recognition","emotion representation","salient","emotional expressions","facial expressions","bidirectional mapping","machine learning","unified framework","object detection","orthogonal property","complementary facial representation"]},{"p_id":10046,"title":"Deep Learning with Dynamic Spiking Neurons and Fixed Feedback Weights","abstract":"Recent work in computer science has shown the power of deep learning driven by the backpropagation algorithm in networks of artificial neurons. But real neurons in the brain are different from most of these artificial ones in at least three crucial ways: they emit spikes rather than graded outputs, their inputs and outputs are related dynamically rather than by piecewise-smooth functions, and they have no known way to coordinate arrays of synapses in separate forward and feedback pathways so that they change simultaneously and identically, as they do in backpropagation. Given these differences, it is unlikely that current deep learning algorithms can operate in the brain, but we that show these problems can be solved by two simple devices: learning rules can approximate dynamic input-output relations with piecewise-smooth functions, and a variation on the feedback alignment algorithm can train deep networks without having to coordinate forward and feedback synapses. Our results also show that deep spiking networks learn much better if each neuron computes an intracellular teaching signal that reflects that cell's nonlinearity. With this mechanism, networks of spiking neurons show useful learning in synapses at least nine layers upstream from the output cells and perform well compared to other spiking networks in the literature on the MNIST digit recognition task.","keywords_author":null,"keywords_other":["NEURAL-NETWORKS","RECOGNITION","NETS"],"max_cite":3.0,"pub_year":2017.0,"sources":"['ieee', 'wos']","rawkeys":["neural-networks","recognition","nets"],"tags":["neural networks","recognition","nets"]},{"p_id":10054,"title":"Deep convolutional learning for Content Based Image Retrieval","abstract":"In this paper we propose a model retraining method for learning more efficient convolutional representations for Content Based Image Retrieval. We employ a deep CNN model to obtain the feature representations from the activations of the convolutional layers using max-pooling, and subsequently we adapt and retrain the network, in order to produce more efficient compact image descriptors, which improve both the retrieval performance and the memory requirements, relying on the available information. Our method suggests three basic model retraining approaches. That is, the Fully Unsupervised Retraining, if no information except from the dataset itself is available, the Retraining with Relevance Information, if the labels of the training dataset are available, and the Relevance Feedback based Retraining, if feedback from users is available. The experimental evaluation on three publicly available image retrieval datasets indicates the effectiveness of the proposed method in learning more efficient representations for the retrieval task, outperforming other CNN-based retrieval techniques, as well as conventional hand-crafted feature-based approaches in all the used datasets. (C) 2017 Elsevier B.V. All rights reserved.","keywords_author":["Content Based Image Retrieval","Convolutional neural networks","Deep learning","Content Based Image Retrieval","Convolutional neural networks","Deep learning"],"keywords_other":["Retrieval techniques","Experimental evaluation","Feature based approaches","Feature representation","RECOGNITION","Convolutional neural network","Memory requirements","Retrieval performance","Content based image retrieval"],"max_cite":3.0,"pub_year":2018.0,"sources":"['ieee', 'scp', 'wos']","rawkeys":["retrieval performance","recognition","convolutional neural networks","experimental evaluation","deep learning","feature based approaches","memory requirements","retrieval techniques","content based image retrieval","convolutional neural network","feature representation"],"tags":["retrieval performance","recognition","experimental evaluation","feature based approaches","memory requirements","machine learning","retrieval techniques","convolutional neural network","feature representation","content-based image retrieval"]},{"p_id":1871,"title":"Representation Learning for Single-Channel Source Separation and Bandwidth Extension","abstract":"In this paper, we use deep representation learning for model-based single-channel source separation (SCSS) and artificial bandwidth extension (ABE). Both tasks are ill-posed and source-specific prior knowledge is required. In addition to well-known generative models such as restricted Boltzmann machines and higher order contractive autoencoders two recently introduced deep models, namely generative stochastic networks (GSNs) and sum-product networks (SPNs), are used for learning spectrogram representations. For SCSS we evaluate the deep architectures on data of the 2 nd CHiME speech separation challenge and provide results for a speaker dependent, a speaker independent, a matched noise condition and an unmatched noise condition task. GSNs obtain the best PESQ and overall perceptual score on average in all four tasks. Similarly, frame-wise GSNs are able to reconstruct the missing frequency bands in ABE best, measured in frequency-domain segmental SNR. They outperform SPNs embedded in hidden Markov models and the other representation models significantly.","keywords_author":["Bandwidth extension","deep neural networks (DNNs)","generative stochastic networks","representation learning","single-channel source separation (SCSS)","sum-product networks","Bandwidth extension","deep neural networks (DNNs)","generative stochastic networks","representation learning","single-channel source separation (SCSS)","sum-product networks"],"keywords_other":["REGRESSION","frequency-domain segmental SNR","speaker dependent","Bandwidth","ALGORITHM","generative stochastic networks","Speech processing","Neural networks","Stochastic processes","ABE","SCSS","higher order contractive autoencoders","GSN","missing frequency band reconstruction","artificial bandwidth extension","matched noise condition","sumproduct networks","speaker independent","HIDDEN MARKOV-MODELS","2ndCHiME speech separation challenge","NONNEGATIVE MATRIX FACTORIZATION","hidden Markov models","learning (artificial intelligence)","Learning systems","SIGNAL","ill-posed prior knowledge","unmatched noise condition task","SPEAKER ADAPTATION","RECOGNITION","Hidden Markov models","deep representation learning","source-specific prior knowledge","generative models","model-based single-channel source separation","restricted Boltzmann machines","frame-wise GSN","Data models","Adaptation models","source separation","overall perceptual score","INFORMATION","Spectrogram","SPN","SUPERVISED SPEECH SEPARATION","PESQ","speaker recognition","signal denoising","Boltzmann machines","DEEP NEURAL-NETWORKS"],"max_cite":8.0,"pub_year":2015.0,"sources":"['wos', 'ieee']","rawkeys":["speaker dependent","nonnegative matrix factorization","2ndchime speech separation challenge","adaptation models","deep neural-networks","generative stochastic networks","scss","frame-wise gsn","learning systems","hidden markov-models","bandwidth","bandwidth extension","pesq","higher order contractive autoencoders","speaker adaptation","deep neural networks (dnns)","missing frequency band reconstruction","artificial bandwidth extension","regression","matched noise condition","algorithm","speaker independent","sumproduct networks","learning (artificial intelligence)","recognition","neural networks","spectrogram","boltzmann machines","ill-posed prior knowledge","stochastic processes","supervised speech separation","unmatched noise condition task","representation learning","deep representation learning","source-specific prior knowledge","single-channel source separation (scss)","generative models","model-based single-channel source separation","signal","hidden markov models","abe","restricted boltzmann machines","source separation","overall perceptual score","spn","gsn","sum-product networks","speech processing","speaker recognition","signal denoising","data models","information","frequency-domain segmental snr"],"tags":["nonnegative matrix factorization","2ndchime speech separation challenge","adaptation models","generative stochastic networks","frame-wise gsn","convolutional neural network","learning systems","bandwidth","bandwidth extension","pesq","single channel source separations","speaker independents","higher order contractive autoencoders","machine learning","speaker adaptation","sensor pattern noise","missing frequency band reconstruction","algorithms","artificial bandwidth extension","regression","matched noise condition","generative model","recognition","neural networks","boltzmann machines","ill-posed prior knowledge","stochastic processes","supervised speech separation","unmatched noise condition task","representation learning","deep representation learning","source-specific prior knowledge","restricted boltzmann machine","model-based single-channel source separation","signals","hidden markov models","abe","source separation","overall perceptual score","sum-product networks","gsn","speech processing","speaker recognition","signal denoising","data models","information","speaker dependents","frequency-domain segmental snr","spectrograms"]},{"p_id":10063,"title":"MiRTDL: A Deep Learning Approach for miRNA Target Prediction","abstract":"MicroRNAs (miRNAs) regulate genes that are associated with various diseases. To better understand miRNAs, the miRNA regulatory mechanism needs to be investigated and the real targets identified. Here, we present miRTDL, a new miRNA target prediction algorithm based on convolutional neural network (CNN). The CNN automatically extracts essential information from the input data rather than completely relying on the input dataset generated artificially when the precise miRNA target mechanisms are poorly known. In this work, the constraint relaxing method is first used to construct a balanced training dataset to avoid inaccurate predictions caused by the existing unbalanced dataset. The miRTDL is then applied to 1,606 experimentally validated miRNA target pairs. Finally, the results show that our miRTDL outperforms the existing target prediction algorithms and achieves significantly higher sensitivity, specificity and accuracy of 88.43, 96.44, and 89.98 percent, respectively. We also investigate the miRNA target mechanism, and the results show that the complementation features are more important than the others.","keywords_author":["Constraint relaxation","convolutional neural network","miRNA","target prediction"],"keywords_other":["SUPPORT VECTOR MACHINE","IDENTIFICATION","ACCESSIBILITY","RECOGNITION","MICRORNA TARGETS"],"max_cite":4.0,"pub_year":2016.0,"sources":"['wos']","rawkeys":["identification","recognition","accessibility","target prediction","convolutional neural network","mirna","support vector machine","constraint relaxation","microrna targets"],"tags":["identification","recognition","machine learning","accessibility","target prediction","convolutional neural network","micrornas","constraint relaxation","microrna targets"]},{"p_id":51025,"title":"Personality mining from biographical data with the \"adjectival Marker \" technique","abstract":"The last decade has witnessed significant work in personality mining from lexical cues in social media data. Not much work has yet been undertaken in extracting these lexical cues from biographical data populating social media. Most of this work involves a large crowd of researchers leveraging dictionary-based approaches such as LIWC (which primarily focus on function words). By means of this paper we intend to introduce a novel method of personality mining from social media data called \"Adjectival-marker Technique\". This method involves extracting lexical features from descriptive texts (e.g. biographical data) to train a learning model, so as to predict the respective personality traits of the subject. Conceptually, it draws heavily from the last 78 years of work in lexical psychology and the Big Five personality test. However, it is not only a computational variant of the primordial theories of lexical psychology, but is also competent in conferring a substantial accuracy of personality prediction, matching that obtained by psychometric tests. In this study, we propose a variant of the Lexical Hypothesis from psychology. This modified hypothesis is validated by the computational results of personality prediction achieved by the Adjectival Marker Technique discussed below. The paper also discusses some insights illustrating the coherence of people's judgments about the subject's personality (virtual personality). The average accuracy (i.e. matching that achieved by psychometric tests for Big 5) for prediction approximated to Extraversion - 82.82% Agreeableness - 89.62%, Conscientiousness - 92.48% and Imaginativeness\/Intellect - 81.67%.","keywords_author":["Machine Learning","Natural Language Processing","Psychology","Social Computing","User Personality Determination"],"keywords_other":["Social computing","Social media datum","Personality minings","Psychology","Personality predictions","User personalities","NAtural language processing","Computational results"],"max_cite":0.0,"pub_year":2015.0,"sources":"['scp']","rawkeys":["social media datum","user personality determination","natural language processing","machine learning","social computing","computational results","personality predictions","user personalities","personality minings","psychology"],"tags":["recognition","social media datum","user personality determination","natural language processing","machine learning","social computing","computational results","personality predictions","user personalities","personality minings"]},{"p_id":59219,"title":"Predicting molecular properties with covariant compositional networks","abstract":"Density functional theory (DFT) is the most successful and widely used approach for computing the electronic structure of matter. However, for tasks involving large sets of candidate molecules, running DFT separately for every possible compound of interest is forbiddingly expensive. In this paper, we propose a neural network based machine learning algorithm which, assuming a sufficiently large training sample of actual DFT results, can instead learn to predict certain properties of molecules purely from their molecular graphs. Our algorithm is based on the recently proposed covariant compositional networks framework and involves tensor reduction operations that are covariant with respect to permutations of the atoms. This new approach avoids some of the representational limitations of other neural networks that are popular in learning from molecular graphs and yields promising results in numerical experiments on the Harvard Clean Energy Project and QM9 molecular datasets. Published by AIP Publishing.","keywords_author":null,"keywords_other":["GRAPH KERNELS","RECOGNITION","PICTORIAL STRUCTURES","DESIGN"],"max_cite":1.0,"pub_year":2018.0,"sources":"['wos']","rawkeys":["design","recognition","pictorial structures","graph kernels"],"tags":["design","recognition","pictorial structures","graph kernels"]},{"p_id":10068,"title":"Classification of affective states via EEG and deep learning","abstract":"Human emotions play a key role in numerous decision-making processes. The ability to correctly identify likes and dislikes as well as excitement and boredom would facilitate novel applications in neuromarketing, affective entertainment, virtual rehabilitation and forensic neuroscience that leverage on sub-conscious human affective states. In this neuroinformatics investigation, we seek to recognize human preferences and excitement passively through the use of electroencephalography (EEG) when a subject is presented with some 3D visual stimuli. Our approach employs the use of machine learning in the form of deep neural networks to classify brain signals acquired using a brain-computer interface (BCI). In the first part of our study, we attempt to improve upon our previous work, which has shown that EEG preference classification is possible although accuracy rates remain relatively low at 61%-67% using conventional deep learning neural architectures, where the challenge mainly lies in the accurate classification of unseen data from a cohort-wide sample that introduces inter-subject variability on top of the existing intra-subject variability. Such an approach is significantly more challenging and is known as subjectindependent EEG classification as opposed to the more commonly adopted but more time-consuming and less general approach of subject-dependent EEG classification. In this new study, we employ deep networks that allow dropouts to occur in the architecture of the neural network. The results obtained through this simple feature modification achieved a classification accuracy of up to 79%. Therefore, this study has shown that the use of a deep learning classifier was able to achieve an increase in emotion classification accuracy of between 13% and 18% through the simple adoption of the use of dropouts compared to a conventional deep learner for EEG preference classification. In the second part of our study, users are exposed to a roller-coaster experience as the emotional stimuli which are expected to evoke the emotion of excitement, while simultaneously wearing virtual reality goggles, which delivers the virtual reality experience of excitement, and an EEG headset, acquires the raw brain signals detected when exposed to this excitement stimuli. Here, a deep learning approach is used to improve the excitement detection rate to well above the 90% accuracy level. In a prior similar study, the use of conventional machine learning approaches involving k-Nearest Neighbour (kNN) classifiers and Support Vector Machines (SVM) only achieved prediction accuracy rates of between 65% and 89%. Using a deep learning approach here, rates of 78%-96% were achieved. This demonstrates the superiority of adopting a deep learning approach over other machine learning approaches for detecting human excitement when immersed in an immersive virtual reality environment.","keywords_author":["Deep learning","Dropouts","Electroencephalography (EEG)","Emotion classification","Excitement classification","Neuroinformatics","Preference classification","Virtual reality","Neuroinformatics","emotion classification","preference classification","excitement classification","electroencephalography (EEG)","deep learning","virtual reality","dropouts"],"keywords_other":["MUSIC PREFERENCE","RESPONSES","RECOGNITION","RHYTHM","STIMULI","SHAPES"],"max_cite":0.0,"pub_year":2018.0,"sources":"['scp', 'wos']","rawkeys":["rhythm","neuroinformatics","recognition","emotion classification","music preference","preference classification","deep learning","virtual reality","dropouts","responses","shapes","electroencephalography (eeg)","excitement classification","stimuli"],"tags":["rhythm","dropout","neuroinformatics","recognition","emotion classification","music preference","preference classification","virtual reality","eeg","machine learning","responses","shape","excitement classification","stimuli"]},{"p_id":18264,"title":"Learning a transferable change rule from a recurrent neural network for land cover change detection","abstract":"When exploited in remote sensing analysis, a reliable change rule with transfer ability can detect changes accurately and be applied widely. However, in practice, the complexity of land cover changes makes it difficult to use only one change rule or change feature learned from a given multi-temporal dataset to detect any other new target images without applying other learning processes. In this study, we consider the design of an efficient change rule having transferability to detect both binary and multi-class changes. The proposed method relies on an improved Long Short-Term Memory (LSTM) model to acquire and record the change information of long-term sequence remote sensing data. In particular, a core memory cell is utilized to learn the change rule from the information concerning binary changes or multi-class changes. Three gates are utilized to control the input, output and update of the LSTM model for optimization. In addition, the learned rule can be applied to detect changes and transfer the change rule from one learned image to another new target multi-temporal image. In this study, binary experiments, transfer experiments and multi-class change experiments are exploited to demonstrate the superiority of our method. Three contributions of this work can be summarized as follows: (1) the proposed method can learn an effective change rule to provide reliable change information for multi-temporal images; (2) the learned change rule has good transferability for detecting changes in new target images without any extra learning process, and the new target images should have a multi-spectral distribution similar to that of the training images; and (3) to the authors' best knowledge, this is the first time that deep learning in recurrent neural networks is exploited for change detection. In addition, under the framework of the proposed method, changes can be detected under both binary detection and multi-class change detection.","keywords_author":["Change detection","LSTMmodel","Recurrent neural network","Transferability;multi-spectral image","change detection","LSTM model","transferability","multi-spectral image","recurrent neural network"],"keywords_other":["LSTMmodel","Multi-temporal image","Remote sensing data","TIME-SERIES","UNSUPERVISED CHANGE DETECTION","SATELLITE DATA","HYPERSPECTRAL IMAGES","Change detection","CLASSIFICATION","MODEL","RECOGNITION","Remote sensing analysis","Long short term memory","REMOTELY-SENSED DATA","Multispectral images","Information concerning"],"max_cite":25.0,"pub_year":2016.0,"sources":"['ieee', 'scp', 'wos']","rawkeys":["remote sensing data","time-series","change detection","information concerning","classification","transferability;multi-spectral image","hyperspectral images","transferability","lstm model","multi-spectral image","lstmmodel","recognition","multispectral images","remotely-sensed data","recurrent neural network","model","remote sensing analysis","satellite data","unsupervised change detection","multi-temporal image","long short term memory"],"tags":["remote sensing data","recognition","model","contrastive divergence","multi-spectral imaging","multispectral images","neural networks","long short-term memory","remote sensing analysis","lstm model","satellite data","unsupervised change detection","hyperspectral imaging","information concerning","classification","multi-temporal image","transferability;multi-spectral image","time series"]},{"p_id":34652,"title":"Intelligent video surveillance beyond robust background modeling","abstract":"\u00a9 2017 Elsevier LtdThe increasing number of video surveillance cameras is challenging video control systems. Monitoring centers require tools to guide the process of supervision. Different video analysis methods have effectively met the main requirements from the industry of perimeter protection. High accuracy detection systems are able to process real time video on affordable hardware. However some problematic environments cause a massive number of false alerts. Many approaches in the literature do not consider this kind of environments while others use metrics that dilute their impact on results. An intelligent video solution for perimeter protection must select and show the cameras which are more likely witnessing a relevant event but systems based only on background modeling tend to give importance to problematic situations no matter if an intrusion is taking place or not. We propose to add a module based on machine learning and global features, bringing adaptability to the video surveillance solution, so that problematic situations can be recognized and given the right priority. Tests with thousands of hours of video show how good an intruder detector can perform but also how a simple fault in a camera can flood a monitoring center with alerts. The new proposal is able to learn and recognize events such that alerts from problematic environments can be properly handled.","keywords_author":["Event","Global features","Intrusion detection","Machine learning","Recognition","Video","Video surveillance"],"keywords_other":["Recognition","Event","Video","Global feature","Video surveillance"],"max_cite":2.0,"pub_year":2018.0,"sources":"['scp']","rawkeys":["recognition","intrusion detection","global features","machine learning","video","global feature","event","video surveillance"],"tags":["recognition","events","intrusion detection systems","machine learning","video","global feature","video surveillance"]},{"p_id":10079,"title":"Deep patch learning for weakly supervised object classification and discovery","abstract":"Patch-level image representation is very important for object classification and detection, since it is robust to spatial transformation, scale variation, and cluttered background. Many existing methods usually require fine-grained supervisions (e.g., bounding-box annotations) to learn patch features, which requires a great effort to label images may limit their potential applications. In this paper, we propose to learn patch features via weak supervisions, i.e., only image-level supervisions. To achieve this goal, we treat images as bags and patches as instances to integrate the weakly supervised multiple instance learning constraints into deep neural networks. Also, our method integrates the traditional multiple stages of weakly supervised object classification and discovery into a unified deep convolutional neural network and optimizes the network in an end-to-end way. The network processes the two tasks object classification and discovery jointly, and shares hierarchical deep features. Through this jointly learning strategy, weakly supervised object classification and discovery are beneficial to each other. We test the proposed method on the challenging PASCAL VOC datasets. The results show that our method can obtain state-of-the-art performance on object classification, and very competitive results on object discovery, with faster testing speed than competitors. (C) 2017 Elsevier Ltd. All rights reserved.","keywords_author":["Patch feature learning","Multiple instance learning","Weakly supervised learning","Convolutional neural network","End-to-end","Object classification","Object discovery"],"keywords_other":["NETWORKS","IMAGE CLASSIFICATION","RECOGNITION","LOCALIZATION"],"max_cite":2.0,"pub_year":2017.0,"sources":"['ieee', 'scp', 'wos']","rawkeys":["object discovery","recognition","localization","patch feature learning","end-to-end","networks","object classification","convolutional neural network","multiple instance learning","image classification","weakly supervised learning"],"tags":["object discovery","recognition","localization","end to end","patch feature learning","networks","object classification","convolutional neural network","multiple instance learning","image classification","weakly supervised learning"]},{"p_id":1890,"title":"Learning Hierarchical Spectral-Spatial Features for Hyperspectral Image Classification","abstract":"This paper proposes a spectral-spatial feature learning (SSFL) method to obtain robust features of hyperspectral images (HSIs). It combines the spectral feature learning and spatial feature learning in a hierarchical fashion. Stacking a set of SSFL units, a deep hierarchical model called the spectral-spatial networks (SSN) is further proposed for HSI classification. SSN can exploit both discriminative spectral and spatial information simultaneously. Specifically, SSN learns useful high-level features by alternating between spectral and spatial feature learning operations. Then, kernel-based extreme learning machine (KELM), a shallow neural network, is embedded in SSN to classify image pixels. Extensive experiments are performed on two benchmark HSI datasets to verify the effectiveness of SSN. Compared with state-of-the-art methods, SSN with a deep hierarchical architecture obtains higher classification accuracy in terms of the overall accuracy, average accuracy, and kappa (\u03ba) coefficient of agreement, especially when the number of the training samples is small.","keywords_author":["Hierarchical learning","hyperspectral image classification","kernel-based extreme learning machine","spectral-spatial feature","Hierarchical learning","hyperspectral image classification","kernel-based extreme learning machine","spectral-spatial feature"],"keywords_other":["SSN","HSI","spectral\u2013spatial feature","image classification","BELIEF NETWORKS","MORPHOLOGICAL PROFILES","shallow neural network","DEEP","Feature extraction","MACHINE","kappa coefficient","kernel-based extreme learning machine","SSFL method","Machine learning","hyperspectral imaging","UNIVERSAL APPROXIMATORS","image pixel classification","Hyperspectral sensors","MARKOV RANDOM-FIELDS","learning (artificial intelligence)","Training","Learning systems","TIME-SERIES PREDICTION","RECOGNITION","HSI datasets","deep hierarchical model","neural nets","FEATURE-EXTRACTION","deep hierarchical architecture","Hierarchical learning","hierarchical spectral-spatial feature learning","Accuracy","hyperspectral image classification","NEURAL-NETWORKS","feature extraction","spectral-spatial networks","KELM","Support vector machines"],"max_cite":19.0,"pub_year":2016.0,"sources":"['wos', 'ieee']","rawkeys":["deep","kelm","learning systems","spectral\u2013spatial feature","image classification","shallow neural network","neural-networks","machine","kappa coefficient","machine learning","morphological profiles","kernel-based extreme learning machine","hyperspectral imaging","universal approximators","time-series prediction","image pixel classification","hierarchical learning","spectral-spatial feature","learning (artificial intelligence)","recognition","feature-extraction","ssfl method","training","hsi datasets","deep hierarchical model","belief networks","neural nets","deep hierarchical architecture","accuracy","hierarchical spectral-spatial feature learning","ssn","hyperspectral image classification","hyperspectral sensors","hsi","markov random-fields","feature extraction","support vector machines","spectral-spatial networks"],"tags":["markov random fields","deep","kelm","learning systems","spectral\u2013spatial feature","image classification","shallow neural network","machine","kappa coefficient","machine learning","morphological profiles","kernel-based extreme learning machine","hyperspectral imaging","universal approximators","image pixel classification","hierarchical learning","spectral-spatial feature","recognition","time series prediction","neural networks","ssfl method","training","hsi datasets","deep hierarchical model","belief networks","deep hierarchical architecture","accuracy","hierarchical spectral-spatial feature learning","ssn","hyperspectral image classification","hyperspectral sensors","feature extraction","spectral-spatial networks"]},{"p_id":10086,"title":"Multimodal integration learning of robot behavior using deep neural networks","abstract":"For humans to accurately understand the world around them, multimodal integration is essential because it enhances perceptual precision and reduces ambiguity. Computational models replicating such human ability may contribute to the practical use of robots in daily human living environments; however, primarily because of scalability problems that conventional machine learning algorithms suffer from, sensory-motor information processing in robotic applications has typically been achieved via modal-dependent processes. In this paper, we propose a novel computational framework enabling the integration of sensory-motor time-series data and the self-organization of multimodal fused representations based on a deep learning approach. To evaluate our proposed model, we conducted two behavior-learning experiments utilizing a humanoid robot; the experiments consisted of object manipulation and bell-ringing tasks. From our experimental results, we show that large amounts of sensory-motor information, including raw RGB images, sound spectrums, and joint angles, are directly fused to generate higher-level multimodal representations. Further, we demonstrated that our proposed framework realizes the following three functions: (1) cross-modal memory retrieval utilizing the information complementation capability of the deep autoencoder; (2) noise-robust behavior recognition utilizing the generalization capability of multimodal features; and (3) multimodal causality acquisition and sensory-motor prediction based on the acquired causality. (C) 2014 The Authors. Published by Elsevier B.V.","keywords_author":["Cross-modal memory retrieval","Deep learning","Multimodal integration","Object manipulation","Object manipulation","Multimodal integration","Cross-modal memory retrieval","Deep learning"],"keywords_other":["Deep learning","Generalization capability","NEUROLOGICAL DAMAGE","Object manipulation","Robotic applications","MEMORY-IMPAIRED INDIVIDUALS","RECOGNITION","Memory retrieval","Conventional machines","Computational framework","Multimodal integration"],"max_cite":33.0,"pub_year":2014.0,"sources":"['ieee', 'scp', 'wos']","rawkeys":["object manipulation","recognition","generalization capability","deep learning","neurological damage","conventional machines","computational framework","memory retrieval","robotic applications","cross-modal memory retrieval","multimodal integration","memory-impaired individuals"],"tags":["object manipulation","recognition","generalization capability","neurological damage","machine learning","conventional machines","computational framework","memory retrieval","robotic applications","cross-modal memory retrieval","multimodal integration","memory-impaired individuals"]},{"p_id":10096,"title":"Multi-modal deep feature learning for RGB-D object detection","abstract":"We present a novel multi-modal deep feature learning architecture for RGB-D object detection. The current paradigm for object detection typically consists of two stages: objectness estimation and region-wise object recognition. Most existing RGB-D object detection approaches treat the two stages separately by extracting RGB and depth features individually, thus ignore the correlated relationship between these two modalities. In contrast, our proposed method is designed to take full advantages of both depth and color cues by exploiting both modality-correlated and modality-specific features and jointly performing RGBD objectness estimation and region-wise object recognition. Specifically, shared weights strategy and a parameter-free correlation layer are exploited to carry out RGB-D-correlated objectness estimation and region-wise recognition in conjunction with RGB-specific and depth-specific procedures. The parameters of these three networks are simultaneously optimized via end-to-end multi-task learning. The multi modal RGB-D objectness estimation results and RGB-D object recognition results are both boosted by late-fusion ensemble. To validate the effectiveness of the proposed approach, we conduct extensive experiments on two challenging RGB-D benchmark datasets, NYU Depth v2 and SUN RGB-D. The experimental results show that by introducing the modality-correlated feature representation, the proposed multi-modal RGB-D object detection approach is substantially superior to the state-of-the-art competitors. Moreover, compared to the expensive deep architecture (VGG16) that the state-of-the-art methods preferred, our approach, which is built upon more lightweight deep architecture (AlexNet), performs slightly better. (C) 2017 Elsevier Ltd. All rights reserved.","keywords_author":["RGB-D objectness estimation","RGB-D object detection","Multi-modal learning","Convolutional neural networks"],"keywords_other":["CONTEXT","RECOGNITION"],"max_cite":1.0,"pub_year":2017.0,"sources":"['wos']","rawkeys":["multi-modal learning","recognition","convolutional neural networks","context","rgb-d objectness estimation","rgb-d object detection"],"tags":["multi-modal learning","recognition","convolutional neural network","context","rgb-d objectness estimation","rgb-d object detection"]},{"p_id":10101,"title":"Towards Bayesian Deep Learning: A Framework and Some Existing Methods","abstract":"While perception tasks such as visual object recognition and text understanding play an important role in human intelligence, subsequent tasks that involve inference, reasoning, and planning require an even higher level of intelligence. The past few years have seen major advances in many perception tasks using deep learning models. For higher-level inference, however, probabilistic graphical models with their Bayesian nature are still more powerful and flexible. To achieve integrated intelligence that involves both perception and inference, it is naturally desirable to tightly integrate deep learning and Bayesian models within a principled probabilistic framework, which we call Bayesian deep learning. In this unified framework, the perception of text or images using deep learning can boost the performance of higher-level inference and in return, the feedback from the inference process is able to enhance the perception of text or images. This paper proposes a general framework for Bayesian deep learning and reviews its recent applications on recommender systems, topic models, and control. In this paper, we also discuss the relationship and differences between Bayesian deep learning and other related topics such as the Bayesian treatment of neural networks.","keywords_author":["Artificial intelligence","Bayesian networks","data mining","deep learning","machine learning","neural networks","Artificial intelligence","data mining","Bayesian networks","neural networks","deep learning","machine learning"],"keywords_other":["Learning models","NETWORKS","Inference process","Unified framework","RECOGNITION","Visual object recognition","Probabilistic framework","Bayesian model","Human intelligence","RECOMMENDER SYSTEMS","Probabilistic graphical models"],"max_cite":7.0,"pub_year":2016.0,"sources":"['scp', 'wos']","rawkeys":["learning models","artificial intelligence","probabilistic framework","bayesian model","data mining","recognition","deep learning","neural networks","machine learning","probabilistic graphical models","unified framework","recommender systems","networks","inference process","bayesian networks","visual object recognition","human intelligence"],"tags":["learning models","probabilistic framework","bayesian model","data mining","recognition","neural networks","machine learning","probabilistic graphical models","unified framework","recommender systems","networks","inference process","bayesian networks","visual object recognition","human intelligence"]},{"p_id":10105,"title":"A Survey of Data Mining and Deep Learning in Bioinformatics","abstract":"The fields of medicine science and health informatics have made great progress recently and have led to in-depth analytics that is demanded by generation, collection and accumulation of massive data. Meanwhile, we are entering a new period where novel technologies are starting to analyze and explore knowledge from tremendous amount of data, bringing limitless potential for information growth. One fact that cannot be ignored is that the techniques of machine learning and deep learning applications play a more significant role in the success of bioinformatics exploration from biological data point of view, and a linkage is emphasized and established to bridge these two data analytics techniques and bioinformatics in both industry and academia. This survey concentrates on the review of recent researches using data mining and deep learning approaches for analyzing the specific domain knowledge of bioinformatics. The authors give a brief but pithy summarization of numerous data mining algorithms used for preprocessing, classification and clustering as well as various optimized neural network architectures in deep learning methods, and their advantages and disadvantages in the practical applications are also discussed and compared in terms of their industrial usage. It is believed that in this review paper, valuable insights are provided for those who are dedicated to start using data analytics methods in bioinformatics.","keywords_author":["Bioinformatics","Biomedicine","Data mining","Deep learning","Machine learning","Bioinformatics","Biomedicine","Data mining","Machine learning","Deep learning"],"keywords_other":["PREDICTION","DISEASE","K-MEANS","BIG DATA","CLUSTERING-ALGORITHM","SYSTEM","NEURAL-NETWORKS","RECOGNITION","ENSEMBLE CLASSIFIER","CHEST X-RAYS"],"max_cite":0.0,"pub_year":2018.0,"sources":"['ieee', 'scp', 'wos']","rawkeys":["k-means","neural-networks","data mining","ensemble classifier","recognition","big data","deep learning","disease","machine learning","prediction","system","biomedicine","bioinformatics","chest x-rays","clustering-algorithm"],"tags":["data mining","recognition","big data","neural networks","chest x-rays","machine learning","disease","prediction","system","biomedicine","bioinformatics","kernel methods","clustering algorithms","ensemble classifiers"]},{"p_id":10112,"title":"Pest identification via deep residual learning in complex background","abstract":"Agricultural pests severely affect both agricultural production and the storage of crops. To prevent damage caused by agricultural pests, the pest category needs to be correctly identified and targeted control measures need to be taken; therefore, it is important to develop an agricultural pest identification system based on computer vision technology. To achieve pest identification with the complex farmland background, a pest identification method is proposed that uses deep residual learning. Compared to support vector machine and traditional BP neural networks, the pest image recognition accuracy of this method is noticeably improved in the complex farmland background. Furthermore, in comparison to plain deep convolutional neural networks such as Alexnet, the recognition performance in this method was further improved after optimized by deep residual learning. A classification accuracy of 98.67% for 10 classes of crop pest images with complex farmland background was achieved. Accordingly, the method has a high value of practical application, and can be integrated with currently used agricultural networking systems into actual agricultural pest control tasks. (C)2017 Elsevier B.V. All rights reserved.","keywords_author":["Agricultural pests","Image recognition","Convolutional neural networks","Deep residual learning","ResNet"],"keywords_other":["RECOGNITION","CONVOLUTIONAL NEURAL-NETWORKS"],"max_cite":2.0,"pub_year":2017.0,"sources":"['ieee', 'wos']","rawkeys":["recognition","convolutional neural networks","deep residual learning","resnet","agricultural pests","convolutional neural-networks","image recognition"],"tags":["recognition","deep residual learning","agricultural pests","residual network","image recognition","convolutional neural network"]},{"p_id":10114,"title":"Deep Learning Driven Visual Path Prediction from a Single Image","abstract":"Capabilities of inference and prediction are the significant components of visual systems. Visual path prediction is an important and challenging task among them, with the goal to infer the future path of a visual object in a static scene. This task is complicated as it needs high-level semantic understandings of both the scenes and underlying motion patterns in video sequences. In practice, cluttered situations have also raised higher demands on the effectiveness and robustness of models. Motivated by these observations, we propose a deep learning framework, which simultaneously performs deep feature learning for visual representation in conjunction with spatiotemporal context modeling. After that, a unified path-planning scheme is proposed to make accurate path prediction based on the analytic results returned by the deep context models. The highly effective visual representation and deep context models ensure that our framework makes a deep semantic understanding of the scenes and motion patterns, consequently improving the performance on visual path prediction task. In experiments, we extensively evaluate the model's performance by constructing two large benchmark datasets from the adaptation of video tracking datasets. The qualitative and quantitative experimental results show that our approach outperforms the state-of-the-art approaches and owns a better generalization capability.","keywords_author":["convolutional neural networks","Deep learning","visual context model","Visual path prediction","Visual path prediction","visual context model","convolutional neural networks","deep learning"],"keywords_other":["Deep learning","Generalization capability","FEATURES","REPRESENTATION","Visual context","MODEL","Visual representations","State-of-the-art approach","Path prediction","RECOGNITION","Convolutional neural network","Semantic understanding","SCENE CLASSIFICATION"],"max_cite":3.0,"pub_year":2016.0,"sources":"['ieee', 'scp', 'wos']","rawkeys":["recognition","convolutional neural networks","generalization capability","features","deep learning","model","semantic understanding","representation","scene classification","convolutional neural network","state-of-the-art approach","visual context","visual path prediction","path prediction","visual context model","visual representations"],"tags":["recognition","generalization capability","model","features","semantic understanding","machine learning","representation","scene classification","convolutional neural network","state-of-the-art approach","visual context","visual path prediction","path prediction","visual context model","visual representations"]},{"p_id":10124,"title":"Abnormal event detection in crowded scenes based on deep learning","abstract":"\u00a9 2016, Springer Science+Business Media New York.In this paper, we propose to use the deep learning technique for abnormal event detection by extracting spatiotemporal features from video sequences. Human eyes are often attracted to abnormal events in video sequences, thus we firstly extract saliency information (SI) of video frames as the feature representation in the spatial domain. Optical flow (OF) is estimated as an important feature of video sequences in the temporal domain. To extract the accurate motion information, multi-scale histogram optical flow (MHOF) can be obtained through OF. We combine MHOF and SI into the spatiotemporal features of video frames. Finally a deep learning network, PCANet, is adopted to extract high-level features for abnormal event detection. Experimental results show that the proposed abnormal event detection method can obtain much better performance than the existing ones on the public video database.","keywords_author":["Abnormal event detection","Crowd analysis","Deep learning","Optical flow","Saliency information","Abnormal event detection","Crowd analysis","Saliency information","Optical flow","Deep learning"],"keywords_other":["Deep learning","Spatio temporal features","Saliency information","TRACKING","Crowd analysis","High-level features","DETECTION MODEL","QUALITY ASSESSMENT","Feature representation","RECOGNITION","SALIENT REGION DETECTION","Abnormal event detections","Important features","ORIENTATION","STEREOSCOPIC IMAGES"],"max_cite":10.0,"pub_year":2016.0,"sources":"['ieee', 'scp', 'wos']","rawkeys":["high-level features","recognition","stereoscopic images","deep learning","detection model","quality assessment","orientation","abnormal event detection","saliency information","salient region detection","important features","tracking","optical flow","abnormal event detections","feature representation","spatio temporal features","crowd analysis"],"tags":["stereoscopic image","high-level features","recognition","salient region detections","machine learning","information retrieval","orientation","saliency information","tracking","important features","detection models","optical flow","abnormal event detections","feature representation","spatio temporal features","crowd analysis"]},{"p_id":10125,"title":"Ensemble Deep Learning for Biomedical Time Series Classification","abstract":"Ensemble learning has been proved to improve the generalization ability effectively in both theory and practice. In this paper, we briefly outline the current status of research on it first. Then, a new deep neural network-based ensemble method that integrates filtering views, local views, distorted views, explicit training, implicit training, subview prediction, and Simple Average is proposed for biomedical time series classification. Finally, we validate its effectiveness on the Chinese Cardiovascular Disease Database containing a large number of electrocardiogram recordings. The experimental results show that the proposed method has certain advantages compared to some well-known ensemble methods, such as Bagging and AdaBoost.","keywords_author":null,"keywords_other":["NEURAL-NETWORK ENSEMBLES","ERROR","RECOGNITION","ALGORITHMS","FORESTS","CLASSIFIERS"],"max_cite":3.0,"pub_year":2016.0,"sources":"['wos']","rawkeys":["error","recognition","forests","classifiers","algorithms","neural-network ensembles"],"tags":["error","recognition","forests","classifier","algorithms","neural network ensembles"]},{"p_id":10131,"title":"Deep Multimetric Learning for Shape-Based 3D Model Retrieval","abstract":"Recently, feature-learning-based 3D shape retrieval methods have been receiving more and more attention in the 3D shape analysis community. In these methods, the hand-crafted metrics or the learned linear metrics are usually used to compute the distances between shape features. Since there are complex geometric structural variations with 3D shapes, the single hand-crafted metric or learned linear metric cannot characterize the manifold, where 3D shapes lie well. In this paper, by exploring the nonlinearity of the deep neural network and the complementarity among multiple shape features, we propose a novel deep multimetric network for 3D shape retrieval. The developed multimetric network minimizes a discriminative loss function that, for each type of shape feature, the outputs of the network from the same class are encouraged to be as similar as possible and the outputs from different classes are encouraged to be as dissimilar as possible. Meanwhile, the Hilbert-Schmidt independence criterion is employed to enforce the outputs of different types of shape features to be as complementary as possible. Furthermore, the weights of the learned multiple distance metrics can be adaptively determined in our developed deep metric network. The weighted distance metric is then used as the similarity for shape retrieval. We conduct experiments with the proposed method on the four benchmark shape datasets. Experimental results demonstrate that the proposed method can obtain better performance than the learned deep single metric and outperform the state-of-the-art 3D shape retrieval methods.","keywords_author":["3D shape retrieval","3D shape descriptor","deep neural network","multiple shape features","metric learning"],"keywords_other":["FEATURES","NEURAL-NETWORKS","RECOGNITION","DESCRIPTORS","ROBUST"],"max_cite":0.0,"pub_year":2017.0,"sources":"['ieee', 'wos']","rawkeys":["neural-networks","recognition","deep neural network","features","3d shape retrieval","3d shape descriptor","multiple shape features","robust","descriptors","metric learning"],"tags":["3d shape descriptors","robustness","recognition","features","neural networks","3d shape retrieval","multiple shape features","convolutional neural network","descriptors","metric learning"]},{"p_id":10133,"title":"Deep Learning in Visual Computing and Signal Processing","abstract":"Deep learning is a subfield of machine learning, which aims to learn a hierarchy of features from input data. Nowadays, researchers have intensively investigated deep learning algorithms for solving challenging problems in many areas such as image classification, speech recognition, signal processing, and natural language processing. In this study, we not only review typical deep learning algorithms in computer vision and signal processing but also provide detailed information on how to apply deep learning to specific areas such as road crack detection, fault diagnosis, and human activity detection. Besides, this study also discusses the challenges of designing and training deep neural networks.","keywords_author":null,"keywords_other":["DIMENSIONALITY","MICRO-DOPPLER SIGNATURES","SUPPORT VECTOR MACHINE","FAULT-DIAGNOSIS","ALGORITHM","CLASSIFICATION","RECOGNITION","CHEMICAL-PROCESSES","FACE VERIFICATION","CONVOLUTIONAL NEURAL-NETWORKS"],"max_cite":3.0,"pub_year":2017.0,"sources":"['ieee', 'wos']","rawkeys":["algorithm","dimensionality","recognition","face verification","micro-doppler signatures","classification","convolutional neural-networks","fault-diagnosis","support vector machine","chemical-processes"],"tags":["dimensionality","recognition","machine learning","chemical process","face verification","micro-doppler signatures","classification","convolutional neural network","algorithms","fault diagnosis"]},{"p_id":10141,"title":"Learning deep generative models with doubly stochastic gradient MCMC","abstract":"Deep generative models (DGMs), which are often organized in a hierarchical manner, provide a principled framework of capturing the underlying causal factors of data. Recent work on DGMs focussed on the development of efficient and scalable variational inference methods that learn a single model under some mean-field or parameterization assumptions. However, little work has been done on extending Markov chain Monte Carlo (MCMC) methods to Bayesian DGMs, which enjoy many advantages compared with variational methods. We present doubly stochastic gradient MCMC, a simple and generic method for (approximate) Bayesian inference of DGMs in a collapsed continuous parameter space. At each MCMC sampling step, the algorithm randomly draws a mini-batch of data samples to estimate the gradient of log-posterior and further estimates the intractable expectation over hidden variables via a neural adaptive importance sampler, where the proposal distribution is parameterized by a deep neural network and learnt jointly along with the sampling process. We demonstrate the effectiveness of learning various DGMs on a wide range of tasks, including density estimation, data generation, and missing data imputation. Our method outperforms many state-of-the-art competitors.","keywords_author":["Bayesian methods","deep generative models (DGMs)","deep learning","Markov chain Monte Carlo (MCMC)","stochastic gradient","Bayesian methods","deep generative models (DGMs)","deep learning","Markov chain Monte Carlo (MCMC)","stochastic gradient"],"keywords_other":["BELIEF NETWORKS","Missing data imputations","Stochastic gradient","Markov chain Monte Carlo","Generative model","Variational inference methods","ALGORITHM","NEURAL-NETWORKS","RECOGNITION","Bayesian methods","Proposal distribution","Markov chain Monte Carlo method"],"max_cite":0.0,"pub_year":2018.0,"sources":"['ieee', 'scp', 'wos']","rawkeys":["neural-networks","algorithm","generative model","markov chain monte carlo (mcmc)","recognition","bayesian methods","deep learning","variational inference methods","proposal distribution","markov chain monte carlo","stochastic gradient","missing data imputations","belief networks","deep generative models (dgms)","markov chain monte carlo method"],"tags":["deep generative model","generative model","recognition","bayesian methods","neural networks","variational inference methods","proposal distribution","markov chain monte carlo","machine learning","stochastic gradient","missing data imputations","belief networks","algorithms","markov chain monte carlo method"]},{"p_id":10154,"title":"A deep matrix factorization method for learning attribute representations","abstract":"Semi-Non-negative Matrix Factorization is a technique that learns a low-dimensional representation of a dataset that lends itself to a clustering interpretation. It is possible that the mapping between this new representation and our original data matrix contains rather complex hierarchical information with implicit lower-level hidden attributes, that classical one level clustering methodologies cannot interpret. In this work we propose a novel model, Deep Semi-NMF, that is able to learn such hidden representations that allow themselves to an interpretation of clustering according to different, unknown attributes of a given dataset. We also present a semi-supervised version of the algorithm, named Deep WSF, that allows the use of (partial) prior information for each of the known attributes of a dataset, that allows the model to be used on datasets with mixed attribute knowledge. Finally, we show that our models are able to learn low-dimensional representations that are better suited for clustering, but also classification, outperforming Semi-Non-negative Matrix Factorization, but also other state-of-the-art methodologies variants.","keywords_author":["deep semi-NMF","Deep WSF","face classification","face clustering","matrix factorization","Semi-NMF","semi-supervised learning","unsupervised feature learning","WSF","Semi-NMF","deep semi-NMF","unsupervised feature learning","face clustering","semi-supervised learning","Deep WSF","WSF","matrix factorization","face classification"],"keywords_other":["Semi-NMF","Face clustering","Face classification","deep semi-NMF","DIMENSIONALITY","IMAGE REPRESENTATION","Semi- supervised learning","Unsupervised feature learning","NONNEGATIVE MATRIX","RECOGNITION","ALGORITHMS","POSE","Matrix factorizations","Deep WSF"],"max_cite":11.0,"pub_year":2017.0,"sources":"['ieee', 'scp', 'wos']","rawkeys":["wsf","dimensionality","recognition","semi- supervised learning","unsupervised feature learning","face clustering","pose","nonnegative matrix","deep wsf","semi-nmf","matrix factorization","semi-supervised learning","matrix factorizations","face classification","algorithms","deep semi-nmf","image representation"],"tags":["wsf","dimensionality","recognition","unsupervised feature learning","face clustering","pose","nonnegative matrix","deep wsf","nonnegative matrix factorization","semi-nmf","semi-supervised learning","face classification","algorithms","deep semi-nmf","image representation"]},{"p_id":10158,"title":"Recent progress in analog memory-based accelerators for deep learning","abstract":"We survey recent progress in the use of analog memory devices to build neuromorphic hardware accelerators for deep learning applications. After an overview of deep learning and the application opportunities for deep neural network (DNN) hardware accelerators, we briefly discuss the research area of customized digital accelerators for deep learning. We discuss how the strengths and weaknesses of analog memory-based accelerators match well to the weaknesses and strengths of digital accelerators, and attempt to identify where the future hardware opportunities might be found. We survey the extensive but rapidly developing literature on what would be needed from an analog memory device to enable such a DNN accelerator, and summarize progress with various analog memory candidates including non-volatile memory such as resistive RAM, phase change memory, Li-ion-based devices, capacitor-based and other CMOS devices, as well as photonics-based devices and systems. After surveying how recent circuits and systems work, we conclude with a description of the next research steps that will be needed in order to move closer to the commercialization of viable analog-memory-based DNN hardware accelerators.","keywords_author":["analog memory","non-volatile memory","hardware accelerators","deep learning"],"keywords_other":["ASSOCIATIVE MEMORY","NEUROMORPHIC SYSTEMS","HOPFIELD MODEL","NEURAL-NETWORKS","RECOGNITION","RRAM","PHASE-CHANGE MEMORY","SYNAPSES","MEMRISTIVE DEVICES","OPTICAL IMPLEMENTATION"],"max_cite":0.0,"pub_year":2018.0,"sources":"['ieee', 'wos']","rawkeys":["neural-networks","analog memory","hardware accelerators","associative memory","recognition","synapses","deep learning","neuromorphic systems","hopfield model","non-volatile memory","rram","phase-change memory","optical implementation","memristive devices"],"tags":["analog memory","hardware accelerators","associative memory","recognition","synapses","neural networks","neuromorphic systems","machine learning","hopfield model","non-volatile memory","phase-change memory","optical implementation","memristive devices","resistive switching memory"]},{"p_id":10163,"title":"Virtual screening of inorganic materials synthesis parameters with deep learning","abstract":"Virtual materials screening approaches have proliferated in the past decade, driven by rapid advances in first-principles computational techniques, and machine-learning algorithms. By comparison, computationally driven materials synthesis screening is still in its infancy, and is mired by the challenges of data sparsity and data scarcity: Synthesis routes exist in a sparse, high-dimensional parameter space that is difficult to optimize over directly, and, for some materials of interest, only scarce volumes of literature-reported syntheses are available. In this article, we present a framework for suggesting quantitative synthesis parameters and potential driving factors for synthesis outcomes. We use a variational autoencoder to compress sparse synthesis representations into a lower dimensional space, which is found to improve the performance of machine-learning tasks. To realize this screening framework even in cases where there are few literature data, we devise a novel data augmentation methodology that incorporates literature synthesis data from related materials systems. We apply this variational autoencoder framework to generate potential SrTiO3 synthesis parameter sets, propose driving factors for brookite TiO2 formation, and identify correlations between alkali-ion intercalation and MnO2 polymorph selection.","keywords_author":null,"keywords_other":["MATERIALS DISCOVERY","CLEAN ENERGY PROJECT","NANOCRYSTALS","STRONTIUM-TITANATE","PEROVSKITES","SOL-GEL","PHOTOCATALYTIC ACTIVITY","RECOGNITION","BROOKITE","TEMPERATURE"],"max_cite":3.0,"pub_year":2017.0,"sources":"['wos']","rawkeys":["sol-gel","recognition","photocatalytic activity","perovskites","clean energy project","strontium-titanate","brookite","nanocrystals","temperature","materials discovery"],"tags":["sol-gel","recognition","photocatalytic activity","perovskites","clean energy project","strontium-titanate","brookite","nanocrystals","temperature","materials discovery"]},{"p_id":1985,"title":"Explicit knowledge extraction in information-theoretic supervised multi-layered SOM","abstract":"In this paper, we examine the effectiveness of SOM knowledge to train multi-layered neural networks. We have known that the SOM can produce very rich knowledge, used for visualization and class structure interpretation. It is expected that this SOM knowledge can be used for many different purposes in addition to visualization and interpretation. By using more flexible information-theoretic SOM, we examine the effectiveness of SOM knowledge for training multi-layered networks. We applied the method to the spam mail identification problem. We found that SOM knowledge greatly facilitated the learning of multi-layered networks and could be used to improve generalization performance.","keywords_author":["SOM","information-theoretic","supervised learning","multi-layered visualization","interpretation","generalization"],"keywords_other":["multilayer perceptrons","information theory","knowledge acquisition","self-organizing maps","information-theoretic supervised multilayered SOM","learning (artificial intelligence)","unsolicited e-mail","multilayered neural network learning","Training","Knowledge engineering","Biological neural networks","Visualization","Testing","self-organising feature maps","knowledge extraction","multilayered neural network training","spam mail identification problem","Neurons"],"max_cite":2.0,"pub_year":2014.0,"sources":"['ieee']","rawkeys":["multilayer perceptrons","supervised learning","multi-layered visualization","information theory","self-organising feature maps","visualization","knowledge acquisition","unsolicited e-mail","generalization","knowledge extraction","som","knowledge engineering","biological neural networks","neurons","learning (artificial intelligence)","self-organizing maps","training","multilayered neural network training","information-theoretic","interpretation","testing","multilayered neural network learning","information-theoretic supervised multilayered som","spam mail identification problem"],"tags":["supervised learning","multi-layered visualization","information theory","self-organising feature maps","visualization","knowledge acquisition","unsolicited e-mail","machine learning","knowledge extraction","knowledge engineering","biological neural networks","neurons","interpretability","recognition","training","multilayered neural network training","information-theoretic","testing","multilayered neural network learning","self-organizing map","information-theoretic supervised multilayered som","multi layer perceptron","spam mail identification problem"]},{"p_id":10188,"title":"A DEEP Q-LEARNING NETWORK FOR SHIP STOWAGE PLANNING PROBLEM","abstract":"Ship stowage plan is the management connection of quae crane scheduling and yard crane scheduling. The quality of ship stowage plan affects the productivity greatly. Previous studies mainly focuses on solving stowage planning problem with online searching algorithm, efficiency of which is significantly affected by case size. In this study, a Deep Q-Learning Network (DQN) is proposed to solve ship stowage planning problem. With DQN, massive calculation and training is done in pre-training stage, while in application stage stowage plan can be made in seconds. To formulate network input, decision factors are analyzed to compose feature vector of stowage plan. States subject to constraints, available action and reward function of Q-value are designed. With these information and design, an 8-layer DQN is formulated with an evaluation function of mean square error is composed to learn stowage planning. At the end of this study, several production cases are solved with proposed DQN to validate the effectiveness and generalization ability. Result shows a good availability of DQN to solve ship stowage planning problem.","keywords_author":["Deep Q-Leaning Network (DQN)","Container terminal","Ship stowage plan","Markov decision process","Value function approximation","Generalization"],"keywords_other":["CONTAINER YARDS"],"max_cite":0.0,"pub_year":2017.0,"sources":"['wos']","rawkeys":["ship stowage plan","container yards","value function approximation","generalization","deep q-leaning network (dqn)","container terminal","markov decision process"],"tags":["recognition","ship stowage plan","container yards","markov decision processes","value function approximation","deep q-leaning network (dqn)","container terminal"]},{"p_id":26611,"title":"Visual Tracking with Convolutional Random Vector Functional Link Network","abstract":"\u00a9 2017 IEEE.Deep neural network-based methods have recently achieved excellent performance in visual tracking task. As very few training samples are available in visual tracking task, those approaches rely heavily on extremely large auxiliary dataset such as ImageNet to pretrain the model. In order to address the discrepancy between the source domain (the auxiliary data) and the target domain (the object being tracked), they need to be finetuned during the tracking process. However, those methods suffer from sensitivity to the hyper-parameters such as learning rate, maximum number of epochs, size of mini-batch, and so on. Thus, it is worthy to investigate whether pretraining and fine tuning through conventional back-prop is essential for visual tracking. In this paper, we shed light on this line of research by proposing convolutional random vector functional link (CRVFL) neural network, which can be regarded as a marriage of the convolutional neural network and random vector functional link network, to simplify the visual tracking system. The parameters in the convolutional layer are randomly initialized and kept fixed. Only the parameters in the fully connected layer need to be learned. We further propose an elegant approach to update the tracker. In the widely used visual tracking benchmark, without any auxiliary data, a single CRVFL model achieves 79.0% with a threshold of 20 pixels for the precision plot. Moreover, an ensemble of CRVFL yields comparatively the best result of 86.3%.","keywords_author":["Convolutional neural network (CNN)","convolutional random vector functional link (CRVFL)","deep learning","random vector functional link (RVFL)","visual tracking","Convolutional neural network (CNN)","convolutional random vector functional link (CRVFL)","deep learning","random vector functional link (RVFL)","visual tracking"],"keywords_other":["Hyper-parameter","SELECTION","Visual tracking systems","Training sample","Functional links","Tracking process","NET","NEURAL-NETWORKS","RECOGNITION","Convolutional neural network","APPEARANCE MODEL","SPARSE REPRESENTATION","OBJECT TRACKING","Deep neural networks","ROBUST","Functional-link network"],"max_cite":10.0,"pub_year":2017.0,"sources":"['ieee', 'scp', 'wos']","rawkeys":["random vector functional link (rvfl)","net","convolutional neural network","tracking process","visual tracking systems","neural-networks","training sample","hyper-parameter","robust","convolutional random vector functional link (crvfl)","appearance model","functional links","recognition","object tracking","deep learning","functional-link network","sparse representation","convolutional neural network (cnn)","selection","deep neural networks","visual tracking"],"tags":["robustness","convolutional neural network","tracking process","visual tracking systems","training sample","machine learning","hyper-parameter","convolutional random vector functional link (crvfl)","functional links","appearance modeling","recognition","object tracking","neural networks","functional-link network","sparse representation","nets","selection","random vector functional link","visual tracking"]},{"p_id":10228,"title":"Advances in deep learning approaches for image tagging","abstract":"The advent of mobile devices and media cloud services has led to the unprecedented growth of personal photo collections. One of the fundamental problems in managing the increasing number of photos is automatic image tagging. Image tagging is the task of assigning human-friendly tags to an image so that the semantic tags can better reflect the content of the image and therefore can help users better access that image. The quality of image tagging depends on the quality of concept modeling which builds a mapping from concepts to visual images. While significant progresses are made in the past decade on image tagging, the previous approaches can only achieve limited success due to the limited concept representation ability fromhand-crafted features (e.g., Scale-Invariant Feature Transform, GIST, Histogram of Oriented Gradients, etc.). Further progresses are made, since the efficient and effective deep learning algorithms have been developed. The purpose of this paper is to categorize and evaluate different image tagging approaches based on deep learning techniques. We also discuss the relevant problems and applications to image tagging, including data collection, evaluation metrics, and existing commercial systems. We conclude the advantages of different image tagging paradigms and propose several promising research directions for future works.","keywords_author":["Deep learning","Image tagging","Image tagging","Deep learning"],"keywords_other":["FEATURES","ALGORITHM","DATABASE","RECOGNITION","CATEGORIZATION"],"max_cite":0.0,"pub_year":2017.0,"sources":"['scp', 'wos']","rawkeys":["algorithm","recognition","features","deep learning","image tagging","database","categorization"],"tags":["recognition","features","databases","machine learning","image tagging","categorization","algorithms"]},{"p_id":42997,"title":"DepthSynth: Real-Time Realistic Synthetic Data Generation from CAD Models for 2.5D Recognition","abstract":"\u00a9 2017 IEEE. Recent progress in computer vision has been dominated by deep neural networks trained over larges amount of labeled data. Collecting such datasets is however a tedious, often impossible task; hence a surge in approaches relying solely on synthetic data for their training. For depth images however, discrepancies with real scans still noticeably affect the end performance. We thus propose an end-To-end framework which simulates the whole mechanism of these devices, generating realistic depth data from 3D models by comprehensively modeling vital factors e.g. sensor noise, material reflectance, surface geometry. Not only does our solution cover a wider range of sensors and achieve more realistic results than previous methods, assessed through extended evaluation, but we go further by measuring the impact on the training of neural networks for various recognition tasks; demonstrating how our pipeline seamlessly integrates such architectures and consistently enhances their performance.","keywords_author":["CAD","data-Augmentation","deep-learning","depth-sensor","image-synthesis","recognition","simulation"],"keywords_other":["Depth sensors","Image synthesis","recognition","Data augmentation","simulation"],"max_cite":0.0,"pub_year":2018.0,"sources":"['scp']","rawkeys":["recognition","simulation","deep-learning","depth-sensor","data augmentation","image synthesis","cad","data-augmentation","depth sensors","image-synthesis"],"tags":["recognition","computer-aided diagnosis","machine learning","data augmentation","image synthesis","depth sensors","simulation"]},{"p_id":2039,"title":"Context dependent encoding using convolutional dynamic networks","abstract":"Perception of sensory signals is strongly influenced by their context, both in space and time. In this paper, we propose a novel hierarchical model, called convolutional dynamic networks, that effectively utilizes this contextual information, while inferring the representations of the visual inputs. We build this model based on a predictive coding framework and use the idea of empirical priors to incorporate recurrent and top-down connections. These connections endow the model with contextual information coming from temporal as well as abstract knowledge from higher layers. To perform inference efficiently in this hierarchical model, we rely on a novel scheme based on a smoothing proximal gradient method. When trained on unlabeled video sequences, the model learns a hierarchy of stable attractors, representing low-level to high-level parts of the objects. We demonstrate that the model effectively utilizes contextual information to produce robust and stable representations for object recognition in video sequences, even in case of highly corrupted inputs.","keywords_author":["Context","deep learning","dynamic models","empirical priors","object recognition","Context","deep learning","dynamic models","empirical priors","object recognition","Context","deep learning","dynamic models","empirical priors","object recognition."],"keywords_other":["convolutional dynamic networks","Predictive models","inference mechanisms","hierarchical model","ALGORITHM","VISUAL-CORTEX","image sequences","Mathematical model","Stable attractors","Video sequences","Hierarchical model","video sequences","Deep learning","context dependent encoding","IMAGE SET CLASSIFICATION","Context modeling","gradient methods","NATURAL IMAGES","Context","inference","smoothing methods","smoothing proximal gradient method","Context dependent","Contextual information","deep learning","video signal processing","Predictive coding","MODEL","RECOGNITION","TIME","dynamic models","object recognition","predictive coding framework","State-space methods","empirical priors"],"max_cite":1.0,"pub_year":2015.0,"sources":"['scp', 'wos', 'ieee']","rawkeys":["convolutional dynamic networks","visual-cortex","inference mechanisms","contextual information","predictive models","hierarchical model","image sequences","context modeling","video sequences","context dependent encoding","image set classification","gradient methods","time","inference","natural images","smoothing methods","smoothing proximal gradient method","algorithm","mathematical model","recognition","deep learning","video signal processing","model","stable attractors","dynamic models","predictive coding","object recognition","state-space methods","predictive coding framework","empirical priors","context","context dependent"],"tags":["convolutional dynamic networks","visual-cortex","inference mechanisms","contextual information","predictive models","hierarchical model","image sequences","context modeling","video sequences","context dependent encoding","image set classification","machine learning","gradient methods","time","dynamical model","inference","natural images","algorithms","smoothing methods","smoothing proximal gradient method","mathematical model","recognition","video signal processing","model","stable attractors","predictive coding","object recognition","state-space methods","predictive coding framework","empirical priors","context","context dependent"]},{"p_id":108551,"title":"TextProposals: A text-specific selective search algorithm for word spotting in the wild","abstract":"Motivated by the success of powerful while expensive techniques to recognize words in a holistic way (Goel et al., 2013; Almazan et al., 2014; Jaderberg et al., 2016) object proposals techniques emerge as an alternative to the traditional text detectors. In this paper we introduce a novel object proposals method that is specifically designed for text. We rely on a similarity based region grouping algorithm that generates a hierarchy of word hypotheses. Over the nodes of this hierarchy it is possible to apply a holistic word recognition method in an efficient way.","keywords_author":["Object proposals","Scene text","Perceptual organization","Grouping"],"keywords_other":["STABLE EXTREMAL REGIONS","NATURAL SCENE IMAGES","RECOGNITION","EXTRACTION"],"max_cite":2.0,"pub_year":2017.0,"sources":"['wos']","rawkeys":["recognition","object proposals","scene text","perceptual organization","natural scene images","stable extremal regions","extraction","grouping"],"tags":["recognition","object proposals","scene text","perceptual organization","natural scene images","stable extremal regions","extraction","grouping"]},{"p_id":26634,"title":"A Survey of Stealth Malware Attacks, Mitigation Measures, and Steps Toward Autonomous Open World Solutions","abstract":"\u00a9 2016 IEEE. As our professional, social, and financial existences become increasingly digitized and as our government, healthcare, and military infrastructures rely more on computer technologies, they present larger and more lucrative targets for malware. Stealth malware in particular poses an increased threat because it is specifically designed to evade detection mechanisms, spreading dormant, in the wild for extended periods of time, gathering sensitive information or positioning itself for a high-impact zero-day attack. Policing the growing attack surface requires the development of efficient anti-malware solutions with improved generalization to detect novel types of malware and resolve these occurrences with as little burden on human experts as possible. In this paper, we survey malicious stealth technologies as well as existing solutions for detecting and categorizing these countermeasures autonomously. While machine learning offers promising potential for increasingly autonomous solutions with improved generalization to new malware types, both at the network level and at the host level, our findings suggest that several flawed assumptions inherent to most recognition algorithms prevent a direct mapping between the stealth malware recognition problem and a machine learning solution. The most notable of these flawed assumptions is the closed world assumption: that no sample belonging to a class outside of a static training set will appear at query time. We present a formalized adaptive open world framework for stealth malware recognition and relate it mathematically to research from other machine learning domains.","keywords_author":["anomaly detection","extreme value theory","intrusion detection","machine learning","malware","novelty detection","open set","outlier detection","recognition","rootkits","Stealth"],"keywords_other":["recognition","Stealth","Outlier Detection","Extreme value theory","Anomaly detection","open set","Novelty detection"],"max_cite":6.0,"pub_year":2017.0,"sources":"['scp']","rawkeys":["recognition","stealth","anomaly detection","outlier detection","intrusion detection","rootkits","extreme value theory","machine learning","malware","open set","novelty detection"],"tags":["recognition","stealth","anomaly detection","outlier detection","rootkits","intrusion detection systems","extreme value theory","machine learning","malware","open set","novelty detection"]},{"p_id":59405,"title":"Maybe a tad early for a Grand Unified Theory: Commentary on \"Towards a Grand Unified Theory of sports performance\"","abstract":null,"keywords_author":null,"keywords_other":["COORDINATION","TRANSITIONS","SOCCER","MODEL","NEURAL-NETWORKS","RECOGNITION","DISCRETE MOVEMENTS","DYNAMICS","PATTERNS","GAMES"],"max_cite":0.0,"pub_year":2017.0,"sources":"['ieee', 'wos']","rawkeys":["neural-networks","recognition","model","transitions","soccer","patterns","dynamics","coordination","games","discrete movements"],"tags":["transition","recognition","model","neural networks","soccer","patterns","dynamics","coordination","games","discrete movements"]},{"p_id":83982,"title":"Joint Hypergraph Learning for Tag-Based Image Retrieval","abstract":"As the image sharing websites like Flickr become more and more popular, extensive scholars concentrate on tag-based image retrieval. It is one of the important ways to find images contributed by social users. In this research field, tag information and diverse visual features have been investigated. However, most existing methods use these visual features separately or sequentially. In this paper, we propose a global and local visual features fusion approach to learn the relevance of images by hypergraph approach. A hypergraph is constructed first by utilizing global, local visual features, and tag information. Then, we propose a pseudo-relevance feedback mechanism to obtain the pseudo-positive images. Finally, with the hypergraph and pseudo relevance feedback, we adopt the hypergraph learning algorithm to calculate the relevance score of each image to the query. Experimental results demonstrate the effectiveness of the proposed approach.","keywords_author":["Tag-based image retrieval","hypergraph","feature fusion","visual feature","pseudo relevance feedback"],"keywords_other":["RELEVANCE","FEATURES","SOCIAL MEDIA","SEARCH RERANKING","CONTEXTS","MODEL","RECOGNITION","SEGMENTATION"],"max_cite":0.0,"pub_year":2018.0,"sources":"['wos']","rawkeys":["recognition","model","segmentation","features","pseudo relevance feedback","visual feature","social media","relevance","search reranking","contexts","feature fusion","tag-based image retrieval","hypergraph"],"tags":["recognition","model","segmentation","features","pseudo relevance feedback","visual feature","social media","relevance","search reranking","context","feature fusion","tag-based image retrieval","hypergraph"]},{"p_id":10257,"title":"Age Groups Classification in Social Network Using Deep Learning","abstract":"Social networks have a large amount of data available, but often, people do not provide some of their personal data, such as age, gender, and other demographics. Although the sentiment analysis uses such data to develop useful applications in people's daily lives, there are still failures in this type of analysis, either by the restricted number of words contained in the word dictionaries or because they do not consider the most diverse parameters that can influence the sentiments in a sentence; thus, more reliable results can be obtained, if the users profile information and their writing characteristics are considered. This research suggests that one of the most relevant parameter contained in the user profile is the age group, showing that there are typical behaviors among users of the same age group, specifically, when these users write about the same topic. A detailed analysis with 7000 sentences was performed to determine which characteristics are relevant, such as, the use of punctuation, number of characters, media sharing, topics, among others; and which ones can be disregarded for the age groups classification. Different learning machine algorithms are tested for the classification of the teenager and adult age group, and the deep convolutional neural network had the best performance, reaching a precision of 0.95 in the validation tests. Furthermore, in order to validate the usefulness of the proposed model for classifying age groups, it is implemented into the enhanced sentiment metric (eSM). In the performance validation, subjective tests are performed and the eSM with the proposed model reached a root mean square error and a Pearson correlation coefficient of 0.25 and 0.94, respectively, outperforming the eSM metric, when the age group information is not available.","keywords_author":["artificial neural networks","deep networ","machine learning","sentiment analysis","Social network services","text analysis","Social network services","sentiment analysis","machine learning","text analysis","artificial neural networks","deep network"],"keywords_other":["Pearson correlation coefficients","Performance validation","LANGUAGE USE","Text analysis","Sentiment analysis","Root mean square errors","Social network services","RECOGNITION","GENDER","Convolutional neural network","SENTIMENT ANALYSIS","deep networ"],"max_cite":4.0,"pub_year":2017.0,"sources":"['ieee', 'scp', 'wos']","rawkeys":["pearson correlation coefficients","recognition","performance validation","root mean square errors","machine learning","gender","social network services","deep network","text analysis","artificial neural networks","convolutional neural network","language use","sentiment analysis","deep networ"],"tags":["pearson correlation coefficients","recognition","neural networks","performance validation","root mean square errors","machine learning","gender","social network services","text analysis","convolutional neural network","deep networks","language use","sentiment analysis","deep networ"]},{"p_id":59409,"title":"A Study of the Effect of RRAM Reliability Soft Errors on the Performance of RRAM-Based Neuromorphic Systems","abstract":"Resistive RAM (RRAM) device has been extensively used as a scalable nonvolatile memory cell in neuromorphic systems due to its several advantages, including its small size and low-power requirements. However, resulting from the stochastic nature of the oxygen vacancies, the RRAM device suffers from reliability soft errors. In this paper, we provide for the first time a modeling framework to compute the effect of those soft errors on the system accuracy. Applying the proposed technique on a case-study system used to recognize the MNIST data set, our simulation results show that the system accuracy can degrade from 91.6% to 43% due to the RRAM reliability soft errors. To overcome this loss in the system performance, various possible adjustments to the parameters of the neuron pulses are analyzed. Furthermore, in this paper, two methodologies are proposed for automatically detecting and fixing the degradation in the system accuracy caused by the RRAM reliability soft errors. Using the suggested methodologies, the system accuracy of our case-study system can be restored back from 43% to 91.6% with small increase in the training cycle duration and with as small as 0.1% increment in the energy consumption of the system.","keywords_author":["MNIST recognition system","neuromorphic systems","nonvolatile memory","resistive RAM (RRAM) arrays","RRAM reliability soft errors"],"keywords_other":["PULSE","DEVICE","1T1R","CLASSIFICATION","DESIGN","ARRAY","RECOGNITION","NEURAL-NETWORK","MEMORY","CHIP"],"max_cite":0.0,"pub_year":2017.0,"sources":"['ieee', 'wos']","rawkeys":["design","array","device","recognition","mnist recognition system","memory","chip","neuromorphic systems","pulse","resistive ram (rram) arrays","classification","neural-network","nonvolatile memory","1t1r","rram reliability soft errors"],"tags":["design","arrays","pulses","mnist recognition system","memory","recognition","chip","devices","neural networks","neuromorphic systems","resistive ram (rram) arrays","non-volatile memory","classification","1t1r","rram reliability soft errors"]},{"p_id":18451,"title":"Image reconstruction from bag-of-visual-words","abstract":"\u00a9 2014 IEEE.The objective of this study is to reconstruct images from Bag-of-Visual-Words (BoVW), which is the de facto standard feature for image retrieval and recognition. BoVW is defined here as a histogram of quantized descriptors extracted densely on a regular grid at a single scale. Despite its wide use, no report describes reconstruction of the original image of a BoVW. This task is challenging for two reasons: 1) BoVW includes quantization errors when local descriptors are assigned to visual words. 2) BoVW lacks spatial information of local descriptors when we count the occurrence of visual words. To tackle this difficult task, we use a large-scale image database to estimate the spatial arrangement of local descriptors. Then this task creates a jigsaw puzzle problem with adjacency and global location costs of visual words. Solving this optimization problem is also challenging because it is known as an NP-Hard problem. We propose a heuristic but efficient method to optimize it. To underscore the effectiveness of our method, we apply it to BoVWs extracted from about 100 different categories and demonstrate that it can reconstruct the original images, although the image features lack spatial information and include quantization errors.","keywords_author":["computer vision","image reconstruction","recognition","retrieval"],"keywords_other":["Spatial arrangements","Optimization problems","recognition","Bag-of-visual-words","De facto standard","retrieval","Spatial informations","Quantization errors"],"max_cite":24.0,"pub_year":2014.0,"sources":"['scp', 'wos']","rawkeys":["image reconstruction","quantization errors","recognition","optimization problems","spatial arrangements","bag-of-visual-words","retrieval","de facto standard","computer vision","spatial informations"],"tags":["image reconstruction","quantization errors","recognition","optimization problems","spatial arrangements","bag-of-visual-words","retrieval","de facto standard","computer vision","spatial informations"]},{"p_id":43029,"title":"Perception-driven procedural texture generation from examples","abstract":"\u00a9 2018 Elsevier B.V. Procedural textures are widely used in computer games and animations for efficiently rendering natural scenes. They are generated using mathematical functions, and users need to tune the model parameters to produce desired texture. However, unless one has a good knowledge of these procedural models, it is difficult to predict which model can produce what types of textures. This paper proposes a framework for generating new procedural textures from examples. The new texture can have the same perceptual attributes as those of the input example or re-defined by the users. To achieve this goal, we first introduce a PCA-based Convolutional Network (PCN) to effectively learn texture features. These PCN features can be used to accurately predict the perceptual scales of the input example and a procedural model that can generate the input. Perceptual scales of the input can be redefined by users and further mapped to a point in the perceptual texture space, which has been established in advance by using a training dataset. Finally, we determine the parameters of the procedural generation model by performing perceptual similarity measurement in the perceptual texture space. Extensive experiments show that our method has produced promising results.","keywords_author":["Convolutional neural network","Deep learning","PCA-based Convolutional Network","Procedural texture","Texture generation","Texture perception","Procedural texture","Texture generation","Texture perception","Convolutional neural network","Deep learning","PCA-based Convolutional Network"],"keywords_other":["Procedural textures","Convolutional networks","FEATURES","NETWORKS","NOISE","CLASSIFICATION","Texture perception","Texture generation","RECOGNITION","RETRIEVAL","Convolutional neural network","MODELS","SCALE"],"max_cite":0.0,"pub_year":2018.0,"sources":"['ieee', 'scp', 'wos']","rawkeys":["procedural texture","noise","procedural textures","features","deep learning","recognition","convolutional networks","networks","classification","convolutional neural network","models","pca-based convolutional network","retrieval","texture perception","texture generation","scale"],"tags":["recognition","model","noise","features","procedural textures","machine learning","networks","classification","convolutional neural network","pca-based convolutional network","retrieval","texture perception","texture generation","scale"]},{"p_id":10265,"title":"Deep Learning and Developmental Learning: Emergence of Fine-to-Coarse Conceptual Categories at Layers of Deep Belief Network","abstract":"In this paper, I investigate conceptual categories derived from developmental processing in a deep neural network. The similarity matrices of deep representation at each layer of neural network are computed and compared with their raw representation. While the clusters generated by raw representation stand at the basic level of abstraction, conceptual categories obtained from deep representation shows a bottom-up transition procedure. Results demonstrate a developmental course of learning from specific to general level of abstraction through learned layers of representations in a deep belief network.","keywords_author":["deep learning","deep representation","developmental learning","levels of abstraction","deep learning","developmental learning","deep representation","levels of abstraction"],"keywords_other":["Concept Formation","Neural Networks (Computer)","Learning","Humans","Human Development","VISUAL-CORTEX","NEURAL-NETWORKS","RECOGNITION","INFANCY"],"max_cite":2.0,"pub_year":2016.0,"sources":"['ieee', 'scp', 'wos']","rawkeys":["neural-networks","deep representation","levels of abstraction","neural networks (computer)","recognition","visual-cortex","deep learning","infancy","concept formation","learning","humans","human development","developmental learning"],"tags":["deep representation","levels of abstraction","recognition","visual-cortex","neural networks","infancy","concept formation","machine learning","humans","human developments","developmental learning"]},{"p_id":10267,"title":"Deep learning in pharmacy: The prediction of aqueous solubility based on deep belief network","abstract":"The aqueous solubility of a drug is a significant factor for its bioavailability. Since many drugs on the market are the oral drugs, their absorption and metabolism in organisms are closely related to its aqueous solubility. As one of the most important properties of drug, the molecule aqueous solubility has received increasing attentions in drug discovery field. The methods of shallow machine learning have been applied to the field of pharmacy, with some success. In this paper, we established a multilayer deep belief network based on semi-supervised learning model to predict the aqueous solubility of compounds. This method can be used for recognizing whether compounds are soluble or not. Firstly, we discussed the influence of feature dimension to predict accuracy. Secondly, we analyzed the parameters of model in predicting aqueous solubility of drugs and contrasted the shallow machine learning with the similar deep architecture. The results showed that the model we proposed can predict aqueous solubility accurately, the accuracy of DBN reached 85.9%. The stable performance on the evaluation metrics confirms the practicability of our model. Moreover, the DBN model could be applied to reduce the cost and time of drug discovery by predicting aqueous solubility of drugs.","keywords_author":["aqueous solubility","deep belief network","deep learning","semi-supervised machine learning","aqueous solubility","deep learning","semi-supervised machine learning","deep belief network"],"keywords_other":["Semi-supervised","Feature dimensions","FEATURES","Deep architectures","Semi- supervised learning","Evaluation metrics","ALGORITHM","Deep belief networks","Aqueous solubility","NEURAL-NETWORKS","RECOGNITION","Stable performance"],"max_cite":1.0,"pub_year":2017.0,"sources":"['ieee', 'scp', 'wos']","rawkeys":["neural-networks","algorithm","semi-supervised machine learning","recognition","semi- supervised learning","features","deep learning","stable performance","semi-supervised","deep architectures","aqueous solubility","evaluation metrics","feature dimensions","deep belief network","deep belief networks"],"tags":["semi-supervised machine learning","recognition","features","neural networks","stable performance","semi-supervised","machine learning","deep architectures","aqueous solubility","evaluation metrics","semi-supervised learning","feature dimensions","algorithms","deep belief networks"]},{"p_id":10280,"title":"Fusing texture, shape and deep model-learned information at decision level for automated classification of lung nodules on chest CT","abstract":"The separation of malignant from benign lung nodules on chest computed tomography (CT) is important for the early detection of lung cancer, since early detection and management offer the best chance for cure. Although deep learning methods have recently produced a marked improvement in image classification there are still challenges as these methods contain myriad parameters and require large-scale training sets that are not usually available for most routine medical imaging studies. In this paper, we propose an algorithm for lung nodule classification that fuses the texture, shape and deep model-learned information (Fuse-TSD) at the decision level. This algorithm employs a gray level co-occurrence matrix (GLCM)-based texture descriptor, a Fourier shape descriptor to characterize the heterogeneity of nodules and a deep convolutional neural network (DCNN) to automatically learn the feature representation of nodules on a slice-by-slice basis. It trains an AdaBoosted back propagation neural network (BPNN) using each feature type and fuses the decisions made by three classifiers to differentiate nodules. We evaluated this algorithm against three approaches on the LIDC-IDRI dataset. When the nodules with a composite malignancy rate 3 were discarded, regarded as benign or regarded as malignant, our Fuse-TSD algorithm achieved an AUC of 96.65%, 94.45% and 81.24%, respectively, which was substantially higher than the AUC obtained by other approaches.","keywords_author":["Lung nodule classification","Chest CT","Deep convolutional neural network (DCNN)","Back propagation neural network (BPNN)","AdaBoost, information fusion"],"keywords_other":["PULMONARY NODULES","FEATURES","NEURAL-NETWORKS","RECOGNITION","CANCER","SCANS","COMPUTED-TOMOGRAPHY IMAGES"],"max_cite":2.0,"pub_year":2018.0,"sources":"['ieee', 'wos']","rawkeys":["neural-networks","chest ct","recognition","cancer","features","lung nodule classification","pulmonary nodules","computed-tomography images","deep convolutional neural network (dcnn)","back propagation neural network (bpnn)","information fusion","scans","adaboost"],"tags":["chest ct","recognition","back propagation neural networks","cancer","features","neural networks","lung nodule classification","pulmonary nodules","computed-tomography images","convolutional neural network","scans","information fusion","adaboost"]},{"p_id":108585,"title":"A generic neural acoustic beamforming architecture for robust multi-channel speech processing","abstract":"Acoustic beamforming can greatly improve the performance of Automatic Speech Recognition(ASR) and speech enhancement systems when multiple channels are available. We recently proposed a way to support the model-based Generalized Eigenvalue beamforming operation with a powerful neural network for spectral mask estimation. The enhancement system has a number of desirable properties. In particular, neither assumptions need to be made about the nature of the acoustic transfer function (e.g., being anechonic), nor does the array configuration need to be known. While the system has been originally developed to enhance speech in noisy environments, we show in this article that it is also effective in suppressing reverberation, thus leading to a generic trainable multi-channel speech enhancement system for robust speech processing. To support this claim, we consider two distinct datasets: The CHiME 3challenge, which features challenging real-world noise distortions, and the Reverbchallenge, which focuses on distortions caused by reverberation. We evaluate the system both with respect to a speech enhancement and a recognition task. For the first task we propose a new way to cope with the distortions introduced by the Generalized Eigenvalue beamformer by renormalizing the target energy for each frequency bin, and measure its effectiveness in terms of the PESQ score. For the latter we feed the enhanced signal to a strong DNN back-end and achieve state-of-the-art ASR results on both datasets. We further experiment with different network architectures for spectral mask estimation: One small feed-forward network with only one hidden layer, one Convolutional Neural Network and one bi-directional Long Short-Term Memory network, showing that even a small network is capable of delivering significant performance improvements. (C) 2017 Elsevier Ltd. All rights reserved.","keywords_author":["Robust speech recognition","Acoustic beamforming","Multi-channel speech enhancement","Deep neural network"],"keywords_other":["NETWORKS","RECOGNITION"],"max_cite":2.0,"pub_year":2017.0,"sources":"['wos']","rawkeys":["recognition","deep neural network","acoustic beamforming","robust speech recognition","networks","multi-channel speech enhancement"],"tags":["recognition","robust speech recognition","networks","convolutional neural network","multi-channel speech enhancement","acoustic beam-forming"]},{"p_id":10294,"title":"Computer-aided assessment of breast density: comparison of supervised deep learning and feature-based statistical learning","abstract":"Breast density is one of the most significant factors that is associated with cancer risk. In this study, our purpose was to develop a supervised deep learning approach for automated estimation of percentage density (PD) on digital mammograms (DMs). The input 'for processing' DMs was first log-transformed, enhanced by a multi-resolution preprocessing scheme, and subsampled to a pixel size of 800 mu m x 800 mu m from 100 mu m x 100 mu m. A deep convolutional neural network (DCNN) was trained to estimate a probability map of breast density (PMD) by using a domain adaptation resampling method. The PD was estimated as the ratio of the dense area to the breast area based on the PMD. The DCNN approach was compared to a feature-based statistical learning approach. Gray level, texture and morphological features were extracted and a least absolute shrinkage and selection operator was used to combine the features into a feature-based PMD. With approval of the Institutional Review Board, we retrospectively collected a training set of 478 DMs and an independent test set of 183 DMs from patient files in our institution. Two experienced mammography quality standards act radiologists interactively segmented PD as the reference standard. Ten-fold cross-validation was used for model selection and evaluation with the training set. With cross-validation, DCNN obtained a Dice's coefficient (DC) of 0.79 +\/- 0.13 and Pearson's correlation (r) of 0.97, whereas feature-based learning obtained DC = 0.72 +\/- 0.18 and r = 0.85. For the independent test set, DCNN achieved DC = 0.76 +\/- 0.09 and r = 0.94, while feature-based learning achieved DC = 0.62 +\/- 0.21 and r = 0.75. Our DCNN approach was significantly better and more robust than the feature-based learning approach for automated PD estimation on DMs, demonstrating its potential use for automated density reporting as well as for model-based risk prediction.","keywords_author":["breast density","deep convolutional neural network (DCNN)","statistical learning","cancer risk prediction"],"keywords_other":["TISSUE","PREDICTION","IMAGE-ANALYSIS","CONVOLUTION NEURAL-NETWORK","CANCER RISK","CLASSIFICATION","RECOGNITION","SEGMENTATION","PARENCHYMAL PATTERNS","MAMMOGRAPHIC DENSITIES"],"max_cite":1.0,"pub_year":2018.0,"sources":"['ieee', 'wos']","rawkeys":["cancer risk prediction","tissue","mammographic densities","recognition","segmentation","prediction","breast density","parenchymal patterns","classification","statistical learning","cancer risk","convolution neural-network","image-analysis","deep convolutional neural network (dcnn)"],"tags":["cancer risk prediction","tissue","recognition","segmentation","prediction","breast density","mammographic density","parenchymal patterns","classification","convolutional neural network","cancer risk","statistical learning","image analysis"]},{"p_id":10300,"title":"Learning in the machine: Random backpropagation and the deep learning channel","abstract":"Random backpropagation (RBP) is a variant of the backpropagation algorithm for training neural networks, where the transpose of the forward matrices are replaced by fixed random matrices in the calculation of the weight updates. It is remarkable both because of its effectiveness, in spite of using random matrices to communicate error information, and because it completely removes the taxing requirement of maintaining symmetric weights in a physical neural system. To better understand random backpropagation, we first connect it to the notions of local learning and learning channels. Through this connection, we derive several alternatives to RBP, including skipped RBP (SRBP), adaptive RBP (ARBP), sparse RBP, and their combinations (e.g. ASRBP) and analyze their computational complexity. We then study their behavior through simulations using the MNIST and CIFAR-10 benchmark datasets. These simulations show that most of these variants work robustly, almost as well as backpropagation, and that multiplication by the derivatives of the activation functions is important. As a follow-up, we study also the low-end of the number of bits required to communicate error information over the learning channel. We then provide partial intuitive explanations for some of the remarkable properties of RBP and its variations. Finally, we prove several mathematical results for RBP and its variants including: (1) the convergence to optimal fixed points for linear chains of arbitrary length; (2) convergence to fixed points for linear autoencoders with decorrelated data; (3) long-term existence of solutions for linear systems with a single hidden layer, and their convergence in special cases; and (4) convergence to fixed points of non-linear chains, when the derivative of the activation functions is included. (C) 2018 Elsevier B.V. All rights reserved.","keywords_author":["Backpropagation","Deep learning","Local learning","Neural networks","Deep learning","Neural networks","Backpropagation","Local learning"],"keywords_other":["Existence of Solutions","Benchmark datasets","Random matrices","MODEL","RECOGNITION","Error information","Activation functions","Neural systems","Hidden layers","Local learning"],"max_cite":0.0,"pub_year":2018.0,"sources":"['scp', 'wos']","rawkeys":["benchmark datasets","recognition","model","deep learning","neural networks","error information","hidden layers","random matrices","existence of solutions","activation functions","local learning","neural systems","backpropagation"],"tags":["benchmark datasets","recognition","model","neural networks","error information","hidden layers","machine learning","random matrices","existence of solutions","activation functions","local learning","neural systems","backpropagation"]},{"p_id":10308,"title":"Unsupervised feature learning and automatic modulation classification using deep learning model","abstract":"Recently, deep learning has received a lot of attention in many machine learning applications for its superior classification performance in speech recognition, natural language understanding and image processing. However, it still lacks attention in automatic modulation classification (AMC) until now. Here, we introduce the application of deep learning in AMC. We propose a fully connected 2 layer feed-forward deep neural network (DNN) with layerwise unsupervised pretraining for the classification of digitally modulated signals in various channel conditions. The system uses independent autoencoders (AEs) for feature learning with multiple hidden nodes. Signal information from the received samples is extracted and preprocessed via I and Q components, and formed into training input to 1st AE layer. A probabilistic based method is employed at the output layer to detect the correct modulation signal. Simulation results show that a significant improvement can be achieved compared to the other conventional machine learning methods in the literature. Moreover, we also show that our proposed method can extract the features from cyclic-stationary data samples. A good classification accuracy was achieved, even when the proposed deep network is trained and tested at different SNRs. This shows the future potential of the deep learning model for application to AMC. (C) 2017 Elsevier B.V. All rights reserved.","keywords_author":["Autoencoders","Automatic modulation classification","Deep learning networks","Digital modulation","Deep learning networks","Automatic modulation classification","Digital modulation","Autoencoders"],"keywords_other":["Natural language understanding","Automatic modulation classification","Learning network","Digital modulations","Unsupervised feature learning","Machine learning applications","NEURAL-NETWORKS","RECOGNITION","ALGORITHMS","Autoencoders","Automatic modulation classification (AMC)"],"max_cite":2.0,"pub_year":2017.0,"sources":"['ieee', 'scp', 'wos']","rawkeys":["neural-networks","automatic modulation classification (amc)","recognition","unsupervised feature learning","learning network","machine learning applications","deep learning networks","autoencoders","natural language understanding","digital modulation","digital modulations","algorithms","automatic modulation classification"],"tags":["recognition","unsupervised feature learning","learning network","machine learning applications","deep learning network","neural networks","auto encoders","natural language understanding","digital modulations","algorithms","automatic modulation classification"]},{"p_id":75857,"title":"Benchmarking Deep Networks for Predicting Residue-Specific Quality of Individual Protein Models in CASP11","abstract":"Quality assessment of a protein model is to predict the absolute or relative quality of a protein model using computational methods before the native structure is available. Single-model methods only need one model as input and can predict the absolute residue-specific quality of an individual model. Here, we have developed four novel single-model methods (Wang_deep_1, Wang_deep_2, Wang_deep_3, and Wang_SVM) based on stacked denoising autoencoders (SdAs) and support vector machines (SVMs). We evaluated these four methods along with six other methods participating in CASP11 at the global and local levels using Pearson's correlation coefficients and ROC analysis. As for residue-specific quality assessment, our four methods achieved better performance than most of the six other CASP11 methods in distinguishing the reliably modeled residues from the unreliable measured by ROC analysis; and our SdA-based method Wang_deep_1 has achieved the highest accuracy, 0.77, compared to SVM-based methods and our ensemble of an SVM and SdAs. However, we found that Wang_deep_2 and Wang_deep_3, both based on an ensemble of multiple SdAs and an SVM, performed slightly better than Wang_deep_1 in terms of ROC analysis, indicating that integrating an SVM with deep networks works well in terms of certain measurements.","keywords_author":null,"keywords_other":["RECOGNITION","SINGLE","SECONDARY STRUCTURE"],"max_cite":11.0,"pub_year":2016.0,"sources":"['wos']","rawkeys":["secondary structure","single","recognition"],"tags":["secondary structure","single","recognition"]},{"p_id":10337,"title":"LiftingNet: A Novel Deep Learning Network with Layerwise Feature Learning from Noisy Mechanical Data for Fault Classification","abstract":"The key challenge of intelligent fault diagnosis is to develop features that can distinguish different categories. Because of the unique properties of mechanical data, predetermined features based on prior knowledge are usually used as inputs for fault classification. However, proper selection of features often requires expertise knowledge and becomes more difficult and time consuming when volume of data increases. In this paper, a novel deep learning network (LiftingNet) is proposed to learn features adaptively from raw mechanical data without prior knowledge. Inspired by convolutional neural network and second generation wavelet transform, the LiftingNet is constructed to classify mechanical data even though inputs contain considerable noise and randomness. The LiftingNet consists of split layer, predict layer, update layer, pooling layer, and full-connection layer. Different kernel sizes are allowed in convolutional layers to improve learning ability. As a multilayer neural network, deep features are learned from shallow ones to represent complex structures in raw data. Feasibility and effectiveness of the LiftingNet is validated by two motor bearing datasets. Results show that the proposed method could achieve layerwise feature learning and successfully classify mechanical data even with different rotating speed and under the influence of random noise.","keywords_author":["Convolutional neural network (CNN)","deep learning","intelligent fault diagnosis","second generation wavelet transform (SGWT)","Convolutional neural network (CNN)","deep learning","intelligent fault diagnosis","second generation wavelet transform (SGWT)"],"keywords_other":["CONVOLUTIONAL NEURAL-NETWORK","Second generation wavelet transform","DIAGNOSIS","ROTATING MACHINERY","CHALLENGES","Complex structure","Intelligent fault diagnosis","Learning abilities","SUPPORT VECTOR MACHINE","Learning network","GEAR","BIG DATA","RECOGNITION","Fault classification","Convolutional neural network","Feature learning"],"max_cite":3.0,"pub_year":2018.0,"sources":"['ieee', 'scp', 'wos']","rawkeys":["big data","complex structure","learning network","challenges","convolutional neural network","second generation wavelet transform","feature learning","support vector machine","learning abilities","fault classification","diagnosis","recognition","deep learning","convolutional neural network (cnn)","intelligent fault diagnosis","gear","second generation wavelet transform (sgwt)","convolutional neural-network","rotating machinery"],"tags":["diagnosis","recognition","big data","complex structure","learning network","machine learning","second generation wavelet transform (sgwt)","challenges","gear","convolutional neural network","learning abilities","feature learning","intelligent fault diagnosis","fault classification","rotating machinery"]},{"p_id":10344,"title":"Feature learning and change feature classification based on deep learning for ternary change detection in SAR images","abstract":"Ternary change detection aims to detect changes and group the changes into positive change and negative change. It is of great significance in the joint interpretation of spatial-temporal synthetic aperture radar images. In this study, sparse autoencoder, convolutional neural networks (CNN) and unsupervised clustering are combined to solve ternary change detection problem without any supervison. Firstly, sparse autoencoder is used to transform log-ratio difference image into a suitable feature space for extracting key changes and suppressing outliers and noise. And then the learned features are clustered into three classes, which are taken as the pseudo labels for training a CNN model as change feature classifier. The reliable training samples for CNN are selected from the feature maps learned by sparse autoencoder with certain selection rules. Having training samples and the corresponding pseudo labels, the CNN model can be trained by using back propagation with stochastic gradient descent. During its training procedure, CNN is driven to learn the concept of change, and more powerful model is established to distinguish different types of changes. Unlike the traditional methods, the proposed framework integrates the merits of sparse autoencoder and CNN to learn more robust difference representations and the concept of change for ternary change detection. Experimental results on real datasets validate the effectiveness and superiority of the proposed framework. (C) 2017 International Society for Photogrammetry and Remote Sensing, Inc. (ISPRS). Published by Elsevier B.V. All rights reserved.","keywords_author":["Convolutional neural networks","Deep learning","Representation learning","Sparse autoencoder","Synthetic aperture radar","Ternary change detection","Ternary change detection","Deep learning","Synthetic aperture radar","Representation learning","Sparse autoencoder","Convolutional neural networks"],"keywords_other":["LOG-RATIO IMAGE","Stochastic gradient descent","Representation learning","REPRESENTATION","Change detection","Unsupervised clustering","NEURAL-NETWORKS","Feature classification","RECOGNITION","MODEL","Convolutional neural network","Auto encoders","Joint interpretation"],"max_cite":3.0,"pub_year":2017.0,"sources":"['ieee', 'scp', 'wos']","rawkeys":["neural-networks","recognition","convolutional neural networks","model","deep learning","synthetic aperture radar","ternary change detection","unsupervised clustering","auto encoders","representation","feature classification","representation learning","change detection","joint interpretation","log-ratio image","convolutional neural network","stochastic gradient descent","sparse autoencoder"],"tags":["recognition","model","contrastive divergence","neural networks","synthetic aperture radar","ternary change detection","unsupervised clustering","machine learning","auto encoders","feature classification","representation","representation learning","stacked autoencoders","joint interpretation","log-ratio image","convolutional neural network","stochastic gradient descent"]},{"p_id":92270,"title":"Online multi-layer dictionary pair learning for visual classification","abstract":"Classifier training plays an important role in image classification, while a good classifier could more effectively exploit the discriminative information of input features to separate the difficult samples. Inspired by the recent advance of representation based classifiers and the success of multi-layer architectures in visual recognition, we propose a multi-layer dictionary pair learning based classifier to enhance the image classification performance. With the multi-layer structure and a nonlinear feature transform in each layer, the proposed classifier learning model could accumulate stronger discrimination capability than the previous single-layer representation based classifiers. Furthermore, to make our learning model applicable to datasets with a larger amount of samples, we propose an online training algorithm which updates model parameters with data batches. The so-called online multi-layer dictionary pair learning (OMDPL) method is evaluated on benchmark image classification datasets. With the same input features, OMDPL exhibits better classification performance than other popular classifiers. (C) 2018 Elsevier Ltd. All rights reserved.","keywords_author":["Dictionary pair learning","Image classification","Online training","Multi-layer learning"],"keywords_other":["REPRESENTATION","RECOGNITION"],"max_cite":0.0,"pub_year":2018.0,"sources":"['wos']","rawkeys":["recognition","representation","dictionary pair learning","multi-layer learning","image classification","online training"],"tags":["recognition","representation","dictionary pair learning","multi-layer learning","image classification","online training"]},{"p_id":100470,"title":"Fast and Accurate Approaches for Large-Scale, Automated Mapping of Food Diaries on Food Composition Tables","abstract":"Aim of Study: The use of weighed food diaries in nutritional studies provides a powerful method to quantify food and nutrient intakes Yet, mapping these records onto food composition tables (FCTs) is a challenging, time-consuming and error-prone process Experts make this effort manually and no automation has been previously proposed Our study aimed to assess automated approaches to map food items onto FCTs.","keywords_author":["fuzzy matching","food composition tables","food diaries","macronutrient","food mapping","dietary studies"],"keywords_other":["RECOGNITION","DATASET","EUROPE"],"max_cite":0.0,"pub_year":2018.0,"sources":"['wos']","rawkeys":["food composition tables","recognition","dataset","europe","macronutrient","food mapping","dietary studies","food diaries","fuzzy matching"],"tags":["food composition tables","recognition","europe","macronutrient","food mapping","data sets","dietary studies","food diaries","fuzzy matching"]},{"p_id":10359,"title":"A deep-learning based feature hybrid framework for spatiotemporal saliency detection inside videos","abstract":"Although research on detection of saliency and visual attention has been active over recent years, most of the existing work focuses on still image rather than video based saliency. In this paper, a deep learning based hybrid spatiotemporal saliency feature extraction framework is proposed for saliency detection from video footages. The deep learning model is used for the extraction of high-level features from raw video data, and they are then integrated with other high-level features. The deep learning network has been found extremely effective for extracting hidden features than that of conventional handcrafted methodology. The effectiveness for using hybrid high-level features for saliency detection in video is demonstrated in this work. Rather than using only one static image, the proposed deep learning model take several consecutive frames as input and both the spatial and temporal characteristics are considered when computing saliency maps. The efficacy of the proposed hybrid feature framework is evaluated by five databases with human gaze complex scenes. Experimental results show that the proposed model outperforms five other state-of-the-art video saliency detection approaches. In addition, the proposed framework is found useful for other video content based applications such as video highlights. As a result, a large movie clip dataset together with labeled video highlights is generated. (C) 2018 Elsevier B.V. All rights reserved.","keywords_author":["Convolutional neural networks","Human gaze","Movie highlight extraction","Spatiotemporal saliency detection","Visual dispersion","Spatiotemporal saliency detection","Human gaze","Convolutional neural networks","Visual dispersion","Movie highlight extraction"],"keywords_other":["Saliency detection","Spatiotemporal saliency","VISUAL-ATTENTION","DIMENSIONALITY","Human gaze","SEARCH","High-level features","MODEL","NEURAL-NETWORKS","RECOGNITION","DYNAMIC SCENES","TIME","Convolutional neural network","FIXATION SELECTION","Temporal characteristics","Spatiotemporal saliency detections","Visual Attention","EYE-MOVEMENTS"],"max_cite":2.0,"pub_year":2018.0,"sources":"['ieee', 'scp', 'wos']","rawkeys":["dimensionality","search","visual dispersion","saliency detection","convolutional neural network","eye-movements","neural-networks","spatiotemporal saliency detection","visual-attention","human gaze","time","high-level features","recognition","convolutional neural networks","fixation selection","temporal characteristics","visual attention","spatiotemporal saliency","spatiotemporal saliency detections","model","movie highlight extraction","dynamic scenes"],"tags":["dimensionality","search","visual dispersion","saliency detection","convolutional neural network","human gaze","time","high-level features","recognition","fixation selection","neural networks","temporal characteristics","visual attention","spatiotemporal saliency","spatiotemporal saliency detections","model","movie highlight extraction","dynamic scenes","eye movements"]},{"p_id":18552,"title":"Obstructive sleep apnea after weight loss: A clinical trial comparing gastric bypass and intensive lifestyle intervention","abstract":"Introduction: Few studies have compared the effect of surgical and conservative weight loss strategies on obstructive sleep apnea (OSA). We hypothesized that Roux-en-Y gastric bypass (RYGB) would be more effective than intensive lifestyle intervention (ILI) at reducing the prevalence and severity of OSA (apnea-hypopnea-index [AHI] \u2265 5 events\/hour). Methods: A total of 133 morbidly obese subjects (93 females) were treated with either a 1-year ILI-program (n = 59) or RYGB (n = 74) and underwent repeated sleep recordings with a portable somnograph (Embletta). Results: Participants had a mean (SD) age of 44.7(10.8) years, BMI 45.1(5.7) kg\/m2, and AHI 17.1(21.4) events\/hour. Eighty-four patients (63%) had OSA. The average weight loss was 8% in the ILI-group and 30% in the RYGB-group (p < 0.001). The mean (95%CI) AHI reduced in both treatment groups, although significantly more in the RYGB-group (AHI change -6.0 [ILI] vs -13.1 [RYGB]), between group difference 7.2 (1.3, 13.0), p = 0.017. Twenty-nine RYGB-patients (66%) had remission of OSA, compared to 16 ILI-patients (40%), p = 0.028. At follow-up, after adjusting for age, gender, and baseline AHI, the RYGB-patients had significantly lower adjusted odds for OSA than the ILI-patients - OR (95% CI) 0.33 (0.14, 0.81), p = 0.015. After further adjustment for BMI change, treatment group difference was no longer statistically significant - OR (95% CI) 1.31 (0.32, 5.35), p = 0.709. Conclusion: Our study demonstrates that RYGB was more effective than ILI at reducing the prevalence and severity of OSA. However, our analysis also suggests that weight loss, rather than the surgical procedure per se, explains the beneficial effects.","keywords_author":["Bariatric surgery","General","Obstructive sleep apnea","Weight loss"],"keywords_other":["bariatric surgery","weight loss","obstructive sleep apnea","General"],"max_cite":24.0,"pub_year":2013.0,"sources":"['scp']","rawkeys":["bariatric surgery","weight loss","obstructive sleep apnea","general"],"tags":["bariatric surgery","weight loss","obstructive sleep apnea","recognition"]},{"p_id":10358,"title":"Adaptable deep learning structures for object labeling\/tracking under dynamic visual environments","abstract":"In this paper, we propose a self-adaptive deep neural network architecture suitable for object tracking and labelling. In particular, an adaptation mechanism is introduced that automatically evaluates the performance of the network and then updates its weights to fit the current environmental conditions. The retraining algorithm trusts as much as possible the current conditions (discriminative constraints), while simultaneously providing a minimal degradation of the already obtained knowledge of the network (generative constraints). The underline assumption is that a small weight perturbation is adequate to modify the performance of the network to the new situations, without this constraint implying a small modification of the network output due to the highly non-linear surface that the network models. Under this assumption, we propose a computationally efficient re-training algorithm to tackle the variations of the visual environment, requiring a small number of labelled samples for the adaptation. Weight updating is combined with an unsupervised learning paradigm, implemented through stacked autoencoders, in order to improve convergence, stability and performance of the object tracking and labeling process by propagating the sensory inputs into deep level of hierarchies and therefore structuring the inputs from low representations to more abstract forms. Approximates of current visual environment are provided through a dynamic tracker that combines motion and learning features to automatically create few confident labeled data. The proposed retraining scheme is computationally efficient and able to model non-stationary environments, like the ones appeared in real-life computer vision application scenarios. Experimental results and comparisons are provided on video datasets of very complicated visual content, monitoring industrial workflows of car assembly within a manufactory. The results indicates that our self-adaptive deep neural network architecture is able to correctly label and separate foreground objects from background even under severe visual changes, such as occlusion, illumination variations and change of camera views, and within real time computational constraints.","keywords_author":["Deep learning","Industrial workflows","Neural adaptation","Object tracking and labeling","Stacked autoencoders","Deep learning","Neural adaptation","Stacked autoencoders","Object tracking and labeling","Industrial workflows"],"keywords_other":["Computational constraints","Computer vision applications","TRACKING","VIDEO","Work-flows","Neural adaptation","NEURAL-NETWORKS","RECOGNITION","Computationally efficient","Object Tracking","Non-stationary environment","Autoencoders"],"max_cite":0.0,"pub_year":2018.0,"sources":"['ieee', 'scp', 'wos']","rawkeys":["neural-networks","industrial workflows","object tracking and labeling","object tracking","recognition","deep learning","computer vision applications","neural adaptation","stacked autoencoders","tracking","video","non-stationary environment","autoencoders","computational constraints","work-flows","computationally efficient"],"tags":["workflow","industrial workflows","object tracking and labeling","object tracking","recognition","neural networks","computer vision applications","machine learning","auto encoders","neural adaptation","stacked autoencoders","tracking","non-stationary environment","video","computational constraints","computationally efficient"]},{"p_id":10361,"title":"Face age estimation approach based on deep learning and principle component analysis","abstract":"This paper presents an approach for age estimation based on faces through classifying facial images into predefined age-groups. However, a task such as the one at hand faces several difficulties because of the different aspects of every single person. Factors like exposure, weather, gender and lifestyle all come into play. While some trends are similar for faces from a similar age group, it is problematic to distinguish the aging aspects for every age group. This paper's concentration is in four chosen age groups where the estimation takes place. We employed a fast and effective machine learning method: deep learning so that it could solve the age categorization issue. Principal component analysis (PCA) was used for extracting features and reducing face image. Age estimation was applied to three different aging datasets from Morph and experimental results are reported to validate its efficiency and robustness. Eventually, it is evident from the results that the current approach has achieved high classification results compared with support vector machine (SVM) and k-nearest neighbors (K-NN).","keywords_author":["Age estimation","Deep learning","K-NN","Principal component analysis","Support vector machine","Deep learning","principal component analysis","support vector machine","K-NN","age estimation"],"keywords_other":["NETWORKS","HYPERSPECTRAL DATA","CLASSIFICATION","LOCAL BINARY PATTERNS","RECOGNITION"],"max_cite":0.0,"pub_year":2018.0,"sources":"['scp', 'wos']","rawkeys":["principal component analysis","recognition","hyperspectral data","deep learning","age estimation","networks","classification","local binary patterns","k-nn","support vector machine"],"tags":["principal component analysis","recognition","hyperspectral data","machine learning","age estimation","networks","classification","local binary patterns","k-nearest neighbors"]},{"p_id":10369,"title":"A Deep Learning Approach for Intrusion Detection Using Recurrent Neural Networks","abstract":"Intrusion detection plays an important role in ensuring information security, and the key technology is to accurately identify various attacks in the network. In this paper, we explore how to model an intrusion detection system based on deep learning, and we propose a deep learning approach for intrusion detection using recurrent neural networks (RNN-IDS). Moreover, we study the performance of the model in binary classification and multiclass classification, and the number of neurons and different learning rate impacts on the performance of the proposed model. We compare it with those of J48, artificial neural network, random forest, support vector machine, and other machine learning methods proposed by previous researchers on the benchmark data set. The experimental results show that RNN-IDS is very suitable for modeling a classification model with high accuracy and that its performance is superior to that of traditional machine learning classification methods in both binary and multiclass classification. The RNN-IDS model improves the accuracy of the intrusion detection and provides a new research method for intrusion detection.","keywords_author":["deep learning","intrusion detection","machine learning","Recurrent neural networks","RNN-IDS","Recurrent neural networks","RNN-IDS","intrusion detection","deep learning","machine learning"],"keywords_other":["Binary classification","Computational model","Multi-class classification","Classification models","Intrusion Detection Systems","RECOGNITION","Machine learning methods","RNN-IDS","Machine learning classification"],"max_cite":4.0,"pub_year":2017.0,"sources":"['ieee', 'scp', 'wos']","rawkeys":["machine learning methods","multi-class classification","recognition","deep learning","intrusion detection","intrusion detection systems","machine learning","binary classification","rnn-ids","recurrent neural networks","computational model","classification models","machine learning classification"],"tags":["computational modeling","machine learning methods","recognition","neural networks","intrusion detection systems","machine learning","binary classification","rnn-ids","multi-class classification","classification models","machine learning classification"]},{"p_id":10371,"title":"Transfer Learning with Deep Convolutional Neural Network for SAR Target Classification with Limited Labeled Data","abstract":"t Tremendous progress has been made in object recognition with deep convolutional neural networks (CNNs), thanks to the availability of large-scale annotated dataset. With the ability of learning highly hierarchical image feature extractors, deep CNNs are also expected to solve the Synthetic Aperture Radar (SAR) target classification problems. However, the limited labeled SAR target data becomes a handicap to train a deep CNN. To solve this problem, we propose a transfer learning based method, making knowledge learned from sufficient unlabeled SAR scene images transferrable to labeled SAR target data. We design an assembled CNN architecture consisting of a classification pathway and a reconstruction pathway, together with a feedback bypass additionally. Instead of training a deep network with limited dataset from scratch, a large number of unlabeled SAR scene images are used to train the reconstruction pathway with sacked convolutional auto-encoders (SCAE) at first. Then, these pre-trained convolutional layers are reused to transfer knowledge to SAR target classification tasks, with feedback bypass introducing the reconstruction loss simultaneously. The experimental results demonstrate that transfer learning leads to a better performance in the case of scarce labeled training data and the additional feedback bypass with reconstruction loss helps to boost the capability of classification pathway.","keywords_author":["SAR target recognition","deep CNNs","transfer learning","stacked convolutional auto-encoders"],"keywords_other":["FEATURES","RECOGNITION","MONOGENIC SIGNAL","JOINT SPARSE REPRESENTATION","IMAGES"],"max_cite":7.0,"pub_year":2017.0,"sources":"['ieee', 'wos']","rawkeys":["recognition","images","sar target recognition","features","transfer learning","deep cnns","stacked convolutional auto-encoders","monogenic signal","joint sparse representation"],"tags":["recognition","images","sar target recognition","features","transfer learning","deep cnns","stacked convolutional auto-encoders","monogenic signal","joint sparse representation"]},{"p_id":43145,"title":"A novel finger vein verification system based on two-stream convolutional network learning","abstract":"\u00a9 2018 Convolutional neural networks have been proven to have strong feature representation ability; however, they often require large training samples and high computation that are infeasible for real-time finger vein verification. To address this limitation, we propose a lightweight deep-learning framework for finger vein verification. First, we designed a lightweight two-channel network that has only three convolution layers for finger vein verification. Then, we extracted the mini-ROI from the original image to better solve the displacement problem based on the evaluation of the two-channel network. Finally, we present a two-stream network to integrate the original image and the mini-ROI that achieves results superior to the current state of the art on both the MMCBNU and SDUMLA databases.","keywords_author":["Deep learning","Finger vein verification","Metric network","Two-stream network","Finger vein verification","Deep learning","Metric network","Two-stream network"],"keywords_other":["Two-stream","MAP","Convolutional networks","IDENTIFICATION","Learning frameworks","State of the art","Feature representation","RECOGNITION","Convolutional neural network","Verification systems","FEATURE-EXTRACTION","PATTERNS","Finger vein"],"max_cite":0.0,"pub_year":2018.0,"sources":"['ieee', 'scp', 'wos']","rawkeys":["identification","metric network","recognition","two-stream","deep learning","feature-extraction","map","state of the art","convolutional networks","patterns","finger vein verification","learning frameworks","verification systems","convolutional neural network","two-stream network","feature representation","finger vein"],"tags":["identification","metric network","recognition","two-stream","map","machine learning","patterns","state of the art","finger vein verification","learning frameworks","verification systems","feature extraction","convolutional neural network","two-stream network","feature representation","finger vein"]},{"p_id":10381,"title":"Unsupervised learning to detect loops using deep neural networks for visual SLAM system","abstract":"This paper is concerned of the loop closure detection problem for visual simultaneous localization and mapping systems. We propose a novel approach based on the stacked denoising auto-encoder (SDA), a multi-layer neural network that autonomously learns an compressed representation from the raw input data in an unsupervised way. Different with the traditional bag-of-words based methods, the deep network has the ability to learn the complex inner structures in image data, while no longer needs to manually design the visual features. Our approach employs the characteristics of the SDA to solve the loop detection problem. The workflow of training the network, utilizing the features and computing the similarity score is presented. The performance of SDA is evaluated by a comparison study with Fab-map 2.0 using data from open datasets and physical robots. The results show that SDA is feasible for detecting loops at a satisfactory precision and can therefore provide an alternative way for visual SLAM systems.","keywords_author":["Simultaneous localization and mapping (SLAM)","Loop closure detection","Stacked denoising auto-encoder","Deep neural network"],"keywords_other":["ASSOCIATION","FAB-MAP","FEATURES","ENVIRONMENTS","LARGE-SCALE","GRAPH-SLAM","RECOGNITION","TIME","SIMULTANEOUS LOCALIZATION","APPEARANCE"],"max_cite":6.0,"pub_year":2017.0,"sources":"['ieee', 'wos']","rawkeys":["simultaneous localization and mapping (slam)","association","deep neural network","fab-map","features","recognition","simultaneous localization","appearance","large-scale","time","environments","graph-slam","loop closure detection","stacked denoising auto-encoder"],"tags":["association","recognition","fab-map","features","simultaneous localization","appearance","large-scale","time","graph-slam","robotics","loop closure detection","environment","convolutional neural network","stacked denoising autoencoder"]},{"p_id":10383,"title":"DeepPicker: A deep learning approach for fully automated particle picking in cryo-EM","abstract":"Particle picking is a time-consuming step in single-particle analysis and often requires significant interventions from users, which has become a bottleneck for future automated electron cryo-microscopy (cryo-EM). Here we report a deep learning framework, called DeepPicker, to address this problem and fill the current gaps toward a fully automated cryo-EM pipeline. DeepPicker employs a novel cross-molecule training strategy to capture common features of particles from previously-analyzed micrographs, and thus does not require any human intervention during particle picking. Tests on the recently-published cryo-EM data of three complexes have demonstrated that our deep learning based scheme can successfully accomplish the human-level particle picking process and identify a sufficient number of particles that are comparable to those picked manually by human experts. These results indicate that DeepPicker can provide a practically useful tool to significantly reduce the time and manual effort spent in single-particle analysis and thus greatly facilitate high-resolution cryo-EM structure determination. DeepPicker is released as an open-source program, which can be downloaded from https:\/\/github.com\/nejyeah\/DeepPicker-python. (C) 2016 Elsevier Inc. All rights reserved.","keywords_author":["Cryo-EM","Particle picking","Automation","Deep learning"],"keywords_other":["SELECTION","CRYOMICROSCOPY","MACHINE","ELECTRON-MICROGRAPHS","SYSTEM","HIV-1 ENVELOPE GLYCOPROTEINS","NEURAL-NETWORKS","RECOGNITION","CRYOELECTRON MICROSCOPY","RESOLUTION"],"max_cite":7.0,"pub_year":2016.0,"sources":"['ieee', 'wos']","rawkeys":["cryomicroscopy","neural-networks","electron-micrographs","recognition","particle picking","deep learning","machine","resolution","system","automation","cryo-em","selection","hiv-1 envelope glycoproteins","cryoelectron microscopy"],"tags":["cryomicroscopy","electron-micrographs","recognition","particle picking","automated","neural networks","machine","machine learning","resolution","system","cryo-em","selection","hiv-1 envelope glycoproteins","cryoelectron microscopy"]},{"p_id":26769,"title":"Behavlets: a method for practical player modelling using psychology-based player traits and domain specific features","abstract":"\u00c2\u00a9 2016, The Author(s).As player demographics broaden it has become important to understand variation in player types. Improved player models can help game designers create games that accommodate a range of playing styles, and may also facilitate the design of systems that detect the currently-expressed player type and adapt dynamically in real-time. Existing approaches can model players, but most focus on tracking and classifying behaviour based on simple functional metrics such as deaths, specific choices, player avatar attributes, and completion times. We describe a novel approach which seeks to leverage expert domain knowledge using a theoretical framework linking behaviour and game design patterns. The aim is to derive features of play from sequences of actions which are intrinsically informative about behaviour\u2014which, because they are directly interpretable with respect to psychological theory of behaviour, we name \u2018Behavlets\u2019. We present the theoretical underpinning of this approach from research areas including psychology, temperament theory, player modelling, and game composition. The Behavlet creation process is described in detail; illustrated using a clone of the well-known game Pac-Man, with data gathered from 100 participants. A workshop-based evaluation study is also presented, where nine game design expert participants were briefed on the Behavlet concepts and requisite models, and then attempted to apply the method to games of the well-known first\/third-person shooter genres, exemplified by \u2018Gears of War\u2019, (Microsoft). The participants found 139 Behavlet concepts mapping from behavioural preferences of the temperament types, to design patterns of the shooter genre games. We conclude that the Behavlet approach has significant promise, is complementary to existing methods and can improve theoretical validity of player models.","keywords_author":["Behavlet","Game design patterns","Machine learning","Player modelling","Psychology","Temperament theory"],"keywords_other":["Game design","Concepts mappings","Behavlet","Psychology","Psychological theory","Behavioural preferences","Theoretical framework","Temperament theory"],"max_cite":6.0,"pub_year":2016.0,"sources":"['scp']","rawkeys":["game design","psychological theory","temperament theory","behavlet","concepts mappings","machine learning","behavioural preferences","theoretical framework","psychology","player modelling","game design patterns"],"tags":["game design","psychological theory","recognition","behavlet","machine learning","behavioural preferences","player modeling","game design patterns","theoretical framework","temperament theory","concept maps"]},{"p_id":10396,"title":"Clinical report guided retinal microaneurysm detection with multi-sieving deep learning","abstract":"Timely detection and treatment of microaneurysms is a critical step to prevent the development of vision-threatening eye diseases such as diabetic retinopathy. However, detecting microaneurysms in fundus images is a highly challenging task due to the low image contrast, misleading cues of other red lesions, and the large variation of imaging conditions. Existing methods tend to fail in face of the large intra-class variation and small inter-class variations for microaneurysm detection in fundus images. Recently, hybrid text\/image mining computer-aided diagnosis systems have emerged to offer a promise of bridging the semantic gap between images and diagnostic information. In this paper, we focus on developing an interleaved deep mining technique to cope intelligently with the unbalanced microaneurysm detection problem. Specifically, we present a clinical report guided multi-sieving convolutional neural network, which leverages a small amount of supervised information in clinical reports to identify the potential microaneurysm regions via the image-to-text mapping in the feature space. These potential microaneurysm regions are then interleaved with fundus image information for multi-sieving deep mining in a highly unbalanced classification problem. Critically, the clinical reports are employed to bridge the semantic gap between low-level image features and high-level diagnostic information. We build an efficient microaneurysm detection framework based on the hybrid text\/image interleaving and validate its performance on challenging clinical data sets acquired from diabetic retinopathy patients. Extensive evaluations are carried out in terms of fundus detection and classification. Experimental results show that our framework achieves 99.7% precision and 87.8% recall, comparing favorably with the state-of-the-art algorithms. Integration of expert domain knowledge and image information demonstrates the feasibility of reducing the difficulty of training classifiers under extremely unbalanced data distributions.","keywords_author":["clinical reports","deep learning","Diabetic retinopathy","fundus image analysis","microaneurysm detection","multi-sieving CNN","Diabetic retinopathy","fundus image analysis","multi-sieving CNN","microaneurysm detection","clinical reports","deep learning"],"keywords_other":["Clinical Reports","Lesions","Retinopathy","AUTOMATIC DETECTION","Retina","Hemorrhaging","Image color analysis","FUNDUS PHOTOGRAPHS","Microaneurysms","Multi-Sieving CNN","RECOGNITION","DICTIONARY","SYSTEM","DIABETIC-RETINOPATHY","Diabetic retinopathy","Fundus image"],"max_cite":0.0,"pub_year":2018.0,"sources":"['ieee', 'scp', 'wos']","rawkeys":["fundus photographs","dictionary","microaneurysms","retinopathy","fundus image","image color analysis","system","hemorrhaging","recognition","multi-sieving cnn","deep learning","retina","fundus image analysis","clinical reports","lesions","automatic detection","microaneurysm detection","diabetic retinopathy","diabetic-retinopathy"],"tags":["hemorrhage","microaneurysms","clinical reports","recognition","fundus image","retinopathy","multi-sieving cnn","machine learning","image color analysis","system","fundus photographs","retina","lesions","automatic detection","microaneurysm detection","fundus image analysis","dictionaries","dimensionality reduction"]},{"p_id":18604,"title":"Facebook use and depressive symptomatology: Investigating the role of neuroticism and extraversion in youth","abstract":"The popularity of social networking sites, such as Facebook, has increased rapidly over the past decade, especially among youth. Consequently, the impact of Facebook use on mental health problems (e.g., depressive symptomatology) has become a recent area of concern. Yet, evidence for such a link has been mixed and factors that contribute to heterogeneity of findings have not been identified. In this study, we examined whether the association between Facebook use and depressive symptoms is moderated by individual factors (i.e., personality and sex). To this end, we measured Facebook use, depressive symptoms, and personality domains (i.e., extroversion and neuroticism) among 237 young adults. No direct association was found between Facebook use and depressive symptoms. However, for females with high neuroticism, more frequent Facebook use was associated with lower depressive symptoms. Our findings suggest a complex relationship between Facebook use and depressive symptomatology that appears to vary by sex and personality. Facebook use may be protective against depressive symptoms for female users with high levels of neuroticism, while Facebook use may be unrelated to depressive symptoms among males.\u00a9 2014 Published by Elsevier Ltd.","keywords_author":["Depression","Extraversion","Facebook","Neuroticism","Sex"],"keywords_other":["Extraversion","Neuroticism","Facebook","Depression","Sex"],"max_cite":23.0,"pub_year":2014.0,"sources":"['scp']","rawkeys":["facebook","neuroticism","extraversion","sex","depression"],"tags":["facebook","recognition","neuroticism","sex","depression"]},{"p_id":10413,"title":"Deep unsupervised learning on a desktop PC: a primer for cognitive scientists","abstract":"Deep belief networks hold great promise for the simulation of human cognition because they show how structured and abstract representations may emerge from probabilistic unsupervised learning. These networks build a hierarchy of progressively more complex distributed representations of the sensory data by fitting a hierarchical generative model. However, learning in deep networks typically requires big datasets and it can involve millions of connection weights, which implies that simulations on standard computers are unfeasible. Developing realistic, medium-to-large-scale learning models of cognition would therefore seem to require expertise in programing parallel-computing hardware, and this might explain why the use of this promising approach is still largely confined to the machine learning community. Here we show how simulations of deep unsupervised learning can be easily performed on a desktop PC by exploiting the processors of low cost graphic cards (graphic processor units) without any specific programing effort, thanks to the use of high-level programming routines (available in MATLAB or Python). We also show that even an entry-level graphic card can outperform a small high-performance computing cluster in terms of learning time and with no loss of learning quality. We therefore conclude that graphic card implementations pave the way for a widespread use of deep learning among cognitive scientists for modeling cognition and behavior.","keywords_author":["deep neural networks","unsupervised learning","hierarchical generative models","cognitive modeling","parallel-computing architectures","GPUs","MPI","computer cluster"],"keywords_other":["NETS","ALGORITHM","NEURAL-NETWORKS","RECOGNITION","EMERGENCE","GENERATIVE MODELS"],"max_cite":10.0,"pub_year":2013.0,"sources":"['wos']","rawkeys":["neural-networks","algorithm","computer cluster","recognition","cognitive modeling","deep neural networks","gpus","emergence","unsupervised learning","hierarchical generative models","mpi","nets","parallel-computing architectures","generative models"],"tags":["generative model","graphics processing units","recognition","cognitive modeling","neural networks","emergence","parallel computing architecture","unsupervised learning","convolutional neural network","hierarchical generative models","algorithms","mpi","computer clusters","nets"]},{"p_id":34989,"title":"On kNN Classification and Local Feature Based Similarity Functions","abstract":"In this paper we consider the problem of image content recognition and we address it by using local features and kNN based classification strategies. Specifically, we define a number of image similarity functions relying on local features comparing their performance when used with a kNN classifier. Furthermore, we compare the whole image similarity approach with a novel two steps kNN based classification strategy that first assigns a label to each local feature in the document to be classified and then uses this information to assign a label to the whole image. We perform our experiments solving the task of recognizing landmarks in photos. \u00a9 Springer-Verlag Berlin Heidelberg 2013.","keywords_author":["Image classification","Landmarks","Local features","Machine learning","Pattern recognition","Recognition"],"keywords_other":["Recognition","Landmarks","K-NN classifications","Image similarity","Image content recognition","K-NN classifier","Local feature"],"max_cite":2.0,"pub_year":2013.0,"sources":"['scp']","rawkeys":["recognition","k-nn classifier","local feature","local features","image classification","machine learning","image content recognition","landmarks","pattern recognition","image similarity","k-nn classifications"],"tags":["recognition","k-nn classifier","local feature","image classification","machine learning","image content recognition","landmarks","pattern recognition","image similarity","k-nn classifications"]},{"p_id":10418,"title":"Automatically identifying, counting, and describing wild animals in camera-trap images with deep learning","abstract":"Having accurate, detailed, and up-to-date information about the location and behavior of animals in the wild would improve our ability to study and conserve ecosystems. We investigate the ability to automatically, accurately, and inexpensively collect such data, which could help catalyze the transformation of many fields of ecology, wildlife biology, zoology, conservation biology, and animal behavior into \"big data\" sciences. Motion-sensor \"camera traps\" enable collecting wildlife pictures inexpensively, unobtrusively, and frequently. However, extracting information from these pictures remains an expensive, time-consuming, manual task. We demonstrate that such information can be automatically extracted by deep learning, a cutting-edge type of artificial intelligence. We train deep convolutional neural networks to identify, count, and describe the behaviors of 48 species in the 3.2 million-image Snapshot Serengeti dataset. Our deep neural networks automatically identify animals with >93.8% accuracy, and we expect that number to improve rapidly in years to come. More importantly, if our system classifies only images it is confident about, our system can automate animal identification for 99.3% of the data while still performing at the same 96.6% accuracy as that of crowdsourced teams of human volunteers, saving >8.4 y (i.e., >17,000 h at 40 h\/wk) of human labeling effort on this 3.2 million-image dataset. Those efficiency gains highlight the importance of using deep neural networks to automate data extraction from camera-trap images, reducing a roadblock for this widely used technology. Our results suggest that deep learning could enable the inexpensive, unobtrusive, high-volume, and even real-time collection of a wealth of information about vast numbers of animals in the wild.","keywords_author":["deep learning","deep neural networks","artificial intelligence","camera-trap images","wildlife ecology"],"keywords_other":["FEAR","MANAGEMENT","LANDSCAPE","SOFTWARE","MODEL","RECOGNITION"],"max_cite":0.0,"pub_year":2018.0,"sources":"['wos']","rawkeys":["artificial intelligence","fear","recognition","model","wildlife ecology","deep learning","deep neural networks","camera-trap images","software","landscape","management"],"tags":["fear","recognition","model","wildlife ecology","camera-trap images","machine learning","software","landscape","convolutional neural network","management"]},{"p_id":10437,"title":"Deep learning-based artificial vision for grasp classification in myoelectric hands","abstract":"Objective. Computer vision-based assistive technology solutions can revolutionise the quality of care for people with sensorimotor disorders. The goal of this work was to enable trans-radial amputees to use a simple, yet efficient, computer vision system to grasp and move common household objects with a two-channel myoelectric prosthetic hand. Approach. We developed a deep learning-based artificial vision system to augment the grasp functionality of a commercial prosthesis. Our main conceptual novelty is that we classify objects with regards to the grasp pattern without explicitly identifying them or measuring their dimensions. A convolutional neural network (CNN) structure was trained with images of over 500 graspable objects. For each object, 72 images, at 5 degrees intervals, were available. Objects were categorised into four grasp classes, namely: pinch, tripod, palmar wrist neutral and palmar wrist pronated. The CNN setting was first tuned and tested offline and then in realtime with objects or object views that were not included in the training set. Main results. The classification accuracy in the offline tests reached 85% for the seen and 75% for the novel objects; reflecting the generalisability of grasp classification. We then implemented the proposed framework in realtime on a standard laptop computer and achieved an overall score of 84% in classifying a set of novel as well as seen but randomly-rotated objects. Finally, the system was tested with two trans-radial amputee volunteers controlling an i-limb Ultra (TM) prosthetic hand and a motion control (TM) prosthetic wrist; augmented with a webcam. After training, subjects successfully picked up and moved the target objects with an overall success of up to 88%. In addition, we show that with training, subjects' performance improved in terms of time required to accomplish a block of 24 trials despite a decreasing level of visual feedback. Significance. The proposed design constitutes a substantial conceptual improvement for the control of multi-functional prosthetic hands. We show for the first time that deep-learning based computer vision systems can enhance the grip functionality of myoelectric hands considerably.","keywords_author":["myoelectric hand prosthesis","convolutional neural network","grasp classification"],"keywords_other":["SIGNAL CLASSIFICATION","RECOGNITION","FEEDBACK","PROSTHESIS","OBJECTS","EMG","SURFACE"],"max_cite":8.0,"pub_year":2017.0,"sources":"['ieee', 'wos']","rawkeys":["recognition","emg","signal classification","objects","surface","convolutional neural network","myoelectric hand prosthesis","feedback","prosthesis","grasp classification"],"tags":["surfaces","recognition","electromyography","signal classification","objects","convolutional neural network","myoelectric hand prosthesis","feedback","prosthesis","grasp classification"]},{"p_id":84168,"title":"Correlation-Assisted Imbalance Multimedia Concept Mining and Retrieval","abstract":"In the past decades, we have witnessed an explosion of multimedia data, especially with the development of social media websites and blooming popularity of smart devices. As a result, multimedia semantic concept mining and retrieval whose objective is to mine useful information from the large amount of multimedia data including texts, images, and videos has become more and more important. The huge amount of multimedia data and the semantic gap between low-level features and high-level semantic concepts have made it even more challenging. To address these challenges, the correlations among the classes can provide important context cues to help bridge the semantic gap. Meanwhile, many real-world datasets do not have uniform class distributions while the minority instances actually represent the concept of interests, like frauds in transactions, intrusions in network security, and unusual events in surveillance. Despite extensive research efforts, imbalanced concept retrieval remains one of the most challenging research problems in multimedia data mining. Different from existing frameworks regarding concept correlations among labels, this paper presents a novel concept correlation analysis model using the correlation between the retrieval scores and labels. Experimental results on the TRECVID benchmark datasets demonstrate that the proposed framework can enhance imbalanced concept mining and retrieval even with trivial scores from the minority class.","keywords_author":["Imbalanced data","multimedia big data","multimedia semantic retrieval","rare class mining","concept correlation","information integration"],"keywords_other":["ENVIRONMENTS","RECOGNITION","VIDEO","VECTOR MACHINE CLASSIFIERS"],"max_cite":0.0,"pub_year":2017.0,"sources":"['wos']","rawkeys":["multimedia semantic retrieval","multimedia big data","recognition","concept correlation","imbalanced data","information integration","video","environments","rare class mining","vector machine classifiers"],"tags":["multimedia semantic retrieval","multimedia big data","recognition","concept correlation","imbalanced data","information integration","video","rare class mining","environment","vector machine classifiers"]},{"p_id":59595,"title":"Computer Vision and Natural Language Processing: Recent Approaches in Multimedia and Robotics","abstract":"Integrating computer vision and natural language processing is a novel interdisciplinary field that has received a lot of attention recently. In this survey, we provide a comprehensive introduction of the integration of computer vision and natural language processing in multimedia and robotics applications with more than 200 key references. The tasks that we survey include visual attributes, image captioning, video captioning, visual question answering, visual retrieval, human-robot interaction, robotic actions, and robot navigation. We also emphasize strategies to integrate computer vision and natural language processing models as a unified theme of distributional semantics. We make an analog of distributional semantics in computer vision and natural language processing as image embedding and word embedding, respectively. We also present a unified view for the field and propose possible future directions.","keywords_author":["Computer Vision","Natural Language Processing","Robotics","Language and vision","survey","multimedia","robotics","symbol grounding","distributional semantics","computer vision","natural language processing","visual attribute","image captioning","imitation learning","word2vec","word embedding","image embedding","semantic parsing","lexical semantics"],"keywords_other":["WEB DATA","DISTRIBUTIONAL SEMANTICS","PERCEPTION","NORMS","RECOGNITION","COMMUNICATION","MODELS","GRAMMAR","ANNOTATION","IMAGES"],"max_cite":0.0,"pub_year":2017.0,"sources":"['wos']","rawkeys":["semantic parsing","distributional semantics","image captioning","language and vision","web data","survey","imitation learning","word embedding","models","multimedia","symbol grounding","annotation","image embedding","visual attribute","recognition","images","word2vec","norms","communication","lexical semantics","computer vision","grammar","natural language processing","perception","robotics"],"tags":["semantic parsing","distributional semantics","image captioning","language and vision","web data","survey","imitation learning","word embedding","multimedia","symbol grounding","annotation","image embedding","recognition","images","word2vec","norms","communication","lexical semantics","perceptions","computer vision","visual attributes","model","grammar","natural language processing","robotics"]},{"p_id":59596,"title":"Pre-training the deep generative models with adaptive hyperparameter optimization","abstract":"The performance of many machine learning algorithms depends crucially on the hyperparameter settings, especially in Deep Learning. Manually tuning the hyperparameters is laborious and time consuming. To address this issue, Bayesian optimization (BO) methods and their extensions have been proposed to optimize the hyperparameters automatically. However, they still suffer from highly computational expense when applying to deep generative models (DGMs) due to their strategy of the black-box function optimization. This paper provides a new hyperparameter optimization procedure at the pre-training phase of the DGMs, where we avoid combining all layers as one black-box function by taking advantage of the layer-by-layer learning strategy. Following this procedure, we are able to optimize multiple hyperparameters in an adaptive way by using Gaussian process. In contrast to the traditional BO methods, which mainly focus on the supervised models, the pre-training procedure is unsupervised where there is no validation error can be used. To alleviate this problem, this paper proposes a new holdout loss, the free energy gap, which takes into account both factors of the model fitting and over-fitting. The empirical evaluations demonstrate that our method not only speeds up the process of hyperparameter optimization, but also improves the performances of DGMs significantly in both the supervised and unsupervised learning tasks. (C) 2017 Elsevier B.V. All rights reserved.","keywords_author":["Deep generative model","Hyperparameter optimization","Sequential model-based optimization","Contrastive divergence"],"keywords_other":["NEURAL-NETWORKS","RECOGNITION","BOLTZMANN MACHINES"],"max_cite":0.0,"pub_year":2017.0,"sources":"['wos']","rawkeys":["neural-networks","deep generative model","hyperparameter optimization","recognition","contrastive divergence","boltzmann machines","sequential model-based optimization"],"tags":["deep generative model","recognition","contrastive divergence","hyper-parameter optimizations","neural networks","boltzmann machines","sequential model-based optimization"]},{"p_id":108753,"title":"BIT: Biologically Inspired Tracker","abstract":"Visual tracking is challenging due to image variations caused by various factors, such as object deformation, scale change, illumination change, and occlusion. Given the superior tracking performance of human visual system (HVS), an ideal design of biologically inspired model is expected to improve computer visual tracking. This is, however, a difficult task due to the incomplete understanding of neurons' working mechanism in the HVS. This paper aims to address this challenge based on the analysis of visual cognitive mechanism of the ventral stream in the visual cortex, which simulates shallow neurons (S1 units and C1 units) to extract low-level biologically inspired features for the target appearance and imitates an advanced learning mechanism (S2 units and C2 units) to combine generative and discriminative models for target location. In addition, fast Gabor approximation and fast Fourier transform are adopted for real-time learning and detection in this framework. Extensive experiments on large-scale benchmark data sets show that the proposed biologically inspired tracker performs favorably against the state-of-the-art methods in terms of efficiency, accuracy, and robustness. The acceleration technique in particular ensures that biologically inspired tracker maintains a speed of approximately 45 frames\/s.","keywords_author":["Biologically inspired model","visual tracking","fast Gabor approximation","fast Fourier transform"],"keywords_other":["FEATURES","SET","ATTENTION","MODEL","RECOGNITION","CORTEX","FILTERS","OBJECT TRACKING","SCENE CLASSIFICATION","VISUAL TRACKING"],"max_cite":9.0,"pub_year":2016.0,"sources":"['wos']","rawkeys":["biologically inspired model","fast fourier transform","model","object tracking","features","recognition","set","scene classification","fast gabor approximation","attention","visual tracking","cortex","filters"],"tags":["fast fourier transform","recognition","model","object tracking","features","scene classification","fast gabor approximation","attention","visual tracking","sets","filter","cortex","building information modeling"]},{"p_id":59602,"title":"Visually Grounded Meaning Representations","abstract":"In this paper we address the problem of grounding distributional representations of lexical meaning. We introduce a new model which uses stacked autoencoders to learn higher-level representations from textual and visual input. The visual modality is encoded via vectors of attributes obtained automatically from images. We create a new large-scale taxonomy of 600 visual attributes representing more than 500 concepts and 700 K images. We use this dataset to train attribute classifiers and integrate their predictions with text-based distributional models of word meaning. We evaluate our model on its ability to simulate word similarity judgments and concept categorization. On both tasks, our model yields a better fit to behavioral data compared to baselines and related models which either rely on a single modality or do not make use of attribute-based input.","keywords_author":["Cognitive simulation","computer vision","distributed representations","concept learning","connectionism and neural nets","natural language processing"],"keywords_other":["LARGE SET","MODEL","RECOGNITION","FEATURE PRODUCTION NORMS","CATEGORIZATION","LANGUAGE","NETWORK","SEMANTIC MEMORY","SIMILARITY","ATTRIBUTES"],"max_cite":0.0,"pub_year":2017.0,"sources":"['ieee', 'wos']","rawkeys":["cognitive simulation","large set","network","recognition","model","semantic memory","similarity","natural language processing","attributes","categorization","distributed representations","feature production norms","concept learning","computer vision","connectionism and neural nets","language"],"tags":["large set","semantic memory","recognition","model","similarity","natural language processing","attributes","categorization","distributed representation","feature production norms","networks","concept learning","cognitive simulations","computer vision","connectionism and neural nets","language"]},{"p_id":108754,"title":"Fast indoor scene description for blind people with multiresolution random projections","abstract":"Object recognition forms a substantial need for blind and visually impaired individuals. This paper proposes a new multiobject recognition framework. It consists of coarsely checking the presence of multiple objects in a portable camera-grabbed image at a considered indoor site. The outcome is a list of objects that likely appear in the indoor scene. Such description is meant to uplift the conscience of the blind person in order to better sense his\/her surroundings. The method consists of a library containing (i) a bunch of images represented by means of the Random Projections (RP) technique and (ii) their respective list of objects, both prepared offline. Thus, given an online shot image, its RP representation is generated and matched to the RP patterns of library images. It thus inherits the objects of the closest image from the library. Extensive experiments returned promising recognition accuracies and a processing lapse of real-time standard.(C) 2017 Elsevier Inc. All rights reserved.","keywords_author":["Assistive technologies","Coarse scene description","Multiobject recognition","Multiresolution processing","Random projection","Visually impaired people"],"keywords_other":["IMAGERY","GUIDANCE","PARALLEL FRAMEWORK","STRATEGIES","CLASSIFICATION","RECOGNITION","MANY-CORE PROCESSORS","CONVOLUTIONAL NEURAL-NETWORKS"],"max_cite":0.0,"pub_year":2017.0,"sources":"['ieee', 'wos']","rawkeys":["guidance","parallel framework","many-core processors","recognition","multiresolution processing","random projection","strategies","multiobject recognition","visually impaired people","classification","convolutional neural-networks","coarse scene description","imagery","assistive technologies"],"tags":["guidance","parallel framework","many-core processors","recognition","multiresolution processing","strategies","multiobject recognition","visually impaired people","random projections","classification","convolutional neural network","coarse scene description","imagery","assistive technology"]},{"p_id":2260,"title":"Brain-Like Emergent Temporal Processing: Emergent Open States","abstract":"Informed by brain anatomical studies, we present the developmental network (DN) theory on brain-like temporal information processing. The states of the brain are at its effector end, emergent and open. A finite automaton (FA) is considered an external symbolic model of brain's temporal behaviors, but the FA uses handcrafted states and is without \u201cinternal\u201d representations. The term \u201cinternal\u201d means inside the network \u201cskull.\u201d Using action-based state equivalence and the emergent state representations, the time driven processing of DN performs state-based abstraction and state-based skill transfer. Each state of DN, as a set of actions, is openly observable by the external environment (including teachers). Thus, the external environment can teach the state at every frame time. Through incremental learning and autonomous practice, the DN lumps (abstracts) infinitely many temporal context sequences into a single equivalent state. Using this state equivalence, a skill learned under one sequence is automatically transferred to other infinitely many state-equivalent sequences in the future without the need for explicit learning. Two experiments are shown as examples: The experiments for video processing showed almost perfect recognition rates in disjoint tests. The experiment for text language, using corpora from the Wall Street Journal, treated semantics and syntax in a unified interactive way.","keywords_author":["Attention","behavior","brain-mind architecture","cognition","complexity","computer vision","perception","regression","representation","sequential abstraction","text understanding","time warping","transfer"],"keywords_other":["video processing","handcrafted state","incremental learning","state equivalent sequence","Biological neural networks","brain temporal behavior","brain","Abstracts","action-based state equivalence","FA","DN theory","state-based skill transfer","Context","developmental network","corpora","learning (artificial intelligence)","effector end","Ports (Computers)","finite automaton","video signal processing","medical image processing","Vectors","autonomous practice","Neurons","symbolic model","state representation","Brain modeling","temporal context sequence","text analysis","text language","time driven processing","DN lumps","cognition","brain-like temporal information processing","state-based abstraction"],"max_cite":5.0,"pub_year":2013.0,"sources":"['ieee']","rawkeys":["video processing","handcrafted state","incremental learning","state equivalent sequence","complexity","brain temporal behavior","brain","action-based state equivalence","brain modeling","transfer","ports (computers)","vectors","state-based skill transfer","biological neural networks","regression","brain-mind architecture","abstracts","neurons","behavior","dn theory","corpora","learning (artificial intelligence)","effector end","finite automaton","video signal processing","text understanding","dn lumps","medical image processing","attention","fa","autonomous practice","computer vision","time warping","symbolic model","state representation","representation","sequential abstraction","perception","temporal context sequence","brain-like temporal information processing","text analysis","text language","context","time driven processing","cognition","developmental network","state-based abstraction"],"tags":["video processing","handcrafted state","incremental learning","state equivalent sequence","complexity","brain temporal behavior","brain","action-based state equivalence","brain modeling","ports (computers)","machine learning","vectors","state-based skill transfer","biological neural networks","regression","brain-mind architecture","abstracts","neurons","behavior","dn theory","corpora","recognition","effector end","finite automaton","video signal processing","text understanding","dn lumps","medical image processing","attention","false alarms","perceptions","time warping","autonomous practice","computer vision","symbolic model","state representation","representation","sequential abstraction","temporal context sequence","brain-like temporal information processing","text analysis","text language","context","time driven processing","cognition","developmental network","state-based abstraction"]},{"p_id":75990,"title":"Part-Based Mesh Segmentation: A Survey","abstract":"This paper surveys mesh segmentation techniques and algorithms, with a focus on part-based segmentation, that is, segmentation that divides a mesh (featuring a 3D object) into meaningful parts. Part-based segmentation applies to a single object and also to a family of objects (i.e. co-segmentation). However, we shall not address here chart-based segmentation, though some mesh co-segmentation methods employ such chart-based segmentation in the initial step of their pipeline. Finally, the taxonomy proposed in this paper is new in the sense that one classifies each segmentation algorithm regarding the dimension (i.e. 1D, 2D and 3D) of the representation of object parts. The leading idea behind this survey is to identify the properties and limitations of the state-of-the-art algorithms to shed light on the challenges for future work.","keywords_author":["mesh generation","modelling","3d shape matching","modelling","digital geometry processing","modelling"],"keywords_other":["RANDOM-WALKS","IMAGE SEGMENTATION","POLYHEDRA","AFFINITY AGGREGATION","CURVATURE","RECOGNITION","UNSUPERVISED CO-SEGMENTATION","APPROXIMATE CONVEX DECOMPOSITION","VOLUME-BOUNDING-BOX","3D SHAPE SEGMENTATION"],"max_cite":0.0,"pub_year":2018.0,"sources":"['wos']","rawkeys":["3d shape segmentation","curvature","modelling","recognition","affinity aggregation","digital geometry processing","mesh generation","polyhedra","random-walks","3d shape matching","unsupervised co-segmentation","approximate convex decomposition","image segmentation","volume-bounding-box"],"tags":["3d shape segmentation","curvature","recognition","model","affinity aggregation","digital geometry processing","mesh generation","polyhedra","random walk","3d shape matching","unsupervised co-segmentation","approximate convex decomposition","image segmentation","volume-bounding-box"]},{"p_id":108764,"title":"Robust tracking of fish schools using CNN for head identification","abstract":"Tracking individuals in a fish school with video cameras is one of the most effective ways to quantitatively investigate their behavior which is of great value for biological research. However, tracking large numbers of fish with complex non-rigid deformation, similar appearance and frequent mutual occlusions is a challenge task. In this paper we propose an effective tracking method that can reliably track a large number of fish throughout the entire duration. The first step of the proposed method is to detect fish heads using a scale-space method. Data association across frames is achieved via identifying the head image pattern of each individual fish in each frame, which is accomplished by a convolutional neural network (CNN) specially tailored to suit this task. Then the prediction of the motion state and the recognition result by CNN are combined to associate detections across frames. The proposed method was tested on 5 video clips having different number of fish respectively. Experiment results show that the correctness of their identities is not affected by frequent occlusions. The proposed method outperforms two state-of-the-art fish tracking methods in terms of 7 performance metrics.","keywords_author":["Multi-object tracking","Tracking by identification","Fish school","Convolutional neural network"],"keywords_other":["ETHOVISION","VIDEO TRACKING","ANIMALS","SYSTEM","NEURAL-NETWORKS","RECOGNITION","BEHAVIOR","TAGS"],"max_cite":0.0,"pub_year":2016.0,"sources":"['ieee', 'wos']","rawkeys":["neural-networks","recognition","animals","ethovision","video tracking","fish school","system","tracking by identification","convolutional neural network","multi-object tracking","behavior","tags"],"tags":["recognition","animals","ethovision","video tracking","neural networks","system","tracking by identification","convolutional neural network","fish schooling","multi-object tracking","behavior","tagging"]},{"p_id":100581,"title":"A Survey of Scene Understanding by Event Reasoning in Autonomous Driving","abstract":"Realizing autonomy is a hot research topic for automatic vehicles in recent years. For a long time, most of the efforts to this goal concentrate on understanding the scenes surrounding the ego-vehicle (autonomous vehicle itself). By completing low-level vision tasks, such as detection, tracking and segmentation of the surrounding traffic participants, e.g., pedestrian, cyclists and vehicles, the scenes can be interpreted. However, for an autonomous vehicle, low-level vision tasks are largely insufficient to give help to comprehensive scene understanding. What are and how about the past, the on-going and the future of the scene participants? This deep question actually steers the vehicles towards truly full automation, just like human beings. Based on this thoughtfulness, this paper attempts to investigate the interpretation of traffic scene in autonomous driving from an event reasoning view. To reach this goal, we study the most relevant literatures and the state-of-the-arts on scene representation, event detection and intention prediction in autonomous driving. In addition, we also discuss the open challenges and problems in this field and endeavor to provide possible solutions.","keywords_author":["Autonomous vehicle","scene understanding","event reasoning","intention prediction","scene representation"],"keywords_other":["SALIENCY","LOCALIZATION","TRAFFIC SIGN DETECTION","PERCEPTION","RECOGNITION","MOTION","VEHICLES","VANISHING-POINT DETECTION","ROAD","PEDESTRIAN PATH PREDICTION"],"max_cite":0.0,"pub_year":2018.0,"sources":"['wos']","rawkeys":["pedestrian path prediction","vanishing-point detection","event reasoning","recognition","localization","road","motion","scene representation","perception","vehicles","scene understanding","traffic sign detection","saliency","autonomous vehicle","intention prediction"],"tags":["event reasoning","intention predictions","localization","recognition","motion","roads","vanishing point detection","scene representation","vehicles","perceptions","scene understanding","traffic sign detection","saliency","pedestrian path prediction","autonomous vehicles"]},{"p_id":10471,"title":"Cross-session classification of mental workload levels using EEG and an adaptive deep learning model","abstract":"Evaluation of operator Mental Workload (MW) levels via ongoing electroencephalogram (EEG) is quite promising in Human-Machine (HM) collaborative task environment to alarm the temporal operator performance degradation. However, accurate recognition of MW states via a static pattern classifier with training and testing EEG signals recoded on separate days is particularly challenging as EEG features are differently distributed across different sessions. Motivated by the superiority of the deep learning approaches for stable feature abstractions in higher levels, an adaptive Stacked Denoising AutoEncoder (SDAE) is developed to tackling such cross-session MW classification task in which the weights of the shallow hidden neurons could be adaptively updated during the testing procedure. The generalization capability of the adaptive SDAE is first evaluated under within\/cross-session conditions. Then, we compare it with the state of the art MW classifiers under different feature selection and the noise corruption paradigms. The results indicate a higher performance of the adaptive SDAE in dealing with the cross-session EEG features. By analyzing the optimal step length, the data augmentation scheme and the computational cost for iterative tuning, the adaptive SDAE is also demonstrated to be acceptable for online implementation. (C) 2016 Elsevier Ltd. All rights reserved.","keywords_author":["Deep learning","Electroencephalogram (EEG)","Human-machine system","Mental workload","Operator functional states","Human-machine system","Mental workload","Electroencephalogram (EEG)","Deep learning","Operator functional states"],"keywords_other":["Deep learning","Generalization capability","Online implementation","Mental workload","Optimal step lengths","OPERATOR FUNCTIONAL-STATE","IDENTIFICATION","ALGORITHM","Operator functional state","RECOGNITION","Electro-encephalogram (EEG)","TASK","Session classifications","AUTOMATION","nocv1"],"max_cite":7.0,"pub_year":2017.0,"sources":"['scp', 'wos']","rawkeys":["algorithm","mental workload","operator functional-state","identification","generalization capability","recognition","electro-encephalogram (eeg)","deep learning","operator functional state","session classifications","task","automation","human-machine system","operator functional states","online implementation","optimal step lengths","electroencephalogram (eeg)","nocv1"],"tags":["human-machine systems","mental workload","session classifications","identification","generalization capability","automated","operator functional state","recognition","eeg","machine learning","online implementation","algorithms","optimal step lengths","task","nocv1"]},{"p_id":76009,"title":"An Object-Based Image Analysis Method for Enhancing Classification of Land Covers Using Fully Convolutional Networks and Multi-View Images of Small Unmanned Aerial System","abstract":"Fully Convolutional Networks (FCN) has shown better performance than other classifiers like Random Forest (RF), Support Vector Machine (SVM) and patch-based Deep Convolutional Neural Network (DCNN), for object-based classification using orthoimage only in previous studies; however, for further improving deep learning algorithm performance, multi-view data should be considered for training data enrichment, which has not been investigated for FCN. The present study developed a novel OBIA classification using FCN and multi-view data extracted from small Unmanned Aerial System (UAS) for mapping landcovers. Specifically, this study proposed three methods to automatically generate multi-view training samples from orthoimage training datasets to conduct multi-view object-based classification using FCN, and compared their performances with each other and also with RF, SVM, and DCNN classifiers. The first method does not consider the object surrounding information, while the other two utilized object context information. We demonstrated that all the three versions of FCN multi-view object-based classification outperformed their counterparts utilizing orthoimage data only. Furthermore, the results also showed that when multi-view training samples were prepared with consideration of object surroundings, FCN trained with these samples gave much better accuracy than FCN classification trained without context information. Similar accuracies were achieved from the two methods utilizing object surrounding information, although sample preparation was conducted using two different ways. When comparing FCN with RF, SVM, DCNN implies that FCN generally produced better accuracy than the other classifiers, regardless of using orthoimage or multi-view data.","keywords_author":["FCN","deep learning","object-based","OBIA","UAS","multi-view data","wetland"],"keywords_other":["LIDAR DATA","SUPPORT VECTOR MACHINES","RANDOM FOREST","RECOGNITION","SEGMENTATION","NEWFOUNDLAND","REMOTE-SENSING IMAGERY","DEEP NEURAL-NETWORKS","WETLAND CLASSIFICATION","MULTISOURCE"],"max_cite":0.0,"pub_year":2018.0,"sources":"['ieee', 'wos']","rawkeys":["remote-sensing imagery","uas","recognition","newfoundland","obia","segmentation","deep learning","object-based","deep neural-networks","fcn","multi-view data","multisource","wetland","support vector machines","wetland classification","lidar data","random forest"],"tags":["remote-sensing imagery","object based","recognition","newfoundland","segmentation","uas","machine learning","fully convolutional network","random forests","wetland","wetland classification","convolutional neural network","object based image analysis","multiview data","lidar data","multisources"]},{"p_id":76014,"title":"Automated Landslides Detection for Mountain Cities Using Multi-Temporal Remote Sensing Imagery","abstract":"Landslides that take place in mountain cities tend to cause huge casualties and economic losses, and a precise survey of landslide areas is a critical task for disaster emergency. However, because of the complicated appearance of the nature, it is difficult to find a spatial regularity that only relates to landslides, thus landslides detection based on only spatial information or artificial features usually performs poorly. In this paper, an automated landslides detection approach that is aiming at mountain cities has been proposed based on pre-and post-event remote sensing images, it mainly utilizes the knowledge of landslide-related surface covering changes, and makes full use of the temporal and spatial information. A change detection method using Deep Convolution Neural Network (DCNN) was introduced to extract the areas where drastic alterations have taken place; then, focusing on the changed areas, the Spatial Temporal Context Learning (STCL) was conducted to identify the landslides areas; finally, we use slope degree which is derived from digital elevation model (DEM) to make the result more reliable, and the change of DEMis used for making the detected areas more complete. The approach was applied to detecting the landslides in Shenzhen, Zhouqu County and Beichuan County in China, and a quantitative accuracy assessment has been taken. The assessment indicates that this approach can guarantee less commission error of landslide areal extentwhich is below 17.6% and achieves a quality percentage above 61.1%, and for landslide areas, the detection percentage is also competitive, the experimental results proves the feasibility and accuracy of the proposed approach for the detection landslides in mountain cities.","keywords_author":["landslides detection","remote sensing images","change detection","Deep Convolution Neural Network","Spatial Temporal Context Learning"],"keywords_other":["DEM","RECOGNITION","INVENTORIES","SENSED IMAGERY","SATELLITE IMAGES","INDEX","TIME-SERIES DATA"],"max_cite":0.0,"pub_year":2018.0,"sources":"['wos']","rawkeys":["spatial temporal context learning","recognition","dem","deep convolution neural network","index","sensed imagery","satellite images","change detection","landslides detection","remote sensing images","inventories","time-series data"],"tags":["spatial temporal context learning","recognition","dem","contrastive divergence","remote sensing images","index","sensed imagery","satellite images","inventory","convolutional neural network","landslide detection","time-series data"]},{"p_id":10483,"title":"Autonomous microscopic bunch inspection using region-based deep learning for evaluating graphite powder dispersion","abstract":"The ice-snow melting performance of ice-snow pavement is significantly influenced by the dispersion of graphite powder, particularly through the distribution of graphite powder bunches. In recent years, optical microscope (OP) images have been utilized to detect graphite powder bunches and evaluate their dispersion. However, because graphite powder bunches and other objects in OP images often have various shapes, and conventional manually processed images of tasks have the disadvantage of low efficiency, it is a challenge to detect graphite powder bunches and evaluate their dispersion using OP images. Therefore, this paper presents a novel application of a Faster Region Convolutional Neural Network (Faster R-CNN) using OP images and video sequences for the autonomous detection of graphite powder bunches and an evaluation of their dispersion. The research procedure is as follows: (a) generate a database for the Faster R-CNN, (b) design 30 Faster R-CNNs to find the optimal one, and (c) conduct an analysis of the training and testing results, along with new image testing, comparative studies, and video testing. The results show that a Faster R-CNN with nine anchors and a ratio of 0.3, 1.0, and 1.6, and with the sizes of 32, 128, and 192, has an average precision of the bunches and a dispersion uniformity of 91.2% and 84.0%, respectively. Its mean average precision is 87.5%. The Faster R-CNN is considered optimal in this research. The test time required to evaluate an image with a pixel resolution of 1024 x 1024 pixel in GPU mode is approximately 0.04 s, which means the method based on a Faster R-CNN has the capacity of a quasi-real-time autonomous dispersion evaluation in GPU mode to replace a human-assisted microscopic dispersion evaluation in OP images. The results also provide the possibility for a quasi-real-time evaluation using OP video sequences. Compared with a Fast R-CNN, a Faster R-CNN provides more reasonable bounding boxes for bunches and reliable results in terms of the dispersion uniformity. (C) 2018 Elsevier Ltd. All rights reserved.","keywords_author":["Graphite powder bunch","Dispersion of graphite powder","The Faster R-CNN","Optical microscope (OP)","Region proposal networks"],"keywords_other":["RECOGNITION","MORTAR","FIBER DISPERSION","CEMENT-BASED COMPOSITES","CONVOLUTIONAL NEURAL-NETWORKS"],"max_cite":0.0,"pub_year":2018.0,"sources":"['wos']","rawkeys":["fiber dispersion","optical microscope (op)","mortar","recognition","dispersion of graphite powder","graphite powder bunch","the faster r-cnn","cement-based composites","convolutional neural-networks","region proposal networks"],"tags":["fiber dispersion","optical microscope (op)","mortar","recognition","region proposal network","dispersion of graphite powder","graphite powder bunch","the faster r-cnn","convolutional neural network","cement-based composite"]},{"p_id":2296,"title":"Sequence learning: Analysis and solutions for sparse data in high dimensional spaces","abstract":"We examine the problem of classifying biological sequences, and in particular the challenge of generalizing to novel input data. The high dimensionality of sequence results in an extremely sparsely populated input space. This motivates a need for regularization (a form of inductive bias), in order to achieve generalization. We discuss regularization in the context of regular Neural Networks and Deep Belief Networks, and provide experimental results on an example problem of DNA barcoding classification. Our results support the importance of using an effective regularization method, and indicate the adaptive, data-depended regularization mechanism of a DBN is more powerful than the simple methods of model selection \/ weight decay \/ early stopping.","keywords_author":["deep architecture","DNA barcoding","generalization","generative model","machine learning","neural network","reuglarization","machine learning","generalization","reuglarization","neural network","deep architecture","generative model","DNA barcoding"],"keywords_other":["Sparse data","DNA barcoding classification","biological techniques","biological sequence classification","adaptive data dependent regularization mechanism","Correlation","Neural networks","regular neural networks","molecular biophysics","Model Selection","High dimensionality","high dimensional spaces","sparsely populated input space","Feature extraction","generalization","inductive bias","Sequence learning","data handling","learning (artificial intelligence)","Noise","Inductive bias","High dimensional spaces","Training","Early stopping","Deep belief networks","Input datas","DNA barcoding","belief networks","Complexity theory","DNA","Regularization mechanism","reuglarization","Regularization methods","sequence learning","SIMPLE method","molecular configurations","neural nets","Generative model","Input space","Weight decay","sparse data analysis","bioinformatics","Biological sequences","deep belief networks"],"max_cite":0.0,"pub_year":2012.0,"sources":"['scp', 'ieee']","rawkeys":["neural network","regularization mechanism","input datas","biological techniques","biological sequence classification","adaptive data dependent regularization mechanism","regular neural networks","molecular biophysics","high dimensional spaces","sparsely populated input space","machine learning","generalization","high dimensionality","biological sequences","inductive bias","data handling","sparse data","complexity theory","generative model","learning (artificial intelligence)","noise","neural networks","training","dna","early stopping","weight decay","belief networks","dna barcoding","molecular configurations","neural nets","reuglarization","correlation","input space","dna barcoding classification","sequence learning","simple method","regularization methods","model selection","deep architecture","sparse data analysis","feature extraction","bioinformatics","deep belief networks"],"tags":["regularization mechanism","input datas","biological techniques","biological sequence classification","adaptive data dependent regularization mechanism","regular neural networks","molecular biophysics","high dimensional spaces","sparsely populated input space","machine learning","biological sequences","inductive bias","data handling","sparse data","complexity theory","generative model","recognition","noise","neural networks","training","deep architectures","dna","early stopping","high-dimensional","belief networks","dna barcoding","molecular configurations","weight decay","reuglarization","correlation","input space","dna barcoding classification","sequence learning","simple method","regularization methods","multiple sclerosis","sparse data analysis","feature extraction","bioinformatics","deep belief networks"]},{"p_id":10501,"title":"Automatic Video Event Detection for Imbalance Data Using Enhanced Ensemble Deep Learning","abstract":"With the explosion of multimedia data, semantic event detection from videos has become a demanding and challenging topic. In addition, when the data has a skewed data distribution, interesting event detection also needs to address the data imbalance problem. The recent proliferation of deep learning has made it an essential part of many Artificial Intelligence (AI) systems. Till now, various deep learning architectures have been proposed for numerous applications such as Natural Language Processing (NLP) and image processing. Nonetheless, it is still impracticable for a single model to work well for different applications. Hence, in this paper, a new ensemble deep learning framework is proposed which can be utilized in various scenarios and datasets. The proposed framework is able to handle the over-fitting issue as well as the information losses caused by single models. Moreover, it alleviates the imbalanced data problem in real-world multimedia data. The whole framework includes a suite of deep learning feature extractors integrated with an enhanced ensemble algorithm based on the performance metrics for the imbalanced data. The Support Vector Machine (SVM) classifier is utilized as the last layer of each deep learning component and also as the weak learners in the ensemble module. The framework is evaluated on two large-scale and imbalanced video datasets (namely, disaster and TRECVID). The extensive experimental results illustrate the advantage and effectiveness of the proposed framework. It also demonstrates that the proposed framework outperforms several well-known deep learning methods, as well as the conventional features integrated with different classifiers.","keywords_author":["Deep learning","convolutional neural networks","ensemble learning","imbalanced data","video event detection","multimedia big data"],"keywords_other":["SEMANTIC CONCEPT DETECTION","RECOGNITION","FRAMEWORK"],"max_cite":4.0,"pub_year":2017.0,"sources":"['wos']","rawkeys":["video event detection","multimedia big data","convolutional neural networks","recognition","semantic concept detection","deep learning","framework","imbalanced data","ensemble learning"],"tags":["video event detections","multimedia big data","recognition","semantic concept detection","framework","machine learning","imbalanced data","convolutional neural network","ensemble learning"]},{"p_id":108810,"title":"On Exploration of Classifier Ensemble Synergism in Pedestrian Detection","abstract":"A single feature extractor-classifier is not usually able to deal with the diversity of multiple image scenarios. Therefore, integration of features and classifiers can bring benefits to cope with this problem, particularly when the parts are carefully chosen and synergistically combined. In this paper, we address the problem of pedestrian detection by a novel ensemble method. Initially, histograms of oriented gradients (HOGs) and local receptive fields (LRFs), which are provided by a convolutional neural network, have been both classified by multilayer perceptrons (MLPs) and support vector machines (SVMs). A diversity measure is used to refine the initial set of feature extractors and classifiers. A final classifier ensemble was then structured by an HOG and an LRF as features, classified by two SVMs and one MLP. We have analyzed the following two classes of fusion methods of combining the outputs of the component classifiers: 1) majority vote and 2) fuzzy integral. The first part of the performance evaluation consisted of running the final proposed ensemble over the DaimlerChrysler cropwise data set, which was also artificially modified to simulate sunny and shadowy illumination conditions, which is typical of outdoor scenarios. Then, a window-wise study has been performed over a collected video sequence. Experiments have highlighted a state-of-the-art classification system, performing consistently better than the component classifiers and other methods.","keywords_author":["Convolutional neural network (CNN)","fuzzy integral (FI)","histograms of oriented gradients (HOGs)","majority vote (MV)","multilayer perceptron (MLP)","pedestrian detection","support vector machine (SVM)"],"keywords_other":["DIVERSITY","RECOGNITION","SYSTEM","OBJECT DETECTION"],"max_cite":31.0,"pub_year":2010.0,"sources":"['wos']","rawkeys":["histograms of oriented gradients (hogs)","pedestrian detection","recognition","fuzzy integral (fi)","diversity","majority vote (mv)","system","convolutional neural network (cnn)","support vector machine (svm)","object detection","multilayer perceptron (mlp)"],"tags":["pedestrian detection","majority voting","recognition","diversity","histogram of oriented gradients","machine learning","system","object detection","convolutional neural network","multi layer perceptron","fuzzy integral"]},{"p_id":10506,"title":"Fish species classification in unconstrained underwater environments based on deep learning","abstract":"Underwater video and digital still cameras are rapidly being adopted by marine scientists and managers as a tool for non-destructively quantifying and measuring the relative abundance, cover and size of marine fauna and flora. Imagery recorded of fish can be time consuming and costly to process and analyze manually. For this reason, there is great interest in automatic classification, counting, and measurement of fish. Unconstrained underwater scenes are highly variable due to changes in light intensity, changes in fish orientation due to movement, a variety of background habitats which sometimes also move, and most importantly similarity in shape and patterns among fish of different species. This poses a great challenge for image\/video processing techniques to accurately differentiate between classes or species of fish to perform automatic classification. We present a machine learning approach, which is suitable for solving this challenge. We demonstrate the use of a convolution neural network model in a hierarchical feature combination setup to learn species-dependent visual features of fish that are unique, yet abstract and robust against environmental and intra-and inter-species variability. This approach avoids the need for explicitly extracting features from raw images of the fish using several fragmented image processing techniques. As a result, we achieve a single and generic trained architecture with favorable performance even for sample images of fish species that have not been used in training. Using the LifeCLEF14 and LifeCLEF15 benchmark fish datasets, we have demonstrated results with a correct classification rate of more than 90%.","keywords_author":null,"keywords_other":["COMPUTER VISION","RECOGNITION","NEURAL-NETWORKS"],"max_cite":4.0,"pub_year":2016.0,"sources":"['wos']","rawkeys":["computer vision","neural-networks","recognition"],"tags":["computer vision","recognition","neural networks"]},{"p_id":2315,"title":"Regularization of sequence data for machine learning","abstract":"We examine the problem of classifying biological sequences, and in particular the challenge of generalizing results to novel input data. We observe that the high-dimensionality of sequence data representations results in an extremely sparsely populated input space. This motivates a need for regularization (a form of inductive bias), in order to achieve generalization. We discuss regularization in the context of regular neural networks, deep belief networks and support vector machines, and provide experimental results for these architectures. Our results support the importance of using an effective regularization method and identify which methods work well on a real-world dataset. \u00a9 2011 IEEE.","keywords_author":["deep architecture","DNA barcoding","generalization","machine learning","neural network","non-monophyletic species","support vector machine"],"keywords_other":["non-monophyletic species","Machine-learning","generalization","Support vector","DNA barcoding"],"max_cite":0.0,"pub_year":2011.0,"sources":"['scp', 'ieee']","rawkeys":["neural network","non-monophyletic species","support vector","machine learning","generalization","deep architecture","machine-learning","dna barcoding","support vector machine"],"tags":["non-monophyletic species","recognition","neural networks","deep architectures","machine learning","support vector","dna barcoding"]},{"p_id":108811,"title":"Brain-Inspired Constructive Learning Algorithms with Evolutionally Additive Nonlinear Neurons","abstract":"In this article, inspired partially by the physiological evidence of brain's growth and development, we developed a new type of constructive learning algorithm with evolutionally additive nonlinear neurons. The new algorithms have remarkable ability in effective regression and accurate classification. In particular, the algorithms are able to sustain a certain reduction of the loss function when the dynamics of the trained network are bogged down in the vicinity of the local minima. The algorithm augments the neural network by adding only a few connections as well as neu-rons whose activation functions are nonlinear, nonmonotonic, and self-adapted to the dynamics of the loss functions. Indeed, we analytically demonstrate the reduction dynamics of the algorithm for different problems, and further modify the algorithms so as to obtain an improved generalization capability for the augmented neural networks. Finally, through comparing with the classical algorithm and architecture for neural network construction, we show that our constructive learning algorithms as well as their modified versions have better performances, such as faster training speed and smaller network size, on several representative benchmark datasets including the MNIST dataset for handwriting digits.","keywords_author":["Nonlinear neuron","brain-inspired neural network","constructive learning algorithm","nonmonotonic activation function","training dynamics"],"keywords_other":["REGRESSION","DEEP","MODEL","RECOGNITION","FEEDFORWARD NEURAL-NETWORKS","MEMORY","DYNAMICS"],"max_cite":0.0,"pub_year":2018.0,"sources":"['wos']","rawkeys":["feedforward neural-networks","recognition","model","memory","deep","nonlinear neuron","nonmonotonic activation function","training dynamics","constructive learning algorithm","dynamics","brain-inspired neural network","regression"],"tags":["recognition","model","memory","brain inspired neural networks","deep","neural networks","nonlinear neuron","nonmonotonic activation function","training dynamics","constructive learning algorithm","dynamics","regression"]},{"p_id":10518,"title":"An overview on the evolution and adoption of deep learning applications used in the industry","abstract":"With continuous improvements in performance of microprocessors over the years, they now possess capabilities of supercomputers of earlier decade. Further with the continuous increase in the packaging density on the silicon and General Purpose Graphics Processing Unit (GPGPU) enhancements, has led to utilization the deep learning (DL) techniques, which had lost steam during the last decade. A GPGPU is a parallel programming setup using a combination of GPUs and CPUs that can manipulate large matrices. Interestingly, GPUs were created for faster graphic processing, but found its way into relevant scientific computing. DL is a subset of the artificial intelligence (AI) domain and falls specifically under the set of machine learning (ML) techniques which are based on learning data representations rather than task-specific algorithms. It has been observed that the accuracy and the pragmatism of deploying DL at massive level was restricted by technological issues of executing DL based AI models, with extremely large training sessions running into weeks. DL applications can solve problems of very large order and areas like computer vision\/image processing is one of the early successes and becoming quite a sensation in many areas such as natural language processing (NLP) with state of the art real-time translation capabilities, automatic game playing, optical character recognition especially handwritten text, and so on. This overview traverses the evolution and successful adoption in the various industry verticals. This article is categorized under:","keywords_author":["artificial neural networks","CNN","deep learning","image recognition","LSTM","machine learning","text to speech","artificial neural networks","CNN","deep learning","image recognition","LSTM","machine learning","text to speech"],"keywords_other":["Continuous improvements","Text to speech","Application area","Training sessions","Handwritten texts","NEURAL-NETWORKS","LSTM","RECOGNITION","Packaging density","General purpose graphics processing unit (GPGPU)"],"max_cite":0.0,"pub_year":2018.0,"sources":"['scp', 'wos']","rawkeys":["application area","neural-networks","lstm","recognition","deep learning","handwritten texts","training sessions","packaging density","machine learning","cnn","general purpose graphics processing unit (gpgpu)","artificial neural networks","image recognition","text to speech","continuous improvements"],"tags":["application area","recognition","neural networks","handwritten texts","long short-term memory","packaging density","machine learning","training sessions","convolutional neural network","gpgpu","image recognition","text to speech","continuous improvements"]},{"p_id":67864,"title":"Reducing parameter number in residual networks by sharing weights","abstract":"Deep residual networks have reached the state of the art in many image processing tasks such image classification. However, the cost for a gain in accuracy in terms of depth and memory is prohibitive as it requires a higher number of residual blocks, up to double the initial value. To tackle this problem, we propose in this paper a way to reduce the redundant information of the networks. We share the weights of convolutional layers between residual blocks operating at the same spatial scale. The signal flows multiple times in the same convolutional layer. The resulting architecture, called ShaResNet, contains block specific layers and shared layers. These ShaResNet are trained exactly in the same fashion as the commonly used residual networks. We show, on the one hand, that they are almost as efficient as their sequential counterparts while involving less parameters, and on the other hand that they are more efficient than a residual network with the same number of parameters. For example, a 152-layer-deep residual network can be reduced to 106 convolutional layers, i.e. a parameter gain of 39%, while loosing less than 0.2% accuracy on ImageNet. (c) 2018 Elsevier B.V. All rights reserved.","keywords_author":null,"keywords_other":["RECOGNITION","DEEP"],"max_cite":0.0,"pub_year":2018.0,"sources":"['wos']","rawkeys":["deep","recognition"],"tags":["deep","recognition"]},{"p_id":43289,"title":"On-chip communication network for efficient training of deep convolutional networks on heterogeneous manycore systems","abstract":"\u00a9 2017 IEEE. Convolutional Neural Networks (CNNs) have shown a great deal of success in diverse application domains including computer vision, speech recognition, and natural language processing. However, as the size of datasets and the depth of neural network architectures continue to grow, it is imperative to design high-performance and energy-efficient computing hardware for training CNNs. In this paper, we consider the problem of designing specialized CPU-GPU based heterogeneous manycore systems for energy-efficient training of CNNs. It has already been shown that the typical on-chip communication infrastructures employed in conventional CPU-GPU based heterogeneous manycore platforms are unable to handle both CPU and GPU communication requirements efficiently. To address this issue, we first analyze the on-chip traffic patterns that arise from the computational processes associated with training two deep CNN architectures, namely, LeNet and CDBNet, to perform image classification. By leveraging this knowledge, we design a hybrid Network-on-Chip (NoC) architecture, which consists of both wireline and wireless links, to improve the performance of CPU-GPU based heterogeneous manycore platforms running the above-mentioned CNN training workloads. The proposed NoC achieves 1.8\u00d7 reduction in network latency and improves the network throughput by a factor of 2.2 for training CNNs, when compared to a highly-optimized wireline mesh NoC. For the considered CNN workloads, these network-level improvements translate into 25 percent savings in full-system energy-delay-product (EDP). This demonstrates that the proposed hybrid NoC for heterogeneous manycore architectures is capable of significantly accelerating training of CNNs while remaining energy-efficient.","keywords_author":["Deep learning","Energy-efficient computing","Heterogeneous architectures","Manycore systems","Network-on-chip","System-on-chip","Wireless communication","System-on-chip","Deep learning","Manycore systems","Wireless communication","Energy-efficient computing","Heterogeneous Architectures","Network-on-Chip"],"keywords_other":["INTERCONNECTION","Heterogeneous architectures","Convolutional networks","Manycore systems","NOC","Energy efficient computing","On-chip communication networks","NEURAL-NETWORKS","RECOGNITION","Convolutional neural network","Wireless communications","Heterogeneous many cores","OPTIMIZATION","DESIGN SPACE EXPLORATION","INTEGRATED ANTENNAS","ARCHITECTURE"],"max_cite":0.0,"pub_year":2018.0,"sources":"['ieee', 'scp', 'wos']","rawkeys":["design space exploration","heterogeneous many cores","on-chip communication networks","convolutional neural network","architecture","wireless communication","neural-networks","convolutional networks","wireless communications","optimization","network-on-chip","recognition","deep learning","system-on-chip","integrated antennas","energy-efficient computing","energy efficient computing","heterogeneous architectures","noc","interconnection","manycore systems"],"tags":["heterogeneous architectures","recognition","neural networks","network-on-chip","design space exploration","machine learning","system-on-chip","integrated antennas","heterogeneous many cores","wireless communications","on-chip communication networks","convolutional neural network","optimization","interconnection","architecture","energy efficient computing","manycore systems"]},{"p_id":10523,"title":"SEMI-SUPERVISED LEARNING-BASED LIVE FISH IDENTIFICATION IN AQUACULTURE USING MODIFIED DEEP CONVOLUTIONAL GENERATIVE ADVERSARIAL NETWORKS","abstract":"Aiming at live fish identification in aquaculture, a practical and efficient semi-supervised learning model, based on modified deep convolutional generative adversarial networks (DCGANs), was proposed in this study. Benefiting from the modified DCGANs structure, the presented model can be trained effectively using relatively few labeled training samples. In consideration of the complex poses of fish and the low resolution of sampling images in aquaculture, spatial pyramid pooling and some improved techniques specifically for the presented model were used to make the model more robust. Finally, in tests with two preprocessed and challenging datasets (with 5%, 10%, and 15% labeled training data in the fish recognition ground-truth dataset and 25%, 50%, and 75% labeled training data in the Croatian fish dataset), the feasibility and reliability of the presented model for live fish identification were proved with respective accuracies of 80.52%, 81.66%, and 83.07% for the ground-truth dataset and 65.13%, 78.72%, and 82.95% for the Croatian fish dataset.","keywords_author":["Aquaculture","Deep convolutional generative adversarial networks","Few labeled training samples","Live fish identification","Semi-supervised learning","Spatial pyramid pooling"],"keywords_other":["CLASSIFICATION","SYSTEM","NEURAL-NETWORKS","RECOGNITION","OPTIMIZATION"],"max_cite":0.0,"pub_year":2018.0,"sources":"['wos']","rawkeys":["neural-networks","recognition","live fish identification","deep convolutional generative adversarial networks","system","few labeled training samples","semi-supervised learning","classification","optimization","spatial pyramid pooling","aquaculture"],"tags":["recognition","live fish identification","neural networks","deep convolutional generative adversarial networks","system","few labeled training samples","semi-supervised learning","classification","optimization","spatial pyramid pooling","aquaculture"]},{"p_id":108828,"title":"Computerised auto-scoring system based upon feature extraction and neural network technologies","abstract":"This paper presents an application of pattern recognition for the shooting target paper image. This is a two-stage approach. First, essential features of local images are properly defined and extracted from the target paper image. Then the Fuzzy C-Means clustering is carried out for determining the initial parameters and structure of the RBF classifier, which is further enhanced by the hybrid leaning algorithm. Experimental results show that the system achieves satisfactory performance both in terms of error rates of classification and learning efficiency.","keywords_author":null,"keywords_other":["BACKPROPAGATION","LOCAL MINIMA","RECOGNITION","MINIMA-FREE CONDITION"],"max_cite":1.0,"pub_year":2000.0,"sources":"['wos']","rawkeys":["local minima","recognition","minima-free condition","backpropagation"],"tags":["local minima","recognition","minima-free condition","backpropagation"]},{"p_id":10526,"title":"A deep learning-based method for detecting non-certified work on construction sites","abstract":"The construction industry is a high hazard industry. Accidents frequently occur, and part of them are closely relate to workers who are not certified to carry out specific work. Although workers without a trade certificate are restricted entry to construction sites, few ad-hoc approaches have been commonly employed to check if a worker is carrying out the work for which they are certificated. This paper proposes a novel framework to check whether a site worker is working within the constraints of their certification. Our framework comprises key video clips extraction, trade recognition and worker competency evaluation. Trade recognition is a new proposed method through analyzing the dynamic spatiotemporal relevance between workers and non-worker objects. We also improved the identification results by analyzing, comparing, and matching multiple face images of each worker obtained from videos. The experimental results demonstrate the reliability and accuracy of our deep learning-based method to detect workers who are carrying out work for which they are not certified to facilitate safety inspection and supervision.","keywords_author":["Certification checking","Construction safety","Deep learning","Identification","Trade recognition","Construction safety","Certification checking","Trade recognition","Identification","Deep learning"],"keywords_other":["Safety inspections","Learning-based methods","Certification checking","Construction safety","Competency evaluation","Ad hoc approach","Hazard industries","RECOGNITION","EQUIPMENT","Construction sites","SAFETY"],"max_cite":2.0,"pub_year":2018.0,"sources":"['scp', 'wos']","rawkeys":["learning-based methods","safety inspections","construction safety","hazard industries","identification","recognition","construction sites","deep learning","competency evaluation","safety","equipment","ad hoc approach","certification checking","trade recognition"],"tags":["learning-based methods","safety inspections","construction safety","hazard industries","identification","recognition","construction sites","competency evaluation","machine learning","safety","equipment","ad hoc approach","certification checking","trade recognition"]},{"p_id":10530,"title":"Advanced Deep-Learning Techniques for Salient and Category-Specific Object Detection: A Survey","abstract":"\u00a9 2017 IEEE. Object detection, including objectness detection (OD), salient object detection (SOD), and category-specific object detection (COD), is one of the most fundamental yet challenging problems in the computer vision community. Over the last several decades, great efforts have been made by researchers to tackle this problem, due to its broad range of applications for other computer vision tasks such as activity or event recognition, content-based image retrieval and scene understanding, etc. While numerous methods have been presented in recent years, a comprehensive review for the proposed high-quality object detection techniques, especially for those based on advanced deep-learning techniques, is still lacking. To this end, this article delves into the recent progress in this research field, including 1) definitions, motivations, and tasks of each subdirection; 2) modern techniques and essential research trends; 3) benchmark data sets and evaluation metrics; and 4) comparisons and analysis of the experimental results. More importantly, we will reveal the underlying relationship among OD, SOD, and COD and discuss in detail some open questions as well as point out several unsolved challenges and promising future works.","keywords_author":null,"keywords_other":["Vision communities","VISUAL SALIENCY","Scene understanding","Evaluation metrics","Learning techniques","RECOGNITION","Category specifics","SEGMENTATION","Modern techniques","REGION DETECTION","Salient object detection","Content based image retrieval"],"max_cite":17.0,"pub_year":2018.0,"sources":"['scp', 'wos']","rawkeys":["modern techniques","vision communities","recognition","region detection","salient object detection","segmentation","category specifics","visual saliency","evaluation metrics","learning techniques","content based image retrieval","scene understanding"],"tags":["modern techniques","vision communities","recognition","region detection","salient object detection","segmentation","category specifics","visual saliency","evaluation metrics","learning techniques","scene understanding","content-based image retrieval"]},{"p_id":10533,"title":"Shared acoustic codes underlie emotional communication in music and speech-Evidence from deep transfer learning","abstract":"Music and speech exhibit striking similarities in the communication of emotions in the acoustic domain, in such a way that the communication of specific emotions is achieved, at least to a certain extent, by means of shared acoustic patterns. From an Affective Sciences points of view, determining the degree of overlap between both domains is fundamental to understand the shared mechanisms underlying such phenomenon. From a Machine learning perspective, the overlap between acoustic codes for emotional expression in music and speech opens new possibilities to enlarge the amount of data available to develop music and speech emotion recognition systems. In this article, we investigate time-continuous predictions of emotion (Arousal and Valence) in music and speech, and the Transfer Learning between these domains. We establish a comparative framework including intra- (i.e., models trained and tested on the same modality, either music or speech) and cross-domain experiments (i.e., models trained in one modality and tested on the other). In the cross domain context, we evaluated two strategies the direct transfer between domains, and the contribution of Transfer Learning techniques (feature-representation-transfer based on Denoising Auto Encoders) for reducing the gap in the feature space distributions. Our results demonstrate an excellent cross-domain generalisation performance with and without feature representation transfer in both directions. In the case of music, cross-domain approaches outperformed intra-domain models for Valence estimation, whereas for Speech intra-domain models achieve the best performance. This is the first demonstration of shared acoustic codes for emotional expression in music and speech in the time-continuous domain.","keywords_author":null,"keywords_other":["FEATURES","CLASSIFICATION","EXPRESSION","DATABASE","RECOGNITION","LSTM","TIME","CUES","NETWORK"],"max_cite":2.0,"pub_year":2017.0,"sources":"['ieee', 'wos']","rawkeys":["network","lstm","recognition","features","cues","time","database","expression","classification"],"tags":["recognition","features","databases","long short-term memory","cues","time","expression","networks","classification"]},{"p_id":108838,"title":"NEURAL Networks and Consumer Behavior: NEURAL Models, Logistic Regression, and the Behavioral Perspective Model","abstract":"This paper investigates the ability of connectionist models to explain consumer behavior, focusing on the feedforward neural network model, and explores the possibility of expanding the theoretical framework of the Behavioral Perspective Model to incorporate connectionist constructs. Numerous neural network models of varying complexity are developed to predict consumer loyalty as a crucial aspect of consumer behavior. Their performance is compared with the more traditional logistic regression model and it is found that neural networks offer consistent advantage over logistic regression in the prediction of consumer loyalty. Independently determined Utilitarian and Informational Reinforcement variables are shown to make a noticeable contribution to the explanation of consumer choice. The potential of connectionist models for predicting and explaining consumer behavior is discussed and routes for future research are suggested to investigate the predictive and explanatory capacity of connectionist models, such as neural network models, and for the integration of these into consumer behavior analysis within the theoretical framework of the Behavioral Perspective Model.","keywords_author":["Consumer behavior","Behavioral perspective model","Artificial neural networks","Neural models","NN","Connectionism","Connectionist models"],"keywords_other":["BRAND CHOICE","FACE RECOGNITION","REINFORCEMENT","ECONOMICS","PATTERNS"],"max_cite":0.0,"pub_year":2017.0,"sources":"['wos']","rawkeys":["connectionism","connectionist models","economics","reinforcement","patterns","neural models","artificial neural networks","face recognition","nn","behavioral perspective model","brand choice","consumer behavior"],"tags":["connectionism","connectionist models","economics","recognition","neural networks","patterns","neural models","face recognition","behavioral perspective model","brand choice","consumer behavior"]},{"p_id":108840,"title":"Building three dimensional head models","abstract":"This paper proposes a camera-based real-time system for building a three dimensional (3D) human head model. The proposed system is first trained in a semiautomatic way to locate the user's facial area and is then used to build a 3D model based on the front and profile views of the user's face. This is achieved by directing the user to position his or her face and profile in a highlighted area, which is used to train a neural network to distinguish the background from the face. With a blink from the user, the system is then capable of accurately locating a set of characteristic feature points on the front and profile views of the face, which are used for the adaptation of a generic 3D face model. This adaptation procedure is initialized with a rigid transformation of the model aiming to minimize the distances of the 3D model feature nodes from the calculated 3D coordinates of the 2D feature points. Then, a nonrigid transformation ensures that the feature nodes are displaced optimally close to their exact calculated positions, dragging their neighbors in a way that deforms the facial model in a natural looking manner. A male hair model is created using a 3D ellipsoid, which is truncated and merged with the adapted face model. A cylindrical texture map is finally built from the two image views covering the whole area of the head by exploiting the inherent face symmetry. The final result is a complete, textured model of a specific person's head. (C) 2001 Elsevier Science (USA).","keywords_author":["3D modeling","facial modeling","model adaptation","feature extraction","ellipsoid fitting"],"keywords_other":["IMAGE SEQUENCES","FACE MODEL","RECOGNITION","SEGMENTATION","ADAPTATION","TRACKING"],"max_cite":8.0,"pub_year":2001.0,"sources":"['wos']","rawkeys":["facial modeling","face model","recognition","segmentation","image sequences","tracking","ellipsoid fitting","3d modeling","feature extraction","adaptation","model adaptation"],"tags":["facial modeling","recognition","segmentation","3d models","image sequences","tracking","ellipsoid fitting","face modeling","feature extraction","adaptation","model adaptation"]},{"p_id":10538,"title":"Efficient Imbalanced Multimedia Concept Retrieval by Deep Learning on Spark Clusters","abstract":"The classification of imbalanced datasets has recently attracted significant attention due to its implications in several real-world use cases. The classifiers developed on datasets with skewed distributions tend to favor the majority classes and are biased against the minority class. Despite extensive research interests, imbalanced data classification remains a challenge in data mining research, especially for multimedia data. Our attempt to overcome this hurdle is to develop a convolutional neural network (CNN) based deep learning solution integrated with a bootstrapping technique. Considering that convolutional neural networks are very computationally expensive coupled with big training datasets, we propose to extract features from pre-trained convolutional neural network models and feed those features to another full connected neutral network. Spark implementation shows promising performance of our model in handling big datasets with respect to feasibility and scalability.","keywords_author":["Apache Spark","Classification","Convolutional Neural Network (CNN)","Deep Learning","Imbalanced Data","Semantic Indexing"],"keywords_other":["ENVIRONMENTS","RECOGNITION","CONVOLUTIONAL NEURAL-NETWORKS"],"max_cite":3.0,"pub_year":2017.0,"sources":"['ieee', 'wos']","rawkeys":["apache spark","recognition","deep learning","imbalanced data","convolutional neural network (cnn)","environments","classification","convolutional neural-networks","semantic indexing"],"tags":["apache spark","recognition","machine learning","imbalanced data","classification","environment","convolutional neural network","semantic indexing"]},{"p_id":10541,"title":"Grammars for Games: A Gradient - Based, Game-Theoretic Framework for Optimization in Deep Learning","abstract":"Deep learning is currently the subject of intensive study. However, fundamental concepts such as representations are not formally defined researchers \"know them when they see them\" and there is no common language for describing and analyzing algorithms. This essay proposes an abstract framework that identifies the essential features of current practice and may provide a foundation for future developments. The backbone of almost all deep learning algorithms is backpropagation, which is simply a gradient computation distributed over a neural network. The main ingredients of the framework are, thus, unsurprisingly: (i) game theory, to formalize distributed optimization; and (ii) communication protocols, to track the flow of zeroth and first-order information. The framework allows natural definitions of semantics (as the meaning encoded in functions), representations (as functions whose semantics is chosen to optimized a criterion), and grammars (as communication protocols equipped with first-order convergence guarantees). Much of the essay is spent discussing examples taken from the literature. The ultimate aim is to develop a graphical language for describing the structure of deep learning algorithms that backgrounds the details of the optimization procedure and foregrounds how the components interact. Inspiration is taken from probabilistic graphical models and factor graphs, which capture the essential structural features of multivariate distributions.","keywords_author":["deep learning","representation learning","optimization","game theory","neural networks"],"keywords_other":["ELEMENTS","REINFORCEMENT","APPROXIMATION","ALGORITHM","REPRESENTATIONS","NEURAL-NETWORKS","RECOGNITION","INTELLIGENCE"],"max_cite":0.0,"pub_year":2016.0,"sources":"['ieee', 'wos']","rawkeys":["neural-networks","algorithm","intelligence","recognition","deep learning","neural networks","reinforcement","approximation","representation learning","representations","optimization","game theory","elements"],"tags":["intelligence","recognition","neural networks","machine learning","representation","approximation","representation learning","optimization","algorithms","game theory","elements"]},{"p_id":108847,"title":"Features for robust face-based identity verification","abstract":"In this paper we propose the discrete cosine transform (DCT) mod2 feature set, which utilizes polynomial coefficients derived from 2D DCT coefficients obtained from spatially neighboring blocks. Face verification results on the multi-session VidTIMIT database suggest that the DCT-mod 2 feature set is superior (in terms of robustness to illumination direction changes and discrimination ability) to features extracted using three popular methods: eigenfaces principal component analysis, 2D DCT and 2D Gabor wavelets. Moreover, compared to Gabor wavelets, the DCT-mod 2 feature set is over 80 times faster to compute. Additional experiments on the Weizmann database also show that the DCT-mod 2 approach is more robust than 2D Gabor wavelets and 2D DCT coefficients. (C) 2003 Elsevier Science B.V. All rights reserved.","keywords_author":["face verification","polynomial coefficients","discrete cosine transform","gabor wavelets","principal component analysis"],"keywords_other":["AUTHENTICATION","SPEAKER VERIFICATION","INFORMATION","ALGORITHM","SYSTEMS","RECOGNITION","EIGENFACES","MODELS"],"max_cite":10.0,"pub_year":2003.0,"sources":"['wos']","rawkeys":["algorithm","discrete cosine transform","principal component analysis","recognition","eigenfaces","authentication","speaker verification","face verification","polynomial coefficients","information","models","gabor wavelets","systems"],"tags":["discrete cosine transform","principal component analysis","recognition","model","eigenfaces","authentication","system","speaker verification","face verification","polynomial coefficients","information","algorithms","gabor wavelets"]},{"p_id":10550,"title":"Deep Learning in Intermediate Microeconomics: Using Scaffolding Assignments to Teach Theory and Promote Transfer","abstract":"Intermediate microeconomics is typically viewed as a theory and tools course that relies on algorithmic problems to help students learn and apply economic theory. However, the authors' assessment research suggests that algorithmic problems by themselves do not encourage students to think about where the theory comes from, why the theory is relevant, or under what conditions different theories and tools should be applied. In this article, the authors draw upon current learning theory to develop a sequence of scaffolding assignments that move students from well-structured algorithmic problems to ill-structured applied problems which encourage more elaborate and robust processing of course concepts. Their assessment data suggest that these assignments promote deep learning of economic theory as well as enhanced ability to transfer learning to later courses.","keywords_author":["deep learning","intermediate microeconomics","scaffolding","transfer","writing to learn"],"keywords_other":["WRITING SKILLS","ECONOMICS"],"max_cite":7.0,"pub_year":2013.0,"sources":"['wos']","rawkeys":["writing to learn","transfer","economics","scaffolding","deep learning","intermediate microeconomics","writing skills"],"tags":["writing to learn","economics","recognition","scaffolding","intermediate microeconomics","machine learning","writing skills"]},{"p_id":108857,"title":"The role of neural networks in improving the accuracy of MR spectroscopy for the diagnosis of head and neck squamous cell carcinoma","abstract":"BACKGROUND AND PURPOSE: MR Spectroscopy (MRS) has the unique ability to analyze tissue at the molecular level noninvasively. The purpose of this study was to determine if peak heights revealed by proton MRS (H-1-MRS) signals showed that neural networks (NN) provided better accuracy than linear discriminant analysis (LDA) in differentiating head and neck squamous cell carcinoma (SCCA) from muscle","keywords_author":null,"keywords_other":["PROTON MR","SPECTRA","IN-VIVO","H-1-NMR SPECTROSCOPY","CLASSIFICATION","HUMAN BRAIN-TUMORS","RECOGNITION","MAGNETIC-RESONANCE SPECTROSCOPY","CANCER","EX-VIVO"],"max_cite":11.0,"pub_year":2000.0,"sources":"['wos']","rawkeys":["recognition","cancer","in-vivo","magnetic-resonance spectroscopy","ex-vivo","h-1-nmr spectroscopy","classification","human brain-tumors","proton mr","spectra"],"tags":["recognition","magnetic-resonance-spectroscopy","cancer","in-vivo","ex-vivo","h-1-nmr spectroscopy","classification","human brain-tumors","proton mr","spectra"]},{"p_id":84282,"title":"Recognition of rotor damages in a DC motor using acoustic signals","abstract":"Diagnosis of electrical direct current motors is essential for industrial plants. The emphasis is put on the development of diagnostic methods of solutions for capturing, processing and recognition of diagnostic signals. This paper presents a technique of early fault diagnosis of a DC motor. The proposed approach is based on acoustic signals. A real-world data of the DC motor were used in the analysis. The work provides an original feature extraction method called the shortened method of frequencies selection (SMoFS-15). The obtained results of the presented analysis show that the early fault diagnostic method can be used for monitoring electrical DC motors. The proposed method can also support other fault diagnosis methods based on thermal, current, and vibration signals.","keywords_author":["acoustic signal","motor","machine","fault diagnosis","recognition"],"keywords_other":["WAVELET TRANSFORM","BEARING FAULT-DIAGNOSIS","COMBUSTION ENGINE","SUPPORT VECTOR MACHINE","TECHNICAL CONDITION","VIBRATION SIGNALS","LINEAR DISCRIMINANT-ANALYSIS","GAUSSIAN MIXTURE-MODELS","INDUCTION-MOTORS","ELECTRICAL MACHINES"],"max_cite":7.0,"pub_year":2017.0,"sources":"['wos']","rawkeys":["wavelet transform","recognition","acoustic signal","linear discriminant-analysis","machine","gaussian mixture-models","technical condition","vibration signals","combustion engine","induction-motors","bearing fault-diagnosis","support vector machine","motor","electrical machines","fault diagnosis"],"tags":["wavelet transform","induction motor","recognition","linear discriminant analysis","motor","machine","machine learning","technical condition","bearing fault diagnosis","combustion engine","acoustic signals","vibration signal","fault diagnosis","electric machines","gaussian mixture model"]},{"p_id":59713,"title":"A Two-Stream Deep Fusion Framework for High-Resolution Aerial Scene Classification","abstract":"One of the challenging problems in understanding high-resolution remote sensing images is aerial scene classification. A well-designed feature representation method and classifier can improve classification accuracy. In this paper, we construct a new two-stream deep architecture for aerial scene classification. First, we use two pretrained convolutional neural networks (CNNs) as feature extractor to learn deep features from the original aerial image and the processed aerial image through saliency detection, respectively. Second, two feature fusion strategies are adopted to fuse the two different types of deep convolutional features extracted by the original RGB stream and the saliency stream. Finally, we use the extreme learning machine (ELM) classifier for final classification with the fused features. The effectiveness of the proposed architecture is tested on four challenging datasets: UC-Merced dataset with 21 scene categories, WHU-RS dataset with 19 scene categories, AID dataset with 30 scene categories, and NWPU-RESISC45 dataset with 45 challenging scene categories. The experimental results demonstrate that our architecture gets a significant classification accuracy improvement over all state-of-the-art references.","keywords_author":null,"keywords_other":["FEATURES","LAND-USE CLASSIFICATION","NEURAL-NETWORKS","RECOGNITION","EXTREME LEARNING-MACHINE","SCALE","SHAPE"],"max_cite":1.0,"pub_year":2018.0,"sources":"['ieee', 'wos']","rawkeys":["neural-networks","recognition","features","land-use classification","scale","shape","extreme learning-machine"],"tags":["recognition","features","neural networks","land-use classification","shape","extreme learning machine","scale"]},{"p_id":108865,"title":"Biometric analysis using fused feature set from side face texture and electrocardiogram","abstract":"Multimodal biometric authentication requires fusion of information extracted from different biometric modalities. Face recognition is the most common and versatile biometric parameter used for years. Recently, biosignals such as electrocardiogram (ECG), photoplathysmogram etc. are under study for probable use in authentication work. It is also established that multi-parameter approach in biometric analysis plays a vital role in increasing accuracy and robustness and preventing spoofing in spite of more computational demand. In the present study, information fusion based authentication system is proposed using face and ECG. Instead of conventional face image, a unique frontal face textural signal is proposed. This leads to simpler data processing similar to that of ECG signal. Finally, information from both the signals is fused at mother template generation level. A good accuracy is achieved using mean square deviation method as presented in the result section. A stability study is also made with five volunteers to check the long term variability of the features.","keywords_author":null,"keywords_other":["HUMAN IDENTIFICATION","DISTANCE","RECOGNITION","ECG","EIGENFACES","FUSION"],"max_cite":0.0,"pub_year":2017.0,"sources":"['wos']","rawkeys":["recognition","ecg","eigenfaces","distance","fusion","human identification"],"tags":["recognition","ecg","eigenfaces","distance","fusion","human identification"]},{"p_id":108869,"title":"Second Order Training of a Smoothed Piecewise Linear Network","abstract":"In this paper, we introduce a smoothed piecewise linear network (SPLN) and develop second order training algorithms for it. An embedded feature selection algorithm is developed which minimizes training error with respect to distance measure weights. Then a method is presented which adjusts center vector locations in the SPLN. We also present a gradient method for optimizing the SPLN output weights. Results with several data sets show that the distance measure optimization, center vector optimization, and output weight optimization, individually and together, reduce testing errors in the final network.","keywords_author":["Smoothed PLN","Embedded feature selection","Optimizing center vectors"],"keywords_other":["DATA SET","MATRIX","ALGORITHM","CONVERGENCE","MODEL","MINIMIZATION","ARTIFICIAL NEURAL-NETWORKS","RECOGNITION"],"max_cite":0.0,"pub_year":2017.0,"sources":"['wos']","rawkeys":["convergence","algorithm","optimizing center vectors","recognition","model","minimization","matrix","embedded feature selection","data set","artificial neural-networks","smoothed pln"],"tags":["optimizing center vectors","recognition","model","minimization","matrix","neural networks","data sets","smoothed pln","mathematics","algorithms","embedded feature selections"]},{"p_id":59719,"title":"Improved Class-Specific Codebook with Two-Step Classification for Scene-Level Classification of High Resolution Remote Sensing Images","abstract":"With the rapid advances in sensors of remote sensing satellites, a large number of high-resolution images (HRIs) can be accessed every day. Land use classification using high-resolution images has become increasingly important as it can help to overcome the problems of haphazard, deteriorating environmental quality, loss of prime agricultural lands, and destruction of important wetlands, and so on. Recently, local feature with bag-of-words (BOW) representation has been successfully applied to land-use scene classification with HRIs. However, the BOW representation ignores information from scene labels, which is critical for scene-level land-use classification. Several algorithms have incorporated information from scene labels into BOW by calculating a class-specific codebook from the universal codebook and coding a testing image with a number of histograms. Those methods for mapping the BOW feature to some inaccurate class-specific codebooks may increase the classification error. To effectively solve this problem, we propose an improved class-specific codebook using kernel collaborative representation based classification (KCRC) combined with SPM approach and SVM classifier to classify the testing image in two steps. This model is robust for categories with similar backgrounds. On the standard Land use and Land Cover image dataset, the improved class-specific codebook achieves an average classification accuracy of 93% and demonstrates superiority over other state-of-the-art scene-level classification methods.","keywords_author":["scene-level land use classification","Bag-of-words (BOW)","improved class-specific codebook","kernel collaborative representative based classification combined with SPM","two-step classification"],"keywords_other":["SELECTION","FEATURES","RECOGNITION","SCALE","LEARNING ALGORITHMS"],"max_cite":2.0,"pub_year":2017.0,"sources":"['ieee', 'wos']","rawkeys":["kernel collaborative representative based classification combined with spm","two-step classification","recognition","improved class-specific codebook","bag-of-words (bow)","features","learning algorithms","scene-level land use classification","selection","scale"],"tags":["kernel collaborative representative based classification combined with spm","bag of words","two-step classification","recognition","improved class-specific codebook","features","scene-level land use classification","selection","learning algorithm","scale"]},{"p_id":59720,"title":"Hierarchical multi-class classification in multimodal spacecraft data using DNN and weighted support vector machine","abstract":"Prognostics and health management (PHM) is widely applied to assess the reliability, safety and operation of systems particularly in spacecraft systems. However, spacecraft systems are very complex with intangibility and uncertainty, and it is difficult to model and analyze the complex degradation process, and thus there is no single prognostic method for solving the critical and complicated problem. This paper presents a novel hierarchical multi-class classification method using deep neural networks (DNN) and weighted support vector machine (WSVM) in order to achieve a highly discriminative feature representation for classifying the multimodal spacecraft data. First, the stack auto-Encoder (SAE) or deep belief network is adopted to initialize the initial weights and offsets of the hierarchical multi-layer neural network in order to reduce the dimension of the original multimodal data, and the optimal depth of multi-layer neural network and the discriminative features are also obtained. Second, in order to make the high dimensional spacecraft data more separable, the initialization parameters are online monitored by using a gradient descent method. Finally, a flexible hierarchical estimation method of a multi-class weighted support vector machines (MCWSVM) is applied to classify the multimodal spacecraft data. The performance of the proposed work is evaluated by the classification accuracy, sensitivity, specificity and execution time, respectively. The results demonstrate that the proposed DNN with MCWSVM is efficient in terms of better classification accuracy at a lesser execution time when compared to K-nearest neighbors (KNN), SVM and naive Bayes method (NBM). (C) 2017 Elsevier B.V. All rights reserved.","keywords_author":["Prognostics and health management (PHM)","Deep neural network (DNN)","Multi-modal spacecraft data","Weighted support vector machine (WSVM)","Deep belief network"],"keywords_other":["SVM","FEATURES","RESTRICTED BOLTZMANN MACHINES","SYSTEMS","NEURAL-NETWORKS","RECOGNITION","PATTERN","CABIN","ARCHITECTURE","PCA"],"max_cite":2.0,"pub_year":2017.0,"sources":"['ieee', 'wos']","rawkeys":["cabin","deep neural network (dnn)","neural-networks","pattern","recognition","restricted boltzmann machines","features","multi-modal spacecraft data","prognostics and health management (phm)","svm","deep belief network","weighted support vector machine (wsvm)","systems","pca","architecture"],"tags":["cabin","principal component analysis","recognition","features","neural networks","weighted support vector machine","prognostics and health managements","multi-modal spacecraft data","patterns","machine learning","system","convolutional neural network","restricted boltzmann machine","deep belief networks","architecture"]},{"p_id":108872,"title":"Facial\/License Plate Detection Using a Two-level Cascade Classifier and a Single Convolutional Feature Map","abstract":"In this paper, an object detector is proposed based on a convolution\/subsampling feature map and a two-level cascade classifier. First, a convolution\/subsampling operation alleviates illumination, rotation and noise variances. Then, two classifiers are concatenated to check a large number of windows using a coarse-to-fine strategy. Since the sub-sampled feature map with enhanced pixels was fed into the coarse-level classifier, the checked windows were drastically reduced to a quarter of the original image. A few remaining windows showing detailed data were further checked using a fine-level classifier.","keywords_author":["Face Detection","Coarse-to-fine Strategy","Convolution\/Subsampling Feature Map","Feature Ranking"],"keywords_other":["NEURAL-NETWORK","RECOGNITION","ROBUST FACE DETECTION","IMAGES"],"max_cite":0.0,"pub_year":2015.0,"sources":"['wos']","rawkeys":["robust face detection","recognition","images","convolution\/subsampling feature map","face detection","neural-network","feature ranking","coarse-to-fine strategy"],"tags":["robust face detection","recognition","images","neural networks","convolution\/subsampling feature map","face detection","feature ranking","coarse-to-fine strategy"]},{"p_id":59722,"title":"Kernel Slow Feature Analysis for Scene Change Detection","abstract":"Scene change detection between multitemporal image scenes can be used to interpret the variation of regional land use, and has significant potential in the application of urban development monitoring at the semantic level. The traditional methods directly comparing the independent semantic classes neglect the temporal correlation, and thus suffer from accumulated classification errors. In this paper, we propose a novel scene change detection method via kernel slow feature analysis (KSFA) and postclassification fusion, which integrates independent scene classification with scene change detection to accurately determine scene changes and identify the \"from-to\" transition type. After representation with the bag-of-visual-words model, KSFA is proposed to extract the nonlinear temporally invariant features, to better measure the change probability between corresponding multitemporal image scenes. Two postclassification fusion methods, which are based on Bayesian theory and predefined rules, respectively, are then employed to identify the optimal coupled class combinations of multitemporal scene pairs. Furthermore, in addition to identifying semantic changes, the proposed method can also improve the performance of scene classification, since the unchanged scenes are more likely to belong to the same class. Two experiments with high-resolution remote sensing image scene data sets confirm that the proposed method can increase the accuracy of scene change detection, scene transition identification, and scene classification.","keywords_author":["Bayesian theory","kernel slow feature analysis (KSFA)","rule-based method","remote sensing","scene change detection"],"keywords_other":["LAND-COVER TRANSITIONS","FISHER DISCRIMINANT","FRAMEWORK","CLASSIFICATION","RECOGNITION","REMOTE-SENSING IMAGERY","PLSA","KPCA PLUS LDA"],"max_cite":4.0,"pub_year":2017.0,"sources":"['ieee', 'wos']","rawkeys":["kernel slow feature analysis (ksfa)","remote-sensing imagery","fisher discriminant","recognition","remote sensing","land-cover transitions","framework","scene change detection","kpca plus lda","bayesian theory","classification","rule-based method","plsa"],"tags":["probabilistic latent semantic analysis","kernel slow feature analysis (ksfa)","remote-sensing imagery","fisher discriminant","recognition","remote sensing","framework","scene change detection","kpca plus lda","bayesian theory","classification","land cover transition","rule-based method"]},{"p_id":43347,"title":"Trends in biomedical signal feature extraction","abstract":"\u00a9 2018 Elsevier Ltd Signal analysis involves identifying signal behaviour, extracting linear and non-linear properties, compression or expansion into higher or lower dimensions, and recognizing patterns. Over the last few decades, signal processing has taken notable evolutionary leaps in terms of measurement \u2013 from being simple techniques for analysing analog or digital signals in time, frequency or joint time\u2013frequency (TF) domain, to being complex techniques for analysis and interpretation in a higher dimensional domain. The intention behind this is simple \u2013 robust and efficient feature extraction; i.e. to identify specific signal markers or properties exhibited in one event, and use them to distinguish from characteristics exhibited in another event. The objective of our study is to give the reader a bird's eye view of the biomedical signal processing world with a zoomed-in perspective of feature extraction methodologies which form the basis of machine learning and hence, artificial intelligence. We delve into the vast world of feature extraction going across the evolutionary chain starting with basic A-to-D conversion, to domain transformations, to sparse signal representations and compressive sensing. It should be noted that in this manuscript we have attempted to explain key biomedical signal feature extraction methods in simpler fashion without detailing over mathematical representations. Additionally we have briefly touched upon the aspects of curse and blessings of signal dimensionality which would finally help us in determining the best combination of signal processing methods which could yield an efficient feature extractor. In other words, similar to how the laws of science behind some common engineering techniques are explained, in this review study we have attempted to postulate an approach towards a meaningful explanation behind those methods in developing a convincing and explainable reason as to which feature extraction method is suitable for a given biomedical signal.","keywords_author":["Biomedical signal processing","Dimensionality reduction","Feature extraction","Machine learning","Pattern classification","Feature extraction","Biomedical signal processing","Pattern classification","Dimensionality reduction","Machine learning"],"keywords_other":["Engineering techniques","Feature extraction methods","TRANSFORM","Compressive sensing","Dimensionality reduction","Nonlinear properties","REPRESENTATION","ALGORITHM","TIME-FREQUENCY ANALYSIS","Sparse signal representation","PATHOLOGICAL VOICES","IDENTIFICATION","IMAGE CLASSIFICATION","RECOGNITION","Domain transformation","MIXTURE","Mathematical representations","EMPIRICAL MODE DECOMPOSITION"],"max_cite":0.0,"pub_year":2018.0,"sources":"['ieee', 'scp', 'wos']","rawkeys":["identification","feature extraction methods","biomedical signal processing","pathological voices","time-frequency analysis","image classification","dimensionality reduction","machine learning","mathematical representations","compressive sensing","mixture","transform","engineering techniques","empirical mode decomposition","algorithm","recognition","pattern classification","domain transformation","sparse signal representation","nonlinear properties","representation","feature extraction"],"tags":["identification","feature extraction methods","biomedical signal processing","time-frequency analysis","image classification","dimensionality reduction","machine learning","mathematical representations","compressive sensing","mixture","algorithms","transform","engineering techniques","empirical mode decomposition","recognition","pathological voice","pattern classification","domain transformation","sparse signal representation","nonlinear properties","representation","feature extraction"]},{"p_id":2395,"title":"Two-stream deep architecture for hyperspectral image classification","abstract":"\u00a9 1980-2012 IEEE. Most traditional approaches classify hyperspectral image (HSI) pixels relying only on the spectral values of the input channels. However, the spatial context around a pixel is also very important and can enhance the classification performance. In order to effectively exploit and fuse both the spatial context and spectral structure, we propose a novel two-stream deep architecture for HSI classification. The proposed method consists of a two-stream architecture and a novel fusion scheme. In the two-stream architecture, one stream employs the stacked denoising autoencoder to encode the spectral values of each input pixel, and the other stream takes as input the corresponding image patch and deep convolutional neural networks are employed to process the image patch. In the fusion scheme, the prediction probabilities from two streams are fused by adaptive class-specific weights, which can be obtained by a fully connected layer. Finally, a weight regularizer is added to the loss function to alleviate the overfitting of the class-specific fusion weights. Experimental results on real HSIs demonstrate that the proposed two-stream deep architecture can achieve competitive performance compared with the state-of-the-art methods.","keywords_author":["Class-specific fusion","Convolutional neural networks (CNNs)","Deep learning","Hyperspectral image (HSI) classification","Remote sensing","Stacked denoising autoencoder (SdAE)","Two-stream architecture","Class-specific fusion","convolutional neural networks (CNNs)","deep learning","hyperspectral image (HSI) classification","remote sensing","stacked denoising autoencoder (SdAE)","two-stream architecture"],"keywords_other":["Two-stream","PROFILES","Classification performance","Traditional approaches","State-of-the-art methods","REPRESENTATION","Competitive performance","LATE FUSION","RECOGNITION","Convolutional neural network","Auto encoders","Prediction probabilities","NETWORK"],"max_cite":1.0,"pub_year":2018.0,"sources":"['scp', 'wos', 'ieee']","rawkeys":["state-of-the-art methods","remote sensing","two-stream","convolutional neural network","convolutional neural networks (cnns)","traditional approaches","stacked denoising autoencoder (sdae)","competitive performance","hyperspectral image (hsi) classification","class-specific fusion","prediction probabilities","network","recognition","classification performance","deep learning","two-stream architecture","late fusion","auto encoders","representation","profiles"],"tags":["state-of-the-art methods","remote sensing","two-stream","convolutional neural network","traditional approaches","competitive performance","machine learning","class-specific fusion","prediction probabilities","recognition","classification performance","networks","two-stream architecture","late fusion","auto encoders","hyperspectral image classification","representation","profiles","stacked denoising autoencoder"]},{"p_id":2397,"title":"SAR image classification via deep recurrent encoding neural networks","abstract":"Synthetic aperture radar (SAR) image classification is a fundamental process for SAR image understanding and interpretation. With the advancement of imaging techniques, it permits to produce higher resolution SAR data and extend data amount. Therefore, intelligent algorithms for high-resolution SAR image classification are demanded. Inspired by deep learning technology, an end-to-end classification model from the original SAR image to final classification map is developed to automatically extract features and conduct classification, which is named deep recurrent encoding neural networks (DRENNs). In our proposed framework, a spatial feature learning network based on long-short-term memory (LSTM) is developed to extract contextual dependencies of SAR images, where 2-D image patches are transformed into 1-D sequences and imported into LSTM to learn the latent spatial correlations. After LSTM, non-negative and Fisher constrained autoencoders (NFCAEs) are proposed to improve the discrimination of features and conduct final classification, where nonnegative constraint and Fisher constraint are developed in each autoencoder to restrict the training of the network. The whole DRENN not only combines the spatial feature learning power of LSTM but also utilizes the discriminative representation ability of our NFCAE to improve the classification performance. The experimental results tested on three SAR images demonstrate that the proposed DRENN is able to learn effective feature representations from SAR images and produce competitive classification accuracies to other related approaches.","keywords_author":["Deep learning","Image classification","Recurrent neural network (RNN)","Stacked autoencoders (SAEs)","Synthetic aperture radar (SAR) image","Deep learning","image classification","recurrent neural network (RNN)","stacked autoencoders (SAEs)","synthetic aperture radar (SAR) image"],"keywords_other":["Classification performance","Synthetic aperture radar (SAR) images","SAR image classifications","DATA FUSION","REPRESENTATION","SYNTHETIC-APERTURE RADAR","AUTOENCODER","ALGORITHM","Recurrent neural network (RNN)","Feature representation","RECOGNITION","SHORT-TERM-MEMORY","Intelligent Algorithms","Classification accuracy","MACHINES","Autoencoders"],"max_cite":0.0,"pub_year":2018.0,"sources":"['scp', 'wos', 'ieee']","rawkeys":["short-term-memory","autoencoder","image classification","machines","algorithm","synthetic aperture radar (sar) image","recognition","classification performance","intelligent algorithms","deep learning","recurrent neural network (rnn)","sar image classifications","stacked autoencoders (saes)","feature representation","synthetic aperture radar (sar) images","data fusion","classification accuracy","representation","autoencoders","synthetic-aperture radar"],"tags":["recognition","data fusion","classification performance","intelligent algorithms","neural networks","synthetic aperture radar","classification accuracy","machine","auto encoders","machine learning","representation","short-term-memory","stacked autoencoders","sar image classification","algorithms","feature representation","image classification","synthetic aperture radar (sar) images"]},{"p_id":10589,"title":"Fine-grained leukocyte classification with deep residual learning for microscopic images","abstract":"Background and objective: Leukocyte classification and cytometry have wide applications in medical domain, previous researches usually exploit machine learning techniques to classify leukocytes automatically. However, constrained by the past development of machine learning techniques, for example, extracting distinctive features from raw microscopic images are difficult, the widely used SVM classifier only has relative few parameters to tune, these methods cannot efficiently handle fine-grained classification cases when the white blood cells have up to 40 categories.","keywords_author":["Deep learning","Image analysis","Leukocyte","Residual learning","Leukocyte","Image analysis","Deep learning","Residual learning"],"keywords_other":["BLOOD","Residual learning","White blood cells","Training procedures","Classification methods","Machine learning techniques","Leukocyte","RECOGNITION","OPTIMIZATION","Generalization ability","Neural network classifier"],"max_cite":0.0,"pub_year":2018.0,"sources":"['scp', 'wos']","rawkeys":["neural network classifier","residual learning","recognition","classification methods","deep learning","white blood cells","machine learning techniques","training procedures","blood","optimization","leukocyte","generalization ability","image analysis"],"tags":["neural network classifier","residual learning","recognition","classification methods","white blood cells","machine learning techniques","machine learning","training procedures","blood","optimization","leukocyte","generalization ability","image analysis"]},{"p_id":10593,"title":"Automatic identification and classification in lithology based on deep learning in rock images","abstract":"It is important for geology analysis to make identification and classification in lithology. It is a new way to establish the identification model in machine learning. In this research, a transfer learning model of rock images was built based on the Inception-v3 model. It was adapted to process 173 granite images, 152 phyllite images and 246 breccia images to train the transfer learning model. Images in trained data set and in test data set were used to test the model, respectively. 3 images in each group from the trained data set were selected to test the model. There were no identification and classification errors and the all of the probabilities were more than 90%. 9 images in each group from the test data set were also selected to test the model. There were no identification and classification errors. The probabilities of phyllite group were more than 90%. While, the probabilities of 2 images in granite and 1 image in breccia group were less than 70%. It was thought that there were fewer images with similar pattern leading to the bad results. To verify the hypothesis, cut the images with low probabilities and added 3 images to the trained data set in each group to retrain the model. The 3 images with low probabilities were tested in the retrained model and their probabilities were more than 85%. It showed the model had good robustness and generalization if there were enough images. Compared with the traditional machine learning, the proposed method has much strength. First, there is no need to do manual tuning and it processes the data in the model automatically. Second, there is no specific requirement in image pixel, distance and size. At last, the model can have a robust identification and classification result if a suitable trained data set is adopted.","keywords_author":["Rock images","Deep learning","Lithology identification","Automatic classification","Transfer learning"],"keywords_other":["NEURAL-NETWORKS","RECOGNITION","SR"],"max_cite":1.0,"pub_year":2018.0,"sources":"['wos']","rawkeys":["neural-networks","recognition","deep learning","automatic classification","transfer learning","sr","lithology identification","rock images"],"tags":["recognition","neural networks","automatic classification","transfer learning","sparse representation","machine learning","lithology identification","rock images"]},{"p_id":100717,"title":"Iterative individual plant clustering in maize with assembled 2D LiDAR data","abstract":"A two dimensional (2D) laser scanner was mounted at the front part of a small 4-wheel autonomous robot with differential steering, at an angle of 30 degrees pointing downwards. The machine was able to drive between maize rows and collect concurrent time-stamped data. A robotic total station tracked the position of a prism mounted on the vehicle. The total station and laser scanner data were fused to generate a three dimensional (3D) point cloud. This 3D representation was used to detect individual plant positions, which are of particular interest for applications such as phenotyping, individual plant treatment and precision weeding. Two different methodologies were applied to the 3D point cloud to estimate the position of the individual plants. The first methodology used the Euclidian Clustering on the entire point cloud. The second methodology utilised the position of an initial plant and the fixed plant spacing to search iteratively for the best clusters. The two algorithms were applied at three different plant growth stages. For the first method, results indicated a detection rate up to 73.7% with a root mean square error of 3.6 cm. The second method was able to detect all plants (100% detection rate) with an accuracy of 2.7-3.0 cm, taking the plant spacing of 13 cm into account.","keywords_author":["LiDAR","Individual plant detection","Context","Iterative plant clustering","Total station","Stem detection"],"keywords_other":["SENSOR","AGRICULTURAL ROBOTS","CLASSIFICATION","SYSTEM","MODEL","RECOGNITION","POINT CLOUDS","NAVIGATION","CROPS"],"max_cite":0.0,"pub_year":2018.0,"sources":"['wos']","rawkeys":["recognition","model","point clouds","total station","crops","sensor","agricultural robots","stem detection","system","iterative plant clustering","lidar","classification","context","navigation","individual plant detection"],"tags":["point cloud","recognition","model","sensors","total station","crops","system","stem detection","iterative plant clustering","lidar","agricultural robot","classification","context","navigation","individual plant detection"]},{"p_id":43374,"title":"Effective neural network training with adaptive learning rate based on training loss","abstract":"\u00a9 2018 Elsevier Ltd A method that uses an adaptive learning rate is presented for training neural networks. Unlike most conventional updating methods in which the learning rate gradually decreases during training, the proposed method increases or decreases the learning rate adaptively so that the training loss (the sum of cross-entropy losses for all training samples) decreases as much as possible. It thus provides a wider search range for solutions and thus a lower test error rate. The experiments with some well-known datasets to train a multilayer perceptron show that the proposed method is effective for obtaining a better test accuracy under certain conditions.","keywords_author":["Beam search","Deep learning","Learning rate","Multilayer perceptron","Neural network training","Stochastic gradient descent","Multilayer perceptron","Deep learning","Neural network training","Stochastic gradient descent","Learning rate","Beam search"],"keywords_other":["Stochastic gradient descent","Neural network training","DROPOUT","Updating methods","Training sample","Test accuracy","RECOGNITION","Learning rates","Adaptive learning rates","Beam search"],"max_cite":0.0,"pub_year":2018.0,"sources":"['ieee', 'scp', 'wos']","rawkeys":["test accuracy","training sample","dropout","neural network training","multilayer perceptron","recognition","deep learning","beam search","learning rates","learning rate","stochastic gradient descent","adaptive learning rates","updating methods"],"tags":["test accuracy","training sample","dropout","neural network training","recognition","beam search","machine learning","adaptive learning rates","stochastic gradient descent","learning rates","updating methods","multi layer perceptron"]},{"p_id":10610,"title":"Intelligent alerting for fruit-melon lesion image based on momentum deep learning","abstract":"Sensors and Internet of things (IoT) have been widely used in the digitalized orchards. Traditional disease-pest recognition and early warning systems, which are based on knowledge rule, expose many defects, discommodities, and it is difficult to meet current production management requirements of the fresh planting environment. On purpose to realize an intelligent unattended alerting for disease-pest of fruit-melon, this paper presents the convolutional neural network (CNN) for recognition of fruit-melon skin lesion image which is real-timely acquired by an infrared video sensor, which network is grounded upon so-called momentum deep learning rule. More specifically, (1) a suite of transformation methods of apple skin lesion image is devised to simulate orientation and light disturbance which always occurs in orchards, then to output a self-contained set of almost all lesion images which might appear in various dynamic sensing environment; and (2) the rule of variable momentum learning is formulated to update the free parameters of CNN. Experimental results demonstrate that the proposed presents a satisfying accuracy and recall rate which are up to 97.5 %, 98.5 % respectively. As compared with some shallow learning algorithms and generally accepted deep learning ones, it also offers a gratifying smoothness, stableness after convergence and a quick converging speed. In addition, the statistics from experiments of different benchmark data-sets suggests it is very effective to recognize image pattern.","keywords_author":["CNN","Deep network","Intelligent alerting","Lesion image","Momentum learning","Lesion image","CNN","Deep network","Momentum learning","Intelligent alerting"],"keywords_other":["BELIEF NETWORKS","Skin lesion images","EXPERT-SYSTEM","APPLE-TREES","DISEASE","Intelligent alerting","CNN","NEURAL-NETWORKS","Transformation methods","RECOGNITION","Early Warning System","Convolutional neural network","Lesion image","Internet of Things (IOT)"],"max_cite":4.0,"pub_year":2016.0,"sources":"['ieee', 'scp', 'wos']","rawkeys":["lesion image","neural-networks","momentum learning","recognition","apple-trees","disease","expert-system","cnn","deep network","skin lesion images","internet of things (iot)","convolutional neural network","transformation methods","belief networks","early warning system","intelligent alerting"],"tags":["lesion image","momentum learning","recognition","neural networks","apple-trees","disease","skin lesion images","internet of things (iot)","convolutional neural network","transformation methods","belief networks","early warning system","deep networks","expert system","intelligent alerting"]},{"p_id":10611,"title":"Nrityabodha: Towards understanding Indian classical dance using a deep learning approach","abstract":"Indian classical dance has existed since over 5000 years and is widely practised and performed all over the world. However, the semantic meaning of the dance gestures and body postures as well as the intricate steps accompanied by music and recital of poems is only understood fully by the connoisseur. The common masses who watch a concert rarely appreciate or understand the ideas conveyed by the dancer. Can machine learning algorithms aid a novice to understand the semantic intricacies being expertly conveyed by the dancer? In this work, we aim to address this highly challenging problem and propose deep learning based algorithms to identify body postures and hand gestures in order to comprehend the intended meaning of the dance performance. Specifically, we propose a convolutional neural network and validate its performance on standard datasets for poses and hand gestures as well as on constrained and real-world datasets of classical dance. We use transfer learning to show that the pre trained deep networks can reduce the time taken during training and also improve accuracy. Interestingly, we show with experiments performed using Kinect in constrained laboratory settings and data from Youtube that it is possible to identify body poses and hand gestures of the performer to understand the semantic meaning of the enacted dance piece. (C) 2016 Elsevier B.V. All rights reserved.","keywords_author":["Body pose estimation","Convolutional neural network","Deep learning","Gesture recognition","Histogram-of-gradients","Kinect","Deep learning","Convolutional neural network","Gesture recognition","Body pose estimation","Kinect","Histogram-of-gradients"],"keywords_other":["Body pose estimation","Deep learning","HAND POSE ESTIMATION","Histogram of gradients","TRACKING","POSTURE","RECOGNITION","PARTS","IMAGES","Convolutional neural network","PROPAGATION","MODELS","Kinect","PICTORIAL STRUCTURES","COMPLEX BACKGROUNDS"],"max_cite":5.0,"pub_year":2016.0,"sources":"['scp', 'wos']","rawkeys":["recognition","images","complex backgrounds","posture","deep learning","kinect","gesture recognition","pictorial structures","body pose estimation","tracking","parts","convolutional neural network","models","propagation","hand pose estimation","histogram-of-gradients","histogram of gradients"],"tags":["recognition","images","model","posture","kinect","machine learning","gesture recognition","body pose estimation","histogram of oriented gradients","pictorial structures","hand pose estimations","tracking","parts","convolutional neural network","propagation","complex background"]},{"p_id":76162,"title":"Creating learner-centered assessment strategies for promoting greater student retention and class participation","abstract":null,"keywords_author":["assessment","learner-centered","engagement","higher-order cognition","psychology"],"keywords_other":["MULTIPLE-CHOICE","STYLES","TEACHER","TESTS","MOTIVATION"],"max_cite":1.0,"pub_year":2014.0,"sources":"['wos']","rawkeys":["multiple-choice","teacher","higher-order cognition","assessment","engagement","tests","motivation","styles","psychology","learner-centered"],"tags":["multiple-choice","recognition","testing","higher-order cognition","assessment","engagement","teachers","motivation","styles","learner-centered"]},{"p_id":10634,"title":"Deep BBN Learning for Health Assessment toward Decision-Making on Structures under Uncertainties","abstract":"Structural systems are often exposed to harsh environment, while these environmental factors in turn could degrade the system over time. Their health state and structural conditions are key for structural safety control and decision-making management. Although great efforts have been paid on this field, the high level of variability due to noise and other interferences, and the uncertainties associated with data collection, structural performance and in-service operational environments post great challenges in finding information to assist decision making. The machine learning techniques in recent years have been gaining increasing attentions due to their merits capturing information from statistical representation of events and thus enabling making decision. In this study, the deep Bayesian Belief Network Learning (DBBN) was used to extract structural information and probabilistically determine structural conditions. Different to conventional shallow learning that highly relies on the quality of the hand-crafted features, the deep learning is an end-to-end method to encode the information and interpret vast amount of data with minimizing or no features. A case study was conducted to address the methods for structure under viabilities and uncertainties due to operation, damage and noise interferences. Numerical results revealed that the deep learning exhibits considerably enhanced accuracy for structural diagnostics, as compared to the supervised shallow learning. With predetermined training set, the DBBN could accurately determine the structural health state in terms of damage level, which could dramatically help decision making for further structural retrofit or not. Note that the noise interference could contaminate the data representation and in turn increase the difficulty of the data mining, though the deep learning could reduce the impacts, as compared to conventional shallow learning techniques.","keywords_author":["deep Bayesian belief network learning","deep learning","probabilistic learning","structural condition assessment","deep learning","probabilistic learning","deep Bayesian belief network learning","structural condition assessment"],"keywords_other":["Probabilistic Learning","DIAGNOSIS","deep Bayesian belief network learning","Decision making managements","Structural diagnostics","SUPPORT VECTOR MACHINES","Structural condition","Operational environments","NEURAL-NETWORKS","DAMAGE DETECTION","CLASSIFICATION","RECOGNITION","ALGORITHMS","IDENTIFICATION","OPTIMIZATION","FEATURE-EXTRACTION","Statistical representations","Machine learning techniques"],"max_cite":1.0,"pub_year":2018.0,"sources":"['ieee', 'scp', 'wos']","rawkeys":["identification","deep bayesian belief network learning","classification","neural-networks","structural diagnostics","optimization","algorithms","diagnosis","recognition","deep learning","feature-extraction","machine learning techniques","probabilistic learning","damage detection","decision making managements","statistical representations","structural condition","support vector machines","structural condition assessment","operational environments"],"tags":["identification","deep bayesian belief network learning","classification","machine learning","structural diagnostics","optimization","algorithms","diagnosis","recognition","operating environment","neural networks","machine learning techniques","probabilistic learning","damage detection","decision making managements","statistical representations","structural condition","feature extraction","structural condition assessment"]},{"p_id":2443,"title":"Denoised Senone I-Vectors for Robust Speaker Verification","abstract":"\u00a9 2014 IEEE. Recently, it has been shown that senone i-vectors, whose posteriors are produced by senone deep neural networks (DNNs), outperform the conventional Gaussian mixture model (GMM) i-vectors in both speaker and language recognition tasks. The success of senone i-vectors relies on the capability of the DNN to incorporate phonetic information into the i-vector extraction process. In this paper, we argue that to apply senone i-vectors in noisy environments, it is important to robustify the phonetically discriminative acoustic features and senone posteriors estimated by the DNN. To this end, we propose a deep architecture formed by stacking a deep belief network on top of a denoising autoencoder (DAE). After backpropagation fine-tuning, the network, referred to as denoising autoencoder-deep neural network (DAE-DNN), facilitates the extraction of robust phonetically-discriminitive bottleneck (BN) features and senone posteriors for i-vector extraction. We refer to the resulting i-vectors as denoised BN-based senone i-vectors. Results on NIST 2012 SRE show that senone i-vectors outperform the conventional GMM i-vectors. More interestingly, the BN features are not only phonetically discriminative, results suggest that they also contain sufficient speaker information to produce BN-based senone i-vectors that outperform the conventional senone i-vectors. This work also shows that DAE training is more beneficial to BN feature extraction than senone posterior estimation.","keywords_author":["deep learning","denoising autoencoders","i-vectors","noise robustness","phonetically discriminative features","senone posteriors","Speaker verification","Speaker verification","i-vectors","phonetically discriminative features","senone posteriors","deep learning","denoising autoencoders","noise robustness"],"keywords_other":["Speaker verification","senone posteriors","I vectors","RECOGNITION","Noise robustness","Discriminative features","NETWORK","Autoencoders"],"max_cite":0.0,"pub_year":2018.0,"sources":"['wos', 'scp', 'ieee']","rawkeys":["network","recognition","i vectors","deep learning","phonetically discriminative features","i-vectors","denoising autoencoders","noise robustness","senone posteriors","speaker verification","autoencoders","discriminative features"],"tags":["recognition","denoising autoencoder","phonetically discriminative features","auto encoders","machine learning","noise robustness","senone posteriors","speaker verification","discriminative features","networks","i-vector"]},{"p_id":67988,"title":"Missense variant pathogenicity predictors generalize well across a range of function-specific prediction challenges","abstract":"The steady advances in machine learning and accumulation of biomedical data have contributed to the development of numerous computational models that assess the impact of missense variants. Different methods, however, operationalize impact differently. Two common tasks in this context are the prediction of the pathogenicity of variants and the prediction of their effects on a protein's function. These are related but distinct problems, and it is unclear whether methods developed for one are optimized for the other. The Critical Assessment of Genome Interpretation (CAGI) experiment provides a means to address this question empirically. To this end, we participated in various protein-specific challenges in CAGI with two objectives in mind. First, to compare the performance of methods in the MutPred family with the state-of-the-art. Second and more importantly, to investigate the applicability of general-purpose pathogenicity predictors to the classification of specific function-altering variants without additional training or calibration. We find that our pathogenicity predictors performed competitively with other methods, outputting score distributions in agreement with experimental outcomes. Overall, we conclude that binary classifiers learned from disease-causing mutations are capable of modeling important aspects of the underlying biology and the alteration of protein function resulting from mutations.","keywords_author":["CAGI","functional effect prediction","generalization","machine learning","MutPred","MutPred2","pathogenicity prediction","severity"],"keywords_other":["MECHANISMS","GUIDELINES","HUMAN-DISEASE","INFORMATION","POLYMORPHISMS","GENETIC-VARIATION","MUTATIONS","AMINO-ACID SUBSTITUTIONS","SEQUENCE VARIANTS"],"max_cite":1.0,"pub_year":2017.0,"sources":"['wos']","rawkeys":["cagi","guidelines","sequence variants","polymorphisms","mechanisms","functional effect prediction","mutpred2","genetic-variation","machine learning","severity","generalization","pathogenicity prediction","information","mutpred","amino-acid substitutions","mutations","human-disease"],"tags":["cagi","guidelines","sequence variants","recognition","mechanisms","functional effect prediction","mutpred2","machine learning","severity","pathogenicity prediction","information","mutpred","amino-acid substitutions","mutations","genetic variation","polymorphism","human-disease"]},{"p_id":10645,"title":"A Review of Deep Learning Methods and Applications for Unmanned Aerial Vehicles","abstract":"Deep learning is recently showing outstanding results for solving a wide variety of robotic tasks in the areas of perception, planning, localization, and control. Its excellent capabilities for learning representations from the complex data acquired in real environments make it extremely suitable for many kinds of autonomous robotic applications. In parallel, Unmanned Aerial Vehicles (UAVs) are currently being extensively applied for several types of civilian tasks in applications going from security, surveillance, and disaster rescue to parcel delivery or warehouse management. In this paper, a thorough review has been performed on recent reported uses and applications of deep learning for UAVs, including the most relevant developments as well as their performances and limitations. In addition, a detailed explanation of the main deep learning techniques is provided. We conclude with a description of the main challenges for the application of deep learning for UAV-based solutions.","keywords_author":null,"keywords_other":["IMAGERY","ALGORITHM","CLASSIFICATION","TRAILS","NEURAL-NETWORKS","RECOGNITION","UAV"],"max_cite":1.0,"pub_year":2017.0,"sources":"['ieee', 'wos']","rawkeys":["neural-networks","algorithm","recognition","trails","uav","classification","imagery"],"tags":["unmanned aerial vehicle","recognition","neural networks","trails","classification","algorithms","imagery"]},{"p_id":10649,"title":"Exemplar based Deep Discriminative and Shareable Feature Learning for scene image classification","abstract":"In order to encode the class correlation and class specific information in image representation, we propose a new local feature learning approach named Deep Discriminative and Shareable Feature Learning (DDSFL). DDSFL aims to hierarchically learn feature transformation filter banks to transform raw pixel image patches to features. The learned filter banks are expected to (1) encode common visual patterns of a flexible number of categories; (2) encode discriminative information; and (3) hierarchically extract patterns at different visual levels. Particularly, in each single layer of DDSFL, shareable filters are jointly learned for classes which share the similar patterns. Discriminative power of the filters is achieved by enforcing the features from the same category to be close, while features from different categories to be far away from each other. Furthermore, we also propose two exemplar selection methods to iteratively select training data for more efficient and effective learning. Based on the experimental results, DDSFL can achieve very promising performance, and it also shows great complementary effect to the state-of-the-art Caffe features. (C) 2015 Elsevier Ltd. All rights reserved.","keywords_author":["Deep feature learning","Information sharing","Discriminative training","Scene image classification"],"keywords_other":["RECOGNITION","SCALE"],"max_cite":21.0,"pub_year":2015.0,"sources":"['ieee', 'wos']","rawkeys":["recognition","discriminative training","information sharing","deep feature learning","scene image classification","scale"],"tags":["recognition","discriminative training","information sharing","deep feature learning","scene image classification","scale"]},{"p_id":10650,"title":"Bothered by Abstractness or Engaged by Cohesion? Experts' Explanations Enhance Novices' Deep-Learning","abstract":"Experts' explanations have been shown to better enhance novices' transfer as compared with advanced students' explanations. Based on research on expertise and text comprehension, we investigated whether the abstractness or the cohesion of experts' and intermediates' explanations accounted for novices' learning. In Study 1, we showed that the superior cohesion of experts' explanations accounted for most of novices' transfer, whereas the degree of abstractness did not impact novices' transfer performance. In Study 2, we investigated novices' processing while learning with experts' and intermediates' explanations. We found that novices studying experts' explanations actively self-regulated their processing of the explanations, as they showed mainly deep-processing activities, whereas novices learning with intermediates' explanations were mainly engaged in shallow-processing activities by paraphrasing the explanations. Thus, we concluded that subject-matter expertise is a crucial prerequisite for instructors. Despite the abstract character of experts' explanations, their subject-matter expertise enables them to generate highly cohesive explanations that serve as a valuable scaffold for students' construction of flexible knowledge by engaging them in deep-level processing.","keywords_author":["instructional explanations","expertise","cohesion","transfer","processing"],"keywords_other":["SELF-EXPLANATION","INSTRUCTIONAL EXPLANATIONS","PERSPECTIVE","KNOWLEDGE","TEXT COHERENCE","INFERENCES","CATEGORIZATION","COMPREHENSION","MODERATOR","READERS"],"max_cite":8.0,"pub_year":2015.0,"sources":"['wos']","rawkeys":["comprehension","inferences","knowledge","expertise","moderator","instructional explanations","transfer","processing","readers","self-explanation","perspective","cohesion","categorization","text coherence"],"tags":["comprehension","knowledge","expertise","recognition","instructional explanations","moderation","processing","readers","perspective","cohesion","categorization","inference","text coherence","self-explanations"]},{"p_id":10651,"title":"Structural health monitoring by using a sparse coding-based deep learning algorithm with wireless sensor networks","abstract":"Structural health monitoring has received remarkable attention due to the arising structural safety problems. Most of these structural health problems are accumulative damages such as slight changes in structural deformations which are very hard to be detected. In addition, the complexity of real structure and environmental noises make structural health monitoring more difficult. Existing methods largely use various types of sensors to collect useful parameters and then train a machine learning model to diagnose damage level and location, in which a large amount of training data are needed for the model training, while the labeled data are rare in the real world. To overcome this problem, sparse coding is employed in this paper to achieve structural health monitoring of a bridge equipped with a wireless sensor network, so that a large amount of unlabeled examples can be used to train a feature extractor based on the sparse coding algorithm. Features learned from sparse coding are then used to train a neural network classifier to distinguish different statuses of the bridge. Experimental results show the sparse coding-based deep learning algorithm achieves higher accuracy for structural health monitoring under the same level of environmental noises, compared with some existing methods.","keywords_author":["Sparse coding","Structural health monitoring","Wireless sensor network","Structural health monitoring","Sparse coding","Wireless sensor network"],"keywords_other":["BRIDGES","Structural health","PREDICTION","Accumulative damage","Environmental noise","NOVELTY DETECTION","Machine learning models","IDENTIFICATION","NEURAL-NETWORKS","DAMAGE DETECTION","RECOGNITION","METHODOLOGY","Structural safety","Structural deformation","EXPERIMENTAL VALIDATION","Neural network classifier","Sparse coding"],"max_cite":4.0,"pub_year":2014.0,"sources":"['ieee', 'scp', 'wos']","rawkeys":["neural network classifier","identification","machine learning models","structural safety","structural health monitoring","novelty detection","neural-networks","structural health","accumulative damage","wireless sensor network","methodology","bridges","recognition","experimental validation","sparse coding","environmental noise","structural deformation","damage detection","prediction"],"tags":["neural network classifier","identification","machine learning models","structural safety","wireless sensor networks","structural health monitoring","novelty detection","structural health","accumulative damage","experimental validations","methodology","recognition","neural networks","sparse coding","environmental noise","structural deformation","damage detection","prediction","connectivity"]},{"p_id":108956,"title":"Neural features for pedestrian detection","abstract":"This paper presents a pedestrian detection approach that uses neural features from a fully convolutional network (FCN) instead of features manually designed. We train an AdaBoost detector per layer and compare the performance to find the optimal layer for this task. Combining results of multiple detectors can further improve the performance. In order to adapt the FCN to pedestrian detection task, we fine-tune it with bounding boxes labels. Using neural features generated by fine-tuned FCN, the log-average miss rate (MR) on Caltech pedestrian dataset is 18.79% by a single detector and 16.50% by combining two detectors. We also evaluate the proposed method on INRIA pedestrian dataset and the MR is 11.17% with a single detector and 9.91% through combining two detectors. The improved performance indicates that the proposed neural features are applicable to pedestrian detection task, due to their strong representation. (C) 2017 Elsevier B.V. All rights reserved.","keywords_author":["Pedestrian detection","Neural features","Fully convolutional network"],"keywords_other":["RECOGNITION","ROBUST","HOG"],"max_cite":1.0,"pub_year":2017.0,"sources":"['wos']","rawkeys":["pedestrian detection","recognition","fully convolutional network","hog","robust","neural features"],"tags":["pedestrian detection","robustness","recognition","histogram of oriented gradients","fully convolutional network","neural features"]},{"p_id":2459,"title":"An On-Chip Learning Neuromorphic Autoencoder With Current-Mode Transposable Memory Read and Virtual Lookup Table","abstract":"This paper presents an IC implementation of on-chip learning neuromorphic autoencoder unit in a form of rate-based spiking neural network. With a current-mode signaling scheme embedded in a 500 x 500 6b SRAM-based memory, the proposed architecture achieves simultaneous processing of multiplications and accumulations. In addition, a transposable memory read for both forward and backward propagations and a virtual lookup table are also proposed to perform an unsupervised learning of restricted Boltzmann machine. The IC is fabricated using 28-nm CMOS process and is verified in a three-layer network of encoder-decoder pair for training and recovery of images with two-dimensional 16 x 16 pixels. With a dataset of 50 digits, the IC shows a normalized root mean square error of 0.078. Measured energy efficiencies are 4.46 pJ per synaptic operation for inference and 19.26 pJ per synaptic weight update for learning, respectively. The learning performance is also estimated by simulations if the proposed hardware architecture is extended to apply to a batch training of 60 000 MNIST datasets.","keywords_author":["Autoencoder","deep belief network (DBN)","neuromorphic","on-chip learning","restricted Boltzmann machine (RBM)","unsupervised learning"],"keywords_other":["STORAGE","SPIKING NEURONS","NEURAL-NETWORKS","RECOGNITION","SYNAPTIC PLASTICITY"],"max_cite":0.0,"pub_year":2018.0,"sources":"['wos', 'ieee']","rawkeys":["neural-networks","recognition","synaptic plasticity","on-chip learning","deep belief network (dbn)","autoencoder","storage","restricted boltzmann machine (rbm)","spiking neurons","neuromorphic","unsupervised learning"],"tags":["recognition","synaptic plasticity","on-chip learning","neural networks","auto encoders","storage","spiking neurons","neuromorphic","unsupervised learning","deep belief networks","restricted boltzmann machine"]},{"p_id":10660,"title":"An automated system for epilepsy detection using EEG brain signals based on deep learning approach","abstract":"Epilepsy is a life-threatening and challenging neurological disorder, which is affecting a large number of people all over the world. For its detection, encephalography (EEG) is a commonly used clinical approach, but manual inspection of EEG brain signals is a time-consuming and laborious process, which puts a heavy burden on neurologists and affects their performance. Several automatic systems have been proposed using traditional approaches to assist neurologists, which perform well in detecting binary epilepsy scenarios e.g. normal vs. ictal, but their performance degrades in classifying ternary case e.g. ictal vs. normal vs. inter-ictal. To overcome this problem, we propose a system that is an ensemble of pyramidal one-dimensional convolutional neural network (P-1D-CNN) models. Though a CNN model learns the internal structure of data and outperforms hand-engineered techniques, the main issue is the large number of learnable parameters, whose learning requires a huge volume of data. To overcome this issue, P-1D CNN works on the concept of refinement approach and it involves 61% fewer parameters compared to standard CNN models and as such it has better generalization. Further to overcome the limitations of the small amount of data, we propose two augmentation schemes. We tested the system on the University of Bonn dataset, a benchmark dataset; in almost all the cases concerning epilepsy detection, it gives an accuracy of 99.1 0.9% and outperforms the state-of-the-art systems. In addition, while enjoying the strength of a CNN model, P-1D-CNN model requires 61% less memory space and its detection time is very short (< 0.000481 s), as such it is suitable for real-time clinical setting. It will ease the burden of neurologists and will assist the patients in alerting them before the seizure occurs. The proposed P-1D CNN model is not only suitable for epilepsy detection, but it can be adopted in developing robust expert systems for other similar disorders. (C) 2018 Elsevier Ltd. All rights reserved.","keywords_author":["Electroencephalogram (EEG)","Epilepsy","Seizure","Ictal","Interictal","1D-CNN"],"keywords_other":["CONVOLUTIONAL NEURAL-NETWORK","SEIZURE DETECTION","TRANSFORM","ELECTROENCEPHALOGRAPHY","CLASSIFICATION","IDENTIFICATION","MODEL","RECOGNITION","RECORDINGS","FEATURE-EXTRACTION"],"max_cite":0.0,"pub_year":2018.0,"sources":"['wos']","rawkeys":["identification","model","recognition","convolutional neural-network","feature-extraction","epilepsy","ictal","seizure detection","transform","interictal","recordings","classification","seizure","electroencephalogram (eeg)","electroencephalography","1d-cnn"],"tags":["identification","model","recognition","seizure detection","ictal","epilepsy","seizures","eeg","1-d cnn","interictal","recordings","classification","convolutional neural network","feature extraction","transform"]},{"p_id":68009,"title":"CONSTRUCTION OF A CENTURY SOLAR CHROMOSPHERE DATA SET FOR SOLAR ACTIVITY RELATED RESEARCH","abstract":"This article introduces our ongoing project \"Construction of a Century Solar Chromosphere Data Set for Solar Activity Related Research\". Solar activities are the major sources of space weather that affects human lives. Some of the serious space weather consequences, for instance, include interruption of space communication and navigation, compromising the safety of astronauts and satellites, and damaging power grids. Therefore, the solar activity research has both scientific and social impacts. The major database is built up from digitized and standardized film data obtained by several observatories around the world and covers a timespan more than 100 years. After careful calibration, we will develop feature extraction and data mining tools and provide them together with the comprehensive database for the astronomical community. Our final goal is to address several physical issues: filament behavior in solar cycles, abnormal behavior of solar cycle 24, large-scale solar eruptions, and sympathetic remote brightenings. Significant progresses are expected in data mining algorithms and software development, which will benefit the scientific analysis and eventually advance our understanding of solar cycles.","keywords_author":["solar cycle","H alpha","filament","multi-parameter calibration","standardization","feature extraction","solar activity pattern"],"keywords_other":["CYCLE","RULE","RECOGNITION","HELICITY","FILAMENTS"],"max_cite":0.0,"pub_year":2017.0,"sources":"['wos']","rawkeys":["multi-parameter calibration","recognition","solar cycle","solar activity pattern","rule","cycle","feature extraction","h alpha","filaments","helicity","filament","standardization"],"tags":["rules","multi-parameter calibration","recognition","solar cycle","solar activity pattern","standards","cycle","feature extraction","h alpha","filaments","helicity"]},{"p_id":10666,"title":"An Overview of Deep Learning Based Methods for Unsupervised and Semi-Supervised Anomaly Detection in Videos","abstract":"Videos represent the primary source of information for surveillance applications. Video material is often available in large quantities but in most cases it contains little or no annotation for supervised learning. This article reviews the state-of-the-art deep learning based methods for video anomaly detection and categorizes them based on the type of model and criteria of detection. We also perform simple studies to understand the different approaches and provide the criteria of evaluation for spatio-temporal anomaly detection.","keywords_author":["unsupervised methods","anomaly detection","representation learning","autoencoders","LSTMs","generative adversarial networks","Variational Autoencoders","predictive models"],"keywords_other":["SLOW FEATURE ANALYSIS","LOCALIZATION","NEURAL-NETWORKS","RECOGNITION","CROWDED SCENES"],"max_cite":0.0,"pub_year":2018.0,"sources":"['ieee', 'wos']","rawkeys":["neural-networks","crowded scenes","recognition","localization","anomaly detection","slow feature analysis","predictive models","generative adversarial networks","unsupervised methods","variational autoencoders","lstms","representation learning","autoencoders"],"tags":["unsupervised method","variational autoencoder","crowded scenes","recognition","localization","anomaly detection","neural networks","predictive models","generative adversarial networks","slow feature analysis","auto encoders","lstms","representation learning"]},{"p_id":108972,"title":"Fingerprint liveness detection using local texture features","abstract":"The problem of fingerprint liveness detection has received an increasing attention in the last decade, as attested by the organisation of three editions of an international competition, named LivDet, dedicated to this challenge. LivDet editions and other works in the literature showed that the performance of current fingerprint liveness detection algorithms is not good enough to allow empowering a fingerprint verification system with a module aimed to distinguish alive from fake fingerprint images. However, recent developments have shown that texture-based features can provide promising solutions to this problem. In this study, a novel fingerprint liveness descriptor named binarised statistical image features (BSIFs) is adopted. Similarly to local binary pattern and local phase quantisation-based representations, BSIF encodes the local fingerprint texture into a feature vector by using a set of filters that, unlike other methods, are learnt from natural images. Extensive experiments with over 40,000 live and fake fingerprint images show that the authors' proposed method outperforms most of the state-of-the-art algorithms, allowing a step ahead to the real integration of fingerprint liveness detectors into verification systems.","keywords_author":["fingerprint identification","image texture","statistical analysis","fingerprint liveness detection","local texture features","LivDet","fingerprint verification system","fake fingerprint images","binarised statistical image features","local binary pattern","local phase quantisation-based representations"],"keywords_other":["CLASSIFICATION","RECOGNITION","PERSPIRATION","BINARY PATTERNS"],"max_cite":0.0,"pub_year":2017.0,"sources":"['ieee', 'wos']","rawkeys":["fake fingerprint images","binary patterns","local binary pattern","local texture features","recognition","statistical analysis","local phase quantisation-based representations","fingerprint identification","perspiration","binarised statistical image features","classification","image texture","livdet","fingerprint verification system","fingerprint liveness detection"],"tags":["fake fingerprint images","binary patterns","recognition","statistical analysis","local phase quantisation-based representations","fingerprint identification","perspiration","binarised statistical image features","classification","image texture","livdet","local binary patterns","local texture feature","fingerprint verification system","fingerprint liveness detection"]},{"p_id":10670,"title":"3D computer vision based on machine learning with deep neural networks: A review","abstract":"Recent advances in the field of computer vision can be attributed to the emergence of deep learning techniques, in particular convolutional neural networks. Neural networks, partially inspired by the brain's visual cortex, enable a computer to \"learn\" the most important features of the images it is shown in relation to a specific, specified task. Given sufficient data and time, (deep) convolutional neural networks offer more easily designed, more generalizable, and significantly more accurate end-to-end systems than is possible with previously employed computer vision techniques. This review paper seeks to provide an overview of deep learning in the field of computer vision with an emphasis on recent progress in tasks involving 3D visual data. Through a backdrop of the mammalian visual processing system, we hope to also provide inspiration for future advances in automated visual processing.","keywords_author":["artificial intelligence","computer vision","deep neural networks","machine learning","computer vision","artificial intelligence","machine learning","deep neural networks"],"keywords_other":["3D computer vision","End-to-end systems","VISUAL-SYSTEM","Computer vision techniques","Visual processing systems","FEEDFORWARD","Learning techniques","RECOGNITION","Convolutional neural network","CORTEX","Important features","Visual-processing"],"max_cite":0.0,"pub_year":2017.0,"sources":"['ieee', 'scp', 'wos']","rawkeys":["visual-processing","artificial intelligence","recognition","visual-system","feedforward","deep neural networks","machine learning","end-to-end systems","visual processing systems","computer vision techniques","3d computer vision","convolutional neural network","important features","learning techniques","computer vision","cortex"],"tags":["visual-processing","recognition","machine learning","end-to-end systems","visual processing systems","computer vision techniques","3d computer vision","convolutional neural network","important features","learning techniques","computer vision","visual systems","cortex","feed-forward"]},{"p_id":84403,"title":"Mining textural knowledge in biological images: Applications, methods and trends","abstract":"Texture analysis is a major task in many areas of computer vision and pattern recognition, including biological imaging. Indeed, visual textures can be exploited to distinguish specific tissues or cells in a biological sample, to highlight chemical reactions between molecules, as well as to detect subcellular patterns that can be evidence of certain pathologies. This makes automated texture analysis fundamental in many applications of biomedicine, such as the accurate detection and grading of multiple types of cancer, the differential diagnosis of autoimmune diseases, or the study of physiological processes. Due to their specific characteristics and challenges, the design of texture analysis systems for biological images has attracted ever-growing attention in the last few years. In this paper, we perform a critical review of this important topic. First, we provide a general definition of texture analysis and discuss its role in the context of bioimaging, with examples of applications from the recent literature. Then, we review the main approaches to automated texture analysis, with special attention to the methods of feature extraction and encoding that can be successfully applied to microscopy images of cells or tissues. Our aim is to provide an overview of the state of the art, as well as a glimpse into the latest and future trends of research in this area. (C) 2016 The Authors. Published by Elsevier B.V.","keywords_author":["Textural analysis","Bioimaging","Textural features extraction","Texture classification","Feature encoding","Deep learning"],"keywords_other":["DECISION-SUPPORT-SYSTEM","BAG","FEATURES","BREAST-CANCER","CLASSIFICATION","LOCAL BINARY PATTERNS","RECOGNITION","RETRIEVAL","HEP-2 CELLS","HISTOPATHOLOGY"],"max_cite":3.0,"pub_year":2016.0,"sources":"['ieee', 'wos']","rawkeys":["bioimaging","decision-support-system","recognition","features","deep learning","textural features extraction","histopathology","bag","hep-2 cells","classification","local binary patterns","retrieval","textural analysis","feature encoding","breast-cancer","texture classification"],"tags":["texture classification","texture feature extraction","bioimaging","recognition","features","machine learning","histopathology","hep-2 cells","classification","local binary patterns","retrieval","bagging","feature encoding","decision support systems","texture analysis","breast cancer"]},{"p_id":43443,"title":"Building Emotional Machines: Recognizing Image Emotions through Deep Neural Networks","abstract":"IEEE An image is a very effective tool for conveying emotions. Many researchers have investigated in computing the image emotions by using various features extracted from images. In this paper, we focus on two high level features, the object and the background, and assume that the semantic information of images is a good cue for predicting emotion. An object is one of the most important elements that define an image, and we find out through experiments that there is a high correlation between the object and the emotion in images. Even with the same object, there may be slight difference in emotion due to different backgrounds, and we use the semantic information of the background to improve the prediction performance. By combining the different levels of features, we build an emotion based feed forward deep neural network which produces the emotion values of a given image. The output emotion values in our framework are continuous values in the 2-dimensional space (Valence and Arousal), which are more effective than using a few number of emotion categories in describing emotions. Experiments confirm the effectiveness of our network in predicting the emotion of images.","keywords_author":["Databases","deep network","Emotion prediction","Emotion recognition","Feature extraction","image emotion","Machine learning","Predictive models","Psychology","Task analysis"],"keywords_other":["Predictive models","Emotion predictions","Emotion recognition","Deep networks","Psychology","image emotion","Task analysis"],"max_cite":0.0,"pub_year":2018.0,"sources":"['scp']","rawkeys":["task analysis","databases","emotion prediction","predictive models","machine learning","deep network","emotion recognition","emotion predictions","feature extraction","image emotion","psychology","deep networks"],"tags":["task analysis","recognition","databases","predictive models","machine learning","emotion recognition","emotion predictions","feature extraction","image emotion","deep networks"]},{"p_id":10679,"title":"Automated Pixel-Level Pavement Crack Detection on 3D Asphalt Surfaces Using a Deep-Learning Network","abstract":"The CrackNet, an efficient architecture based on the Convolutional Neural Network (CNN), is proposed in this article for automated pavement crack detection on 3D asphalt surfaces with explicit objective of pixel-perfect accuracy. Unlike the commonly used CNN, CrackNet does not have any pooling layers which downsize the outputs of previous layers. CrackNet fundamentally ensures pixel-perfect accuracy using the newly developed technique of invariant image width and height through all layers. CrackNet consists of five layers and includes more than one million parameters that are trained in the learning process. The input data of the CrackNet are feature maps generated by the feature extractor using the proposed line filters with various orientations, widths, and lengths. The output of CrackNet is the set of predicted class scores for all pixels. The hidden layers of CrackNet are convolutional layers and fully connected layers. CrackNet is trained with 1,800 3D pavement images and is then demonstrated to be successful in detecting cracks under various conditions using another set of 200 3D pavement images. The experiment using the 200 testing 3D images showed that CrackNet can achieve high Precision (90.13%), Recall (87.63%) and F-measure (88.86%) simultaneously. Compared with recently developed crack detection methods based on traditional machine learning and imaging algorithms, the CrackNet significantly outperforms the traditional approaches in terms of F-measure. Using parallel computing techniques, CrackNet is programmed to be efficiently used in conjunction with the data collection software.","keywords_author":null,"keywords_other":["Data collection software","Traditional approaches","Efficient architecture","Imaging algorithm","CONCURRENT ANALYSIS","TRANSFORM","DISTRESS","DOMAIN","CLASSIFICATION","DECOMPOSITION","MODEL","Pavement crack detection","RECOGNITION","Convolutional neural network","Parallel computing techniques","Detection methods","NEURAL-NETWORK","OPTIMIZATION"],"max_cite":9.0,"pub_year":2017.0,"sources":"['scp', 'wos']","rawkeys":["traditional approaches","recognition","model","transform","detection methods","pavement crack detection","distress","parallel computing techniques","classification","convolutional neural network","domain","data collection software","imaging algorithm","concurrent analysis","efficient architecture","neural-network","decomposition","optimization"],"tags":["traditional approaches","recognition","model","neural networks","transform","detection methods","pavement crack detection","distress","parallel computing techniques","classification","convolutional neural network","domain","data collection software","imaging algorithm","concurrent analysis","efficient architecture","optimization","decomposition"]},{"p_id":68035,"title":"Semi-supervised vehicle classification via fusing affinity matrices","abstract":"Vehicle classification plays a fundamental role in various intelligent transportation systems. With the rapid development of traffic surveillance, the amount of visual vehicle data has been increasing tremendously, and can be easily collected. However, it is labor-intensive to manually annotate the semantic labels for these data, posing the challenge of label insufficiency to the vehicle classification tasks. In this context, we use a semi-supervised learning model to classify vehicle types, which only needs a small number of pre-labeled data and propagates these labels to the rest data at hand. In our model, we combine multiple features via fusing their affinity matrices to enhance the classification accuracy. We conduct several experiments to validate our method on a public vehicle dataset. Experimental results support the effectiveness of our method. (C) 2018 Elsevier B.V. All rights reserved.","keywords_author":["Vehicle type classification","Semi-supervised learning","Graph fusion"],"keywords_other":["CONVOLUTIONAL NEURAL-NETWORK","RECOGNITION"],"max_cite":0.0,"pub_year":2018.0,"sources":"['ieee', 'wos']","rawkeys":["recognition","vehicle type classification","semi-supervised learning","convolutional neural-network","graph fusion"],"tags":["recognition","vehicle type classification","semi-supervised learning","convolutional neural network","graph fusion"]},{"p_id":10697,"title":"Learning a deep network with spherical part model for 3D hand pose estimation","abstract":"Hand pose estimation is a hot topic in recent years. It has been widely used in virtual reality since it provides an interface for communication between human and cyberspace. Hand pose estimation is difficult due to some challenges. First, we need to detect human hand which is very changeable. Second, the high degree of freedom leads to difficulties in pose estimation. In this paper, we aim to build a hand pose estimation system which can correctly detect human hand and estimate its pose. We design a model called spherical part model (SPM) and train a deep convolutional neural network using this model. As a result, our network can more accurately estimate hand pose based on prior knowledge of human hand. To demonstrate it, a complete experiment is conducted on two public and one self-build datasets. The results show that our system can outperform other state of the art works. (C) 2018 Elsevier Ltd. All rights reserved.","keywords_author":["Convolutional neural network","Deep learning","Hand detection","Hand pose estimation","Spherical part model","Hand detection","Hand pose estimation","Deep learning","Convolutional neural network","Spherical part model"],"keywords_other":["Hand pose estimations","GAUSSIAN MIXTURE MODEL","High Degree of Freedom","State of the art","Complete experiment","RECOGNITION","3D hand pose estimations","Convolutional neural network","DEPTH","Deep convolutional neural networks","Hand detection","IMAGES"],"max_cite":0.0,"pub_year":2018.0,"sources":"['scp', 'wos']","rawkeys":["3d hand pose estimations","recognition","images","deep learning","deep convolutional neural networks","hand detection","state of the art","depth","hand pose estimations","convolutional neural network","high degree of freedom","spherical part model","complete experiment","hand pose estimation","gaussian mixture model"],"tags":["3d hand pose estimations","recognition","images","state of the art","machine learning","hand detection","depth","hand pose estimations","convolutional neural network","high degree of freedom","spherical part model","complete experiment","gaussian mixture model"]},{"p_id":117202,"title":"Differential Path-Length Factor's Effect on the Characterization of Brain's Hemodynamic Response Function: A Functional Near-Infrared Study","abstract":"Functional near-infrared spectroscopy (fNIRS) has evolved as a neuro-imaging modality over the course of the past two decades. The removal of superfluous information accompanying the optical signal, however, remains a challenge. A comprehensive analysis of each step is necessary to ensure the extraction of actual information from measured fNIRS waveforms. A slight change in shape could alter the features required for fNIRS-BCI applications. In the present study, the effect of the differential path-length factor (DPF) values on the characteristics of the hemodynamic response function (HRF) was investigated. Results were compiled for both simulated data sets and healthy human subjects over a range of DPF values from three to eight. Different sets of activation durations and stimuli were used to generate the simulated signals for further analysis. These signals were split into optical densities under a constrained environment utilizing known values of DPF. Later, different values of DPF were used to analyze the variations of actual HRF. The results, as summarized into four categories, suggest that the DPF can change the main and post-stimuli responses in addition to other interferences. Six healthy subjects participated in this study. Their observed optical brain time-series were fed into an iterative optimization problem in order to estimate the best possible fit of HRF and physiological noises present in the measured signals with free parameters. A series of solutions was derived for different values of DPF in order to analyze the variations of HRF. It was observed that DPF change is responsible for HRF creep from actual values as well as changes in HRF characteristics.","keywords_author":["functional near-infrared spectroscopy","differential path-length factor","hemodynamic response","optimal cortical model","optical brain imaging"],"keywords_other":["TISSUE","OPTICAL PATHLENGTH","FOREARM","SPECTROSCOPY","MOTOR","DEPENDENCE","SIGNALS"],"max_cite":0.0,"pub_year":2018.0,"sources":"['wos']","rawkeys":["signals","tissue","optical pathlength","optical brain imaging","forearm","spectroscopy","optimal cortical model","functional near-infrared spectroscopy","dependence","motor","differential path-length factor","hemodynamic response"],"tags":["signals","tissue","recognition","optical brain imaging","forearm","spectroscopy","optimal cortical model","functional near-infrared spectroscopy","optical pathlength","motor","differential path-length factor","hemodynamic response"]},{"p_id":92627,"title":"Framework for Virtual Cognitive Experiment in Virtual Geographic Environments","abstract":"Virtual Geographic Environment Cognition is the attempt to understand the human cognition of surface features, geographic processes, and human behaviour, as well as their relationships in the real world. From the perspective of human cognition behaviour analysis and simulation, previous work in Virtual Geographic Environments (VGEs) has focused mostly on representing and simulating the real world to create an 'interpretive' virtual world and improve an individual's active cognition. In terms of reactive cognition, building a user 'evaluative' environment in a complex virtual experiment is a necessary yet challenging task. This paper discusses the outlook of VGEs and proposes a framework for virtual cognitive experiments. The framework not only employs immersive virtual environment technology to create a realistic virtual world but also involves a responsive mechanism to record the user's cognitive activities during the experiment. Based on the framework, this paper presents two potential implementation methods: first, training a deep learning model with several hundred thousand street view images scored by online volunteers, with further analysis of which visual factors produce a sense of safety for the individual, and second, creating an immersive virtual environment and Electroencephalogram (EEG)-based experimental paradigm to both record and analyse the brain activity of a user and explore what type of virtual environment is more suitable and comfortable. Finally, we present some preliminary findings based on the first method.","keywords_author":["virtual geographic environments","spatial cognition","brain-computer interface","street-level imagery","deep learning"],"keywords_other":["TOOL","SELF","VGES","NETWORKS","MODEL","TIME","EMBODIED COGNITION","FMRI","PSYCHOLOGY","BRAIN"],"max_cite":0.0,"pub_year":2018.0,"sources":"['ieee', 'wos']","rawkeys":["model","vges","deep learning","virtual geographic environments","street-level imagery","brain-computer interface","self","tool","time","brain","networks","fmri","spatial cognition","psychology","embodied cognition"],"tags":["brain-computer interfaces","recognition","model","vges","virtual geographic environments","street-level imagery","self","machine learning","tool","time","brain","networks","fmri","spatial cognition","embodied cognition"]},{"p_id":10715,"title":"Spectral-spatial feature learning for hyperspectral imagery classification using deep stacked sparse autoencoder","abstract":"Classification of hyperspectral remote sensing imagery is one of the most popular topics because of its intrinsic potential to gather spectral signatures of materials and provides distinct abilities to object detection and recognition. In the last decade, an enormous number of methods were suggested to classify hyperspectral remote sensing data using spectral features, though some are not using all information and lead to poor classification accuracy; on the other hand, the exploration of deep features is recently considered a lot and has turned into a research hot spot in the geoscience and remote sensing research community to enhance classification accuracy. A deep learning architecture is proposed to classify hyperspectral remote sensing imagery by joint utilization of spectral-spatial information. A stacked sparse autoencoder provides unsupervised feature learning to extract high-level feature representations of joint spectralspatial information; then, a soft classifier is employed to train high-level features and to fine-tune the deep learning architecture. Comparative experiments are performed on two widely used hyperspectral remote sensing data (Salinas and PaviaU) and a coarse resolution hyperspectral data in the long-wave infrared range. The obtained results indicate the superiority of the proposed spectral-spatial deep learning architecture against the conventional classification methods. (C) 2017 Society of Photo-Optical Instrumentation Engineers (SPIE)","keywords_author":["deep features","deep learning","hyperspectral imagery classification","softmax regression","spectral-spatial unsupervised feature learning","stacked sparse autoencoder"],"keywords_other":["REPRESENTATION","RECOGNITION","REMOTE-SENSING DATA","OPTIMIZATION","CONVOLUTIONAL NEURAL-NETWORKS"],"max_cite":1.0,"pub_year":2017.0,"sources":"['wos']","rawkeys":["recognition","deep learning","hyperspectral imagery classification","remote-sensing data","spectral-spatial unsupervised feature learning","representation","stacked sparse autoencoder","convolutional neural-networks","optimization","deep features","softmax regression"],"tags":["remote sensing data","recognition","stacked sparse autoencoder","hyperspectral imagery classification","spectral-spatial unsupervised feature learning","sparse representation","machine learning","representation","convolutional neural network","optimization","deep features"]},{"p_id":10717,"title":"Deep learning advances in computer vision with 3D data: A survey","abstract":"Deep learning has recently gained popularity achieving state-of-the-art performance in tasks involving text, sound, or image processing. Due to its outstanding performance, there have been efforts to apply it in more challenging scenarios, for example, 3D data processing. This article surveys methods applying deep learning on 3D data and provides a classification based on how they exploit them. From the results of the examined works, we conclude that systems employing 2D views of 3D data typically surpass voxel-based (3D) deep models, which however, can perform better with more layers and severe data augmentation. Therefore, larger-scale datasets and increased resolutions are required.","keywords_author":["3D data","3D object recognition","3D object retrieval","3D segmentation","Convolutional neural networks","Deep learning","3D data","3D object recognition","3D object retrieval","3D segmentation","convolutional neural networks","deep learning"],"keywords_other":["OBJECT RETRIEVAL","SURFACE-FEATURE","HYPERSPECTRAL DATA","CLASSIFICATION","3D data","3D segmentation","3d object recognition","NEURAL-NETWORKS","SHAPE RETRIEVAL","RECOGNITION","REPRESENTATIONS","Convolutional neural network","SEGMENTATION","DESCRIPTORS","3D object retrieval"],"max_cite":6.0,"pub_year":2017.0,"sources":"['ieee', 'scp', 'wos']","rawkeys":["neural-networks","object retrieval","convolutional neural networks","hyperspectral data","recognition","deep learning","segmentation","shape retrieval","3d data","3d object recognition","3d segmentation","representations","3d object retrieval","classification","convolutional neural network","surface-feature","descriptors"],"tags":["object retrieval","hyperspectral data","recognition","segmentation","neural networks","shape retrieval","3d data","machine learning","representation","3d object recognition","3d segmentation","3d object retrieval","classification","convolutional neural network","descriptors","surface feature"]},{"p_id":92643,"title":"Classification of the trained and untrained emitter types based on class probability output networks","abstract":"Modern airplanes and ships are equipped with radars emitting specific patterns of electromagnetic signals. The radar antennas are detecting these patterns which are required to identify the types of emitters. A conventional way of emitter identification is to categorize the radar patterns according to the sequences of radar frequencies, differences in time of arrivals, and pulse widths of emitting signals by human experts. In this respect, this paper proposes a method of classifying the radar patterns automatically using the network of calculating the p-values for testing the hypotheses of the types of emitters referred to as the class probability output network (CPON). The proposed method also provides a new way of identifying the trained and untrained emitter types. Through the simulation for radar pattern classification, the effectiveness of the proposed approach has been demonstrated. (C) 2017 Elsevier B.V. All rights reserved.","keywords_author":["Classification","Radar patterns","Trained and untrained emitter types","Conditional class probability"],"keywords_other":["NEURAL-NETWORK","RECOGNITION","IDENTIFICATION"],"max_cite":0.0,"pub_year":2017.0,"sources":"['wos']","rawkeys":["trained and untrained emitter types","identification","recognition","radar patterns","classification","neural-network","conditional class probability"],"tags":["trained and untrained emitter types","identification","recognition","conditional class probabilities","neural networks","radar patterns","classification"]},{"p_id":92645,"title":"Extreme Learning Machines Based on Least Absolute Deviation and Their Applications in Analysis Hard Rate of Licorice Seeds","abstract":"Extreme learning machine (ELM) has demonstrated great potential in machine learning and data mining fields owing to its simplicity, rapidity and good generalization performance. In this work, a general framework for ELM regression is first investigated based on least absolute deviation (LAD) estimation (called LADELM), and then we develop two regularized LADELM formulations with the l2-norm and l1-norm regularization, respectively. Moreover, the proposed models are posed as simple linear programming or quadratic programming problems. Furthermore, the proposed models are used directly to analyze the hard rate of licorice seeds using near-infrared spectroscopy data. Experimental results on eight different spectral regions show the feasibility and effectiveness of the proposed models.","keywords_author":["Extreme learning machine","robust regression","least absolute deviation estimation","near-infrared spectroscopy"],"keywords_other":["RECOGNITION","REGRESSION"],"max_cite":0.0,"pub_year":2015.0,"sources":"['wos']","rawkeys":["robust regression","recognition","least absolute deviation estimation","near-infrared spectroscopy","extreme learning machine","regression"],"tags":["recognition","least absolute deviation estimation","near-infrared spectroscopy","extreme learning machine","regression","robust regressions"]},{"p_id":92647,"title":"A new distance measure for non-identical data with application to image classification","abstract":"Distance measures are part and parcel of many computer vision algorithms. The underlying assumption in all existing distance measures is that feature elements are independent and identically distributed. However, in real-world settings, data generally originate from heterogeneous sources even if they do possess a common data generating mechanism. Since these sources are not identically distributed by necessity, the assumption of identical distribution is inappropriate. Here, we use statistical analysis to show that feature elements of local image descriptors are indeed non-identically distributed. To test the effect of omitting the unified distribution assumption, we created a new distance measure called the Poisson-Binomial radius (PBR). PBR is a bin-to-bin distance which accounts for the dispersion of bin-to-bin information. PBR's performance was evaluated on twelve benchmark data sets covering six different classification and recognition applications: texture, material, leaf, scene, ear biometrics and category-level image classification. Results from these experiments demonstrate that PBR outperforms state-of-the-art distance measures for most of the data sets and achieves comparable performance on the rest, suggesting that accounting for different distributions in distance measures can improve performance in classification and recognition tasks.","keywords_author":["Poisson-Binomial distribution","Semi-metric distance","Non-identical data","Distance measure","Image classification","Image recognition"],"keywords_other":["SVM","CLASSIFIERS","SUPPORT VECTOR MACHINES","RETRIEVAL","RECOGNITION","KERNEL","RECOGNIZING INDOOR SCENES","SIMILARITY"],"max_cite":2.0,"pub_year":2017.0,"sources":"['wos']","rawkeys":["poisson-binomial distribution","recognizing indoor scenes","recognition","distance measure","semi-metric distance","similarity","classifiers","non-identical data","svm","kernel","image recognition","retrieval","support vector machines","image classification"],"tags":["poisson-binomial distribution","recognizing indoor scenes","recognition","distance measure","semi-metric distance","similarity","machine learning","non-identical data","kernel","classifier","image recognition","retrieval","image classification"]},{"p_id":92648,"title":"Big data in food safety: An overview","abstract":"Technology is now being developed that is able to handle vast amounts of structured and unstructured data from diverse sources and origins. These technologies are often referred to as big data, and open new areas of research and applications that will have an increasing impact in all sectors of our society. In this paper we assessed to which extent big data is being applied in the food safety domain and identified several promising trends. In several parts of the world, governments stimulate the publication on internet of all data generated in public funded research projects. This policy opens new opportunities for stakeholders dealing with food safety to address issues which were not possible before. Application of mobile phones as detection devices for food safety and the use of social media as early warning of food safety problems are a few examples of the new developments that are possible due to big data.","keywords_author":["Big data","database","food safety","new technologies"],"keywords_other":["RECOMMENDATION","OPPORTUNITIES","INFORMATION","SYSTEM","MODEL","RECOGNITION","TOXICOLOGY","PROTEIN INTERACTION NETWORK","OUTBREAK","GENES"],"max_cite":2.0,"pub_year":2017.0,"sources":"['wos']","rawkeys":["recommendation","food safety","recognition","model","opportunities","big data","new technologies","outbreak","genes","system","database","toxicology","information","protein interaction network"],"tags":["recommendation","food safety","recognition","model","opportunities","databases","big data","protein interaction networks","new technologies","outbreak","genes","system","toxicology","information"]},{"p_id":109040,"title":"Movie genre classification: A multi-label approach based on convolutions through time","abstract":"The task of labeling movies according to their corresponding genre is a challenging classification problem, having in mind that genre is an immaterial feature that cannot be directly pinpointed in any of the movie frames. Hence, off-the-shelf image classification approaches are not capable of handling this task in a straightforward fashion. Moreover, movies may belong to multiple genres at the same time, making movie genre assignment a typical multi-label classification problem, which is per se much more challenging than standard single-label classification. In this paper, we propose a novel deep neural architecture based on convolutional neural networks(ConvNets) for performing multi-label movie-trailer genre classification. It encapsulates an ultra-deep ConvNet with residual connections, and it makes use of a special convolutional layer to extract temporal information from image-based features prior to performing the mapping of movie trailers to genres. We compare the proposed approach with the current state-of-theart methods for movie classification that employ well-known image descriptors and other low-level handcrafted features. Results show that our method substantially outperforms the state-of-the-art for this task, improving classification performance for all movie genres. (C) 2017 Elsevier B.V. All rights reserved.","keywords_author":["Movie genre classification","Convolutional neural networks","Convolutions through time","Multi-label classification"],"keywords_other":["NETWORKS","RECOGNITION","FEATURES"],"max_cite":1.0,"pub_year":2017.0,"sources":"['ieee', 'wos']","rawkeys":["multi-label classification","recognition","convolutional neural networks","movie genre classification","features","convolutions through time","networks"],"tags":["recognition","movie genre classification","features","convolutions through time","networks","convolutional neural network","multi label classification"]},{"p_id":2548,"title":"Generalizing Pooling Functions in CNNs: Mixed, Gated, and Tree","abstract":"\u00a9 1979-2012 IEEE. In this paper, we seek to improve deep neural networks by generalizing the pooling operations that play a central role in the current architectures. We pursue a careful exploration of approaches to allow pooling to learn and to adapt to complex and variable patterns. The two primary directions lie in: (1) learning a pooling function via (two strategies of) combining of max and average pooling, and (2) learning a pooling function in the form of a tree-structured fusion of pooling filters that are themselves learned. In our experiments every generalized pooling operation we explore improves performance when used in place of average or max pooling. We experimentally demonstrate that the proposed pooling operations provide a boost in invariance properties relative to conventional pooling and set the state of the art on several widely adopted benchmark datasets. These benefits come with only a light increase in computational overhead during training (ranging from additional 5 to 15 percent in time complexity) and a very modest increase in the number of model parameters (e.g., additional 1, 9, and 27 parameters for mixed, gated, and 2-level tree pooling operators, respectively). To gain more insights about our proposed pooling methods, we also visualize the learned pooling masks and the embeddings of the internal feature responses for different pooling operations. Our proposed pooling operations are easy to implement and can be applied within various deep neural network architectures.","keywords_author":["Convolutional neural networks","deep learning","pooling functions","supervised classification","Convolutional neural networks","deep learning","pooling functions","supervised classification"],"keywords_other":["Supervised classification","Internal features","Model parameters","Computational overheads","Benchmark datasets","Tree-structured","State of the art","RECOGNITION","Convolutional neural network","CORTEX"],"max_cite":1.0,"pub_year":2018.0,"sources":"['scp', 'wos', 'ieee']","rawkeys":["supervised classification","benchmark datasets","internal features","convolutional neural networks","pooling functions","recognition","deep learning","state of the art","tree-structured","computational overheads","convolutional neural network","cortex","model parameters"],"tags":["supervised classification","benchmark datasets","internal features","pooling function","recognition","state of the art","machine learning","computational overheads","convolutional neural network","tree structures","cortex","model parameters"]},{"p_id":109045,"title":"Low-power motion gesture sensor with a partially open cavity package","abstract":"In an IR proximity-based motion gesture sensor (MGS) mainly composed of an IR LED and photodiodes (PDs), the IR LED is the most power-hungry component. For reducing its power, both the size and the field-of-view (FOV) of each PD can be increased instead. However, it cannot be adapted to conventional MGSs due to the geometrical limitation of their optical structures. In this paper, the optical structure of the proposed MGS with a partially open cavity package allows for power reduction by at least 70.9% compared with conventional MGSs. Optical simulation and test results validate the theoretical analysis presented. (C) 2016 Optical Society of America","keywords_author":null,"keywords_other":["FIELD-OF-VIEW","RECOGNITION","SYSTEM"],"max_cite":3.0,"pub_year":2016.0,"sources":"['wos']","rawkeys":["field-of-view","recognition","system"],"tags":["field of views","recognition","system"]},{"p_id":84468,"title":"DNN Transfer Learning Based Non-Linear Feature Extraction for Acoustic Event Classification","abstract":"Recent acoustic event classification research has focused on training suitable filters to represent acoustic events. However, due to limited availability of target event databases and linearity of conventional filters, there is still room for improving performance. By exploiting the non-linear modeling of deep neural networks (DNNs) and their ability to learn beyond pre-trained environments, this letter proposes a DNN-based feature extraction scheme for the classification of acoustic events. The effectiveness and robustness to noise of the proposed method are demonstrated using a database of indoor surveillance environments.","keywords_author":["acoustic event classification","transfer learning","deep neural network","acoustic feature"],"keywords_other":["DEEP NEURAL-NETWORKS","RECOGNITION"],"max_cite":0.0,"pub_year":2017.0,"sources":"['ieee', 'wos']","rawkeys":["recognition","deep neural network","transfer learning","deep neural-networks","acoustic event classification","acoustic feature"],"tags":["recognition","transfer learning","acoustic features","convolutional neural network","acoustic event classification"]},{"p_id":84471,"title":"Detection and Classification of Acoustic Scenes and Events: Outcome of the DCASE 2016 Challenge","abstract":"Public evaluation campaigns and datasets promote active development in target research areas, allowing direct comparison of algorithms. The second edition of the challenge on detection and classification of acoustic scenes and events (DCASE 2016) has offered such an opportunity for development of the state-of-the-art methods, and succeeded in drawing together a large number of participants from academic and industrial backgrounds. In this paper, we report on the tasks and outcomes of the DCASE 2016 challenge. The challenge comprised four tasks: acoustic scene classification, sound event detection in synthetic audio, sound event detection in real-life audio, and domestic audio tagging. We present each task in detail and analyze the submitted systems in terms of design and performance. We observe the emergence of deep learning as the most popular classification method, replacing the traditional approaches based on Gaussian mixture models and support vector machines. By contrast, feature representations have not changed substantially throughout the years, as mel frequency-based representations predominate in all tasks. The datasets created for and used in DCASE 2016 are publicly available and are a valuable resource for further research.","keywords_author":["Acoustic scene classification","audio datasets","pattern recognition","sound event detection"],"keywords_other":["MUSIC","RECOGNITION","BAG","SEPARATION"],"max_cite":0.0,"pub_year":2018.0,"sources":"['ieee', 'wos']","rawkeys":["audio datasets","sound event detection","recognition","bag","separation","pattern recognition","music","acoustic scene classification"],"tags":["separation","audio datasets","sound event detection","recognition","bagging","pattern recognition","music","acoustic scene classification"]},{"p_id":2552,"title":"Multiscale Rotation-Invariant Convolutional Neural Networks for Lung Texture Classification","abstract":"We propose a newmultiscale rotation-invariant convolutional neural network (MRCNN) model for classifying various lung tissue types on high-resolution computed tomography. MRCNN employs Gabor-local binary pattern that introduces a good property in image analysisinvariance to image scales and rotations. In addition, we offer an approach to deal with the problems caused by imbalanced number of samples between different classes in most of the existing works, accomplished by changing the overlapping size between the adjacent patches. Experimental results on a public interstitial lung disease database show a superior performance of the proposed method to state of the art.","keywords_author":["Convolutional neural network (CNN)","gabor filter","interstitial lung disease (ILD) classification","local binary pattern (LBP)","lung classification","Convolutional neural network (CNN)","gabor filter","interstitial lung disease (ILD) classification","local binary pattern (LBP)","lung classification"],"keywords_other":["Interstitial lung disease","EMPHYSEMA","Number of samples","REPRESENTATION","ALGORITHM","State of the art","Rotation invariant","LOCAL BINARY PATTERNS","RECOGNITION","SEGMENTATION","High-resolution computed tomography","Convolutional neural network","QUANTITATIVE-ANALYSIS","Texture classification","RESOLUTION","DISEASE PATTERNS","Local binary patterns","IMAGES"],"max_cite":2.0,"pub_year":2018.0,"sources":"['ieee', 'scp', 'wos']","rawkeys":["emphysema","quantitative-analysis","rotation invariant","interstitial lung disease","state of the art","convolutional neural network","disease patterns","segmentation","high-resolution computed tomography","texture classification","algorithm","lung classification","images","recognition","local binary pattern (lbp)","convolutional neural network (cnn)","number of samples","resolution","gabor filter","representation","interstitial lung disease (ild) classification","local binary patterns"],"tags":["emphysema","interstitial lung disease","state of the art","convolutional neural network","disease patterns","segmentation","high-resolution computed tomography","rotation invariance","algorithms","texture classification","lung classification","images","recognition","interstitial lung disease classification","number of samples","resolution","quantitative analysis","gabor filter","representation","local binary patterns"]},{"p_id":76280,"title":"Multiple hierarchical deep hashing for large scale image retrieval","abstract":"Learning-based hashing methods are becoming the mainstream for large scale visual search. They consist of two main components: hash codes learning for training data and hash functions learning for encoding new data points. The performance of a content-based image retrieval system crucially depends on the feature representation, and currently Convolutional Neural Networks (CNNs) has been proved effective for extracting high-level visual features for large scale image retrieval. In this paper, we propose a Multiple Hierarchical Deep Hashing (MHDH) approach for large scale image retrieval. Moreover, MHDH seeks to integrate multiple hierarchical non-linear transformations with hidden neural network layer for hashing code generation. The learned binary codes represent potential concepts that connect to class labels. In addition, extensive experiments on two popular datasets demonstrate the superiority of our MHDH over both supervised and unsupervised hashing methods.","keywords_author":["Multimedia","Deep hashing","Large scale image retrieval","Convolutional neural networks"],"keywords_other":["SCENE","QUANTIZATION","CODES","NEURAL-NETWORKS","RECOGNITION"],"max_cite":0.0,"pub_year":2017.0,"sources":"['ieee', 'wos']","rawkeys":["neural-networks","deep hashing","recognition","convolutional neural networks","codes","scene","large scale image retrieval","multimedia","quantization"],"tags":["signals","deep hashing","recognition","neural networks","codes","scene","convolutional neural network","multimedia","large-scale image retrieval"]},{"p_id":84474,"title":"Metrics for Polyphonic Sound Event Detection","abstract":"This paper presents and discusses various metrics proposed for evaluation of polyphonic sound event detection systems used in realistic situations where there are typically multiple sound sources active simultaneously. The system output in this case contains overlapping events, marked as multiple sounds detected as being active at the same time. The polyphonic system output requires a suitable procedure for evaluation against a reference. Metrics from neighboring fields such as speech recognition and speaker diarization can be used, but they need to be partially redefined to deal with the overlapping events. We present a review of the most common metrics in the field and the way they are adapted and interpreted in the polyphonic case. We discuss segment-based and event-based definitions of each metric and explain the consequences of instance-based and class-based averaging using a case study. In parallel, we provide a toolbox containing implementations of presented metrics.","keywords_author":["pattern recognition","audio signal processing","audio content analysis","computational auditory scene analysis","sound events","everyday sounds","polyphonic sound event detection","evaluation of sound event detection"],"keywords_other":["AUDIO","CONTEXT","RECOGNITION","CLASSIFICATION"],"max_cite":20.0,"pub_year":2016.0,"sources":"['wos']","rawkeys":["audio signal processing","everyday sounds","recognition","audio content analysis","computational auditory scene analysis","evaluation of sound event detection","audio","classification","context","pattern recognition","sound events","polyphonic sound event detection"],"tags":["audio signal processing","everyday sounds","recognition","audio content analysis","computational auditory scene analysis","evaluation of sound event detection","audio","classification","context","pattern recognition","sound events","polyphonic sound event detection"]},{"p_id":2556,"title":"A Deep Convolutional Coupling Network for Change Detection Based on Heterogeneous Optical and Radar Images","abstract":"We propose an unsupervised deep convolutional coupling network for change detection based on two heterogeneous images acquired by optical sensors and radars on different dates. Most existing change detection methods are based on homogeneous images. Due to the complementary properties of optical and radar sensors, there is an increasing interest in change detection based on heterogeneous images. The proposed network is symmetric with each side consisting of one convolutional layer and several coupling layers. The two input images connected with the two sides of the network, respectively, are transformed into a feature space where their feature representations become more consistent. In this feature space, the different map is calculated, which then leads to the ultimate detection map by applying a thresholding algorithm. The network parameters are learned by optimizing a coupling function. The learning process is unsupervised, which is different from most existing change detection methods based on heterogeneous images. Experimental results on both homogenous and heterogeneous images demonstrate the promising performance of the proposed network compared with several existing approaches.","keywords_author":["Change detection","deep neural network","denoising autoencoder optical images","synthetic aperture radar images"],"keywords_other":["STATISTICAL-MODEL","REMOTE-SENSING IMAGES","INFORMATION","FRAMEWORK","AUTOMATIC CHANGE DETECTION","UNSUPERVISED CHANGE DETECTION","CLASSIFICATION","NEURAL-NETWORKS","RECOGNITION","MULTITEMPORAL SAR IMAGES"],"max_cite":3.0,"pub_year":2018.0,"sources":"['wos', 'ieee']","rawkeys":["neural-networks","recognition","deep neural network","synthetic aperture radar images","denoising autoencoder optical images","framework","multitemporal sar images","unsupervised change detection","change detection","automatic change detection","classification","information","remote-sensing images","statistical-model"],"tags":["recognition","contrastive divergence","denoising autoencoder optical images","framework","multitemporal sar images","neural networks","statistical models","unsupervised change detection","automatic change detection","classification","convolutional neural network","information","remote sensing images","synthetic aperture radar (sar) images"]},{"p_id":2557,"title":"A Fuzzy Restricted Boltzmann Machine: Novel Learning Algorithms Based on the Crisp Possibilistic Mean Value of Fuzzy Numbers","abstract":"A fuzzy restricted Boltzmann machine (FRBM) is extended from a restricted Boltzmann machine (RBM) by replacing all the real-valued parameters with fuzzy numbers. A new FRBM that employs the crisp possibilistic mean value of a fuzzy number to defuzzify the fuzzy free energy function is presented. This approach is much clearer and easier to obtain the expression of the defuzzified free energy function and its approximation than the centroid method. Several theorems that discuss the error bounds of the approximation to ensure the rationality and validity are also investigated. Learning algorithms are given for the designed FRBM with symmetric triangular fuzzy numbers (STFNs), asymmetric triangular fuzzy numbers, and Gaussian fuzzy numbers. By appropriately choosing the parameters, a theorem is concluded that all FRBMs with symmetric fuzzy numbers will have identical learning algorithm to that of FRBMs with STFNs. This is illustrated by a case of FRBM with Gaussian fuzzy numbers. Two experiments including the MNIST handwriting recognition and the Bars-and-Stripes benchmark are carried out. The results show that the proposed FRBMs significantly outperform RBMs in learning accuracy and generalization ability, especially when encountering unlearned samples and recovering incomplete images.","keywords_author":["Crisp possibilistic mean value","fuzzy number","fuzzy restricted Boltzmann machine (FRBM)","learning algorithm"],"keywords_other":["DEEP BELIEF NETWORKS","NEURAL-NETWORKS","RECOGNITION"],"max_cite":2.0,"pub_year":2018.0,"sources":"['wos', 'ieee']","rawkeys":["neural-networks","fuzzy restricted boltzmann machine (frbm)","recognition","crisp possibilistic mean value","fuzzy number","learning algorithm","deep belief networks"],"tags":["recognition","neural networks","crisp possibilistic mean value","fuzzy numbers","learning algorithm","deep belief networks","fuzzy restricted boltzmannmachine"]},{"p_id":10751,"title":"EPSILON-CP: Using deep learning to combine information from multiple sources for protein contact prediction","abstract":"Background: Accurately predicted contacts allow to compute the 3D structure of a protein. Since the solution space of native residue-residue contact pairs is very large, it is necessary to leverage information to identify relevant regions of the solution space, i.e. correct contacts. Every additional source of information can contribute to narrowing down candidate regions. Therefore, recent methods combined evolutionary and sequence-based information as well as evolutionary and physicochemical information. We develop a new contact predictor (EPSILON-CP) that goes beyond current methods by combining evolutionary, physicochemical, and sequence-based information. The problems resulting from the increased dimensionality and complexity of the learning problem are combated with a careful feature analysis, which results in a drastically reduced feature set. The different information sources are combined using deep neural networks.","keywords_author":["Contact prediction","Deep learning","Meta algorithms","Contact prediction","Meta algorithms","Deep learning"],"keywords_other":["Protein Binding","Long range contacts","RATES","Protein Conformation","Meta-algorithms","Information sources","SEQUENCE","MAPS","Learning problem","RECONSTRUCTION","ORDER","Prediction methods","Critical feature analysis","RECOGNITION","Software","RESIDUE-RESIDUE CONTACTS","Feature analysis","Amino acid compositions","NETWORKS","ENERGIES","Proteins","Computational Biology"],"max_cite":1.0,"pub_year":2017.0,"sources":"['scp', 'wos']","rawkeys":["long range contacts","rates","meta-algorithms","reconstruction","sequence","energies","meta algorithms","maps","prediction methods","recognition","deep learning","critical feature analysis","protein conformation","networks","amino acid compositions","software","contact prediction","residue-residue contacts","protein binding","proteins","feature analysis","computational biology","order","learning problem","information sources"],"tags":["long range contacts","meta-algorithms","reconstruction","sequence","ratings","amino-acid-composition","map","machine learning","energy","prediction methods","recognition","critical feature analysis","protein conformation","networks","software","contact prediction","residue-residue contacts","protein binding","proteins","feature analysis","computational biology","order","learning problem","information sources"]},{"p_id":10752,"title":"Learning traffic as images: A deep convolutional neural network for large-scale transportation network speed prediction","abstract":"This paper proposes a convolutional neural network (CNN)-based method that learns traffic as images and predicts large-scale, network-wide traffic speed with a high accuracy. Spatiotemporal traffic dynamics are converted to images describing the time and space relations of traffic flow via a two-dimensional time-space matrix. A CNN is applied to the image following two consecutive steps: abstract traffic feature extraction and network-wide traffic speed prediction. The effectiveness of the proposed method is evaluated by taking two real-world transportation networks, the second ring road and north-east transportation network in Beijing, as examples, and comparing the method with four prevailing algorithms, namely, ordinary least squares, k-nearest neighbors, artificial neural network, and random forest, and three deep learning architectures, namely, stacked autoencoder, recurrent neural network, and long-short-term memory network. The results show that the proposed method outperforms other algorithms by an average accuracy improvement of 42.91% within an acceptable execution time. The CNN can train the model in a reasonable time and, thus, is suitable for large-scale transportation networks.","keywords_author":["Convolutional neural network","Deep learning","Spatiotemporal feature","Traffic speed prediction","Transportation network","transportation network","traffic speed prediction","spatiotemporal feature","deep learning","convolutional neural network"],"keywords_other":["Spatio temporal features","SVR","Traffic speed","Large-scale transportation","Learning architectures","Accuracy Improvement","RECOGNITION","NONPARAMETRIC REGRESSION","Convolutional neural network","Transportation network","PATTERNS","FLOW PREDICTION","Ordinary least squares"],"max_cite":21.0,"pub_year":2017.0,"sources":"['ieee', 'scp', 'wos']","rawkeys":["learning architectures","ordinary least squares","recognition","spatiotemporal feature","transportation network","deep learning","patterns","svr","flow prediction","large-scale transportation","convolutional neural network","nonparametric regression","traffic speed prediction","spatio temporal features","traffic speed","accuracy improvement"],"tags":["learning architectures","ordinary least squares","recognition","transportation network","support vector regression (svr)","machine learning","patterns","flow prediction","traffic speed prediction","convolutional neural network","large-scale transport","spatio temporal features","traffic speed","accuracy improvement","non-parametric regression"]},{"p_id":2562,"title":"Unsupervised Transfer Learning via Multi-Scale Convolutional Sparse Coding for Biomedical Applications","abstract":"\u00a9 1979-2012 IEEE. The capabilities of (I) learning transferable knowledge across domains; and (II) fine-tuning the pre-learned base knowledge towards tasks with considerably smaller data scale are extremely important. Many of the existing transfer learning techniques are supervised approaches, among which deep learning has the demonstrated power of learning domain transferrable knowledge with large scale network trained on massive amounts of labeled data. However, in many biomedical tasks, both the data and the corresponding label can be very limited, where the unsupervised transfer learning capability is urgently needed. In this paper, we proposed a novel multi-scale convolutional sparse coding (MSCSC) method, that (I) automatically learns filter banks at different scales in a joint fashion with enforced scale-specificity of learned patterns; and (II) provides an unsupervised solution for learning transferable base knowledge and fine-tuning it towards target tasks. Extensive experimental evaluation of MSCSC demonstrates the effectiveness of the proposed MSCSC in both regular and transfer learning tasks in various biomedical domains.","keywords_author":["biomedical application","brain tumors","breast cancer subtypes","convolutional sparse coding","deep learning","low dose ionizing radiation (LDIR)","mouse model","sharable information","Transfer learning","Transfer learning","sharable information","convolutional sparse coding","deep learning","biomedical application","brain tumors","low dose ionizing radiation (LDIR)","mouse model","breast cancer subtypes"],"keywords_other":["Biomedical applications","Transfer learning","sharable information","Breast Cancer","VISION","VISUAL-CORTEX","Brain tumors","Sparse coding","CLASSIFICATION","HISTOLOGY","RECOGNITION","Mouse models","Low dose"],"max_cite":3.0,"pub_year":2018.0,"sources":"['scp', 'wos', 'ieee']","rawkeys":["visual-cortex","low dose","vision","breast cancer subtypes","classification","transfer learning","biomedical application","recognition","deep learning","histology","convolutional sparse coding","mouse model","sparse coding","breast cancer","biomedical applications","sharable information","low dose ionizing radiation (ldir)","brain tumors","mouse models"],"tags":["biomedical applications","recognition","visual-cortex","sharable information","transfer learning","histology","low dose","machine learning","convolutional sparse coding","mouse model","vision","breast cancer subtypes","low dose ionizing radiation (ldir)","classification","brain tumors","sparse coding","breast cancer"]},{"p_id":117251,"title":"Evaluation of support vector machine and artificial neural networks in weed detection using shape features","abstract":"Weed detection is still a challenging problem for robotic weed removal. Small tolerance between the cutting tine and main crop position requires highly precise discrimination of the weed against the main crop. Close similarities between the shape features of sugar beet and common weeds make it impossible to define an exclusive feature to be able to efficiently detect all the weeds with acceptable accuracy. Therefore in this study, it was tried to integrate several shape features to establish a pattern for each variety of the plants. To enable the vision system in the detection of the weeds based on their pattern, support vector machine and artificial neural networks were employed. Four species of common weeds in sugar beet fields were studied. Shape feature sets included Fourier descriptors and moment invariant features. Results showed that the overall classification accuracy of ANN was 92.92%, where 92.50% of weeds were correctly classified. Higher accuracies were obtained when the SVM was used as the classifier with an overall accuracy of 95.00% whereas 93.33% of weeds were correctly classified. Also, 93.33% and 96.67% of sugar beet plants were correctly classified by ANN and SVM respectively.","keywords_author":["Machine vision","Image processing","Pattern recognition","Weeding robot","Precision agriculture","Plant phenotype"],"keywords_other":["CROP","CROP\/WEED DISCRIMINATION","FOURIER DESCRIPTORS","CLASSIFICATION","COLOR","INDEXES","FIELD","RECOGNITION","SEGMENTATION","IMAGE RETRIEVAL"],"max_cite":0.0,"pub_year":2018.0,"sources":"['wos']","rawkeys":["weeding robot","precision agriculture","crop","recognition","segmentation","image processing","indexes","field","image retrieval","machine vision","crop\/weed discrimination","fourier descriptors","color","classification","pattern recognition","plant phenotype"],"tags":["weeding robot","precision agriculture","recognition","segmentation","image processing","crops","field","image retrieval","index","machine vision","crop\/weed discrimination","fourier descriptors","color","classification","pattern recognition","plant phenotyping"]},{"p_id":59922,"title":"Gastric Pathology Image Classification Using Stepwise Fine-Tuning for Deep Neural Networks","abstract":"Deep learning using convolutional neural networks (CNNs) is a distinguished tool for many image classification tasks. Due to its outstanding robustness and generalization, it is also expected to play a key role to facilitate advanced computer-aided diagnosis (CAD) for pathology images. However, the shortage of well-annotated pathology image data for training deep neural networks has become a major issue at present because of the high-cost annotation upon pathologist's professional observation. Faced with this problem, transfer learning techniques are generally used to reinforcing the capacity of deep neural networks. In order to further boost the performance of the state-of-the-art deep neural networks and alleviate insufficiency of well-annotated data, this paper presents a novel stepwise fine-tuning-based deep learning scheme for gastric pathology image classification and establishes a new type of target-correlative intermediate datasets. Our proposed scheme is deemed capable of making the deep neural network imitating the pathologist's perception manner and of acquiring pathology-related knowledge in advance, but with very limited extra cost in data annotation. The experiments are conducted with both well-annotated gastric pathology data and the proposed target-correlative intermediate data on several state-of-the-art deep neural networks. 'I he results congruously demonstrate the feasibility and superiority of our proposed scheme for boosting the classification performance.","keywords_author":null,"keywords_other":["CANCER-DETECTION","ARCHITECTURES","RECOGNITION","SEGMENTATION","LOCAL AUTOCORRELATION FEATURE"],"max_cite":0.0,"pub_year":2018.0,"sources":"['ieee', 'wos']","rawkeys":["cancer-detection","recognition","segmentation","architectures","local autocorrelation feature"],"tags":["recognition","segmentation","local autocorrelation feature","cancer detection","architecture"]},{"p_id":10771,"title":"Using a model of human visual perception to improve deep learning","abstract":"Deep learning algorithms achieve human-level (or better) performance on many tasks, but there still remain situations where humans learn better or faster. With regard to classification of images, we argue that some of those situations are because the human visual system represents information in a format that promotes good training and classification. To demonstrate this idea, we show how occluding objects can impair performance of a deep learning system that is trained to classify digits in the MNIST database. We describe a human inspired segmentation and interpolation algorithm that attempts to reconstruct occluded parts of an image, and we show that using this reconstruction algorithm to pre-process occluded images promotes training and classification performance. (C) 2018 Elsevier Ltd. All rights reserved.","keywords_author":["Deep learning","Segmentation","Visual perception","Segmentation","Visual perception","Deep learning"],"keywords_other":["Human visual perception","Classification performance","CORTEX","Mnist database","Visual perception","Human Visual System","Human levels","NEURAL DYNAMICS","Reconstruction algorithms","RECOGNITION","ATTENTION","Interpolation algorithms","COMPLETION"],"max_cite":0.0,"pub_year":2018.0,"sources":"['scp', 'wos']","rawkeys":["interpolation algorithms","human levels","mnist database","classification performance","recognition","segmentation","deep learning","neural dynamics","completion","human visual perception","attention","visual perception","human visual system","cortex","reconstruction algorithms"],"tags":["interpolation algorithms","human levels","mnist database","classification performance","recognition","segmentation","neural dynamics","machine learning","completion","human visual perception","attention","visual perception","human visual system","cortex","reconstruction algorithms"]},{"p_id":2582,"title":"Symmetrical Dense-Shortcut Deep Fully Convolutional Networks for Semantic Segmentation of Very-High-Resolution Remote Sensing Images","abstract":"Semantic segmentation has emerged as a mainstream method in very-high-resolution remote sensing land-use\/land-cover applications. In this paper, we first review the state-of-the-art semantic segmentation models in both computer vision and remote sensing fields. Subsequently, we introduce two semantic segmentation frameworks: SNFCN and SDFCN, both of which contain deep fully convolutional networks with shortcut blocks. We adopt an overlay strategy as the postprocessing method. Based on our frameworks, we conducted experiments on two online ISPRS datasets: Vaihingen and Potsdam. The results indicate that our frameworks achieve higher overall accuracy than the classic FCN-8s and Seg-Net models. In addition, our postprocessing method can increase the overall accuracy by about 1%-2% and help to eliminate \"salt and pepper\" phenomena and block effects.","keywords_author":["Convolutional neural networks (CNN)","deep learning (DL)","fully convolutional networks (FCN)","remote sensing","SDFCN","semantic segmentation"],"keywords_other":["CLASSIFICATION","STACKED DENOISING AUTOENCODERS","NEURAL-NETWORKS","RECOGNITION","ALGORITHMS","LIMITATIONS"],"max_cite":1.0,"pub_year":2018.0,"sources":"['wos', 'ieee']","rawkeys":["neural-networks","fully convolutional networks (fcn)","recognition","remote sensing","stacked denoising autoencoders","sdfcn","convolutional neural networks (cnn)","limitations","classification","algorithms","semantic segmentation","deep learning (dl)"],"tags":["recognition","remote sensing","neural networks","sdfcn","machine learning","fully convolutional network","limits","classification","convolutional neural network","algorithms","semantic segmentation","stacked denoising autoencoder"]},{"p_id":2588,"title":"An Automatic Cardiac Arrhythmia Classification System With Wearable Electrocardiogram","abstract":"This paper presents an automatic wearable electrocardiogram (ECG) classification and monitoring system with stacked denoising autoencoder (SDAE). We use a wearable device with wireless sensors to obtain the ECG data, and send these ECG data to a computer with Bluetooth 4.2. Then, these ECG data are classified by the automatic cardiac arrhythmia classification system. First, the ECG feature representation is learned by the SDAE with sparsity constraint. Then, the softmax regression is used to classify the ECG beats. In the fine-tuning phase, an active learning is added to improve the performance. In the active learning phase, we use the method that relies on the deep neural networks posterior probabilities to associate confidence measures to select the most informative samples. Breaking-ties and modified breaking-ties methods are used to select the most informative samples. We validate the proposed method on the well-known MIT-BIH arrhythmia database and ECG data obtained from the wearable device. We follow the recommendations of the Association for the Advancement of Medical Instrumentation for class labeling and results presentation. The results show that the classification performance of our proposed approach outperforms the most of the state-of-the-art methods.","keywords_author":["Stacked denoising autoencoder","wearable device","active learning","breaking-ties","modified breaking-ties"],"keywords_other":["ASSOCIATION","MORPHOLOGY","DISEASE","ECG CLASSIFICATION","DATABASE","RECOGNITION","HEARTBEAT INTERVAL FEATURES","CONVOLUTIONAL NEURAL-NETWORKS"],"max_cite":0.0,"pub_year":2018.0,"sources":"['wos', 'ieee']","rawkeys":["wearable device","association","recognition","disease","active learning","database","morphology","heartbeat interval features","modified breaking-ties","convolutional neural-networks","breaking-ties","ecg classification","stacked denoising autoencoder"],"tags":["wearable devices","association","recognition","databases","disease","machine learning","morphology","heartbeat interval features","modified breaking-ties","convolutional neural network","ecg classifications","breaking-ties","stacked denoising autoencoder"]},{"p_id":10784,"title":"Recognition of emotions using multimodal physiological signals and an ensemble deep learning model","abstract":"Background and Objective: Using deep-learning methodologies to analyze multimodal physiological signals becomes increasingly attractive for recognizing human emotions. However, the conventional deep emotion classifiers may suffer from the drawback of the lack of the expertise for determining model structure and the oversimplification of combining multimodal feature abstractions.","keywords_author":["Affective computing","Deep learning","Emotion recognition","Ensemble learning","Physiological signals","Emotion recognition","Affective computing","Physiological signals","Deep learning","Ensemble learning"],"keywords_other":["Emotions","Learning","Humans","ALGORITHM","EXPRESSIONS","Affective Computing","Deep learning","EEG","Physiological signals","BRAIN-COMPUTER INTERFACES","COMMUNICATION","FUNCTIONAL-STATE CLASSIFICATION","VISUAL-STIMULI","Ensemble learning","Emotion recognition","Models, Psychological","BCI","VECTOR","FUSION"],"max_cite":25.0,"pub_year":2017.0,"sources":"['ieee', 'scp', 'wos']","rawkeys":["emotions","affective computing","psychological","visual-stimuli","bci","vector","brain-computer interfaces","ensemble learning","models","expressions","fusion","algorithm","deep learning","eeg","learning","humans","communication","physiological signals","emotion recognition","functional-state classification"],"tags":["brain-computer interfaces","recognition","model","emotion","affective computing","physiological signals","machine learning","eeg","humans","vectors","visual stimuli","communication","emotion recognition","expression","functional-state classification","algorithms","ensemble learning","fusion"]},{"p_id":10799,"title":"A review on the application of deep learning in system health management","abstract":"Given the advancements in modern technological capabilities, having an integrated health management and diagnostic strategy becomes an important part of a system's operational life-cycle. This is because it can be used to detect anomalies, analyse failures and predict the future state based on up-to-date information. By utilising condition data and on-site feedback, data models can be trained using machine learning and statistical concepts. Once trained, the logic for data processing can be embedded on on-board controllers whilst enabling real-time health assessment and analysis. However, this integration inevitably faces several difficulties and challenges for the community; indicating the need for novel approaches to address this vexing issue. Deep learning has gained increasing attention due to its potential advantages with data classification and feature extraction problems. It is an evolving research area with diverse application domains and hence its use for system health management applications must been researched if it can be used to increase overall system resilience or potential cost benefits for maintenance, repair, and overhaul activities. This article presents a systematic review of artificial intelligence based system health management with an emphasis on recent trends of deep learning within the field. Various architectures and related theories are discussed to clarify its potential. Based on the reviewed work, deep learning demonstrates plausible benefits for fault diagnosis and prognostics. However, there are a number of limitations that hinder its widespread adoption and require further development. Attention is paid to overcoming these challenges, with future opportunities being enumerated. (C) 2017 Elsevier Ltd. All rights reserved.","keywords_author":["Artificial intelligence","Deep learning","Fault analysis","Maintenance","Real-time processing","System health management","Artificial intelligence","Deep learning","System health management","Real-time processing","Fault analysis","Maintenance"],"keywords_other":["Fault analysis","FAILURE DIAGNOSIS","BEARING FAULT-DIAGNOSIS","ROTATING MACHINERY","Diagnostic strategy","PROGNOSTICS","CLASSIFICATION","Diverse applications","Maintenance , repair , and overhauls","System health management","Technological capability","Realtime processing","NEURAL-NETWORKS","RECOGNITION","Statistical concepts","BELIEF NETWORKS","FOUND EVENTS","MAINTENANCE"],"max_cite":4.0,"pub_year":2018.0,"sources":"['ieee', 'scp', 'wos']","rawkeys":["repair","diagnostic strategy","technological capability","found events","diverse applications","classification","system health management","bearing fault-diagnosis","neural-networks","recognition","maintenance","deep learning","belief networks","failure diagnosis","artificial intelligence","realtime processing","prognostics","fault analysis","real-time processing","statistical concepts","and overhauls","rotating machinery"],"tags":["repair","diagnostic strategy","technological capability","found events","diverse applications","classification","system health management","machine learning","bearing fault diagnosis","recognition","maintenance","neural networks","belief networks","failure diagnosis","realtime processing","prognostics","fault analysis","real-time processing","statistical concepts","and overhauls","rotating machinery"]},{"p_id":2611,"title":"Jointly Learning Deep Features, Deformable Parts, Occlusion and Classification for Pedestrian Detection","abstract":"Feature extraction, deformation handling, occlusion handling, and classification are four important components in pedestrian detection. Existing methods learn or design these components either individually or sequentially. The interaction among these components is not yet well explored. This paper proposes that they should be jointly learned in order to maximize their strengths through cooperation. We formulate these four components into a joint deep learning framework and propose a new deep network architecture (Code available on www.ee.cuhk.edu.hk\/wlouyang\/projects\/ouyangWiccv13Joint\/index.html). By establishing automatic, mutual interaction among components, the deep model has average miss rate 8.57 percent\/11.71 percent on the Caltech benchmark dataset with new\/original annotations.","keywords_author":["CNN","convolutional neural networks","deep learning","deep model","object detection","CNN","convolutional neural networks","object detection","deep learning","deep model"],"keywords_other":["Deformable models","IMAGE","Pattern analysis","Image edge detection","Image color analysis","MODEL","RECOGNITION","Convolutional neural network","SINGLE"],"max_cite":0.0,"pub_year":2018.0,"sources":"['scp', 'wos', 'ieee']","rawkeys":["image edge detection","convolutional neural networks","model","recognition","deep learning","deep model","single","image color analysis","cnn","image","object detection","convolutional neural network","pattern analysis","deformable models"],"tags":["image edge detection","images","model","recognition","deep model","single","machine learning","image color analysis","object detection","convolutional neural network","pattern analysis","deformable models"]},{"p_id":10806,"title":"Disease Staging and Prognosis in Smokers Using Deep Learning in Chest Computed Tomography","abstract":"Rationale: Deep learning is a powerful tool that may allow for improved outcome prediction.","keywords_author":["artificial intelligence (computer vision systems)","neural networks","chronic obstructive pulmonary disease","X-ray computed tomography"],"keywords_other":["EMPHYSEMA","OBSTRUCTIVE PULMONARY-DISEASE","CLASSIFICATION","COPD","NEURAL-NETWORKS","RECOGNITION","AIRWAY DIMENSIONS","GOODNESS-OF-FIT","CANCER","IMAGES"],"max_cite":3.0,"pub_year":2018.0,"sources":"['wos']","rawkeys":["neural-networks","copd","x-ray computed tomography","emphysema","images","goodness-of-fit","chronic obstructive pulmonary disease","cancer","airway dimensions","neural networks","obstructive pulmonary-disease","recognition","artificial intelligence (computer vision systems)","classification"],"tags":["emphysema","images","goodness-of-fit","chronic obstructive pulmonary disease","cancer","airway dimensions","neural networks","obstructive pulmonary-disease","machine learning","recognition","classification","computed tomography"]},{"p_id":10814,"title":"3D shape recognition and retrieval based on multi-modality deep learning","abstract":"For 3D shape analysis, an effective and efficient feature is the key to popularize its applications in 3D domain where the major challenge lies in designing an effective high-level feature. The three-dimensional shape contains various useful information including visual information, geometric relationships, and other type properties. Thus the strategy of exploring these characteristics is the core of extracting effective 3D shape features. In this paper, we propose a novel 3D feature learning framework which combines different modality data effectively to promote the discriminability of uni-modal feature by using deep learning. The geometric information and visual information are extracted by Convolutional Neural Networks (CNNs) and Convolutional Deep Belief Networks (CDBNs), respectively, and then two independent Deep Belief Networks (DBNs) are employed to learn high-level features from geometric and visual features. Finally, a Restricted Boltzmann Machine (RBM) is trained for mining the deep correlations between different modalities. Extensive experiments demonstrate that the proposed framework achieves better performance. (C) 2017 Elsevier B.V. All rights reserved.","keywords_author":["3D shape","Deep learning","Multi modality","Recognition","Retrieval","3D shape","Recognition","Retrieval","Deep learning","Multi modality"],"keywords_other":["MODEL RETRIEVAL","Restricted boltzmann machine","GEOMETRIC FEATURES","Geometric relationships","Recognition","NETWORKS","3-D shape","Multi modality","3-D OBJECT RETRIEVAL","Convolutional neural network","Retrieval","Three-dimensional shape","SIMILARITY","IMAGES"],"max_cite":1.0,"pub_year":2017.0,"sources":"['ieee', 'scp', 'wos']","rawkeys":["recognition","images","geometric features","deep learning","model retrieval","similarity","three-dimensional shape","3-d shape","3d shape","networks","geometric relationships","convolutional neural network","retrieval","3-d object retrieval","restricted boltzmann machine","multi modality"],"tags":["recognition","images","model retrieval","similarity","three-dimensional shape","machine learning","geometric feature","3-d shape","networks","3d object retrieval","geometric relationships","convolutional neural network","multi-modal","retrieval","restricted boltzmann machine"]},{"p_id":10820,"title":"High-Content Analysis of Breast Cancer Using Single-Cell Deep Transfer Learning","abstract":"High-content analysis has revolutionized cancer drug discovery by identifying substances that alter the phenotype of a cell, which prevents tumor growth and metastasis. The high-resolution biofluorescence images from assays allow precise quantitative measures enabling the distinction of small molecules of a host cell from a tumor. In this work, we are particularly interested in the application of deep neural networks (DNNs), a cutting-edge machine learning method, to the classification of compounds in chemical mechanisms of action (MOAs). Compound classification has been performed using image-based profiling methods sometimes combined with feature reduction methods such as principal component analysis or factor analysis. In this article, we map the input features of each cell to a particular MOA class without using any treatment-level profiles or feature reduction methods. To the best of our knowledge, this is the first application of DNN in this domain, leveraging single-cell information. Furthermore, we use deep transfer learning (DTL) to alleviate the intensive and computational demanding effort of searching the huge parameter's space of a DNN. Results show that using this approach, we obtain a 30% speedup and a 2% accuracy improvement.","keywords_author":["high-content screening","image analysis","deep transfer learning","cancer drug discovery"],"keywords_other":["NEURAL-NETWORKS","RECOGNITION","VISUAL-CORTEX"],"max_cite":13.0,"pub_year":2016.0,"sources":"['ieee', 'wos']","rawkeys":["neural-networks","recognition","visual-cortex","deep transfer learning","cancer drug discovery","high-content screening","image analysis"],"tags":["recognition","visual-cortex","neural networks","deep transfer learning","cancer drug discovery","high-content screening","image analysis"]},{"p_id":10822,"title":"Deep Boosting: Joint feature selection and analysis dictionary learning in hierarchy","abstract":"This work investigates how the traditional image classification pipelines can be extended into a deep architecture, inspired by recent successes of deep neural networks. We propose a deep boosting framework based on layer-by-layer joint feature boosting and dictionary learning. In each layer, we construct a dictionary of filters by combining the filters from the lower layer, and iteratively optimize the image representation with a joint discriminative-generative formulation, i.e. minimization of empirical classification error plus regularization of analysis image generation over training images. For optimization, we perform two iterating steps: (i) to minimize the classification error, select the most discriminative features using the gentle adaboost algorithm; (ii) according to the feature selection, update the filters to minimize the regularization on analysis image representation using the gradient descent method. Once the optimization is converged, we learn the higher layer representation in the same way. Our model delivers several distinct advantages. First, our layer-wise optimization provides the potential to build very deep architectures. Second, the generated image representation is compact and meaningful by jointly considering image classification and generation. In several visual recognition tasks, our framework outperforms existing state-of-the-art approaches. (C) 2015 Elsevier B.V. All rights reserved.","keywords_author":["Representation Learning","Compositional boosting","Dictionary learning","Image Classification"],"keywords_other":["AGE ESTIMATION","REPRESENTATION","PERSON REIDENTIFICATION","ALGORITHM","K-SVD","MODEL","IMAGE CLASSIFICATION","RECOGNITION","RETRIEVAL","OBJECT DETECTION"],"max_cite":3.0,"pub_year":2016.0,"sources":"['ieee', 'wos']","rawkeys":["algorithm","recognition","model","representation","age estimation","representation learning","dictionary learning","object detection","compositional boosting","person reidentification","retrieval","image classification","k-svd"],"tags":["recognition","model","representation","age estimation","person re-identification","representation learning","dictionary learning","object detection","compositional boosting","retrieval","algorithms","image classification","k-svd"]},{"p_id":59981,"title":"A visual familiarity account of evidence for orthographic processing in pigeons (Columbia livia): a reply to Scarf, Corballis, Gunturkun, and Colombo (2017)","abstract":"Scarf et al. (Proc Natl Acad Sci 113(40):11272-11276, 2016) demonstrated that pigeons, as with baboons (Grainger et al. in Science 336(6078):245-248, 2012; Ziegler in Psychol Sci. 10.1177\/0956797612474322, 2013), can be trained to display several behavioural hallmarks of human orthographic processing. But, Vokey and Jamieson (Psychol Sci 25(4):991-996, 2014) demonstrated that a standard, autoassociative neural network model of memory applied to pixel maps of the words and nonwords reproduces all of those results. In a subsequent report, Scarf et al. (Anim Cognit 20(5):999-1002, 2017) demonstrated that pigeons can reproduce one more marker of human orthographic processing: the ability to discriminate visually presented four-letter words from their mirror-reversed counterparts (e.g. \"LEFT\" vs. ). The current report shows that the model of Vokey and Jamieson (2014) reproduces the results of Scarf et al. (2017) and reinforces the original argument: the recent results thought to support a conclusion of orthographic processing in pigeons and baboons are consistent with but do not force that conclusion.","keywords_author":["Orthographic processing","Autoassociative networks","PCA","Visual familiarity","Baboons","Pigeons"],"keywords_other":["RECOGNITION","BABOONS PAPIO-PAPIO"],"max_cite":0.0,"pub_year":2018.0,"sources":"['wos']","rawkeys":["recognition","baboons","visual familiarity","orthographic processing","autoassociative networks","baboons papio-papio","pca","pigeons"],"tags":["principal component analysis","recognition","baboons","visual familiarity","autoassociative networks","baboons papio-papio","orthographic processing","pigeons"]},{"p_id":10830,"title":"Deep learning enhancement of infrared face images using generative adversarial networks","abstract":"This work presents a deep learning framework based on the use of deep convolutional generative adversarial networks (DCGAN) for infrared face image super-resolution. We use DCGAN for upscaling the images by a factor of 4 x 4, starting at a size of 16 x 16 and obtaining a 64 x 64 face image. Tests are conducted using different infrared face datasets operating in the near-infrared (NIR) and the long-wave infrared (LWIR) spectrum. We can see that the proposed framework performs well and preserves important details of the face. This kind of approach can be very useful in security applications where we can scan faces in the crowd or detect faces at a distance and upscale them for further recognition through an infrared or a multispectral face recognition system. (C) 2018 Optical Society of America","keywords_author":null,"keywords_other":["QUALITY ASSESSMENT","SUPERRESOLUTION","RECOGNITION"],"max_cite":0.0,"pub_year":2018.0,"sources":"['wos']","rawkeys":["recognition","quality assessment","superresolution"],"tags":["sparse representation","recognition","information retrieval"]},{"p_id":59984,"title":"How does literacy break mirror invariance in the visual system?","abstract":null,"keywords_author":["multisensory","multi-system","reading","writing","literacy","alphabetization","mirror invariance","mirror discrimination"],"keywords_other":["CORTICAL NETWORKS","DEVELOPMENTAL DYSLEXIA","LANGUAGE NETWORK","VISION","STIMULI","RECOGNITION","AWARENESS","CHILDREN","SPEECH","BRAIN"],"max_cite":9.0,"pub_year":2014.0,"sources":"['wos']","rawkeys":["cortical networks","mirror discrimination","children","developmental dyslexia","literacy","multi-system","recognition","writing","language network","vision","brain","mirror invariance","speech","stimuli","reading","awareness","multisensory","alphabetization"],"tags":["cortical networks","language networks","mirror discrimination","children","developmental dyslexia","literacy","multi-system","recognition","writing","vision","brain","mirror invariance","multi-sensory","speech","stimuli","reading","awareness","alphabetization"]},{"p_id":10838,"title":"Symtosis: A liver ultrasound tissue characterization and risk stratification in optimized deep learning paradigm","abstract":null,"keywords_author":null,"keywords_other":["INTRAVASCULAR ULTRASOUND","MACHINE","POPULATION","DISEASE CLASSIFICATION","RECOGNITION","ALGORITHMS","FATTY LIVER","UNITED-STATES","PREVALENCE","IMAGES"],"max_cite":0.0,"pub_year":2018.0,"sources":"['wos']","rawkeys":["recognition","images","disease classification","machine","population","intravascular ultrasound","prevalence","united-states","algorithms","fatty liver"],"tags":["recognition","images","disease classification","machine","population","intravascular ultrasound","prevalence","united-states","algorithms","fatty liver"]},{"p_id":10840,"title":"Deep learning for automatic stereotypical motor movement detection using wearable sensors in autism spectrum disorders","abstract":"Autism Spectrum Disorders are associated with atypical movements, of which stereotypical motor movements (SMMs) interfere with learning and social interaction. The automatic SMM detection using inertial measurement units (IMU) remains complex due to the strong intra and inter-subject variability, especially when handcrafted features are extracted from the signal. We propose a new application of the deep learning to facilitate automatic SMM detection using multi-axis IMUs. We use a convolutional neural network (CNN) to learn a discriminative feature space from raw data. We show how the CNN can be used for parameter transfer learning to enhance the detection rate on longitudinal data. We also combine the long short-term memory (LSTM) with CNN to model the temporal patterns in a sequence of multi axis signals. Further, we employ ensemble learning to combine multiple LSTM learners into a more robust SMM detector. Our results show that: (1) feature learning outperforms handcrafted features; (2) parameter transfer learning is beneficial in longitudinal settings; (3) using LSTM to learn the temporal dynamic of signals enhances the detection rate especially for skewed training data; (4) an ensemble of LSTM5 provides more accurate and stable detectors. These findings provide a significant step toward accurate SMM detection in real-time scenarios. (C) 2017 Elsevier B.V. All rights reserved.","keywords_author":["Convolutional neural networks","Long short-term memory","Transfer learning","Ensemble learning","Wearable sensors","Autism spectrum disorders"],"keywords_other":["NETWORKS","ACCELEROMETRY","RECOGNITION","BEHAVIOR","MODELS","CHILDREN","TERM"],"max_cite":0.0,"pub_year":2017.0,"sources":"['ieee', 'wos']","rawkeys":["children","convolutional neural networks","recognition","wearable sensors","transfer learning","long short-term memory","term","networks","models","autism spectrum disorders","ensemble learning","accelerometry","behavior"],"tags":["children","model","recognition","wearable sensors","transfer learning","long short-term memory","term","networks","convolutional neural network","autism spectrum disorders","ensemble learning","accelerometry","behavior"]},{"p_id":2651,"title":"Multilinear Principal Component Analysis Network for Tensor Object Classification","abstract":"\u00a9 2017 IEEE.The recently proposed principal component analysis network (PCANet) has performed well with respect to the classification of 2-D images. However, feature extraction may perform less well when dealing with multi-dimensional images, since the spatial relationships within the structures of the images are not fully utilized. In this paper, we develop a multilinear principal component analysis network (MPCANet), which is a tensor extension of PCANet, to extract the high-level semantic features from multi-dimensional images. The extracted features largely minimize the intraclass invariance of tensor objects by making efficient use of spatial relationships within multi-dimensional images. The proposed MPCANet outperforms traditional methods on a benchmark composed of three data sets, including the UCF sports action database, the UCF11 database, and a medical image database. It is shown that even a simple one-layer MPCANet may outperform a two-layer PCANet.","keywords_author":["Deep learning","Medical image classification","MPCANet","PCANet","Tensor object classification","Deep learning","MPCANet","PCANet","tensor object classification","medical image classification"],"keywords_other":["Tensor objects","Medical image database","High-level semantic features","REPRESENTATION","PCANet","MPCANet","RECOGNITION","Spatial relationships","Multi-dimensional images","Intra class"],"max_cite":2.0,"pub_year":2017.0,"sources":"['scp', 'wos', 'ieee']","rawkeys":["recognition","high-level semantic features","tensor object classification","deep learning","multi-dimensional images","pcanet","tensor objects","representation","intra class","mpcanet","medical image database","spatial relationships","medical image classification"],"tags":["recognition","high-level semantic features","tensor object classification","pcanet","multi-dimensional images","tensor objects","machine learning","intra class","representation","mpcanet","medical image database","spatial relationships","medical image classification"]},{"p_id":10844,"title":"A Physics-Based Deep Learning Approach to Shadow Invariant Representations of Hyperspectral Images","abstract":"This paper proposes the Relit Spectral Angle-Stacked Autoencoder, a novel unsupervised feature learning approach for mapping pixel reflectances to illumination invariant encodings. This work extends the Spectral Angle-Stacked Autoencoder so that it can learn a shadow-invariant mapping. The method is inspired by a deep learning technique, Denoising Autoencoders, with the incorporation of a physics-based model for illumination such that the algorithm learns a shadow invariant mapping without the need for any labelled training data, additional sensors, a priori knowledge of the scene or the assumption of Planckian illumination. The method is evaluated using datasets captured from several different cameras, with experiments to demonstrate the illumination invariance of the features and how they can be used practically to improve the performance of high-level perception algorithms that operate on images acquired outdoors.","keywords_author":["Autoencoders","Deep learning","Hyperspectral","Illumination invariance","Unsupervised feature learning","Autoencoders","Illumination invariance","unsupervised feature learning","hyperspectral","deep learning"],"keywords_other":["DIMENSIONALITY","NATURAL SCENES","Physics-based modeling","Illumination invariant","CLASSIFICATION","OPTIMIZATION","Unsupervised feature learning","Illumination invariance","Learning techniques","NEURAL-NETWORKS","RECOGNITION","HyperSpectral","REMOVAL","Learning approach","DAYLIGHT","ILLUMINATION","Autoencoders"],"max_cite":1.0,"pub_year":2018.0,"sources":"['ieee', 'scp', 'wos']","rawkeys":["hyperspectral","natural scenes","dimensionality","illumination","neural-networks","illumination invariance","recognition","removal","deep learning","unsupervised feature learning","physics-based modeling","illumination invariant","autoencoders","learning techniques","classification","optimization","daylight","learning approach"],"tags":["hyperspectral","natural scenes","dimensionality","illumination","physics-based models","recognition","removal","unsupervised feature learning","neural networks","auto encoders","machine learning","illumination invariant","learning techniques","classification","optimization","learning approach","daylighting"]},{"p_id":10848,"title":"Cell segmentation in histopathological images with deep learning algorithms by utilizing spatial relationships","abstract":"In many computerized methods for cell detection, segmentation, and classification in digital histopathology that have recently emerged, the task of cell segmentation remains a chief problem for image processing in designing computer-aided diagnosis (CAD) systems. In research and diagnostic studies on cancer, pathologists can use CAD systems as second readers to analyze high-resolution histopathological images. Since cell detection and segmentation are critical for cancer grade assessments, cellular and extracellular structures should primarily be extracted from histopathological images. In response, we sought to identify a useful cell segmentation approach with histopathological images that uses not only prominent deep learning algorithms (i.e., convolutional neural networks, stacked autoencoders, and deep belief networks), but also spatial relationships, information of which is critical for achieving better cell segmentation results. To that end, we collected cellular and extracellular samples from histopathological images by windowing in small patches with various sizes. In experiments, the segmentation accuracies of the methods used improved as the window sizes increased due to the addition of local spatial and contextual information. Once we compared the effects of training sample size and influence of window size, results revealed that the deep learning algorithms, especially convolutional neural networks and partly stacked autoencoders, performed better than conventional methods in cell segmentation.","keywords_author":["Computer-aided diagnosis systems","Deep learning algorithms","Histopathological images","Segmentation","Spatial relationships","Histopathological images","Deep learning algorithms","Computer-aided diagnosis systems","Segmentation","Spatial relationships"],"keywords_other":["Computer aided diagnosis systems","DIAGNOSIS","TEXTURES","BAG","Extracellular structure","Contextual information","Computer Aided Diagnosis(CAD)","Histopathological images","Segmentation accuracy","NETWORKS","CLASSIFICATION","MODEL","RECOGNITION","Spatial relationships","Convolutional neural network","COLLECTIONS","BREAST-CANCER HISTOPATHOLOGY","NUCLEI SEGMENTATION"],"max_cite":5.0,"pub_year":2017.0,"sources":"['ieee', 'scp', 'wos']","rawkeys":["collections","contextual information","histopathological images","bag","textures","classification","convolutional neural network","computer aided diagnosis(cad)","nuclei segmentation","segmentation","spatial relationships","diagnosis","computer aided diagnosis systems","recognition","networks","extracellular structure","computer-aided diagnosis systems","model","deep learning algorithms","breast-cancer histopathology","segmentation accuracy"],"tags":["contextual information","histopathological images","texture","classification","convolutional neural network","bagging","nuclei segmentation","segmentation","computer-aided diagnosis","spatial relationships","deep learning algorithm","computer aided diagnosis systems","diagnosis","recognition","networks","extracellular structure","model","breast-cancer histopathology","segmentation accuracy","collection"]},{"p_id":2659,"title":"G-CNN: Object Detection via Grid Convolutional Neural Network","abstract":"We propose an object detection system that depends on position-sensitive grid feature maps. State-of-the-art object detection networks rely on convolutional neural networks pre-trained on a large auxiliary data set (e.g., ILSVRC 2012) designed for an image-level classification task. The image level classification task favors translation invariance, while the object detection task needs localization representations that are translation variant to an extent. To address this dilemma, we construct position sensitive convolutional layers, called grid convolutional layers that activate the object's specific locations in the feature maps in the form of grids. With end-to-end training, the region of interesting grid pooling layer shepherds the last set of convolutional layers to learn specialized grid feature maps. Experiments on the PASCAL VOC 2007 data set show that our method outperforms the strong baselines faster region-based convolutional neural network counterpart and region-based fully convolutional networks by a large margin. Our method applied to ResNet-50 improves the mean average precision from 74.8%174.2% to 79.4% without any other tricks. In addition, our approach achieves similar results on different networks (ResNet-101) and data sets (PASCAL VOC 2012 and MS COCO).","keywords_author":["Computer vision","deep learning","grid feature map","object detection","region proposal","Computer vision","deep learning","grid feature map","object detection","region proposal"],"keywords_other":["Classification tasks","Object detection systems","Translation invariance","Detection networks","Region proposal","RECOGNITION","Grid features","Convolutional neural network","Proposals"],"max_cite":1.0,"pub_year":2017.0,"sources":"['ieee', 'scp', 'wos']","rawkeys":["recognition","translation invariance","deep learning","grid feature map","object detection systems","region proposal","classification tasks","proposals","object detection","convolutional neural network","detection networks","computer vision","grid features"],"tags":["recognition","translation invariance","grid feature map","object detection systems","machine learning","classification tasks","proposals","object detection","convolutional neural network","detection networks","region proposals","computer vision","grid features"]},{"p_id":43620,"title":"Deep convolutional image retrieval: A general framework","abstract":"\u00a9 2018 Elsevier B.V.In this paper a Convolutional Neural Network framework for Content Based Image Retrieval is proposed. We employ a deep CNN model to obtain the feature representations from the activations of the deepest layers and we retrain the network in order to produce more efficient image descriptors, relying on the available information. Our method suggests three basic model retraining approaches. That is, the Fully Unsupervised Retraining, if no information except from the dataset itself is available, the Retraining with Relevance Information, if the labels of the dataset are available, and the Relevance Feedback based Retraining, if feedback from users is available. We propose these approaches independently or in a pipeline, where each retraining approach operates as a pretraining step to the subsequent one. We also apply a query expansion method with spatial reranking on top of these approaches in order to boost the retrieval performance. The experimental evaluation on six publicly available image retrieval datasets indicates the effectiveness of the proposed method in learning more efficient representations for the retrieval task, outperforming other CNN-based retrieval techniques, as well as conventional hand-crafted feature-based approaches.","keywords_author":["Content based image retrieval","Convolutional neural networks","Deep learning","Query expansion","Content based image retrieval","Convolutional neural networks","Deep learning","Query expansion"],"keywords_other":["Retrieval techniques","Experimental evaluation","Feature based approaches","Retrieval performance","Feature representation","RECOGNITION","Convolutional neural network","Query expansion","Content based image retrieval"],"max_cite":0.0,"pub_year":2018.0,"sources":"['ieee', 'scp', 'wos']","rawkeys":["retrieval performance","recognition","convolutional neural networks","experimental evaluation","deep learning","feature based approaches","retrieval techniques","content based image retrieval","convolutional neural network","feature representation","query expansion"],"tags":["retrieval performance","recognition","experimental evaluation","feature based approaches","machine learning","retrieval techniques","convolutional neural network","feature representation","content-based image retrieval","query expansion"]},{"p_id":2663,"title":"Fast Deep Neural Networks With Knowledge Guided Training and Predicted Regions of Interests for Real-Time Video Object Detection","abstract":"It has been recognized that deeper and wider neural networks are continuously advancing the state-of-the-art performance of various computer vision and machine learning tasks. However, they often require large sets of labeled data for effective training and suffer from extremely high computational complexity, preventing them from being deployed in real-time systems, for example vehicle object detection from vehicle cameras for assisted driving. In this paper, we aim to develop a fast deep neural network for real-time video object detection by exploring the ideas of knowledge-guided training and predicted regions of interest. Specifically, we will develop a new framework for training deep neural networks on datasets with limited labeled samples using cross-network knowledge projection which is able to improve the network performance while reducing the overall computational complexity significantly. A large pre-trained teacher network is used to observe samples from the training data. A projection matrix is learned to project this teacher-level knowledge and its visual representations from an intermediate layer of the teacher network to an intermediate layer of a thinner and faster student network to guide and regulate the training process. To further speed up the network, we propose to train a low-complexity object detection using traditional machine learning methods, such as support vector machine. Using this low-complexity object detector, we identify the regions of interest that contain the target objects with high confidence. We obtain a mathematical formula to estimate the regions of interest to save the computation for each convolution layer. Our experimental results on vehicle detection from videos demonstrated that the proposed method is able to speed up the network by up to 16 times while maintaining the object detection performance.","keywords_author":["Assisted driving","deep neural networks","knowledge projection","speed optimization","vehicle detection"],"keywords_other":["CLASSIFICATION","SYSTEM","RECOGNITION","VEHICLE DETECTION","CLASSIFIERS","ARCHITECTURE"],"max_cite":0.0,"pub_year":2018.0,"sources":"['wos', 'ieee']","rawkeys":["knowledge projection","recognition","deep neural networks","system","classifiers","vehicle detection","classification","speed optimization","assisted driving","architecture"],"tags":["knowledge projection","recognition","system","vehicle detection","classifier","classification","convolutional neural network","speed optimization","assisted driving","architecture"]},{"p_id":43636,"title":"Person Reidentification Based on Elastic Projections","abstract":"\u00a9 2012 IEEE. Person reidentification usually refers to matching people in different camera views in nonoverlapping multicamera networks. Many existing methods learn a similarity measure by projecting the raw feature to a latent subspace to make the same target's distance smaller than different targets' distances. However, the same targets captured in different camera views should hold the same intrinsic attributes while different targets should hold different intrinsic attributes. Projecting all the data to the same subspace would cause loss of such an information and comparably poor discriminability. To address this problem, in this paper, a method based on elastic projections is proposed to learn a pairwise similarity measure for person reidentification. The proposed model learns two projections, positive projection and negative projection, which are both representative and discriminative. The representability refers to: for the same targets captured in two camera views, the positive projection can bridge the corresponding appearance variation and represent the intrinsic attributes of the same targets, while for the different targets captured in two camera views, the negative projection can explore and utilize the different attributes of different targets. The discriminability means that the intraclass distance should become smaller than its original distance after projection, while the interclass distance becomes larger on the contrary, which is the elastic property of the proposed model. In this case, prior information of the original data space is used to give guidance for the learning phase; more importantly, similar targets (but not the same) are effectively reduced by forcing the same targets to become more similar and different targets to become more distinct. The proposed model is evaluated on three benchmark data sets, including VIPeR, GRID, and CUHK, and achieves better performance than other methods.","keywords_author":["Machine learning","person reidentification","representative and discriminative","video surveillance","Machine learning","person reidentification","representative and discriminative","video surveillance"],"keywords_other":["Person re identifications","Discriminability","Inter-class distance","Representability","Similarity measure","FEATURES","CLASSIFICATION","TRACKING","RANKING","Elastic properties","Multi-camera networks","RECOGNITION","Prior information"],"max_cite":0.0,"pub_year":2018.0,"sources":"['ieee', 'scp', 'wos']","rawkeys":["recognition","multi-camera networks","person re identifications","features","video surveillance","machine learning","representability","tracking","prior information","elastic properties","similarity measure","classification","inter-class distance","discriminability","person reidentification","representative and discriminative","ranking"],"tags":["recognition","multi-camera networks","features","standards","machine learning","representation","person re-identification","prior information","elastic properties","tracking","similarity measure","classification","inter-class distance","representative and discriminative","video surveillance","discrimination"]},{"p_id":35445,"title":"Efficient human action recognition by luminance field trajectory and geometry information","abstract":"In recent years the video event understanding is an active research topic, with many applications in surveillance, security, and multimedia search and mining. In this paper we focus on the human action recognition problem and propose a new Curve-Distance approach based on the geometry modeling of video appearance manifold and the human action time series statistics on the geometry information. Experimental results on the KTH database demonstrate the solution to be effective and promising. \u00a92009 IEEE.","keywords_author":["Curve-distance approach","Machine learning","Recognition","Video event"],"keywords_other":["Human actions","Geometry information","Recognition","Multimedia search","Geometry modeling","Curve-distance approach","Human-action recognition","Machine learning","Research topics","Video event","Video events","Time-series statistics","Luminance fields"],"max_cite":2.0,"pub_year":2009.0,"sources":"['scp']","rawkeys":["recognition","luminance fields","research topics","video events","machine learning","human-action recognition","curve-distance approach","time-series statistics","human actions","geometry modeling","multimedia search","video event","geometry information"],"tags":["recognition","luminance fields","research topics","video events","human activity recognition","machine learning","curve-distance approach","time-series statistics","human actions","multimedia search","geometry model","geometry information"]},{"p_id":35452,"title":"Sequential problems that test generalization in learning classifier systems","abstract":"We present an approach to build sequential decision making problems which can test the generalization capabilities of classifier systems. The approach can be applied to any sequential problem defined over a binary domain and it generates a new problem with bounded sequential difficulty and bounded generalization difficulty. As an example, we applied the approach to generate two problems with simple sequential structure, huge number of states (more than a million), and many generalizations. These problems are used to compare a classifier system with effective generalization (XCS) and a learner without generalization (Q-learning). The experimental results confirm what was previously found mainly using single-step problems: also in sequential problems with huge state spaces, XCS can generalize effectively by detecting those substructures that are necessary for optimal sequential behavior. \u00a9 Springer-Verlag 2009.","keywords_author":["Generalization","Genetics-based machine learning","Learning classifier systems","Reinforcement learning","XCS"],"keywords_other":["Generalization capability","Sequential problems","Sequential structure","State space","Genetics-based machine learning","Sequential decision making","Optimal sequential","Q-learning","Classifier systems","Single-step","Learning classifier system","Number of state"],"max_cite":2.0,"pub_year":2009.0,"sources":"['scp']","rawkeys":["learning classifier system","single-step","sequential structure","generalization capability","classifier systems","state space","reinforcement learning","number of state","generalization","q-learning","xcs","learning classifier systems","sequential problems","genetics-based machine learning","sequential decision making","optimal sequential"],"tags":["learning classifier system","single-step","sequential structure","recognition","generalization capability","classifier systems","state space","reinforcement learning","number of state","xcs","q-learning","sequential problems","genetics-based machine learning","sequential decision making","optimal sequential"]},{"p_id":10878,"title":"A mobile outdoor augmented reality method combining deep learning object detection and spatial relationships for geovisualization","abstract":"The purpose of this study was to develop a robust, fast and markerless mobile augmented reality method for registration, geovisualization and interaction in uncontrolled outdoor environments. We propose a lightweight deep-learning-based object detection approach for mobile or embedded devices; the vision-based detection results of this approach are combined with spatial relationships by means of the host device's built-in Global Positioning System receiver, Inertial Measurement Unit and magnetometer. Virtual objects generated based on geospatial information are precisely registered in the real world, and an interaction method based on touch gestures is implemented. The entire method is independent of the network to ensure robustness to poor signal conditions. A prototype system was developed and tested on the Wuhan University campus to evaluate the method and validate its results. The findings demonstrate that our method achieves a high detection accuracy, stable geovisualization results and interaction.","keywords_author":["Deep learning","Geovisualization","Inertial Measurement Unit","Object detection","Outdoor augmented reality","geovisualization","outdoor augmented reality","deep learning","object detection","Inertial Measurement Unit"],"keywords_other":["FEATURES","WIDE","INFORMATION","NETWORKS","VISUALIZATION","Inertial measurement unit","Global positioning system receivers","Geo-spatial informations","Geo visualizations","Outdoor augmented reality","RECOGNITION","Mobile augmented reality","Spatial relationships","REGISTRATION","Vision-based detection","CONSTRUCTION","DECISION-SUPPORT","TRACKING"],"max_cite":0.0,"pub_year":2017.0,"sources":"['ieee', 'scp', 'wos']","rawkeys":["global positioning system receivers","outdoor augmented reality","visualization","decision-support","features","construction","object detection","registration","spatial relationships","inertial measurement unit","vision-based detection","mobile augmented reality","recognition","deep learning","networks","wide","geo-spatial informations","geo visualizations","geovisualization","tracking","information"],"tags":["global positioning system receivers","outdoor augmented reality","visualization","features","machine learning","construction","object detection","registration","spatial relationships","inertial measurement unit","vision-based detection","mobile augmented reality","recognition","networks","wide","geo-spatial informations","geo visualizations","geovisualization","tracking","information","decision supports"]},{"p_id":10887,"title":"Large scale deep learning for computer aided detection of mammographic lesions","abstract":"Recent advances in machine learning yielded new techniques to train deep neural networks, which resulted in highly successful applications in many pattern recognition tasks such as object detection and speech recognition. In this paper we provide a head-to-head comparison between a state-of-the art in mammography CAD system, relying on a manually designed feature set and a Convolutional Neural Network (CNN), aiming for a system that can ultimately read mammograms independently. Both systems are trained on a large data set of around 45,000 images and results show the CNN outperforms the traditional CAD system at low sensitivity and performs comparable at high sensitivity. We subsequently investigate to what extent features such as location and patient information and commonly used manual features can still complement the network and see improvements at high specificity over the CNN especially with location and context features, which contain information not available to the CNN. Additionally, a reader study was performed, where the network was compared to certified screening radiologists on a patch level and we found no significant difference between the network and the readers. (C) 2016 Elsevier B.V. All rights reserved.","keywords_author":["Breast cancer","Computer aided detection","Convolutional neural networks","Deep learning","Machine learning","Mammography","Computer aided detection","Mammography","Deep learning","Machine learning","Breast cancer","Convolutional neural networks"],"keywords_other":["Sensitivity and Specificity","Patient information","Breast Cancer","Humans","Mammography","Machine Learning","MASSES","IMAGES","Deep learning","High specificity","DIAGNOSIS","DIGITAL MAMMOGRAMS","Deep neural networks","Computer aided detection","TISSUE","Breast Neoplasms","Breast","Radiographic Image Interpretation, Computer-Assisted","RECOGNITION","High sensitivity","GRADIENT","Neural Networks (Computer)","CLASSIFICATION","NEURAL-NETWORKS","SEGMENTATION","Convolutional neural network"],"max_cite":55.0,"pub_year":2017.0,"sources":"['ieee', 'scp', 'wos']","rawkeys":["breast neoplasms","breast","gradient","classification","convolutional neural network","digital mammograms","radiographic image interpretation","masses","neural-networks","segmentation","machine learning","patient information","high specificity","sensitivity and specificity","diagnosis","neural networks (computer)","convolutional neural networks","images","recognition","deep learning","high sensitivity","computer aided detection","humans","mammography","breast cancer","tissue","computer-assisted","deep neural networks"],"tags":["breast neoplasms","breast","gradient","classification","convolutional neural network","digital mammograms","radiographic image interpretation","segmentation","computer-aided diagnosis","machine learning","patient information","high specificity","sensitivity and specificity","diagnosis","recognition","images","high sensitivity","neural networks","multiagent systems","humans","mammography","breast cancer","tissue","computer-assisted"]},{"p_id":35471,"title":"Asymptotic efficiency of kernel support vector machines (SVM)","abstract":"The paper analyzes the asymptotic properties of Vapnik's SVM-estimates of a regression function as the size of the training sample tends to infinity. The estimation problem is considered as infinite-dimensional minimization of a regularized empirical risk functional in a reproducing kernel Hilbert space. The rate of convergence of the risk functional on SVM-estimates to its minimum value is established. The sufficient conditions for the uniform convergence of SVM-estimates to a true regression function with unit probability are given. \u00a9 2009 Springer Science+Business Media, Inc.","keywords_author":["Consistency","Estimation of dependences","Ill-posed problems","Kernel estimate","Machine learning","Rate of convergence","Recognition","Regularization","Support vector machine (SVM)"],"keywords_other":["Rate of convergence","Recognition","Support vector machine (SVM)","Machine learning","Regularization","Kernel estimate","Consistency","Ill-posed problems"],"max_cite":2.0,"pub_year":2009.0,"sources":"['scp']","rawkeys":["recognition","estimation of dependences","kernel estimate","consistency","machine learning","support vector machine (svm)","ill-posed problems","rate of convergence","regularization"],"tags":["recognition","estimation of dependences","machine learning","consistency","ill posed problem","rate of convergence","regularization","kernel estimation"]},{"p_id":10906,"title":"Efficient deep learning model for mitosis detection using breast histopathology images","abstract":"Mitosis detection is one of the critical factors of cancer prognosis, carrying significant diagnostic information required for breast cancer grading. It provides vital clues to estimate the aggressiveness and the proliferation rate of the tumour. The manual mitosis quantification from whole slide images is a very labor-intensive and challenging task. The aim of this study is to propose a supervised model to detect mitosis signature from breast histopathology WSI images. The model has been designed using deep learning architecture with handcrafted features. We used handcrafted features issued from previous medical challenges MITOS @ ICPR 2012, AMIDA-13 and projects (MICO ANR TecSan) expertise. The deep learning architecture mainly consists of five convolution layers, four max-pooling layers, four rectified linear units (ReLU), and two fully connected layers. ReLU has been used after each convolution layer as an activation function. Dropout layer has been included after first fully connected layer to avoid overfitting. Handcrafted features mainly consist of morphological, textural and intensity features. The proposed architecture has shown to have an improved 92% precision, 88% recall and 90% F-score. Prospectively, the proposed model will be very beneficial in routine exam, providing pathologists with efficient and - as we will prove - effective second opinion for breast cancer grading from whole slide images. Last but not the least, this model could lead junior and senior pathologists, as medical researchers, to a superior understanding and evaluation of breast cancer stage and genesis.","keywords_author":["Breast cancer","Mitosis","Deep neural network","Handcrafted features","Convolution","Hematoxylin and eosin"],"keywords_other":["RECOGNITION","SEGMENTATION","NUCLEI","CANCER HISTOLOGY IMAGES"],"max_cite":0.0,"pub_year":2017.0,"sources":"['ieee', 'wos']","rawkeys":["recognition","deep neural network","handcrafted features","segmentation","convolution","mitosis","hematoxylin and eosin","cancer histology images","nuclei","breast cancer"],"tags":["recognition","segmentation","convolution","mitosis","hematoxylin and eosin","cancer histology images","convolutional neural network","hand-crafted features","nuclei","breast cancer"]},{"p_id":10920,"title":"Deep learning of support vector machines with class probability output networks","abstract":"Deep learning methods endeavor to learn features automatically at multiple levels and allow systems to learn complex functions mapping from the input space to the output space for the given data. The ability to learn powerful features automatically is increasingly important as the volume of data and range of applications of machine learning methods continues to grow. This paper proposes a new deep architecture that uses support vector machines (SVMs) with class probability output networks (CPONs) to provide better generalization power for pattern classification problems. As a result, deep features are extracted without additional feature engineering steps, using multiple layers of the SVM classifiers with CPONs. The proposed structure closely approaches the ideal Bayes classifier as the number of layers increases. Using a simulation of classification problems, the effectiveness of the proposed method is demonstrated. (C) 2014 Elsevier Ltd. All rights reserved.","keywords_author":["Class probability output network","Deep learning","Support vector machine","Uncertainty measure","Deep learning","Support vector machine","Class probability output network","Uncertainty measure"],"keywords_other":["Deep learning","Output network","Support Vector Machines","Bayes Theorem","Pattern Recognition, Automated","ALGORITHM","RECOGNITION","Feature engineerings","Machine learning methods","Uncertainty measures","Class probabilities","Support vector machine (SVMs)","Pattern classification problems","STACKED GENERALIZATION"],"max_cite":21.0,"pub_year":2015.0,"sources":"['ieee', 'scp', 'wos']","rawkeys":["machine learning methods","algorithm","class probabilities","recognition","automated","deep learning","pattern classification problems","uncertainty measures","class probability output network","output network","stacked generalization","feature engineerings","uncertainty measure","support vector machines","support vector machine","pattern recognition","bayes theorem","support vector machine (svms)"],"tags":["machine learning methods","class probabilities","recognition","automated","pattern classification problems","uncertainty measures","machine learning","class probability output network","output network","stacked generalization","feature engineerings","algorithms","pattern recognition","bayes theorem"]},{"p_id":10930,"title":"Transfer learning and deep convolutional neural networks for safety guardrail detection in 2D images","abstract":"Safety has been a concern for the construction industry for decades. Unsafe conditions and behaviors are considered as the major causes of construction accidents. The current safety inspection of conditions and behaviors heavily rely on human efforts which are limited onsite. To improve the safety performance of the industry, a more efficient approach to identify the unsafe conditions on site is required to supplement the current manual inspection practice. A promising way to supplement the current manual safety inspection is automated and intelligent monitoring\/inspection through information and sensing technologies, including localization techniques, environment monitoring, image processing and etc. To assess the potential benefits of contemporary technologies for onsite safety inspection, the authors focused on real-time guardrail detection, as unprotected edges are the ones cause for workers falling from heights.","keywords_author":["Computer vision","Construction safety","Guardrail detection","Convolutional neural networks","Transfer learning","VGG-16"],"keywords_other":["MANAGEMENT","SITES","CONSTRUCTION SAFETY","POSTURE","SYSTEM","RESOURCES","RECOGNITION","WORKERS","OBJECT DETECTION","TRACKING"],"max_cite":0.0,"pub_year":2018.0,"sources":"['wos']","rawkeys":["construction safety","recognition","convolutional neural networks","posture","workers","transfer learning","vgg-16","system","tracking","guardrail detection","sites","object detection","management","computer vision","resources"],"tags":["construction safety","recognition","posture","workers","transfer learning","vgg-16","system","tracking","guardrail detection","sites","object detection","convolutional neural network","management","computer vision","resources"]},{"p_id":84664,"title":"HR practices, context and knowledge transfer in M&A","abstract":"This paper presents a theoretical framework of the role of HR practices in overcoming the differences and conflicts between the source and recipient of knowledge during the exploitation of synergies in the post-merger integration (PMI) process. The presented analysis of an international case study explores the crucial yet neglected relationship in M&A between the context and process variables in the PMI stage. The results of the analysis suggest a model that includes the unique characteristics of and relationship between source and recipient, knowledge integration mechanisms and the role of HR practices during the knowledge transfer process. Directions for future research are finally suggested.","keywords_author":["HR practices","knowledge","post-acquisition","transfer"],"keywords_other":["MERGERS","HUMAN-RESOURCES","HORIZONTAL ACQUISITIONS","MANAGING CULTURE","PERFORMANCE","ABSORPTIVE-CAPACITY","CULTURAL INTEGRATION","CROSS-BORDER ACQUISITIONS","JOINT VENTURES","CAUSAL AMBIGUITY"],"max_cite":2.0,"pub_year":2016.0,"sources":"['wos']","rawkeys":["post-acquisition","knowledge","transfer","cultural integration","absorptive-capacity","performance","managing culture","cross-border acquisitions","hr practices","horizontal acquisitions","human-resources","joint ventures","causal ambiguity","mergers"],"tags":["knowledge","cultural integration","absorptive-capacity","performance","postacquisition","recognition","managing culture","human resources","cross-border acquisitions","hr practices","horizontal acquisitions","joint ventures","causal ambiguity","mergers"]},{"p_id":10937,"title":"Unsupervised obstacle detection in driving environments using deep-learning-based stereovision","abstract":"A vision-based obstacle detection system is a key enabler for the development of autonomous robots and vehicles and intelligent transportation systems. This paper addresses the problem of urban scene monitoring and tracking of obstacles based on unsupervised, deep-learning approaches. Here, we design an innovative hybrid encoder that integrates deep Boltzmann machines (DBM) and auto-encoders (AE). This hybrid auto-encode (HAE) model combines the greedy learning features of DBM with the dimensionality reduction capacity of AE to accurately and reliably detect the presence of obstacles. We combine the proposed hybrid model with the one-class support vector machines (OCSVM) to visually monitor an urban scene. We also propose an efficient approach to estimating obstacles location and track their positions via scene densities. Specifically, we address obstacle detection as an anomaly detection problem. If an obstacle is detected by the OCSVM algorithm, then localization and tracking algorithm is executed. We validated the effectiveness of our approach by using experimental data from two publicly available dataset, the Malaga stereovision urban dataset (MSVUD) and the Daimler urban segmentation dataset (DUSD). Results show the capacity of the proposed approach to reliably detect obstacles. (C) 2017 Elsevier B.V. All rights reserved.","keywords_author":["Autoencoder","DBM","Deep learning","Monitoring","OCSVM","Stereovision","Deep learning","DBM","Autoencoder","OCSVM","Monitoring","Stereovision"],"keywords_other":["Dimensionality reduction","Intelligent transportation systems","BELIEF NETWORK","LIDAR","ALGORITHM","Deep boltzmann machines","Localization and tracking","RECOGNITION","One-class support vector machines (OCSVM)","Auto encoders","Obstacle detection system","OCSVM","SUPPORT"],"max_cite":1.0,"pub_year":2018.0,"sources":"['ieee', 'scp', 'wos']","rawkeys":["stereovision","algorithm","localization and tracking","recognition","support","deep learning","one-class support vector machines (ocsvm)","autoencoder","auto encoders","obstacle detection system","dbm","intelligent transportation systems","deep boltzmann machines","lidar","ocsvm","belief network","dimensionality reduction","monitoring"],"tags":["stereovision","localization and tracking","recognition","support","auto encoders","machine learning","obstacle detection system","intelligent transportation systems","lidar","deep boltzmann machines","one-class support vector machine","belief networks","algorithms","dimensionality reduction","monitoring"]},{"p_id":109241,"title":"Object Detection Based on Fast\/Faster RCNN Employing Fully Convolutional Architectures","abstract":"Modern object detectors always include two major parts: a feature extractor and a feature classifier as same as traditional object detectors. The deeper and wider convolutional architectures are adopted as the feature extractor at present. However, many notable object detection systems such as Fast\/Faster RCNN only consider simple fully connected layers as the feature classifier. In this paper, we declare that it is beneficial for the detection performance to elaboratively design deep convolutional networks (ConvNets) of various depths for feature classification, especially using the fully convolutional architectures. In addition, this paper also demonstrates how to employ the fully convolutional architectures in the Fast\/Faster RCNN. Experimental results show that a classifier based on convolutional layer is more effective for object detection than that based on fully connected layer and that the better detection performance can be achieved by employing deeper ConvNets as the feature classifier.","keywords_author":null,"keywords_other":["NEURAL-NETWORKS","RECOGNITION","FEATURES"],"max_cite":0.0,"pub_year":2018.0,"sources":"['wos']","rawkeys":["neural-networks","recognition","features"],"tags":["neural networks","recognition","features"]},{"p_id":43705,"title":"DNN-Based Score Calibration with Multitask Learning for Noise Robust Speaker Verification","abstract":"\u00a9 2014 IEEE. This paper proposes and investigates several deep neural network (DNN) based score compensation, transformation, and calibration algorithms for enhancing the noise robustness of i-vector speaker verification systems. Unlike conventional calibration methods where the required score shift is a linear function of SNR or log-duration, the DNN approach learns the complex relationship between the score shifts and the combination of i-vector pairs and uncalibrated scores. Furthermore, with the flexibility of DNNs, it is possible to explicitly train a DNN to recover the clean scores without having to estimate the score shifts. To alleviate the overfitting problem, multitask learning is applied to incorporate auxiliary information such as SNRs and speaker ID of training utterances into the DNN. Experiments on NIST 2012 SRE show that score calibration derived from multitask DNNs can improve the performance of the conventional score-shift approch significantly, especially under noisy conditions.","keywords_author":["Deep learning","multi-task learning","noise robustness","score calibration","speaker verification","Deep learning","speaker verification","score calibration","multi-task learning","noise robustness"],"keywords_other":["Calibration algorithm","Multitask learning","Speaker verification","NEURAL-NETWORKS","RECOGNITION","Speaker verification system","Noise robustness","Auxiliary information","Complex relationships","Over fitting problem"],"max_cite":0.0,"pub_year":2018.0,"sources":"['ieee', 'scp', 'wos']","rawkeys":["neural-networks","over fitting problem","speaker verification system","multi-task learning","auxiliary information","recognition","deep learning","calibration algorithm","noise robustness","speaker verification","score calibration","multitask learning","complex relationships"],"tags":["over fitting problem","speaker verification system","auxiliary information","recognition","neural networks","machine learning","calibration algorithm","noise robustness","speaker verification","score calibration","multitask learning","complex relationships"]},{"p_id":43707,"title":"3D multi-scale FCN with random modality voxel dropout learning for Intervertebral Disc Localization and Segmentation from Multi-modality MR Images","abstract":"\u00a9 2018 Elsevier B.V. Intervertebral discs (IVDs) are small joints that lie between adjacent vertebrae. The localization and segmentation of IVDs are important for spine disease diagnosis and measurement quantification. However, manual annotation is time-consuming and error-prone with limited reproducibility, particularly for volumetric data. In this work, our goal is to develop an automatic and accurate method based on fully convolutional networks (FCN) for the localization and segmentation of IVDs from multi-modality 3D MR data. Compared with single modality data, multi-modality MR images provide complementary contextual information, which contributes to better recognition performance. However, how to effectively integrate such multi-modality information to generate accurate segmentation results remains to be further explored. In this paper, we present a novel multi-scale and modality dropout learning framework to locate and segment IVDs from four-modality MR images. First, we design a 3D multi-scale context fully convolutional network, which processes the input data in multiple scales of context and then merges the high-level features to enhance the representation capability of the network for handling the scale variation of anatomical structures. Second, to harness the complementary information from different modalities, we present a random modality voxel dropout strategy which alleviates the co-adaption issue and increases the discriminative capability of the network. Our method achieved the 1st place in the MICCAI challenge on automatic localization and segmentation of IVDs from multi-modality MR images, with a mean segmentation Dice coefficient of 91.2% and a mean localization error of 0.62 mm. We further conduct extensive experiments on the extended dataset to validate our method. We demonstrate that the proposed modality dropout strategy with multi-modality images as contextual information improved the segmentation accuracy significantly. Furthermore, experiments conducted on extended data collected from two different time points demonstrate the efficacy of our method on tracking the morphological changes in a longitudinal study.","keywords_author":["Deep learning","Dropout","Intervertebral discs","Localization","Magnetic resonance imaging","Multi-modality","Segmentation","Multi-modality","Magnetic resonance imaging","Intervertebral discs","Localization","Segmentation","Deep learning","Dropout"],"keywords_other":["LOW-BACK-PAIN","Convolutional networks","FEATURES","Contextual information","AUTOMATIC SEGMENTATION","DEGENERATION","VIEWS","Anatomical structures","MODEL","Multi modality","Localization","RECOGNITION","Intervertebral discs","Automatic localization","Dropout","SPINAL MRI","BED REST","CONVOLUTIONAL NEURAL-NETWORKS"],"max_cite":0.0,"pub_year":2018.0,"sources":"['ieee', 'scp', 'wos']","rawkeys":["localization","contextual information","convolutional neural-networks","low-back-pain","automatic segmentation","views","segmentation","features","intervertebral discs","degeneration","convolutional networks","bed rest","multi-modality","recognition","spinal mri","deep learning","automatic localization","dropout","model","anatomical structures","magnetic resonance imaging","multi modality"],"tags":["localization","contextual information","convolutional neural network","low-back-pain","automatic segmentation","views","segmentation","features","intervertebral discs","degeneration","machine learning","bed rest","recognition","spinal mri","multi-modal","automatic localization","dropout","model","anatomical structures","magnetic resonance imaging"]},{"p_id":10942,"title":"Deep learning-based subdivision approach for large scale macromolecules structure recovery from electron cryo tomograms","abstract":"Motivation: Cellular Electron CryoTomography (CECT) enables 3D visualization of cellular organization at near-native state and in sub-molecular resolution, making it a powerful tool for analyzing structures of macromolecular complexes and their spatial organizations inside single cells. However, high degree of structural complexity together with practical imaging limitations makes the systematic de novo discovery of structures within cells challenging. It would likely require averaging and classifying millions of subtomograms potentially containing hundreds of highly heterogeneous structural classes. Although it is no longer difficult to acquire CECT data containing such amount of subtomograms due to advances in data acquisition automation, existing computational approaches have very limited scalability or discrimination ability, making them incapable of processing such amount of data.","keywords_author":null,"keywords_other":["LOCALIZATION","MICROSCOPE TOMOGRAPHY","CRYOELECTRON TOMOGRAPHY","CELLS","CLASSIFICATION","RECOGNITION","SUBTOMOGRAMS","BIOLOGY IN-SITU","RECONSTRUCTIONS","PROTEIN COMPLEXES"],"max_cite":1.0,"pub_year":2017.0,"sources":"['wos']","rawkeys":["biology in-situ","cells","recognition","localization","reconstructions","microscope tomography","protein complexes","classification","subtomograms","cryoelectron tomography"],"tags":["biology in-situ","cells","recognition","localization","microscope tomography","protein complexes","reconstruction","classification","subtomograms","cryoelectron tomography"]},{"p_id":10943,"title":"THE DEEP LEARNING VISION FOR HETEROGENEOUS NETWORK TRAFFIC CONTROL: PROPOSAL, CHALLENGES, AND FUTURE PERSPECTIVE","abstract":"Recently, deep learning, an emerging machine learning technique, is garnering a lot of research attention in several computer science areas. However, to the best of our knowledge, its application to improve heterogeneous network traffic control (which is an important and challenging area by its own merit) has yet to appear because of the difficult challenge in characterizing the appropriate input and output patterns for a deep learning system to correctly reflect the highly dynamic nature of large-scale heterogeneous networks. In this vein, in this article, we propose appropriate input and output characterizations of heterogeneous network traffic and propose a supervised deep neural network system. We describe how our proposed system works and how it differs from traditional neural networks. Also, preliminary results are reported that demonstrate the encouraging performance of our proposed deep learning system compared to a benchmark routing strategy (Open Shortest Path First (OSPF)) in terms of significantly better signaling overhead, throughput, and delay.","keywords_author":null,"keywords_other":["RECOGNITION","MACHINES"],"max_cite":13.0,"pub_year":2017.0,"sources":"['ieee', 'wos']","rawkeys":["recognition","machines"],"tags":["machine","recognition"]},{"p_id":109253,"title":"CIRCULAR TRAFFIC SIGN CLASSIFICATION USING HOGBASED RING PARTITIONED MATCHING","abstract":"This paper presents a technique to classify the circular traffic sign based-on HOG (histogram of oriented gradients) and a ring partitioned matching. The method divides an image into several ring areas, and calculates the HOG feature on each ring area. In the matching process, the weight is assigned to each ring for calculating the distance of HOG feature between tested image and reference image. The experimental results show that the proposed algorithm achieves a high classification rate of 97.8%, without the need of many prepared sample images. The results also show that the best values of the number of orientation bins and the cell size of the HOG parameters are 5 and 10 x 10 pixels respectively.","keywords_author":["HOG","traffic sign classification","ring partitioned","template matching"],"keywords_other":["CAMERA","RECOGNITION","VISION","SYSTEM"],"max_cite":0.0,"pub_year":2017.0,"sources":"['wos']","rawkeys":["traffic sign classification","recognition","ring partitioned","system","vision","camera","hog","template matching"],"tags":["traffic sign classification","recognition","histogram of oriented gradients","system","vision","cameras","template matching","ring partition"]},{"p_id":10975,"title":"Multi-sensor data integration using deep learning for characterization of defects in steel elements","abstract":"Nowadays, there is a strong demand for inspection systems integrating both high sensitivity under various testing conditions and advanced processing allowing automatic identification of the examined object state and detection of threats. This paper presents the possibility of utilization of a magnetic multi-sensor matrix transducer for characterization of defected areas in steel elements and a deep learning based algorithm for integration of data and final identification of the object state. The transducer allows sensing of a magnetic vector in a single location in different directions. Thus, it enables detecting and characterizing any material changes that affect magnetic properties regardless of their orientation in reference to the scanning direction. To assess the general application capability of the system, steel elements with rectangular-shaped artificial defects were used. First, a database was constructed considering numerical and measurements results. A finite element method was used to run a simulation process and provide transducer signal patterns for different defect arrangements. Next, the algorithm integrating responses of the transducer collected in a single position was applied, and a convolutional neural network was used for implementation of the material state evaluation model. Then, validation of the obtained model was carried out. In this paper, the procedure for updating the evaluated local state, referring to the neighboring area results, is presented. Finally, the results and future perspective are discussed.","keywords_author":["Convolutional neural network","Data aggregation","Deep learning","Large data processing","Magnetic nondestructive testing","Matrix transducer","Multi-sensor data integration","magnetic nondestructive testing","matrix transducer","multi-sensor data integration","large data processing","data aggregation","deep learning","convolutional neural network"],"keywords_other":["Learning-based algorithms","Automatic identification","Multi-sensor data integration","Data aggregation","MAGNETIC-FLUX LEAKAGE","CONVOLUTIONAL NEURAL-NETWORKS","RECOGNITION","SIMULATION","Convolutional neural network","Magnetic non-destructive testing","General applications","Future perspectives"],"max_cite":2.0,"pub_year":2018.0,"sources":"['ieee', 'scp', 'wos']","rawkeys":["future perspectives","recognition","multi-sensor data integration","learning-based algorithms","deep learning","large data processing","simulation","magnetic nondestructive testing","magnetic non-destructive testing","matrix transducer","automatic identification","convolutional neural-networks","convolutional neural network","general applications","magnetic-flux leakage","data aggregation"],"tags":["future perspectives","recognition","multi-sensor data integration","learning-based algorithms","simulation","large data processing","machine learning","magnetic nondestructive testing","magnetic flux leakage","automatic identification","convolutional neural network","general applications","matrix transducer","data aggregation"]},{"p_id":27369,"title":"Temperature based Restricted Boltzmann Machines","abstract":"Restricted Boltzmann machines (RBMs), which apply graphical models to learning probability distribution over a set of inputs, have attracted much attention recently since being proposed as building blocks of multi-layer learning systems called deep belief networks (DBNs). Note that temperature is a key factor of the Boltzmann distribution that RBMs originate from. However, none of existing schemes have considered the impact of temperature in the graphical model of DBNs. In this work, we propose temperature based restricted Boltzmann machines (TRBMs) which reveals that temperature is an essential parameter controlling the selectivity of the firing neurons in the hidden layers. We theoretically prove that the effect of temperature can be adjusted by setting the parameter of the sharpness of the logistic function in the proposed TRBMs. The performance of RBMs can be improved by adjusting the temperature parameter of TRBMs. This work provides a comprehensive insights into the deep belief networks and deep learning architectures from a physical point of view.","keywords_author":null,"keywords_other":["CONTRASTIVE DIVERGENCE","NEURAL-NETWORKS","RECOGNITION","MODELS","MAXWELL-BOLTZMANN","STATISTICS"],"max_cite":6.0,"pub_year":2016.0,"sources":"['ieee', 'scp', 'wos']","rawkeys":["maxwell-boltzmann","neural-networks","statistics","recognition","contrastive divergence","models"],"tags":["maxwell-boltzmann","statistics","recognition","model","contrastive divergence","neural networks"]},{"p_id":10993,"title":"Nonlinear metric learning with deep independent subspace analysis network for face verification","abstract":"Face verification is the task of determining whether two given face images represent the same person or not. It is a very challenging task, as the face images, captured in the uncontrolled environments, may have large variations in illumination, expression, pose, background, etc. The crucial problem is how to compute the similarity of two face images. Metric learning has provided a viable solution to this problem. Until now, many metric learning algorithms have been propos'ed, but they are usually limited to learning a linear transformation In this paper, we propose a nonlinear metric learning method, which learns an explicit mapping from the original space to an optimal subspace using deep Independent Subspace Analysis (ISA) network. Compared to the linear or kernel based metric learning methods, the proposed deep ISA network is a deep and local learning architecture, and therefore exhibits more powerful ability to learn the nature of highly variable dataset. We evaluate our method on the Labeled Faces in the Wild dataset, and results show superior performance over some state-of-the-art methods.","keywords_author":["Deep learning architecture","Face verification","Independent subspace analysis","Metric learning","metric learning","independent subspace analysis","deep learning architecture","face verification"],"keywords_other":["Deep learning","Independent subspace analysis","State-of-the-art methods","Viable solutions","Metric learning","Optimal subspace","RECOGNITION","DIMENSIONALITY REDUCTION","Face Verification","PATTERNS","Local learning"],"max_cite":0.0,"pub_year":2013.0,"sources":"['ieee', 'scp', 'wos']","rawkeys":["state-of-the-art methods","optimal subspace","recognition","deep learning architecture","deep learning","independent subspace analysis","patterns","face verification","viable solutions","local learning","metric learning","dimensionality reduction"],"tags":["state-of-the-art methods","optimal subspace","recognition","independent subspace analysis","machine learning","deep learning architectures","patterns","face verification","viable solutions","local learning","metric learning","dimensionality reduction"]},{"p_id":101108,"title":"Automatic Building Segmentation of Aerial Imagery UsingMulti-Constraint Fully Convolutional Networks","abstract":"Automatic building segmentation from aerial imagery is an important and challenging task because of the variety of backgrounds, building textures and imaging conditions. Currently, research using variant types of fully convolutional networks (FCNs) has largely improved the performance of this task. However, pursuing more accurate segmentation results is still critical for further applications such as automatic mapping. In this study, a multi-constraint fully convolutional network (MC-FCN) model is proposed to perform end-to-end building segmentation. Our MC-FCN model consists of a bottom-up\/top-down fully convolutional architecture and multi-constraints that are computed between the binary cross entropy of prediction and the corresponding ground truth. Since more constraints are applied to optimize the parameters of the intermediate layers, the multi-scale feature representation of the model is further enhanced, and hence higher performance can be achieved. The experiments on a very-high-resolution aerial image dataset covering 18 km(2) and more than 17,000 buildings indicate that our method performs well in the building segmentation task. The proposed MC-FCN method significantly outperforms the classic FCN method and the adaptive boosting method using features extracted by the histogram of oriented gradients. Compared with the state-of-the-art U-Net model, MC-FCN gains 3.2% (0.833 vs. 0.807) and 2.2% (0.893 vs. 0.874) relative improvements of Jaccard index and kappa coefficient with the cost of only 1.8% increment of the model-training time. In addition, the sensitivity analysis demonstrates that constraints at different positions have inconsistent impact on the performance of the MC-FCN.","keywords_author":["aerial imagery","building detection","convolutional neural network","multi-constraint fully convolutional networks","feature pyramid"],"keywords_other":["INFORMATION","CLASSIFICATION","IDENTIFICATION","NEURAL-NETWORKS","RECOGNITION","SCALE"],"max_cite":0.0,"pub_year":2018.0,"sources":"['ieee', 'wos']","rawkeys":["neural-networks","feature pyramid","aerial imagery","identification","multi-constraint fully convolutional networks","recognition","building detection","classification","convolutional neural network","information","scale"],"tags":["feature pyramid","aerial imagery","identification","multi-constraint fully convolutional networks","recognition","neural networks","building detection","classification","convolutional neural network","information","scale"]},{"p_id":10998,"title":"Deep Learning with a Spatiotemporal Descriptor of Appearance and Motion Estimation for Video Anomaly Detection","abstract":"The automatic detection and recognition of anomalous events in crowded and complex scenes on video are the research objectives of this paper. The main challenge in this system is to create models for detecting such events due to their changeability and the territory of the context of the scenes. Due to these challenges, this paper proposed a novel HOME FAST (Histogram of Orientation, Magnitude, and Entropy with Fast Accelerated Segment Test) spatiotemporal feature extraction approach based on optical flow information to capture anomalies. This descriptor performs the video analysis within the smart surveillance domain and detects anomalies. In deep learning, the training step learns all the normal patterns from the high-level and low-level information. The events are described in testing and, if they differ from the normal pattern, are considered as anomalous. The overall proposed system robustly identifies both local and global abnormal events from complex scenes and solves the problem of detection under various transformations with respect to the state-of-the-art approaches. The performance assessment of the simulation outcome validated that the projected model could handle different anomalous events in a crowded scene and automatically recognize anomalous events with success.","keywords_author":["anomaly detection","appearance","deep learning","motion estimation","spatiotemporal"],"keywords_other":["LOCALIZATION","SURVEILLANCE","STREAMS","RECOGNITION","BEHAVIOR DETECTION","CROWDED SCENES","EVENT DETECTION"],"max_cite":0.0,"pub_year":2018.0,"sources":"['wos']","rawkeys":["crowded scenes","recognition","localization","anomaly detection","deep learning","spatiotemporal","appearance","motion estimation","streams","surveillance","event detection","behavior detection"],"tags":["spatio temporal","crowded scenes","recognition","localization","anomaly detection","machine learning","appearance","motion estimation","surveillance","event detection","streaming","behavior detection"]},{"p_id":11009,"title":"Speedup of deep learning ensembles for semantic segmentation using a model compression technique","abstract":"Deep Learning (DL) has been proven as a powerful recognition method as evidenced by its success in recent computer vision competitions. The most accurate results have been obtained by ensembles of DL models that pool their results. However, such ensembles are computationally costly, making them inapplicable to real-time applications. In this paper, we apply model compression techniques to the problem of semantic segmentation, which is one of the most challenging problems in computer vision. Our results suggest that compressed models can approach the accuracy of full ensembles on this task, combining the diverse strengths of networks of very different architectures, while maintaining real-time performance. (C) 2017 Published by Elsevier Inc.","keywords_author":["Semantic segmentation","Model compression","Transfer learning","Real-time application"],"keywords_other":["RECOGNITION"],"max_cite":0.0,"pub_year":2017.0,"sources":"['ieee', 'wos']","rawkeys":["model compression","recognition","transfer learning","real-time application","semantic segmentation"],"tags":["model compression","recognition","transfer learning","real-time application","semantic segmentation"]},{"p_id":11014,"title":"Appearance based pedestrians' head pose and body orientation estimation using deep learning","abstract":"Pedestrian orientation recognition, including head and body directions, is a demanding task in human activity-recognition scenarios. While moving in one direction, a pedestrian may be focusing his visual attention in another direction. The analysis of such orientation estimation via computer-vision applications is sometimes desirable for automated pedestrian intention and behavior analysis. This paper highlights appearance-based pedestrian head-pose and full-body orientation prediction by employing a deep-learning mechanism. A supervised deep convolutional neural-network model is presented as a deep-learning building block for classification. Two separate datasets are prepared for head-pose and full-body orientation estimation. The proposed model is subsequently trained separately on the two prepared datasets with eight orientation bins. Testing of the proposed model is performed with publicly available datasets, as well as self-taken real-time image sequences. The experiments reveal mean accuracies of 0.91 for head-pose estimation and 0.92 for full-body orientation estimation. The performance results illustrate that the proposed approach effectively classifies head-poses and body orientations simultaneously in different setups. The comparison with existing state-of-the-art approaches demonstrates the effectiveness of the presented approach. (C) 2017 Elsevier B.V. All rights reserved.","keywords_author":["Convolutional neural network (CNN)","Full-body orientation","Head-pose","Pedestrians","Proposed training dataset"],"keywords_other":["TRACKING","RECOGNITION","IMAGES","FEATURES"],"max_cite":4.0,"pub_year":2017.0,"sources":"['ieee', 'wos']","rawkeys":["recognition","images","features","proposed training dataset","tracking","convolutional neural network (cnn)","full-body orientation","pedestrians","head-pose"],"tags":["recognition","images","features","pedestrians","proposed training dataset","tracking","full-body orientation","convolutional neural network","head pose"]},{"p_id":68362,"title":"Joint gender classification and age estimation by nearly orthogonalizing their semantic spaces","abstract":"In human face-based biometrics, gender classification and age estimation are two important research topics. Although a variety of approaches have been proposed to handle them, just a few of them are solved jointly, even so, these joint methods do not specifically concern the semantic difference between human gender and age, which is intuitively helpful for joint learning, consequently leaving us a room of further improving their performance. To this end, in this work we firstly propose a general learning framework for jointly estimating human gender and age by attempting to formulate such semantic relationships as a form of near orthogonality regularization and then to incorporate it into the objective of the joint learning framework. In order to evaluate the effectiveness of the proposed framework, we exemplify it by respectively taking the widely used binary-class SVM for gender classification, and two threshold-based ordinal regression methods (i.e., the discriminant learning for ordinal regression and support vector ordinal regression) for age estimation, and crucially coupling both through the proposed semantic formulation. Moreover, we construct its nonlinear counterpart by deriving a representer theorem for the joint learning strategy. Finally, extensive experiments on four aging datasets, i.e., FG-NET, Morph Album I, Album II and Images of Groups demonstrate the effectiveness and superiority of the proposed strategy. (C) 2017 Elsevier B.V. All rights reserved.","keywords_author":["Gender classification","Age estimation","Nearly orthogonal semantic spaces","Support vector ordinal regression","Discriminant learning for ordinal regression"],"keywords_other":["REGRESSION","PARTIAL LEAST-SQUARES","FACE","FEATURES","LOCAL CIRCULAR PATTERNS","MODEL","LARGE DATABASE","RECOGNITION","BINARY PATTERNS","ATTRIBUTES"],"max_cite":0.0,"pub_year":2018.0,"sources":"['ieee', 'wos']","rawkeys":["binary patterns","recognition","model","features","partial least-squares","discriminant learning for ordinal regression","attributes","age estimation","gender classification","support vector ordinal regression","local circular patterns","nearly orthogonal semantic spaces","face","regression","large database"],"tags":["binary patterns","recognition","model","features","partial least squares","discriminant learning for ordinal regression","attributes","age estimation","gender classification","support vector ordinal regression","local circular patterns","nearly orthogonal semantic spaces","face","regression","large database"]},{"p_id":35596,"title":"Random sampling for fast face sketch synthesis","abstract":"\u00a9 2017 Elsevier LtdExemplar-based face sketch synthesis plays an important role in both digital entertainment and law enforcement. It generally consists of two parts: neighbor selection and recognition weight representation. In this paper, we proposed a simple but effective method which employs offline random sampling instead of K-NN used in state-of-the-art methods. The proposed random sampling strategy reduces the time consuming for synthesis and has stronger scalability than state-of-the-art methods. In addition, we introduced locality constraint to model the distinct correlations between the test patch and random sampled patches. Extensive experiments on public face sketch databases demonstrate the superiority of the proposed method in comparison to state-of-the-art methods, in terms of both synthesis quality and time consumption. The proposed method could be extended to other heterogeneous face image transformation problems such as face hallucination. We release the source codes of our proposed methods and the evaluation metrics for future study online: http:\/\/www.ihitworld.com\/RSLCR.html.","keywords_author":["Face sketch synthesis","Locality constraint","Neighbor selection","Random sampling","Weight computation","Face sketch synthesis","Locality constraint","Neighbor selection","Random sampling","Weight computation"],"keywords_other":["Digital entertainment","Face sketch synthesis","State-of-the-art methods","Random sampling","IMAGE SUPERRESOLUTION","Locality constraint","Evaluation metrics","RETRIEVAL","RECOGNITION","ALGORITHMS","SPARSE REPRESENTATION","Neighbor selection","Face hallucination"],"max_cite":2.0,"pub_year":2018.0,"sources":"['scp', 'wos']","rawkeys":["state-of-the-art methods","recognition","neighbor selection","face sketch synthesis","sparse representation","evaluation metrics","digital entertainment","weight computation","retrieval","face hallucination","algorithms","random sampling","image superresolution","locality constraint"],"tags":["state-of-the-art methods","recognition","neighbor selection","face sketch synthesis","sparse representation","evaluation metrics","digital entertainment","weight computation","retrieval","face hallucination","algorithms","random sampling","locality constraint"]},{"p_id":43815,"title":"A closer look at batch size in mini-batch training of deep auto-encoders","abstract":"\u00a9 2017 IEEE. In deep learning community, gradient based methods are typically employed to train the proposed models. These methods generally operate in a mini-batch training manner wherein a small fraction of the training data is invoked to compute an approximative gradient. It is reported that models trained with large batch are prone to generalize worse than those trained with small batch. Several inspiring works are conducted to figure out the underlying reason of this phenomenon, but almost all of them focus on classification tasks. In this paper, we investigate the influence of batch size on regression task. More specifically, we tested the generalizability of deep auto-encoder trained with varying batch size and checked some well-known measures relating to model generalization. Our experimental results lead to three conclusions. First, there exist no obvious generalization gap in regression model such as auto-encoders. Second, with a same train loss as target, small batch generally lead to solutions closer to the starting point than large batch. Third, spectral norm of weight matrices is closely related to generalizability of the model, but different layers contribute variously to the generalization performance.","keywords_author":["batch size","Deep learning","generalization","spectral norm","stochastic gradient decent"],"keywords_other":["Model generalization","Spectral norms","Stochastic gradient","Classification tasks","Generalization performance","generalization","Batch sizes","Gradient-based method"],"max_cite":0.0,"pub_year":2018.0,"sources":"['scp']","rawkeys":["deep learning","generalization","classification tasks","batch sizes","spectral norm","model generalization","stochastic gradient","spectral norms","stochastic gradient decent","batch size","gradient-based method","generalization performance"],"tags":["recognition","machine learning","classification tasks","stochastic gradient","model generalization","spectral norms","stochastic gradient decent","batch size","gradient-based method","generalization performance"]},{"p_id":11051,"title":"Automatic Classification of Cancerous Tissue in Laserendomicroscopy Images of the Oral Cavity using Deep Learning","abstract":"Oral Squamous Cell Carcinoma (OSCC) is a common type of cancer of the oral epithelium. Despite their high impact on mortality, sufficient screening methods for early diagnosis of OSCC often lack accuracy and thus OSCCs are mostly diagnosed at a late stage. Early detection and accurate outline estimation of OSCCs would lead to a better curative outcome and a reduction in recurrence rates after surgical treatment. Confocal Laser Endomicroscopy (CLE) records sub-surface micro-anatomical images for in vivo cell structure analysis. Recent CLE studies showed great prospects for a reliable, real-time ultrastructural imaging of OSCC in situ. We present and evaluate a novel automatic approach for OSCC diagnosis using deep learning technologies on CLE images. The method is compared against textural feature-based machine learning approaches that represent the current state of the art. For this work, CLE image sequences (7894 images) from patients diagnosed with OSCC were obtained from 4 specific locations in the oral cavity, including the OSCC lesion. The present approach is found to outperform the state of the art in CLE image recognition with an area under the curve (AUC) of 0.96 and a mean accuracy of 88.3% (sensitivity 86.6%, specificity 90%).","keywords_author":null,"keywords_other":["DIAGNOSIS","SQUAMOUS-CELL CARCINOMA","CONFOCAL LASER ENDOMICROSCOPY","NECK-CANCER","NEURAL-NETWORKS","RECOGNITION","HEAD","DYSPLASIA"],"max_cite":5.0,"pub_year":2017.0,"sources":"['ieee', 'wos']","rawkeys":["diagnosis","neural-networks","neck-cancer","recognition","confocal laser endomicroscopy","head","dysplasia","squamous-cell carcinoma"],"tags":["diagnosis","neck-cancer","recognition","confocal laser endomicroscopy","head","neural networks","dysplasia","squamous-cell carcinoma"]},{"p_id":11052,"title":"Continuous robust sound event classification using time-frequency features and deep learning","abstract":"The automatic detection and recognition of sound events by computers is a requirement for a number of emerging sensing and human computer interaction technologies. Recent advances in this field have been achieved by machine learning classifiers working in conjunction with time-frequency feature representations. This combination has achieved excellent accuracy for classification of discrete sounds. The ability to recognise sounds under real-world noisy conditions, called robust sound event classification, is an especially challenging task that has attracted recent research attention. Another aspect of real-word conditions is the classification of continuous, occluded or overlapping sounds, rather than classification of short isolated sound recordings. This paper addresses the classification of noise-corrupted, occluded, overlapped, continuous sound recordings. It first proposes a standard evaluation task for such sounds based upon a common existing method for evaluating isolated sound classification. It then benchmarks several high performing isolated sound classifiers to operate with continuous sound data by incorporating an energy-based event detection front end. Results are reported for each tested system using the new task, to provide the first analysis of their performance for continuous sound event detection. In addition it proposes and evaluates a novel Bayesian-inspired front end for the segmentation and detection of continuous sound recordings prior to classification.","keywords_author":null,"keywords_other":["IMAGE FEATURE","MACHINE","AUDIO CLASSIFICATION","NEURAL-NETWORKS","RECOGNITION"],"max_cite":0.0,"pub_year":2017.0,"sources":"['ieee', 'wos']","rawkeys":["neural-networks","recognition","machine","audio classification","image feature"],"tags":["image features","recognition","neural networks","machine","audio classification"]},{"p_id":11061,"title":"On combining multiscale deep learning features for the classification of hyperspectral remote sensing imagery","abstract":"In recent years, satellite imagery has greatly improved in both spatial and spectral resolution. One of the major unsolved problems in highly developed remote sensing imagery is the manual selection and combination of appropriate features according to spectral and spatial properties. Deep learning framework can learn global and robust features from the training data set automatically, and it has achieved state-of-the-art classification accuracies over different image classification tasks. In this study, a technique is proposed which attempts to classify hyperspectral imagery by incorporating deep learning features. Firstly, deep learning features are extracted by multiscale convolutional auto-encoder. Then, based on the learned deep learning features, a logistic regression classifier is trained for classification. Finally, parameters of deep learning framework are analysed and the potential development is introduced. Experiments are conducted on the well-known Pavia data set which is acquired by the reflective optics system imaging spectrometer sensor. It is found that the deep learning-based method provides a more accurate classification result than the traditional ones.","keywords_author":null,"keywords_other":["PROFILES","OBJECTS","RECOGNITION","EXTRACTION"],"max_cite":37.0,"pub_year":2015.0,"sources":"['ieee', 'wos']","rawkeys":["profiles","recognition","extraction","objects"],"tags":["profiles","recognition","extraction","objects"]},{"p_id":19270,"title":"Multimodal emotion recognition using EEG and eye tracking data","abstract":"\u00a9 2014 IEEE. This paper presents a new emotion recognition method which combines electroencephalograph (EEG) signals and pupillary response collected from eye tracker. We select 15 emotional film clips of 3 categories (positive, neutral and negative). The EEG signals and eye tracking data of five participants are recorded, simultaneously, while watching these videos. We extract emotion-relevant features from EEG signals and eye tracing data of 12 experiments and build a fusion model to improve the performance of emotion recognition. The best average accuracies based on EEG signals and eye tracking data are 71.77% and 58.90%, respectively. We also achieve average accuracies of 73.59% and 72.98% for feature level fusion strategy and decision level fusion strategy, respectively. These results show that both feature level fusion and decision level fusion combining EEG signals and eye tracking data can improve the performance of emotion recognition model.","keywords_author":null,"keywords_other":["Emotions","Male","Algorithms","Young Adult","Humans","Pattern Recognition, Automated","Electroencephalography","Multimodal Imaging","Models, Psychological","Brain","Eye Movements","Female","Iris"],"max_cite":21.0,"pub_year":2014.0,"sources":"['scp', 'ieee']","rawkeys":["iris","emotions","automated","male","female","humans","brain","psychological","models","young adult","algorithms","pattern recognition","electroencephalography","multimodal imaging","eye movements"],"tags":["iris","model","automated","emotion","male","recognition","eeg","humans","brain","young adult","algorithms","pattern recognition","female","multimodal imaging","eye movements"]},{"p_id":60233,"title":"Deep Belief Network Based Hybrid Model for Building Energy Consumption Prediction","abstract":"To enhance the prediction performance for building energy consumption, this paper presents a modified deep belief network (DBN) based hybrid model. The proposed hybrid model combines the outputs from the DBN model with the energy-consuming pattern to yield the final prediction results. The energy-consuming pattern in this study represents the periodicity property of building energy consumption and can be extracted from the observed historical energy consumption data. The residual data generated by removing the energy-consuming pattern from the original data are utilized to train the modified DBN model. The training of the modified DBN includes two steps, the first one of which adopts the contrastive divergence (CD) algorithm to optimize the hidden parameters in a pre-train way, while the second one determines the output weighting vector by the least squares method. The proposed hybrid model is applied to two kinds of building energy consumption data sets that have different energy-consuming patterns (daily-periodicity and weekly-periodicity). In order to examine the advantages of the proposed model, four popular artificial intelligence methodsthe backward propagation neural network (BPNN), the generalized radial basis function neural network (GRBFNN), the extreme learning machine (ELM), and the support vector regressor (SVR) are chosen as the comparative approaches. Experimental results demonstrate that the proposed DBN based hybrid model has the best performance compared with the comparative techniques. Another thing to be mentioned is that all the predictors constructed by utilizing the energy-consuming patterns perform better than those designed only by the original data. This verifies the usefulness of the incorporation of the energy-consuming patterns. The proposed approach can also be extended and applied to some other similar prediction problems that have periodicity patterns, e.g., the traffic flow forecasting and the electricity consumption prediction.","keywords_author":["building energy consumption prediction","deep belief network","contrastive divergence algorithm","least squares learning","energy-consuming pattern"],"keywords_other":["GENETIC ALGORITHM","INCORPORATING PRIOR KNOWLEDGE","PARTICLE SWARM OPTIMIZATION","CLASSIFICATION","SUPPORT VECTOR REGRESSION","NEURAL-NETWORKS","RECOGNITION","EXTREME LEARNING-MACHINE","FORECASTING-MODEL","MONOTONICITY"],"max_cite":2.0,"pub_year":2018.0,"sources":"['ieee', 'wos']","rawkeys":["monotonicity","neural-networks","recognition","incorporating prior knowledge","building energy consumption prediction","least squares learning","genetic algorithm","forecasting-model","classification","deep belief network","contrastive divergence algorithm","support vector regression","extreme learning-machine","energy-consuming pattern","particle swarm optimization"],"tags":["monotonicity","recognition","incorporating prior knowledge","neural networks","building energy consumption prediction","support vector regression (svr)","forecasting models","least squares learning","genetic algorithm","energy-consuming pattern","classification","contrastive divergence algorithm","extreme learning machine","deep belief networks","particle swarm optimization"]},{"p_id":11085,"title":"A deep learning framework for financial time series using stacked autoencoders and long-short term memory","abstract":"The application of deep learning approaches to finance has received a great deal of attention from both investors and researchers. This study presents a novel deep learning framework where wavelet transforms (WT), stacked autoencoders (SAEs) and long-short term memory (LSTM) are combined for stock price forecasting. The SAEs for hierarchically extracted deep features is introduced into stock price forecasting for the first time. The deep learning framework comprises three stages. First, the stock price time series is decomposed by WT to eliminate noise. Second, SAEs is applied to generate deep high-level features for predicting the stock price. Third, high-level denoising features are fed into LSTM to forecast the next day's closing price. Six market indices and their corresponding index futures are chosen to examine the performance of the proposed model. Results show that the proposed model outperforms other similar models in both predictive accuracy and profitability performance.","keywords_author":null,"keywords_other":["PREDICTION","REPRESENTATION","NETS","CLASSIFICATION","COMPONENT ANALYSIS","MODEL","RECOGNITION","ARTIFICIAL NEURAL-NETWORKS","INDEX","STOCK-MARKET"],"max_cite":7.0,"pub_year":2017.0,"sources":"['ieee', 'wos']","rawkeys":["stock-market","recognition","model","artificial neural-networks","prediction","representation","index","classification","nets","component analysis"],"tags":["stock market","recognition","model","neural networks","prediction","representation","index","classification","nets","component analysis"]},{"p_id":11087,"title":"Automatic land cover classification of geo-tagged field photos by deep learning","abstract":"With more and more crowdsourcing geo-tagged field photos available online, they are becoming a potentially valuable source of information for environmental studies. However, the labelling and recognition of these photos are time-consuming. To utilise such information, a land cover type recognition model for field photos was proposed based on the deep learning technique. This model combines a pre-trained convolutional neural network (CNN) as the image feature extractor and the multinomial logistic regression model as the feature classifier. The pre-trained CNN model Inception-v3 was used in this study. The labelled field photos from the Global Geo-Referenced Field Photo Library (http:\/\/eomf.ou.eduiphotos) were chosen for model training and validation. The results indicated that our recognition model achieved an acceptable accuracy (48.40% for top-1 prediction and 76.24% for top-3 prediction) of land cover classification. With accurate self-assessment of confidence, the model can be applied to classify numerous online geo-tagged field photos for environmental information extraction. (C) 2017 Elsevier Ltd. All rights reserved.","keywords_author":["Deep learning","Convolutional neural network","Transfer learning","Multinomial logistic regression","Land cover","Crowdsourced photos"],"keywords_other":["IMAGERY","RECOGNITION","FACE DETECTION","PALSAR","DECIDUOUS RUBBER PLANTATIONS","BASIN"],"max_cite":6.0,"pub_year":2017.0,"sources":"['wos']","rawkeys":["crowdsourced photos","recognition","deciduous rubber plantations","multinomial logistic regression","deep learning","transfer learning","land cover","face detection","palsar","basin","convolutional neural network","imagery"],"tags":["crowdsourced photos","recognition","deciduous rubber plantations","transfer learning","land cover","machine learning","face detection","multiple linear regressions","palsar","basin","convolutional neural network","imagery"]},{"p_id":11088,"title":"Empirical Mode Decomposition based ensemble deep learning for load demand time series forecasting","abstract":"Load demand forecasting is a critical process in the planning of electric utilities. An ensemble method composed of Empirical Mode Decomposition (EMD) algorithm and deep learning approach is presented in this work. For this purpose, the load demand series were first decomposed into several intrinsic mode functions (IMFs). Then a Deep Belief Network (DBN) including two restricted Boltzmann machines (RBMs) was used to model each of the extracted IMFs, so that the tendencies of these IMFs can be accurately predicted. Finally, the prediction results of all IMFs can be combined by either unbiased or weighted summation to obtain an aggregated output for load demand. The electricity load demand data sets from Australian Energy Market Operator (AEMO) are used to test the effectiveness of the proposed EMD-based DBN approach. Simulation results demonstrated attractiveness of the proposed method compared with nine forecasting methods. (C) 2017 Elsevier B.V. All rights reserved.","keywords_author":["Deep learning","Empirical Mode Decomposition","Ensemble method","Load demand forecasting","Neural networks","Random forests","Support vector regression","Time series forecasting","Empirical Mode Decomposition","Deep learning","Ensemble method","Time series forecasting","Load demand forecasting","Neural networks","Support vector regression","Random forests"],"keywords_other":["Random forests","Load demand","NETS","FRAMEWORK","Support vector regression (SVR)","Empirical Mode Decomposition","RANDOM FORESTS","ALGORITHM","Time series forecasting","SUPPORT VECTOR REGRESSION","SYSTEM","ELECTRICITY","ARTIFICIAL NEURAL-NETWORKS","RECOGNITION","CLASSIFIERS","Ensemble methods"],"max_cite":20.0,"pub_year":2017.0,"sources":"['scp', 'wos']","rawkeys":["ensemble methods","load demand","classifiers","random forests","time series forecasting","support vector regression (svr)","system","load demand forecasting","support vector regression","empirical mode decomposition","algorithm","recognition","deep learning","framework","artificial neural-networks","neural networks","nets","ensemble method","electricity"],"tags":["recognition","electricity","neural networks","framework","support vector regression (svr)","machine learning","ensemble methods","load demand","random forests","system","load demand forecasting","classifier","nets","algorithms","time series forecasting","empirical mode decomposition"]},{"p_id":11089,"title":"Detection of anomalous behavior in a robot system based on deep learning elements","abstract":"The preprocessing procedure for anomalous behavior of robot system elements is proposed in the paper. It uses a special kind of a neural network called an autoencoder to solve two problems. The first problem is to decrease the dimensionality of the training data using the autoencoder to calculate the Mahalanobis distance, which can be viewed as one of the best metrics to detect the anomalous behavior of robots or sensors in the robot systems. The second problem is to apply the autoencoder to transfer learning. The autoencoder is trained by means of the target data which corresponds to the extreme operational conditions of the robot system. The source data containing the normal and anomalous observations derived from the normal operation conditions is reconstructed to the target data using the trained autoencoder. The reconstructed source data is used to define a optimal threshold for making decision on the anomaly of the observation based on the Mahalanobis distance.","keywords_author":["autoencoder","detection of anomalous behavior","machine learning","Mahalanobis distance","robot systems","transfer learning","robot systems","detection of anomalous behavior","autoencoder","Mahalanobis distance","transfer learning","machine learning"],"keywords_other":["RECOGNITION"],"max_cite":1.0,"pub_year":2016.0,"sources":"['ieee', 'scp', 'wos']","rawkeys":["recognition","transfer learning","mahalanobis distance","machine learning","autoencoder","robot systems","detection of anomalous behavior"],"tags":["recognition","robotic systems","transfer learning","machine learning","auto encoders","mahalanobis distances","detection of anomalous behavior"]},{"p_id":68435,"title":"Classification of Hyperspectral Images by Gabor Filtering Based Deep Network","abstract":"In this paper, a novel spectral-spatial classification method based on Gabor filtering and deep network (GFDN) is proposed. First, Gabor features are extracted by performing Gabor filtering on the first three principal components of the hyperspectral image, which can typically characterize the low-level spatial structures of different orientations and scales. Then, the Gabor features and spectral features are simply stacked to form the fused features. Afterwards, deep features are captured by training a stacked sparse autoencoder deep network with the fused features obtained above as inputs. Since the number of training samples of hyperspectral images is often very limited, which negatively affects the classification performance in deep learning, an effective way of constructing virtual samples is designed to generate more training samples, automatically. By jointly utilizing both the real and virtual samples, the parameters of the deep network can be better trained and updated, which can result in classification results of higher accuracies. Experiments performed on four real hyperspectral datasets show that the proposed method outperforms several recently proposed classification methods in terms of classification accuracies.","keywords_author":["Deep learning","Gabor filter","hyperspectral image (HSI) classification","stacked sparse autoencoders (SSAE)","virtual samples"],"keywords_other":["ATTRIBUTE PROFILES","FEATURES","REPRESENTATION","RECOGNITION","SPECTRAL-SPATIAL CLASSIFICATION","FUSION","EXTINCTION PROFILES","FEATURE-EXTRACTION"],"max_cite":0.0,"pub_year":2018.0,"sources":"['ieee', 'wos']","rawkeys":["virtual samples","recognition","gabor filter","hyperspectral image (hsi) classification","features","deep learning","feature-extraction","attribute profiles","spectral-spatial classification","representation","extinction profiles","stacked sparse autoencoders (ssae)","fusion"],"tags":["virtual samples","recognition","gabor filter","features","spectral-spatial classification","stacked sparse autoencoder","attribute profiles","machine learning","hyperspectral image classification","representation","extinction profiles","feature extraction","fusion"]},{"p_id":11101,"title":"Automatic modulation classification based on deep learning for unmanned aerial vehicles","abstract":"Deep learning has recently attracted much attention due to its excellent performance in processing audio, image, and video data. However, few studies are devoted to the field of automatic modulation classification (AMC). It is one of the most well-known research topics in communication signal recognition and remains challenging for traditional methods due to complex disturbance from other sources. This paper proposes a heterogeneous deep model fusion (HDMF) method to solve the problem in a unified framework. The contributions include the following: (1) a convolutional neural network (CNN) and long short-term memory (LSTM) are combined by two different ways without prior knowledge involved; (2) a large database, including eleven types of single-carrier modulation signals with various noises as well as a fading channel, is collected with various signal-to-noise ratios (SNRs) based on a real geographical environment; and (3) experimental results demonstrate that HDMF is very capable of coping with the AMC problem, and achieves much better performance when compared with the independent network.","keywords_author":["Automatic modulation classification","Classifier fusion","Convolutional neural network","Deep learning","Long short-term memory","deep learning","automatic modulation classification","classifier fusion","convolutional neural network","long short-term memory"],"keywords_other":["Communication signals","Geographical environment","Automatic modulation classification","Single carrier modulation","Convolutional Neural Networks (CNN)","MODEL","RECOGNITION","Convolutional neural network","Classifier fusion","Automatic modulation classification (AMC)"],"max_cite":0.0,"pub_year":2018.0,"sources":"['ieee', 'scp', 'wos']","rawkeys":["geographical environment","automatic modulation classification (amc)","recognition","model","deep learning","long short-term memory","convolutional neural networks (cnn)","classifier fusion","single carrier modulation","communication signals","convolutional neural network","automatic modulation classification"],"tags":["geographical environment","recognition","model","long short-term memory","machine learning","classifier fusion","single carrier modulation","communication signals","convolutional neural network","automatic modulation classification"]},{"p_id":11103,"title":"Nondestructive Freshness Discriminating of Shrimp Using Visible\/Near-Infrared Hyperspectral Imaging Technique and Deep Learning Algorithm","abstract":"In this study, visible and near-infrared hyperspectral imaging (HSI) technique combined with deep learning algorithm was investigated for discriminating the freshness of shrimp during cold storage. Shrimps were labeled into two freshness grades (fresh and stale) according to their total volatile basic nitrogen contents. Spectral features were extracted from the HSI data by stacked auto-encoders (SAEs)-based deep learning algorithm and then used to classify the freshness grade of shrimp by a logistic regression (LR)-based deep learning algorithm. The results demonstrated that the SAEs-LR achieved satisfactory total classification accuracy of 96.55 and 93.97% for freshness grade of shrimp in calibration (116 samples) and prediction (116 samples) sets, respectively. An image processing algorithm was also developed for visualizing the classification map of freshness grade. Results confirmed the possibility of rapid and nondestructive detecting freshness grade of shrimp by the combination of hyperspectral imaging technique and deep learning algorithm. The SAEs-LR method adds a new tool for the multivariate analysis of hyperspectral image for shrimp quality inspections.","keywords_author":["Detection","Cold storage","Freshness","Stacked auto-encoders","Logistic regression","Hyperspectral imaging"],"keywords_other":["MEAT","PREDICTION","REFLECTANCE SPECTROSCOPY","TOOL","CHEMOMETRICS","QUALITY","FEATURES","CLASSIFICATION","TVB-N","RECOGNITION"],"max_cite":1.0,"pub_year":2017.0,"sources":"['ieee', 'wos']","rawkeys":["chemometrics","cold storage","quality","recognition","logistic regression","features","meat","prediction","reflectance spectroscopy","tool","hyperspectral imaging","detection","classification","freshness","stacked auto-encoders","tvb-n"],"tags":["chemometrics","cold storage","quality","recognition","features","meat","prediction","reflectance spectroscopy","stacked autoencoders","tool","hyperspectral imaging","logistic regressions","detection","classification","freshness","tvb-n"]},{"p_id":11129,"title":"Dynamic deep learning algorithm based on incremental compensation for fault diagnosis model","abstract":"As one of research and practice hotspots in the field of intelligent manufacturing, the machine learning approach is applied to diagnose and predict equipment fault for running state data. Despite deep learning approach overcomes the problem that the traditional machine learning approaches for fault diagnosis is difficult to characterize the complex mapping between the massive fault data, the exponentially grown and newly generated data is learned repeatedly, and these approaches cannot incrementally correct the model to adapt the situation that the states and properties of equipment are changed over time, resulting in the increase of time costs and the decrease of diagnosis accuracy of model. In this paper, a dynamic deep learning algorithm based on incremental compensation is proposed. Firstly, the feature modes of the newly generated data are extracted by using deep learning algorithm; it is then compared with the fault modes extracted from the historical data. Next, a similarity computing model is presented to dynamically adjust the weights of incrementally merged modes. Finally, the SVM algorithm is employed to classify the weighted modes by supervised way, and the BP algorithm utilized to fine tune the model, in order to complete the dynamic and compensatory adjustment of deep learning with original modes and incremental modes. The experimental results of bearing running data demonstrate that the proposed approach could significantly improve the accuracy of diagnosis and save the time cost, contributing to meet the varied needs of the real- time equipment fault diagnosis.","keywords_author":["Deep learning","Denoising autoencoder","Dynamic compensation","Fault diagnosis","Incremental learning","Deep learning","Dynamic compensation","Fault diagnosis","Denoising Autoencoder","Incremental learning"],"keywords_other":["Intelligent Manufacturing","BEARINGS","Similarity computing","REPRESENTATION","Machine learning approaches","MACHINE","CLASSIFICATION","Real time equipment","Dynamic compensation","RECOGNITION","Incremental learning","Denoising Autoencoder","NETWORK","Fault diagnosis model"],"max_cite":0.0,"pub_year":2018.0,"sources":"['scp', 'wos']","rawkeys":["dynamic compensation","real time equipment","network","recognition","similarity computing","deep learning","denoising autoencoder","fault diagnosis model","incremental learning","machine","representation","machine learning approaches","classification","intelligent manufacturing","bearings","fault diagnosis"],"tags":["bearing","dynamic compensation","real time equipment","recognition","similarity computation","denoising autoencoder","fault diagnosis model","incremental learning","machine","machine learning","representation","machine learning approaches","networks","classification","intelligent manufacturing","fault diagnosis"]},{"p_id":11134,"title":"Unsupervised Spectral-Spatial Feature Learning via Deep Residual Conv-Deconv Network for Hyperspectral Image Classification","abstract":"Supervised approaches classify input data using a set of representative samples for each class, known as training samples. The collection of such samples is expensive and time demanding. Hence, unsupervised feature learning, which has a quick access to arbitrary amounts of unlabeled data, is conceptually of high interest. In this paper, we propose a novel network architecture, fully Conv-Deconv network, for unsupervised spectral-spatial feature learning of hyperspectral images, which is able to be trained in an end-to-end manner. Specifically, our network is based on the so-called encoder-decoder paradigm, i.e., the input 3-D hyperspectral patch is first transformed into a typically lower dimensional space via a convolutional subnetwork (encoder), and then expanded to reproduce the initial data by a deconvolutional subnetwork (decoder). However, during the experiment, we found that such a network is not easy to be optimized. To address this problem, we refine the proposed network architecture by incorporating: 1) residual learning and 2) a new unpooling operation that can use memorized max-pooling indexes. Moreover, to understand the \"black box,\" we make an in-depth study of the learned feature maps in the experimental analysis. A very interesting discovery is that some specific \"neurons\" in the first residual block of the proposed network own good description power for semantic visual patterns in the object level, which provide an opportunity to achieve \"free\" object detection. This paper, for the first time in the remote sensing community, proposes an end-to-end fully Conv-Deconv network for unsupervised spectral-spatial feature learning. Moreover, this paper also introduces an in-depth investigation of learned features. Experimental results on two widely used hyperspectral data, Indian Pines and Pavia University, demonstrate competitive performance obtained by the proposed methodology compared with other studied approaches.","keywords_author":["Convolutional network","deconvolutional network","hyperspectral image classification","residual learning","unsupervised spectral-spatial feature learning"],"keywords_other":["HIGH-RESOLUTION","ATTRIBUTE PROFILES","FRAMEWORK","ALGORITHM","SUPPORT VECTOR MACHINES","NEURAL-NETWORKS","RECOGNITION","SEGMENTATION","FEATURE-EXTRACTION","MORPHOLOGICAL PROFILES"],"max_cite":3.0,"pub_year":2018.0,"sources":"['wos']","rawkeys":["neural-networks","algorithm","residual learning","recognition","segmentation","feature-extraction","framework","attribute profiles","unsupervised spectral-spatial feature learning","morphological profiles","hyperspectral image classification","deconvolutional network","high-resolution","support vector machines","convolutional network"],"tags":["residual learning","recognition","segmentation","neural networks","framework","attribute profiles","unsupervised spectral-spatial feature learning","morphological profiles","hyperspectral image classification","deconvolutional network","machine learning","high resolution","feature extraction","convolutional neural network","algorithms"]},{"p_id":11146,"title":"Action Recognition Based on Efficient Deep Feature Learning in the Spatio-Temporal Domain","abstract":"Hand-crafted feature functions are usually designed based on the domain knowledge of a presumably controlled environment and often fail to generalize, as the statistics of real-world data cannot always be modeled correctly. Data-driven feature learning methods, on the other hand, have emerged as an alternative that often generalize better in uncontrolled environments. We present a simple, yet robust, 2-D convolutional neural network extended to a concatenated 3-D network that learns to extract features from the spatio-temporal domain of raw video data. The resulting network model is used for content-based recognition of videos. Relying on a 2-D convolutional neural network allows us to exploit a pretrained network as a descriptor that yielded the best results on the largest and challenging ILSVRC-2014 dataset. Experimental results on commonly used benchmarking video datasets demonstrate that our results are state-of-the-art in terms of accuracy and computational time without requiring any preprocessing (e.g., optic flow) or a priori knowledge on data capture (e.g., camera motion estimation), whichmakes it more general and flexible than other approaches. Our implementation is made available.","keywords_author":["Computer vision for automation","recognition","visual learning"],"keywords_other":["POINTS","LATENT STRUCTURE","ROBOT"],"max_cite":6.0,"pub_year":2016.0,"sources":"['ieee', 'wos']","rawkeys":["recognition","robot","computer vision for automation","visual learning","latent structure","points"],"tags":["latent structures","recognition","computer vision for automation","visual learning","robotics","point"]},{"p_id":93076,"title":"Device-Free Wireless Sensing: Challenges, Opportunities, and Applications","abstract":"Recent developments on DFWS have shown that wireless signals can be utilized not only as a communication medium to transmit data, but also as an enabling tool for realizing non-intrusive device-free sensing. DFWS has many potential applications, for example, human detection and localization, human activity and gesture recognition, surveillance, elder or patient monitoring, emergency rescue, and so on. With the development and maturity of DFWS, we believe it will eventually empower traditional wireless networks with the augmented ability to sense the surrounding environment, and evolve wireless communication networks into intelligent sensing networks that could sense human-scale context information within the deployment area of the network. The research field of DFWS has emerged quickly recently. This article tries to provide an integrated picture of this emerging field and hopefully inspire future research. Specifically, we present the working principle and system architecture of the DFWS system, review its potential applications, and discuss research challenges and opportunities.","keywords_author":null,"keywords_other":["RECOGNITION","LOCALIZATION"],"max_cite":0.0,"pub_year":2018.0,"sources":"['wos']","rawkeys":["recognition","localization"],"tags":["recognition","localization"]},{"p_id":11171,"title":"SVM or deep learning? A comparative study on remote sensing image classification","abstract":"With constant advancements in remote sensing technologies resulting in higher image resolution, there is a corresponding need to be able to mine useful data and information from remote sensing images. In this paper, we study auto-encoder (SAE) and support vector machine (SVM), and to examine their sensitivity, we include additional umber of training samples using the active learning frame. We then conduct a comparative evaluation. When classifying remote sensing images, SVM can also perform better than SAE in some circumstances, and active learning schemes can be used to achieve high classification accuracy in both methods.","keywords_author":["Spatial big data","Sparse auto-encoder","Support vector machine","Active learning","Remote sensing"],"keywords_other":["NEURAL-NETWORKS","RECOGNITION","SUPPORT VECTOR MACHINES"],"max_cite":8.0,"pub_year":2016.0,"sources":"['ieee', 'wos']","rawkeys":["neural-networks","recognition","remote sensing","sparse auto-encoder","active learning","support vector machines","spatial big data","support vector machine"],"tags":["recognition","remote sensing","neural networks","machine learning","stacked autoencoders","spatial big data"]},{"p_id":68518,"title":"Blind modulation format identification using nonlinear power transformation","abstract":"This paper proposes and experimentally demonstrates a blind modulation format identification (MFI) method delivering high accuracy (> 99%) even in a low OSNR regime (< 10 dB). By using nonlinear power transformation and peak detection, the proposed MFI can recognize whether the signal modulation format is BPSK, QPSK, 8-PSK or 16-QAM. Experimental results demonstrate that the proposed MFI can achieve a successful identification rate as high as 99% when the incoming signal OSNR is 7 dB. Key parameters, such as FFT length and laser phase noise tolerance of the proposed method, have been characterized. (C) 2017 Optical Society of America","keywords_author":null,"keywords_other":["RECOGNITION","OPTICAL RECEIVERS","DIGITAL COHERENT RECEIVERS"],"max_cite":3.0,"pub_year":2017.0,"sources":"['wos']","rawkeys":["optical receivers","recognition","digital coherent receivers"],"tags":["optical receivers","recognition","digital coherent receivers"]},{"p_id":68519,"title":"Modulation format identification enabled by the digital frequency-offset loading technique for hitless coherent transceiver","abstract":"We propose a blind and fast modulation format identification (MFI) enabled by the digital frequency-offset (FO) loading technique for hitless coherent transceiver. Since modulation format information is encoded to the FO distribution during digital signal processing (DSP) at the transmitter side (Tx), we can use the fast Fourier transformation based FO estimation (FFT-FOE) method to obtain the FO distribution of individual data block after constant modulus algorithm (CMA) pre-equalization at the receiver side, in order to realize non-data-aided (NDA) and fast MFI. The obtained FO can be also used for subsequent FO compensation (FOC), without additional complexity. We numerically investigate and experimentally verify the proposed MFI with high accuracy and fast format switching among 28 Gbaud dual-polarization (DP)-4\/8\/16\/64QAM, time domain hybrid-4\/16QAM, and set partitioning (SP)-128QAM. In particular, the proposed MFI brings no performance degradation, in term of tolerance of amplified spontaneous emission (ASE) noise, laser linewidth, and fiber nonlinearity. Finally, a hitless coherent transceiver enabled by the proposed MFI with switching-block of only 2048 symbols is demonstrated over 1500 km standard single mode fiber (SSMF) transmission. (C) 2018 Optical Society of America under the terms of the OSA Open Access Publishing Agreement","keywords_author":null,"keywords_other":["RECOVERY","RECOGNITION","RECEIVERS"],"max_cite":0.0,"pub_year":2018.0,"sources":"['wos']","rawkeys":["recognition","receivers","recovery"],"tags":["recognition","receivers","recovery"]},{"p_id":43944,"title":"Facial expression recognition via deep learning","abstract":"\u00a9 2017 IEEE. Automated Facial Expression Recognition has remained a challenging and interesting problem in computer vision. The recognition of facial expressions is difficult problem for machine learning techniques, since people can vary significantly in the way they show their expressions. Deep learning is a new area of research within machine learning method which can classify images of human faces into emotion categories using Deep Neural Networks (DNN). Convolutional neural networks (CNN) have been widely used to overcome the difficulties in facial expression classification. In this paper, we present a new architecture network based on CNN for facial expressions recognition. We fine tuned our architecture with Visual Geometry Group model (VGG) to improve results. To evaluate our architecture we tested it with many largely public databases (CK+, MUG, and RAFD). Obtained results show that the CNN approach is very effective in image expression recognition on many public databases which achieve an improvements in facial expression analysis.","keywords_author":["Architecture","Classification","CNN","Deep Learning","Facial Expression","Recognition"],"keywords_other":["Facial expressions recognition","Recognition","Machine learning techniques","Convolutional Neural Networks (CNN)","Facial expression recognition","Recognition of facial expressions","Facial Expressions","Facial expression classification"],"max_cite":0.0,"pub_year":2018.0,"sources":"['scp']","rawkeys":["facial expression recognition","recognition of facial expressions","recognition","deep learning","facial expressions","machine learning techniques","cnn","facial expression classification","classification","facial expression","architecture","facial expressions recognition","convolutional neural networks (cnn)"],"tags":["facial expression recognition","recognition of facial expressions","recognition","facial expressions","machine learning","machine learning techniques","facial expression classification","classification","convolutional neural network","architecture"]},{"p_id":11177,"title":"Human body part estimation from depth images via spatially-constrained deep learning","abstract":"Object recognition, human pose estimation and scene recognition are applications which are frequently solved through a decomposition into a collection of parts. The resulting local representation has significant advantages, especially in the case of occlusions and when the subject is non-rigid. Detection and recognition require modelling the appearance of the different object parts as well as their spatial layout. This representation has been particularly successful in body part estimation from depth images.","keywords_author":["Convolutional networks","Deep learning","Depth images","Segmentation","Spatial layout","Segmentation","Spatial layout","Deep learning","Convolutional networks","Depth images"],"keywords_other":["Deep learning","Depth image","Convolutional networks","Spatial layout","NETWORKS","Human bodies","RECOGNITION"],"max_cite":11.0,"pub_year":2014.0,"sources":"['scp', 'wos']","rawkeys":["spatial layout","recognition","depth image","segmentation","deep learning","convolutional networks","networks","depth images","human bodies"],"tags":["spatial layout","recognition","depth image","segmentation","machine learning","networks","convolutional neural network","human bodies"]},{"p_id":68520,"title":"Blind Density-Peak-Based Modulation Format Identification for Elastic Optical Networks","abstract":"Optical modulation format identification is critical in the next generation of heterogeneous and reconfigurable optical networks. Here, we present a blind modulation format identification method by applying fast density-peak-based pattern recognition in the autonomous receiver of elastic optical networks. In this paper, we find that the different modulation format types show different energy level features which can be used as a metric to identify these modulation formats in two-dimensional Stokes plane. The proposed method does not require training symbols, and is insensitive to carrier phase noise, frequency offset as well as polarization mixing. The effectiveness is verified via numerical simulations and experiments with PDM-BPSK, PDM-QPSK, PDM-8PSK, PDM-16PSK, PDM-8QAM, and PDM-16QAM. The results show that high identification accuracy can be realized using our proposed method over wide optical signal-to-noise ratio ranges. Meanwhile, we also discuss the influence of the residual chromatic dispersion, polarization mode dispersion, and polarization dependent loss impairments to our proposed method. We believe that the simple and flexible identification method would certainly bring a great convenience to the future optical networks.","keywords_author":["Digital signal processing","modulation format identification","optical performance monitoring"],"keywords_other":["STOKES SPACE","DIFFUSION","DIGITAL COHERENT RECEIVERS","RECOGNITION","FAST SEARCH","FIND"],"max_cite":0.0,"pub_year":2018.0,"sources":"['wos']","rawkeys":["optical performance monitoring","recognition","modulation format identification","fast search","stokes space","digital coherent receivers","digital signal processing","find","diffusion"],"tags":["optical performance monitoring","recognition","modulation format identification","fast search","stokes space","digital coherent receivers","digital signal processing","find","diffusion"]},{"p_id":68521,"title":"Stokes Space Modulation Format Identification for Optical Signals Using Probabilistic Neural Network","abstract":"A Stokes space modulation format identification (MFI) method using probabilistic neural network (PNN) is proposed for coherent optical receivers. According to amplitude histograms obtained by the distribution of Stokes vectors on the s(1) axis, the incoming signals are first classified intoPDM-mPSK, PDM-16QAM, and PDM-64 QAM signals based on PNN. To further identify PDM-mPSK signals, the constellation feature of Stokes vectors on s(2)-s(3) plane is extracted by image processing techniques and then processed by PNN. The high identification accuracy is demonstrated via numerical simulations with 28-Gbaud PDM-QPSK, PDM-8PSK, PDM-16QAM, and PDM-64QAM signals over a wide optical signal-to-noise ratio range. Owing to the characteristic of PNN, the proposed MFI method has simple training process, small training data size and a small number of required symbols. Proof-of-concept experiments have been implemented to verify the effectiveness of the proposed MFI method among 28-Gbaud PDM-QPSK, PDM-8PSK, and PDM-16QAM signals.","keywords_author":["Modulation format identification","coherent optical communications","probabilistic neural network"],"keywords_other":["TRANSFORM","DIGITAL COHERENT RECEIVERS","CLASSIFICATION","ALGORITHM","BLIND","RECOGNITION"],"max_cite":0.0,"pub_year":2018.0,"sources":"['wos']","rawkeys":["algorithm","coherent optical communications","recognition","modulation format identification","digital coherent receivers","classification","probabilistic neural network","transform","blind"],"tags":["coherent optical communications","recognition","modulation format identification","digital coherent receivers","classification","algorithms","transform","blind","probabilistic neural networks"]},{"p_id":11179,"title":"Comparisons of Deep Learning Algorithms for MNIST in Real-Time Environment","abstract":"Recognizing handwritten digits was challenging task in a couple of years ago. Thanks to machine learning algorithms, today, the issue has solved but those algorithms require much time to train and to recognize digits. Thus, using one of those algorithms to an application that works in real-time, is complex. Notwithstanding use of a trained model, if the model uses deep neural networks it requires much more time to make a prediction and becomes more complicated as well as memory usage also increases. It leads real-time application to delay and to work slowly even using trained model. A memory usage is also essential as using smaller memory of trained models works considerable faster comparing to models with huge pre-processed memory. For this work, we implemented four models on the basis of unlike algorithms which are capsule network, deep residual learning model, convolutional neural network and multinomial logistic regression to recognize handwritten digits. These models have unlike structure and they have showed a great results on MNIST before so we aim to compare them in real-time environment. The dataset MNIST seems most suitable for this work since it is popular in the field and basically used in many state-of-the-art algorithms beyond those models mentioned above. We purpose revealing most suitable algorithm to recognize handwritten digits in real-time environment. Also, we give comparisons of train and evaluation time, memory usage and other essential indexes of all four models.","keywords_author":["Capsule networks","Dynamic routing","Residual learning","CNN","Logistic regression"],"keywords_other":["DROPOUT","NEURAL-NETWORKS","RECOGNITION"],"max_cite":0.0,"pub_year":2018.0,"sources":"['wos']","rawkeys":["neural-networks","residual learning","dropout","recognition","logistic regression","dynamic routing","cnn","capsule networks"],"tags":["residual learning","dropout","recognition","dynamic routing","neural networks","logistic regressions","capsule networks","convolutional neural network"]},{"p_id":11195,"title":"Learning features for offline handwritten signature verification using deep convolutional neural networks","abstract":"Verifying the identity of a person using handwritten signatures is challenging in the presence of skilled forgeries, where a forger has access to a person's signature and deliberately attempt to imitate it. In offline (static) signature verification, the dynamic information of the signature writing process is lost, and it is difficult to design good feature extractors that can distinguish genuine signatures and skilled forgeries. This reflects in a relatively poor performance, with verification errors around 7% in the best systems in the literature. To address both the difficulty of obtaining good features, as well as improve system performance, we propose learning the representations from signature images, in a Writer-Independent format, using Convolutional Neural Networks. In particular, we propose a novel formulation of the problem that includes knowledge of skilled forgeries from a subset of users in the feature learning process, that aims to capture visual cues that distinguish genuine signatures and forgeries regardless of the user. Extensive experiments were conducted on four datasets: GPDS, 1VICYT, CEDAR and Brazilian PUC-PR datasets. On GPDS-160, we obtained a large improvement in state-of-the-art performance, achieving 1.72% Equal Error Rate, compared to 6.97% in the literature. We also verified that the features generalize beyond the GPDS dataset, surpassing the state-of-the-art performance in the other datasets, without requiring the representation to be fine-tuned to each particular dataset. (C) 2017 Elsevier Ltd. All rights reserved.","keywords_author":["Convolutional Neural Networks","Deep learning","Feature learning","Signature verification","Signature verification","Convolutional Neural Networks","Feature learning","Deep learning"],"keywords_other":["State-of-the-art performance","CLASSIFIER","Signature verification","ONLINE","STATE","ART","Handwritten signatures","IDENTIFICATION","FORGERIES","Off-line handwritten signature verification","RECOGNITION","Convolutional neural network","Feature learning","Dynamic information","Feature extractor"],"max_cite":6.0,"pub_year":2017.0,"sources":"['scp', 'wos']","rawkeys":["dynamic information","online","identification","convolutional neural networks","recognition","signature verification","deep learning","state","state-of-the-art performance","handwritten signatures","forgeries","classifier","convolutional neural network","off-line handwritten signature verification","feature learning","feature extractor","art"],"tags":["dynamic information","identification","recognition","signature verification","state","state-of-the-art performance","machine learning","handwritten signatures","social networks","classifier","convolutional neural network","off-line handwritten signature verification","feature learning","feature extractor","forgery","art"]},{"p_id":109518,"title":"Image reconstruction by domain-transform manifold learning","abstract":"Image reconstruction is essential for imaging applications across the physical and life sciences, including optical and radar systems, magnetic resonance imaging, X-ray computed tomography, positron emission tomography, ultrasound imaging and radio astronomy(1-3). During image acquisition, the sensor encodes an intermediate representation of an object in the sensor domain, which is subsequently reconstructed into an image by an inversion of the encoding function. Image reconstruction is challenging because analytic knowledge of the exact inverse transform may not exist a priori, especially in the presence of sensor non-idealities and noise. Thus, the standard reconstruction approach involves approximating the inverse function with multiple ad hoc stages in a signal processing chain(4,5), the composition of which depends on the details of each acquisition strategy, and often requires expert parameter tuning to optimize reconstructionperformance. Here we present a unified framework for image reconstruction-automated transform by manifold approximation (AUTOMAP)-which recasts image reconstruction as a data-driven supervised learning task that allows a mapping between the sensor and the image domain to emerge from an appropriate corpus of training data. We implement AUTOMAP with a deep neural network and exhibit its flexibility in learning reconstruction transforms for various magnetic resonance imaging acquisition strategies, using the same network architecture and hyperparameters. We further demonstrate that manifold learning during training results in sparse representations of domain transforms along low-dimensional data manifolds, and observe superior immunity to noise and a reduction in reconstruction artefacts compared with conventional handcrafted reconstruction methods. In addition to improving the reconstruction performance of existing acquisition methodologies, we anticipate that AUTOMAP and other learned reconstruction approaches will accelerate the development of new acquisition strategies across imaging modalities.","keywords_author":null,"keywords_other":["APPROXIMATION","SIGNAL","MODEL","RECOGNITION","MULTILAYER FEEDFORWARD NETWORKS","ARTIFICIAL NEURAL-NETWORKS","MRI","CT"],"max_cite":4.0,"pub_year":2018.0,"sources":"['ieee', 'wos']","rawkeys":["recognition","model","artificial neural-networks","multilayer feedforward networks","approximation","mri","ct","signal"],"tags":["signals","recognition","model","neural networks","multilayer feedforward networks","approximation","computed tomography","magnetic resonance imaging"]},{"p_id":11219,"title":"DeepSkeleton: Learning Multi-Task Scale-Associated Deep Side Outputs for Object Skeleton Extraction in Natural Images","abstract":"Object skeletons are useful for object representation and object detection. They are complementary to the object contour, and provide extra information, such as how object scale (thickness) varies among object parts. But object skeleton extraction from natural images is very challenging, because it requires the extractor to be able to capture both local and non-local image context in order to determine the scale of each skeleton pixel. In this paper, we present a novel fully convolutional network with multiple scale-associated side outputs to address this problem. By observing the relationship between the receptive field sizes of the different layers in the network and the skeleton scales they can capture, we introduce two scale-associated side outputs to each stage of the network. The network is trained by multi-task learning, where one task is skeleton localization to classify whether a pixel is a skeleton pixel or not, and the other is skeleton scale prediction to regress the scale of each skeleton pixel. Supervision is imposed at different stages by guiding the scale-associated side outputs toward the ground-truth skeletons at the appropriate scales. The responses of the multiple scale-associated side outputs are then fused in a scale-specific way to detect skeleton pixels using multiple scales effectively. Our method achieves promising results on two skeleton extraction datasets, and significantly outperforms other competitors. In addition, the usefulness of the obtained skeletons and scales (thickness) are verified on two object detection applications: foreground object segmentation and object proposal detection.","keywords_author":["Skeleton","fully convolutional network","scale-associated side outputs","multi-task learning","object segmentation","object proposal detection"],"keywords_other":["SELECTION","RECOGNITION","SHOCK GRAPHS"],"max_cite":2.0,"pub_year":2017.0,"sources":"['ieee', 'wos']","rawkeys":["object proposal detection","multi-task learning","recognition","skeleton","selection","fully convolutional network","shock graphs","object segmentation","scale-associated side outputs"],"tags":["object proposal detection","recognition","skeleton","selection","fully convolutional network","shock graphs","multitask learning","object segmentation","scale-associated side outputs"]},{"p_id":11220,"title":"Deep learning in remote sensing scene classification: a data augmentation enhanced convolutional neural network framework","abstract":"The recent emergence of deep learning for characterizing complex patterns in remote sensing imagery reveals its high potential to address some classic challenges in this domain, e.g. scene classification. Typical deep learning models require extremely large datasets with rich contents to train a multilayer structure in order to capture the essential features of scenes. Compared with the benchmark datasets used in popular deep learning frameworks, however, the volumes of available remote sensing datasets are particularly limited, which have restricted deep learning methods from achieving full performance gains. In order to address this fundamental problem, this article introduces a methodology to not only enhance the volume and completeness of training data for any remote sensing datasets, but also exploit the enhanced datasets to train a deep convolutional neural network that achieves state-of-the-art scene classification performance. Specifically, we propose to enhance any original dataset by applying three operations - flip, translation, and rotation to generate augmented data - and use the augmented dataset to train and obtain a more descriptive deep model. The proposed methodology is validated in three recently released remote sensing datasets, and confirmed as an effective technique that significantly contributes to potentially revolutionary changes in remote sensing scene classification, empowered by deep learning.","keywords_author":["big data","convolutional neural network (CNN)","data augmentation","deep learning","remote sensing scene classification","deep learning","remote sensing scene classification","convolutional neural network (CNN)","big data","data augmentation"],"keywords_other":["COVER CLASSIFICATION","RECOGNITION","IMAGERY"],"max_cite":4.0,"pub_year":2017.0,"sources":"['ieee', 'scp', 'wos']","rawkeys":["remote sensing scene classification","recognition","big data","deep learning","data augmentation","convolutional neural network (cnn)","imagery","cover classification"],"tags":["remote sensing scene classification","recognition","big data","machine learning","data augmentation","convolutional neural network","imagery","cover classification"]},{"p_id":68569,"title":"Computational mechanisms underlying cortical responses to the affordance properties of visual scenes","abstract":"Biologically inspired deep convolutional neural networks (CNNs), trained for computer vision tasks, have been found to predict cortical responses with remarkable accuracy. However, the internal operations of these models remain poorly understood, and the factors that account for their success are unknown. Here we develop a set of techniques for using CNNs to gain insights into the computational mechanisms underlying cortical responses. We focused on responses in the occipital place area (OPA), a scene-selective region of dorsal occipitoparietal cortex. In a previous study, we showed that fMRI activation patterns in the OPA contain information about the navigational affordances of scenes; that is, information about where one can and cannot move within the immediate environment. We hypothesized that this affordance information could be extracted using a set of purely feedforward computations. To test this idea, we examined a deep CNN with a feedforward architecture that had been previously trained for scene classification. We found that responses in the CNN to scene images were highly predictive of fMRI responses in the OPA. Moreover the CNN accounted for the portion of OPA variance relating to the navigational affordances of scenes. The CNN could thus serve as an image-computable candidate model of affordance-related responses in the OPA. We then ran a series of in silico experiments on this model to gain insights into its internal operations. These analyses showed that the computation of affordance-related features relied heavily on visual information at high-spatial frequencies and cardinal orientations, both of which have previously been identified as low-level stimulus preferences of scene-selective visual cortex. These computations also exhibited a strong preference for information in the lower visual field, which is consistent with known retinotopic biases in the OPA. Visualizations of feature selectivity within the CNN suggested that affordance-based responses encoded features that define the layout of the spatial environment, such as boundary-defining junctions and large extended surfaces. Together, these results map the sensory functions of the OPA onto a fully quantitative model that provides insights into its visual computations. More broadly, they advance integrative techniques for understanding visual cortex across multiple level of analysis: from the identification of cortical sensory functions to the modeling of their underlying algorithms.","keywords_author":null,"keywords_other":["NATURAL SCENES","PERCEPTION","REPRESENTATIONS","HIERARCHICAL-MODELS","RECOGNITION","CORTEX","DEEP NEURAL-NETWORKS","ORIENTATION","CATEGORIES","BRAIN"],"max_cite":0.0,"pub_year":2018.0,"sources":"['wos']","rawkeys":["natural scenes","recognition","brain","categories","deep neural-networks","perception","representations","hierarchical-models","orientation","cortex"],"tags":["natural scenes","recognition","hierarchical model","representation","brain","categories","perceptions","convolutional neural network","orientation","cortex"]},{"p_id":68583,"title":"High-Level Prediction Signals in a Low-Level Area of the Macaque Face-Processing Hierarchy","abstract":"Theories like predictive coding propose that lower-order brain areas compare their inputs to predictions derived from higher-order representations and signal their deviation as a prediction error. Here, we investigate whether the macaque face-processing system, a three-level hierarchy in the ventral stream, employs such a coding strategy. We show that after statistical learning of specific face sequences, the lower-level face area ML computes the deviation of actual from predicted stimuli. But these signals do not reflect the tuning characteristic of ML. Rather, they exhibit identity specificity and view invariance, the tuning properties of higher-level face areas AL and AM. Thus, learning appears to endow lower-level areas with the capability to test predictions at a higher level of abstraction than what is afforded by the feedforward sweep. These results provide evidence for computational architectures like predictive coding and suggest a new quality of functional organization of information-processing hierarchies beyond pure feedforward schemes.","keywords_author":null,"keywords_other":["V1","FIGURE","VISUAL-CORTEX","SYSTEM","RECOGNITION","INFERENCE","MOTION","PATCHES","ENHANCEMENT"],"max_cite":1.0,"pub_year":2017.0,"sources":"['wos']","rawkeys":["recognition","visual-cortex","enhancement","motion","patches","system","figure","inference","v1"],"tags":["figures","recognition","visual-cortex","enhancement","motion","patches","system","inference","v1"]},{"p_id":11239,"title":"Exploring geo-tagged photos for land cover validation with deep learning","abstract":"Land cover validation plays an important role in the process of generating and distributing land cover thematic maps, which is usually implemented by high cost of sample interpretation with remotely sensed images or field survey. With an increasing availability of geo-tagged landscape photos, the automatic photo recognition methodologies, e.g., deep learning, can be effectively utilised for land cover applications. However, they have hardly been utilised in validation processes, as challenges remain in sample selection and classification for highly heterogeneous photos. This study proposed an approach to employ geo-tagged photos for land cover validation by using the deep learning technology. The approach first identified photos automatically based on the VGG-16 network. Then, samples for validation were selected and further classified by considering photos distribution and classification probabilities. The implementations were conducted for the validation of the GlobeLand30 land cover product in a heterogeneous area, western California. Experimental results represented promises in land cover validation, given that GlobeLand30 showed an overall accuracy of 83.80% with classified samples, which was close to the validation result of 80.45% based on visual interpretation. Additionally, the performances of deep learning based on ResNet-50 and AlexNet were also quantified, revealing no substantial differences in final validation results. The proposed approach ensures geo-tagged photo quality, and supports the sample classification strategy by considering photo distribution, with accuracy improvement from 72.07% to 79.33% compared with solely considering the single nearest photo. Consequently, the presented approach proves the feasibility of deep learning technology on land cover information identification of geo-tagged photos, and has a great potential to support and improve the efficiency of land cover validation.","keywords_author":["Land cover","Accuracy assessment","Crowdsourced photos","Convolutional neural network","Sample classification"],"keywords_other":["MAP","DATA SET","DESIGN","PRODUCTS","RECOGNITION","CLASSIFICATION ACCURACY ASSESSMENT","AREA","REGION"],"max_cite":0.0,"pub_year":2018.0,"sources":"['wos']","rawkeys":["design","crowdsourced photos","recognition","products","sample classification","data set","land cover","classification accuracy assessment","map","convolutional neural network","area","accuracy assessment","region"],"tags":["design","crowdsourced photos","productivity","recognition","sample classification","land cover","classification accuracy assessment","map","data sets","regions","convolutional neural network","area","accuracy assessment"]},{"p_id":11243,"title":"Improving Deep Neural Network Based Speech Synthesis through Contextual Feature Parametrization and Multi-Task Learning","abstract":"We propose three techniques to improve speech synthesis based on deep neural network (DNN). First, at the DNN input we use real-valued contextual feature vector to represent phoneme identity, part of speech and pause information instead of the conventional binary vector. Second, at the DNN output layer, parameters for pitch-scaled spectrum and aperiodicity measures are estimated for constructing the excitation signal used in our baseline synthesis vocoder. Third, the bidirectional recurrent neural network architecture with long short term memory (BLSTM) units is adopted and trained with multi-task learning for DNN-based speech synthesis. Experimental results demonstrate that the quality of synthesized speech has been improved by adopting the new input vector and output parameters. The proposed BLSTM architecture for DNN is also beneficial to learning the mapping function from the input contextual feature to the speech parameters and to improve speech quality.","keywords_author":["DNN-based speech synthesis","Vocoder","Speech parametrization","BLSTM","Phoneme embedded vector","Multi-task learning","Pitch-scaled spectrum"],"keywords_other":["SELECTION","EXTRACTION","REPRESENTATIONS","RECOGNITION","DIVERGENCE","GENERATION"],"max_cite":0.0,"pub_year":2017.0,"sources":"['ieee', 'wos']","rawkeys":["multi-task learning","dnn-based speech synthesis","recognition","selection","phoneme embedded vector","speech parametrization","blstm","representations","pitch-scaled spectrum","divergence","generation","extraction","vocoder"],"tags":["dnn-based speech synthesis","recognition","vocoders","selection","representation","phoneme embedded vector","speech parametrization","blstm","multitask learning","pitch-scaled spectrum","divergence","generation","extraction"]},{"p_id":68594,"title":"Image interpretation above and below the object level","abstract":"Computational models of vision have advanced in recent years at a rapid rate, rivalling in some areas human-level performance. Much of the progress to date has focused on analysing the visual scene at the object level-the recognition and localization of objects in the scene. Human understanding of images reaches a richer and deeper image understanding both 'below' the object level, such as identifying and localizing object parts and sub-parts, as well as 'above' the object level, such as identifying object relations, and agents with their actions and interactions. In both cases, understanding depends on recovering meaningful structures in the image, and their components, properties and inter-relations, a process referred here as ` image interpretation'. In this paper, we describe recent directions, based on human and computer vision studies, towards human-like image interpretation, beyond the reach of current schemes, both below the object level, as well as some aspects of image interpretation at the level of meaningful configurations beyond the recognition of individual objects, and in particular, interactions between two people in close contact. In both cases the recognition process depends on the detailed interpretation of so-called 'minimal images', and at both levels recognition depends on combining 'bottom-up' processing, proceeding from low to higher levels of a processing hierarchy, together with 'top-down' processing, proceeding from high to lower levels stages of visual analysis.","keywords_author":["visual recognition","visual interpretation","social interactions","interaction recognition","minimal images"],"keywords_other":["REPRESENTATION","INFANTS","VISION","SOCIAL-DOMINANCE","HIERARCHICAL-MODELS","RECOGNITION","CORTEX"],"max_cite":1.0,"pub_year":2018.0,"sources":"['ieee', 'wos']","rawkeys":["visual recognition","recognition","social interactions","interaction recognition","representation","vision","infants","visual interpretation","minimal images","hierarchical-models","social-dominance","cortex"],"tags":["visual recognition","recognition","social interactions","hierarchical model","interaction recognition","representation","vision","infant","visual interpretation","minimal images","social-dominance","cortex"]},{"p_id":11252,"title":"Detecting non-hardhat-use by a deep learning method from far-field surveillance videos","abstract":"Hardhats are an important safety measure used to protect construction workers from accidents. However, accidents caused in ignorance of wearing hardhats still occur. In order to strengthen the supervision of construction workers to avoid accidents, automatic non-hardhat-use (NHU) detection technology can play an important role. Existing automatic methods of detecting hardhat avoidance are commonly limited to the detection of objects in near-field surveillance videos. This paper proposes the use of a high precision, high speed and widely applicable Faster R-CNN method to detect construction workers' NHU. To evaluate the performance of Faster R-CNN, more than 100,000 construction worker image frames were randomly selected from the far-field surveillance videos of 25 different construction sites over a period of more than a year. The research analyzed various visual conditions of the construction sites and classified image frames according to their visual conditions. The image frames were input into Faster R-CNN according to different visual categories. The experimental results demonstrate that the high precision, high recall and fast speed of the method can effectively detect construction workers' NHU in different construction site conditions, and can facilitate improved safety inspection and supervision.","keywords_author":["Construction safety","Deep learning","Far-field surveillance video","Faster R-CNN","Non-hardhat-use","Construction safety","Non-hardhat-use","Far-field surveillance video","Deep learning","Faster R-CNN"],"keywords_other":["Faster R-CNN","Safety inspections","MONITORING-SYSTEM","SITES","Construction workers","Construction safety","CONSTRUCTION-INDUSTRY","TRAUMATIC BRAIN-INJURIES","RECOGNITION","Surveillance video","ERGONOMICS","Construction sites","Non-hardhat-use","Detection technology"],"max_cite":6.0,"pub_year":2018.0,"sources":"['scp', 'wos']","rawkeys":["safety inspections","construction safety","recognition","surveillance video","far-field surveillance video","traumatic brain-injuries","construction sites","deep learning","non-hardhat-use","construction-industry","sites","detection technology","construction workers","faster r-cnn","ergonomics","monitoring-system"],"tags":["safety inspections","construction safety","recognition","monitoring system","far-field surveillance video","surveillance video","construction sites","traumatic brain injury","non-hardhat-use","machine learning","construction industry","sites","detection technology","construction workers","faster r-cnn","ergonomics"]},{"p_id":27672,"title":"Staff-line removal with selectional auto-encoders","abstract":"\u00a9 2017 Elsevier LtdStaff-line removal is an important preprocessing stage as regards most Optical Music Recognition systems. The common procedures employed to carry out this task involve image processing techniques. In contrast to these traditional methods, which are based on hand-engineered transformations, the problem can also be approached from a machine learning point of view if representative examples of the task are provided. We propose doing this through the use of a new approach involving auto-encoders, which select the appropriate features of an input feature set (Selectional Auto-Encoders). Within the context of the problem at hand, the model is trained to select those pixels of a given image that belong to a musical symbol, thus removing the lines of the staves. Our results show that the proposed technique is quite competitive and significantly outperforms the other state-of-art strategies considered, particularly when dealing with grayscale input images.","keywords_author":["Auto-encoders","Convolutional networks","Optical music recognition","Staff-line removal","Staff-line removal","Optical music recognition","Auto-encoders","Convolutional networks"],"keywords_other":["Convolutional networks","New approaches","MUSIC","RECOGNITION","NOTATION","Image processing technique","Auto encoders","Optical music recognition","Musical symbols","Input features","Line removal"],"max_cite":6.0,"pub_year":2017.0,"sources":"['ieee', 'scp', 'wos']","rawkeys":["new approaches","recognition","staff-line removal","musical symbols","image processing technique","auto encoders","convolutional networks","input features","optical music recognition","line removal","music","notation","auto-encoders"],"tags":["new approaches","recognition","staff-line removal","musical symbols","image processing technique","auto encoders","input features","optical music recognition","line removal","convolutional neural network","notation","music"]},{"p_id":11294,"title":"Learning representative and discriminative image representation by deep appearance and spatial coding","abstract":"How to build a suitable image representation remains a critical problem in computer vision. Traditional Bag-of-Feature (BoF) based models build image representation by the pipeline of local feature extraction, feature coding and spatial pooling. However, three major shortcomings hinder the performance, i.e., the limitation of hand-designed features, the discrimination loss in local appearance coding and the lack of spatial information. To overcome the above limitations, in this paper, we propose a generalized BoF-based framework, which is hierarchically learned by exploring recently developed deep learning methods. First, with raw images as input, we densely extract local patches and learn local features by stacked Independent Subspace Analysis network. The learned features are then transformed to appearance codes by sparse Restricted Boltzmann Machines. Second, we perform spatial max-pooling on a set of over-complete spatial regions, which is generated by covering various spatial distributions, to incorporate more flexible spatial information. Third, a structured sparse Auto-encoder is proposed to explore the region representations into the image-level signature. To learn the proposed hierarchy, we layerwise pre-train the network in unsupervised manner, followed by supervised fine-tuning with image labels. Extensive experiments on different benchmarks, i.e., UIUC-Sports, Caltech-101, Caltech-256, Scene-15 and MIT Indoor-67, demonstrate the effectiveness of our proposed model. (C) 2015 Elsevier Inc. All rights reserved.","keywords_author":["Deep learning","Image classification","Structured sparsity","Image classification","Deep learning","Structured sparsity"],"keywords_other":["Deep learning","Independent subspace analysis","Restricted boltzmann machine","Local feature extraction","FEATURES","Critical problems","CLASSIFICATION","Structured sparsities","Image representations","RECOGNITION","Spatial informations"],"max_cite":4.0,"pub_year":2015.0,"sources":"['ieee', 'scp', 'wos']","rawkeys":["recognition","features","deep learning","structured sparsities","independent subspace analysis","classification","local feature extraction","critical problems","image classification","restricted boltzmann machine","image representations","structured sparsity","spatial informations"],"tags":["recognition","features","machine learning","independent subspace analysis","classification","local feature extraction","critical problems","structured sparsity","image classification","restricted boltzmann machine","image representation","spatial informations"]},{"p_id":11315,"title":"Deep Learning for Understanding Faces Machines may be just as good, or better, than humans","abstract":null,"keywords_author":null,"keywords_other":["ALIGNMENT","WILD","RECOGNITION"],"max_cite":0.0,"pub_year":2018.0,"sources":"['wos']","rawkeys":["alignment","recognition","wild"],"tags":["alignment","recognition","wild"]},{"p_id":117812,"title":"Bird and whale species identification using sound images","abstract":"Image identification of animals is mostly centred on identifying them based on their appearance, but there are other ways images can be used to identify animals, including by representing the sounds they make with images. In this study, the authors present a novel and effective approach for automated identification of birds and whales using some of the best texture descriptors in the computer vision literature. The visual features of sounds are built starting from the audio file and are taken from images constructed from different spectrograms and from harmonic and percussion images. These images are divided into sub-windows from which sets of texture descriptors are extracted. The experiments reported in this study using a dataset of Bird vocalisations targeted for species recognition and a dataset of right whale calls targeted for whale detection (as well as three well-known benchmarks for music genre classification) demonstrate that the fusion of different texture features enhances performance. The experiments also demonstrate that the fusion of different texture features with audio features is not only comparable with existing audio signal approaches but also statistically improves some of the stand-alone audio features. The code for the experiments will be publicly available at https:\/\/www.dropbox.com\/s\/bguw035yrqz0pwp\/ElencoCode.docx?dl=0.","keywords_author":["image texture","sound images","visual features","percussion images","harmonic images","bird vocalisations","texture features","audio signal approaches","image identification"],"keywords_other":["FEATURES","MUSIC-GENRE CLASSIFICATION","ANIMALS","SYSTEM","NEURAL-NETWORKS","TEXTURE CLASSIFICATION","RECOGNITION","SIGNALS","SCALE","PATTERNS"],"max_cite":0.0,"pub_year":2018.0,"sources":"['wos']","rawkeys":["texture features","percussion images","patterns","sound images","image texture","visual features","neural-networks","features","system","image identification","music-genre classification","texture classification","recognition","harmonic images","scale","animals","signals","bird vocalisations","audio signal approaches"],"tags":["texture features","percussion images","patterns","sound images","image texture","visual feature","features","system","image identification","texture classification","recognition","neural networks","harmonic images","scale","animals","music genre classification","signals","bird vocalisations","audio signal approaches"]},{"p_id":11321,"title":"Analysis of Ribosome Stalling and Translation Elongation Dynamics by Deep Learning","abstract":"Ribosome stalling is manifested by the local accumulation of ribosomes at specific codon positions of mRNAs. Here, we present ROSE, a deep learning framework to analyze high-throughput ribosome profiling data and estimate the probability of a ribosome stalling event occurring at each genomic location. Extensive validation tests on independent data demonstrated that ROSE possessed higher prediction accuracy than conventional prediction models, with an increase in the area under the receiver operating characteristic curve by up to 18.4%. In addition, genome-wide statistical analyses showed that ROSE predictions can be well correlated with diverse putative regulatory factors of ribosome stalling. Moreover, the genome-wide ribosome stalling landscapes of both human and yeast computed by ROSE recovered the functional interplays between ribosome stalling and cotranslational events in protein biogenesis, including protein targeting by the signal recognition particles and protein secondary structure formation. Overall, our study provides a novel method to complement the ribosome profiling techniques and further decipher the complex regulatory mechanisms underlying translation elongation dynamics encoded in the mRNA sequence.","keywords_author":null,"keywords_other":["FACTOR EF-P","RNA","PROTEIN-SYNTHESIS","IN-VIVO","GENOME","SEQUENCE","NEURAL-NETWORKS","SYNONYMOUS CODON USAGE","RECOGNITION","GENE-EXPRESSION"],"max_cite":0.0,"pub_year":2017.0,"sources":"['ieee', 'wos']","rawkeys":["neural-networks","recognition","synonymous codon usage","in-vivo","rna","factor ef-p","sequence","gene-expression","protein-synthesis","genome"],"tags":["recognition","synonymous codon usage","in-vivo","neural networks","rna","factor ef-p","sequence","genomics","protein synthesis","gene expression"]},{"p_id":11325,"title":"Relevance of deep learning to facilitate the diagnosis of HER2 status in breast cancer","abstract":"Tissue biomarker scoring by pathologists is central to defining the appropriate therapy for patients with cancer. Yet, inter-pathologist variability in the interpretation of ambiguous cases can affect diagnostic accuracy. Modern artificial intelligence methods such as deep learning have the potential to supplement pathologist expertise to ensure constant diagnostic accuracy. We developed a computational approach based on deep learning that automatically scores HER2, a biomarker that defines patient eligibility for anti-HER2 targeted therapies in breast cancer. In a cohort of 71 breast tumour resection samples, automated scoring showed a concordance of 83% with a pathologist. The twelve discordant cases were then independently reviewed, leading to a modification of diagnosis from initial pathologist assessment for eight cases. Diagnostic discordance was found to be largely caused by perceptual differences in assessing HER2 expression due to high HER2 staining heterogeneity. This study provides evidence that deep learning aided diagnosis can facilitate clinical decision making in breast cancer by identifying cases at high risk of misdiagnosis.","keywords_author":null,"keywords_other":["HETEROGENEITY","VARIABILITY","ACCURACY","DIGITAL MICROSCOPY","HER-2\/NEU","NEURAL-NETWORKS","RECOGNITION","IMMUNOHISTOCHEMISTRY","DISCORDANCE","IMAGES"],"max_cite":7.0,"pub_year":2017.0,"sources":"['wos']","rawkeys":["accuracy","neural-networks","immunohistochemistry","recognition","images","variability","discordance","digital microscopy","her-2\/neu","heterogeneity"],"tags":["accuracy","immunohistochemistry","recognition","images","neural networks","variability","discordance","digital microscopy","her-2\/neu","heterogeneity"]},{"p_id":11326,"title":"Detecting anomalous events in videos by learning deep representations of appearance and motion","abstract":"Anomalous event detection is of utmost importance in intelligent video surveillance. Currently, most approaches for the automatic analysis of complex video scenes typically rely on hand-crafted appearance and motion features. However, adopting user defined representations is clearly suboptimal, as it is desirable to learn descriptors specific to the scene of interest. To cope with this need, in this paper we propose Appearance and Motion DeepNet (AMDN), a novel approach based on deep neural networks to automatically learn feature representations. To exploit the complementary information of both appearance and motion patterns, we introduce a novel double fusion framework, combining the benefits of traditional early fusion and late fusion strategies. Specifically, stacked denoising autoencoders are proposed to separately learn both appearance and motion features as well as a joint representation (early fusion). Then, based on the learned features, multiple one-class SVM models are used to predict the anomaly scores of each input. Finally, a novel late fusion strategy is proposed to combine the computed scores and detect abnormal events. The proposed ADMN is extensively evaluated on publicly available video surveillance datasets including UCSD pedestian, Subway, and Train, showing competitive performance with respect to state of the art approaches. (C) 2016 Elsevier Inc. All rights reserved.","keywords_author":["Video surveillance","Abnormal event detection","Unsupervised learning","Stacked denoising auto-encoders","Feature fusion"],"keywords_other":["RECOGNITION","PATTERNS","SUPPORT"],"max_cite":7.0,"pub_year":2017.0,"sources":"['wos']","rawkeys":["recognition","support","patterns","abnormal event detection","stacked denoising auto-encoders","unsupervised learning","video surveillance","feature fusion"],"tags":["recognition","support","patterns","unsupervised learning","video surveillance","abnormal event detections","feature fusion","stacked denoising autoencoder"]},{"p_id":52292,"title":"Plan and Goal Structure Reconstruction: An Automated and Incremental Method Based on Observation of a Single Agent","abstract":"Plan reconstruction is a task that appears in plan recognition and learning by practice. The document introduces a method of building a goal graph, which holds a reconstructed plan structure, based on analysis of data provided by observation of an agent. The approach is STRIPS-free and uses only a little knowledge about an environment. The process is automatic, thus, it does not require manual annotation of observations. The paper provides details of the developed algorithm. In the experimental study properties of a goal graph were evaluated. Possible application areas of the method are described. \u00a9 2012 IFIP International Federation for Information Processing.","keywords_author":["agent system","data analysis","hierarchical plan","machine learning","recognition","reconstruction","sub-goals"],"keywords_other":["Agent systems","recognition","Experimental studies","Manual annotation","Incremental method","Structure reconstruction","hierarchical plan","sub-goals"],"max_cite":0.0,"pub_year":2012.0,"sources":"['scp']","rawkeys":["manual annotation","agent systems","recognition","agent system","sub-goals","machine learning","experimental studies","structure reconstruction","reconstruction","hierarchical plan","data analysis","incremental method"],"tags":["manual annotation","agent systems","recognition","sub-goals","machine learning","experimental studies","structure reconstruction","reconstruction","hierarchical plan","data analysis","incremental method"]},{"p_id":11333,"title":"Benign and Malignant Lung Nodule Classification Based on Deep Learning Feature","abstract":"Classifying benign and malignant lung nodules is an important task in the diagnosis of lung cancer. In this study, lung nodules are classified based on deep learning features. A deep learning network structure is built according to the stacked generalization principle and a sparse autoencoder. This structure can simulate the human visual perception principle. The features of a region of interest are extracted from image data to identify the intrinsic characteristic that is most suitable for classification. Experimental findings show that the proposed method can extract features automatically and yield accurate classification results.","keywords_author":["CT Image","Sparse Autoencoder","Deep Learning Feature","Lung Nodule Classification"],"keywords_other":["RECOGNITION"],"max_cite":7.0,"pub_year":2015.0,"sources":"['wos']","rawkeys":["recognition","lung nodule classification","ct image","deep learning feature","sparse autoencoder"],"tags":["recognition","deep learning features","lung nodule classification","stacked autoencoders","ct images"]},{"p_id":109637,"title":"Target Reconstruction Based on 3-D Scattering Center Model for Robust SAR ATR","abstract":"This paper proposes a robust synthetic aperture radar (SAR) automatic target recognition method based on the 3-D scattering center model. The 3-D scattering center model is established offline from the CAD model of the target using a forward method, which can efficiently predict the 2-D scattering centers as well as the scattering filed of the target at arbitrary poses. For the SAR images to be classified, the 2-D scattering centers are extracted based on the attributed scattering center model and matched with the predicted scattering center set using a neighbor matching algorithm. The selected model scattering centers are used to reconstruct an SAR image based on the 3-D scattering center model, which is compared with the test image to reach a robust similarity. The designed similarity measure comprehensively considers the image correlation between the test image and the model reconstructed image and the model redundancy as for describing the test image. As for target recognition, the model with the highest similarity is determined to the target type of the test SAR image when it is denied to be an outlier. Experiments are conducted on both the data simulated by an electromagnetic code and the data measured in the moving and stationary target acquisition recognition program under standard operating condition and various extended operating conditions to validate the effectiveness and robustness of the proposed method.","keywords_author":["3-D scattering center model","automatic target recognition (ATR)","neighbor matching","reconstruction","synthetic aperture radar (SAR)"],"keywords_other":["REPRESENTATION","RADAR IMAGES","RECOGNITION","3D"],"max_cite":0.0,"pub_year":2018.0,"sources":"['ieee', 'wos']","rawkeys":["3-d scattering center model","recognition","synthetic aperture radar (sar)","neighbor matching","radar images","representation","automatic target recognition (atr)","3d","reconstruction"],"tags":["radar imaging","3-d scattering center model","recognition","synthetic aperture radar","neighbor matching","representation","automatic target recognition (atr)","reconstruction","three-dimensional"]},{"p_id":11342,"title":"Deep Learning Based Regression and Multiclass Models for Acute Oral Toxicity Prediction with Automatic Chemical Feature Extraction","abstract":"Median lethal death, LD50, is a general indicator of compound acute oral toxicity (AOT). Various in silico methods were developed for AOT prediction to reduce costs and time. In this study, we developed an improved molecular graph encoding convolutional neural networks (MGE-CNN) architecture to construct three types of high-quality AOT models: regression model (deepAOT-R), multiclassification model (deepAOT-C), and multitask model (deepAOT-CR). These predictive models highly outperformed previously reported models. For the two external data sets containing 1673 (test set I) and 375 (test set II) compounds, the R-2 and mean absolute errors (MAEs) of deepAOT-R on the test set I were 0.864 and 0.195, and the prediction accuracies of deepAOT-C were 95.5% and 96.3% on test sets I and II, respectively. The two external prediction accuracies of deepAOT-CR are 95.0% and 94.1%, while the R-2 and MAE are 0.861 and 0.204 for test set I, respectively. We then performed forward and backward exploration of deepAOT models for deep fingerprints, which could support shallow machine learning methods more efficiently than traditional fingerprints or descriptors, We further performed automatic feature learning, a key essence of deep learning, to map the corresponding activation values into fragment space and derive AOT-related chemical substructures by reverse mining of the features. Our deep learning architecture for AOT is generally applicable in predicting and exploring other toxicity or property end points of chemical compounds. The two deepAOT models are freely available at http:\/\/repharma.pku.edu.cn\/DLAOT\/DLAOThome.php or http:\/\/w-ww.pkumdl.cn\/DLAOT\/ DLAOThome.php.","keywords_author":null,"keywords_other":["Multi-classification","IDENTIFICATION","Machine Learning","Mean absolute error","MACHINE","Learning architectures","MOLECULAR-STRUCTURE","AQUEOUS SOLUBILITY","BIOLOGICAL-ACTIVITY","Forward-and-backward","STRUCTURAL ALERTS","RECOGNITION","Machine learning methods","Administration, Oral","Toxicity Tests","Models, Statistical","RATS","DRUG DISCOVERY","Informatics","Convolutional neural network","Toxicity predictions","Prediction accuracy","DESCRIPTORS","Automation","Regression Analysis","Lethal Dose 50"],"max_cite":3.0,"pub_year":2017.0,"sources":"['scp', 'wos']","rawkeys":["learning architectures","statistical","identification","administration","regression analysis","convolutional neural network","multi-classification","toxicity tests","molecular-structure","rats","toxicity predictions","machine","machine learning","lethal dose 50","prediction accuracy","models","oral","descriptors","structural alerts","recognition","drug discovery","aqueous solubility","forward-and-backward","biological-activity","informatics","machine learning methods","automation","mean absolute error"],"tags":["learning architectures","identification","automated","administration","regression analysis","convolutional neural network","multi-classification","toxicity tests","toxicity prediction","machine","machine learning","lethal dose 50","prediction accuracy","oral","descriptors","structural alerts","rat","recognition","drug discovery","aqueous solubility","forward-and-backward","biological-activity","informatics","machine learning methods","statistics","model","molecular structure","mean absolute error"]},{"p_id":27732,"title":"Learning 3D faces from 2D images via Stacked Contractive Autoencoder","abstract":"\u00a9 2017 3D face reconstruction from a 2D face image has been found important to various applications such as face detection and recognition because a 3D face provides more semantic information than 2D image. This paper proposes a deep learning framework for 3D face reconstruction. The framework is designed to compute subspace feature of arbitrary face image, then map the feature to its counterpart in another subspace learned with 3D faces, and reconstruct the 3D face using the counterpart feature. During the course of training, we learn 2D and 3D subspaces through Stacked Contractive Autoencoders (SCAE), use a one-layer fully connected neural network to learn the mapping, and use the pre-trained parameters of the SCAEs and the one-layer network to initialize a deep feedforward neural network whose input are face images and output are 3D faces. The network is optimized by gradient descent algorithm with back-propagation. Extensive experimental results on various data sets indicate the effectiveness of the proposed SCAE-based 3D face reconstruction method.","keywords_author":["3D face reconstruction","Deep learning","Neural network","Stacked Contractive Autoencoder","Subspace","Deep learning","Stacked Contractive Autoencoder","Subspace","Neural network","3D face reconstruction"],"keywords_other":["Face detection and recognition","MORPHABLE MODEL","DEEP","Fully connected neural network","REPRESENTATION","SINGLE IMAGE","CONSTRAINTS","NETWORKS","3D face reconstruction","Learning frameworks","RECOGNITION","Auto encoders","SHAPE","Subspace","RECONSTRUCTION","Gradient descent algorithms","Semantic information"],"max_cite":5.0,"pub_year":2017.0,"sources":"['ieee', 'scp', 'wos']","rawkeys":["neural network","3d face reconstruction","deep","gradient descent algorithms","reconstruction","constraints","stacked contractive autoencoder","fully connected neural network","face detection and recognition","semantic information","subspace","recognition","deep learning","learning frameworks","networks","morphable model","single image","auto encoders","representation","shape"],"tags":["single images","3d face reconstruction","deep","gradient descent algorithms","reconstruction","constraints","stacked contractive autoencoder","machine learning","fully connected neural network","face detection and recognition","semantic information","subspace","recognition","neural networks","learning frameworks","networks","morphable model","auto encoders","representation","shape"]},{"p_id":109659,"title":"Hybrid tracking model and GSLM based neural network for crowd behavior recognition","abstract":"Crowd behaviors analysis is the 'state of art' research topic in the field of computer vision which provides applications in video surveillance to crowd safety, event detection, security, etc. Literature presents some of the works related to crowd behavior detection and analysis. In crowd behavior detection, varying density of crowds and motion patterns appears to be complex occlusions for the researchers. This work presents a novel crowd behavior detection system to improve these restrictions. The proposed crowd behavior detection system is developed using hybrid tracking model and integrated features enabled neural network. The object movement and activity in the proposed crowded behavior detection system is assessed using proposed GSLM-based neural network. GSLM based neural network is developed by integrating the gravitational search algorithm with LM algorithm of the neural network to increase the learning process of the network. The performance of the proposed crowd behavior detection system is validated over five different videos and analyzed using accuracy. The experimentation results in the crowd behavior detection with a maximum accuracy of 93% which proves the efficacy of the proposed system in video surveillance with security concerns.","keywords_author":["crowd video","crowd behavior","tracking","recognition","neural network","gravitational search algorithm"],"keywords_other":null,"max_cite":0.0,"pub_year":2017.0,"sources":"['wos']","rawkeys":["neural network","crowd video","recognition","crowd behavior","tracking","gravitational search algorithm"],"tags":["gravitational search algorithms","crowd video","recognition","crowd behavior","neural networks","tracking"]},{"p_id":11358,"title":"Matching Software-Generated Sketches to Face Photographs with a Very Deep CNN, Morphed Faces, and Transfer Learning","abstract":"Sketches obtained from eyewitness descriptions of criminals have proven to be useful in apprehending criminals, particularly when there is a lack of evidence. Automated methods to identify subjects depicted in sketches have been proposed in the literature, but their performance is still unsatisfactory when using software-generated sketches and when tested using extensive galleries with a large amount of subjects. Despite the success of deep learning in several applications including face recognition, little work has been done in applying it for face photograph-sketch recognition. This is mainly a consequence of the need to ensure robust training of deep networks by using a large number of images, yet limited quantities are publicly available. Moreover, most algorithms have not been designed to operate on software-generated face composite sketches which are used by numerous law enforcement agencies worldwide. This paper aims to tackle these issues with the following contributions: 1) a very deep convolutional neural network is utilised to determine the identity of a subject in a composite sketch by comparing it to face photographs and is trained by applying transfer learning to a state-of-the-art model pretrained for face photograph recognition; 2) a 3-D morphable model is used to synthesise both photographs and sketches to augment the available training data, an approach that is shown to significantly aid performance; and 3) the UoM-SGFS database is extended to contain twice the number of subjects, now having 1200 sketches of 600 subjects. An extensive evaluation of popular and state-of-the-art algorithms is also performed due to the lack of such information in the literature, where it is demonstrated that the proposed approach comprehensively outperforms state-of-the-art methods on all publicly available composite sketch datasets.","keywords_author":["augmentation","convolutional neural network","database","Deep learning","face photos","morphological model","software-generated composite sketches","Deep learning","convolutional neural network","software-generated composite sketches","face photos","morphological model","augmentation","database"],"keywords_other":["augmentation","Algorithm design and analysis","face photos","Morphological model","RECOGNITION","ALGORITHMS","Face","Convolutional neural network"],"max_cite":0.0,"pub_year":2018.0,"sources":"['ieee', 'scp', 'wos']","rawkeys":["augmentation","recognition","deep learning","morphological model","face photos","software-generated composite sketches","database","convolutional neural network","algorithm design and analysis","algorithms","face"],"tags":["augmentation","recognition","databases","morphological model","face photos","machine learning","software-generated composite sketches","convolutional neural network","algorithm design and analysis","algorithms","face"]},{"p_id":11368,"title":"Deep learning-based banknote fitness classification using the reflection images by a visible-light one-dimensional line image sensor","abstract":"In automatic paper currency sorting, fitness classification is a technique that assesses the quality of banknotes to determine whether a banknote is suitable for recirculation or should be replaced. Studies on using visible-light reflection images of banknotes for evaluating their usability have been reported. However, most of them were conducted under the assumption that the denomination and input direction of the banknote are predetermined. In other words, a pre-classification of the type of input banknote is required. To address this problem, we proposed a deep learning-based fitness-classification method that recognizes the fitness level of a banknote regardless of the denomination and input direction of the banknote to the system, using the reflection images of banknotes by visible-light one-dimensional line image sensor and a convolutional neural network (CNN). Experimental results on the banknote image databases of the Korean won (KRW) and the Indian rupee (INR) with three fitness levels, and the Unites States dollar (USD) with two fitness levels, showed that our method gives better classification accuracy than other methods.","keywords_author":["Convolutional neural network","Deep learning","Fitness classification","Reflection images of banknote","Visible-light one-dimensional line image sensor","fitness classification","deep learning","reflection images of banknote","visible-light one-dimensional line image sensor","convolutional neural network"],"keywords_other":["Recirculations","Classification methods","Paper currency","NETWORKS","Unites state","RECOGNITION","Convolutional neural network","Visible light","Banknote images","Classification accuracy"],"max_cite":0.0,"pub_year":2018.0,"sources":"['scp', 'wos']","rawkeys":["recirculations","visible-light one-dimensional line image sensor","recognition","classification methods","banknote images","deep learning","reflection images of banknote","classification accuracy","networks","fitness classification","convolutional neural network","visible light","paper currency","unites state"],"tags":["recirculations","visible-light one-dimensional line image sensor","recognition","classification methods","banknote images","reflection images of banknote","classification accuracy","machine learning","networks","united-states","fitness classification","convolutional neural network","visible light","paper currency"]},{"p_id":85099,"title":"Data-driven vs. model-driven: Fast face sketch synthesis","abstract":"Face sketch synthesis refers to the technique generating a sketch from an input photo. Existing methods are data-driven, which synthesize a sketch by linearly combining K candidate sketch patches which are purposely selected from the training data. However, these methods have large computation cost due to neighbor selection process that perform neighbor searching on a large scale of training image patches. Instead of the aforementioned commonly used data-driven strategy, we propose to learn some models from training photos to training sketches which could speed the synthesis process a lot while preserving comparable or even better synthesis performance. Specially, we learn some ridge regressors from training photo patch intensities to training sketch patch intensities. An initial estimation is obtained from these regressors. Simultaneously, a high-frequency image is hallucinated from some ridge regressors which are learned from the high-frequency information of training photos and sketches. The high-frequency image is superimposed to the initial estimation to compensate the filtered details due to the dense average in the initial estimation process. Extensive experiments on public face sketch database illustrate the effectiveness of proposed model-driven strategy. (C) 2017 Elsevier B.V. All rights reserved.","keywords_author":["Face sketch synthesis","Model-driven","Data-driven","Face recognition","Image quality assessment"],"keywords_other":["PHOTO SYNTHESIS","RECOGNITION","IMAGE QUALITY ASSESSMENT"],"max_cite":2.0,"pub_year":2017.0,"sources":"['ieee', 'wos']","rawkeys":["image quality assessment","model-driven","recognition","face sketch synthesis","data-driven","face recognition","photo synthesis"],"tags":["image quality assessment","photosynthesis","model-driven","recognition","face sketch synthesis","face recognition","data driven"]},{"p_id":85104,"title":"Back projection: An effective postprocessing method for GAN-based face sketch synthesis","abstract":"We consider the image transformation problems in this paper, where an input face photo is transformed into a sketch, i.e. face sketch synthesis. It plays important role in video surveillance-based law enforcement. Recent methods for such problems typically train feed-forward convolutional neural networks (CNN) or graphical probabilistic models. In this paper, inspired by the recent success in generating images of generative adversarial networks (GAN), we employ GAN to perform this task. However, accompanying with fine textures generated by GAN model, noise appears among the generated results. We proposed a back projection method to reconstruct the synthesized results. Extensive experiments on public face databases illustrate the effectiveness and superiority of the proposed method compared with state-of-the-art methods. The proposed back projection strategy can be extended to other GAN-based image-to-image translation problems. Data and implementation code in this paper are available online at www.ihitworld.com\/WNN\/Back_Projection.zip. (C) 2017 Elsevier B.V. All rights reserved.","keywords_author":["Face sketch recognition","Back projection","Generative adversarial networks"],"keywords_other":["PHOTO SYNTHESIS","RECOGNITION","INFORMATION"],"max_cite":2.0,"pub_year":2017.0,"sources":"['ieee', 'wos']","rawkeys":["recognition","face sketch recognition","generative adversarial networks","back projection","information","photo synthesis"],"tags":["photosynthesis","recognition","face sketch recognition","generative adversarial networks","back projection","information"]},{"p_id":93297,"title":"Recognizing physical contexts of mobile video learners via smartphone sensors","abstract":"Current studies can effectively recognize several human activities in a single semantic context, but don't recognize the semantics of a single activity in different contexts. The main challenge is the conflicting phone usages as well as the special requirements of the energy consumption. This paper tests a classic learning scenario regarding mobile video viewing and validates the proposed recognition method by comprehensively taking the recognizing accuracy, effectiveness and the energy consumption into consideration. Readings of four carefully-selected sensors are collected and a wide range of machine learning algorithms are investigated. The results show the combination of accelerometer, light and sound sensors is better than that of acceleration, light and gyroscope sensors, the features with respect to energy spectral don't improve the recognition accuracy, and the system reaches robustness in a few minutes. The proposed method is simple, effective and practical in real applications of pervasive learning. (C) 2017 Elsevier B.V. All rights reserved.","keywords_author":["Physical context","Smartphone sensors","Context recognition","Mobile video learners"],"keywords_other":["RECOGNITION","FEATURE-SELECTION","SYSTEMS","CLASSIFICATION"],"max_cite":0.0,"pub_year":2017.0,"sources":"['wos']","rawkeys":["physical context","recognition","mobile video learners","feature-selection","smartphone sensors","classification","context recognition","systems"],"tags":["physical context","recognition","mobile video learners","system","feature selection","smartphone sensors","classification","context recognition"]},{"p_id":11382,"title":"A New Generalized Deep Learning Framework Combining Sparse Autoencoder and Taguchi Method for Novel Data Classification and Processing","abstract":"Deep autoencoder neural networks have been widely used in several image classification and recognition problems, including hand-writing recognition, medical imaging, and face recognition. The overall performance of deep autoencoder neural networks mainly depends on the number of parameters used, structure of neural networks, and the compatibility of the transfer functions. However, an inappropriate structure design can cause a reduction in the performance of deep autoencoder neural networks. A novel framework, which primarily integrates the Taguchi Method to a deep autoencoder based system without considering to modify the overall structure of the network, is presented. Several experiments are performed using various data sets from different fields, i.e., network security and medicine. The results show that the proposed method is more robust than some of the well-known methods in the literature as most of the time our method performed better. Therefore, the results are quite encouraging and verified the overall performance of the proposed framework.","keywords_author":null,"keywords_other":["ARTIFICIAL NEURAL-NETWORK","EEG","REGRESSION","EPILEPSY","DESIGN","SYSTEM","MODEL","RECOGNITION"],"max_cite":0.0,"pub_year":2018.0,"sources":"['wos']","rawkeys":["design","recognition","model","epilepsy","artificial neural-network","eeg","system","regression"],"tags":["design","recognition","model","neural networks","epilepsy","eeg","system","regression"]},{"p_id":27768,"title":"Interrogating Feature Learning Models to Discover Insights Into the Development of Human Expertise in a Real-Time, Dynamic Decision-Making Task","abstract":"Copyright \u00a9 2016 Cognitive Science Society, Inc. Tetris provides a difficult, dynamic task environment within which some people are novices and others, after years of work and practice, become extreme experts. Here we study two core skills; namely, (a) choosing the goal or objective function that will maximize performance and (b)a feature-based analysis of the current game board to determine where to place the currently falling zoid (i.e., Tetris piece) so as to maximize the goal. In Study 1, we build cross-entropy reinforcement learning (CERL) models (Szita & Lorincz, 2006) to determine whether different goals result in different feature weights. Two of these optimization strategies quickly rise to performance plateaus, whereas two others continue toward higher but more jagged (i.e., variable) heights. In Study 2, we compare the zoid placement decisions made by our best CERL models with those made by 67 human players. Across 370,131 human game episodes, two CERL models picked the same zoid placements as our lowest scoring human for 43% of the placements and as our three best scoring experts for 65% of the placements. Our findings suggest that people focus on maximizing points, not number of lines cleared or number of levels reached. They also show that goal choice influences the choice of zoid placements for CERLs and suggest that the same is true of humans. Tetris has a repetitive task structure that makes Tetris more tractable and more like a traditional experimental psychology paradigm than many more complex games or tasks. Hence, although complex, Tetris is not overwhelmingly complex and presents a right-sized challenge to cognitive theories, especially those of integrated cognitive systems.","keywords_author":["Cognitive skill","Cross-entropy reinforcement learning","Expertise","Experts","Machine learning","Methods","Perceptual learning","Strategies","Tetris"],"keywords_other":["Decision Making","Learning","Humans","Reinforcement (Psychology)","Problem Solving"],"max_cite":5.0,"pub_year":2017.0,"sources":"['scp']","rawkeys":["decision making","tetris","expertise","reinforcement (psychology)","experts","cross-entropy reinforcement learning","learning","machine learning","humans","methods","problem solving","strategies","perceptual learning","cognitive skill"],"tags":["decision making","tetris","expertise","recognition","experts","cross-entropy reinforcement learning","machine learning","problem solving","humans","methods","strategies","perceptual learning","cognitive skill"]},{"p_id":19583,"title":"HACDB: Handwritten Arabic characters database for automatic character recognition","abstract":"Automatic off-line Arabic handwriting recognition based on segmentation still faces big challenges. A database, covering all shapes of handwritten Arabic characters, is required to facilitate the recognition process. This paper introduces a new database for handwritten Arabic characters (HACDB), designed to cover all shapes of Arabic characters including overlapping ones. It contains 6,600 shapes of characters written by 50 writers. This database can be used for training and testing the words for their recognition after segmentation. Also, it presents the possibility for comparing different approaches and evaluate their accuracy on a common base. \u00a9 2013 University Paris 13.","keywords_author":["Arabic","character","database","recognition"],"keywords_other":["Recognition process","recognition","Arabic","Arabic characters","Training and testing","Arabic handwriting recognition","Common-base","character"],"max_cite":20.0,"pub_year":2013.0,"sources":"['ieee', 'scp', 'wos']","rawkeys":["arabic handwriting recognition","arabic characters","recognition","database","training and testing","arabic","character","recognition process","common-base"],"tags":["arabic handwriting recognition","arabic characters","recognition","databases","training and testing","arabic","character","recognition process","common-base"]},{"p_id":76927,"title":"Perception Science in the Age of Deep Neural Networks","abstract":null,"keywords_author":["perception","neuroscience","psychology","neural networks","deep learning","artificial intelligence"],"keywords_other":["NEUROSCIENCE","EXPERTISE","OBJECT RECOGNITION","VISION","ALGORITHM","REPRESENTATIONS","VENTRAL STREAM","FUSIFORM FACE AREA","CORTEX","BRAIN"],"max_cite":3.0,"pub_year":2017.0,"sources":"['ieee', 'wos']","rawkeys":["algorithm","artificial intelligence","neuroscience","expertise","fusiform face area","ventral stream","deep learning","neural networks","object recognition","brain","psychology","vision","perception","representations","cortex"],"tags":["ventral stream","neuroscience","expertise","fusiform face area","recognition","neural networks","machine learning","representation","object recognition","brain","vision","perceptions","algorithms","cortex"]},{"p_id":85119,"title":"Pairwise Identity Verification via Linear Concentrative Metric Learning","abstract":"This paper presents a study of metric learning systems on pairwise identity verification, including pairwise face verification and pairwise speaker verification, respectively. These problems are challenging because the individuals in training and testing are mutually exclusive, and also due to the probable setting of limited training data. For such pairwise verification problems, we present a general framework of metric learning systems and employ the stochastic gradient descent algorithm as the optimization solution. We have studied both similarity metric learning and distance metric learning systems, of either a linear or shallow nonlinear model under both restricted and unrestricted training settings. Extensive experiments demonstrate that with limited training pairs, learning a linear system on similar pairs only is preferable due to its simplicity and superiority, i.e., it generally achieves competitive performance on both the labeled faces in the wild face dataset and the NIST speaker dataset. It is also found that a pretrained deep nonlinear model helps to improve the face verification results significantly.","keywords_author":["Face verification","identity verification","metric learning","pairwise metric","siamese neural networks","speaker verification"],"keywords_other":["COMPRESSION","SPEAKER VERIFICATION","RANKING","CLASSIFICATION","SUPPORT VECTOR MACHINES","NEURAL-NETWORKS","RECOGNITION","DESCRIPTORS","FACE VERIFICATION","SIMILARITY"],"max_cite":0.0,"pub_year":2018.0,"sources":"['ieee', 'wos']","rawkeys":["neural-networks","compression","identity verification","pairwise metric","recognition","similarity","speaker verification","face verification","classification","siamese neural networks","support vector machines","ranking","descriptors","metric learning"],"tags":["pairwise metric","identity verification","siamese neural network","recognition","neural networks","codes","standards","similarity","machine learning","speaker verification","face verification","classification","descriptors","metric learning"]},{"p_id":85122,"title":"Self-learning for face clustering","abstract":"In this paper, we simulate the learning way of human to propose a self-learning framework for face clustering. Specifically, we first perform a decorrelation operation on face images through patch-based two-dimensional reconstruction, which has a similar function to the retina. Then we group the semantically similar faces by using a novel self-paced learning model, which is inspired by three major observations: (i) The learning process of human gradually proceeds from easy to complex tasks; (ii) The prior knowledge of human might change with the increase of learned experience; (iii) More prior knowledge usually leads to better prediction accuracy. Experiments on benchmark face databases demonstrate the effectiveness and efficiency of the proposed framework. (C) 2018 Elsevier Ltd. All rights reserved.","keywords_author":["Face clustering","Patch-based two-dimensional reconstruction","Self-paced learning"],"keywords_other":["REPRESENTATION","RECOGNITION","EIGENFACES"],"max_cite":0.0,"pub_year":2018.0,"sources":"['ieee', 'wos']","rawkeys":["recognition","self-paced learning","eigenfaces","face clustering","representation","patch-based two-dimensional reconstruction"],"tags":["recognition","self-paced learning","eigenfaces","face clustering","representation","patch-based two-dimensional reconstruction"]},{"p_id":85126,"title":"Deep variance network: An iterative, improved CNN framework for unbalanced training datasets","abstract":"Convolutional neural network (CNN) has demonstrated its superior ability to achieve amazing accuracy in computer vision field. Nevertheless, for practical domain-specific image recognition tasks, it still remains difficult to obtain massive high-quality labeled datasets due to the strong requirements for extensive, tedious manual processing. Inspired by the well-known observation that human brain can accurately recognize objects without relying on massive congeneric examples, we propose a novel deep variance network (DVN) to further enhance the generalization ability of CNN in this paper, which could still produce higher recognition accuracy even with unbalanced training datasets than original CNN. The key idea of our' DVN is built upon the intrinsic exploitation of inter-class homogeneity and intra-class heterogeneity. Towards such goal, we make the first attempt to incorporate a hierarchical Bayesian model into the powerful CNN framework, which can transfer the joint feature distribution from certain object's complete training dataset to other object's incomplete training dataset in an iterative way. In each training cycle, the CNN resulted features are clustered into discrimination-related subspaces to guide the learning and adaptive adjustment of homogeneity and heterogeneity over unbalanced training datasets. In practice, we furnish several state-of-the-art deep networks with our proposed DVN, and conduct extensive experiments and comprehensive evaluations over CIFAR-10, MNIST, and SVHN benchmarks. The experiments have shown that, most of the furnished deep networks can benefit from our DVN, wherein they gain at most 6.9% accuracy improvement over CIFAR-10 benchmark, 52.83% error reduction over MNIST benchmark, and an improvement of 6.2% over SVHN datasets. (C) 2018 Elsevier Ltd. All rights reserved.","keywords_author":["Deep variance network","Unbalanced training datasets","Convolutional neural network","Homogeneity","Heterogeneity"],"keywords_other":["SELECTION","RECOGNITION","SALIENCY DETECTION","CLASSIFICATION"],"max_cite":0.0,"pub_year":2018.0,"sources":"['wos']","rawkeys":["recognition","unbalanced training datasets","saliency detection","homogeneity","classification","convolutional neural network","deep variance network","selection","heterogeneity"],"tags":["recognition","unbalanced training datasets","saliency detection","homogeneity","classification","convolutional neural network","deep variance network","selection","heterogeneity"]},{"p_id":85129,"title":"High-frequency details enhancing DenseNet for super-resolution","abstract":"Convolutional neural networks based models have made impressive advances for single-image super-resolution task. To advance the reconstruction quality of high-frequency details of the images, which are difficult to recover in super-resolution task, this paper proposes a super-resolution method using a high-frequency information enhancing densely connected convolutional neural network (SRDN) which can make the network pay more attention to high-frequency regions' reconstruction like edges and textures during training. Our method applies relatively higher weights on the gradient descent values of these high-frequency regions' pixels before they are propagated backward to update the parameters of the network during training. After that, we use a Generative Adversarial Network to finetune the trained model for finer texture details and more photo-realistic results. Experiments show that our approach can achieve a significant boost in the reconstruction quality of high-frequency details at high magnification ratios. We also design a novel measurement to evaluate the high-frequency details' difference (HFD) between the ground truth image and the generated image. (c) 2018 Elsevier B.V. All rights reserved.","keywords_author":["Super-resolution","CNN","Details enhancing","GAN"],"keywords_other":["QUALITY ASSESSMENT","SINGLE IMAGE SUPERRESOLUTION","RECOGNITION"],"max_cite":0.0,"pub_year":2018.0,"sources":"['ieee', 'wos']","rawkeys":["recognition","details enhancing","gan","quality assessment","cnn","single image superresolution","super-resolution"],"tags":["recognition","generative adversarial networks","sparse representation","detail enhancement","information retrieval","convolutional neural network"]},{"p_id":76938,"title":"A Shared Vision for Machine Learning in Neuroscience","abstract":"With ever-increasing advancements in technology, neuroscientists are able to collect data in greater volumes and with finer resolution. The bottleneck in understanding how the brain works is consequently shifting away from the amount and type of data we can collect and toward what we actually do with the data. There has been a growing interest in leveraging this vast volume of data across levels of analysis, measurement techniques, and experimental paradigms to gain more insight into brain function. Such efforts are visible at an international scale, with the emergence of big data neuroscience initiatives, such as the BRAIN initiative (Bargmann et al., 2014), the Human Brain Project, the Human Connectome Project, and the National Institute of Mental Health's Research Domain Criteria initiative. With these large-scale projects, much thought has been given to data-sharing across groups (Poldrack and Gorgolewski, 2014; Sejnowski et al., 2014); however, even with such data-sharing initiatives, funding mechanisms, and infrastructure, there still exists the challenge of how to cohesively integrate all the data. At multiple stages and levels of neuroscience investigation, machine learning holds great promise as an addition to the arsenal of analysis tools for discovering how the brain works.","keywords_author":["machine learning","reinforcement learning","explainable artificial intelligence"],"keywords_other":["DEPRESSION","REINFORCEMENT","BRAIN IMAGES","STIMULATION","BIG DATA","INDEPENDENT COMPONENT ANALYSIS","MEMORY","REGISTRATION","CORTEX","OPTIMIZATION"],"max_cite":1.0,"pub_year":2018.0,"sources":"['wos']","rawkeys":["memory","big data","machine learning","reinforcement","reinforcement learning","stimulation","independent component analysis","cortex","depression","optimization","registration","brain images","explainable artificial intelligence"],"tags":["recognition","memory","big data","machine learning","reinforcement learning","stimulation","independent component analysis","brain imaging","depression","optimization","registration","cortex","explainable artificial intelligence"]},{"p_id":35981,"title":"An Efficient Approach to Power System Uncertainty Analysis With High-Dimensional Dependencies","abstract":"\u00a9 2017 IEEE. The integration of high penetration of renewable energy brings greater uncertainties for the operation of future power systems due to its intermittency and lack of predictability. The uncertainties brought by wide scale renewables might have dependencies with each other because their outputs are mainly influenced by weather. However, an analysis of such uncertainties with complex dependencies faces the \"curse of dimensionality\". This challenges the power system uncertainty analysis in probabilistic forecasting, power system operation optimization, and power system planning. This paper proposes an efficient approach that is able to handle high-dimensional dependencies. The approach uses the high-dimensional Copula theory and discrete convolution method to conduct a high-dimensional dependent discrete convolution (DDC) calculation. A recursive algorithm is proposed to decompose the computation of DDC into multiple convolutions between each pair of stochastic variables so that the \"curse of dimensionality\" is solved. The computational complexity of the proposed method is linear with respect to the number of dimensions and guarantees computational efficiency. Finally, illustrative examples of power system reserve requirement evaluation and wind power capacity credit assessment analysis are used to verify the effectiveness and superiority of the proposed approach.","keywords_author":["Copula","correlation","dependent","discrete convolution","high dimension","renewable energy","uncertainty"],"keywords_other":["Uncertainty","dependent","Load modeling","Renewable energies","Copula","Discrete convolution","High dimensions"],"max_cite":2.0,"pub_year":2018.0,"sources":"['scp', 'wos']","rawkeys":["high dimensions","high dimension","dependent","uncertainty","discrete convolution","copula","load modeling","renewable energy","correlation","renewable energies"],"tags":["high dimensions","recognition","uncertainty","discrete convolution","copula","load modeling","correlation","renewable energies"]},{"p_id":109710,"title":"Multilevel image segmentation with adaptive image context based thresholding","abstract":"Neural network based image segmentation techniques primarily focus on the selection of appropriate thresholding points in the image feature space. Research initiatives in this direction aim at addressing this problem of effective threshold selection for activation functions. Multilevel activation functions resort to fixed and uniform thresholding mechanisms. These functions assume homogeneity of the image information content. In this paper, we propose a collection of adaptive thresholding approaches to multilevel activation functions. The proposed thresholding mechanisms incorporate the image context information in the thresholding process. Applications of these mechanisms are demonstrated on the segmentation of real life multilevel intensity images using a self-supervised multilayer self-organizing neural network (MLSONN) and a supervised pyramidal neural network (PyraNet).","keywords_author":["Multilevel image segmentation","Multilevel activation function","Multilevel thresholding mechanisms","Self-organizing neural networks","Pyramidal neural network","Context sensitive thresholding"],"keywords_other":["HOPFIELD NEURAL-NETWORK","REMOTELY-SENSED IMAGES","CLASSIFICATION","ALGORITHM","QUANTITATIVE-EVALUATION","RECOGNITION","HISTOGRAM","ENTROPY"],"max_cite":19.0,"pub_year":2011.0,"sources":"['wos']","rawkeys":["multilevel thresholding mechanisms","algorithm","recognition","histogram","hopfield neural-network","classification","multilevel image segmentation","pyramidal neural network","quantitative-evaluation","remotely-sensed images","entropy","multilevel activation function","self-organizing neural networks","context sensitive thresholding"],"tags":["multilevel thresholding mechanisms","recognition","remote sensing images","histograms","classification","multilevel image segmentation","pyramidal neural network","algorithms","quantitative evaluation","entropy","multilevel activation function","self-organizing neural networks","context sensitive thresholding","hopfield neural network"]},{"p_id":93332,"title":"High-Order Resting-State Functional Connectivity Network for MCI Classification","abstract":"Brain functional connectivity ( FC) network, estimated with resting-state functional magnetic resonance imaging ( RS-fMRI) technique, has emerged as a promising approach for accurate diagnosis of neurodegenerative diseases. However, the conventional FC network is essentially low-order in the sense that only the correlations among brain regions ( in terms of RS-fMRI time series) are taken into account. The features derived from this type of brain network may fail to serve as an effective disease biomarker. To overcome this drawback, we propose extraction of novel high-order FC correlations that characterize how the low-order correlations between different pairs of brain regions interact with each other. Specifically, for each brain region, a sliding window approach is first performed over the entire RS-fMRI time series to generate multiple short overlapping segments. For each segment, a low-order FC network is constructed, measuring the short-term correlation between brain regions. These loworder networks ( obtained from all segments) describe the dynamics of short-term FC along the time, thus also forming the correlation time series for every pair of brain regions. To overcome the curse of dimensionality, we further group the correlation time series into a small number of different clusters according to their intrinsic common patterns. Then, the correlation between the respective mean correlation time series of different clusters is calculated to represent the high-order correlation among different pairs of brain regions. Finally, we design a pattern classifier, by combining features of both loworder and high-order FC networks. Experimental results verify the effectiveness of the high-order FC network on disease diagnosis. (C) 2016 Wiley Periodicals, Inc.","keywords_author":["mild cognitive impairment","functional connectivity","low-order and high-order networks","brain disease diagnosis"],"keywords_other":["DIAGNOSIS","INVERSE COVARIANCE ESTIMATION","REPRESENTATION","DEMENTIA","BIOMARKERS","RECOGNITION","MILD COGNITIVE IMPAIRMENT","ALZHEIMERS-DISEASE","BRAIN CONNECTIVITY","GRAPHICAL LASSO"],"max_cite":17.0,"pub_year":2016.0,"sources":"['ieee', 'wos']","rawkeys":["diagnosis","functional connectivity","graphical lasso","low-order and high-order networks","recognition","alzheimers-disease","biomarkers","mild cognitive impairment","dementia","representation","brain connectivity","inverse covariance estimation","brain disease diagnosis"],"tags":["diagnosis","functional connectivity","graphical lasso","low-order and high-order networks","recognition","alzheimers-disease","biomarkers","mild cognitive impairment","dementia","representation","brain connectivity","inverse covariance estimation","brain disease diagnosis"]},{"p_id":76960,"title":"Content-Adaptive Sketch Portrait Generation by Decompositional Representation Learning","abstract":"Sketch portrait generation benefits a wide range of applications such as digital entertainment and law enforcement. Although plenty of efforts have been dedicated to this task, several issues still remain unsolved for generating vivid and detail-preserving personal sketch portraits. For example, quite a few artifacts may exist in synthesizing hairpins and glasses, and textural details may be lost in the regions of hair or mustache. Moreover, the generalization ability of current systems is somewhat limited since they usually require elaborately collecting a dictionary of examples or carefully tuning features\/ components. In this paper, we present a novel representation learning framework that generates an end-to-end photo-sketch mapping through structure and texture decomposition. In the training stage, we first decompose the input face photo into different components according to their representational contents (i.e., structural and textural parts) by using a pre-trained convolutional neural network (CNN). Then, we utilize a branched fully CNN for learning structural and textural representations, respectively. In addition, we design a sorted matching mean square error metric to measure texture patterns in the loss function. In the stage of sketch rendering, our approach automatically generates structural and textural representations for the input photo and produces the final result via a probabilistic fusion scheme. Extensive experiments on several challenging benchmarks suggest that our approach outperforms example-based synthesis algorithms in terms of both perceptual and objective metrics. In addition, the proposed method also has better generalization ability across data set without additional training.","keywords_author":["Sketch generation","representation learning","fully convolutional network"],"keywords_other":["MODEL","RETRIEVAL","RECOGNITION","SPARSE REPRESENTATION","PHOTO SYNTHESIS","SIMILARITY"],"max_cite":3.0,"pub_year":2017.0,"sources":"['ieee', 'wos']","rawkeys":["recognition","model","sketch generation","similarity","sparse representation","fully convolutional network","representation learning","retrieval","photo synthesis"],"tags":["photosynthesis","recognition","model","sketch generation","similarity","sparse representation","fully convolutional network","representation learning","retrieval"]},{"p_id":27811,"title":"A theory of local learning, the learning channel, and the optimality of backpropagation","abstract":"\u00a9 2016 Elsevier Ltd In a physical neural system, where storage and processing are intimately intertwined, the rules for adjusting the synaptic weights can only depend on variables that are available locally, such as the activity of the pre- and post-synaptic neurons, resulting in local learning rules. A systematic framework for studying the space of local learning rules is obtained by first specifying the nature of the local variables, and then the functional form that ties them together into each learning rule. Such a framework enables also the systematic discovery of new learning rules and exploration of relationships between learning rules and group symmetries. We study polynomial local learning rules stratified by their degree and analyze their behavior and capabilities in both linear and non-linear units and networks. Stacking local learning rules in deep feedforward networks leads to deep local learning. While deep local learning can learn interesting representations, it cannot learn complex input\u2013output functions, even when targets are available for the top layer. Learning complex input\u2013output functions requires local deep learning where target information is communicated to the deep layers through a backward learning channel. The nature of the communicated information about the targets and the structure of the learning channel partition the space of learning algorithms. For any learning algorithm, the capacity of the learning channel can be defined as the number of bits provided about the error gradient per weight, divided by the number of required operations per weight. We estimate the capacity associated with several learning algorithms and show that backpropagation outperforms them by simultaneously maximizing the information rate and minimizing the computational cost. This result is also shown to be true for recurrent networks, by unfolding them in time. The theory clarifies the concept of Hebbian learning, establishes the power and limitations of local learning rules, introduces the learning channel which enables a formal analysis of the optimality of backpropagation, and explains the sparsity of the space of learning rules discovered so far.","keywords_author":["Backpropagation","Deep learning","Hebbian learning","Learning channel","Supervised learning","Unsupervised learning","Deep learning","Backpropagation","Hebbian learning","Learning channel","Supervised learning","Unsupervised learning"],"keywords_other":["Feedback","Feed-forward network","BINOCULAR INTERACTION","ALGORITHM","VISUAL-CORTEX","Machine Learning","Systematic framework","MEMORY","Deep learning","RECEPTIVE-FIELDS","Hebbian learning","Algorithms","MODEL","RECOGNITION","Computational costs","Learning channel","SYNAPTIC PLASTICITY","Post-synaptic neurons","Neural Networks (Computer)","NEURAL-NETWORKS","Recurrent networks","BCM THEORY"],"max_cite":5.0,"pub_year":2016.0,"sources":"['scp', 'wos']","rawkeys":["supervised learning","visual-cortex","memory","computational costs","binocular interaction","neural-networks","machine learning","feedback","hebbian learning","algorithms","algorithm","neural networks (computer)","receptive-fields","recognition","systematic framework","deep learning","unsupervised learning","bcm theory","model","post-synaptic neurons","synaptic plasticity","feed-forward network","learning channel","recurrent networks","backpropagation"],"tags":["supervised learning","visual-cortex","memory","random forests","computational costs","binocular interaction","machine learning","feedback","hebbian learning","algorithms","recognition","systematic framework","neural networks","unsupervised learning","bcm theory","model","post-synaptic neurons","synaptic plasticity","feed-forward network","learning channel","recurrent networks","backpropagation"]},{"p_id":93350,"title":"Discriminative Sparse Features for Alzheimer's Disease Diagnosis Using Multimodal Image Data","abstract":"Background: Feature extraction in medical image processing still remains a challenge, especially in high-dimensionality datasets, where the expected number of available samples is considerably lower than the dimension of the feature space. This is a common problem in real-world data, and, specifically, in medical image pro-cessing as, while images are composed of hundreds of thousands voxels, only a reduced number of patients are available.","keywords_author":["Sparse features","multimodel data","mild cognitive impairment","support vector classifiers","computer aided diagnosis","ADNI"],"keywords_other":["MR-IMAGES","EIGENBRAIN","REPRESENTATION","CLASSIFICATION","INDEPENDENT COMPONENT ANALYSIS","SCANS","RECOGNITION","FMRI","CLASSIFIERS"],"max_cite":0.0,"pub_year":2018.0,"sources":"['wos']","rawkeys":["eigenbrain","recognition","mild cognitive impairment","sparse features","representation","adni","classifiers","independent component analysis","mr-images","multimodel data","classification","fmri","support vector classifiers","computer aided diagnosis","scans"],"tags":["eigenbrain","recognition","mild cognitive impairment","sparse features","computer-aided diagnosis","representation","adni","independent component analysis","mr-images","classifier","classification","fmri","multimodel data","support vector classifiers","scans"]},{"p_id":93351,"title":"Automatic computation of regions of interest by robust principal component analysis. Application to automatic dementia diagnosis","abstract":"Computer aided diagnosis systems based on brain imaging are a powerful tool to assist in the diagnosis of Alzheimer's Disease (AD). The goal is the automatic recognizing of neurodegenerative patterns that characterize the disease. In this regard, determining regions related to the disease results crucial to select the most discriminative voxels and to optimize the number of features to be used in the learning algorithm. In this paper, we propose a method based on the robust principal component analysis (Robust PCA) algorithm that allows to automatically compute Regions Of Interest (ROIs) over a training set of images and rank them according to their diagnostic relevance. Robust PCA is used to compute the sparse ' error matrix, which is, in turn, employed to determine the brain areas related to the Alzheimer's disease. These areas are further used as a mask to select and weight the most discriminative voxels to construct a classification model. We then describe a method to fuse the features computed from different image modalities based on the weights assigned by the individual Support Vector Classifiers during the training process. The method presented' here has been applied to multimodal image containing both functional (18F-FDG PET) and structural (Magnetic Resonance) data. Experiments, conducted using 68 control subjects and 70 CE patients, show the effectiveness of the proposed approach for the exploratory analysis. At the same time, classification experiments using the features computed by the proposed method and assessed by cross-validation showed accuracy values up to 92% and AUC (Area Under the Curve) of 0.95. Thus, the proposal seems as an effective technique to reveal ROIs in differential diagnosis applications and to combine multimodal image data, outperforming other classification methods, including the voxelas-features (VAF) baseline. (C) 2017 Elsevier B.V. All rights reserved.","keywords_author":["Robust PCA","Exploratory analysis","Multimodal image data","Regions of interest","Information fusion","Alzheimer's disease","Computer-aided diagnosis"],"keywords_other":["ALZHEIMERS-DISEASE","RECOGNITION","CLASSIFICATION","MRI"],"max_cite":1.0,"pub_year":2017.0,"sources":"['wos']","rawkeys":["recognition","robust pca","regions of interest","alzheimers-disease","computer-aided diagnosis","exploratory analysis","multimodal image data","mri","classification","alzheimer's disease","information fusion"],"tags":["recognition","robust pca","region of interest","alzheimers-disease","anomaly detection","computer-aided diagnosis","exploratory analysis","multimodal image data","classification","magnetic resonance imaging","information fusion"]},{"p_id":44201,"title":"More for less: Insights into convolutional nets for 3D point cloud recognition","abstract":"\u00a9 2017 IEEE. With the recent breakthrough in commodity 3D imaging solutions such as depth sensing, photogrammetry, stereoscopic vision and structured light, 3D shape recognition is becoming an increasingly important problem. A longstanding question is what should be the format of the 3D shape (such as voxel, mesh, point-cloud etc.) and what could be a good generic feature representation for shape recognition. This question is particularly important in the context of convolutional neural network (CNN) whose efficacy and complexity depends upon the choice of input shape format and the design of network. It has been seen that both 3D voxel representation as well as collection of rendered views on 2D images have produced competing results. Similarly, it have been seen that networks with few million parameters and networks with several hundred million parameters have similar performance. In this work we compare these solutions and provide an analysis on the factors resulting in increase in the parameters without significantly improving accuracy. On the basis of the above analysis we propose a representation method (point cloud to 2D grid) and architecture that results in much less parameters for the CNN but has competing accuracy.","keywords_author":["3DOR","Deep learning","Point cloud","Recognition"],"keywords_other":["Stereoscopic vision","Point cloud","Recognition","3DOR","3D shape recognition","Convolutional Neural Networks (CNN)","Representation method","Voxel representation"],"max_cite":0.0,"pub_year":2018.0,"sources":"['scp']","rawkeys":["point cloud","recognition","deep learning","representation method","3dor","3d shape recognition","stereoscopic vision","voxel representation","convolutional neural networks (cnn)"],"tags":["point cloud","recognition","machine learning","3dor","3d shape recognition","convolutional neural network","stereoscopic vision","voxel representation","representation method"]},{"p_id":76973,"title":"300 Faces In-The-Wild Challenge: database and results","abstract":"Computer Vision has recently witnessed great research advance towards automatic facial points detection. Numerous methodologies have been proposed during the last few years that achieve accurate and efficient performance. However, fair comparison between these methodologies is infeasible mainly due to two issues. (a) Most existing databases, captured under both constrained and unconstrained (in-the-wild) conditions have been annotated using different mark-ups and, in most cases, the accuracy of the annotations is low. (b) Most published works report experimental results using different training\/testing sets, different error metrics and, of course, landmark points with semantically different locations. In this paper, we aim to overcome the aforementioned problems by (a) proposing a semi-automatic annotation technique that was employed to re-annotate most existing facial databases under a unified protocol, and (b) presenting the 300 Faces In The-Wild Challenge (300-W), the first facial landmark localization challenge that was organized twice, in 2013 and 2015. To the best of our knowledge, this is the first effort towards a unified annotation scheme of massive databases and a fair experimental comparison of existing facial landmark localization systems. The images and annotations of the new testing database that was used in the 300-W challenge are available from http:\/\/ibug.docic.ac.uk\/resources\/300-W_IMAVISi. (C) 2016 Elsevier B.V. All rights reserved.","keywords_author":["Facial landmark localization","Challenge","Semi-automatic annotation tool","Facial database"],"keywords_other":["FEATURES","POSE ESTIMATION","ACTIVE APPEARANCE MODELS","ALIGNMENT","RECOGNITION","PICTORIAL STRUCTURES"],"max_cite":40.0,"pub_year":2016.0,"sources":"['wos']","rawkeys":["active appearance models","challenge","pose estimation","recognition","features","pictorial structures","facial database","semi-automatic annotation tool","alignment","facial landmark localization"],"tags":["active appearance models","pose estimation","recognition","features","pictorial structures","facial database","challenges","semi-automatic annotation tool","alignment","facial landmark localization"]},{"p_id":11444,"title":"Bullying incidences identification within an immersive environment using HD EEG-based analysis: A Swarm Decomposition and Deep Learning approach","abstract":"Bullying is an everlasting phenomenon and the first, yet difficult, step towards the solution is its detection. Conventional approaches for bullying incidence identification include questionnaires, conversations and psychological tests. Here, unlike the conventional approaches, two experiments are proposed that involve visual stimuli with cases of bullying- and non-bullying-related ones, set within a 2D (simple video preview) and a Virtual Reality (VR) (immersive video preview) context. In both experimental settings, brain activity is recorded using high density (HD) (256 channels) electroencephalogram (EEG), and analyzed to identify the bullying stimuli type (bullying\/non-bullying) and context (2D\/VR). The proposed classification analysis uses a convolutional neural network (CNN), applying deep learning on the oscillatory modes (OCMs) embedded within the raw HD EEG data. The extraction of OCMs from the HD EEG data is achieved with swarm decomposition (SWD), which efficiently accounts for the non-stationarity and noise contamination of the raw HD EEG data. Experimental results from 17 subjects indicate that the new SWD\/CNN approach achieves high discrimination accuracy (AUC = 0.987 between bullying\/non-bullying stimuli type; AUC = 0.975, between bullying\/non-bullying stimuli type and 2D\/VR context), paving the way for better understanding of how brain's responses could act as indicators of bullying experience within immersive environments.","keywords_author":null,"keywords_other":["MIRROR NEURONS","RECOGNITION","EMPATHY"],"max_cite":0.0,"pub_year":2017.0,"sources":"['wos']","rawkeys":["recognition","empathy","mirror neurons"],"tags":["mirror neuron","recognition","empathy"]},{"p_id":11464,"title":"Deep-learning Versus OBIA for Scattered Shrub Detection with Google Earth Imagery: Ziziphus lotus as Case Study","abstract":"There is a growing demand for accurate high-resolution land cover maps in many fields, e.g., in land-use planning and biodiversity conservation. Developing such maps has been traditionally performed using Object-Based Image Analysis (OBIA) methods, which usually reach good accuracies, but require a high human supervision and the best configuration for one image often cannot be extrapolated to a different image. Recently, deep learning Convolutional Neural Networks (CNNs) have shown outstanding results in object recognition in computer vision and are offering promising results in land cover mapping. This paper analyzes the potential of CNN-based methods for detection of plant species of conservation concern using free high-resolution Google Earth (TM) images and provides an objective comparison with the state-of-the-art OBIA-methods. We consider as case study the detection of Ziziphus lotus shrubs, which are protected as a priority habitat under the European Union Habitats Directive. Compared to the best performing OBIA-method, the best CNN-detector achieved up to 12% better precision, up to 30% better recall and up to 20% better balance between precision and recall. Besides, the knowledge that CNNs acquired in the first image can be re-utilized in other regions, which makes the detection process very fast. A natural conclusion of this work is that including CNN-models as classifiers, e.g., ResNet-classifier, could further improve OBIA methods. The provided methodology can be systematically reproduced for other species detection using our codes available through (https:\/\/github.com\/EGuirado\/CNN-remotesensing).","keywords_author":["Ziziphus lotus","plant species detection","land cover mapping","Convolutional Neural Networks (CNNs)","Object-Based Image Analysis (OBIA)","remote sensing"],"keywords_other":["MAPPING SHRUB","LAND-COVER","CLASSIFICATION","RANDOM FOREST","RECOGNITION","OPTIMIZATION","CONVOLUTIONAL NEURAL-NETWORKS"],"max_cite":4.0,"pub_year":2017.0,"sources":"['ieee', 'wos']","rawkeys":["object-based image analysis (obia)","recognition","remote sensing","ziziphus lotus","classification","convolutional neural-networks","mapping shrub","optimization","plant species detection","convolutional neural networks (cnns)","land cover mapping","land-cover","random forest"],"tags":["recognition","remote sensing","ziziphus lotus","land cover","random forests","classification","convolutional neural network","mapping shrub","optimization","object based image analysis","plant species detection","land cover mapping"]},{"p_id":60616,"title":"AUDIO-VISUAL SPEECH-PROCESSING SYSTEM FOR POLISH APPLICABLE TO HUMAN-COMPUTER INTERACTION","abstract":"This paper describes an audio-visual speech recognition system for the Polish language as well as a set of performance tests under various acoustic conditions. We first present the overall structure of AVASR systems with three main areas: audio feature extraction, visual feature extraction, and (subsequently) audiovisual speech integration. We present the MFCC features for an audio stream with the standard HMM modeling technique, then we describe the appearance and shape-based visual features. Subsequently, we present two feature integration techniques, feature concatenation, and model fusion. We also discuss the results of a set of experiments conducted to select the best system setup for Polish under noisy audio conditions. The experiments simulate human-computer interaction in a computer control case with voice commands in difficult audio environments. With the Active Appearance Model (AAM) and multi-stream Hidden Markov Model (HMM), we can improve system accuracy by reducing the word error rate by more than 30% (as compared to audio-only speech recognition) when the signal-to-noise ratio drops to 0 dB.","keywords_author":["audio-visual speech recognition","visual feature extraction","human-computer interaction"],"keywords_other":["FUSION","RECOGNITION","MODELS"],"max_cite":0.0,"pub_year":2018.0,"sources":"['wos']","rawkeys":["recognition","visual feature extraction","human-computer interaction","audio-visual speech recognition","models","fusion"],"tags":["recognition","model","audio visual speech recognition","visual feature extraction","human-computer interaction","fusion"]},{"p_id":68811,"title":"CrowdGIS: Updating Digital Maps via Mobile Crowdsensing","abstract":"Accurate digital maps play a crucial role in various location-based services and applications. However, store information is usually missing or outdated in current maps. In this paper, we propose CrowdGIS, an automatic store self-updating system for digital maps that leverages street views and sensing data crowdsourced from mobile users. We first develop a new weighted artificial neural network to learn the underlying relationship between estimated positions and real positions to localize user's shooting positions. Then, a novel text detection method is designed by considering two valuable features, including the color and texture information of letters. In this way, we can recognize complete store name instead of individual letters as in the previous study. Furthermore, we transfer the shooting position to the location of recognized stores in the map. Finally, CrowdGIS considers three updating categories (replacing, adding, and deleting) to update changed stores in the map based on the kernel density estimate model. We implement CrowdGIS and conduct extensive experiments in a real outdoor region for 1 month. The evaluation results demonstrate that CrowdGIS effectively accommodates store variations and updates stores to maintain an up-to-date map with high accuracy.","keywords_author":["Digital map update","mobile crowdsensing"],"keywords_other":["RECOGNITION"],"max_cite":0.0,"pub_year":2018.0,"sources":"['wos']","rawkeys":["mobile crowdsensing","recognition","digital map update"],"tags":["multiple classifier systems","recognition","digital map update"]},{"p_id":68812,"title":"AUTOMATED ULCER AND BLEEDING CLASSIFICATION FROM WCE IMAGES USING MULTIPLE FEATURES FUSION AND SELECTION","abstract":"In the area of medical imaging and computer vision, automatic diagnosis of ulcer and bleeding from wireless capsule endoscopy images has been an active research domain. It contains several challenges including low contrast, complex background, lesion shape and color which affect its segmentation and classification accuracy. In this article, a novel method for automated detection and classification of stomach infection is implemented. The proposed method consists of four major steps including preprocessing, lesion segmentation, image representation and classification. The lesion contrast is improved in preprocessing step by employing 3D-box filtering, 3D-median filtering and HSV transformation. In the second step, geometric features are extracted and applied to the saturated channel to give a binary image. The binary image is further improved by fusion of generated mask. After that, extraction of three types of features including color, shape and surf is performed from HSV and binary segmented images and their information is fused by a serial based method. A principal component analysis (PCA) and correlation coefficient based feature selection approach is proposed which are classified by multi class support vector machine (M-SVM). The proposed method is evaluated on personally collected images of three different classes including ulcer, bleeding and healthy. The M-SVM performs well with a maximum accuracy of 98.3% which shows the authenticity of presented method.","keywords_author":["WCE","contrast stretching","lesion segmentation","feature extraction","feature selection","classification"],"keywords_other":["CAPSULE ENDOSCOPY VIDEOS","RECOGNITION","SEGMENTATION"],"max_cite":0.0,"pub_year":2018.0,"sources":"['wos']","rawkeys":["recognition","segmentation","wce","feature selection","classification","feature extraction","capsule endoscopy videos","contrast stretching","lesion segmentation"],"tags":["recognition","wireless capsule endoscopy","segmentation","feature selection","classification","feature extraction","capsule endoscopy videos","contrast stretching","lesion segmentation"]},{"p_id":11468,"title":"The benefit of deep processing and high educational level for verbal learning in young and middle-aged adults","abstract":"Background and aims; The aim of the present study was to examine whether deeper processing of words during encoding in middle-aged adults leads to a smaller increase in word-learning performance and a smaller decrease in retrieval effort than in young adults. It was also assessed whether high education attenuates age-related differences in performance. Methods: Accuracy of recall and recognition, and reaction times of recognition, after performing incidental and intentional learning tasks were compared between 40 young (25-35) and 40 middle-aged (50-60) adults with low and high educational levels. Results: Age differences in recall increased with depth of processing, whereas age differences in accuracy and reaction times of recognition did not differ across levels. High education does not moderate age-related differences in performance. Conclusions: These findings suggest a smaller benefit of deep processing in middle age, when no retrieval cues are available.","keywords_author":["aging","deep processing","education","verbal learning"],"keywords_other":["HEALTH","POPULATION","COGNITIVE DECLINE","RECOGNITION","ORIENTING TASKS","RECALL","DEPTH","TIME","COMPLAINTS","MEMORY"],"max_cite":0.0,"pub_year":2007.0,"sources":"['wos']","rawkeys":["education","recognition","memory","verbal learning","cognitive decline","time","population","depth","deep processing","aging","orienting tasks","recall","health","complaints"],"tags":["education","recognition","memory","verbal learning","cognitive decline","time","population","depth","aged","deep processing","orienting tasks","recall","health","complaints"]},{"p_id":11473,"title":"Deep learning for automatic usability evaluations based on images: A case study of the usability heuristics of thermostats","abstract":"Thermostats are designed for increasing requirements on indoor thermal comfort. Nevertheless, they are critical devices for saving energy in buildings and households. However, when thermostats do not accomplish the usability requirements, the end-users do not save energy. Then, when a thermostat is designed or validated, one of the leading problems that must be tackled is the usability evaluation. Generally, the evaluation is based on usability heuristics that are done by experts and designers and involve a very complicated cycling process in which usability experts need to be included in the complete usability evaluation. On the other hand, there are several proposals for generating an automatic usability analysis that can be used by designers or end-users. However, they are limited by the methodologies that are implemented in the evaluation because usability evaluations necessitate a large amount of data abstraction, and the amount of processed information is enormous; As an alternative, Artificial Intelligence can help to solve this problem, especially machine learning techniques with deep learning capabilities that can reach a high level of data abstraction with a significant amount of information and implement an automatic usability evaluation based on images. Convolutional networks that are included in deep learning can classify complex problems, attain highly accurate results. This paper proposes to train a convolutional network with standard usability heuristics for evaluating usability, which is an easy method for evaluating usability in thermostats, based on images. The proposed automatic method gives excellent results for evaluating usability heuristics in the heuristic assigned. This paper provides a complete methodology, using deep learning, for automatically evaluating the usability heuristics of thermostats. (C) 2017 Elsevier B.V. All rights reserved.","keywords_author":["Automatic usability evaluation","Deep learning","Images","Thermostats","Automatic usability evaluation","Deep learning","Thermostats","Images"],"keywords_other":["Convolutional networks","Machine learning techniques","Amount of information","PROGRAMMABLE THERMOSTATS","INTERFACES","DESIGN","Images","ENERGY SAVINGS","Usability evaluation","Learning capabilities","RECOGNITION","USER","ENVIRONMENTS","Usability requirements","FUZZY-LOGIC","Indoor thermal comfort","METRICS"],"max_cite":0.0,"pub_year":2018.0,"sources":"['scp', 'wos']","rawkeys":["design","indoor thermal comfort","automatic usability evaluation","amount of information","convolutional networks","environments","thermostats","metrics","user","recognition","images","energy savings","deep learning","machine learning techniques","usability requirements","usability evaluation","fuzzy-logic","interfaces","programmable thermostats","learning capabilities"],"tags":["design","fuzzy logic","convolutional neural network","environment","indoor thermal comfort","automatic usability evaluation","amount of information","machine learning","thermostats","metrics","recognition","images","machine learning techniques","usability requirements","usability evaluation","energy saving","interface","programmable thermostats","learning capabilities","users"]},{"p_id":11482,"title":"Combining deep residual neural network features with supervised machine learning algorithms to classify diverse food image datasets","abstract":"Obesity is increasing worldwide and can cause many chronic conditions such as type-2 diabetes, heart disease, sleep apnea, and some cancers. Monitoring dietary intake through food logging is a key method to maintain a healthy lifestyle to prevent and manage obesity. Computer vision methods have been applied to food logging to automate image classification for monitoring dietary intake. In this work we applied pretrained ResNet-152 and GoogleNet convolutional neural networks (CNNs), initially trained using ImageNet Large Scale Visual Recognition Challenge (ILSVRC) dataset with MatConvNet package, to extract features from food image datasets; Food 5K, Food-11, RawFooT-DB, and Food-101. Deep features were extracted from CNNs and used to train machine learning classifiers including artificial neural network (ANN), support vector machine (SVM), Random Forest, and Naive Bayes. Results show that using ResNet-152 deep features with SVM with RBF kernel can accurately detect food items with 99.4% accuracy using Food-5K validation food image dataset and 98.8% with Food-5K evaluation dataset using ANN, SVM-RBF, and Random Forest classifiers. Trained with ResNet-152 features, ANN can achieve 91.34%, 99.28% when applied to Food-11 and RawFooT-DB food image datasets respectively and SVM with RBF kernel can achieve 64.98% with Food-101 image dataset. From this research it is clear that using deep CNN features can be used efficiently for diverse food item image classification. The work presented in this research shows that pretrained ResNet-152 features provide sufficient generalisation power when applied to a range of food image classification tasks.","keywords_author":["Convolutional neural networks","Deep learning","Feature extraction","Food logging","Obesity","Obesity","Food logging","Deep learning","Convolutional neural networks","Feature extraction"],"keywords_other":["Visual recognition","Classification tasks","Obesity","Random forest classifier","Chronic conditions","RECOGNITION","Convolutional neural network","Neural network features","Supervised machine learning"],"max_cite":0.0,"pub_year":2018.0,"sources":"['ieee', 'scp', 'wos']","rawkeys":["obesity","visual recognition","recognition","convolutional neural networks","deep learning","random forest classifier","classification tasks","food logging","neural network features","convolutional neural network","feature extraction","supervised machine learning","chronic conditions"],"tags":["obesity","visual recognition","recognition","machine learning","random forest classifier","classification tasks","food logging","neural network features","feature extraction","convolutional neural network","supervised machine learning","chronic conditions"]},{"p_id":11496,"title":"Detection and Labeling of Vertebrae in MR Images Using Deep Learning with Clinical Annotations as Training Data","abstract":"The purpose of this study was to investigate the potential of using clinically provided spine label annotations stored in a single institution image archive as training data for deep learning-based vertebral detection and labeling pipelines. Lumbar and cervical magnetic resonance imaging cases with annotated spine labels were identified and exported from an image archive. Two separate pipelines were configured and trained for lumbar and cervical cases respectively, using the same setup with convolutional neural networks for detection and parts-based graphical models to label the vertebrae. The detection sensitivity, precision and accuracy rates ranged between 99.1-99.8, 99.6-100, and 98.8-99.8% respectively, the average localization error ranges were 1.18-1.24 and 2.38-2.60 mm for cervical and lumbar cases respectively, and with a labeling accuracy of 96.0-97.0%. Failed labeling results typically involved failed S1 detections or missed vertebrae that were not fully visible on the image. These results show that clinically annotated image data from one image archive is sufficient to train a deep learning-based pipeline for accurate detection and labeling of MR images depicting the spine. Further, these results support using deep learning to assist radiologists in their work by providing highly accurate labels that only require rapid confirmation.","keywords_author":["Archive","Artificial neural networks (ANNs)","Machine learning","Magnetic resonance imaging","Archive","Artificial neural networks (ANNs)","Machine learning","Magnetic resonance imaging"],"keywords_other":["Image archives","Sensitivity and Specificity","Localization errors","Humans","Archive","Machine Learning","Cervical Vertebrae","Highly accurate","Magnetic Resonance Imaging","Lumbar Vertebrae","Detection sensitivity","RECOGNITION","GraphicaL model","Radiology Information Systems","LOCALIZATION","Neural Networks (Computer)","Labeling accuracies","CT IMAGES","SEGMENTATION","Convolutional neural network","Thoracic Vertebrae","Spine","CONVOLUTIONAL NEURAL-NETWORKS"],"max_cite":3.0,"pub_year":2017.0,"sources":"['ieee', 'scp', 'wos']","rawkeys":["highly accurate","localization","image archives","convolutional neural-networks","convolutional neural network","graphical model","artificial neural networks (anns)","segmentation","detection sensitivity","lumbar vertebrae","cervical vertebrae","machine learning","radiology information systems","spine","sensitivity and specificity","neural networks (computer)","recognition","humans","ct images","localization errors","archive","thoracic vertebrae","labeling accuracies","magnetic resonance imaging"],"tags":["highly accurate","localization","image archives","convolutional neural network","graphical model","segmentation","detection sensitivity","lumbar vertebrae","cervical vertebrae","machine learning","radiology information systems","spine","sensitivity and specificity","recognition","neural networks","humans","ct images","localization errors","archive","thoracic vertebrae","labeling accuracies","magnetic resonance imaging"]},{"p_id":11498,"title":"RNA-protein binding motifs mining with a new hybrid deep learning based cross-domain knowledge integration approach","abstract":"Background: RNAs play key roles in cells through the interactions with proteins known as the RNA-binding proteins (RBP) and their binding motifs enable crucial understanding of the post-transcriptional regulation of RNAs. How the RBPs correctly recognize the target RNAs and why they bind specific positions is still far from clear. Machine learning-based algorithms are widely acknowledged to be capable of speeding up this process. Although many automatic tools have been developed to predict the RNA-protein binding sites from the rapidly growing multi-resource data, e. g. sequence, structure, their domain specific features and formats have posed significant computational challenges. One of current difficulties is that the cross-source shared common knowledge is at a higher abstraction level beyond the observed data, resulting in a low efficiency of direct integration of observed data across domains. The other difficulty is how to interpret the prediction results. Existing approaches tend to terminate after outputting the potential discrete binding sites on the sequences, but how to assemble them into the meaningful binding motifs is a topic worth of further investigation.","keywords_author":["CLIP-seq","Convolutional neural network","Deep belief network","Multimodal deep learning","RNA-binding protein","RNA-binding protein","CLIP-seq","Deep belief network","Convolutional neural network","Multimodal deep learning"],"keywords_other":["Protein Binding","RESIDUES","GENOME","IDENTIFICATION","Machine Learning","Nucleotide Motifs","Multi-modal","RNA-binding protein","SEQUENCE","Binding Sites","SPECIFICITIES","VARIANTS","Algorithms","RNA","Sequence Analysis, RNA","Deep belief networks","CLIP-seq","RECOGNITION","PREDICTION","Neural Networks (Computer)","RNA-Binding Proteins","NEURAL-NETWORKS","Convolutional neural network","Data Mining"],"max_cite":14.0,"pub_year":2017.0,"sources":"['scp', 'wos']","rawkeys":["identification","clip-seq","sequence","convolutional neural network","genome","neural-networks","rna-binding proteins","machine learning","rna","deep belief network","algorithms","specificities","data mining","neural networks (computer)","recognition","multimodal deep learning","variants","multi-modal","rna-binding protein","sequence analysis","residues","protein binding","prediction","nucleotide motifs","binding sites","deep belief networks"],"tags":["identification","clip-seq","sequence","convolutional neural network","specificity","rna-binding proteins","machine learning","rna","algorithms","genomics","data mining","recognition","neural networks","multimodal deep learning","variants","multi-modal","sequence analysis","residues","protein binding","prediction","nucleotide motifs","binding sites","deep belief networks"]},{"p_id":36088,"title":"NFV-bench: A dependability benchmark for network function virtualization systems","abstract":"\u00a9 2017 IEEE. Network function virtualization (NFV) envisions the use of cloud computing and virtualization technology to reduce costs and innovate network services. However, this paradigm shift poses the question whether NFV will be able to fulfill the strict performance and dependability objectives required by regulations and customers. Thus, we propose a dependability benchmark to support NFV providers at making informed decisions about which virtualization, management, and applicationlevel solutions can achieve the best dependability. We define in detail the use cases, measures, and faults to be injected. Moreover, we present a benchmarking case study on two alternative, production-grade virtualization solutions, namely VMware ESXi\/vSphere (hypervisor-based) and Linux\/Docker (containerbased), on which we deploy an NFV-oriented IMS system. Despite the promise of higher performance and manageability, our experiments suggest that the container-based configuration can be less dependable than the hypervisor-based one, and point out which faults NFV designers should address to improve dependability.","keywords_author":["Benchmarking","Cloud computing","Container-based virtualization","Dependability","Docker","Fault injection","Hypervisors","Linux","NFV","VMware ESXi","Vmware vsphere"],"keywords_other":["Fault injection","VMware ESXi","Hypervisors","Vmware vsphere","Dependability","Docker"],"max_cite":2.0,"pub_year":2017.0,"sources":"['scp']","rawkeys":["vmware esxi","container-based virtualization","cloud computing","nfv","hypervisors","docker","fault injection","linux","benchmarking","dependability","vmware vsphere"],"tags":["hypervisor","vmware esxi","container-based virtualization","recognition","benchmark","cloud computing","docker","fault injection","linux","network function virtualization","vmware vsphere"]},{"p_id":60669,"title":"Don't Classify Ratings of Affect; Rank Them!","abstract":"How should affect be appropriately annotated and how should machine learning best be employed to map manifestations of affect to affect annotations? What is the use of ratings of affect for the study of affective computing and how should we treat them? These are the key questions this paper attempts to address by investigating the impact of dissimilar representations of annotated affect on the efficacy of affect modelling. In particular, we compare several different binary-class and pairwise preference representations for automatically learning from ratings of affect. The representations are compared and tested on three datasets: one synthetic dataset (testing \"in vitro\") and two affective datasets (testing \"in vivo\"). The synthetic dataset couples a number of attributes with generated rating values. The two affective datasets contain physiological and contextual user attributes, and speech attributes, respectively; these attributes are coupled with ratings of various affective and cognitive states. The main results of the paper suggest that ratings (when used) should be naturally transformed to ordinal (ranked) representations for obtaining more reliable and generalisable models of affect. The findings of this paper have a direct impact on affect annotation and modelling research but, most importantly, challenge the traditional state-of-practice in affective computing and psychometrics at large.","keywords_author":["Affect annotation","affect modelling","ratings","ranks","preference learning","classification","computer games","sensitive artificial listener (SAL) corpus"],"keywords_other":["EMOTIONS","RECOGNITION","MODELS"],"max_cite":25.0,"pub_year":2014.0,"sources":"['ieee', 'wos']","rawkeys":["preference learning","recognition","emotions","ranks","sensitive artificial listener (sal) corpus","affect modelling","classification","affect annotation","models","ratings","computer games"],"tags":["preference learning","recognition","model","emotion","standards","sensitive artificial listener (sal) corpus","affective annotations","affective modeling","classification","ratings","computer games"]},{"p_id":11520,"title":"Noise-Resistant Deep Learning for Object Classification in Three-Dimensional Point Clouds Using a Point Pair Descriptor","abstract":"Object retrieval and classification in point cloud data are challenged by noise, irregular sampling density, and occlusion. To address this issue, we propose a point pair descriptor that is robust to noise and occlusion and achieves high retrieval accuracy. We further show how the proposed descriptor can be used in a four-dimensional (4-D) convolutional neural network for the task of object classification. We propose a novel 4-D convolutional layer that is able to learn class-specific clusters in the descriptor histograms. Finally, we provide experimental validation on three benchmark datasets, which confirms the superiority of the proposed approach.","keywords_author":["Recognition","object detection","segmentation and categorization","RGB-D perception"],"keywords_other":null,"max_cite":0.0,"pub_year":2018.0,"sources":"['ieee', 'wos']","rawkeys":["rgb-d perception","recognition","object detection","segmentation and categorization"],"tags":["rgb-d perception","recognition","object detection","segmentation and categorization"]},{"p_id":109827,"title":"Script Identification of Multi-Script Documents: A Survey","abstract":"In recent years, with the widespread of Internet and digitized processing of multi-script documents worldwide, script identification techniques have become more important in the pattern recognition field. Script identification concerns methods for identifying different scripts in multi-lingual, multi-script documents. This paper presents a comprehensive overview on research activities in the field and focuses on the most valuable results obtained so far. The most vital processes in script identification are addressed in detail: identification and discriminating methods, features extraction (local and global), and classification. Different kinds of approaches have been developed and promising results have been achieved. This paper reports SoA performance results. This paper reports methods concerning handwritten, printed, and hybrid document processing. More research is necessary to meet the performance levels essential for everyday applications.","keywords_author":["Handwriting recognition","optical character recognition (OCR)","character recognition","multi-script documents","script identification"],"keywords_other":["FEATURES","TEXT LINES","CLASSIFICATION","DATABASE","RECOGNITION","SEGMENTATION","LANGUAGE","ENVIRONMENT","BILINGUAL DOCUMENTS","IMAGES"],"max_cite":3.0,"pub_year":2017.0,"sources":"['wos']","rawkeys":["bilingual documents","recognition","images","segmentation","features","optical character recognition (ocr)","script identification","database","character recognition","text lines","classification","environment","language","multi-script documents","handwriting recognition"],"tags":["bilingual documents","recognition","images","segmentation","databases","features","script identification","text lines","character recognition","classification","environment","optical character recognition","language","multi-script documents","handwriting recognition"]},{"p_id":109828,"title":"Script identification algorithms: a survey","abstract":"Script identification is being widely accepted techniques for selection of the particular script OCR (Optical Character Recognition) in multilingual document images. Extensive research has been done in this field, but still it suffers from low identification accuracy. This is due to the presence of faded document images, illuminations and positions while scanning. Noise is also a major obstacle in the script identification process. However, it can only be minimized up to a level, but cannot be removed completely. In this paper, an attempt is made to analyze and classify various script identification schemes for document images. The comparison is also made between these schemes, and discussion is made based upon their merits and demerits on a common platform. This will help the researchers to understand the complexity of the issue and identify possible directions for research in this field.","keywords_author":["Script identification","Feature matching","Classifier","Optical character recognition","Document analysis"],"keywords_other":["LANGUAGE IDENTIFICATION","FEATURES","TEXT LINES","CLASSIFICATION","RECOGNITION","DOCUMENT IMAGES","BILINGUAL DOCUMENTS"],"max_cite":1.0,"pub_year":2017.0,"sources":"['wos']","rawkeys":["bilingual documents","recognition","features","document images","script identification","language identification","document analysis","text lines","classifier","classification","optical character recognition","feature matching"],"tags":["bilingual documents","recognition","features","document images","script identification","language identification","document analysis","text lines","classifier","classification","optical character recognition","feature matching"]},{"p_id":11528,"title":"Deep Learning with Convolutional Neural Network for Differentiation of Liver Masses at Dynamic Contrast-enhanced CT: A Preliminary Study","abstract":"Purpose: To investigate diagnostic performance by using a deep learning method with a convolutional neural network (CNN) for the differentiation of liver masses at dynamic contrast agent-enhanced computed tomography (CT).","keywords_author":null,"keywords_other":["DIAGNOSIS","FEATURES","RECOGNITION","TUMORS","EARLY HEPATOCELLULAR-CARCINOMA","LESIONS"],"max_cite":3.0,"pub_year":2017.0,"sources":"['ieee', 'wos']","rawkeys":["diagnosis","recognition","features","tumors","lesions","early hepatocellular-carcinoma"],"tags":["diagnosis","recognition","features","tumor","lesions","early hepatocellular-carcinoma"]},{"p_id":60682,"title":"Affect Modeling with Field-based Physiological Responses","abstract":"Using the physiological system to perform affect modeling has great potential but also introduces many challenging issues in pervasive and interactive computing. With the advances in low-power mobile sensors, it is now possible to create a good quality of affect models based on physiological responses, which are useful in understanding how people express affect in real-world environments. In this paper, we have investigated an affect modeling technique that analyzes physiological changes and models user affect with data gathered in the field. In particular, we have identified a number of sensor channels and features that are discriminable in recognizing stress with Support Vector Machines. We have empirically investigated the value of creating an affect model by using a subset of informative features for an individual on physiological data collected in real-world environments (i.e. outside the lab), and we provide a discussion of the remaining challenging issues in performing field-based physiological analysis.","keywords_author":["user models","ubiquitous computing","health informatics"],"keywords_other":["CLASSIFICATION","LIFE","RECOGNITION","STRESS","INTELLIGENCE","TERM","PATTERNS","EMOTION","SENSORS"],"max_cite":0.0,"pub_year":2015.0,"sources":"['wos']","rawkeys":["intelligence","recognition","emotion","sensors","stress","term","life","health informatics","patterns","ubiquitous computing","user models","classification"],"tags":["user modeling","intelligence","recognition","emotion","sensors","stress","term","life","health informatics","patterns","ubiquitous computing","classification"]},{"p_id":11535,"title":"Pick-place of dynamic objects by robot manipulator based on deep learning and easy user interface teaching systems","abstract":"Purpose - Development of autonomous robot manipulator for human-robot assembly tasks is a key component to reach high effectiveness. In such tasks, the robot real-time object recognition is crucial. In addition, the need for simple and safe teaching techniques need to be considered, because: small size robot manipulators' presence in everyday life environments is increasing requiring non-expert operators to teach the robot; and in small size applications, the operator has to teach several different motions in a short time.","keywords_author":["Teaching methods","Object recognition","Robot manipulator","Deep belief neural network","Robot grasping"],"keywords_other":["RECOGNITION"],"max_cite":0.0,"pub_year":2017.0,"sources":"['ieee', 'wos']","rawkeys":["deep belief neural network","recognition","teaching methods","robot grasping","object recognition","robot manipulator"],"tags":["recognition","robotic manipulators","robotic grasping","teaching methods","deep belief neural networks","object recognition"]},{"p_id":68895,"title":"A segmentation method for greenhouse vegetable foliar disease spots images using color information and region growing","abstract":"This paper presents a novel image processing method using color information and region growing for segmenting greenhouse vegetable foliar disease spots images captured under real field conditions. Disease images captured under real field conditions are suffering from uneven illumination and complicated background, which is a big challenge to achieve robust disease spots segmentation. A disease spots segmentation method consisting of two pipelined procedures is proposed in this paper. Firstly a comprehensive color feature and its detection method are presented. The comprehensive color feature (CCF) consists of three color components, Excess Red Index (ExR), H component of HSV color space and b* component of L*a*b* color space, which implements powerful discrimination of disease spots and clutter background. Then an interactive region growing method based on the CCF map is used to achieve disease spots segmentation from clutter background. To evaluate the robustness and accuracy, the proposed segmentation method is assessed by cucumber downy mildew images. Results show that the proposed method can achieve accurate and robust segmentation under real field conditions. (c) 2017 Elsevier B.V. All rights reserved.","keywords_author":["Greenhouse vegetables","Foliar disease spots","CCF","Region growing","Image segmentation"],"keywords_other":["SPACE","EXTRACTION","LEAF-SPOT","FIELD","RECOGNITION"],"max_cite":2.0,"pub_year":2017.0,"sources":"['wos']","rawkeys":["greenhouse vegetables","leaf-spot","recognition","space","ccf","field","region growing","foliar disease spots","extraction","image segmentation"],"tags":["greenhouse vegetables","leaf-spot","recognition","space","ccf","field","region growing","foliar disease spots","extraction","image segmentation"]},{"p_id":52562,"title":"Failure analysis of a complex learning framework incorporating multi-modal and semi-supervised learning","abstract":"Machine learning is used in many applications, from machine vision to speech recognition to decision support systems, and it is used to test applications. However, though much has been done to evaluate the performance of machine learning algorithms, little has been done to verify the algorithms or examine their failure modes. Moreover, complex learning frameworks often require stepping beyond black box evaluation to distinguish between errors based on natural limits on learning and errors that arise from mistakes in implementation. We present a conceptual architecture, failure model and taxonomy, and failure modes and effects analysis (FMEA) of a semi-supervised, multi-modal learning system, and provide specific examples from its use in a radiological analysis assistant system. The goal of the research described in this paper is to provide a foundation from which dependability analysis of systems using semi-supervised, multi-modal learning can be conducted. The methods presented provide a first step towards that overall goal. \u00a9 2011 IEEE.","keywords_author":["dependability","failure analysis","failure modes","machine learning"],"keywords_other":["Semi-supervised learning","Dependability analysis","Semi-supervised","Conceptual architecture","Failure modes and effects analysis","Complex learning","Test applications","Multi-modal","Failure model","dependability","Black boxes"],"max_cite":0.0,"pub_year":2011.0,"sources":"['scp']","rawkeys":["failure model","failure modes","complex learning","black boxes","dependability","machine learning","semi-supervised","failure modes and effects analysis","semi-supervised learning","failure analysis","conceptual architecture","multi-modal","test applications","dependability analysis"],"tags":["failure mode","failure model","recognition","complex learning","black boxes","semi-supervised","machine learning","failure modes and effects analysis","semi-supervised learning","failure analysis","conceptual architecture","multi-modal","test applications","dependability analysis"]},{"p_id":11604,"title":"First-year students' approaches to learning, and factors related to change or stability in their deep approach during a pharmacy course","abstract":"Research shows that a surface approach to learning is more common among students in the natural sciences, while students representing the 'soft' sciences are more likely to apply a deep approach. However, findings conflict concerning the stability of approaches to learning in general. This study explores the variation in students' approaches to learning and aims to analyse factors that are particularly related to the deep approach. The participants were first-year pharmacy students who completed a questionnaire at the beginning and end of the course. The students were interviewed to determine factors related to changes in their deep approach. The results revealed significant changes in approaches to learning at the group level. Yet, closer analysis showed much more variation at the individual level. Findings based on the interviews indicated different factors, self-regulation skills among them, which seemed to explain both the changes in and stability of the students' deep approach.","keywords_author":["approach to learning","change","self-regulation","motivation","pharmacy education"],"keywords_other":["ORIENTATIONS","EXPERIENCES","STRATEGIES","HIGHER-EDUCATION","ENVIRONMENTS","PERCEPTIONS","MOTIVATION","CONCEPTIONS","PSYCHOLOGY","PREFERENCES"],"max_cite":1.0,"pub_year":2017.0,"sources":"['wos']","rawkeys":["self-regulation","experiences","approach to learning","preferences","strategies","psychology","environments","motivation","perceptions","higher-education","change","pharmacy education","conceptions","orientations"],"tags":["self-regulation","recognition","approaches to learning","preferences","orientation","strategies","motivation","perceptions","environment","higher-education","change","pharmacy education","conceptions","experience"]},{"p_id":3419,"title":"Efficient Processing of Deep Neural Networks: A Tutorial and Survey","abstract":"\u00a9 2017 IEEE. Deep neural networks (DNNs) are currently widely used for many artificial intelligence (AI) applications including computer vision, speech recognition, and robotics. While DNNs deliver state-of-the-art accuracy on many AI tasks, it comes at the cost of high computational complexity. Accordingly, techniques that enable efficient processing of DNNs to improve energy efficiency and throughput without sacrificing application accuracy or increasing hardware cost are critical to the wide deployment of DNNs in AI systems. This article aims to provide a comprehensive tutorial and survey about the recent advances toward the goal of enabling efficient processing of DNNs. Specifically, it will provide an overview of DNNs, discuss various hardware platforms and architectures that support DNNs, and highlight key trends in reducing the computation cost of DNNs either solely via hardware design changes or via joint hardware design and DNN algorithm changes. It will also summarize various development resources that enable researchers and practitioners to quickly get started in this field, and highlight important benchmarking metrics and design considerations that should be used for evaluating the rapidly growing number of DNN hardware designs, optionally including algorithmic codesigns, being proposed in academia and industry. The reader will take away the following concepts from this article: understand the key design considerations for DNNs; be able to evaluate different DNN hardware implementations with benchmarks and comparison metrics; understand the tradeoffs between various hardware architectures and platforms; be able to evaluate the utility of various DNN design techniques for efficient processing; and understand recent implementation trends and opportunities.","keywords_author":["ASIC","computer architecture","convolutional neural networks","dataflow processing","deep learning","deep neural networks","energy-efficient accelerators","low power","machine learning","spatial architectures","VLSI","ASIC","computer architecture","convolutional neural networks","dataflow processing","deep learning","deep neural networks","energy-efficient accelerators","low power","machine learning","spatial architectures","VLSI"],"keywords_other":["Dataflow","Energy efficient","Low Power","VLSI","ALGORITHM","COPROCESSOR","RECOGNITION","Convolutional neural network","BINDING"],"max_cite":4.0,"pub_year":2017.0,"sources":"['scp', 'wos', 'ieee']","rawkeys":["vlsi","algorithm","recognition","convolutional neural networks","deep learning","deep neural networks","energy-efficient accelerators","dataflow","asic","dataflow processing","machine learning","spatial architectures","binding","convolutional neural network","low power","coprocessor","computer architecture","energy efficient"],"tags":["vlsi","recognition","spatial architecture","energy-efficient accelerators","asic","dataflow processing","machine learning","binding","energy efficiency","convolutional neural network","algorithms","data flow","low power","coprocessor","computer architecture"]},{"p_id":3424,"title":"Aggregating Rich Hierarchical Features for Scene Classification in Remote Sensing Imagery","abstract":"\u00a9 2008-2012 IEEE.Scene classification is one of the most important issues in remote sensing image processing. To obtain a high discriminative feature representation for an image to be classified, traditional methods usually consider to densely accumulate hand-crafted low-level descriptors (e.g., scale-invariant feature transform) by feature encoding techniques. However, the performance is largely limited by the hand-crafted descriptors as they are not capable of describing the rich semantic information contained in various remote sensing images. To alleviate this problem, we propose a novel method to extract discriminative image features from the rich hierarchical information contained in convolutional neural networks (CNNs). Specifically, the low-level and middle-level intermediate convolutional features are, respectively, encoded by vector of locally aggregated descriptors (VLAD) and then reduced by principal component analysis to obtain hierarchical global features; meanwhile, the fully connected features are average pooled and subsequently normalized to form new global features. The proposed encoded mixed-resolution representation (EMR) is the concatenation of all the above-mentioned global features. Due to the usage of encoding strategies (VLAD and average pooling), our method can deal with images of different sizes. In addition, to reduce the computational consumption in the training stage, we directly extract EMR from VGG-VD and ResNet pretrained on the ImageNet dataset. We show in this paper that CNNs pretrained on the natural image dataset are more easily applied to the remote sensing dataset when the local structure similarity between two datasets is higher. Experimental evaluations on the UC-Merced and Brazilian Coffee Scenes datasets demonstrate that our method is superior to the state of the art.","keywords_author":["Convolutional neural networks (CNNs)","mixed-resolution representation","remote sensing scene classification","vector of locally aggregated descriptors (VLAD)","Convolutional neural networks (CNNs)","mixed-resolution representation","remote sensing scene classification","vector of locally aggregated descriptors (VLAD)"],"keywords_other":["Experimental evaluation","DEEP","MULTISCALE","Scale invariant feature transforms","Hierarchical information","RECOGNITION","Convolutional neural network","SCALE","Discriminative features","Vector of locally aggregated descriptors","Remote sensing image processing","Computational consumption"],"max_cite":5.0,"pub_year":2017.0,"sources":"['scp', 'wos', 'ieee']","rawkeys":["vector of locally aggregated descriptors (vlad)","remote sensing scene classification","remote sensing image processing","recognition","experimental evaluation","mixed-resolution representation","deep","vector of locally aggregated descriptors","computational consumption","discriminative features","convolutional neural network","multiscale","scale invariant feature transforms","convolutional neural networks (cnns)","scale","hierarchical information"],"tags":["remote sensing scene classification","remote sensing image processing","recognition","experimental evaluation","mixed-resolution representation","deep","vector of locally aggregated descriptors","computational consumption","discriminative features","convolutional neural network","multiscale","scale invariant feature transforms","scale","hierarchical information"]},{"p_id":109923,"title":"Multi-oriented text detection and verification in video frames and scene images","abstract":"In this paper, we bring forth a novel approach of video text detection using Fourier-Laplacian filtering in the frequency domain that includes a verification technique using Hidden Markov Model (HMM). The proposed approach deals with the text region appearing not only in horizontal or vertical directions, but also in any other oblique or curved orientation in the image. Until now only a few methods have been proposed that look into curved text detection in video frames, wherein lies our novelty. In our approach, we first apply Fourier-Laplacian transform on the image followed by an ideal Laplacian-Gaussian filtering. Thereafter K-means clustering is employed to obtain the asserted text areas depending on a maximum difference map. Next, the obtained connected components (CC) are skeletonized to distinguish various text strings. Complex components are disintegrated into simpler ones according to a junction removal algorithm followed by a concatenation performed on possible combination of the disjoint skeletons to obtain the corresponding text area. Finally these text hypotheses are verified using HMM-based text\/non-text classification system. False positives are thus eliminated giving us a robust text detection performance. We have tested our framework in multi-oriented text lines in four scripts, namely, English, Chinese, Devanagari and Bengali, in video frames and scene texts. The results obtained show that proposed approach surpasses existing methods on text detection. (C) 2017 Elsevier B.V. All rights reserved.","keywords_author":["Scene text and video text retrieval","Text extraction","Fourier-Laplacian","Hidden Markov model","Skeletonization"],"keywords_other":["CAPTION","EXTRACTION","CLASSIFICATION","MODEL","RECOGNITION","REMOVAL","COMPREHENSIVE SURVEY","TRACKING"],"max_cite":0.0,"pub_year":2017.0,"sources":"['ieee', 'wos']","rawkeys":["fourier-laplacian","scene text and video text retrieval","recognition","model","removal","skeletonization","caption","tracking","classification","hidden markov model","comprehensive survey","text extraction","extraction"],"tags":["fourier-laplacian","hidden markov models","scene text and video text retrieval","recognition","model","removal","skeleton","tracking","caption","classification","comprehensive survey","text extraction","extraction"]},{"p_id":109924,"title":"Multiorientation scene text detection via coarse-to-fine supervision-based convolutional networks","abstract":"Text detection in natural scenes has long been an open challenge and a lot of approaches have been presented, in which the deep learning-based methods have achieved state-of-the-art performance. However, most of them merely use coarse-level supervision information, limiting the detection effectiveness. We propose a deep method utilizing coarse-to-fine supervisions for multiorientation scene text detection. The coarse-to-fine supervisions are generated in three levels: coarse text region (TR), text central line, and fine character shape. With these multiple supervisions, the multiscale feature pyramids and deeply supervised nets are integrated in a unified architecture, and the corresponding convolutional kernels are learned jointly. An effective top-down pipeline is developed to obtain more precise text segmentation regions and their relationship from coarse TR. In addition, the proposed method can handle texts in multiple orientations and languages. Four public datasets, i.e., ICDAR2013, MSRA-TD500, USTB, and street view text dataset, are used to evaluate the performance of our proposed method. The experimental results show that our method achieves the state-of-the-art performance. (C) 2018 SPIE and IS&T","keywords_author":["scene text detection","multiorientation texts","text segmentation","convolutional neural networks"],"keywords_other":["ALGORITHMS","RECOGNITION","NATURAL IMAGES","LOCALIZATION"],"max_cite":0.0,"pub_year":2018.0,"sources":"['wos']","rawkeys":["recognition","convolutional neural networks","localization","text segmentation","scene text detection","natural images","algorithms","multiorientation texts"],"tags":["recognition","localization","text segmentation","scene text detection","convolutional neural network","natural images","algorithms","multiorientation texts"]},{"p_id":3428,"title":"Robust Spatial Filtering with Graph Convolutional Neural Networks","abstract":"Convolutional neural networks (CNNs) have recently led to incredible breakthroughs on a variety of pattern recognition problems. Banks of finite-impulse response filters are learned on a hierarchy of layers, each contributing more abstract information than the previous layer. The simplicity and elegance of the convolutional filtering process makes them perfect for structured problems, such as image, video, or voice, where vertices are homogeneous in the sense of number, location, and strength of neighbors. The vast majority of classification problems, for example in the pharmaceutical, homeland security, and financial domains are unstructured. As these problems are formulated into unstructured graphs, the heterogeneity of these problems, such as number of vertices, number of connections per vertex, and edge strength, cannot be tackled with standard convolutional techniques. We propose a novel neural learning framework that is capable of handling both homogeneous and heterogeneous data while retaining the benefits of traditional CNN successes. Recently, researchers have proposed variations of CNNs that can handle graph data. In an effort to create learnable filter banks of graphs, these methods either induce constraints on the data or require preprocessing. As opposed to spectral methods, our framework, which we term Graph-CNNs, defines filters as polynomials of functions of the graph adjacency matrix. Graph-CNNs can handle both heterogeneous and homogeneous graph data, including graphs having entirely different vertex or edge sets. We perform experiments to validate the applicability of Graph-CNNs to a variety of structured and unstructured classification problems and demonstrate state-of-the-art results on document and molecule classification problems.","keywords_author":["Convolutional neural networks","deep learning","graph signal processing","Convolutional neural networks","deep learning","graph signal processing"],"keywords_other":["Three-dimensional display","Structured problems","CLASSIFICATION","Heterogeneous data","Pattern recognition problems","RECOGNITION","Convolutional neural network","Spatial filterings","Filtering process","Financial domains"],"max_cite":1.0,"pub_year":2017.0,"sources":"['scp', 'wos', 'ieee']","rawkeys":["three-dimensional display","recognition","convolutional neural networks","spatial filterings","filtering process","deep learning","financial domains","heterogeneous data","classification","convolutional neural network","pattern recognition problems","structured problems","graph signal processing"],"tags":["recognition","spatial filters","filtering process","structural problems","machine learning","financial domains","heterogeneous data","three-dimensional displays","classification","convolutional neural network","pattern recognition problems","graph signal processing"]},{"p_id":109925,"title":"Tracking Based Multi-Orientation Scene Text Detection: A Unified Framework With Dynamic Programming","abstract":"There are a variety of grand challenges for multi-orientation text detection in scene videos, where the typical issues include skew distortion, low contrast, and arbitrary motion. Most conventional video text detection methods using individual frames have limited performance. In this paper, we propose a novel tracking based multi-orientation scene text detection method using multiple frames within a unified framework via dynamic programming. First, a multi-information fusion-based multi-orientation text detection method in each frame is proposed to extensively locate possible character candidates and extract text regions with multiple channels and scales. Second, an optimal tracking trajectory is learned and linked globally over consecutive frames by dynamic programming to finally refine the detection results with all detection, recognition, and prediction information. Moreover, the effectiveness of our proposed system is evaluated with the state-of-the-art performances on several public data sets of multi-orientation scene text images and videos, including MSRA-TD500, USTB-SV1K, and ICDAR 2015 Scene Videos.","keywords_author":["Scene text detection","tracking-based text detection","multi-orientation scene text","dynamic programming"],"keywords_other":["LOCALIZATION","VIDEO IMAGES","EXTRACTION","SYSTEM","LINE DETECTION","LICENSE PLATES","RECOGNITION","NATURAL IMAGES","TRANSLATION","OPTIMIZATION"],"max_cite":2.0,"pub_year":2017.0,"sources":"['ieee', 'wos']","rawkeys":["recognition","tracking-based text detection","localization","video images","license plates","translation","system","scene text detection","multi-orientation scene text","dynamic programming","line detection","natural images","optimization","extraction"],"tags":["recognition","tracking-based text detection","localization","license plates","translation","system","scene text detection","multi-orientation scene text","dynamic programming","video image","line detection","natural images","optimization","extraction"]},{"p_id":109927,"title":"TextBoxes plus plus : A Single-Shot Oriented Scene Text Detector","abstract":"Scene text detection is an important step of scene text recognition system and also a challenging problem. Different from general object detections, the main challenges of scene text detection lie on arbitrary orientations, small sizes, and significantly variant aspect ratios of text in natural images. In this paper, we present an end-to-end trainable fast scene text detector, named TextBoxes++, which detects arbitraryoriented scene text with both high accuracy and efficiency in a single network forward pass. No post-processing other than efficient non-maximum suppression is involved. We have evaluated the proposed TextBoxes++ on four public data sets. In all experiments, TextBoxes++ outperforms competing methods in terms of text localization accuracy and runtime. More specifically, TextBoxes++ achieves an f-measure of 0.817 at 11.6 frames\/s for 1024 x 1024 ICDAR 2015 incidental text images and an f-measure of 0.5591 at 19.8 frames\/s for 768 x 768 COCO-Text images. Furthermore, combined with a text recognizer, TextBoxes++ significantly outperforms the state-of-the- art approaches for word spotting and end-to-end text recognition tasks on popular benchmarks.","keywords_author":["Scene text detection","multi-oriented text","word spotting","scene text recognition","convolutional neural networks"],"keywords_other":["DICTIONARIES","CLASSIFICATION","LINE DETECTION","RECOGNITION","NATURAL IMAGES","WILD","NEURAL-NETWORK"],"max_cite":0.0,"pub_year":2018.0,"sources":"['wos']","rawkeys":["recognition","convolutional neural networks","scene text detection","multi-oriented text","classification","neural-network","line detection","natural images","word spotting","dictionaries","scene text recognition","wild"],"tags":["recognition","neural networks","scene text detection","classification","convolutional neural network","line detection","natural images","word spotting","dictionaries","scene text recognition","wild","multiorientation texts"]},{"p_id":11623,"title":"Noise-enhanced convolutional neural networks","abstract":"Injecting carefully chosen noise can speed convergence in the backpropagation training of a convolutional neural network (CNN). The Noisy CNN algorithm speeds training on average because the backpropagation algorithm is a special case of the generalized expectation-maximization (EM) algorithm and because such carefully chosen noise always speeds up the EM algorithm on average. The CNN framework gives a practical way to learn and recognize images because backpropagation scales with training data. It has only linear time complexity in the number of training samples. The Noisy CNN algorithm finds a special separating hyperplane in the network's noise space. The hyperplane arises from the likelihood-based positivity condition that noise-boosts the EM algorithm. The hyperplane cuts through a uniform-noise hypercube or Gaussian ball in the noise space depending on the type of noise used. Noise chosen from above the hyperplane speeds training on average. Noise chosen from below slows it on average. The algorithm can inject noise anywhere in the multilayered network. Adding noise to the output neurons reduced the average per-iteration training-set cross entropy by 39% on a standard MNIST image test set of handwritten digits. It also reduced the average per-iteration training-set classification error by 47%. Adding noise to the hidden layers can also reduce these performance measures. The noise benefit is most pronounced for smaller data sets because the largest EM hill-climbing gains tend to occur in the first few iterations. This noise effect can assist random sampling from large data sets because it allows a smaller random sample to give the same or better performance than a noiseless sample gives. (C) 2015 Elsevier Ltd. All rights reserved.","keywords_author":["2 Backpropagation","Noise injection","Convolutional neural network","Expectation-maximization algorithm","Stochastic resonance","Sampling from big data sets"],"keywords_other":["BENEFITS","STOCHASTIC RESONANCE","RECOGNITION","LEVY NOISE"],"max_cite":11.0,"pub_year":2016.0,"sources":"['ieee', 'wos']","rawkeys":["benefits","recognition","stochastic resonance","expectation-maximization algorithm","2 backpropagation","levy noise","convolutional neural network","sampling from big data sets","noise injection"],"tags":["benefits","recognition","stochastic resonance","expectation-maximization algorithms","2 backpropagation","levy noise","convolutional neural network","sampling from big data sets","noise injection"]},{"p_id":109926,"title":"Method for unconstrained text detection in natural scene image","abstract":"Text detection in natural scene images is an important prerequisite for many content-based multimedia understanding applications. The authors present a simple and effective text detection method in natural scene image. Firstly, MSERs are extracted by the V-MSER algorithm from channels of G, H, S, O-1, and O-2, as component candidates. Since text is composed of character candidates, the authors design an MRF model to exploit the relationship between characters. Secondly, in order to filter out non-text components, they design a set of two-layers filtering scheme: most of the non-text components can be filtered by the first layer of the filtering scheme; the second layer filtering scheme is an AdaBoost classifier, which is trained by the features of compactness, horizontal variance and vertical variance, and aspect ratio. Then, only four simple features are adopted to generate component pairs. Finally, according to the orientation similarity of the component pairs, component pairs which have roughly the same orientation are merged into text lines. The proposed method is evaluated on two public datasets: ICDAR 2011 and MSRA-TD500. It achieves 82.94 and 75% F-measure, respectively. Especially, the experimental results, on their URMQ_LHASA-TD220 dataset which contains 220 images for multi-orientation and multi-language text lines evaluation, show that the proposed method is general for detecting scene text lines in different languages.","keywords_author":["text detection","multimedia systems","image filtering","image classification","natural language processing","unconstrained text detection","natural scene image","content-based multimedia understanding applications","V-MSER algorithm","character candidates","MRF model","nontext component filterig","two-layers filtering scheme","first layer filtering scheme","second layer filtering scheme","AdaBoost classifier","compactness","horizontal variance","vertical variance","aspect ratio","component pair orientation similarity","ICDAR 2011","MSRA-TD500","URMQ_LHASA-TD220 dataset","multiorientation text lines evaluation","multilanguage text lines evaluation"],"keywords_other":["STABLE EXTREMAL REGIONS","RECOGNITION"],"max_cite":1.0,"pub_year":2017.0,"sources":"['ieee', 'wos']","rawkeys":["horizontal variance","image filtering","text detection","second layer filtering scheme","two-layers filtering scheme","urmq_lhasa-td220 dataset","image classification","content-based multimedia understanding applications","first layer filtering scheme","multilanguage text lines evaluation","aspect ratio","stable extremal regions","compactness","adaboost classifier","icdar 2011","recognition","nontext component filterig","natural scene image","unconstrained text detection","v-mser algorithm","multimedia systems","character candidates","msra-td500","vertical variance","natural language processing","mrf model","component pair orientation similarity","multiorientation text lines evaluation"],"tags":["horizontal variance","image filtering","text detection","second layer filtering scheme","two-layers filtering scheme","urmq_lhasa-td220 dataset","image classification","content-based multimedia understanding applications","first layer filtering scheme","multilanguage text lines evaluation","aspect ratio","stable extremal regions","natural scene images","compactness","icdar 2011","recognition","nontext component filterig","unconstrained text detection","v-mser algorithm","multimedia systems","character candidates","msra-td500","ada boost classifiers","vertical variance","natural language processing","mrf model","component pair orientation similarity","multiorientation text lines evaluation"]},{"p_id":3429,"title":"Over-the-Air Deep Learning Based Radio Signal Classification","abstract":"We conduct an in depth study on the performance of deep learning based radio signal classification for radio communications signals. We consider a rigorous baseline method using higher order moments and strong boosted gradient tree classification, and compare performance between the two approaches across a range of configurations and channel impairments. We consider the effects of carrier frequency offset, symbol rate, and multipath fading in simulation, and conduct over-the-air measurement of radio classification performance in the lab using software radios, and we compare performance and training strategies for both. Finally, we conclude with a discussion of remaining problems, and design considerations for using such techniques.","keywords_author":["Cognitive radio","deep learning","modulation","neural networks","pattern recognition","sensor systems and applications","wireless communication","Cognitive radio","deep learning","modulation","neural networks","pattern recognition","sensor systems and applications","wireless communication"],"keywords_other":["Classification performance","Baseline methods","Channel conditions","In-depth study","Wireless communications","RECOGNITION","Over the airs","Design considerations","INTERCEPTION","Higher order moments"],"max_cite":0.0,"pub_year":2018.0,"sources":"['scp', 'wos', 'ieee']","rawkeys":["higher order moments","recognition","classification performance","sensor systems and applications","deep learning","neural networks","over the airs","cognitive radio","modulation","design considerations","channel conditions","wireless communications","interception","pattern recognition","wireless communication","in-depth study","baseline methods"],"tags":["higher order moments","recognition","classification performance","sensor systems and applications","neural networks","over the airs","machine learning","cognitive radio","modulation","design considerations","channel conditions","wireless communications","interception","pattern recognition","in-depth study","baseline methods"]},{"p_id":11626,"title":"Learning Pooling for Convolutional Neural Network","abstract":"Convolutional neural networks (CNNs) consist of alternating convolutional layers and pooling layers. The pooling layer is obtained by applying pooling operator to aggregate information within each small region of the input feature channels and then down sampling the results. Typically, hand-crafted pooling operations are used to aggregate information within a region, but they are not guaranteed to minimize the training error. To overcome this drawback, we propose a learned pooling operation obtained by end-to-end training which is called LEAP (LEArning Pooling). Specifically, in our method, one shared linear combination of the neurons in the region is learned for each feature channel (map). In fact, average pooling can be seen as one special case of our method where all the weights are equal. In addition, inspired by the LEAP operation, We propose one simplified convolution operation to replace the traditional convolution which consumes many extra parameters. The simplified convolution greatly reduces the number of parameters while maintaining comparable performance. By combining the proposed LEAP method and the simplified convolution, we demonstrate the state-of-the-art classification performance with moderate parameters on three public object recognition benchmarks: CIFAR10 dataset, CIFAR100 dataset, and ImageNet2012 dataset.","keywords_author":["Convolutional Neural Networks","Learning Pooling","Object Recognition","Simplified Convolution","Convolutional Neural Networks","Object Recognition","Learning Pooling","Simplified Convolution"],"keywords_other":["Classification performance","Linear combinations","State of the art","Down sampling","RECOGNITION","Convolutional neural network","Learning Pooling","Input features","Training errors"],"max_cite":7.0,"pub_year":2017.0,"sources":"['ieee', 'scp', 'wos']","rawkeys":["simplified convolution","recognition","convolutional neural networks","classification performance","linear combinations","training errors","state of the art","learning pooling","input features","object recognition","convolutional neural network","down sampling"],"tags":["simplified convolution","recognition","classification performance","linear combinations","training errors","state of the art","learning pooling","input features","object recognition","convolutional neural network","down sampling"]},{"p_id":11628,"title":"Convolutional neural networks analyzed via convolutional sparse coding","abstract":"Convolutional neural networks (CNN) have led to many state-of-the-art results spanning through various fields. However, a clear and profound theoretical understanding of the forward pass, the core algorithm of CNN, is still lacking. In parallel, within the wide field of sparse approximation, Convolutional Sparse Coding (CSC) has gained increasing attention in recent years. A theoretical study of this model was recently conducted, establishing it as a reliable and stable alternative to the commonly practiced patch-based processing. Herein, we propose a novel multi-layer model, ML-CSC, in which signals are assumed to emerge from a cascade of CSC layers. This is shown to be tightly connected to CNN, so much so that the forward pass of the CNN is in fact the thresholding pursuit serving the ML-CSC model. This connection brings a fresh view to CNN, as we are able to attribute to this architecture theoretical claims such as uniqueness of the representations throughout the network, and their stable estimation, all guaranteed under simple local sparsity conditions. Lastly, identifying the weaknesses in the above pursuit scheme, we propose an alternative to the forward pass, which is connected to deconvolutional and recurrent networks, and also has better theoretical guarantees.","keywords_author":["Basis pursuit","Convolutional neural networks","Convolutional sparse coding","Deep learning","Forward pass","Sparse representation","Thresholding algorithm","Deep Learning","Convolutional Neural Networks","Forward Pass","Sparse Representation","Convolutional Sparse Coding","Thresholding Algorithm","Basis Pursuit"],"keywords_other":["Forward pass","INVARIANT SCATTERING","APPROXIMATION","NOISE","REPRESENTATIONS","Sparse representation","SIGNAL RECOVERY","RECOGNITION","Convolutional neural network","Thresholding algorithms","DICTIONARIES","Basis pursuit","Sparse coding"],"max_cite":5.0,"pub_year":2017.0,"sources":"['scp', 'wos']","rawkeys":["invariant scattering","recognition","convolutional neural networks","noise","basis pursuit","deep learning","forward pass","sparse representation","thresholding algorithms","convolutional sparse coding","approximation","representations","convolutional neural network","signal recovery","dictionaries","sparse coding","thresholding algorithm"],"tags":["invariant scattering","recognition","noise","basis pursuit","forward pass","sparse representation","machine learning","convolutional sparse coding","representation","approximation","convolutional neural network","signal recovery","dictionaries","sparse coding","thresholding algorithm"]},{"p_id":11622,"title":"Directly Connected Convolutional Neural Networks","abstract":"Convolutional neural networks (CNNs) have better performance in feature extraction and classification. Most of the applications are based on a traditional structure of CNNs. However, due to the fixed structure, it may not be effective for large dataset which will spend much time for training. So, we use a new algorithm to optimize CNNs, called directly connected convolutional neural networks (DCCNNs). In DCCNNs, the down-sampling layer can directly connect the output layer with three-dimensional matrix operation, without full connection (i.e., matrix vectorization). Thus, DCCNNs have less weights and neurons than CNNs. We conduct the comparison experiments on five image databases: MNIST, COIL-20, AR, Extended Yale B, and ORL. The experiments show that the model has better recognition accuracy and faster convergence than CNNs. Furthermore, two applications (i.e., water quality evaluation and image classification) following the proposed concepts further confirm the generality and capability of DCCNNs.","keywords_author":["Convolutional neural networks","directly connected","three-dimensional matrix","water quality evaluation","image classification"],"keywords_other":["IMAGE CLASSIFICATION","RECOGNITION","PREDICTION","FEATURES"],"max_cite":0.0,"pub_year":2018.0,"sources":"['ieee', 'wos']","rawkeys":["recognition","convolutional neural networks","features","prediction","water quality evaluation","three-dimensional matrix","directly connected","image classification"],"tags":["recognition","features","prediction","water quality evaluation","convolutional neural network","three-dimensional matrix","directly connected","image classification"]},{"p_id":109932,"title":"A robust proposal generation method for text lines in natural scene images","abstract":"Motivated by the success of object proposal generation methods for object detection, we propose a novel method for generating text line proposals from natural scene images. Our strategy is to detect text regions which we define as part of text lines containing a whole character or transitions between two adjacent characters. We observe that, if we scale text regions to a small and fixed size, their image gradients exhibit certain patterns irrespective of text shapes and language types. Based on this observation, we propose simple features which consist of means and standard deviations of image gradients to train a Random Forest so as to detect text regions over multiple image scales and color channels. Text regions are then merged into text line candidates which are ranked based on the Random Forest responses combined with the shapes of the candidates, e.g., horizontally elongated candidates are given higher scores, because they are more likely to contain texts. Even though our method is trained on English, our experiments demonstrate that it achieves high recall with a few thousand good quality proposals on four standard benchmarks, including multi-language datasets. Following the One-to-One and Many-to-One detection criteria, our method achieves 91.6%, 87.4%, 92.1% and 97.9% recall on the ICDAR 2013 Robust Reading Dataset, Street View Text Dataset, Pan's multilingual Dataset and Sampled KAIST Scene Text Dataset respectively, with an average of less than 1250 proposals. (c) 2018 Elsevier B.V. All rights reserved.","keywords_author":["Scene text detection","Feature extraction","Text line proposals","Random Forest"],"keywords_other":["RECOGNITION","EXTRACTION","CLASSIFICATION","GRADIENTS"],"max_cite":0.0,"pub_year":2018.0,"sources":"['wos']","rawkeys":["recognition","text line proposals","scene text detection","gradients","extraction","classification","feature extraction","random forest"],"tags":["recognition","text line proposals","random forests","scene text detection","gradient","classification","feature extraction","extraction"]},{"p_id":3439,"title":"A Deep Learning Approach to UAV Image Multilabeling","abstract":"In this letter, we face the problem of multilabeling unmanned aerial vehicle (UAV) imagery, typically characterized by a high level of information content, by proposing a novel method based on convolutional neural networks. These are exploited as a means to yield a powerful description of the query image, which is analyzed after subdividing it into a grid of tiles. The multilabel classification task of each tile is performed by the combination of a radial basis function neural network and a multilabeling layer (ML) composed of customized thresholding operations. Experiments conducted on two different UAV image data sets demonstrate the promising capability of the proposed method compared to the state of the art, at the expense of a higher but still contained computation time.","keywords_author":["Convolutional neural networks (CNNs)","image multilabeling","Otsu's algorithm","unmanned aerial vehicles (UAVs)","urban monitoring","Convolutional neural networks (CNNs)","image multilabeling","Otsu's algorithm","unmanned aerial vehicles (UAVs)","urban monitoring"],"keywords_other":["Radial basis function neural networks","Multi-label classifications","Computation time","State of the art","RECOGNITION","Convolutional neural network","Learning approach","Image datasets","Information contents"],"max_cite":6.0,"pub_year":2017.0,"sources":"['ieee', 'scp', 'wos']","rawkeys":["image datasets","recognition","unmanned aerial vehicles (uavs)","urban monitoring","information contents","state of the art","image multilabeling","radial basis function neural networks","otsu's algorithm","multi-label classifications","computation time","convolutional neural network","convolutional neural networks (cnns)","learning approach"],"tags":["unmanned aerial vehicle","image datasets","recognition","urban monitoring","information contents","state of the art","image multilabeling","radial basis function neural networks","otsu's algorithm","convolutional neural network","multi label classification","learning approach","computational time"]},{"p_id":109929,"title":"Multi-oriented text detection from natural scene images based on a CNN and pruning non-adjacent graph edges","abstract":"Due to the complex backgrounds, size variations, and changes in perspective and orientation in natural scene images, detecting multi-oriented text is a difficult problem that has recently attracted considerable attention from research communities. In this paper, we present a novel method that effectively and robustly detects multi-oriented text in natural scene images. First, the candidate characters are generated by an exhaustive segmentation-based method that can extract characters in arbitrary orientations. Second, a convolutional neural network (CNN) model is employed to filter out the non-character regions; this model is also robust to arbitrary character orientations. Finally, text-line grouping is treated as a problem of pruning non-adjacent graph edges from a graph in which each vertex represents a character candidate region. To evaluate our algorithm, we compare it with other existing algorithms by performing experiments on three public datasets: ICDAR 2013, the Oriented Scene Text Dataset (OSTD) and USTB-SV1K. The results show that the proposed method handles any arbitrary text orientation well, and it achieves promising results on these three public datasets.","keywords_author":["Text detection","Scene image","Multi-orientation","CNN"],"keywords_other":["LOCALIZATION","EXTRACTION","MODEL","RECOGNITION","READING TEXT","STABLE EXTREMAL REGIONS","VIDEO"],"max_cite":0.0,"pub_year":2018.0,"sources":"['ieee', 'wos']","rawkeys":["scene image","recognition","model","localization","reading text","multi-orientation","text detection","cnn","video","stable extremal regions","extraction"],"tags":["scene image","recognition","model","localization","reading text","multi-orientation","text detection","video","convolutional neural network","stable extremal regions","extraction"]},{"p_id":3452,"title":"Learning Deep NBNN Representations for Robust Place Categorization","abstract":"This letter presents an approach for semantic place categorization using data obtained from RGB cameras. Previous studies on visual place recognition and classification have shown that by considering features derived from pretrained convolutional neural networks (CNNs) in combination with part-based classification models, high recognition accuracy can be achieved, even in the presence of occlusions and severe viewpoint changes. Inspired by these works, we propose to exploit local deep representations, representing images as set of regions applying a Naive Bayes nearest neighbor (NBNN) model for image classification. As opposed to previous methods, where CNNs are merely used as feature extractors, our approach seamlessly integrates the NBNN model into a fully CNN. Experimental results show that the proposed algorithm outperforms previous methods based on pretrained CNN models and that, when employed in challenging robot place recognition tasks, it is robust to occlusions, environmental and sensor changes.","keywords_author":["Recognition","semantic scene understanding","visual learning"],"keywords_other":["DATABASE","LOCALIZATION","SCENE"],"max_cite":0.0,"pub_year":2017.0,"sources":"['ieee', 'wos']","rawkeys":["semantic scene understanding","recognition","localization","visual learning","scene","database"],"tags":["semantic scene understanding","recognition","localization","databases","visual learning","scene"]},{"p_id":68990,"title":"Joint Discriminative Dictionary and Classifier Learning for ALS Point Cloud Classification","abstract":"To efficiently recognize on-ground objects in airborne laser scanning (ALS) point clouds, we design a method that jointly learns a discriminative dictionary and a classifier. In the method, the point cloud is segmented into hierarchical point clusters, which are organized by a tree structure. Then, the feature of each point cluster is extracted. The feature of a leaf node is obtained by aggregating the features of all its parent nodes. The feature of the leaf node is called the hierarchical aggregation feature. The hierarchical aggregation features are encoded by sparse coding. We introduce a new label consistency constraint called \"discriminative sparse-code error,\" and combine it with the reconstruction error, the classification error, and L-1-norm sparsity constraint to form a unified objective function. The objective function is efficiently solved by using the proposed label consistency feature sign method. We obtain an overcomplete discriminative dictionary and an optimal linear classifier. Experiments performed on different ALS point cloud scenes have shown that the hierarchical aggregation features combined with the learned classifier can significantly enhance the classification results, and also demonstrated the superior performance of our method over other techniques in point cloud classification.","keywords_author":["Airborne laser scanning (ALS) point clouds","classification","discriminative dictionary learning","hierarchical aggregation feature","point clusters","sparse coding"],"keywords_other":["LIDAR DATA","FEATURES","MULTISCALE","EXTRACTION","K-SVD","LASER-SCANNING DATA","IMAGE CLASSIFICATION","SEGMENTATION","RECOGNITION","SPARSE REPRESENTATION"],"max_cite":1.0,"pub_year":2018.0,"sources":"['ieee', 'wos']","rawkeys":["airborne laser scanning (als) point clouds","lidar data","recognition","segmentation","features","hierarchical aggregation feature","sparse coding","sparse representation","laser-scanning data","classification","multiscale","discriminative dictionary learning","image classification","point clusters","extraction","k-svd"],"tags":["airborne laser scanning (als) point clouds","lidar data","recognition","segmentation","features","hierarchical aggregation feature","sparse coding","sparse representation","laser-scanning data","classification","multiscale","discriminative dictionary learning","image classification","point clusters","extraction","k-svd"]},{"p_id":11648,"title":"Adaptive activation functions in convolutional neural networks","abstract":"Activation functions play important roles in deep convolutional neural networks. This work focuses on learning activation functions via combining basic activation functions in a data-driven way. We explore three strategies to learn the activation functions, and allow the activation operation to be adaptive to inputs. We firstly explore two strategies to linearly and nonlinearly combine basic activation functions, respectively. Then we further investigate a strategy that basic activation functions are combined in a way of a hierarchical integration. Experiments demonstrate that the proposed activation functions lead to better performances than ReLU and its variants on benchmarks with various scales. (C) 2017 Elsevier B.V. All rights reserved.","keywords_author":["Convolutional neural networks","Activation function learning","Adaptive activation","Hierarchical activation"],"keywords_other":["RECOGNITION","WINNER-TAKE-ALL"],"max_cite":2.0,"pub_year":2018.0,"sources":"['wos']","rawkeys":["recognition","convolutional neural networks","winner-take-all","hierarchical activation","adaptive activation","activation function learning"],"tags":["recognition","winner-take-all","hierarchical activation","adaptive activation","convolutional neural network","activation function learning"]},{"p_id":68994,"title":"Joint Margin, Cograph, and Label Constraints for Semisupervised Scene Parsing From Point Clouds","abstract":"To parse large-scale urban scenes using the supervised methods, a large amount of training data that can account for the vast visual and structural variance of urban environment is necessary. Unfortunately, such training data are mostly obtained by tedious and time-consuming manual work. To overcome the drawback, we propose a semisupervised learning framework that combines the margin, cograph, and label constraints into an objective function for point cloud parsing. Mathematically, the margin constraint is presented to learn a novel distance criterion that can effectively recognize points of different classes. The graph regularization is then employed to characterize the intrinsic geometry structure of the data manifold and explore relationships among points. The label consistency regularization is introduced to ensure the category consistency of the clustered points and single point. To classify the out-of-sample data, the framework successfully transforms the semisupervised classification results into the linear classifier by adopting a linear regression. An iterative algorithm is utilized to efficiently and effectively optimize the objective function with characteristics of multiple variables and highly nonlinear. The point clouds of four urban scenes are used to validate our method. The experimental results show that our method outperforms the state-of-the-art algorithms.","keywords_author":["Classification","feature","label consistency","manifold regularization","margin constraint","point cloud"],"keywords_other":["CONTEXTUAL CLASSIFICATION","URBAN AREAS","OBLIQUE AERIAL IMAGES","TARGET DETECTION","LIDAR DATA","FEATURES","LASER-SCANNING DATA","RECOGNITION","SEGMENTATION","FUSION"],"max_cite":0.0,"pub_year":2018.0,"sources":"['ieee', 'wos']","rawkeys":["point cloud","urban areas","margin constraint","recognition","label consistency","features","oblique aerial images","segmentation","laser-scanning data","target detection","manifold regularization","classification","feature","contextual classification","fusion","lidar data"],"tags":["point cloud","urban areas","margin constraint","recognition","label consistency","features","oblique aerial images","segmentation","laser-scanning data","target detection","manifold regularization","classification","contextual classification","fusion","lidar data"]},{"p_id":11651,"title":"Gender classification: a convolutional neural network approach","abstract":"An approach using a convolutional neural network (CNN) is proposed for real-time gender classification based on facial images. The proposed CNN architecture exhibits a much reduced design complexity when compared with other CNN solutions applied in pattern recognition. The number of processing layers in the CNN is reduced to only four by fusing the convolutional and subsampling layers. Unlike in conventional CNNs, we replace the convolution operation with cross-correlation, hence reducing the computational load. The network is trained using a second-order backpropagation learning algorithm with annealed global learning rates. Performance evaluation of the proposed CNN solution is conducted on two publicly available face databases of SUMS and AT&T. We achieve classification accuracies of 98.75% and 99.38% on the SUMS and AT&T databases, respectively. The neural network is able to process and classify a 32 x 32 pixel face image in less than 0.27 ms, which corresponds to a very high throughput of over 3700 images per second. Training converges within less than 20 epochs. These results correspond to a superior classification performance, verifying that the proposed CNN is an effective real-time solution for gender recognition.","keywords_author":["Gender classification","convolutional neural network","fused convolutional and subsampling layers","backpropagation"],"keywords_other":["RECOGNITION"],"max_cite":5.0,"pub_year":2016.0,"sources":"['wos']","rawkeys":["recognition","fused convolutional and subsampling layers","gender classification","convolutional neural network","backpropagation"],"tags":["recognition","fused convolutional and subsampling layers","gender classification","convolutional neural network","backpropagation"]},{"p_id":68997,"title":"Identification of pole-like structures from mobile lidar data of complex road environment","abstract":"Pole-like structures (PLSs) located in road environment are important roadway assets. They play a vital role in road safety inspection and road planning. The use of light detection and ranging (lidar) based mobile mapping technology for mapping of PLSs is an important area of research as it holds the potential for automation. Point cloud data of rural, peri-urban, and urban road environment are used in this study, which pose special challenge in view of the complexity of terrain, unlike well-planned roads, which have been the subject of interest in existing literature for identification of PLSs. A new five-step method is proposed in this article. The first two steps, i.e. ground filtering and voxelization of filtered non-ground points, are used for data size reduction. Next three steps are used to extract PLSs from reduced data. The proposed method was tested on point cloud data of three test sites having different levels of complexities. PLSs including partially occluded pole, tilted pole, pole situated very close to other objects, and vertical pole attached to tilted pole were accurately identified. Average correctness and completeness, respectively of 92.6% and 94.9%, were achieved in three different complex test sites, i.e. urban, peri-urban, and rural sites, respectively. Computation complexity shows that our proposed method delivers fast and computationally efficient solution for identifying the PLSs from volumetric mobile lidar point cloud. Impact of PLSs on road safety and road planning is also addressed for these selected test sites.","keywords_author":null,"keywords_other":["AUTOMATED EXTRACTION","SUPERVOXEL","LIGHT POLES","CLASSIFICATION","LASER-SCANNING DATA","HOUGH FOREST","SEGMENTATION","RECOGNITION","POINT-CLOUDS","OBJECT DETECTION"],"max_cite":4.0,"pub_year":2016.0,"sources":"['wos']","rawkeys":["hough forest","point-clouds","recognition","segmentation","light poles","automated extraction","laser-scanning data","supervoxel","classification","object detection"],"tags":["point cloud","recognition","segmentation","light poles","automated extraction","laser-scanning data","hough forests","classification","object detection","supervoxels"]},{"p_id":3472,"title":"Deep multimodal learning: A survey on recent advances and trends","abstract":"The success of deep learning has been a catalyst to solving increasingly complex machine-learning problems, which often involve multiple data modalities. We review recent advances in deep multimodal learning and highlight the state-of the art, as well as gaps and challenges in this active research field. We first classify deep multimodal learning architectures and then discuss methods to fuse learned multimodal representations in deep-learning architectures. We highlight two areas of research-regularization strategies and methods that learn or optimize multimodal fusion structures-as exciting areas for future work.","keywords_author":null,"keywords_other":["Complex machines","Multiple data","Multi-modal fusion","MEDICAL IMAGE-ANALYSIS","Learning architectures","Research fields","Multi-modal learning","State of the art","NEURAL-NETWORKS","CLASSIFICATION","RECOGNITION","ALGORITHMS","FUSION","Multi-modal","OPTIMIZATION","VIDEO"],"max_cite":2.0,"pub_year":2017.0,"sources":"['wos', 'scp', 'ieee']","rawkeys":["complex machines","learning architectures","multi-modal learning","neural-networks","recognition","research fields","multi-modal fusion","state of the art","video","classification","multi-modal","multiple data","algorithms","medical image-analysis","optimization","fusion"],"tags":["complex machines","learning architectures","medical image analysis","multi-modal learning","recognition","research fields","multi-modal fusion","neural networks","state of the art","video","classification","multi-modal","multiple data","algorithms","optimization","fusion"]},{"p_id":11664,"title":"Audio Recapture Detection With Convolutional Neural Networks","abstract":"In this paper, we investigate how features can be effectively learned by deep neural networks for audio forensic problems. By providing a preliminary feature preprocessing based on electric network frequency (ENF) analysis, we propose a convolutional neural network (CNN) for training and classification of genuine and recaptured audio recordings. Hierarchical representations which contain levels of details of the ENF components are learned from the deep neural networks and can be used for further classification. The proposed method works for small audio clips of 2 second duration, whereas the state of the art may fail with such small audio clips. Experimental results demonstrate that the proposed network yields high detection accuracy with each ENF harmonic component represented as a single-channel input. The performance can be further improved by a combined input representation which incorporates both the fundamental ENF and its harmonics. The convergence property of the network and the effect of using an analysis window with various sizes are also studied. Performance comparison against the support tensor machine demonstrates the advantage of using CNN for the task of audio recapture detection. Moreover, visualization of the intermediate feature maps provides some insight into what the deep neural networks actually learn and how they make decisions.","keywords_author":["Audio recapture detection","convolutional neural network (CNN)","electric network frequency (ENF)"],"keywords_other":["RECOGNITION","VIDEO","FORENSICS"],"max_cite":0.0,"pub_year":2016.0,"sources":"['ieee', 'wos']","rawkeys":["recognition","electric network frequency (enf)","audio recapture detection","convolutional neural network (cnn)","video","forensics"],"tags":["recognition","audio recapture detection","video","convolutional neural network","forensics","electric network frequency"]},{"p_id":11674,"title":"Facial landmark localization by enhanced convolutional neural network","abstract":"Facial landmark localization is important to many facial recognition and analysis tasks, such as face attributes analysis, head pose estimation, 3D face modeling, and facial expression analysis. In this paper, we propose a new approach to localizing landmarks in facial image by deep convolutional neural network (DCNN). We make two enhancements on the CNN to adapt it to the feature localization task as follows. First, we replace the commonly used max pooling by depth-wise convolution to obtain better localization performance. Second, we define a response map for each facial points as a 2D probability map indicating the presence likelihood, and train our model with a KL divergence loss. To obtain robust localization results, our approach first takes the expectations of the response maps of enhanced CNN and then applies auto-encoder model to the global shape vector, which is effective to rectify the outlier points by the prior global landmark configurations. The proposed ECNN method achieves 5.32% mean error on the experiments on the 300-W dataset, which is comparable to the state-of-the-art performance on this standard benchmark, showing the effectiveness of our methods. (C) 2017 Elsevier B.V. All rights reserved.","keywords_author":["Convolutional neural network","Deep learning","Face alignment","Face recognition","Facial landmark localization","Face recognition","Face alignment","Facial landmark localization","Convolutional neural network","Deep learning"],"keywords_other":["State-of-the-art performance","Localization performance","FACE ALIGNMENT","Face alignment","POSE ESTIMATION","Facial landmark","Head Pose Estimation","Facial recognition","Facial expression analysis","RECOGNITION","Convolutional neural network","POINT DETECTION"],"max_cite":1.0,"pub_year":2018.0,"sources":"['ieee', 'scp', 'wos']","rawkeys":["pose estimation","facial landmark","face alignment","recognition","state-of-the-art performance","deep learning","facial recognition","face recognition","convolutional neural network","facial expression analysis","head pose estimation","localization performance","point detection","facial landmark localization"],"tags":["pose estimation","facial landmark","face alignment","recognition","state-of-the-art performance","machine learning","facial recognition","face recognition","convolutional neural network","facial expression analysis","head pose estimation","localization performance","point detection","facial landmark localization"]},{"p_id":11675,"title":"Multi-Input Convolutional Neural Network for Flower Grading","abstract":"Flower grading is a significant task because it is extremely convenient for managing the flowers in greenhouse and market. With the development of computer vision, flower grading has become an interdisciplinary focus in both botany and computer vision. A new dataset named BjfuGloxinia contains three quality grades; each grade consists of 107 samples and 321 images. A multi-input convolutional neural network is designed for large scale flower grading. Multi-input CNN achieves a satisfactory accuracy of 89.6% on the BjfuGloxinia after data augmentation. Compared with a single-input CNN, the accuracy of multi-input CNN is increased by 5% on average, demonstrating that multi-input convolutional neural network is a promising model for flower grading. Although data augmentation contributes to the model, the accuracy is still limited by lack of samples diversity. Majority of misclassification is derived from the medium class. The image processing based bud detection is useful for reducing the misclassification, increasing the accuracy of flower grading to approximately 93.9%.","keywords_author":null,"keywords_other":["RECOGNITION","VISION","SYSTEM"],"max_cite":2.0,"pub_year":2017.0,"sources":"['wos']","rawkeys":["recognition","system","vision"],"tags":["recognition","system","vision"]},{"p_id":11676,"title":"ROAD SIGN CLASSIFICATION SYSTEM USING CASCADE CONVOLUTIONAL NEURAL NETWORK","abstract":"We proposed a road sign classification system using C-CNN (cascade convolutional neural network) classifier. The cascade configuration is designed so that the classifier can easily converge with the data. Our system consists of six stages of Network in Network (NiN) architecture based CNN classifier. The data augmentation method is used to enrich the training and testing dataset which also tests the robustness of our system. Our Japan road sign dataset consists of ten classes with 7,500 examples for each class. Each image cropped from real street images is taken by the camera attached to the top of the car. From the experiments, our system is more efficient compared with bag-of-features method. The execution time of our system is less than 20 ms using appropriate hardware configuration, which is suitable for real-time application approaches like an autonomous car or driver assistance system.","keywords_author":["Road sign classification","C-CNN","Data augmentation","NiN architecture"],"keywords_other":["RECOGNITION"],"max_cite":2.0,"pub_year":2017.0,"sources":"['wos']","rawkeys":["recognition","c-cnn","road sign classification","data augmentation","nin architecture"],"tags":["recognition","c-cnn","road sign classification","data augmentation","nin architecture"]},{"p_id":11684,"title":"Infrared and visible image fusion with convolutional neural networks","abstract":"The fusion of infrared and visible images of the same scene aims to generate a composite image which can provide a more comprehensive description of the scene. In this paper, we propose an infrared and visible image fusion method based on convolutional neural networks (CNNs). In particular, a siamese convolutional network is applied to obtain a weight map which integrates the pixel activity information from two source images. This CNN-based approach can deal with two vital issues in image fusion as a whole, namely, activity level measurement and weight assignment. Considering the different imaging modalities of infrared and visible images, the merging procedure is conducted in a multi-scale manner via image pyramids and a local similarity-based strategy is adopted to adaptively adjust the fusion mode for the decomposed coefficients. Experimental results demonstrate that the proposed method can achieve state-of-the-art results in terms of both visual quality and objective assessment.","keywords_author":["Infrared and visible image fusion","convolutional neural networks","image pyramids","activity level measurement","weight assignment"],"keywords_other":["WAVELET TRANSFORM","INFORMATION","DECOMPOSITION","RECOGNITION","CONTOURLET TRANSFORM"],"max_cite":0.0,"pub_year":2018.0,"sources":"['wos']","rawkeys":["wavelet transform","recognition","convolutional neural networks","image pyramids","infrared and visible image fusion","weight assignment","information","contourlet transform","activity level measurement","decomposition"],"tags":["wavelet transform","recognition","weight assignment","image pyramids","infrared and visible image fusion","convolutional neural network","information","contourlet transform","activity level measurement","decomposition"]},{"p_id":11690,"title":"Finger-vein biometric identification using convolutional neural network","abstract":"A novel approach using a convolutional neural network (CNN) for finger-vein biometric identification is presented in this paper. Unlike existing biometric techniques such as fingerprint and face, vein patterns are inside the body, making them virtually impossible to replicate. This also makes finger-vein biometrics a more secure alternative without being susceptible to forgery, damage, or change with time. In conventional finger-vein recognition methods, complex image processing is required to remove noise and extract and enhance the features before the image classification can be performed in order to achieve high performance accuracy. In this regard, a significant advantage of the CNN over conventional approaches is its ability to simultaneously extract features, reduce data dimensionality, and classify in one network structure. In addition, the method requires only minimal image preprocessing since the CNN is robust to noise and small misalignments of the acquired images. In this paper, a reduced-complexity four-layer CNN with fused convolutional-subsampling architecture is proposed for finger-vein recognition. For network training, we have modified and applied the stochastic diagonal Levenberg-Marquardt algorithm, which results in a faster convergence time. The proposed CNN is tested on a finger-vein database developed in-house that contains 50 subjects with 10 samples from each finger. An identification rate of 100.00% is achieved, with an 80\/20 percent ratio for separation of training and test samples, respectively. An additional number of subjects have also been tested, in which for 81 subjects an accuracy of 99.38% is achieved.","keywords_author":["Finger vein","convolutional neural network","biometric identification"],"keywords_other":["RECOGNITION","PATTERNS"],"max_cite":6.0,"pub_year":2016.0,"sources":"['wos']","rawkeys":["recognition","biometric identification","patterns","convolutional neural network","finger vein"],"tags":["recognition","biometric identification","patterns","convolutional neural network","finger vein"]},{"p_id":11695,"title":"Hyperspectral image reconstruction by deep convolutional neural network for classification","abstract":"Spatial features of hyperspectral imagery (HSI) have gained an increasing attention in the latest years. Considering deep convolutional neural network (CNN) can extract a hierarchy of increasingly spatial features, this paper proposes an HSI reconstruction model based on deep CNN to enhance spatial features. The framework proposes a new spatial features-based strategy for band selection to define training label with rich information for the first time. Then, hyperspectral data is trained by deep CNN to build a model with optimized parameters which is suitable for HSI reconstruction. Finally, the reconstructed image is classified by the efficient extreme learning machine (ELM) with a very simple structure. Experimental results indicate that framework built based on CNN and ELM provides competitive performance with small number of training samples. Specifically, by using the reconstructed image, the average accuracy of ELM can be improved as high as 30.04%, while performs tens to hundreds of times faster than those state-of-the-art classifiers.","keywords_author":["Hyperspectral imagery","Deep convolutional neural network","Extreme learning machine","Reconstruction","Band selection","Pattern classification"],"keywords_other":["SUPERRESOLUTION","RECOGNITION","EXTREME LEARNING-MACHINE","SPECTRAL-SPATIAL CLASSIFICATION","MATRIX FACTORIZATION","FUSION"],"max_cite":24.0,"pub_year":2016.0,"sources":"['ieee', 'wos']","rawkeys":["band selection","recognition","superresolution","spectral-spatial classification","deep convolutional neural network","matrix factorization","pattern classification","hyperspectral imagery","reconstruction","extreme learning machine","fusion","extreme learning-machine"],"tags":["band selection","recognition","spectral-spatial classification","nonnegative matrix factorization","sparse representation","reconstruction","hyperspectral imaging","convolutional neural network","extreme learning machine","fusion","pattern classification"]},{"p_id":110005,"title":"Hybrid sensing and encoding using pad phone for home robot control","abstract":"For the patients with limb disorder to control the home robot movement, the human intention sensing and encoding are the two important tasks. This paper focuses on a new augmented reality brain computer interface (ARBCI) of the stable state visual evoked potential (SSVEP), the human intention recognition algorithms using SSVEP and Electra-hologram (EOG) respectively, and the encoding design of the human intentions. Firstly, the new ARBCI is developed which includes the SSVEP collector and a specific environment augmented reality stimulator of the symbols of the robot operations. Furthermore, the robot control instructions are encoded. Secondly, the sliding window superposition-average algorithm (SWSA) is proposed for human intention recognition on the basis of SSVEP. The stimulation frequency feature from the augmented reality stimulator is extracted by using SWSA to control the power supply and the robot speed. Thirdly, the intentional blinking EOG threshold is defined according to the experiments. Then, a fusion recognition (FR) algorithm of amplitude and sampling tune is developed on the basis of EOG, which is for the home robot direction controls. It is experimentally proved that the ARBCI improves the eye comfort compared with that of the original BCI stimulator. Besides, the SWSA can save 4 s to sensing a SSVEP intention meanwhile keep the same recognition accuracy compared with the traditional superposition-average method. In addition, the human intention sensing accuracy can reach 100% by using ARBCI and SWSA and the FR if the sensing time is adequate. A EOG intention sensing time is about 0.5 s by using the FR algorithm.","keywords_author":["BCI","Feature extraction","Recognition","Robot","Control"],"keywords_other":["BCI","BRAIN-COMPUTER INTERFACE"],"max_cite":0.0,"pub_year":2018.0,"sources":"['wos']","rawkeys":["recognition","robot","brain-computer interface","control","feature extraction","bci"],"tags":["brain-computer interfaces","recognition","control","robotics","feature extraction"]},{"p_id":52663,"title":"Modeling and designing of machine learning procedures as applied to game playing using artificial intelligence","abstract":"In this paper, we are proposing a method which is different from many practical computer programs have been developed to exhibit useful types of learning. For problems such as speech recognition, different algorithms based on machine learning outperform all other approaches that have been attempted to date. In the field known as data mining, machine learning algorithms are being used commonly to discover valuable knowledge from large commercial databases containing equipment maintenance records, loan applications, financial transactions, medical records etc. Thus, it seems inevitable that machine learning will play an integral role in computer science and computer technology. In this paper, modeling and designing of a general learning system is proposed that presents new machine learning procedures used to arrive at \"knowledgeable\" static evaluators for checker board positions. The static evaluators are compared with each other, and with the linear polynomial using two different numerical indices reflecting the extent to which they agree with the choices of checker experts in the course of tabulated book games. The new static evaluators are found to perform about equally well, despite the relative simplicity of the second; and they perform noticeably better than the linear polynomial. \u00a9 2011 ACM.","keywords_author":["critic","evaluation function","experimental generator","game playing","generalize","inference","machine learning","rules","target function"],"keywords_other":["rules","game playing","Machine-learning","generalize","evaluation function","inference","target function","critic","experimental generator"],"max_cite":0.0,"pub_year":2011.0,"sources":"['scp']","rawkeys":["rules","game playing","machine learning","generalize","evaluation function","inference","target function","machine-learning","critic","experimental generator"],"tags":["rules","game playing","recognition","machine learning","evaluation function","inference","target functions","critic","experimental generator"]},{"p_id":11705,"title":"A convolutional neural network approach for objective video quality assessment","abstract":"This paper describes an application of neural networks in the field of objective measurement method designed to automatically assess the perceived quality of digital videos. This challenging issue aims to emulate human judgment and to replace very,complex and time consuming subjective quality assessment. Several metrics have been proposed in literature to tackle this issue. They are based on a general framework that combines different stages, each of them addressing complex problems. The ambition of this paper is not to present a global perfect quality metric but rather to focus on an original way to use neural networks in such a framework in the context of reduced reference (RR) quality metric. Especially, we point out the interest of such a tool for combining features and pooling them in order to compute quality scores. The proposed approach solves some problems inherent to objective metrics that should predict subjective quality score obtained using the single stimulus continuous quality evaluation (SSCQE) method. This latter has been adopted by video quality expert group (VQEG) in its recently finalized reduced referenced and no reference (RRNR-TV) test plan. The originality of such approach compared to previous attempts to use neural networks for quality assessment, relies on the use of a convolutional neural network (CNN) that allows a continuous time scoring of the video. Objective features are extracted on a frame-by-frame basis on both the reference and the distorted sequences; they are derived from a perceptual-based representation and integrated along the temporal axis using a time-delay neural network (TDNN). Experiments conducted on different MPEG-2 videos, with bit rates ranging 2-6 Mb\/s, show the effectiveness of the proposed approach to get a plausible model of temporal pooling from the human vision system (HVS) point of view. More specifically, a linear correlation criteria, between objective and subjective scoring, up to 0.92 has been obtained on a set of typical TV videos.","keywords_author":["convolutional neural network (CNN)","MPEG-2","temporal pooling","video quality assessment"],"keywords_other":["RECOGNITION","VISION","SCALE"],"max_cite":56.0,"pub_year":2006.0,"sources":"['ieee', 'wos']","rawkeys":["recognition","mpeg-2","temporal pooling","vision","convolutional neural network (cnn)","scale","video quality assessment"],"tags":["recognition","mpeg-2","temporal pooling","vision","convolutional neural network","scale","video quality assessment"]},{"p_id":11712,"title":"Script identification in the wild via discriminative convolutional neural network","abstract":"Script identification facilitates many important applications in document\/video analysis. This paper investigates a relatively new problem: identifying scripts in natural images. The basic idea is combining deep features and mid-level representations into a globally trainable deep model. Specifically, a set of deep feature maps is firstly extracted by a pre-trained CNN model from the input images, where the local deep features are densely collected. Then, discriminative clustering is performed to learn a set of discriminative patterns based on such local features. A mid-level representation is obtained by encoding the local features based on the learned discriminative patterns (codebook). Finally, the mid-level representations and the deep features are jointly optimized in a deep network. Benefiting from such a fine-grained classification strategy, the optimized deep model, termed Discriminative Convolutional Neural Network (DisCNN), is capable of effectively revealing the subtle differences among the scripts difficult to be distinguished, e.g. Chinese and Japanese. In addition, a large scale dataset containing 16,291 in-the wild text images in 13 scripts, namely SIW-13, is created for evaluation. Our method is not limited to identifying text images, and performs effectively on video and document scripts as well, not requiring any preprocess like binarization, segmentation or hand-crafted features. The experimental comparisons on the datasets including SIW-13, CVSI-2015 and Multi-Script consistently demonstrate DisCNN a state-of-the-art approach for script identification. (C) 2015 Elsevier Ltd. All rights reserved.","keywords_author":["Convolutional neural network","Dataset","Discriminative clustering","Mid-level representation","Script identification","Script identification","Convolutional neural network","Mid-level representation","Discriminative clustering","Dataset"],"keywords_other":["Discriminative clustering","COLOR DOCUMENTS","INVARIANT TEXTURE FEATURES","EXTRACTION","Mid-level representation","SYSTEM","RECOGNITION","Dataset","IMAGES","Convolutional neural network","ROTATION","VIDEO FRAMES","Script identification"],"max_cite":28.0,"pub_year":2016.0,"sources":"['scp', 'wos']","rawkeys":["invariant texture features","recognition","dataset","images","mid-level representation","color documents","rotation","script identification","system","video frames","discriminative clustering","convolutional neural network","extraction"],"tags":["invariant texture features","recognition","images","mid-level representation","color documents","rotation","script identification","system","data sets","discriminative clustering","convolutional neural network","video frame","extraction"]},{"p_id":118209,"title":"Secure data uploading scheme for a smart home system","abstract":"Smart home systems have attracted the attention of users and intelligent device developers because of their convenience, efficiency and other characteristics. Security and privacy protection are major obstacles for the practical application of smart home systems. Home area networks (HANs) are an important part of smart home systems. A large number of researchers have devoted themselves to proposing secure transport strategies and key agreement schemes for HANs. However, there are still many problems with these schemes, such as the inability of the cloud to verify the integrity of uploaded data and the absolute right of home gateways to monitor and modify the data. In this paper, we propose a secure data uploading scheme, which ensures that the cloud validates the data integrity while avoiding malicious home gateways that monitor and modify the data. A security proof and simulations demonstrate that our protocol is secure and efficient. (C) 2018 Elsevier Inc. All rights reserved.","keywords_author":["Secure data uploading","Smart home system","Home area networks (HANs)","Data integrity validation"],"keywords_other":["NETWORKS","RECOGNITION","ENERGY","MANAGEMENT"],"max_cite":1.0,"pub_year":2018.0,"sources":"['wos']","rawkeys":["data integrity validation","secure data uploading","recognition","smart home system","home area networks (hans)","energy","networks","management"],"tags":["data integrity validation","secure data uploading","recognition","smart-home system","energy","networks","home area network","management"]},{"p_id":11713,"title":"Remote Sensing Image Fusion With Deep Convolutional Neural Network","abstract":"Remote sensing images with different spatial and spectral resolution, such as panchromatic (PAN) images and multispectral (MS) images, can be captured by many earth-observing satellites. Normally, PAN images possess high spatial resolution but low spectral resolution, while MS images have high spectral resolution with low spatial resolution. In order to integrate spatial and spectral information contained in the PAN and MS images, image fusion techniques are commonly adopted to generate remote sensing images at both high spatial and spectral resolution. In this study, based on the deep convolutional neural network, a remote sensing image fusion method that can adequately extract spectral and spatial features from source images is proposed. The major innovation of this study is that the proposed fusion method contains a two branches network with the deeper structure which can capture salient features of the MS and PAN images separately. Besides, the residual learning is adopted in our network to thoroughly study the relationship between the high- and low-resolution MS images. The proposed method mainly consists of two procedures. First, spatial and spectral features are respectively extracted from the MS and PAN images by convolutional layers with different depth. Second, the feature fusion procedure utilizes the extracted features from the former step to yield fused images. By evaluating the performance on the Quick Bird and Gaofen-1 images, our proposed method provides better results compared with other classical methods.","keywords_author":["Deep convolutional neural network","multispectral image","panchromatic image","remote sensing image fusion"],"keywords_other":["DICTIONARIES","WAVELET TRANSFORM","VISIBLE IMAGES","QUALITY","CURVELETS","CLASSIFICATION","RECOGNITION","CODE","SPARSE REPRESENTATION"],"max_cite":0.0,"pub_year":2018.0,"sources":"['ieee', 'wos']","rawkeys":["wavelet transform","quality","recognition","curvelets","multispectral image","deep convolutional neural network","sparse representation","panchromatic image","visible images","classification","code","dictionaries","remote sensing image fusion"],"tags":["wavelet transform","quality","recognition","curvelets","multispectral images","panchromatic images","codes","sparse representation","visible images","classification","convolutional neural network","dictionaries","remote sensing image fusion"]},{"p_id":19914,"title":"Recognizing landmarks using automated classification techniques: An evaluation of various visual features","abstract":"In this paper, the performance of several visual features is evaluated in automatically recognizing landmarks (monuments, statues, buildings, etc.) in pictures. A number of landmarks were selected for the test. Pictures taken from a test set were classified automatically trying to guess which landmark they contained. We evaluated both global and local features. As expected, local features performed better given their capability of being less affected to visual variations and given that landmarks are mainly static objects that generally also maintain static local features. Between the local features, SIFT outperformed SURF and ColorSIFT. \u00a9 2010 IEEE.","keywords_author":["Image classification","Image indexing","Landmarks","Recognition"],"keywords_other":["Test sets","Automated classification","Static objects","Local feature","Visual feature"],"max_cite":19.0,"pub_year":2010.0,"sources":"['scp']","rawkeys":["static objects","test sets","automated classification","recognition","visual feature","local feature","landmarks","image classification","image indexing"],"tags":["static objects","test sets","automated classification","recognition","visual feature","local feature","landmarks","image classification","image indexing"]},{"p_id":11727,"title":"3D object understanding with 3D Convolutional Neural Networks","abstract":"Feature engineering plays an important role in object understanding. Expressive discriminative features can guarantee the success of object understanding tasks. With remarkable ability of data abstraction, deep hierarchy architecture has the potential to represent objects. For 3D objects with multiple views, the existing deep learning methods can not handle all the views with high quality. In this paper, we propose a 3D convolutional neural network, a deep hierarchy model which has a similar structure with convolutional neural network. We employ stochastic gradient descent (SGD) method to pretrain the convolutional layer, and then a back-propagation method is proposed to fine-tune the whole network. Finally, we use the result of the two phases for 3D object retrieval. The proposed method is shown to out-perform the state-of-the-art approaches by experiments conducted on publicly available 3D object datasets. (C) 2015 Elsevier Inc. All rights reserved.","keywords_author":["Deep representation","3D Convolutional Neural Networks","3D object understanding"],"keywords_other":["MODEL RETRIEVAL","VIEWS","MACHINE","DATABASES","RELEVANCE FEEDBACK","RECOGNITION","SEARCH ENGINE","MECHANISM","SHAPE DESCRIPTOR","SIMILARITY"],"max_cite":6.0,"pub_year":2016.0,"sources":"['ieee', 'wos']","rawkeys":["3d object understanding","deep representation","views","recognition","search engine","3d convolutional neural networks","databases","mechanism","model retrieval","similarity","machine","shape descriptor","relevance feedback"],"tags":["3d object understanding","deep representation","views","recognition","search engine","3d convolutional neural networks","databases","model retrieval","mechanisms","similarity","machine","shape descriptors","random forests"]},{"p_id":11733,"title":"Foreign Exchange Rates Forecasting with Convolutional Neural Network","abstract":"In this paper, we introduce a model based on Convolutional Neural Network for forecasting foreign exchange rates. Additionally, a method of transforming exchange rates data from 1D structure to 2D structure is proposed. The transaction of the foreign exchange market has periodic characteristics, however, due to the technical limitations, these characteristics cannot be utilized by existing time series forecasting models. In this paper, we propose a model which can process 2D structure exchange rates data and put these characteristics to good use. Exchange rates Euro against US dollar, US dollar against Japanese yen and British Pound Sterling against US dollar are researched in this paper. Our experimental results show that, when compared with Artificial Neural Network, Support Vector Regression and Gated Recurrent Unit, the proposed model can effectively improve the accuracy of long-term forecasting.","keywords_author":["Foreign exchange rates forecasting","Convolutional neural network","Long-term forecasting","Adaptive gradient algorithm"],"keywords_other":["TIME-SERIES","RECOGNITION","PREDICTION"],"max_cite":0.0,"pub_year":2017.0,"sources":"['ieee', 'wos']","rawkeys":["recognition","time-series","long-term forecasting","prediction","adaptive gradient algorithm","foreign exchange rates forecasting","convolutional neural network"],"tags":["recognition","long-term forecasting","prediction","adaptive gradient algorithm","foreign exchange rates forecasting","convolutional neural network","time series"]},{"p_id":11737,"title":"Identification of rice diseases using deep convolutional neural networks","abstract":"The automatic identification and diagnosis of rice diseases are highly desired in the field of agricultural information. Deep learning is a hot research topic in pattern recognition and machine learning at present, it can effectively solve these problems in vegetable pathology. In this study, we propose a novel rice diseases identification method based on deep convolutional neural networks (CNNs) techniques. Using a dataset of 500 natural images of diseased and healthy rice leaves and stems captured from rice experimental field, CNNs are trained to identify 10 common rice diseases. Under the 10-fold cross-validation strategy, the proposed CNNs-based model achieves an accuracy of 95.48%. This accuracy is much higher than conventional machine learning model. The simulation results for the identification of rice diseases show the feasibility and effectiveness of the proposed method. (C) 2017 Elsevier B.V. All rights reserved.","keywords_author":["Convolutional neural networks","Deep learning","Identification of rice diseases","Image recognition","Identification of rice diseases","Convolutional neural networks","Deep learning","Image recognition"],"keywords_other":["Automatic identification","Natural images","TIME-VARYING SYSTEMS","10-fold cross-validation","NOISES","DESIGN","Identification method","Hot research topics","RECOGNITION","Convolutional neural network","Conventional machines","Agricultural informations"],"max_cite":5.0,"pub_year":2017.0,"sources":"['ieee', 'scp', 'wos']","rawkeys":["design","agricultural informations","convolutional neural networks","recognition","10-fold cross-validation","deep learning","time-varying systems","conventional machines","identification of rice diseases","automatic identification","convolutional neural network","identification method","image recognition","noises","natural images","hot research topics"],"tags":["design","agricultural informations","noise","recognition","10-fold cross-validation","machine learning","time-varying systems","conventional machines","identification of rice diseases","automatic identification","convolutional neural network","identification method","image recognition","natural images","hot research topics"]},{"p_id":3551,"title":"Accurate object localization in remote sensing images based on convolutional neural networks","abstract":"In this paper, we focus on tackling the problem of automatic accurate localization of detected objects in high-resolution remote sensing images. The two major problems for object localization in remote sensing images caused by the complex context information such images contain are achieving generalizability of the features used to describe objects and achieving accurate object locations. To address these challenges, we propose a new object localization framework, which can be divided into three processes: region proposal, classification, and accurate object localization process. First, a region proposal method is used to generate candidate regions with the aim of detecting all objects of interest within these images. Then, generic image features from a local image corresponding to each region proposal are extracted by a combination model of 2-D reduction convolutional neural networks (CNNs). Finally, to improve the location accuracy, we propose an unsupervised score-based bounding box regression (USB-BBR) algorithm, combined with a nonmaximum suppression algorithm to optimize the bounding boxes of regions that detected as objects. Experiments show that the dimension-reduction model performs better than the retrained and fine-tuned models and the detection precision of the combined CNN model is much higher than that of any single model. Also our proposed USB-BBR algorithm can more accurately locate objects within an image. Compared with traditional features extraction methods, such as elliptic Fourier transform-based histogram of oriented gradients and local binary pattern histogram Fourier, our proposed localization framework shows robustness when dealing with different complex backgrounds.","keywords_author":["Convolutional neural network (CNN)","object localization","remote sensing images","unsupervised score-based bounding box regression (USB-BBR)","Convolutional neural network (CNN)","object localization","remote sensing images","unsupervised score-based bounding box regression (USB-BBR)"],"keywords_other":["Histogram of oriented gradients","TARGET DETECTION","High resolution remote sensing images","MODEL","RECOGNITION","Convolutional neural network","Remote sensing images","Non-maximum suppression","Dimension reduction model","Context information","SCENE CLASSIFICATION","Local binary patterns"],"max_cite":14.0,"pub_year":2017.0,"sources":"['ieee', 'scp', 'wos']","rawkeys":["recognition","model","high resolution remote sensing images","histogram of oriented gradients","scene classification","convolutional neural network (cnn)","object localization","target detection","dimension reduction model","convolutional neural network","local binary patterns","non-maximum suppression","remote sensing images","context information","unsupervised score-based bounding box regression (usb-bbr)"],"tags":["recognition","model","high resolution remote sensing images","histogram of oriented gradients","scene classification","object localization","target detection","dimension reduction model","convolutional neural network","local binary patterns","non-maximum suppression","remote sensing images","context information","unsupervised score-based bounding box regression (usb-bbr)"]},{"p_id":19936,"title":"Interactive sketching of urban procedural models","abstract":"\u00a9 2016 ACM.3D modeling remains a notoriously difficult task for novices despite significant research effort to provide intuitive and automated systems. We tackle this problem by combining the strengths of two popular domains: sketch-based modeling and procedural modeling. On the one hand, sketch-based modeling exploits our ability to draw but requires detailed, unambiguous drawings to achieve complex models. On the other hand, procedural modeling automates the creation of precise and detailed geometry but requires the tedious definition and parameterization of procedural models. Our system uses a collection of simple procedural grammars, called snippets, as building blocks to turn sketches into realistic 3D models. We use a machine learning approach to solve the inverse problem of finding the procedural model that best explains a user sketch. We use nonphotorealistic rendering to generate artificial data for training convolutional neural networks capable of quickly recognizing the procedural rule intended by a sketch and estimating its parameters. We integrate our algorithm in a coarse-to-fine urban modeling system that allows users to create rich buildings by successively sketching the building mass, roof, facades, windows, and ornaments. A user study shows that by using our approach non-expert users can generate complex buildings in just a few minutes.","keywords_author":["Inverse procedural modeling","Machine learning","Sketching","Inverse Procedural Modeling","Sketching","Machine Learning"],"keywords_other":["Non-Photorealistic Rendering","3D OBJECT","Machine learning approaches","Automated systems","DESIGN","Procedural modeling","RECOGNITION","Convolutional neural network","ARCHITECTURE","Sketch-based modeling","Sketching","RECONSTRUCTION","Complex buildings"],"max_cite":18.0,"pub_year":2016.0,"sources":"['scp', 'wos']","rawkeys":["inverse procedural modeling","complex buildings","design","3d object","non-photorealistic rendering","recognition","sketch-based modeling","automated systems","machine learning","machine learning approaches","reconstruction","convolutional neural network","sketching","architecture","procedural modeling"],"tags":["inverse procedural modeling","complex buildings","design","3d object","non-photorealistic rendering","recognition","sketch-based modeling","automated systems","machine learning","machine learning approaches","reconstruction","sketch","convolutional neural network","architecture","procedural modeling"]},{"p_id":3555,"title":"Complex-Valued Convolutional Neural Network and Its Application in Polarimetric SAR Image Classification","abstract":"Following the great success of deep convolutional neural networks (CNNs) in computer vision, this paper proposes a complex-valued CNN (CV-CNN) specifically for synthetic aperture radar (SAR) image interpretation. It utilizes both amplitude and phase information of complex SAR imagery. All elements of CNN including input-output layer, convolution layer, activation function, and pooling layer are extended to the complex domain. Moreover, a complex backpropagation algorithm based on stochastic gradient descent is derived for CV-CNN training. The proposed CV-CNN is then tested on the typical polarimetric SAR image classification task which classifies each pixel into known terrain types via supervised training. Experiments with the benchmark data sets of Flevoland and Oberpfaffenhofen show that the classification error can be further reduced if employing CV-CNN instead of conventional real-valued CNN with the same degrees of freedom. The performance of CV-CNN is comparable to that of existing state-of-the-art methods in terms of overall classification accuracy.","keywords_author":["Complex-valued convolutional neural network (CV-CNN)","deep learning","synthetic aperture radar (SAR)","terrain classification","Complex-valued convolutional neural network (CV-CNN)","deep learning","synthetic aperture radar (SAR)","terrain classification"],"keywords_other":["AGRICULTURAL CROPS","Stochastic gradient descent","Synthetic aperture radar (SAR) images","State-of-the-art methods","Terrain classification","Supervised trainings","DECOMPOSITION","MULTIFREQUENCY","RECOGNITION","Convolutional neural network","Classification errors","Classification accuracy"],"max_cite":7.0,"pub_year":2017.0,"sources":"['ieee', 'scp', 'wos']","rawkeys":["state-of-the-art methods","recognition","decomposition","deep learning","synthetic aperture radar (sar)","classification accuracy","multifrequency","agricultural crops","classification errors","supervised trainings","stochastic gradient descent","convolutional neural network","terrain classification","synthetic aperture radar (sar) images","complex-valued convolutional neural network (cv-cnn)"],"tags":["state-of-the-art methods","recognition","decomposition","synthetic aperture radar","classification accuracy","machine learning","multifrequency","agricultural crops","classification errors","supervised trainings","stochastic gradient descent","convolutional neural network","terrain classification","synthetic aperture radar (sar) images","complex-valued convolutional neural network (cv-cnn)"]},{"p_id":19940,"title":"Multimodal video classification with stacked contractive autoencoders","abstract":"\u00a9 2015 Elsevier B.V.In this paper we propose a multimodal feature learning mechanism based on deep networks (i.e., stacked contractive autoencoders) for video classification. Considering the three modalities in video, i.e., image, audio and text, we first build one Stacked Contractive Autoencoder (SCAE) for each single modality, whose outputs will be joint together and fed into another Multimodal Stacked Contractive Autoencoder (MSCAE). The first stage preserves intra-modality semantic relations and the second stage discovers inter-modality semantic correlations. Experiments on real world dataset demonstrate that the proposed approach achieves better performance compared with the state-of-the-art methods.","keywords_author":["Deep learning","Multimodal","Stacked contractive autoencoder","Video classification","Multimodal","Video classification","Deep learning","Stacked contractive autoencoder"],"keywords_other":["Deep learning","EXEMPLARS","State-of-the-art methods","FUSION","REPRESENTATION","Video classification","EVENT DETECTION","RECOGNITION","Auto encoders","Better performance","Semantic relations","Multi-modal","Multimodal features"],"max_cite":18.0,"pub_year":2016.0,"sources":"['wos', 'scp', 'ieee']","rawkeys":["state-of-the-art methods","multimodal","semantic relations","recognition","stacked contractive autoencoder","deep learning","multimodal features","better performance","auto encoders","representation","exemplars","multi-modal","video classification","fusion","event detection"],"tags":["state-of-the-art methods","multimodal","semantic relations","recognition","stacked contractive autoencoder","multimodal features","better performance","auto encoders","machine learning","exemplar","representation","multi-modal","video classification","fusion","event detection"]},{"p_id":11749,"title":"Convolutional neural networks for ocular smartphone-based biometrics","abstract":"Ocular biometrics in the visible spectrum has emerged as an area of significant research activity. In this paper, we propose a hybrid convolution-based model, for verifying a pair of periocular images containing the iris. We compose the hybrid model as a combination of an unsupervised and a supervised convolution neural network, and augment the combination with the well-known geometry-based Root SIFT model. We also compare the performance of both convolution-based models against each other, as well as, with the baseline Root SIFT. In the first (unsupervised w.r.t target dataset) convolution based deep learning approach, we use a stacked convolutional architecture, using external models learned a-priori on external facial and periocular data, on top of the baseline Root SIFT model applied on the provided data, and apply different score fusion models. In the second (supervised w.r.t target dataset) approach, we again use a stacked convolution architecture; but here, we learn the feature vector in a supervised manner. On the MICHE-II dataset, we obtain an AUROC of 0.946 and 0.981, and EER of 0.092 and 0.066, for the two models respectively. The hybrid model we propose, which combines these two convolutional neural networks, outperforms the constituents, in case both images arise from the same device type, but not necessarily so otherwise, obtaining a AUROC of 0.986 and EER of 0.053. We also benchmark our performance on the standard VISOB database, where we outperform the state of the art methods, achieving a TPR of 99.5% at a FPR of 0.001%. Given the robustness and significant performance of our methodology, our system can be used in practical applications with minimal error. (C) 2017 Elsevier B.V. All rights reserved.","keywords_author":["Ocular biometrics","Convolutional neural networks","Iris based recognition","Periocular images","Smartphone biometrics"],"keywords_other":["RECOGNITION","FACE"],"max_cite":1.0,"pub_year":2017.0,"sources":"['wos']","rawkeys":["recognition","convolutional neural networks","periocular images","smartphone biometrics","ocular biometrics","face","iris based recognition"],"tags":["recognition","periocular images","smartphone biometrics","ocular biometrics","convolutional neural network","face","iris based recognition"]},{"p_id":3565,"title":"Label-Sensitive Deep Metric Learning for Facial Age Estimation","abstract":"In this paper, we present a label-sensitive deep metric learning (LSDML) approach for facial age estimation. Motivated by the fact that human age labels are chronologically correlated, our proposed LSDML aims to seek a series of hierarchical nonlinear transformations by deep residual network to project face samples to a latent common space, where the similarity of face pairs is equivalently isotonic to the age difference in a ranking-preserving manner. Since traversal access to total negative samples catastrophically costs and leads to suboptimal, our model learns to mine hard meaningful samples in parallel to learning feature similarity, so that the local manifold of face samples is preserved in the transformed subspace. To better improve the performance on the data set that contains few labeled samples, we further extend our LSDML to a multi-source LSDML method, which aims at maximizing the cross-population correlation of different face aging data sets. Extensive experimental results on four benchmarking data sets show the effectiveness of our proposed approach.","keywords_author":["biometrics","deep learning","Facial age estimation","metric learning","residual network","Facial age estimation","metric learning","deep learning","residual network","biometrics"],"keywords_other":["REGRESSION","FACE VERIFICATION","MANIFOLD","RANKING","FRAMEWORK","Metric learning","Negative samples","SIMILARITY","Multisources","RECOGNITION","DATABASE","IMAGES","Age differences","Feature similarities","PATTERNS","Age estimation","Non-linear transformations","Common spaces"],"max_cite":1.0,"pub_year":2018.0,"sources":"['ieee', 'scp', 'wos']","rawkeys":["negative samples","age differences","feature similarities","patterns","facial age estimation","multisources","ranking","metric learning","regression","recognition","common spaces","images","deep learning","framework","database","biometrics","non-linear transformations","similarity","age estimation","face verification","residual network","manifold"],"tags":["negative samples","feature similarities","databases","patterns","manifolds","facial age estimation","multisources","machine learning","age-differences","metric learning","regression","recognition","common spaces","images","framework","biometrics","non-linear transformations","similarity","standards","age estimation","face verification","residual network"]},{"p_id":11757,"title":"Effective training of convolutional neural networks with small, specialized datasets","abstract":"This work proposes a supervised layer-wise strategy to train deep convolutional neural networks (DCNs) particularly suited for small, specialized image datasets. DCNs are increasingly being used with considerable success in image classification tasks and trained over large datasets (with more than 1M images and 10K classes). Pre-trained successful DCNs can then be used for new smaller datasets (10K to 100K images) through a transfer learning process which cannot guarantee competitive a-priori performance if the new data is of different or specialized nature (medical imaging, plant recognition, etc.). We therefore seek out to find competitive techniques to train DCNs for such small datasets, and hereby describe a supervised greedy layer-wise method analogous to that used in unsupervised deep networks. Our method consistently outperforms the traditional methods that train a full DCN architecture in a single stage, yielding an average of over 20% increase in classification performance across all DCN architectures and datasets used in this work. Furthermore, we obtain more interpretable and cleaner visual features. Our method is better suited for small, specialized datasets since we require a training cycle for each DCN layer and this increases its computing time almost linearly with the number of layers. Nevertheless, it still remains as a fraction of the computing time required to generate pre-trained models with large generic datasets, and poses no additional requirements on hardware. This constitutes a solid alternative for training DCNs when transfer learning is not possible and, furthermore, suggests that state of the art DCN performance with large datasets might yet be improved at the expense of a higher computing time.","keywords_author":["Convolutional networks","Deep learning","Greedy layer-wise training","Convolutional networks","deep learning","greedy layer-wise training"],"keywords_other":["Plant recognition","Classification performance","Convolutional networks","Transfer learning","DEEP","Layer-wise","Number of layers","State of the art","RECOGNITION","Convolutional neural network"],"max_cite":0.0,"pub_year":2017.0,"sources":"['ieee', 'scp', 'wos']","rawkeys":["recognition","classification performance","deep learning","deep","number of layers","state of the art","plant recognition","convolutional networks","transfer learning","greedy layer-wise training","convolutional neural network","layer-wise"],"tags":["recognition","classification performance","transfer learning","deep","number of layers","state of the art","machine learning","plant recognition","greedy layer-wise training","convolutional neural network","layer-wise"]},{"p_id":3569,"title":"Learning Deep Off-the-Person Heart Biometrics Representations","abstract":"Since the beginning of the new millennium, the electrocardiogram (ECG) has been studied as a biometric trait for security systems and other applications. Recently, with devices such as smartphones and tablets, the acquisition of ECG signal in the off-the-person category has made this biometric signal suitable for real scenarios. In this paper, we introduce the usage of deep learning techniques, specifically convolutional networks, for extracting useful representation for heart biometrics recognition. Particularly, we investigate the learning of feature representations for heart biometrics through two sources: on the raw heartbeat signal and on the heartbeat spectrogram. We also introduce heartbeat data augmentation techniques, which are very important to generalization in the context of deep learning approaches. Using the same experimental setup for six methods in the literature, we show that our proposal achieves state-of-the-art results in the two off-the-person publicly available databases.","keywords_author":["biometric systems","deep learning","Electrocardiogram","off-the-person category","Electrocardiogram","biometric systems","deep learning","off-the-person category"],"keywords_other":["off-the-person category","Biometric systems","Convolutional networks","Heartbeat signals","HUMAN IDENTIFICATION","SIGNAL","Heart beats","SYSTEMS","Feature representation","Learning techniques","ECG","RECOGNITION","Learning approach"],"max_cite":0.0,"pub_year":2018.0,"sources":"['scp', 'wos', 'ieee']","rawkeys":["heartbeat signals","off-the-person category","recognition","ecg","deep learning","convolutional networks","heart beats","electrocardiogram","learning techniques","biometric systems","systems","feature representation","learning approach","signal","human identification"],"tags":["heartbeat signals","signals","off-the-person category","recognition","ecg","machine learning","heart beats","system","learning techniques","convolutional neural network","biometric systems","feature representation","learning approach","human identification"]},{"p_id":3570,"title":"Leveraging Content Sensitiveness and User Trustworthiness to Recommend Fine-Grained Privacy Settings for Social Image Sharing","abstract":"To configure successful privacy settings for social image sharing, two issues are inseparable: 1) content sensitiveness of the images being shared; and 2) trustworthiness of the users being granted to see the images. This paper aims to consider these two inseparable issues simultaneously to recommend fine-grained privacy settings for social image sharing. For achieving more compact representation of image content sensitiveness (privacy), two approaches are developed: 1) a deep network is adapted to extract 1024-D discriminative deep features; and 2) a deep multiple instance learning algorithm is adopted to identify 280 privacy-sensitive object classes and events. Second, users on the social network are clustered into a set of representative social groups to generate a discriminative dictionary for user trustworthiness characterization. Finally, both the image content sensitiveness and the user trustworthiness are integrated to train a tree classifier to recommend fine-grained privacy settings for social image sharing. Our experimental studies have demonstrated both the efficiency and the effectiveness of our proposed algorithms.","keywords_author":["Privacy setting recommendation","image content sensitiveness","user trustworthiness","deep multiple instance learning","tree classifier","social image sharing"],"keywords_other":["NETWORKS","PROTECTION","RECOGNITION","VIDEO"],"max_cite":0.0,"pub_year":2018.0,"sources":"['wos', 'ieee']","rawkeys":["deep multiple instance learning","recognition","user trustworthiness","video","networks","image content sensitiveness","social image sharing","tree classifier","privacy setting recommendation","protection"],"tags":["deep multiple instance learning","recognition","tree classifiers","user trustworthiness","video","networks","image content sensitiveness","social image sharing","privacy setting recommendation","protection"]},{"p_id":28156,"title":"Recognizing multi-view objects with occlusions using a deep architecture","abstract":"\u00a9 2015 Elsevier Inc. All rights reserved.Image-based object recognition is employed widely in many computer vision applications such as image semantic annotation and object location. However, traditional object recognition algorithms based on the 2D features of RGB data have difficulty when objects overlap and image occlusion occurs. At present, RGB-D cameras are being used more widely and the RGB-D depth data can provide auxiliary information to address these challenges. In this study, we propose a deep learning approach for the efficient recognition of 3D objects with occlusion. First, this approach constructs a multi-view shape model based on 3D objects by using an encode-decode deep learning network to represent the features. Next, 3D object recognition in indoor scenes is performed using random forests. The application of deep learning to RGB-D data is beneficial for recovering missing information due to image occlusion. Our experimental results demonstrate that this approach can significantly improve the efficiency of feature representation and the performance of object recognition with occlusion.","keywords_author":["Deep learning","Depth data","Multi-view","Object recognition","RGB-D","Deep learning","Depth data","Multi-view","Object recognition","RGB-D"],"keywords_other":["Deep learning","INVARIANT","GRAPH","Multi-views","Computer vision applications","INFORMATION","Feature representation","Object recognition algorithm","Image based object recognition","RECOGNITION","Depth data","Image semantic annotation","RETRIEVAL"],"max_cite":5.0,"pub_year":2015.0,"sources":"['ieee', 'scp', 'wos']","rawkeys":["recognition","deep learning","computer vision applications","graph","object recognition","image based object recognition","object recognition algorithm","depth data","image semantic annotation","multi-view","information","retrieval","rgb-d","feature representation","invariant","multi-views"],"tags":["recognition","computer vision applications","graph","machine learning","invariance","object recognition","image based object recognition","object recognition algorithm","depth data","image semantic annotation","information","retrieval","rgb-d","feature representation","multi-views"]},{"p_id":3581,"title":"Knowledge Guided Disambiguation for Large-Scale Scene Classification With Multi-Resolution CNNs","abstract":"Convolutional neural networks (CNNs) have made remarkable progress on scene recognition, partially due to these recent large-scale scene datasets, such as the Places and Places2. Scene categories are often defined by multi-level information, including local objects, global layout, and background environment, thus leading to large intra-class variations. In addition, with the increasing number of scene categories, label ambiguity has become another crucial issue in large-scale classification. This paper focuses on large-scale scene recognition and makes two major contributions to tackle these issues. First, we propose a multi-resolution CNN architecture that captures visual content and structure at multiple levels. The multi-resolution CNNs are composed of coarse resolution CNNs and fine resolution CNNs, which are complementary to each other. Second, we design two knowledge guided disambiguation techniques to deal with the problem of label ambiguity: 1) we exploit the knowledge from the confusion matrix computed on validation data to merge ambiguous classes into a super category and 2) we utilize the knowledge of extra networks to produce a soft label for each image. Then, the super categories or soft labels are employed to guide CNN training on the Places2. We conduct extensive experiments on three large-scale image datasets (ImageNet, Places, and Places2), demonstrating the effectiveness of our approach. Furthermore, our method takes part in two major scene recognition challenges, and achieves the second place at the Places2 challenge in ILSVRC 2015, and the first place at the LSUN challenge in CVPR 2016. Finally, we directly test the learned representations on other scene benchmarks, and obtain the new state-of-the-art results on the MIT Indoor67 (86.7%) and SUN397 (72.0%). We release the code and models at https:\/\/github.com\/wanglimin\/MRCNN-Scene-Recognition.","keywords_author":["Scene recognition","large-scale recognition","multi-resolutions","disambiguation","convolutional neural network"],"keywords_other":["REPRESENTATION","RECOGNIZING INDOOR SCENES","RECOGNITION"],"max_cite":4.0,"pub_year":2017.0,"sources":"['wos', 'ieee']","rawkeys":["multi-resolutions","recognizing indoor scenes","recognition","representation","convolutional neural network","large-scale recognition","disambiguation","scene recognition"],"tags":["recognizing indoor scenes","recognition","representation","multiresolution","convolutional neural network","large-scale recognition","disambiguation","scene recognition"]},{"p_id":11775,"title":"Staff-line detection and removal using a convolutional neural network","abstract":"Staff-line removal is an important preprocessing stage for most optical music recognition systems. Common procedures to solve this task involve image processing techniques. In contrast to these traditional methods based on hand-engineered transformations, the problem can also be approached as a classification task in which each pixel is labeled as either staff or symbol, so that only those that belong to symbols are kept in the image. In order to perform this classification, we propose the use of convolutional neural networks, which have demonstrated an outstanding performance in image retrieval tasks. The initial features of each pixel consist of a square patch from the input image centered at that pixel. The proposed network is trained by using a dataset which contains pairs of scores with and without the staff lines. Our results in both binary and grayscale images show that the proposed technique is very accurate, outperforming both other classifiers and the state-of-the-art strategies considered. In addition, several advantages of the presented methodology with respect to traditional procedures proposed so far are discussed.","keywords_author":["Convolutional neural networks","Music staff-line removal","Optical music recognition","Pixel classification","Music staff-line removal","Optical music recognition","Pixel classification","Convolutional neural networks"],"keywords_other":["Gray-scale images","Classification tasks","State of the art","RECOGNITION","NOTATION","Image processing technique","Convolutional neural network","Pixel classification","Optical music recognition","Line removal"],"max_cite":4.0,"pub_year":2017.0,"sources":"['ieee', 'scp', 'wos']","rawkeys":["recognition","convolutional neural networks","image processing technique","state of the art","classification tasks","optical music recognition","line removal","convolutional neural network","pixel classification","gray-scale images","notation","music staff-line removal"],"tags":["recognition","image processing technique","state of the art","classification tasks","optical music recognition","line removal","convolutional neural network","pixel classification","gray-scale images","notation","music staff-line removal"]},{"p_id":3583,"title":"Con-Text: Text Detection for Fine-Grained Object Classification","abstract":"This paper focuses on fine-grained object classification using recognized scene text in natural images. While the state-of-the-art relies on visual cues only, this paper is the first work which proposes to combine textual and visual cues. Another novelty is the textual cue extraction. Unlike the state-of-the-art text detection methods, we focus more on the background instead of text regions. Once text regions are detected, they are further processed by two methods to perform text recognition, i.e., ABBYY commercial OCR engine and a state-of-the-art character recognition algorithm. Then, to perform textual cue encoding, bi- and trigrams are formed between the recognized characters by considering the proposed spatial pairwise constraints. Finally, extracted visual and textual cues are combined for fine-grained classification. The proposed method is validated on four publicly available data sets: ICDAR03, ICDAR13, Con-Text, and Flickr-logo. We improve the state-of-the-art end-to-end character recognition by a large margin of 15% on ICDAR03. We show that textual cues are useful in addition to visual cues for fine-grained classification. We show that textual cues are also useful for logo retrieval. Adding textual cues outperforms visualand textual-only in fine-grained classification (70.7% to 60.3%) and logo retrieval (57.4% to 54.8%).","keywords_author":["Multimodal fusion","fine-grained classification","logo-retrieval","text detection","text saliency"],"keywords_other":["LOCALIZATION","FEATURES","SEARCH","ATTENTION","RECOGNITION","CATEGORIZATION","IMAGES"],"max_cite":0.0,"pub_year":2017.0,"sources":"['wos', 'ieee']","rawkeys":["recognition","images","localization","features","search","text detection","fine-grained classification","text saliency","attention","categorization","logo-retrieval","multimodal fusion"],"tags":["recognition","images","localization","features","search","text detection","fine-grained classification","logo retrieval","attention","categorization","multimodal fusion","text saliency"]},{"p_id":60932,"title":"Hierarchical prostate MRI segmentation via level set clustering with shape prior","abstract":"Efficient and accurate segmentation of prostate is of great interest in image-guided prostate interventions and diagnosis of prostate cancer. In this paper, a novel hierarchical level set clustering approach is proposed to segment prostate from MR image, which makes full use of statistics information of manual segmentation result and incorporates shape prior into the segmentation task. The medium slice of prostate MR data, which is segmented artificially, is used to offer prior information and guide the segmentation of other slices. The Bhattacharyya coefficient between manual segmentation result of medium slice and local block region of pending slice is calculated to estimate the likelihood of local prostate region in pending slice. An adaptive blurring process is implemented before the optimization of level set function to restrain the redundancy texture information and retain the edge information in the meantime. We can capture the contour of prostate with a level set evolution embedded shape prior which is derived from the segmented result of medium slice. A comparative performance evaluation is carried out over a large set of experiments using real prostate magnetic resonance images and synthetic magnetic resonance data to demonstrate the validity of our method, showing significant improvements on both segmentation accuracy and noise sensitivity comparing to the state-of-the-art approaches. (C) 2017 Elsevier B.V. All rights reserved.","keywords_author":["Hierarchical","Level set","Shape prior","Prostate segmentation"],"keywords_other":["DISCRIMINANT-ANALYSIS","FEATURES","FRAMEWORK","ENERGY","RETRIEVAL","RECOGNITION","MAGNETIC-RESONANCE IMAGES","HUMAN POSE RECOVERY","EVOLUTION"],"max_cite":3.0,"pub_year":2017.0,"sources":"['ieee', 'wos']","rawkeys":["magnetic-resonance images","prostate segmentation","recognition","features","framework","discriminant-analysis","energy","shape prior","retrieval","evolution","hierarchical","human pose recovery","level set"],"tags":["prostate segmentation","recognition","features","denoising autoencoder","framework","energy","shape prior","retrieval","biological","hierarchical","human pose recovery","magnetic resonance imaging","level set"]},{"p_id":11781,"title":"Noise robust sound event classification with convolutional neural network","abstract":"Automatic sound recognition (ASR) is a remarkable field of research in recent years. The ability to automatically recognize sound events through computers in a complex audio environment is very useful for machine hearing, acoustic surveillance and multimedia retrieval applications. On the other hand, ASR task become highly difficult as the ambient noise levels increase and many traditional methods show very weak performance under noise. Recent studies has shown that spectrogram image features (SIF) have high performance under noise, while success rates in clean conditions are relatively lower than in the state-of-the-art approaches. In this study, after converting highly overlapped spectrograms into linear quantized images and reducing dimensions by applying various image resizing methods, feature extraction and classification are performed with convolutional neural networks (CNN), which have very high performance in image classification. In the mismatched case, the proposed method achieves a performance improvement of 4.5%, which is equivalent to a relative error reduction of 63.4%, with a classification success of 97.4%, while the multicondition training method achieves an average of 98.63% success rate. (C) 2017 Elsevier B.V. All rights reserved.","keywords_author":["Sound event classification","Convolutional neural networks","Spectrogram"],"keywords_other":["DEEP","FEATURES","AUDIO","RETRIEVAL","RECOGNITION"],"max_cite":1.0,"pub_year":2017.0,"sources":"['ieee', 'wos']","rawkeys":["recognition","convolutional neural networks","features","deep","spectrogram","sound event classification","audio","retrieval"],"tags":["recognition","features","deep","sound event classification","audio","convolutional neural network","retrieval","spectrograms"]},{"p_id":3590,"title":"Frankenstein: Learning Deep Face Representations Using Small Data","abstract":"Deep convolutional neural networks have recently proven extremely effective for difficult face recognition problems in uncontrolled settings. To train such networks, very large training sets are needed with millions of labeled images. For some applications, such as near-infrared (NIR) face recognition, such large training data sets are not publicly available and difficult to collect. In this paper, we propose a method to generate very large training data sets of synthetic images by compositing real face images in a given data set. We show that this method enables to learn models from as few as 10 000 training images, which perform on par with models trained from 500 000 images. Using our approach, we also obtain state-of-the-art results on the CASIA NIR-VIS2.0 heterogeneous face recognition data set.","keywords_author":["deep learning","Face recognition","small training data","Face recognition","deep learning","small training data"],"keywords_other":["Near infra red","VERIFICATION","Small training","Face representations","Training data sets","CLASSIFICATION","State of the art","RECOGNITION","Convolutional neural network","Synthetic images","Labeled images","IMAGES"],"max_cite":1.0,"pub_year":2018.0,"sources":"['ieee', 'scp', 'wos']","rawkeys":["recognition","images","small training data","deep learning","labeled images","near infra red","state of the art","synthetic images","verification","face recognition","classification","convolutional neural network","training data sets","small training","face representations"],"tags":["recognition","images","small training data","labeled images","near infra red","state of the art","machine learning","synthetic images","verification","face recognition","classification","convolutional neural network","training data sets","small training","face representations"]},{"p_id":3592,"title":"Tropical Cyclone Intensity Estimation Using a Deep Convolutional Neural Network","abstract":"Tropical cyclone intensity estimation is a challenging task as it required domain knowledge while extracting features, significant pre-processing, various sets of parameters obtained from satellites, and human intervention for analysis. The inconsistency of results, significant pre-processing of data, complexity of the problem domain, and problems on generalizability are some of the issues related to intensity estimation. In this study, we design a deep convolutional neural network architecture for categorizing hurricanes based on intensity using graphics processing unit. Our model has achieved better accuracy and lower root-mean-square error by just using satellite images than 'state-of-the-art' techniques. Visualizations of learned features at various layers and their deconvolutions are also presented for understanding the learning process.","keywords_author":["Convolutional neural networks","Deep learning","Image processing","Tropical cyclone category and intensity estimation","Deep learning","image processing","convolutional neural networks","tropical cyclone category and intensity estimation"],"keywords_other":["DVORAK TECHNIQUE","Tropical cyclone","INFRARED IMAGE DATA","Human intervention","Pre-processing of data","Root mean square errors","ANGLE VARIANCE TECHNIQUE","RECOGNITION","Extracting features","Convolutional neural network","Intensity estimation","BACKPROPAGATION","Tropical cyclone intensity"],"max_cite":1.0,"pub_year":2018.0,"sources":"['ieee', 'scp', 'wos']","rawkeys":["recognition","convolutional neural networks","dvorak technique","deep learning","image processing","root mean square errors","angle variance technique","pre-processing of data","tropical cyclone","tropical cyclone intensity","convolutional neural network","human intervention","intensity estimation","infrared image data","tropical cyclone category and intensity estimation","extracting features","backpropagation"],"tags":["recognition","dvorak technique","image processing","root mean square errors","machine learning","angle variance technique","pre-processing of data","tropical cyclone category and intensity estimation","tropical cyclone intensity","convolutional neural network","human intervention","intensity estimation","infrared image data","tropical cyclones","extracting features","backpropagation"]},{"p_id":60939,"title":"Exploring the microstructure manifold: Image texture representations applied to ultrahigh carbon steel microstructures","abstract":"We introduce a microstructure dataset focusing on complex, hierarchical structures found in a single Ultrahigh carbon steel under a range of heat treatments. Applying image representations from contemporary computer vision research to these microstructures, we discuss how both supervised and unsupervised machine learning techniques can be used to yield insight into microstructural trends and their relationship to processing conditions. We evaluate and compare keypoint-based and convolutional neural network representations by classifying microstructures according to their primary micro constituent, and by classifying a subset of the microstructures according to the annealing conditions that generated them. Using t-SNE, a nonlinear dimensionality reduction and visualization technique, we demonstrate graphical methods of exploring microstructure and processing datasets, and for understanding and interpreting high-dimensional microstructure representations. (C) 2017 Acta Materialia Inc. Published by Elsevier Ltd. All rights reserved.","keywords_author":["Multiscale","Microstructure","Processing","Steels","Computer vision"],"keywords_other":["COMPUTER VISION","NETWORKS","CLASSIFICATION","RECOGNITION","LOCAL FEATURES"],"max_cite":2.0,"pub_year":2017.0,"sources":"['wos']","rawkeys":["recognition","processing","local features","microstructure","networks","steels","classification","multiscale","computer vision"],"tags":["recognition","local feature","processing","microstructure","networks","classification","steel","multiscale","computer vision"]},{"p_id":3601,"title":"An Incremental Framework for Video-Based Traffic Sign Detection, Tracking, and Recognition","abstract":"\u00a9 2000-2011 IEEE. Video-based traffic sign detection, tracking, and recognition is one of the important components for the intelligent transport systems. Extensive research has shown that pretty good performance can be obtained on public data sets by various state-of-the-art approaches, especially the deep learning methods. However, deep learning methods require extensive computing resources. In addition, these approaches mostly concentrate on single image detection and recognition task, which is not applicable in real-world applications. Different from previous research, we introduce a unified incremental computational framework for traffic sign detection, tracking, and recognition task using the mono-camera mounted on a moving vehicle under non-stationary environments. The main contributions of this paper are threefold: 1) to enhance detection performance by utilizing the contextual information, this paper innovatively utilizes the spatial distribution prior of the traffic signs; 2) to improve the tracking performance and localization accuracy under non-stationary environments, a new efficient incremental framework containing off-line detector, online detector, and motion model predictor together is designed for traffic sign detection and tracking simultaneously; and 3) to get a more stable classification output, a scale-based intra-frame fusion method is proposed. We evaluate our method on two public data sets and the performance has shown that the proposed system can obtain results comparable with the deep learning method with less computing resource in a near-real-time manner.","keywords_author":["detection","incremental learning","ITS","Machine learning","recognition","tracking","traffic sign"],"keywords_other":["Localization accuracy","Contextual information","Detection performance","Intelligent transport systems","Traffic sign detection","State-of-the-art approach","Computational framework","Non-stationary environment"],"max_cite":20.0,"pub_year":2017.0,"sources":"['scp', 'wos', 'ieee']","rawkeys":["traffic sign","recognition","contextual information","detection performance","incremental learning","machine learning","tracking","non-stationary environment","computational framework","detection","traffic sign detection","state-of-the-art approach","localization accuracy","its","intelligent transport systems"],"tags":["recognition","contextual information","detection performance","incremental learning","machine learning","tracking","intelligent transportation systems","non-stationary environment","computational framework","detection","traffic sign detection","state-of-the-art approach","localization accuracy","traffic signs"]},{"p_id":11796,"title":"Gearbox Fault Identification and Classification with Convolutional Neural Networks","abstract":"Vibration signals of gearbox are sensitive to the existence of the fault. Based on vibration signals, this paper presents an implementation of deep learning algorithm convolutional neural network (CNN) used for fault identification and classification in gearboxes. Different combinations of condition patterns based on some basic fault conditions are considered. 20 test cases with different combinations of condition patterns are used, where each test case includes 12 combinations of different basic condition patterns. Vibration signals are preprocessed using statistical measures from the time domain signal such as standard deviation, skewness, and kurtosis. In the frequency domain, the spectrum obtained with FFT is divided into multiple bands, and the root mean square (RMS) value is calculated for each one so the energy maintains its shape at the spectrum peaks. The achieved accuracy indicates that the proposed approach is highly reliable and applicable in fault diagnosis of industrial reciprocating machinery. Comparing with peer algorithms, the present method exhibits the best performance in the gearbox fault diagnosis.","keywords_author":null,"keywords_other":["DIAGNOSIS","SUPPORT VECTOR MACHINE","VIBRATION","MODEL","RECOGNITION","SEGMENTATION","MECHANISM","SIGNALS","SUBJECT","DEMODULATION"],"max_cite":27.0,"pub_year":2015.0,"sources":"['ieee', 'wos']","rawkeys":["diagnosis","signals","recognition","model","segmentation","mechanism","demodulation","support vector machine","subject","vibration"],"tags":["diagnosis","signals","recognition","model","segmentation","mechanisms","machine learning","demodulation","subjectivity","vibration"]},{"p_id":60948,"title":"A classification method for moving targets in the wild based on microphone array and linear sparse auto-encoder","abstract":"Moving target classification is an important issue in wireless sensors. The wild environment makes it a difficult problem for the acoustic signals. In this paper, a new classification method for moving targets in the wild is proposed based on microphone array and linear sparse auto-encoder (LSAE). First, the acoustic signals of moving targets are enhanced by delay-and-sum (DS) beamformer in the narrowband way for the simplicity. The enhancing effects are given a detailed analysis. Then, a spatial feature named noise likelihood (NLH) is presented to further resist the interferences and noise widely existing in the wild. The NLH has a good ability to distinguish between the moving targets and noise. Moreover, to make full use of both the signals beamformed and the NLH, a classification network combining the LSAE layers to learn their representations by self-taught learning and the softmax layer for the classification is built. Experiments show that not only the representations learned by the LSAE layers are robust and much distinguishable but also the proposed method achieves a much better classification performance in comparison with the baseline classifiers for moving targets in the wild. (C) 2017 Elsevier B.V. All rights reserved.","keywords_author":["Microphone array","Delay-and-sum","Noise likelihood","Sparse auto-encoder","Wireless sensors"],"keywords_other":["IMPLEMENTATION","DESIGN","IDENTIFICATION","SYSTEM","RECOGNITION","TRACKING"],"max_cite":1.0,"pub_year":2017.0,"sources":"['wos']","rawkeys":["design","noise likelihood","identification","recognition","sparse auto-encoder","wireless sensors","microphone array","system","tracking","delay-and-sum","implementation"],"tags":["microphone arrays","design","noise likelihood","identification","recognition","system","stacked autoencoders","tracking","delay-and-sum","implementation","wireless sensor"]},{"p_id":11799,"title":"Blind inpainting using the fully convolutional neural network","abstract":"Most of existing inpainting techniques require to know beforehandwhere those damaged pixels are, i.e., non-blind inpainting methods. However, in many applications, such information may not be readily available. In this paper, we propose a novel blind inpainting method based on a fully convolutional neural network. We term this method as blind inpainting convolutional neural network (BICNN). It purely cascades three convolutional layers to directly learn an end-to-end mapping between a pre-acquired dataset of corrupted\/ground truth subimage pairs. Stochastic gradient descent with standard backpropagation is used to train the BICNN. Once the BICNN is learned, it can automatically identify and remove the corrupting patterns from a corrupted image without knowing the specific regions. The learned BICNN takes a corrupted image of any size as input and directly produces a clean output by only one pass of forward propagation. Experimental results indicate that the proposed method can achieve a better inpainting performance than the existing inpainting methods for various corrupting patterns.","keywords_author":["Blind inpainting","Convolutional neural network","Deep learning","Image processing","Image processing","Blind inpainting","Deep learning","Convolutional neural network"],"keywords_other":["Deep learning","Stochastic gradient descent","IMAGE","Corrupted images","Inpainting techniques","Inpainting method","Forward propagation","RECOGNITION","Convolutional neural network","Inpainting"],"max_cite":6.0,"pub_year":2017.0,"sources":"['ieee', 'scp', 'wos']","rawkeys":["inpainting method","recognition","deep learning","image processing","inpainting techniques","forward propagation","image","inpainting","convolutional neural network","stochastic gradient descent","corrupted images","blind inpainting"],"tags":["inpainting method","recognition","images","image processing","inpainting techniques","machine learning","forward propagation","inpainting","stochastic gradient descent","convolutional neural network","corrupted images","blind inpainting"]},{"p_id":101914,"title":"Need for Cognition and False Memory: Can One's Natural Processing Style Be Manipulated by External Factors?","abstract":"The purpose of this experiment was to provide an enhanced understanding of need for cognition (NFC) and its influence on one's memory accuracy. People who are high in NFC tend to put more cognitive effort into their mental processes than their low-NFC counterparts. To determine whether one's natural processing tendencies, as determined by NFC, can be influenced by external factors, manipulations to levels of processing were added. Participants viewed word lists from the Deese-Roediger-McDermott (DRM) paradigm and were instructed to process half of the DRM lists deeply and the other half shallowly. After all the lists were presented, participants completed 3 successive recall tests. The deep processing condition produced higher rates of false memories for both NFC groups than the shallow processing condition. In addition, the high-NFC group produced higher rates of target recall in both the deep and shallow conditions than the low-NFC group. However, the high-NFC group also produced higher rates of false recall for the shallowly processed lists. These data indicate that high-NFC people exhibit enhanced target recall for word lists, which may come at the expense of overall accuracy due to the increase of false recall.","keywords_author":null,"keywords_other":["WORD LISTS","ENHANCE","RETENTION","RECOGNITION","ILLUSIONS","RECALL"],"max_cite":1.0,"pub_year":2015.0,"sources":"['wos']","rawkeys":["recognition","word lists","illusions","retention","enhance","recall"],"tags":["recognition","enhancement","word lists","illusions","retention","recall"]},{"p_id":36382,"title":"A review of predictive quality of experience management in video streaming services","abstract":"\u00a9 1963-12012 IEEE. Satisfying the requirements of devices and users of online video streaming services is a challenging task. It requires not only managing the network quality of service but also to exert real-time control, addressing the user's quality of experience (QoE) expectations. QoE management is an end-to-end process that, due to the ever-increasing variety of video services, has become too complex for conventional 'reactive' techniques. Herein, we review the most significant 'predictive' QoE management methods for video streaming services, showing how different machine learning approaches may be used to perform proactive control. We pinpoint a selection of the best suited machine learning methods, highlighting advantages and limitations in specific service conditions. The review leads to lessons learned and guidelines to better address QoE requirements in complex video services.","keywords_author":["Machine learning","Quality of experience management","Video streaming services","Machine learning","quality of experience management","video streaming services"],"keywords_other":["Video streaming services","QOE MANAGEMENT","Predictive models","Computational model","RANDOM NEURAL-NETWORKS","Quality of experience managements","MODEL","RECOGNITION","Quality of experience (QoE)","Prediction algorithms","VARIABLE SELECTION","Streaming media"],"max_cite":1.0,"pub_year":2018.0,"sources":"['scp', 'wos']","rawkeys":["variable selection","recognition","model","predictive models","qoe management","machine learning","quality of experience (qoe)","prediction algorithms","video streaming services","quality of experience management","quality of experience managements","streaming media","random neural-networks","computational model"],"tags":["computational modeling","variable selection","recognition","model","predictive models","qoe management","machine learning","quality of experience (qoe)","prediction algorithms","video streaming services","quality of experience management","random neural network","streaming media"]},{"p_id":3615,"title":"Deep Learning Segmentation of Optical Microscopy Images Improves 3-D Neuron Reconstruction","abstract":"Digital reconstruction, or tracing, of 3-D neuron structure from microscopy images is a critical step toward reversing engineering the wiring and anatomy of a brain. Despite a number of prior attempts, this task remains very challenging, especially when images are contaminated by noises or have discontinued segments of neurite patterns. An approach for addressing such problems is to identify the locations of neuronal voxels using image segmentation methods, prior to applying tracing or reconstruction techniques. This preprocessing step is expected to remove noises in the data, thereby leading to improved reconstruction results. In this paper, we proposed to use 3-D convolutional neural networks (CNNs) for segmenting the neuronal microscopy images. Specifically, we designed a novel CNN architecture, that takes volumetric images as the inputs and their voxel-wise segmentation maps as the outputs. The developed architecture allows us to train and predict using large microscopy images in an end-to-end manner. We evaluated the performance of our model on a variety of challenging 3-D microscopy images from different organisms. Results showed that the proposed methods improved the tracing performance significantly when combined with different reconstruction algorithms.","keywords_author":["BigNeuron","Deep learning","image denoising","image segmentation","neuron reconstruction","Deep learning","image denoising","image segmentation","neuron reconstruction","BigNeuron"],"keywords_other":["Machine Learning","Brain","Reconstruction algorithms","BigNeuron","Digital reconstruction","BRAIN","Algorithms","RECOGNITION","Segmentation methods","Neurons","Reversing engineerings","Neural Networks (Computer)","Neuron reconstruction","Microscopy","NETWORKS","BIGNEURON","Convolutional neural network","Reconstruction techniques","Imaging, Three-Dimensional"],"max_cite":2.0,"pub_year":2017.0,"sources":"['wos', 'scp', 'ieee']","rawkeys":["brain","convolutional neural network","reconstruction algorithms","machine learning","algorithms","microscopy","neurons","neural networks (computer)","recognition","digital reconstruction","deep learning","image denoising","networks","three-dimensional","segmentation methods","bigneuron","image segmentation","reversing engineerings","neuron reconstruction","reconstruction techniques","imaging"],"tags":["brain","convolutional neural network","reconstruction algorithms","machine learning","algorithms","microscopy","neurons","recognition","images","digital reconstruction","neural networks","image denoising","networks","three-dimensional","segmentation methods","bigneuron","image segmentation","neuron reconstruction","reconstruction techniques","reverse engineering"]},{"p_id":3620,"title":"Words Matter: Scene Text for Image Classification and Retrieval","abstract":"Text in natural images typically adds meaning to an object or scene. In particular, text specifies which business places serve drinks (e.g., cafe, teahouse) or food (e.g., restaurant, pizzeria), and what kind of service is provided (e.g., massage, repair). The mere presence of text, its words, and meaning are closely related to the semantics of the object or scene. This paper exploits textual contents in images for fine-grained business place classification and logo retrieval. There are four main contributions. First, we show that the textual cues extracted by the proposed method are effective for the two tasks. Combining the proposed textual and visual cues outperforms visual only classification and retrieval by a large margin. Second, to extract the textual cues, a generic and fully unsupervised word box proposal method is introduced. The method reaches state-of-the-art word detection recall with a limited number of proposals. Third, contrary to what is widely acknowledged in text detection literature, we demonstrate that high recall in word detection is more important than high f-score at least for both tasks considered in this work. Last, this paper provides a large annotated text detection dataset with 10 K images and 27 601 word boxes.","keywords_author":["Professional communication","image retrieval","computers and information processing","image analysis","image classification","text recognition","object detection"],"keywords_other":["RECOGNITION","VIDEO"],"max_cite":0.0,"pub_year":2017.0,"sources":"['wos', 'ieee']","rawkeys":["recognition","professional communication","image retrieval","computers and information processing","video","object detection","text recognition","image classification","image analysis"],"tags":["recognition","professional communication","image retrieval","computers and information processing","video","object detection","text recognition","image classification","image analysis"]},{"p_id":28200,"title":"Computer-aided diagnosis for colonoscopy","abstract":null,"keywords_author":null,"keywords_other":["ENDOCYTOSCOPY","ENDOSCOPY","COLORECTAL POLYP HISTOLOGY","CLASSIFICATION","SYSTEM","RECOGNITION","AUTOFLUORESCENCE","IMAGING MAGNIFYING COLONOSCOPY","PATTERNS","LESIONS"],"max_cite":5.0,"pub_year":2017.0,"sources":"['scp', 'wos']","rawkeys":["endocytoscopy","recognition","imaging magnifying colonoscopy","patterns","system","endoscopy","colorectal polyp histology","classification","lesions","autofluorescence"],"tags":["endocytoscopy","recognition","imaging magnifying colonoscopy","patterns","system","endoscopy","colorectal polyp histology","classification","lesions","autofluorescence"]},{"p_id":3625,"title":"Photo Filter Recommendation by Category-Aware Aesthetic Learning","abstract":"Nowadays, social media has become a popular platform for the public to share photos. To make photos more visually appealing, users usually apply filters on their photos without domain knowledge. However, due to the growing number of filter types, it becomes a major issue for users to choose the best filter type. For this purpose, filter recommendation for photo aesthetics takes an important role in image quality ranking problems. In these years, several works have declared that convolutional neural networks (CNNs) outperform traditional methods in image aesthetic categorization, which classifies images into high or low quality. Most of them do not consider the effect on filtered images; hence, we propose a novel image aesthetic learning for filter recommendation. Instead of binarizing image quality, we adjust the state-of-the-art CNN architectures and design a pairwise loss function to learn the embedded aesthetic responses in hidden layers for filtered images. Based on our pilot study, we observe image categories (e.g., portrait, landscape, food) will affect user preference on filter selection. We further integrate category classification into our proposed aesthetic-oriented models. To the best of our knowledge, there is no public dataset for aesthetic judgment with filtered images. We create a new dataset called filter aesthetic comparison dataset (FACD). It is the first dataset containing 28 160 filtered images and 42 240 user preference labels. We conduct experiments on the collected FACD for filter recommendation, and the results show that our proposed category-aware aesthetic learning outperforms aesthetic classification methods (e.g., 12% relative improvement).","keywords_author":["Aesthetic","convolutional neural network (CNN)","filter recommendation","image quality","pairwise comparison"],"keywords_other":["NETWORKS","IMAGE QUALITY ASSESSMENT","RANK","RECOGNITION","ENHANCEMENT","SIMILARITY"],"max_cite":0.0,"pub_year":2017.0,"sources":"['wos', 'ieee']","rawkeys":["image quality assessment","filter recommendation","recognition","enhancement","aesthetic","rank","similarity","pairwise comparison","convolutional neural network (cnn)","networks","image quality"],"tags":["image quality assessment","filter recommendation","aesthetics","recognition","enhancement","similarity","standards","pairwise comparison","networks","convolutional neural network","image quality"]},{"p_id":3626,"title":"Compact Hash Codes for Efficient Visual Descriptors Retrieval in Large Scale Databases","abstract":"In this paper, we present an efficient method for visual descriptors retrieval based on compact hash codes computed using a multiple k-means assignment. The method has been applied to the problem of approximate nearest neighbor (ANN) search of local and global visual content descriptors, and it has been tested on different datasets: three large scale standard datasets of engineered features of up to one billion descriptors (BIGANN) and, supported by recent progress in convolutional neural networks (CNNs), on CIFAR-10, MNIST, INRIA Holidays, Oxford 5K, and Paris 6K datasets; also, the recent DEEP1B dataset, composed by one billion CNN-based features, has been used. Experimental results show that, despite its simplicity, the proposed method obtains a very high performance that makes it superior to more complex state-of-the-art methods.","keywords_author":["Convolutional neural network (CNN)","hashing","nearest neighbor search","retrieval","SIFT"],"keywords_other":["NEAREST-NEIGHBOR SEARCH","LEARNING BINARY-CODES","OPTIMIZED PRODUCT QUANTIZATION","MULTIINDEX","RECOGNITION","IMAGE RETRIEVAL"],"max_cite":0.0,"pub_year":2017.0,"sources":"['wos', 'ieee']","rawkeys":["sift","recognition","learning binary-codes","image retrieval","convolutional neural network (cnn)","nearest neighbor search","hashing","retrieval","nearest-neighbor search","optimized product quantization","multiindex"],"tags":["recognition","learning binary-codes","image retrieval","nearest neighbor search","hashing","convolutional neural network","multi-index","retrieval","scale invariant feature transforms","optimized product quantization"]},{"p_id":11819,"title":"Medical image retrieval using deep convolutional neural network","abstract":"With a widespread use of digital imaging data in hospitals, the size of medical image repositories is increasing rapidly. This causes difficulty in managing and querying these large databases leading to the need of content based medical image retrieval (CBMIR) systems. A major challenge in CBMIR systems is the semantic gap that exists between the low level visual information captured by imaging devices and high level semantic information perceived by human. The efficacy of such systems is more crucial in terms of feature representations that can characterize the high-level information completely. In this paper, we propose a framework of deep learning for CBMIR system by using deep convolutional neural network (CNN) that is trained for classification of medical images. An intermodal dataset that contains twenty-four classes and five modalities is used to train the network. The learned features and the classification results are used to retrieve medical images. For retrieval, best results are achieved when class based predictions are used. An average classification accuracy of 99.77% and a mean average precision of 0.69 is achieved for retrieval task. The proposed method is best suited to retrieve multimodal medical images for different body organs. (C) 2017 Elsevier B.V. All rights reserved.","keywords_author":["Content based medical image retrieval (CBMIR)","Convolutional neural networks (CNNs)","Deep learning","Similarity metric","Content based medical image retrieval (CBMIR)","Convolutional neural networks (CNNs)","Similarity metric","Deep learning"],"keywords_other":["Content based medical image retrieval","FEATURES","Classification results","Similarity metrics","CLASSIFICATION","Feature representation","High-level information","RECOGNITION","Convolutional neural network","SEMANTICS","Multimodal medical images","Classification accuracy"],"max_cite":12.0,"pub_year":2017.0,"sources":"['ieee', 'scp', 'wos']","rawkeys":["recognition","multimodal medical images","features","deep learning","semantics","classification accuracy","high-level information","feature representation","similarity metrics","content based medical image retrieval (cbmir)","classification","content based medical image retrieval","convolutional neural network","convolutional neural networks (cnns)","similarity metric","classification results"],"tags":["recognition","multimodal medical images","features","semantics","classification accuracy","machine learning","high-level information","similarity metrics","classification","convolutional neural network","content based medical image retrieval","feature representation","classification results"]},{"p_id":11820,"title":"Classifying Radio Galaxies with the Convolutional Neural Network","abstract":"We present the application of a. deep machine learning technique to classify radio images of extended sources on a morphological basis using convolutional neural networks (CNN). In this study, we have taken the case of the. Fanaroff-Riley (FR) class of radio galaxies as well as radio galaxies with bent-tailed morphology. We have used archival data from the Very Large Array (VLA)-Faint Images of the Radio Sky at Twenty Centimeters survey and existing visually classified samples available in the literature to train a neural network for morphological classification of these categories of radio sources. Our training sample size for each of these categories is similar to 200 sources, which has been augmented by rotated versions of the same. Our study shows that CNNs can classify images of the FRI and FRII and bent-tailed radio galaxies with high accuracy (maximum precision at 95%) using well-defined samples and a. \"fusion classifier,\" which combines the results of binary classifications, while allowing for a mechanism to find sources with unusual morphologies. The individual precision is highest for bent-tailed radio galaxies at 95% and is 91% and 75% for the FRI and FRII classes, respectively, whereas the recall is highest for FRI and FRIIs at 91% each, while the bent-tailed class has a recall of 79%. These results show that our results are comparable to that of manual classification, while being much faster. Finally, we discuss the computational and data-related challenges associated with the. morphological classification of radio galaxies with CNNs.","keywords_author":["methods: miscellaneous","methods: observational","radio continuum: galaxies","techniques: miscellaneous"],"keywords_other":["SELECTION","SORTING TRIPLES","SQUARE KILOMETER ARRAY","1ST DATABASE","CONFIG SAMPLE","POPULATION","CLASSIFICATION","SKY","RECOGNITION","AGN"],"max_cite":3.0,"pub_year":2017.0,"sources":"['ieee', 'wos']","rawkeys":["methods: observational","methods: miscellaneous","sorting triples","recognition","techniques: miscellaneous","1st database","population","radio continuum: galaxies","config sample","classification","sky","selection","square kilometer array","agn"],"tags":["methods: observational","methods: miscellaneous","sorting triples","recognition","techniques: miscellaneous","1st database","population","radio continuum: galaxies","config sample","classification","sky","square kilometer arrays","selection","agn"]},{"p_id":11822,"title":"NIRFaceNet: A convolutional neural network for near-infrared face identification","abstract":"Near-infrared (NIR) face recognition has attracted increasing attention because of its advantage of illumination invariance. However, traditional face recognition methods based on NIR are designed for and tested in cooperative-user applications. In this paper, we present a convolutional neural network (CNN) for NIR face recognition (specifically face identification) in non-cooperative-user applications. The proposed NIRFaceNet is modified from GoogLeNet, but has a more compact structure designed specifically for the Chinese Academy of Sciences Institute of Automation (CASIA) NIR database and can achieve higher identification rates with less training time and less processing time. The experimental results demonstrate that NIRFaceNet has an overall advantage compared to other methods in the NIR face recognition domain when image blur and noise are present. The performance suggests that the proposed NIRFaceNet method may be more suitable for non-cooperative-user applications.","keywords_author":["Convolutional neural network","Illumination invariance","Near-infrared face recognition","near-infrared face recognition","illumination invariance","convolutional neural network"],"keywords_other":["Near Infrared","ZERNIKE MOMENTS","Identification rates","Face identification","REPRESENTATION","Illumination invariance","Chinese Academy of Sciences","RECOGNITION","Compact structures","Convolutional neural network","Face recognition methods","ILLUMINATION"],"max_cite":2.0,"pub_year":2016.0,"sources":"['scp', 'wos']","rawkeys":["face identification","illumination","recognition","illumination invariance","zernike moments","identification rates","representation","near-infrared face recognition","chinese academy of sciences","convolutional neural network","compact structures","near infrared","face recognition methods"],"tags":["face identification","illumination","recognition","zernike moments","identification rates","representation","near-infrared","near-infrared face recognition","chinese academy of sciences","illumination invariant","convolutional neural network","compact structures","face recognition methods"]},{"p_id":11824,"title":"Image Classification Based on the Boost Convolutional Neural Network","abstract":"Convolutional neural networks (CNNs), which are composed of multiple processing layers to learn the representations of data with multiple abstract levels, are the most successful machine learning models in recent years. However, these models can have millions of parameters and many layers, which are difficult to train, and sometimes several days or weeks are required to tune the parameters. Within this paper, we present the usage of a trained deep convolutional neural network model to extract the features of the images, and then, used the AdaBoost algorithm to assemble the Softmax classifiers into recognizable images. This method resulted in a 3% increase of accuracy of the trained CNN models, and dramatically reduced the retraining time cost, and thus, it has good application prospects.","keywords_author":["boosting","Convolutional neural network","deep learning","ensemble learning","Convolutional neural network","ensemble learning","deep learning","boosting"],"keywords_other":["Ensemble learning","Machine learning models","Application prospect","Multiple processing","MODEL","RECOGNITION","AdaBoost algorithm","Abstract levels","Convolutional neural network","Boosting"],"max_cite":1.0,"pub_year":2018.0,"sources":"['ieee', 'scp', 'wos']","rawkeys":["recognition","model","deep learning","machine learning models","application prospect","multiple processing","abstract levels","convolutional neural network","ensemble learning","boosting","adaboost algorithm"],"tags":["recognition","model","machine learning models","machine learning","application prospect","multiple processing","abstraction level","convolutional neural network","ensemble learning","boosting","adaboost algorithm"]},{"p_id":3634,"title":"Learning to Predict Eye Fixations via Multiresolution Convolutional Neural Networks","abstract":"Eye movements in the case of freely viewing natural scenes are believed to be guided by local contrast, global contrast, and top-down visual factors. Although a lot of previous works have explored these three saliency cues for several years, there still exists much room for improvement on how to model them and integrate them effectively. This paper proposes a novel computation model to predict eye fixations, which adopts a multiresolution convolutional neural network (Mr-CNN) to infer these three types of saliency cues from raw image data simultaneously. The proposed Mr-CNN is trained directly from fixation and nonfixation pixels with multiresolution input image regions with different contexts. It utilizes image pixels as inputs and eye fixation points as labels. Then, both the local and global contrasts are learned by fusing information in multiple contexts. Meanwhile, various top-down factors are learned in higher layers. Finally, optimal combination of top-down factors and bottom-up contrasts can be learned to predict eye fixations. The proposed approach significantly outperforms the state-of-the-art methods on several publically available benchmark databases, demonstrating the superiority of Mr-CNN. We also apply our method to the RGB-D image saliency detection problem. Through learning saliency cues induced by depth and RGB information on pixel level jointly and their interactions, our model achieves better performance on predicting eye fixations in RGB-D images.","keywords_author":["Contrast","convolutional neural network (CNN)","eye fixation prediction","RGB-D","saliency detection","Contrast","convolutional neural network (CNN)","eye fixation prediction","RGB-D","saliency detection"],"keywords_other":["SELECTION","Multiple contexts","Benchmark database","Global contrasts","Computation model","State-of-the-art methods","VISUAL-ATTENTION","Optimal combination","MODEL","HUMAN GAZE","RECOGNITION","SALIENCY DETECTION","Convolutional neural network","SCALE","Image saliencies","OBJECTS"],"max_cite":2.0,"pub_year":2018.0,"sources":"['ieee', 'scp', 'wos']","rawkeys":["state-of-the-art methods","saliency detection","contrast","convolutional neural network","benchmark database","visual-attention","human gaze","recognition","global contrasts","convolutional neural network (cnn)","objects","eye fixation prediction","computation model","selection","scale","optimal combination","image saliencies","model","multiple contexts","rgb-d"],"tags":["state-of-the-art methods","saliency detection","contrast","convolutional neural network","benchmark database","human gaze","computational modeling","recognition","global contrasts","objects","eye fixation prediction","visual attention","selection","scale","optimal combination","image saliencies","model","multiple contexts","rgb-d"]},{"p_id":11830,"title":"Convolutional Neural Network Based Fault Detection for Rotating Machinery","abstract":"Vibration analysis is a well-established technique for condition monitoring of rotating machines as the vibration patterns differ depending on the fault or machine condition. Currently, mainly manually-engineered features, such as the ball pass frequencies of the raceway, RMS, kurtosis an crest, are used for automatic fault detection. Unfortunately, engineering and interpreting such features requires a significant level of human expertise. To enable non-experts in vibration analysis to perform condition monitoring, the overhead of feature engineering for specific faults needs to be reduced as much as possible. Therefore, in this article we propose a feature learning model for condition monitoring based on convolutional neural networks. The goal of this approach is to autonomously learn useful features for bearing fault detection from the data itself. Several types of bearing faults such as outer-raceway faults and lubrication degradation are considered, but also healthy bearings and rotor imbalance are included. For each condition, several bearings are tested to ensure generalization of the fault-detection system. Furthermore, the feature-learning based approach is compared to a feature-engineering based approach using the same data to objectively quantify their performance. The results indicate that the feature-learning system, based on convolutional neural networks, significantly outperforms the classical feature-engineering based approach which uses manually engineered features and a random forest classifier. The former achieves an accuracy of 93.61 percent and the latter an accuracy of 87.25 percent. (C) 2016 Elsevier Ltd. All rights reserved.","keywords_author":["Condition monitoring","Convolutional neural network","Fault detection","Feature learning","Machine learning","Vibration analysis","Condition monitoring","Fault detection","Vibration analysis","Machine learning","Convolutional neural network","Feature learning"],"keywords_other":["DIAGNOSIS","Well-established techniques","Automatic fault detection","ALGORITHM","Random forest classifier","RECOGNITION","Bearing fault detection","Convolutional neural network","Feature learning","Fault detection systems","Feature learning system"],"max_cite":45.0,"pub_year":2016.0,"sources":"['ieee', 'scp', 'wos']","rawkeys":["diagnosis","algorithm","feature learning system","recognition","condition monitoring","automatic fault detection","machine learning","random forest classifier","well-established techniques","fault detection","fault detection systems","convolutional neural network","feature learning","bearing fault detection","vibration analysis"],"tags":["diagnosis","feature learning system","recognition","condition monitoring","automatic fault detection","machine learning","random forest classifier","well-established techniques","fault detection","fault detection systems","convolutional neural network","algorithms","feature learning","bearing fault detection","vibration analysis"]},{"p_id":11832,"title":"Convolutional Neural Networks for Water Body Extraction from Landsat Imagery","abstract":"Traditional machine learning methods for water body extraction need complex spectral analysis and feature selection which rely on wealth of prior knowledge. They are time-consuming and hard to satisfy our request for accuracy, automation level and a wide range of application. We present a novel deep learning framework for water body extraction from Landsat imagery considering both its spectral and spatial information. The framework is a hybrid of convolutional neural networks (CNN) and logistic regression (LR) classifier. CNN, one of the deep learning methods, has acquired great achievements on various visual-related tasks. CNN can hierarchically extract deep features from raw images directly, and distill the spectral-spatial regularities of input data, thus improving the classification performance. Experimental results based on three Landsat imagery datasets show that our proposed model achieves better performance than support vector machine (SVM) and artificial neural network (ANN).","keywords_author":["convolutional neural networks","deep learning","landsat imagery","Water body extraction","Water body extraction","landsat imagery","deep learning","convolutional neural networks"],"keywords_other":["Classification performance","Waterbodies","Landsat imagery","DELINEATION","CLASSIFICATION","Learning frameworks","RECOGNITION","Convolutional neural network","Machine learning methods","Logistic regressions","Spatial informations"],"max_cite":4.0,"pub_year":2017.0,"sources":"['ieee', 'scp', 'wos']","rawkeys":["machine learning methods","recognition","convolutional neural networks","classification performance","delineation","deep learning","waterbodies","water body extraction","landsat imagery","learning frameworks","logistic regressions","classification","convolutional neural network","spatial informations"],"tags":["machine learning methods","recognition","delineation","classification performance","waterbodies","water body extraction","machine learning","landsat imagery","learning frameworks","logistic regressions","classification","convolutional neural network","spatial informations"]},{"p_id":11840,"title":"Classification of breast cancer histology images using Convolutional Neural Networks","abstract":"Breast cancer is one of the main causes of cancer death worldwide. The diagnosis of biopsy tissue with hematoxylin and eosin stained images is non-trivial and specialists often disagree on the final diagnosis. Computer-aided Diagnosis systems contribute to reduce the cost and increase the efficiency of this process. Conventional classification approaches rely on feature extraction methods designed for a specific problem based on field-knowledge. To overcome the many difficulties of the feature-based approaches, deep learning methods are becoming important alternatives. A method for the classification of hematoxylin and eosin stained breast biopsy images using Convolutional Neural Networks (CNNs) is proposed. Images are classified in four classes, normal tissue, benign lesion, in situ carcinoma and invasive carcinoma, and in two classes, carcinoma and non-carcinoma. The architecture of the network is designed to retrieve information at different scales, including both nuclei and overall tissue organization. This design allows the extension of the proposed system to whole-slide histology images. The features extracted by the CNN are also used for training a Support Vector Machine classifier. Accuracies of 77.8% for four class and 83.3% for carcinoma\/non-carcinoma are achieved. The sensitivity of our method for cancer cases is 95.6%.","keywords_author":null,"keywords_other":["CYTOLOGICAL IMAGES","DIAGNOSIS","RECOGNITION"],"max_cite":9.0,"pub_year":2017.0,"sources":"['ieee', 'wos']","rawkeys":["diagnosis","recognition","cytological images"],"tags":["diagnosis","recognition","cytological images"]},{"p_id":11844,"title":"White Blood Cells Classification with Deep Convolutional Neural Networks","abstract":"The necessary step in the diagnosis of leukemia by the attending physician is to classify the white blood cells in the bone marrow, which requires the attending physician to have a wealth of clinical experience. Now the deep learning is very suitable for the study of image recognition classification, and the effect is not good enough to directly use some famous convolution neural network (CNN) models, such as AlexNet model, GoogleNet model, and VGGFace model. In this paper, we construct a new CNN model called WBCNet model that can fully extract features of the microscopic white blood cell image by combining batch normalization algorithm, residual convolution architecture, and improved activation function. WBCNet model has 33 layers of network architecture, whose speed has greatly been improved compared with the traditional CNN model in training period, and it can quickly identify the category of white blood cell images. The accuracy rate is 77.65% for Top-1 and 98.65% for Top-5 on the training set, while 83% for Top-1 on the test set. This study can help doctors diagnose leukemia, and reduce misdiagnosis rate.","keywords_author":["batch normalization","convolution neural network","data augmentation","deep learning","residual network","White blood cell","White blood cell","convolution neural network","residual network","data augmentation","batch normalization","deep learning"],"keywords_other":["White blood cells","Convolution neural network","Data augmentation","batch normalization","RECOGNITION","Normalization algorithms","Activation functions","White blood cell images","Deep convolutional neural networks"],"max_cite":0.0,"pub_year":2018.0,"sources":"['scp', 'wos']","rawkeys":["recognition","convolution neural network","deep learning","white blood cell images","deep convolutional neural networks","white blood cells","data augmentation","batch normalization","activation functions","residual network","normalization algorithms","white blood cell"],"tags":["recognition","white blood cells","white blood cell images","machine learning","data augmentation","activation functions","residual network","convolutional neural network","bayesian networks","normalization algorithms"]},{"p_id":11845,"title":"Material structure-property linkages using three-dimensional convolutional neural networks","abstract":"The core materials knowledge needed in the accelerated design, development, and deployment of new and improved materials is most accessible when cast in the form of computationally low cost (reduced order) and reliable process-structure-property (PSP) linkages. Quantification of the material structure (also referred as microstructure) is the core challenge in this task. Conventionally, microstructure quantification has been addressed using highly simplified measures suggested by the governing physics, with the list of measures often suitably augmented by the intuition of the materials expert. In this paper, we develop an objective (data-driven) approach to efficiently and accurately link a three-dimensional (3-D) microstructure to its effective (homogenized) properties. Our method employs a 3-D convolutional neural network (CNN) to learn the salient features of the material microstructures that lead to good predictive performance for the effective property of interest. We then utilize 3-D CNN learned features as estimators of higher-order spatial correlations, and formulate an integrated framework combining 3-D CNN features with 2-point spatial correlations. In this work, we created an extremely large microstructure-property benchmark dataset of 5900 microstructures, and demonstrated that our CNN based approach not only learns interpretable microstructure features, but also leads to improved accuracy in property predictions for new microstructures, while achieving a dramatic reduction in the computation time. (C) 2017 Acta Materialia Inc. Published by Elsevier Ltd. All rights reserved.","keywords_author":["Convolutional neural networks","Spatial correlations","Structure-property linkages","Principal component analysis"],"keywords_other":["ELEMENT","SIMULATIONS","QUANTIFICATION","COMPOSITES","DESIGN","MICROSTRUCTURE","RECOGNITION","PHASE-FIELD","STRUCTURE EVOLUTION LINKAGES","EFFECTIVE ELASTIC PROPERTIES"],"max_cite":0.0,"pub_year":2017.0,"sources":"['ieee', 'wos']","rawkeys":["element","design","principal component analysis","recognition","convolutional neural networks","structure evolution linkages","quantification","microstructure","phase-field","composites","spatial correlations","simulations","structure-property linkages","effective elastic properties"],"tags":["design","structure evolution linkages","principal component analysis","recognition","quantification","microstructure","phase-field","composition","convolutional neural network","spatial correlations","structure-property linkages","elements","simulation","effective elastic properties"]},{"p_id":11850,"title":"Multiple object extraction from aerial imagery with convolutional neural networks","abstract":"An automatic system to extract terrestrial objects from aerial imagery has many applications in a wide range of areas. However, in general, this task has been performed by human experts manually, so that it is very costly and time consuming. There have been many attempts at automating this task, but many of the existing works are based on class-specific features and classifiers. In this article, the authors propose a convolutional neural network (CNN)-based building and road extraction system. This takes raw pixel values in aerial imagery as input and outputs predicted three-channel label images (building road background). Using CNNs, both feature extractors and classifiers are automatically constructed. The authors propose a new technique to train a single CNN efficiently for extracting multiple kinds of objects simultaneously. Finally, they show that the proposed technique improves the prediction performance and surpasses state-of-the-art results tested on a publicly available aerial imagery dataset. (C) 2016 Society for Imaging Science and Technology.","keywords_author":null,"keywords_other":["Automatic systems","Class specific features","Multiple objects","State of the art","Input and outputs","RECOGNITION","Convolutional neural network","Prediction performance","Feature extractor"],"max_cite":16.0,"pub_year":2016.0,"sources":"['scp', 'wos']","rawkeys":["recognition","state of the art","automatic systems","input and outputs","class specific features","convolutional neural network","multiple objects","feature extractor","prediction performance"],"tags":["recognition","state of the art","automatic systems","input and outputs","class specific features","convolutional neural network","multiple objects","feature extractor","prediction performance"]},{"p_id":11855,"title":"Vehicle Type Classification Using a Semisupervised Convolutional Neural Network","abstract":"In this paper, we propose a vehicle type classification method using a semisupervised convolutional neural network from vehicle frontal-view images. In order to capture rich and discriminative information of vehicles, we introduce sparse Laplacian filter learning to obtain the filters of the network with large amounts of unlabeled data. Serving as the output layer of the network, the softmax classifier is trained by multitask learning with small amounts of labeled data. For a given vehicle image, the network can provide the probability of each type to which the vehicle belongs. Unlike traditional methods by using handcrafted visual features, our method is able to automatically learn good features for the classification task. The learned features are discriminative enough to work well in complex scenes. We build the challenging BIT-Vehicle dataset, including 9850 high-resolution vehicle frontal-view images. Experimental results on our own dataset and a public dataset demonstrate the effectiveness of the proposed method.","keywords_author":["Feature learning","filter learning","multitask learning","neural network","vehicle type classification"],"keywords_other":["CASCADE","ALGORITHM","RECOGNITION","FACE DETECTION","ARCHITECTURE"],"max_cite":28.0,"pub_year":2015.0,"sources":"['ieee', 'wos']","rawkeys":["algorithm","neural network","recognition","vehicle type classification","face detection","multitask learning","filter learning","feature learning","cascade","architecture"],"tags":["recognition","neural networks","vehicle type classification","face detection","multitask learning","filter learning","algorithms","feature learning","cascade","architecture"]},{"p_id":36444,"title":"A self-organizing deep belief network for nonlinear system modeling","abstract":"\u00a9 2018 In this paper, a self-organizing deep belief network (SODBN) with growing and pruning algorithms is proposed for nonlinear system modeling. Although deep learning-based DBN has been widely used in recent years, actually more detailed researches about how to dynamically determine its structure are seldom observed in the existing literatures. The SODBN can automatically determine its structure using growing and pruning algorithms instead of artificial experience. Firstly, the structure of SODBN is constructed automatically by changing the number of both hidden layers and the hidden neurons during the training process. The self-organizing strategy is implemented by automatic growing and pruning algorithm (AGP), which is actually equivalent to adding and pruning the connecting weights between neurons. Secondly, the weights are dynamically adjusted during the process of structure self-organization. SODBN is able to adjust the weights in the dynamic process of self-organizing structure, and is helpful to improve the network performances, including running time and accuracy. Finally, the proposed SODBN has been tested on three benchmark problems, including nonlinear system modeling, water quality prediction in practical wastewater treatment system as well as air pollutants concentrations prediction. The corresponding experimental results show that SODBN has better performances than some existing neural networks.","keywords_author":["Automatic growing and pruning algorithm","Deep learning","Dynamic weights adjustment","Self-organizing deep belief network","Wastewater treatment system","Self-organizing deep belief network","Deep learning","Automatic growing and pruning algorithm","Dynamic weights adjustment","Wastewater treatment system"],"keywords_other":["Nonlinear system modeling","PREDICTION","PM2.5","Bench-mark problems","Water quality predictions","WEIGHTS","FUZZY NEURAL-NETWORKS","Wastewater treatment system","ALGORITHM","Deep belief networks","IDENTIFICATION","Dynamic weight","CONVERGENCE","RECOGNITION","Self-organizing structures","Growing and pruning"],"max_cite":1.0,"pub_year":2018.0,"sources":"['scp', 'wos']","rawkeys":["convergence","self-organizing deep belief network","identification","fuzzy neural-networks","pm2.5","weights","automatic growing and pruning algorithm","wastewater treatment system","self-organizing structures","algorithm","recognition","deep learning","dynamic weight","water quality predictions","nonlinear system modeling","bench-mark problems","growing and pruning","prediction","deep belief networks","dynamic weights adjustment"],"tags":["self-organizing deep belief network","identification","pm2.5","automatic growing and pruning algorithm","wastewater treatment system","self-organizing structures","machine learning","mathematics","algorithms","recognition","neural networks","dynamic weighting","weight","water quality predictions","nonlinear system modeling","bench-mark problems","growing and pruning","prediction","deep belief networks","dynamic weights adjustment"]},{"p_id":28253,"title":"Computational validation of the motor contribution to speech perception","abstract":"Action perception and recognition are core abilities fundamental for human social interaction. A parieto-frontal network (the mirror neuron system) matches visually presented biological motion information onto observers' motor representations. This process of matching the actions of others onto our own sensorimotor repertoire is thought to be important for action recognition, providing a non-mediated \"motor perception\" based on a bidirectional flow of information along the mirror parieto-frontal circuits. State-of-the-art machine learning strategies for hand action identification have shown better performances when sensorimotor data, as opposed to visual information only, are available during learning. As speech is a particular type of action (with acoustic targets), it is expected to activate a mirror neuron mechanism. Indeed, in speech perception, motor centers have been shown to be causally involved in the discrimination of speech sounds. In this paper, we review recent neurophysiological and machine learning-based studies showing (a) the specific contribution of the motor system to speech perception and (b) that automatic phone recognition is significantly improved when motor data are used during training of classifiers (as opposed to learning from purely auditory data). \u00a9 2014 Cognitive Science Society, Inc.","keywords_author":["Automatic speech recognition","Machine learning","Motor theory of speech perception","Transcranial magnetic stimulation"],"keywords_other":["Humans","Pattern Recognition, Automated","Recognition (Psychology)","Psychomotor Performance","Speech Perception","Artificial Intelligence"],"max_cite":5.0,"pub_year":2014.0,"sources":"['scp', 'ieee']","rawkeys":["artificial intelligence","motor theory of speech perception","recognition (psychology)","automated","automatic speech recognition","speech perception","machine learning","humans","transcranial magnetic stimulation","psychomotor performance","pattern recognition"],"tags":["motor theory of speech perception","recognition","automated","automatic speech recognition","speech perception","machine learning","humans","transcranial magnetic stimulation","psychomotor performance","pattern recognition"]},{"p_id":11869,"title":"A modified convolutional neural network for face sketch synthesis","abstract":"A novel deep learning method for face sketch synthesis is proposed in this work. It builds a lightweight neural network which contains two convolutional layers, a pooling layer and a multilayer perceptron convolutional layer to learn a mapping from face photos to sketches. Unlike conventional example-based methods which need to solve complex optimization problems, the proposed method only computes convolution and pooling operations, hence significantly improves the synthesis efficiency. Besides, due to the global feature extraction of the convolutional layer, it achieves more continuous and faithful facial contours. Experiments on three benchmark datasets demonstrate that compared with several state-of-the -arts, the proposed method achieves highly competitive numerical results and is more robust to illumination and expression variations. (C) 2017 Elsevier Ltd. All rights reserved.","keywords_author":["Face sketch synthesis","Convolutional neural networks","Sparse coding"],"keywords_other":["SPARSE REPRESENTATION","IMAGE SUPERRESOLUTION","RECOGNITION"],"max_cite":1.0,"pub_year":2018.0,"sources":"['ieee', 'wos']","rawkeys":["recognition","convolutional neural networks","face sketch synthesis","sparse representation","image superresolution","sparse coding"],"tags":["recognition","face sketch synthesis","sparse representation","convolutional neural network","sparse coding"]},{"p_id":11871,"title":"Spatiotemporal Satellite Image Fusion Using Deep Convolutional Neural Networks","abstract":"We propose a novel spatiotemporal fusion method based on deep convolutional neural networks (CNNs) under the application background of massive remote sensing data. In the training stage, we build two five-layer CNNs to deal with the problems of complicated correspondence and large spatial resolution gaps between MODIS and Landsat images. Specifically, we first learn a nonlinear mapping CNN between MODIS and low-spatial-resolution (LSR) Landsat images and then learn a super-resolution CNN between LSR Landsat and original Landsat images. In the prediction stage, instead of directly taking the outputs of CNNs as the fusion result, we design a fusion model consisting of high-pass modulation and a weighting strategy to make full use of the information in prior images. Specifically, we firstmap the input MODIS images to transitional images via the learned nonlinear mapping CNN and further improve the transitional images to LSR Landsat images via the fusion model; then, via the learned SR CNN, the LSR Landsat images are supersolved to transitional images, which are further improved to Landsat images via the fusion model. Compared with the previous learning-based fusion methods, mainly referring to the sparse-representation-based methods, our CNNs-based spatiotemporalmethod has the following advantages: 1) automatically extracting effective image features; 2) learning an end-to-end mapping between MODIS and LSR Landsat images; and 3) generating more favorable fusion results. To examine the performance of the proposed fusion method, we conduct experiments on two representative Landsat-MODIS datasets by comparing with the sparse-representation-based spatiotemporal fusion model. The quantitative evaluations on all possibleprediction dates and the comparison of fusion results on one key date in both visual effect and quantitative evaluationsdemonstrate that the proposed method can generate more accurate fusionresults.","keywords_author":["Convolutional neural network (CNN)","nonlinear mapping (NLM)","spatial resolution","temporal resolution"],"keywords_other":["TIME-SERIES","LANDSAT DATA","REFLECTANCE FUSION","MODEL","RECOGNITION","DYNAMICS","RESOLUTION","MODIS"],"max_cite":1.0,"pub_year":2018.0,"sources":"['ieee', 'wos']","rawkeys":["spatial resolution","temporal resolution","recognition","model","time-series","convolutional neural network (cnn)","reflectance fusion","landsat data","modis","dynamics","resolution","nonlinear mapping (nlm)"],"tags":["spatial resolution","dynamics","recognition","model","reflectance fusion","nonlinear mappings","convolutional neural network","landsat data","modis","resolution","time series","temporal resolution"]},{"p_id":11873,"title":"Deep convolutional neural networks for thermal infrared object tracking","abstract":"Unlike the visual object tracking, thermal infrared object tracking can track a target object in total darkness. Therefore, it has broad applications, such as in rescue and video surveillance at night. However, there are few studies in this field mainly because thermal infrared images have several unwanted attributes, which make it difficult to obtain the discriminative features of the target. Considering the powerful representational ability of convolutional neural networks and their successful application in visual tracking, we transfer the pre-trained convolutional neural networks based on visible images to thermal infrared tracking. We observe that the features from the fully-connected layer are not suitable for thermal infrared tracking due to the lack of spatial information of the target, while the features from the convolution layers are. Besides, the features from a single convolution layer are not robust to various challenges. Based on this observation, we propose a correlation filter based ensemble tracker with multi-layer convolutional features for thermal infrared tracking (MCFTS). Firstly, we use pre-trained convolutional neural networks to extract the features of the multiple convolution layers of the thermal infrared target. Then, a correlation filter is used to construct multiple weak trackers with the corresponding convolution layer features. These weak trackers give the response maps of the target's location. Finally, we propose an ensemble method that coalesces these response maps to get a stronger one. Furthermore, a simple but effective scale estimation strategy is exploited to boost the tracking accuracy. To evaluate the performance of the proposed tracker, we carry out experiments on two thermal infrared tracking benchmarks: VOT-TIR 2015 and VOT-TIR 2016. The experimental results demonstrate that our tracker is effective and achieves promising performance. (C) 2017 Elsevier B.V. All rights reserved.","keywords_author":["Thermal infrared tracking","Convolutional features","Correlation filter","Ensemble method"],"keywords_other":["MODEL","RECOGNITION"],"max_cite":7.0,"pub_year":2017.0,"sources":"['wos']","rawkeys":["ensemble method","correlation filter","recognition","model","convolutional features","thermal infrared tracking"],"tags":["recognition","model","correlation filters","convolutional features","ensemble methods","thermal infrared tracking"]},{"p_id":28261,"title":"Machine learning-based assessment tool for imbalance and vestibular dysfunction with virtual reality rehabilitation system","abstract":"Background and objective: Dizziness is a major consequence of imbalance and vestibular dysfunction. Compared to surgery and drug treatments, balance training is non-invasive and more desired. However, training exercises are usually tedious and the assessment tool is insufficient to diagnose patient's severity rapidly. Methods: An interactive virtual reality (VR) game-based rehabilitation program that adopted Cawthorne-Cooksey exercises, and a sensor-based measuring system were introduced. To verify the therapeutic effect, a clinical experiment with 48 patients and 36 normal subjects was conducted. Quantified balance indices were measured and analyzed by statistical tools and a Support Vector Machine (SVM) classifier. Results: In terms of balance indices, patients who completed the training process are progressed and the difference between normal subjects and patients is obvious. Conclusions: Further analysis by SVM classifier show that the accuracy of recognizing the differences between patients and normal subject is feasible, and these results can be used to evaluate patients' severity and make rapid assessment. \u00a9 2014 Elsevier Ireland Ltd.","keywords_author":["Assessment","Machine learning","Vestibular dysfunction","Virtual reality"],"keywords_other":["Measuring systems","Humans","Therapy, Computer-Assisted","Clinical experiments","Dizziness","Interactive virtual reality","Pattern Recognition, Automated","Artificial Intelligence","Female","Treatment Outcome","Algorithms","Vestibular Diseases","Diagnosis, Computer-Assisted","Rehabilitation programs","Assessment","Male","User-Computer Interface","Middle Aged","Adult","Rehabilitation System","Vestibular dysfunction","Therapeutic effects","Video Games","Biofeedback, Psychology"],"max_cite":5.0,"pub_year":2014.0,"sources":"['scp']","rawkeys":["dizziness","automated","assessment","interactive virtual reality","rehabilitation system","adult","machine learning","middle aged","algorithms","psychology","diagnosis","therapy","therapeutic effects","virtual reality","clinical experiments","humans","user-computer interface","treatment outcome","rehabilitation programs","artificial intelligence","vestibular dysfunction","computer-assisted","male","measuring systems","vestibular diseases","pattern recognition","video games","female","biofeedback"],"tags":["dizziness","measurement system","automated","assessment","interactive virtual reality","rehabilitation system","adult","machine learning","middle aged","algorithms","clinical-experience","diagnosis","therapy","recognition","therapeutic effects","virtual reality","humans","user-computer interface","treatment outcome","rehabilitation programs","vestibular dysfunction","computer-assisted","male","vestibular diseases","pattern recognition","video games","female","biofeedback"]},{"p_id":11879,"title":"Vision-Based Fall Detection with Convolutional Neural Networks","abstract":"One of the biggest challenges in modern societies is the improvement of healthy aging and the support to older persons in their daily activities. In particular, given its social and economic impact, the automatic detection of falls has attracted considerable attention in the computer vision and pattern recognition communities. Although the approaches based on wearable sensors have provided high detection rates, some of the potential users are reluctant to wear them and thus their use is not yet normalized. As a consequence, alternative approaches such as vision-based methods have emerged. We firmly believe that the irruption of the Smart Environments and the Internet of Things paradigms, together with the increasing number of cameras in our daily environment, forms an optimal context for vision-based systems. Consequently, here we propose a vision-based solution using Convolutional Neural Networks to decide if a sequence of frames contains a person falling. To model the video motion and make the system scenario independent, we use optical flow images as input to the networks followed by a novel three-step training phase. Furthermore, our method is evaluated in three public datasets achieving the state-of-the-art results in all three of them.","keywords_author":null,"keywords_other":["RECOGNITION","SENSOR","SYSTEM","OLDER-ADULTS"],"max_cite":1.0,"pub_year":2017.0,"sources":"['ieee', 'wos']","rawkeys":["recognition","older-adults","system","sensor"],"tags":["recognition","older-adults","system","sensors"]},{"p_id":11884,"title":"Multiview Convolutional Neural Networks for Multidocument Extractive Summarization","abstract":"Multidocument summarization has gained popularity in many real world applications because vital information can be extracted within a short time. Extractive summarization aims to generate a summary of a document or a set of documents by ranking sentences and the ranking results rely heavily on the quality of sentence features. However, almost all previous algorithms require hand-crafted features for sentence representation. In this paper, we leverage on word embedding to represent sentences so as to avoid the intensive labor in feature engineering. An enhanced convolutional neural networks (CNNs) termed multiview CNNs is successfully developed to obtain the features of sentences and rank sentences jointly. Multiview learning is incorporated into the model to greatly enhance the learning capability of original CNN. We evaluate the generic summarization performance of our proposed method on five Document Understanding Conference datasets. The proposed system outperforms the state-of-the-art approaches and the improvement is statistically significant shown by paired t-test.","keywords_author":["Convolutional neural networks (CNNs)","deep learning","multidocument summarization (MDS)","multiview learning","word embedding","Convolutional neural networks (CNNs)","deep learning","multidocument summarization (MDS)","multiview learning","word embedding"],"keywords_other":["Multi-view learning","Multi-document summarization","Document understanding","Extractive summarizations","State-of-the-art approach","Learning capabilities","Feature engineerings","Convolutional neural network","RECOGNITION"],"max_cite":1.0,"pub_year":2017.0,"sources":"['ieee', 'scp', 'wos']","rawkeys":["multi-view learning","recognition","deep learning","multi-document summarization","document understanding","multiview learning","multidocument summarization (mds)","word embedding","feature engineerings","convolutional neural network","state-of-the-art approach","learning capabilities","convolutional neural networks (cnns)","extractive summarizations"],"tags":["multi-view learning","recognition","multi-document summarization","machine learning","document understanding","multidocument summarization (mds)","word embedding","feature engineerings","convolutional neural network","state-of-the-art approach","learning capabilities","extractive summarizations"]},{"p_id":11900,"title":"Face Occlusion Detection Using Deep Convolutional Neural Networks","abstract":"With the rise of crimes associated with Automated Teller Machines (ATMs), security reinforcement by surveillance techniques has been a hot topic on the security agenda. As a result, cameras are frequently installed with ATMs, so as to capture the facial images of users. The main objective is to support follow-up criminal investigations in the event of an incident. However, in the case of miss-use, the user's face is often occluded. Therefore, face occlusion detection has become very important to prevent crimes connected with ATM usage. Traditional approaches to solving the problem typically comprise a succession of steps: localization, segmentation, feature extraction and recognition. This paper proposes an end-to-end facial occlusion detection framework, which is robust and effective by combining region proposal algorithm and Convolutional Neural Networks (CNN). The framework utilizes a coarse-to-fine strategy, which consists of two CNNs. The first CNN detects the head element within an upper body image while the second distinguishes which facial part is occluded from the head image. In comparison with previous approaches, the usage of CNN is optimal from a system point of view as the design is based on the end-to-end principle and the model operates directly on image pixels. For evaluation purposes, a face occlusion database consisting of over 50 000 images, with annotated facial parts, was used. Experimental results revealed that the proposed framework is very effective. Using the bespoke face occlusion dataset, Aleix and Robert (AR) face dataset and the Labeled Face in the Wild (LFW) database, we achieved over 85.61%, 97.58% and 100% accuracies for head detection when the Intersection over Union-section (IoU) is larger than 0.5, and 94.55%, 98.58% and 95.41% accuracies for occlusion discrimination, respectively.","keywords_author":["Automated teller machine (ATM)","convolutional neural network (CNN)","face occlusion detection","multi-task learning (MTL)"],"keywords_other":["FEATURES","CASCADE","RECOGNITION","RAPID OBJECT DETECTION","PROPOSALS"],"max_cite":1.0,"pub_year":2016.0,"sources":"['ieee', 'wos']","rawkeys":["rapid object detection","recognition","features","automated teller machine (atm)","convolutional neural network (cnn)","face occlusion detection","multi-task learning (mtl)","proposals","cascade"],"tags":["rapid object detection","atm","recognition","features","face occlusion detection","multitask learning","proposals","convolutional neural network","cascade"]},{"p_id":11901,"title":"Classification of crystallization outcomes using deep convolutional neural networks","abstract":"The Machine Recognition of Crystallization Outcomes (MARCO) initiative has assembled roughly half a million annotated images of macromolecular crystallization experiments from various sources and setups. Here, state-of-the-art machine learning algorithms are trained and tested on different parts of this data set. We find that more than 94% of the test images can be correctly labeled, irrespective of their experimental origin. Because crystal recognition is key to high-density screening and the systematic analysis of crystallization experiments, this approach opens the door to both industrial and fundamental research applications.","keywords_author":null,"keywords_other":["SUITE","PROTEIN-CRYSTALLIZATION","TRIALS","IMAGE CLASSIFICATION","VISUAL ANALYSIS","RECOGNITION","PLATES","TEXTURE","TRAINING SET"],"max_cite":0.0,"pub_year":2018.0,"sources":"['wos']","rawkeys":["suite","recognition","visual analysis","training set","trials","texture","image classification","protein-crystallization","plates"],"tags":["suite","recognition","visual analysis","training sets","connectivity","texture","trial","image classification","protein-crystallization"]},{"p_id":11908,"title":"Inferring low-dimensional microstructure representations using convolutional neural networks","abstract":"We apply recent advances in machine learning and computer vision to a central problem in materials informatics: the statistical representation of microstructural images. We use activations in a pretrained convolutional neural network to provide a high-dimensional characterization of a set of synthetic microstructural images. Next, we usemanifold learning to obtain a low-dimensional embedding of this statistical characterization. We show that the low-dimensional embedding extracts the parameters used to generate the images. According to a variety of metrics, the convolutional neural network method yields dramatically better embeddings than the analogous method derived from two-point correlations alone.","keywords_author":null,"keywords_other":["ELECTRON BACKSCATTER DIFFRACTION","STATISTICS","DATA SCIENCE","CLASSIFICATION","MATERIALS DESIGN","INFORMATICS","RECOGNITION","ALGORITHMS","MICROSCOPY","RECONSTRUCTION"],"max_cite":1.0,"pub_year":2017.0,"sources":"['wos']","rawkeys":["informatics","statistics","recognition","reconstruction","materials design","classification","algorithms","data science","microscopy","electron backscatter diffraction"],"tags":["informatics","statistics","recognition","reconstruction","materials design","classification","algorithms","data science","microscopy","ebsd"]},{"p_id":85639,"title":"Perceptual multi-channel visual feature fusion for scene categorization","abstract":"Effectively recognizing sceneries from a variety of categories is an indispensable but challenging technique in computer vision and intelligent systems. In this work, we propose a novel image kernel based on human gaze shifting, aiming at discovering the mechanism of humans perceiving visually\/semantically salient regions within a scenery. More specifically, we first design a weakly supervised embedding algorithm which projects the local image features (i.e., graphlets in this work) onto the pre-defined semantic space. Thereby, we describe each graphlet by multiple visual features at both low-level and high-level. It is generally acknowledged that humans attend to only a few regions within a scenery. Thus we formulate a sparsity-constrained graphlet ranking algorithm which incorporates visual clues at both the low-level and the high-level. According to human visual perception, these top-ranked graphlets are either visually or semantically salient. We sequentially connect them into a path which mimics human gaze shifting. Lastly, a so-called gaze shifting kernel (GSK) is calculated based on the learned paths from a collection of scene images. And a kernel SVM is employed for calculating the scene categories. Comprehensive experiments on a series of well-known scene image sets shown the competitiveness and robustness of our GSK. We also demonstrated the high consistency of the predicted path with real human gaze shifting path. (C) 2017 Published by Elsevier Inc.","keywords_author":["Image kernel","Feature fusion","Scene categoriztion","Perception"],"keywords_other":["PREFERENCE RELATIONS","SUPPORT VECTOR MACHINE","NETWORKS","FRAMEWORK","CLASSIFICATION","MODEL","RECOGNITION"],"max_cite":0.0,"pub_year":2018.0,"sources":"['wos']","rawkeys":["recognition","model","framework","networks","perception","classification","image kernel","preference relations","support vector machine","feature fusion","scene categoriztion"],"tags":["recognition","model","framework","machine learning","networks","classification","image kernel","perceptions","preference relations","feature fusion","scene categoriztion"]},{"p_id":3731,"title":"Ear verification under uncontrolled conditions with convolutional neural networks","abstract":"The capabilities of biometric systems have recently made extraordinary leaps by the emergence of deep learning. However, due to the lack of enough training data, the applications of the deep neural network in the ear recognition filed have run into the bottleneck. Moreover, the effect of fine-tuning from some pre-trained models is far less than expected due to the diversity among different tasks. Therefore, the authors propose a large-scale ear database and explore the robust convolutional neural network (CNN) architecture for the ear feature representation. The images in this USTB-Helloear database were taken under uncontrolled conditions with illumination, pose variation and different level of ear occlusions. Then they fine-tuned and modified some deep models on the proposed database through the ear verification experiments. First, they replaced the last pooling layers by spatial pyramid pooling layers to fit arbitrary data size and obtain multi-level features. In the training phase, the CNNs were trained both under the supervision of the softmax loss and centre loss to obtain more compact and discriminative features to identify unseen ears. Finally, three CNNs with different scales of ear images were assembled as the multi-scale ear representations for ear verification. The experimental results demonstrate the effectiveness of the proposed modified CNN deep model.","keywords_author":["feature extraction","biometrics (access control)","image representation","ear","image recognition","convolution","learning (artificial intelligence)","feedforward neural nets","biometric systems","deep learning","deep neural network","ear recognition","large-scale ear database","ear feature representation","USTB-Helloear database","ear occlusions","spatial pyramid pooling layers","arbitrary data size","multilevel features","softmax loss","discriminative features","unseen ears","multiscale ear representations","convolutional neural network architecture","ear verification","CNN deep model"],"keywords_other":["FUSION","RECOGNITION","REPRESENTATION","FACE"],"max_cite":0.0,"pub_year":2018.0,"sources":"['ieee', 'wos']","rawkeys":["ear occlusions","softmax loss","unseen ears","convolutional neural network architecture","large-scale ear database","convolution","discriminative features","arbitrary data size","biometrics (access control)","fusion","ear feature representation","feedforward neural nets","image representation","ear verification","learning (artificial intelligence)","ear recognition","recognition","deep learning","ustb-helloear database","multilevel features","face","deep neural network","spatial pyramid pooling layers","representation","cnn deep model","feature extraction","image recognition","biometric systems","ear","multiscale ear representations"],"tags":["ear occlusions","softmax loss","convolutional neural network","unseen ears","convolutional neural network architecture","large-scale ear database","convolution","machine learning","discriminative features","arbitrary data size","fusion","ear feature representation","feedforward neural nets","image representation","ear verification","recognition","ear recognition","ustb-helloear database","biometrics","multilevel features","face","spatial pyramid pooling layers","representation","cnn deep model","feature extraction","image recognition","biometric systems","ear","multiscale ear representations"]},{"p_id":3734,"title":"Single Infrared Image Optical Noise Removal Using a Deep Convolutional Neural Network","abstract":"In this paper, we propose a deep learning method for single infrared image optical noise removal. With a fully convolutional neural network, it is able to eliminate the optical noise in single infrared image. Our architecture consists of two networks: a denoising network and a conditional discriminator. The denoising network takes a noise image as input and outputs a denoising result, while the discriminator tries to make the output look more like the target. Actually, only in the testing phase, this method is feed-forward. Significant image quality in experiments is achieved compared with the existing method.","keywords_author":["Deep learning","infrared imaging","optical noise","Deep learning","infrared imaging","optical noise"],"keywords_other":["STATISTICS","Learning methods","De-noising","NONUNIFORMITY CORRECTION","Input and outputs","FOCAL-PLANE ARRAYS","QUALITY ASSESSMENT","RECOGNITION","Feed forward","Convolutional neural network","Optical noise","Noise image","Testing phase","SENSORS"],"max_cite":0.0,"pub_year":2018.0,"sources":"['scp', 'wos', 'ieee']","rawkeys":["statistics","feed forward","infrared imaging","recognition","sensors","deep learning","learning methods","quality assessment","input and outputs","convolutional neural network","focal-plane arrays","optical noise","nonuniformity correction","testing phase","de-noising","noise image"],"tags":["statistics","infrared imaging","recognition","sensors","learning methods","machine learning","information retrieval","noise image","input and outputs","convolutional neural network","optical noise","nonuniformity correction","testing phase","de-noising","focal-plane-arrays","feed-forward"]},{"p_id":11933,"title":"Finding strong lenses in CFHTLS using convolutional neural networks","abstract":"We train and apply convolutional neural networks, a machine learning technique developed to learn from and classify image data, to Canada-France-Hawaii Telescope Legacy Survey (CFHTLS) imaging for the identification of potential strong lensing systems. An ensemble of four convolutional neural networks was trained on images of simulated galaxy-galaxy lenses. The training sets consisted of a total of 62 406 simulated lenses and 64 673 nonlens negative examples generated with two different methodologies. An ensemble of trained networks was applied to all of the 171 deg(2) of the CFHTLS wide field image data, identifying 18 861 candidates including 63 known and 139 other potential lens candidates. A second search of 1.4 million early-type galaxies selected from the survey catalogue as potential deflectors, identified 2465 candidates including 117 previously known lens candidates, 29 confirmed lenses\/high-quality lens candidates, 266 novel probable or potential lenses and 2097 candidates we classify as false positives. For the catalogue-based search we estimate a completeness of 21-28 per cent with respect to detectable lenses and a purity of 15 per cent, with a false-positive rate of 1 in 671 images tested. We predict a human astronomer reviewing candidates produced by the system would identify 20 probable lenses and 100 possible lenses per hour in a sample selected by the robot. Convolutional neural networks are therefore a promising tool for use in the search for lenses in current and forthcoming surveys such as the Dark Energy Survey and the Large Synoptic Survey Telescope.","keywords_author":["gravitational lensing: strong","methods: statistical"],"keywords_other":["PLANE","CANDIDATES","SEARCH","SPACE WARPS","GRAVITATIONAL LENS","DIGITAL SKY SURVEY","SAMPLE","RECOGNITION","EARLY-TYPE GALAXIES","STELLAR"],"max_cite":5.0,"pub_year":2017.0,"sources":"['wos']","rawkeys":["methods: statistical","gravitational lensing: strong","recognition","search","early-type galaxies","candidates","stellar","gravitational lens","plane","sample","space warps","digital sky survey"],"tags":["methods: statistical","gravitational lensing: strong","recognition","search","early-type galaxies","sampling","candidates","stellar","gravitational lens","plane","space warps","digital sky survey"]},{"p_id":11934,"title":"Intelligent monitor system based on cloud and convolutional neural networks","abstract":"Nowadays, cloud-based services are widely developed. The deployment of cloud technology has boosted the development and application of web services. It reduces the overhead of software virtual machine, and supports a wider range of operating systems. Moreover, it enhances the utilization of infrastructure. With the development of artificial intelligence (AI) technology, especially artificial neural network (ANN), intelligent monitor systems are being raised and developed in our daily life. However, a simple task with a single ANN costs a lot of time and computation resources. Hence, we propose using a cloud-based system to share computation resources for ANN to reduce redundant computation. In this paper, we present an intelligent monitor system, which is based on cloud technology, to provide intelligent monitor services. The system is designed with hybrid convolutional neural networks. It has been used for several intelligent monitor tasks, such as scene change detection, stranger recognition, facial expression recognition and action recognition.","keywords_author":["Cloud computing","Artificial neural network","Intelligent monitor system","Convolutional neural network"],"keywords_other":["RECOGNITION"],"max_cite":0.0,"pub_year":2016.0,"sources":"['ieee', 'wos']","rawkeys":["intelligent monitor system","recognition","cloud computing","convolutional neural network","artificial neural network"],"tags":["recognition","cloud computing","neural networks","intelligent monitoring systems","convolutional neural network"]},{"p_id":20138,"title":"Evaluation of a smartphone human activity recognition application with able-bodied and stroke participants","abstract":"\u00a9 2016 Capela et al. Background: Mobile health monitoring using wearable sensors is a growing area of interest. As the world's population ages and locomotor capabilities decrease, the ability to report on a person's mobility activities outside a hospital setting becomes a valuable tool for clinical decision-making and evaluating healthcare interventions. Smartphones are omnipresent in society and offer convenient and suitable sensors for mobility monitoring applications. To enhance our understanding of human activity recognition (HAR) system performance for able-bodied and populations with gait deviations, this research evaluated a custom smartphone-based HAR classifier on fifteen able-bodied participants and fifteen participants who suffered a stroke. Methods: Participants performed a consecutive series of mobility tasks and daily living activities while wearing a BlackBerry Z10 smartphone on their waist to collect accelerometer and gyroscope data. Five features were derived from the sensor data and used to classify participant activities (decision tree). Sensitivity, specificity and F-scores were calculated to evaluate HAR classifier performance. Results: The classifier performed well for both populations when differentiating mobile from immobile states (F-score > 94 %). As activity recognition complexity increased, HAR system sensitivity and specificity decreased for the stroke population, particularly when using information derived from participant posture to make classification decisions. Conclusions: Human activity recognition using a smartphone based system can be accomplished for both able-bodied and stroke populations; however, an increase in activity classification complexity leads to a decrease in HAR performance with a stroke population. The study results can be used to guide smartphone HAR system development for populations with differing movement characteristics.","keywords_author":["Accelerometry\/instrumentation","Activities of Daily Living","Ambulatory\/instrumentation","Cellular Phone","Monitoring","Movement"],"keywords_other":["Male","Mobile Applications","Activities of Daily Living","Young Adult","Humans","Accelerometry","Aged","Middle Aged","Adult","Recognition (Psychology)","Smartphone","Motor Activity","Stroke","Stroke Rehabilitation","Monitoring, Ambulatory","Female"],"max_cite":18.0,"pub_year":2016.0,"sources":"['scp']","rawkeys":["ambulatory","movement","aged","young adult","adult","monitoring","ambulatory\/instrumentation","recognition (psychology)","mobile applications","middle aged","stroke","smartphone","humans","motor activity","activities of daily living","stroke rehabilitation","accelerometry\/instrumentation","accelerometry","male","cellular phone","female"],"tags":["ambulatory","movement","aged","young adult","adult","monitoring","ambulatory\/instrumentation","mobile applications","middle aged","stroke","recognition","smartphone","humans","motor activity","activities of daily living","stroke rehabilitation","accelerometry\/instrumentation","accelerometry","male","cellular phone","female"]},{"p_id":11950,"title":"3D object retrieval based on multi-view convolutional neural networks","abstract":"Recently, 3D objects have been widely designed and applied in various technical applications. In this paper, we propose a novel 3D model retrieval method based on Multi-View Convolutional Neural Networks (MVCNN). By integrating visual information from multiple views, we construct a composite CNN structure to generate single terse descriptor with powerful discrimination for individual 3D object. Our method can benefit from the hidden relevance of visual information in deep structure. Instead of computing similarities between each pair of view-feature, we only need to measure the comparability of two object once, which brings high efficiency. Moreover, this method can avoid camera constraint when capturing multi-view representation. Extensive experiments on NTU and ITI datasets can support the superiority of the proposed method.","keywords_author":["3D object retrieval","CNN","Deep learning","Multi-view","3D object retrieval","Multi-view","CNN","Deep learning"],"keywords_other":["3D model retrieval methods","MODEL RETRIEVAL","Multi-views","Visual information","SYSTEM","Deep structure","Technical applications","High-efficiency","RECOGNITION","Convolutional neural network","3D object retrieval"],"max_cite":1.0,"pub_year":2017.0,"sources":"['ieee', 'scp', 'wos']","rawkeys":["3d model retrieval methods","technical applications","recognition","high-efficiency","visual information","deep learning","model retrieval","system","cnn","3d object retrieval","deep structure","convolutional neural network","multi-view","multi-views"],"tags":["3d model retrieval methods","technical applications","recognition","high-efficiency","visual information","model retrieval","machine learning","system","3d object retrieval","deep structure","convolutional neural network","multi-views"]},{"p_id":11962,"title":"Designing architectures of convolutional neural networks to solve practical problems","abstract":"The Convolutional Neural Network (CNN) figures among the state-of-the-art Deep Learning (DL) algorithms due to its robustness to support data shift, scale variations, and its capability of extracting relevant information from large-scale input data. However, setting appropriate parameters to define CNN architectures is still a challenging issue, mainly to tackle real-world problems. A typical approach consists in empirically assessing different CNN settings in order to select the most appropriate one. This procedure has clear limitations, including the choice of suitable predefined configurations as well as the high computational cost involved in evaluating each of them. This work presents a novel methodology to tackle the previously mentioned issues, providing mechanisms to estimate effective CNN configurations, including the size of convolutional masks (convolutional kernels) and the number of convolutional units (CNN neurons) per layer. Based on the False Nearest Neighbors (FNN), a well-known tool from the area of Dynamical Systems, the proposed method helps estimating CNN architectures that are less complex and produce good results. Our experiments confirm that architectures estimated through the proposed approach are as effective as the complex ones defined by empirical and computationally intensive strategies. (C) 2017 Elsevier Ltd. All rights reserved.","keywords_author":["Convolutional neural network","Architecture assessment","Dynamical systems","Handwritten digit recognition","Face recognition","Object recognition"],"keywords_other":["RECOGNITION","MODELS"],"max_cite":0.0,"pub_year":2018.0,"sources":"['wos']","rawkeys":["recognition","handwritten digit recognition","object recognition","dynamical systems","architecture assessment","face recognition","convolutional neural network","models"],"tags":["recognition","model","handwritten digit recognition","object recognition","dynamical systems","architecture assessment","face recognition","convolutional neural network"]},{"p_id":118460,"title":"Cardiac arrhythmia classification using the phase space sorted by Poincare sections","abstract":"Many methods for automatic heartbeat classification have been applied and reported in literature, but methods, which used the basin geometry of quasi-periodic oscillations of electrocardiogram (ECG) signal in the phase space for classifying cardiac arrhythmias, frequently extracted a limited amount of information of this geometry. Therefore, in this study, we proposed a novel technique based on Poincare section to quantify the basin of quasi-periodic oscillations, which can fill the mentioned gap to some extent. For this purpose, we first reconstructed the two-dimensional phase space of ECG signal. Then, we sorted this space using the Poincare sections in different angles. Finally, we evaluated the geometric features extracted from the sorted spaces of five heartbeat groups recommend by the association for the advancement of medical instrumentation (AAMI) by using the sequential forward selection (SFS) algorithm. The results of this algorithm indicated that a combination of nine features extracted from the sorted phase space along with per and post instantaneous heart rate could significantly separate the five heartbeat groups (99.23% and 96.07% for training and testing sets, respectively). Comparing these results with the results of earlier work also indicated that our proposed method had a figure of merit (FOM) about 32.12%. Therefore, this new technique not only can quantify the basin geometry of quasiperiodic oscillations of ECG signal in the phase space, but also its output can improve the performance of detection systems developed for the cardiac arrhythmias, especially in the five heartbeat groups recommend by the AAMI. (C) 2017 Nalecz Institute of Biocybernetics and Biomedical Engineering of the Polish Academy of Sciences. Published by Elsevier B.V. All rights reserved.","keywords_author":["Phase space","Poincare section","ECG","Cardiac arrhythmia"],"keywords_other":["WAVELET TRANSFORM","MORPHOLOGY","FRAMEWORK","FEATURE-EXTRACTION","RECOGNITION","DYNAMIC FEATURES","HEARTBEAT INTERVAL FEATURES","NEURAL-NETWORK","ECG SIGNALS","BEAT CLASSIFICATION"],"max_cite":1.0,"pub_year":2017.0,"sources":"['wos']","rawkeys":["wavelet transform","recognition","ecg","beat classification","cardiac arrhythmia","dynamic features","feature-extraction","framework","morphology","poincare section","ecg signals","heartbeat interval features","neural-network","phase space"],"tags":["wavelet transform","recognition","ecg","beat classification","cardiac arrhythmia","dynamic features","framework","neural networks","morphology","poincare section","ecg signals","heartbeat interval features","feature extraction","phase space"]},{"p_id":36542,"title":"Bacterial promoter prediction: Selection of dynamic and static physical properties of DNA for reliable sequence classification","abstract":"\u00a9 2018 World Scientific Publishing Europe Ltd. Predicting promoter activity of DNA fragment is an important task for computational biology. Approaches using physical properties of DNA to predict bacterial promoters have recently gained a lot of attention. To select an adequate set of physical properties for training a classifier, various characteristics of DNA molecule should be taken into consideration. Here, we present a systematic approach that allows us to select less correlated properties for classification by means of both correlation and cophenetic coefficients as well as concordance matrices. To prove this concept, we have developed the first classifier that uses not only sequence and static physical properties of DNA fragment, but also dynamic properties of DNA open states. Therefore, the best performing models with accuracy values up to 90% for all types of sequences were obtained. Furthermore, we have demonstrated that the classifier can serve as a reliable tool enabling promoter DNA fragments to be distinguished from promoter islands despite the similarity of their nucleotide sequences.","keywords_author":["DNA physical properties","Machine learning","promoter recognition","Machine learning","promoter recognition","DNA physical properties"],"keywords_other":["PROFILES","ELECTROSTATIC POTENTIALS","DATA SETS","GENOMES","CORRELATION-COEFFICIENT","RECOGNITION","REGIONS","ARABIDOPSIS","ANNOTATION","COLI RNA-POLYMERASE"],"max_cite":1.0,"pub_year":2018.0,"sources":"['scp', 'wos']","rawkeys":["recognition","machine learning","electrostatic potentials","data sets","regions","correlation-coefficient","promoter recognition","genomes","arabidopsis","annotation","profiles","coli rna-polymerase","dna physical properties"],"tags":["cloud computing","recognition","machine learning","electrostatic potential","data sets","regions","promoter recognition","arabidopsis","annotation","profiles","coli rna-polymerase","dna physical properties","genomics"]},{"p_id":11968,"title":"Tire Defects Classification with Multi-Contrast Convolutional Neural Networks","abstract":"The objective of this study is to improve the accuracy in tire defect classification with limited training samples under varying illuminations. We investigate an algorithm based on deep learning to achieve high accuracy with limited samples. First, image contrast normalizations and data augmentation were used to avoid overfitting problems of the network with a large number of parameters. Furthermore, multi-column CNN is proposed by combining several CNNs trained on differently preprocessed data into a multi-column CNN (MC-CNN), and then their predictions are averaged as the output of the proposed network. An average accuracy of 98.47% is achieved with the proposed CNN-based method. Experimental results show that our scheme receives satisfactory classification accuracy and outperforms state-of-the-art methods on the same tire defect dataset.","keywords_author":["CNN","Deep learning","multi-contrast","object classification","tire defects","Deep learning","object classification","CNN","multi-contrast","tire defects"],"keywords_other":["RECOGNITION","TRANSFORM"],"max_cite":2.0,"pub_year":2018.0,"sources":"['ieee', 'scp', 'wos']","rawkeys":["recognition","deep learning","cnn","object classification","tire defects","multi-contrast","transform"],"tags":["recognition","machine learning","object classification","convolutional neural network","tire defects","multi-contrast","transform"]},{"p_id":3776,"title":"Improvement of Generalization Ability of Deep CNN via Implicit Regularization in Two-Stage Training Process","abstract":"Optimization of deep learning is no longer an imminent problem, due to various gradient descent methods and the improvements of network structure, including activation functions, the connectivity style, and so on. Then the actual application depends on the generalization ability, which determines whether a network is effective. Regularization is an efficient way to improve the generalization ability of deep CNN, because it makes it possible to train more complex models while maintaining a lower overfitting. In this paper, we propose to optimize the feature boundary of deep CNN through a two-stage training method (pre-training process and implicit regularization training process) to reduce the overfitting problem. In the pre-training stage, we train a network model to extract the image representation for anomaly detection. In the implicit regularization training stage, we re-train the network based on the anomaly detection results to regularize the feature boundary and make it converge in the proper position. Experimental results on five image classification benchmarks show that the two-stage training method achieves a state-of-the-art performance and that it, in conjunction with more complicated anomaly detection algorithm, obtains better results. Finally, we use a variety of strategies to explore and analyze how implicit regularization plays a role in the two-stage training process. Furthermore, we explain how implicit regularization can be interpreted as data augmentation and model ensemble.","keywords_author":["Deep CNN","image classification","overfitting","generalization","anomaly detection","implicit regularization"],"keywords_other":["NETWORKS","RECOGNITION","SUPPORT","PCA"],"max_cite":0.0,"pub_year":2018.0,"sources":"['wos', 'ieee']","rawkeys":["deep cnn","recognition","pca","anomaly detection","support","implicit regularization","generalization","networks","overfitting","image classification"],"tags":["principal component analysis","recognition","deep cnn","support","anomaly detection","implicit regularization","networks","overfitting","image classification"]},{"p_id":11970,"title":"Invariant moments based convolutional neural networks for image analysis","abstract":"The paper proposes a method using convolutional neural network to effectively evaluate the discrimination between face and non face patterns, gender classification using facial images and facial expression recognition. The novelty of the method lies in the utilization of the initial trainable convolution kernels coefficients derived from the zernike moments by varying the moment order. The performance of the proposed method was compared with the convolutional neural network architecture that used random kernels as initial training parameters. The multilevel configuration of zernike moments was significant in extracting the shape information suitable for hierarchical feature learning to carry out image analysis and classification. Furthermore the results showed an outstanding performance of zernike moment based kernels in terms of the computation time and classification accuracy.","keywords_author":["Zernike moments","convolution kernel","invariant moments","pattern recognition","hierarchical feature learning"],"keywords_other":["FEATURES","CLASSIFICATION","COMPONENT ANALYSIS","RECOGNITION","ZERNIKE MOMENTS"],"max_cite":0.0,"pub_year":2017.0,"sources":"['wos']","rawkeys":["convolution kernel","invariant moments","recognition","features","zernike moments","hierarchical feature learning","classification","component analysis","pattern recognition"],"tags":["convolution kernel","recognition","features","zernike moments","hierarchical feature learning","classification","invariant moment","component analysis","pattern recognition"]},{"p_id":11973,"title":"Face detection using convolutional neural networks and gabor filters","abstract":"This paper proposes a method for detecting facial regions by combining a Gabor filter and a convolutional neural network. The first stage uses the Gabor filter which extracts intrinsic facial features. As a result of this transformation we obtain four subimages. The second stage of the method concerns the application of the convolutional neural network to these four images. The approach presented in this paper yields better classification performance in comparison to the results obtained by the convolutional neural network alone.","keywords_author":null,"keywords_other":["Gabor filters","Classifications","RECOGNITION","Convolutional neural networks","Facial features"],"max_cite":20.0,"pub_year":2005.0,"sources":"['ieee', 'scp', 'wos']","rawkeys":["classifications","recognition","convolutional neural networks","gabor filters","facial features"],"tags":["facial feature","gabor filter","recognition","classification","convolutional neural network"]},{"p_id":3782,"title":"Threat of Adversarial Attacks on Deep Learning in Computer Vision: A Survey","abstract":"Deep learning is at the heart of the current rise of artificial intelligence. In the field of computer vision, it has become the workhorse for applications ranging from self-driving cars to surveillance and security. Whereas, deep neural networks have demonstrated phenomenal success (often beyond human capabilities) in solving complex problems, recent studies show that they are vulnerable to adversarial attacks in the form of subtle perturbations to inputs that lead a model to predict incorrect outputs. For images, such perturbations are often too small to be perceptible, yet they completely fool the deep learning models. Adversarial attacks pose a serious threat to the success of deep learning in practice. This fact has recently led to a large influx of contributions in this direction. This paper presents the first comprehensive survey on adversarial attacks on deep learning in computer vision. We review the works that design adversarial attacks, analyze the existence of such attacks and propose defenses against them. To emphasize that adversarial attacks are possible in practical conditions, we separately review the contributions that evaluate adversarial attacks in the real-world scenarios. Finally, drawing on the reviewed literature, we provide a broader outlook of this research direction.","keywords_author":["adversarial learning","adversarial perturbation","black-box attack","Deep learning","perturbation detection","white-box attack","Deep learning","adversarial perturbation","black-box attack","white-box attack","adversarial learning","perturbation detection"],"keywords_other":["White box","Predictive models","Perturbation detection","Computational model","adversarial perturbation","NEURAL-NETWORKS","RECOGNITION","Adversarial learning","Perturbation method","BACKPROPAGATION","Task analysis","Black boxes"],"max_cite":0.0,"pub_year":2018.0,"sources":"['scp', 'wos', 'ieee']","rawkeys":["black-box attack","neural-networks","adversarial learning","perturbation method","recognition","task analysis","deep learning","predictive models","black boxes","adversarial perturbation","perturbation detection","white box","computational model","white-box attack","backpropagation"],"tags":["black-box attack","computational modeling","adversarial learning","task analysis","recognition","neural networks","predictive models","black boxes","machine learning","adversarial perturbation","perturbation detection","white box","perturbation methods","white-box attack","backpropagation"]},{"p_id":11978,"title":"Convolutional Recurrent Neural Networks for Polyphonic Sound Event Detection","abstract":"Sound events often occur in unstructured environments where they exhibit wide variations in their frequency content and temporal structure. Convolutional neural networks (CNNs) are able to extract higher level features that are invariant to local spectral and temporal variations. Recurrent neural networks (RNNs) are powerful in learning the longer term temporal context in the audio signals. CNNs and RNNs as classifiers have recently shown improved performances over established methods in various sound recognition tasks. We combine these two approaches in a convolutional recurrent neural network (CRNN) and apply it on a polyphonic sound event detection task. We compare the performance of the proposed CRNN method with CNN, RNN, and other established methods, and observe a considerable improvement for four different datasets consisting of everyday sound events.","keywords_author":["Convolutional neural networks (CNNs)","deep neural networks","recurrent neural networks (RNNs)","sound event detection","Convolutional neural networks (CNNs)","deep neural networks","recurrent neural networks (RNNs)","sound event detection"],"keywords_other":["Temporal variation","Recurrent neural network (RNNs)","Frequency contents","Temporal structures","RECOGNITION","Sound event detection","Convolutional neural network","Unstructured environments","Sound recognition"],"max_cite":14.0,"pub_year":2017.0,"sources":"['ieee', 'scp', 'wos']","rawkeys":["recurrent neural network (rnns)","recurrent neural networks (rnns)","recognition","sound recognition","deep neural networks","temporal structures","convolutional neural network","frequency contents","convolutional neural networks (cnns)","unstructured environments","temporal variation","sound event detection"],"tags":["recognition","sound recognition","neural networks","temporal structures","frequency contents","convolutional neural network","unstructured environments","temporal variation","sound event detection"]},{"p_id":11985,"title":"Feature-Fused SAR Target Discrimination Using Multiple Convolutional Neural Networks","abstract":"Target discrimination has been one of the hottest issues in the interpretation of synthetic aperture radar (SAR) images. However, the presence of speckle noise and the absence of robust features make SAR discrimination difficult to deal with. Recently, convolutional neural network has obtained state-of-the-art results in pattern recognition. In this letter, we propose a target discrimination framework that jointly uses intensity and edge information of SAR images. This framework contains three parts, namely, feature extraction block, feature fusion block, and final classification block. In addition, a novel feature fusion method that can preserve the spatial relationship of different features is introduced. Experimental results on the miniSAR data demonstrate the effectiveness of our method.","keywords_author":["Convolutional neural network (CNN)","feature fusion","synthetic aperture radar (SAR)","target discrimination"],"keywords_other":["RECOGNITION"],"max_cite":0.0,"pub_year":2017.0,"sources":"['ieee', 'wos']","rawkeys":["recognition","synthetic aperture radar (sar)","feature fusion","convolutional neural network (cnn)","target discrimination"],"tags":["recognition","synthetic aperture radar","feature fusion","convolutional neural network","target discrimination"]},{"p_id":11986,"title":"Automatic detection and classification of leukocytes using convolutional neural networks","abstract":"The detection and classification of white blood cells (WBCs, also known as Leukocytes) is a hot issue because of its important applications in disease diagnosis. Nowadays the morphological analysis of blood cells is operated manually by skilled operators, which results in some drawbacks such as slowness of the analysis, a non-standard accuracy, and the dependence on the operator's skills. Although there have been many papers studying the detection of WBCs or classification of WBCs independently, few papers consider them together. This paper proposes an automatic detection and classification system for WBCs from peripheral blood images. It firstly proposes an algorithm to detect WBCs from the microscope images based on the simple relation of colors R, B and morphological operation. Then a granularity feature (pairwise rotation invariant co-occurrence local binary pattern, PRICoLBP feature) and SVM are applied to classify eosinophil and basophil from other WBCs firstly. Lastly, convolution neural networks are used to extract features in high level from WBCs automatically, and a random forest is applied to these features to recognize the other three kinds of WBCs: neutrophil, monocyte and lymphocyte. Some detection experiments on Cellavison database and ALL-IDB database show that our proposed detection method has better effect almost than iterative threshold method with less cost time, and some classification experiments show that our proposed classification method has better accuracy almost than some other methods.","keywords_author":["White blood cell","Detection","Classification","Convolutional neural networks","Random forest"],"keywords_other":["RECOGNITION","BLOOD-CELL SEGMENTATION","IMAGE"],"max_cite":1.0,"pub_year":2017.0,"sources":"['wos']","rawkeys":["blood-cell segmentation","recognition","convolutional neural networks","image","detection","classification","random forest","white blood cell"],"tags":["blood-cell segmentation","recognition","images","white blood cells","random forests","detection","classification","convolutional neural network"]},{"p_id":11989,"title":"Rank-based pooling for deep convolutional neural networks","abstract":"Pooling is a key mechanism in deep convolutional neural networks (CNNs) which helps to achieve translation invariance. Numerous studies, both empirically and theoretically, show that pooling consistently boosts the performance of the CNNs. The conventional pooling methods are operated on activation values. In this work, we alternatively propose rank-based pooling. It is derived from the observations that ranking list is invariant under changes of activation values in a pooling region, and thus rank-based pooling operation may achieve more robust performance. In addition, the reasonable usage of rank can avoid the scale problems encountered by value-based methods. The novel pooling mechanism can be regarded as an instance of weighted pooling where a weighted sum of activations is used to generate the pooling output. This pooling mechanism can also be realized as rank-based average pooling (RAP), rank-based weighted pooling (RWP) and rank-based stochastic pooling (RSP) according to different weighting strategies. As another major contribution, we present a novel criterion to analyze the discriminant ability of various pooling methods, which is heavily under-researched in machine learning and computer vision community. Experimental results on several image benchmarks show that rank-based pooling outperforms the existing pooling methods in classification performance. We further demonstrate better performance on CIFAR datasets by integrating RSP into Network-in-Network. (C) 2016 Elsevier Ltd. All rights reserved.","keywords_author":["Convolutional neural network","Deep learning","Image classification","Pooling","Pooling","Deep learning","Image classification","Convolutional neural network"],"keywords_other":["Deep learning","Classification performance","Vision communities","Neural Networks (Computer)","Translation invariance","Weighting strategies","Machine Learning","RECOGNITION","Robust performance","Convolutional neural network","CORTEX","Pooling"],"max_cite":6.0,"pub_year":2016.0,"sources":"['ieee', 'scp', 'wos']","rawkeys":["vision communities","neural networks (computer)","classification performance","recognition","translation invariance","deep learning","weighting strategies","machine learning","convolutional neural network","robust performance","pooling","cortex","image classification"],"tags":["vision communities","recognition","classification performance","translation invariance","neural networks","weighting strategies","machine learning","convolutional neural network","robust performance","pooling","cortex","image classification"]},{"p_id":36571,"title":"Facial feature point detection: A comprehensive survey","abstract":"\u00a9 2017 Elsevier B.V.This paper presents a comprehensive survey of facial feature point detection with the assistance of abundant manually labeled images. Facial feature point detection favors many applications such as face recognition, animation, tracking, hallucination, expression analysis and 3D face modeling. Existing methods are categorized into two primary categories according to whether there is the need of a parametric shape model: parametric shape model-based methods and nonparametric shape model-based methods. Parametric shape model-based methods are further divided into two secondary classes according to their appearance models: local part model-based methods (e.g. constrained local model) and holistic model-based methods (e.g. active appearance model). Nonparametric shape model-based methods are divided into several groups according to their model construction process: exemplar-based methods, graphical model-based methods, cascaded regression-based methods, and deep learning based methods. Though significant progress has been made, facial feature point detection is still limited in its success by wild and real-world conditions: large variations across poses, expressions, illuminations, and occlusions. A comparative illustration and analysis of representative methods provides us a holistic understanding and deep insight into facial feature point detection, which also motivates us to further explore more promising future schemes.","keywords_author":["Deep learning","Face alignment","Facial feature point detection","Facial landmark localization","Deep learning","Face alignment","Facial feature point detection","Facial landmark localization"],"keywords_other":["Learning-based methods","FACE ALIGNMENT","Face alignment","SHAPE MODELS","REGRESSION","LANDMARK LOCALIZATION","VIEW","Facial feature point detection","REPRESENTATION","Facial landmark","ALGORITHM","Constrained local models","FEATURE TRACKING","ACTIVE APPEARANCE MODELS","RECOGNITION","Expression analysis","Exemplar based methods","Active appearance models"],"max_cite":1.0,"pub_year":2018.0,"sources":"['ieee', 'scp', 'wos']","rawkeys":["active appearance models","algorithm","face alignment","facial feature point detection","facial landmark","feature tracking","landmark localization","learning-based methods","deep learning","recognition","shape models","representation","expression analysis","view","constrained local models","regression","exemplar based methods","facial landmark localization"],"tags":["active appearance models","facial feature point detection","face alignment","facial landmark","feature tracking","landmark localization","learning-based methods","recognition","shape model","views","machine learning","expression analysis","representation","algorithms","constrained local models","regression","exemplar based methods","facial landmark localization"]},{"p_id":11996,"title":"Macular OCT Classification Using a Multi-Scale Convolutional Neural Network Ensemble","abstract":"Computer-aided diagnosis (CAD) of retinal pathologies is a current active area in medical image analysis. Due to the increasing use of retinal optical coherence tomography (OCT) imaging technique, a CAD system in retinal OCT is essential to assist ophthalmologist in the early detection of ocular diseases and treatment monitoring. This paper presents a novel CAD system based on a multi-scale convolutional mixture of expert (MCME) ensemble model to identify normal retina, and two common types of macular pathologies, namely, dry age-related macular degeneration, and diabetic macular edema. The proposed MCME modular model is a data-driven neural structure, which employs a new cost function for discriminative and fast learning of image features by applying convolutional neural networks on multiple-scale sub-images. MCME maximizes the likelihood function of the training data set and ground truth by considering a mixture model, which tries also to model the joint interaction between individual experts by using a correlated multivariate component for each expert module instead of only modeling the marginal distributions by independent Gaussian components. Two different macular OCT data sets from Heidelberg devices were considered for the evaluation of the method, i.e., a local data set of OCT images of 148 subjects and a public data set of 45 OCT acquisitions. For comparison purpose, we performed a wide range of classification measures to compare the results with the best configurations of the MCME method. With the MCME model of four scale-dependent experts, the precision rate of 98.86%, and the area under the receiver operating characteristic curve (AUC) of 0.9985 were obtained on average.","keywords_author":["CAD system","classification","macular pathology","Multi-scale Convolutional Mixture of Experts (MCME)","Optical Coherence Tomography (OCT)"],"keywords_other":["DIAGNOSIS","LAYER SEGMENTATION","OPTICAL COHERENCE TOMOGRAPHY","DEGENERATION","ARCHITECTURES","BURDEN","EXPERTS","RECOGNITION","MIXTURE","IMAGES"],"max_cite":1.0,"pub_year":2018.0,"sources":"['ieee', 'wos']","rawkeys":["diagnosis","optical coherence tomography","images","architectures","macular pathology","recognition","degeneration","experts","burden","classification","mixture","multi-scale convolutional mixture of experts (mcme)","cad system","optical coherence tomography (oct)","layer segmentation"],"tags":["diagnosis","optical coherence tomography","images","macular pathology","recognition","degeneration","experts","burden","classification","mixture","multi-scale convolutional mixture of experts (mcme)","cad system","architecture","layer segmentation"]},{"p_id":77531,"title":"Comparative analysis of image classification methods for automatic diagnosis of ophthalmic images","abstract":"There are many image classification methods, but it remains unclear which methods are most helpful for analyzing and intelligently identifying ophthalmic images. We select representative slit-lamp images which show the complexity of ocular images as research material to compare image classification algorithms for diagnosing ophthalmic diseases. To facilitate this study, some feature extraction algorithms and classifiers are combined to automatic diagnose pediatric cataract with same dataset and then their performance are compared using multiple criteria. This comparative study reveals the general characteristics of the existing methods for automatic identification of ophthalmic images and provides new insights into the strengths and shortcomings of these methods. The relevant methods (local binary pattern + SVMs, wavelet transformation + SVMs) which achieve an average accuracy of 87% and can be adopted in specific situations to aid doctors in preliminarily disease screening. Furthermore, some methods requiring fewer computational resources and less time could be applied in remote places or mobile devices to assist individuals in understanding the condition of their body. In addition, it would be helpful to accelerate the development of innovative approaches and to apply these methods to assist doctors in diagnosing ophthalmic disease.","keywords_author":null,"keywords_other":["FEATURES","MACULAR DEGENERATION","RANKING","ALGORITHM","RECOGNITION","TEXTURE"],"max_cite":1.0,"pub_year":2017.0,"sources":"['wos']","rawkeys":["algorithm","recognition","features","macular degeneration","texture","ranking"],"tags":["recognition","features","standards","macular degeneration","texture","algorithms"]},{"p_id":12003,"title":"Objects Classification by Learning-Based Visual Saliency Model and Convolutional Neural Network","abstract":"Humans can easily classify different kinds of objects whereas it is quite difficult for computers. As a hot and difficult problem, objects classification has been receiving extensive interests with broad prospects. Inspired by neuroscience, deep learning concept is proposed. Convolutional neural network (CNN) as one of the methods of deep learning can be used to solve classification problem. But most of deep learning methods, including CNN, all ignore the human visual information processing mechanism when a person is classifying objects. Therefore, in this paper, inspiring the completed processing that humans classify different kinds of objects, we bring forth a new classification method which combines visual attention model and CNN. Firstly, we use the visual attention model to simulate the processing of human visual selection mechanism. Secondly, we use CNN to simulate the processing of how humans select features and extract the local features of those selected areas. Finally, not only does our classification method depend on those local features, but also it adds the human semantic features to classify objects. Our classification method has apparently advantages in biology. Experimental results demonstrated that our method made the efficiency of classification improve significantly.","keywords_author":null,"keywords_other":["SCENE","RECEPTIVE-FIELD HISTOGRAMS","RECOGNITION","ATTENTION"],"max_cite":1.0,"pub_year":2016.0,"sources":"['wos']","rawkeys":["attention","recognition","scene","receptive-field histograms"],"tags":["attention","recognition","scene","receptive-field histograms"]},{"p_id":12015,"title":"A light and faster regional convolutional neural network for object detection in optical remote sensing images","abstract":"Detection of objects from satellite optical remote sensing images is very important for many commercial and governmental applications. With the development of deep convolutional neural networks (deep CNNs), the field of object detection has seen tremendous advances. Currently, objects in satellite remote sensing images can be detected using deep CNNs. In general, optical remote sensing images contain many dense and small objects, and the use of the original Faster Regional CNN framework does not yield a suitably high precision. Therefore, after careful analysis we adopt dense convoluted networks, a multi-scale representation and various combinations of improvement schemes to enhance the structure of the base VGG16-Net for improving the precision. We propose an approach to reduce the test-time (detection time) and memory requirements. To validate the effectiveness of our approach, we perform experiments using satellite remote sensing image datasets of aircraft and automobiles. The results show that the improved network structure can detect objects in satellite optical remote sensing images more accurately and efficiently.","keywords_author":["Deep convolution neural network","Deep learning (DL)","Object detection","Remote sensing images","Deep convolution neural network","Deep learning (DL)","Remote sensing images","Object detection"],"keywords_other":["DEEP","Governmental applications","RECOGNITION","Satellite optical remote sensing","Convolutional neural network","Remote sensing images","Convolution neural network","Satellite remote sensing","Multiscale representations","Deep convolutional neural networks"],"max_cite":0.0,"pub_year":2018.0,"sources":"['scp', 'wos']","rawkeys":["recognition","deep convolution neural network","multiscale representations","convolution neural network","deep","satellite remote sensing","deep convolutional neural networks","governmental applications","satellite optical remote sensing","object detection","convolutional neural network","remote sensing images","deep learning (dl)"],"tags":["recognition","multiscale representations","satellite remote sensing","deep","machine learning","governmental applications","satellite optical remote sensing","object detection","convolutional neural network","remote sensing images"]},{"p_id":85744,"title":"Improved Unsupervised Color Segmentation Using a Modified HSV Color Model and a Bagging Procedure in K-Means plus plus Algorithm","abstract":"Accurate color image segmentation has stayed as a relevant topic between the researches\/scientific community due to the wide range of application areas such as medicine and agriculture. A major issue is the presence of illumination variations that obstruct precise segmentation. On the other hand, the machine learning unsupervised techniques have become attractive principally for the easy implementations. However, there is not an easy way to verify or ensure the accuracy of the unsupervised techniques; so these techniques could lead to an unknown result. This paper proposes an algorithm and a modification to the HSV color model in order to improve the accuracy of the results obtained from the color segmentation using the K-means++ algorithm. The proposal gives better segmentation and less erroneous color detections due to illumination conditions. This is achieved shifting the hue and rearranging the.. equation in order to avoid undefined conditions and increase robustness in the color model.","keywords_author":null,"keywords_other":["CLASSIFICATION","MACHINE","RECOGNITION","IMAGE SEGMENTATION"],"max_cite":0.0,"pub_year":2018.0,"sources":"['wos']","rawkeys":["recognition","machine","classification","image segmentation"],"tags":["recognition","machine","classification","image segmentation"]},{"p_id":12016,"title":"Using 3D Convolutional Neural Network in Surveillance Videos for Recognizing Human Actions","abstract":"Human action recognition is a very important component of visual surveillance systems. The demand for automatic surveillance systems play a crucial role in the circumstances where continuous patrolling by human guards are not possible. The analysis in surveillance scenarios often requires the detection of certain specific human actions. The automated recognition of human actions in detecting certain human actions are considered here. The main aim is to develop a novel 3D Convolutional Neural Network (CNN) model for human action recognition in realistic environment. The features are extracted from both the spatial and the temporal dimensions by performing 3D convolutions, by which, capturing the motion information encoded in multiple adjacent frames. The evolved model generates multiple information from the input frames, and the information from all the channels are combined and that is to be the final feature. The developed model automatically tends to recognize specific human actions which needs attention in the real world environment like in pathways or in corridors of any organization. This proposed work is well suitable for the situations like where continuous patrolling of humans are not possible, to prevent certain human actions which are not allowed inside the organisation premises.","keywords_author":["Security surveillance","convolutional neural networks","3D convolution","feature extraction","image analysis and action recognition"],"keywords_other":["RECOGNITION","TRACKING"],"max_cite":0.0,"pub_year":2018.0,"sources":"['wos']","rawkeys":["recognition","convolutional neural networks","image analysis and action recognition","tracking","feature extraction","3d convolution","security surveillance"],"tags":["recognition","image analysis and action recognition","tracking","feature extraction","convolutional neural network","3d convolution","security surveillance"]},{"p_id":12018,"title":"Deep embedding convolutional neural network for synthesizing CT image from T1-Weighted MR image","abstract":"Recently, more and more attention is drawn to the field of medical image synthesis across modalities. Among them, the synthesis of computed tomography (CT) image from T1-weighted magnetic resonance (MR) image is of great importance, although the mapping between them is highly complex due to large gaps of appearances of the two modalities. In this work, we aim to tackle this MR-to-CT synthesis task by a novel deep embedding convolutional neural network (DECNN). Specifically, we generate the feature maps from MR images, and then transform these feature maps forward through convolutional layers in the network. We can further compute a tentative CT synthesis from the midway of the flow of feature maps, and then embed this tentative CT synthesis result back to the feature maps. This embedding operation results in better feature maps, which are further transformed forward in DECNN. After repeating this embedding procedure for several times in the network, we can eventually synthesize a final CT image in the end of the DECNN. We have validated our proposed method on both brain and prostate imaging datasets, by also comparing with the state-of-the-art methods. Experimental results suggest that our DECNN (with repeated embedding operations) demonstrates its superior performances, in terms of both the perceptive quality of the synthesized CT image and the run-time cost for synthesizing a CT image. (C) 2018 Published by Elsevier B.V.","keywords_author":["Image synthesis","Deep convolutional neural network","Embedding block"],"keywords_other":["RADIOTHERAPY","REPRESENTATION","PET IMAGES","RECOGNITION","ATTENUATION CORRECTION","AUTO-CONTEXT","SEGMENTATION","REGISTRATION","GENERATION","BRAIN"],"max_cite":0.0,"pub_year":2018.0,"sources":"['ieee', 'wos']","rawkeys":["attenuation correction","auto-context","recognition","segmentation","pet images","deep convolutional neural network","radiotherapy","representation","brain","image synthesis","registration","embedding block","generation"],"tags":["attenuation correction","auto-context","recognition","segmentation","pet images","radiotherapy","representation","brain","convolutional neural network","image synthesis","registration","embedding block","generation"]},{"p_id":61167,"title":"Parallel search strategy in kernel feature space to track FLIR target","abstract":"Robust tracking in the forward looking infrared (FLIR) sequences is still a challenging problem in the field of computer vision. Because images acquired by the infrared sensors are characterized by low signal-to clutter ratios (SCR) and targets of interest may exhibit profound appearance variations due to ego-motion of the sensor platform and complex maneuvers. Though many efforts have been delivered, there are still some issues to be addressed. First, intensity features are not enough to deal with complex appearance variations for the challenging sequence. Second, to obtain satisfying estimation of target state, a plenty of particles have to be employed to approximate its probability density function (pdf). To deal with the two problems, a parallel search strategy based on kernel sparse representation (KL1PS tracker) is proposed to perform the tracking task in the FLIR sequences. With the ability of capturing the nonlinear features, kernel method is introduced to deal with complex appearance variations. After the kernel function is constructed based on the histogram features, both the target templates and candidates are mapped into the kernel feature space. Then efficient state particles are selected based on sparse representation which can be used to estimate the target state. The proposed method is tested on the AMCOM database, and the experimental results demonstrate its excellent performance in tracking accuracy compared with some state-of-the-art trackers. (C) 2016 Elsevier B.V. All rights reserved.","keywords_author":["Sparse representation","Kernel method","FLIR target tracking"],"keywords_other":["PARTICLE FILTERS","SELECTION","TUTORIAL","SPARSE APPEARANCE MODEL","FRAMEWORK","ROBUST VISUAL TRACKING","RECOGNITION","INFRARED IMAGERY","OBJECT TRACKING"],"max_cite":0.0,"pub_year":2016.0,"sources":"['wos']","rawkeys":["recognition","object tracking","tutorial","framework","sparse representation","infrared imagery","flir target tracking","particle filters","robust visual tracking","sparse appearance model","selection","kernel method"],"tags":["recognition","object tracking","tutorial","framework","selection","sparse representation","infrared imagery","flir target tracking","robust visual tracking","sparse appearance model","kernel methods","particle filter"]},{"p_id":12027,"title":"Decoding of visual activity patterns from fMRI responses using multivariate pattern analyses and convolutional neural network","abstract":"Decoding of human brain activity has always been a primary goal in neuroscience especially with functional magnetic resonance imaging (fMRI) data. In recent years, Convolutional neural network (CNN) has become a popular method for the extraction of features due to its higher accuracy, however it needs a lot of computation and training data. In this study, an algorithm is developed using Multivariate pattern analysis (MVPA) and modified CNN to decode the behavior of brain for different images with limited data set. Selection of significant features is an important part of fMRI data analysis, since it reduces the computational burden and improves the prediction performance; significant features are selected using t-test. MVPA uses machine learning algorithms to classify different brain states and helps in prediction during the task. General linear model (GLM) is used to find the unknown parameters of every individual voxel and the classification is done using multi-class support vector machine (SVM). MVPA-CNN based proposed algorithm is compared with region of interest (ROI) based method and MVPA based estimated values. The proposed method showed better overall accuracy (68.6%) compared to ROI (61.88%) and estimation values (64.17%).","keywords_author":["Convolutional neural network","fMRI","GLM","MVPA","SVM","Convolutional neural network","fMRI","MVPA","GLM","SVM"],"keywords_other":["Linear Models","Humans","Neuropsychological Tests","STATES","Visual Acuity","Brain Mapping","Photic Stimulation","Brain","NATURAL IMAGES","Support Vector Machine","RECONSTRUCTION","Female","ACTIVATION PATTERNS","HUMAN BRAIN ACTIVITY","Multivariate Analysis","RECOGNITION","Male","PREDICTION","Neural Networks (Computer)","FRAMEWORK","Magnetic Resonance Imaging"],"max_cite":3.0,"pub_year":2017.0,"sources":"['ieee', 'scp', 'wos']","rawkeys":["human brain activity","brain","reconstruction","fmri","convolutional neural network","neuropsychological tests","support vector machine","brain mapping","svm","natural images","mvpa","visual acuity","neural networks (computer)","recognition","framework","photic stimulation","humans","glm","multivariate analysis","states","activation patterns","male","prediction","linear models","female","magnetic resonance imaging"],"tags":["human brain activity","brain","visual-acuity","reconstruction","fmri","convolutional neural network","neuropsychological tests","generalized linear model","brain mapping","machine learning","natural images","recognition","neural networks","framework","multi-voxel pattern analysis","photic stimulation","humans","multivariate analysis","activation patterns","state","male","prediction","linear models","female","magnetic resonance imaging"]},{"p_id":36604,"title":"Building A Globally Optimized Computational Intelligent Image Processing Algorithm for On-Site Nitrogen Status Analysis in Plants","abstract":"IEEE Estimating nutrient content in plants is a very crucial task in the application of precision farming. This work will be more challenging if it is conducted nondestructively based on plant images captured on field due to the variation of lighting conditions. This paper proposes a computational intelligence image processing to analyze nitrogen status in wheat plants. We developed an ensemble of deep learning multilayer perceptron (DL-MLP) which was fused by committee machines for color normalization and image segmentation using the 24-patch Macbeth color checker as the color reference. This paper also focuses on building a genetic algorithm based global optimization to fine tune the color normalization and nitrogen estimation results. In our experiments, we discovered that the developed DL-MLP and global optimization can successfully normalize plant images by reducing color variabilities compared to other color normalization methods. Furthermore, this algorithm is able to enhance the nitrogen estimation results compared to other non-global optimization methods as well as the most renowned SPAD meter based nitrogen measurement.","keywords_author":["computing methodologies","data","database applications","database management","Estimation","Feature extraction","feature extraction or construction","general","general","Image color analysis","image processing and computer vision","Image segmentation","information technology and systems","knowledge and data engineering tools and techniques","Machine learning","Neural networks","Nitrogen"],"keywords_other":["general","Database applications","Information technology and systems","Database management","Image processing and computer vision","Image color analysis","data","Knowledge and data engineering tools and techniques","Computing methodologies"],"max_cite":1.0,"pub_year":2018.0,"sources":"['scp', 'wos']","rawkeys":["general","nitrogen","estimation","neural networks","database management","feature extraction or construction","database applications","image color analysis","image processing and computer vision","machine learning","data","feature extraction","knowledge and data engineering tools and techniques","information technology and systems","image segmentation","computing methodologies"],"tags":["nitrogen","recognition","estimation","neural networks","database management","feature extraction or construction","database applications","image color analysis","image processing and computer vision","machine learning","data","feature extraction","knowledge and data engineering tools and techniques","information technology and systems","image segmentation","computing methodologies"]},{"p_id":12034,"title":"Ear detection under uncontrolled conditions with multiple scale faster Region-based convolutional neural networks","abstract":"Ear detection is an important step in ear recognition approaches. Most existing ear detection techniques are based on manually designing features or shallow learning algorithms. However, researchers found that the pose variation, occlusion, and imaging conditions provide a great challenge to the traditional ear detection methods under uncontrolled conditions. This paper proposes an efficient technique involving Multiple Scale Faster Region-based Convolutional Neural Networks (Faster R-CNN) to detect ears from 2D profile images in natural images automatically. Firstly, three regions of different scales are detected to infer the information about the ear location context within the image. Then an ear region filtering approach is proposed to extract the correct ear region and eliminate the false positives automatically. In an experiment with a test set of 200 web images (with variable photographic conditions), 98% of ears were accurately detected. Experiments were likewise conducted on the Collection J2 of University of Notre Dame Biometrics Database (UND-J2) and University of Beira Interior Ear dataset (UBEAR), which contain large occlusion, scale, and pose variations. Detection rates of 100% and 98.22%, respectively, demonstrate the effectiveness of the proposed approach.","keywords_author":["Biometrics","Deep learning","Ear detection","Faster R-CNN","Location context","biometrics","deep learning","ear detection","location context","Faster R-CNN"],"keywords_other":["RECOGNITION","3D"],"max_cite":4.0,"pub_year":2017.0,"sources":"['scp', 'wos']","rawkeys":["recognition","deep learning","3d","location context","biometrics","faster r-cnn","ear detection"],"tags":["recognition","machine learning","three-dimensional","biometrics","location context","faster r-cnn","ear detection"]},{"p_id":69378,"title":"Discriminative pose-free descriptors for face and object matching","abstract":"Pose invariant matching is a very important problem with various applications like recognizing faces in uncontrolled scenarios in which the facial images appear in wide variety of pose and illumination conditions along with low resolution. Here we propose two discriminative pose-free descriptors, Subspace Point Representation (DPF-SPR) and Layered Canonical Correlated (DPF-LCC) descriptor, for matching faces and objects across pose. Training examples at very few poses are used to generate virtual intermediate pose subspaces. An image is represented by a feature set obtained by projecting its low-level feature on these subspaces and a discriminative transform is applied to make this feature set suitable for recognition. We represent this discriminative feature set by two novel descriptors. In one approach, we transform it to a vector by using subspace to point representation technique. In the second approach, a layered structure of canonical correlated subspaces are formed, onto which the feature set is projected. Experiments on recognizing faces and objects across pose and comparisons with state-of-the-art show the effectiveness of the proposed approach. (C) 2017 Elsevier Ltd. All rights reserved.","keywords_author":["Face recognition","Object recognition","Pose invariant matching","Metric learning","Canonical correlation","Subspace to point representation"],"keywords_other":["VERIFICATION","FEATURES","CLASSIFICATION","UNSUPERVISED DOMAIN ADAPTATION","RECOGNITION","DICTIONARY","SPARSE REPRESENTATION"],"max_cite":1.0,"pub_year":2017.0,"sources":"['ieee', 'wos']","rawkeys":["recognition","features","sparse representation","object recognition","canonical correlation","dictionary","pose invariant matching","classification","face recognition","unsupervised domain adaptation","verification","metric learning","subspace to point representation"],"tags":["recognition","features","sparse representation","canonical correlations","object recognition","pose invariant matching","unsupervised domain adaptation","verification","face recognition","classification","metric learning","dictionaries","subspace to point representation"]},{"p_id":12035,"title":"Exploring the learning capabilities of convolutional neural networks for robust image watermarking","abstract":"Existing techniques of watermarking make use of transform domain to have better robustness towards attacks. Here, we propose a novel learning based auto-encoder Convolutional Neural Network (CNN) for non-blind watermarking which outperforms the existing frequency domain techniques in terms of imperceptibility and robustness adding new dimension of usage of CNNs towards security. As these CNNs efficiently learn the features and represent the input at the output, they find applications in all the fields of science. Code book images of different size are generated using the proposed architecture and subjected to different attacks. Results of the proposed method are compared with state of the art methods at different noises and attacks such as Gaussian, speckle, compression effects, cropping, filtering, etc. The proposed scheme is validated against various possible attacks and its out performance with state of the art methods is presented. Further, transfer learning capabilities of auto-encoder CNN for efficient way of learning new code book is presented. The inability of intruder towards retrieval of data without the knowledge of architecture and keys employed is also discussed. (C) 2016 Elsevier Ltd. All rights reserved.","keywords_author":["Watermarking","Convolutional neural networks","Auto-encoders","Back propagation","Embedding","Extraction","Transfer-learning"],"keywords_other":["RECOGNITION"],"max_cite":0.0,"pub_year":2016.0,"sources":"['ieee', 'wos']","rawkeys":["recognition","convolutional neural networks","transfer-learning","watermarking","back propagation","embedding","extraction","auto-encoders"],"tags":["embeddings","recognition","transfer learning","watermarking","auto encoders","convolutional neural network","extraction","backpropagation"]},{"p_id":12041,"title":"Detection of bars in galaxies using a deep convolutional neural network","abstract":"We present an automated method for the detection of bar structure in optical images of galaxies using a deep convolutional neural network that is easy to use and provides good accuracy. In our study, we use a sample of 9346 galaxies in the redshift range of 0.009-0.2 from the Sloan Digital Sky Survey (SDSS), which has 3864 barred galaxies, the rest being unbarred. We reach a top precision of 94 per cent in identifying bars in galaxies using the trained network. This accuracy matches the accuracy reached by human experts on the same data without additional information about the images. Since deep convolutional neural networks can be scaled to handle large volumes of data, the method is expected to have great relevance in an era where astronomy data is rapidly increasing in terms of volume, variety, volatility, and velocity along with other V's that characterize big data. With the trained model, we have constructed a catalogue of barred galaxies from SDSS and made it available online.","keywords_author":["methods: data analysis","techniques: image processing","catalogues","galaxies: general"],"keywords_other":["BARRED GALAXIES","ARCHITECTURES","DIGITAL SKY SURVEY","RECOGNITION","MORPHOLOGICAL CLASSIFICATIONS","EVOLUTION","IMAGES"],"max_cite":0.0,"pub_year":2018.0,"sources":"['wos']","rawkeys":["recognition","images","architectures","morphological classifications","techniques: image processing","galaxies: general","evolution","methods: data analysis","catalogues","digital sky survey","barred galaxies"],"tags":["recognition","images","morphological classifications","techniques: image processing","galaxies: general","biological","methods: data analysis","catalogues","architecture","digital sky survey","barred galaxies"]},{"p_id":102153,"title":"Transfer learning with seasonal and trend adjustment for cross-building energy forecasting","abstract":"Large scale smart meter deployments have resulted in popularization of sensor-based electricity forecasting which relies on historical sensor data to infer future energy consumption. Although those approaches have been very successful, they require significant quantities of historical data, often over extended periods of time, to train machine learning models and achieve accurate predictions. New buildings and buildings with newly installed meters have small historical datasets that are insufficient to create accurate predictions. Transfer learning methods have been proposed as a way to use cross-domain datasets to improve predictions. However, these methods do not consider the effects of seasonality within domains. Consequently, this paper proposes Hephaestus, a novel transfer learning method for cross-building energy forecasting based on time series multi-feature regression with seasonal and trend adjustment. This method enables energy prediction with merged data from similar buildings with different distributions and different seasonal profiles. Thus, it improves energy prediction accuracy for a new building with limited data by using datasets from other similar buildings. Hephaestus works in the pre- and post-processing phases and therefore can be used with any standard machine learning algorithm. The case study presented here demonstrates that the proposed approach can improve energy prediction for a school by 11.2% by using additional data from other schools. (C) 2018 Published by Elsevier B.V.","keywords_author":["Energy forecasting","Transfer learning","Cross-building forecasting","Seasonal adjustment","Trend adjustment","Energy consumption"],"keywords_other":["PREDICTION","NEURAL-NETWORK MODEL","CONSUMPTION","HETEROGENEOUS DOMAIN ADAPTATION","FRAMEWORK","ALGORITHM","SUPPORT VECTOR REGRESSION","LOAD","RECOGNITION","RESIDENTIAL BUILDINGS"],"max_cite":0.0,"pub_year":2018.0,"sources":"['ieee', 'wos']","rawkeys":["algorithm","heterogeneous domain adaptation","recognition","energy consumption","transfer learning","framework","load","energy forecasting","neural-network model","prediction","consumption","support vector regression","cross-building forecasting","trend adjustment","residential buildings","seasonal adjustment"],"tags":["heterogeneous domain adaptation","recognition","energy consumption","residential building","transfer learning","framework","load","energy forecasting","prediction","support vector regression (svr)","consumption","neural network model","cross-building forecasting","algorithms","trend adjustment","seasonal adjustment"]},{"p_id":12046,"title":"A multi-scale convolutional neural network for phenotyping high-content cellular images","abstract":"Motivation: Identifying phenotypes based on high-content cellular images is challenging. Conventional image analysis pipelines for phenotype identification comprise multiple independent steps, with each step requiring method customization and adjustment of multiple parameters.","keywords_author":null,"keywords_other":["QUANTIFICATION","SUBCELLULAR STRUCTURES","CLASSIFICATION","INHIBITOR","RECOGNITION","ALSTERPAULLONE","POPULATIONS","MICROSCOPE IMAGES","PERTURBATION","SINGLE-CELL"],"max_cite":2.0,"pub_year":2017.0,"sources":"['ieee', 'wos']","rawkeys":["recognition","quantification","single-cell","alsterpaullone","microscope images","classification","perturbation","populations","subcellular structures","inhibitor"],"tags":["inhibitors","recognition","single cells","microscopic image","quantification","population","alsterpaullone","classification","perturbation","subcellular structures"]},{"p_id":69390,"title":"Image classification based on scheme of principal node analysis","abstract":"This paper presents a scheme of principal node analysis (PNA) with the aim to improve the representativeness of the learned codebook so as to enhance the classification rate of scene image. Original images are normalized into gray ones and the scale-invariant feature transform (SIFT) descriptors are extracted from each image in the preprocessing stage. Then, the PNA-based scheme is applied to the SIFT descriptors with iteration and selection algorithms. The principal nodes of each image are selected through spatial analysis of the SIFT descriptors with Manhattan distance (L1 norm) and Euclidean distance (L2 norm) in order to increase the representativeness of the codebook. With the purpose of evaluating the performance of our scheme, the feature vector of the image is calculated by two baseline methods after the codebook is constructed. The L1-PNA- and L2-PNA-based baseline methods are tested and compared with different scales of codebooks over three public scene image databases. The experimental results show the effectiveness of the proposed scheme of PNA with a higher categorization rate. (C) 2016 SPIE and IS&T","keywords_author":["codebook learning","image classification","principal node analysis","sparse code spatial pyramid matching","kernel spatial pyramid matching"],"keywords_other":["RECOGNITION","K-SVD"],"max_cite":0.0,"pub_year":2016.0,"sources":"['wos']","rawkeys":["codebook learning","recognition","kernel spatial pyramid matching","sparse code spatial pyramid matching","principal node analysis","image classification","k-svd"],"tags":["codebook learning","recognition","kernel spatial pyramid matching","sparse code spatial pyramid matching","principal node analysis","image classification","k-svd"]},{"p_id":69392,"title":"Low-Rank Latent Pattern Approximation With Applications to Robust Image Classification","abstract":"This paper develops a novel method to address the structural noise in samples for image classification. Recently, regression-related classification methods have shown promising results when facing the pixelwise noise. However, they become weak in coping with the structural noise due to ignoring of relationships between pixels of noise image. Meanwhile, most of them need to implement the iterative process for computing representation coefficients, which leads to the high time consumption. To overcome these problems, we exploit a latent pattern model called low-rank latent pattern approximation (LLPA) to reconstruct the test image having structural noise. The rank function is applied to characterize the structure of the reconstruction residual between test image and the corresponding latent pattern. Simultaneously, the error between the latent pattern and the reference image is constrained by Frobenius norm to prevent overfitting. LLPA involves a closed-form solution by the virtue of a singular value thresholding operator. The provided theoretic analysis demonstrates that LLPA indeed removes the structural noise during classification task. Additionally, LLPA is further extended to the form of matrix regression by connecting multiple training samples, and alternating direction of multipliers method with Gaussian back substitution algorithm is used to solve the extended LLPA. Experimental results on several popular data sets validate that the proposed methods are more robust to image classification with occlusion and illumination changes, as compared to some existing state-of-the-art reconstruction-based methods and one deep neural network-based method.","keywords_author":["Latent pattern","low-rank","structural noise","distance metric","ADMM","image classification"],"keywords_other":["REPRESENTATION","PRINCIPAL COMPONENT ANALYSIS","FRAMEWORK","EXPRESSION VARIANT FACES","MATRIX COMPLETION","RECOGNITION","FEATURE-EXTRACTION"],"max_cite":1.0,"pub_year":2017.0,"sources":"['ieee', 'wos']","rawkeys":["principal component analysis","recognition","structural noise","admm","framework","feature-extraction","representation","expression variant faces","distance metric","low-rank","matrix completion","image classification","latent pattern"],"tags":["principal component analysis","recognition","structural noise","framework","alternating direction method of multipliers","representation","distance metrics","expression variant faces","feature extraction","low-rank","matrix completion","image classification","latent pattern"]},{"p_id":12048,"title":"Support Vector Machine Histogram: New Analysis and Architecture Design Method of Deep Convolutional Neural Network","abstract":"Deep convolutional neural network (DCNN) is a kind of hierarchical neural network models and attracts attention in recent years since it has shown high classification performance. DCNN can acquire the feature representation which is a parameter indicating the feature of the input by learning. However, its internal analysis and the design of the network architecture have many unclear points and it cannot be said that it has been sufficiently elucidated. We propose the novel DCNN analysis method \"Support vector machine (SVM) histogram\" as a prescription to deal with these problems. This is a method that examines the spatial distribution of DCNN extracted feature representation by using the decision boundary of linear SVM. We show that we can interpret DCNN hierarchical processing using this method. In addition, by using the result of SVM histogram, DCNN architecture design becomes possible. In this study, we designed the architecture of the application to large scale natural image dataset. In the result, we succeeded in showing higher accuracy than the original DCNN.","keywords_author":["Deep convolutional neural network","Support vector machine","Architecture design","CIFAR-10","ImageNet"],"keywords_other":["RECOGNITION","NEOCOGNITRON"],"max_cite":0.0,"pub_year":2018.0,"sources":"['ieee', 'wos']","rawkeys":["recognition","neocognitron","deep convolutional neural network","architecture design","cifar-10","imagenet","support vector machine"],"tags":["recognition","neocognitron","machine learning","cifar-10","architecture designs","imagenet","convolutional neural network"]},{"p_id":12060,"title":"Convolutional Neural Network-Based Human Detection in Nighttime Images Using Visible Light Camera Sensors","abstract":"Because intelligent surveillance systems have recently undergone rapid growth, research on accurately detecting humans in videos captured at a long distance is growing in importance. The existing research using visible light cameras has mainly focused on methods of human detection for daytime hours when there is outside light, but human detection during nighttime hours when there is no outside light is difficult. Thus, methods that employ additional near-infrared (NIR) illuminators and NIR cameras or thermal cameras have been used. However, in the case of NIR illuminators, there are limitations in terms of the illumination angle and distance. There are also difficulties because the illuminator power must be adaptively adjusted depending on whether the object is close or far away. In the case of thermal cameras, their cost is still high, which makes it difficult to install and use them in a variety of places. Because of this, research has been conducted on nighttime human detection using visible light cameras, but this has focused on objects at a short distance in an indoor environment or the use of video-based methods to capture multiple images and process them, which causes problems related to the increase in the processing time. To resolve these problems, this paper presents a method that uses a single image captured at night on a visible light camera to detect humans in a variety of environments based on a convolutional neural network. Experimental results using a self-constructed Dongguk night-time human detection database (DNHD-DB1) and two open databases (Korea advanced institute of science and technology (KAIST) and computer vision center (CVC) databases), as well as high-accuracy human detection in a variety of environments, show that the method has excellent performance compared to existing methods.","keywords_author":["intelligent surveillance system","nighttime human detection","visible light image","convolutional neural network"],"keywords_other":["SYSTEM","PEDESTRIAN DETECTION","BACKGROUND IMAGE","ENVIRONMENTS","RECOGNITION","TIME","GENERATION","SURVEILLANCE","ENHANCEMENT","TRACKING"],"max_cite":1.0,"pub_year":2017.0,"sources":"['wos']","rawkeys":["intelligent surveillance system","pedestrian detection","visible light image","recognition","enhancement","nighttime human detection","system","background image","time","tracking","environments","convolutional neural network","surveillance","generation"],"tags":["pedestrian detection","visible light image","recognition","enhancement","intelligent surveillance systems","nighttime human detection","system","background image","time","tracking","environment","convolutional neural network","surveillance","generation"]},{"p_id":12061,"title":"Sea Ice Concentration Estimation during Freeze-Up from SAR Imagery Using a Convolutional Neural Network","abstract":"In this study, a convolutional neural network (CNN) is used to estimate sea ice concentration using synthetic aperture radar (SAR) scenes acquired during freeze-up in the Gulf of St. Lawrence on the east coast of Canada. The ice concentration estimates from the CNN are compared to those from a neural network (multi-layer perceptron or MLP) that uses hand-crafted features as input and a single layer of hidden nodes. The CNN is found to be less sensitive to pixel level details than the MLP and produces ice concentration that is less noisy and in closer agreement with that from image analysis charts. This is due to the multi-layer (deep) structure of the CNN, which enables abstract image features to be learned. The CNN ice concentration is also compared with ice concentration estimated from passive microwave brightness temperature data using the ARTIST sea ice (ASI) algorithm. The bias and RMS of the difference between the ice concentration from the CNN and that from image analysis charts is reduced as compared to that from either the MLP or ASI algorithm. Additional results demonstrate the impact of varying the input patch size, varying the number of CNN layers, and including the incidence angle as an additional input.","keywords_author":["ice concentration","SAR imagery","convolutional neural network"],"keywords_other":["REPRESENTATION","CLASSIFICATION","BALTIC SEA","RECOGNITION","CHARTS"],"max_cite":0.0,"pub_year":2017.0,"sources":"['ieee', 'wos']","rawkeys":["recognition","baltic sea","charts","representation","sar imagery","classification","convolutional neural network","ice concentration"],"tags":["recognition","baltic sea","charts","representation","sar imagery","classification","convolutional neural network","ice concentration"]},{"p_id":69406,"title":"Dynamic texture representation using a deep multi-scale convolutional network","abstract":"This work addresses dynamic texture representation and recognition via a convolutional multilayer architecture. The proposed method considers an image sequence as a concatenation of spatial images along the time axis as well as spatio-temporal images along both horizontal and vertical axes of an image sequence and uses multilayer convolutional operations to describe each plane. The filters used are learned via principal component analysis (PCA) on each of the three orthogonal planes of an image sequence. A particularly advantageous attribute of the technique is the unsupervised training procedure of the proposed network. An inter-database evaluation has been performed to investigate the generalisation capability of the proposed approach. Moreover, a multi-scale extension of the proposed architecture is presented to capture texture details at multiple resolutions. Through extensive evaluations on different databases, it is shown that the proposed PCA-based network on three orthogonal planes (PCANet-TOP) yields very discriminative features for dynamic texture classification. (C) 2016 Elsevier Inc. All rights reserved.","keywords_author":["Dynamic texture","Multilayer convolutional architectures","PCA","Multi-scale analysis"],"keywords_other":["STATISTICAL IMAGE FEATURES","CLASSIFICATION","MODEL","LOCAL BINARY PATTERNS","RECOGNITION"],"max_cite":5.0,"pub_year":2016.0,"sources":"['ieee', 'wos']","rawkeys":["recognition","model","dynamic texture","multilayer convolutional architectures","multi-scale analysis","statistical image features","classification","local binary patterns","pca"],"tags":["principal component analysis","dynamic textures","model","recognition","multilayer convolutional architectures","statistical image features","classification","local binary patterns","multi scale analysis"]},{"p_id":69408,"title":"DEVELOPMENT OF A COMPUTER SYSTEM FOR IDENTITY AUTHENTICATION USING ARTIFICIAL NEURAL NETWORKS","abstract":"The aim of the study is to increase the effectiveness of automated face recognition to authenticate identity, considering features of change of the face parameters over time. The improvement of the recognition accuracy, as well as consideration of the features of temporal changes in a human face can be based on the methodology of artificial neural networks. Hybrid neural networks, combining the advantages of classical neural networks and fuzzy logic systems, allow using the network learnability along with the explanation of the findings. The structural scheme of intelligent system for identification based on artificial neural networks is proposed in this work. It realizes the principles of digital information processing and identity recognition taking into account the forecast of key characteristics' changes over time (e.g., due to aging). The structural scheme has a three-tier architecture and implements preliminary processing, recognition and identification of images obtained as a result of monitoring. On the basis of expert knowledge, the fuzzy base of products is designed. It allows assessing possible changes in key characteristics, used to authenticate identity based on the image. To take this possibility into consideration, a neuro-fuzzy network of ANFIS type was used, which implements the algorithm of Tagaki-Sugeno. The conducted experiments showed high efficiency of the developed neural network and a low value of learning errors, which allows recommending this approach for practical implementation. Application of the developed system of fuzzy production rules that allow predicting changes in individuals over time, will improve the recognition accuracy, reduce the number of authentication failures and improve the efficiency of information processing and decision-making in applications, such as authentication of bank customers, users of mobile applications, or in video monitoring systems of sensitive sites.","keywords_author":["artificial neural networks","facial recognition","fuzzy knowledge base","identity authentication","video monitoring system"],"keywords_other":["RECOGNITION"],"max_cite":2.0,"pub_year":2017.0,"sources":"['wos']","rawkeys":["fuzzy knowledge base","recognition","video monitoring system","identity authentication","facial recognition","artificial neural networks"],"tags":["fuzzy knowledge base","recognition","video monitoring systems","neural networks","identity authentication","facial recognition"]},{"p_id":3892,"title":"Indoor Relocalization in Challenging Environments with Dual-Stream Convolutional Neural Networks","abstract":"This paper presents an indoor relocalization system using a dual-stream convolutional neural network (CNN) with both color images and depth images as the network inputs. Aiming at the pose regression problem, a deep neural network architecture for RGB-D images is introduced, a training method by stages for the dual-stream CNN is presented, different depth image encoding methods are discussed, and a novel encoding method is proposed. By introducing the range information into the network through a dual-stream architecture, we not only improved the relocalization accuracy by about 20% compared with the state-of-the-art deep learning method for pose regression, but also greatly enhanced the system robustness in challenging scenes such as large-scale, dynamic, fast movement, and night-time environments. To the best of our knowledge, this is the first work to solve the indoor relocalization problems based on deep CNNs with RGB-D camera. The method is first evaluated on the Microsoft 7-Scenes data set to show its advantage in accuracy compared with other CNNs. Large-scale indoor relocalization is further presented using our method. The experimental results show that 0.3 m in position and 4 degrees in orientation accuracy could be obtained. Finally, this method is evaluated on challenging indoor data sets collected from motion capture system. The results show that the relocalization performance is hardly affected by dynamic objects, motion blur, or night-time environments.","keywords_author":["Convolutional neural network (CNN)","deep learning","depth encoding","pose regression","relocalization","Convolutional neural network (CNN)","deep learning","depth encoding","pose regression","relocalization"],"keywords_other":["Training methods","FAB-MAP","Stream architecture","System robustness","Range information","Regression problem","State of the art","SLAM","LARGE-SCALE","Motion capture system","RECOGNITION","Convolutional neural network","REGISTRATION","SIMULTANEOUS LOCALIZATION","APPEARANCE"],"max_cite":1.0,"pub_year":2018.0,"sources":"['ieee', 'scp', 'wos']","rawkeys":["state of the art","simultaneous localization","slam","stream architecture","convolutional neural network","large-scale","registration","recognition","fab-map","deep learning","motion capture system","appearance","convolutional neural network (cnn)","training methods","range information","system robustness","relocalization","regression problem","depth encoding","pose regression"],"tags":["state of the art","simultaneous localization","stream architecture","convolutional neural network","machine learning","large-scale","registration","recognition","fab-map","motion capture system","appearance","training methods","range information","system robustness","relocation","robotics","regression problem","depth encoding","pose regression"]},{"p_id":12086,"title":"Biased Dropout and Crossmap Dropout: Learning towards effective Dropout regularization in convolutional neural network","abstract":"Training a deep neural network with a large number of parameters often leads to overfitting problem. Recently, Dropout has been introduced as a simple, yet effective regularization approach to combat overfitting in such models. Although Dropout has shown remarkable results on many deep neural network cases, its actual effect on CNN has not been thoroughly explored. Moreover, training a Dropout model will significantly increase the training time as it takes longer time to converge than a non-Dropout model with the same architecture. To deal with these issues, we address Biased Dropout and Crossmap Dropout, two novel approaches of Dropout extension based on the behavior of hidden units in CNN model. Biased Dropout divides the hidden units in a certain layer into two groups based on their magnitude and applies different Dropout rate to each group appropriately. Hidden units with higher activation value, which give more contributions to the network final performance, will be retained by a lower Dropout rate, while units with lower activation value will be exposed to a higher Dropout rate to compensate the previous part. The second approach is Crossmap Dropout, which is an extension of the regular Dropout in convolution layer. Each feature map in a convolution layer has a strong correlation between each other, particularly in every identical pixel location in each feature map. Crossmap Dropout tries to maintain this important correlation yet at the same time break the correlation between each adjacent pixel with respect to all feature maps by applying the same Dropout mask to all feature maps, so that all pixels or units in equivalent positions in each feature map will be either dropped or active during training. Our experiment with various benchmark datasets shows that our approaches provide better generalization than the regular Dropout. Moreover, our Biased Dropout takes faster time to converge during training phase, suggesting that assigning noise appropriately in hidden units can lead to an effective regularization. (C) 2018 Elsevier Ltd. All rights reserved.","keywords_author":["Dropout","Regularization","Convolutional neural network"],"keywords_other":["RECOGNITION"],"max_cite":0.0,"pub_year":2018.0,"sources":"['wos']","rawkeys":["dropout","regularization","recognition","convolutional neural network"],"tags":["dropout","regularization","recognition","convolutional neural network"]},{"p_id":12093,"title":"Using convolutional neural network to identify irregular segmentation objects from very high-resolution remote sensing imagery","abstract":"Convolutional neural network (CNN) has shown great success in computer vision tasks, but their application in land-use type classifications within the context of object-based image analysis has been rarely explored, especially in terms of the identification of irregular segmentation objects. Thus, a blocks-based object-based image classification (BOBIC) method was proposed to carry out end-to-end classification for segmentation objects using CNN. Specifically, BOBIC takes advantage of CNN to automatically extract complex features from the original image data, thereby avoiding the uncertainty caused by the manual extraction of features in OBIC. Additionally, OBIC compensates for the shortcomings of CNN whereby it is difficult to delineate a clear right boundary for ground objects at the pixel level. Using three high-resolution test images, the proposed BOBIC was compared with support vector machine (SVM) and random forest (RF) classifiers, and then, the effect of image blocks and mixed objects on classification accuracy was evaluated for the proposed BOBIC. Compared with conventional SVM and RF classifiers, the inclusion of CNN improved the OBIC classification performance substantially (5% to 10% increases in overall accuracy), and it also alleviated the effect derived from mixed objects. (C) The Authors. Published by SPIE under a Creative Commons Attribution 3.0 Unported License.","keywords_author":["object-based image classification","convolutional neural network","multiresolution segmentation","irregular segmented object","image block"],"keywords_other":["MAPPING LAND-COVER","MULTIRESOLUTION","AUTOMATED DETECTION","INFORMATION","MACHINE LEARNING ALGORITHMS","CLASSIFICATION","SUPPORT VECTOR MACHINES","RANDOM FOREST","RECOGNITION","SCALE"],"max_cite":0.0,"pub_year":2018.0,"sources":"['wos']","rawkeys":["image block","irregular segmented object","recognition","automated detection","multiresolution segmentation","machine learning algorithms","multiresolution","object-based image classification","mapping land-cover","classification","information","convolutional neural network","support vector machines","scale","random forest"],"tags":["irregular segmented object","recognition","automated detection","multiresolution segmentation","machine learning algorithms","machine learning","random forests","multiresolution","object-based image classification","mapping land-cover","classification","information","convolutional neural network","scale","image blocks"]},{"p_id":85823,"title":"Case-Based Statistical Learning: A Non-Parametric Implementation With a Conditional-Error Rate SVM","abstract":"Machine learning has been successfully applied to many areas of science and engineering. Some examples include time series prediction, optical character recognition, signal and image classification in biomedical applications for diagnosis and prognosis and so on. In the theory of semi-supervised learning, we have a training set and an unlabeled data, that are employed to fit a prediction model or learner, with the help of an iterative algorithm, such as the expectation-maximization algorithm. In this paper, a novel non-parametric approach of the so-called case-based statistical learning is proposed in a low-dimensional classification problem. This supervised feature selection scheme analyzes the discrete set of outcomes in the classification problem by hypothesis-testing and makes assumptions on these outcome values to obtain the most likely prediction model at the training stage. A novel prediction model is described in terms of the output scores of a confidence-based support vector machine classifier under class-hypothesis testing. To have a more accurate prediction by considering the unlabeled points, the distribution of unlabeled examples must be relevant for the classification problem. The estimation of the error rates from a well-trained support vector machines allows us to propose a non-parametric approach avoiding the use of Gaussian density function based models in the likelihood ratio test.","keywords_author":["Statistical learning and decision theory","support vector machines (SVM)","hypothesis testing","partial least squares","conditional-error rate"],"keywords_other":["DIAGNOSIS","PARTIAL LEAST-SQUARES","SUPPORT VECTOR MACHINE","PRINCIPAL COMPONENT ANALYSIS","CROSS-VALIDATION","CONFIDENCE","RECOGNITION","CLASSIFIER DESIGN","MRI","ALZHEIMERS-DISEASE"],"max_cite":4.0,"pub_year":2017.0,"sources":"['wos']","rawkeys":["diagnosis","support vector machines (svm)","principal component analysis","recognition","alzheimers-disease","partial least-squares","partial least squares","hypothesis testing","statistical learning and decision theory","confidence","mri","cross-validation","support vector machine","conditional-error rate","classifier design"],"tags":["diagnosis","principal component analysis","recognition","alzheimers-disease","partial least squares","machine learning","hypothesis testing","statistical learning and decision theory","confidence","computer vision","conditional-error rate","magnetic resonance imaging","classifier design"]},{"p_id":36675,"title":"Performance improvement of deep neural network classifiers by a simple training strategy","abstract":"\u00a9 2017 Elsevier Ltd Improving the classification performance of Deep Neural Networks (DNN) is of primary interest in many different areas of science and technology involving the use of DNN classifiers. In this study, we present a simple training strategy to improve the classification performance of a DNN. In order to attain our goal, we propose to divide the internal parameter space of the DNN into partitions and optimize these partitions individually. We apply our proposed strategy with the popular L-BFGS optimization algorithm even though it can be applied with any optimization algorithm. We evaluate the performance improvement obtained by using our proposed method by testing it on a number of well-known classification benchmark data sets and by performing statistical analysis procedures on classification results. The DNN classifier trained with the proposed strategy is also compared with the state-of-the-art classifiers to demonstrate its effectiveness. Our classification experiments show that the proposed method significantly enhances the training process of the DNN classifier and yields considerable improvements in the accuracy of the classification results.","keywords_author":["Autoencoder","Deep learning","Deep neural network","Limited memory BFGS","Softmax classifier","Stacked autoencoder","Autoencoder","Deep neural network","Deep learning","Limited memory BFGS","Softmax classifier","Stacked autoencoder"],"keywords_other":["Limited memory bfgs","Classification performance","Internal parameters","Optimization algorithms","AUTOENCODER","Classification results","RECOGNITION","Auto encoders","Science and Technology","SCALE OPTIMIZATION","Neural network classifier"],"max_cite":2.0,"pub_year":2018.0,"sources":"['ieee', 'scp', 'wos']","rawkeys":["science and technology","neural network classifier","recognition","classification performance","deep neural network","deep learning","scale optimization","autoencoder","auto encoders","internal parameters","limited memory bfgs","softmax classifier","optimization algorithms","stacked autoencoder","classification results"],"tags":["science and technology","neural network classifier","recognition","classification performance","scale optimization","machine learning","auto encoders","internal parameters","stacked autoencoders","limited memory bfgs","softmax classifier","convolutional neural network","optimization algorithms","classification results"]},{"p_id":12112,"title":"Segmentation of bone structure in X-ray images using convolutional neural network","abstract":"The segmentation process represents a first step necessary for any automatic method of extracting information from an image. In the case of X-ray images, through segmentation we can differentiate the bone tissue from the rest of the image. There are nowadays several segmentation techniques, but in general, they all require the human intervention in the segmentation process. Consequently, this article proposes a new segmentation method for the X-ray images using a Convolutional Neural Network (CNN). In present, the convolutional networks are the best techniques for image segmentation. This fact is demonstrated by their wide usage in all the fields, including the medical one. As the X-ray images have large dimensions, for reducing the training time, the method proposed by the present article selects only certain areas (maximum interest areas) from the entire image. The neural network is used as pixel classifier thus causing the label of each pixel (bone or none-bone) from a raw pixel values in a square area. We will also present the method through which the network final configuration was chosen and we will make a comparative analysis with other 3 CNN configurations. The network chosen by us obtained the best results for all the evaluation metrics used, i.e. warping error, rand error and pixel error.","keywords_author":["Biomedical image processing","Convolution","Image segmentation","Neural network","image segmentation","neural network","convolution","biomedical image processing"],"keywords_other":["RECEPTIVE FIELDS","RECOGNITION","CORTEX"],"max_cite":20.0,"pub_year":2013.0,"sources":"['ieee', 'scp', 'wos']","rawkeys":["neural network","recognition","convolution","biomedical image processing","receptive fields","cortex","image segmentation"],"tags":["recognition","neural networks","convolution","biomedical image processing","random forests","cortex","image segmentation"]},{"p_id":94036,"title":"Hemispheric dissociation of reward processing in humans: Insights from deep brain stimulation","abstract":"Rewards have various effects on human behavior and multiple representations in the human brain. Behaviorally, rewards notably enhance response vigor in incentive motivation paradigms and bias subsequent choices in instrumental learning paradigms. Neurally, rewards affect activity in different fronto-striatal regions attached to different motor effectors, for instance in left and right hemispheres for the two hands. Here we address the question of whether manipulating reward-related brain activity has local or general effects, with respect to behavioral paradigms and motor effectors. Neuronal activity was manipulated in a single hemisphere using unilateral deep brain stimulation (DBS) in patients with Parkinson's disease. Results suggest that DBS amplifies the representation of reward magnitude within the targeted hemisphere, so as to affect the behavior of the contralateral hand specifically. These unilateral DBS effects on behavior include both boosting incentive motivation and biasing instrumental choices. Furthermore, using computational modeling we show that DBS effects on incentive motivation can predict DBS effects on instrumental learning (or vice versa). Thus, we demonstrate the feasibility of causally manipulating reward-related neuronal activity in humans, in a manner that is specific to a class of motor effectors but that generalizes to different computational processes. As these findings proved independent from therapeutic effects on parkinsonian motor symptoms, they might provide insight into DBS impact on non-motor disorders, such as apathy or hypomania. (C) 2013 Elsevier Ltd. All rights reserved.","keywords_author":["Reinforcement learning","Incentive motivation","Hemispheric dissociation","Deep brain stimulation","Parkinson's disease"],"keywords_other":["DOPAMINE","PARKINSONS-DISEASE","REINFORCEMENT","PREDICTION","NEUROSCIENCE","SUBTHALAMIC NUCLEUS STIMULATION","APATHY","NEUROPSYCHIATRIC DISORDER","MOTIVATION","BASAL GANGLIA"],"max_cite":4.0,"pub_year":2013.0,"sources":"['wos']","rawkeys":["parkinson's disease","neuroscience","basal ganglia","neuropsychiatric disorder","incentive motivation","apathy","parkinsons-disease","subthalamic nucleus stimulation","dopamine","prediction","reinforcement","hemispheric dissociation","deep brain stimulation","motivation","reinforcement learning"],"tags":["parkinson's disease","neuroscience","basal ganglia","neuropsychiatric disorder","incentive motivation","apathy","recognition","subthalamic nucleus stimulation","dopamine","prediction","hemispheric dissociation","deep brain stimulation","motivation","reinforcement learning"]},{"p_id":12117,"title":"Ultrasound image-based thyroid nodule automatic segmentation using convolutional neural networks","abstract":"Delineation of thyroid nodule boundaries from ultrasound images plays an important role in calculation of clinical indices and diagnosis of thyroid diseases. However, it is challenging for accurate and automatic segmentation of thyroid nodules because of their heterogeneous appearance and components similar to the background. In this study, we employ a deep convolutional neural network (CNN) to automatically segment thyroid nodules from ultrasound images.","keywords_author":["Thyroid nodule","Ultrasound image","Convolutional neural network","Segmentation"],"keywords_other":["LEVEL SET","RECOGNITION"],"max_cite":2.0,"pub_year":2017.0,"sources":"['ieee', 'wos']","rawkeys":["recognition","segmentation","convolutional neural network","thyroid nodule","ultrasound image","level set"],"tags":["recognition","segmentation","ultrasound images","convolutional neural network","thyroid nodule","level set"]},{"p_id":12121,"title":"Feature Analysis of Unsupervised Learning for Multi-task Classification Using Convolutional Neural Network","abstract":"This study analyzes the characteristics of unsupervised feature learning using a convolutional neural network (CNN) to investigate its efficiency for multi-task classification and compare it to supervised learning features. We keep the conventional CNN structure and introduce modifications into the convolutional auto-encoder design to accommodate a subsampling layer and make a fair comparison. Moreover, we introduce non-maximum suppression and dropout for a better feature extraction and to impose sparsity constraints. The experimental results indicate the effectiveness of our sparsity constraints. We also analyze the efficiency of unsupervised learning features using the t-SNE and variance ratio. The experimental results show that the feature representation obtained in unsupervised learning is more advantageous for multi-task learning than that obtained in supervised learning.","keywords_author":["Auto-encoder","Convolutional neural networks","Deep learning","Multi-task learning","Unsupervised learning","Unsupervised learning","Convolutional neural networks","Multi-task learning","Auto-encoder","Deep learning"],"keywords_other":["Sparsity constraints","Multitask learning","Convolutional Neural Networks (CNN)","Unsupervised feature learning","Feature representation","RECOGNITION","Convolutional neural network","Auto encoders","Non-maximum suppression"],"max_cite":0.0,"pub_year":2018.0,"sources":"['ieee', 'scp', 'wos']","rawkeys":["auto-encoder","multi-task learning","convolutional neural networks","recognition","unsupervised feature learning","deep learning","auto encoders","multitask learning","unsupervised learning","non-maximum suppression","convolutional neural network","sparsity constraints","feature representation","convolutional neural networks (cnn)"],"tags":["recognition","unsupervised feature learning","auto encoders","machine learning","multitask learning","unsupervised learning","non-maximum suppression","convolutional neural network","sparsity constraints","feature representation"]},{"p_id":12123,"title":"Softly combining an ensemble of classifiers learned from a single convolutional neural network for scene categorization","abstract":"In this paper we propose to train an ensemble of classifiers from a single convolutional neural network (CNN) and softly combine these classifiers for scene categorization. Specifically, we explore the hierarchical structure of a CNN to extract multiple types of features from images, and train a multi-class classifier corresponding to each type of features. To combine these classifiers effectively, a soft combination strategy is introduced. Considering the fact that different images may need to be discriminated by using different types of features, we train a set of auxiliary binary-class classifiers to estimate the quality of categorizing an image by using the corresponding multi-class classifiers, so that a dynamic weight can be assigned to each of the multi-class classifiers for combination. On the other hand, because features extracted from different layers of a CNN differ largely in their levels of abstraction, classifiers trained based on these features have quite different capabilities for scene categorization. To address this issue, in the soft combination strategy we adopt the genetic algorithm to learn another set of static weights for the multi-class classifiers for combination. The static weights are to adapt the multi-class classifiers to given datasets. Finally, to categorize an image, the multi-class classifiers are combined by using both dynamic and static weights. We conduct experiments on two challenging benchmark datasets, MIT-indoor scene 67 and SUN397. Experiment results show that the proposed method is effective for scene categorization and can give superior results to state-of-the-art approaches. (C) 2018 Elsevier B.V. All rights reserved.","keywords_author":["Scene categorization","Convolutional neural networks","Ensemble learning","Soft combination","Genetic algorithm"],"keywords_other":["FEATURES","REPRESENTATION","GENETIC ALGORITHM","IMAGE CLASSIFICATION","RECOGNITION"],"max_cite":0.0,"pub_year":2018.0,"sources":"['ieee', 'wos']","rawkeys":["scene categorization","recognition","convolutional neural networks","features","representation","soft combination","genetic algorithm","ensemble learning","image classification"],"tags":["scene categorization","recognition","features","representation","soft combination","genetic algorithm","convolutional neural network","ensemble learning","image classification"]},{"p_id":3934,"title":"Efficient Hardware Architectures for Deep Convolutional Neural Network","abstract":"Convolutional neural network (CNN) is the state-of-the-art deep learning approach employed in various applications. Real-time CNN implementations in resource limited embedded systems are becoming highly desired recently. To ensure the programmable flexibility and shorten the development period, field programmable gate array is appropriate to implement the CNN models. However, the limited bandwidth and on-chip memory storage are the bottlenecks of the CNN acceleration. In this paper, we propose efficient hardware architectures to accelerate deep CNN models. The theoretical derivation of parallel fast finite impulse response algorithm (FFA) is introduced. Based on FFAs, the corresponding fast convolution units (FCUs) are developed for the computation of convolutions in the CNN models. Novel data storage and reuse schemes are proposed, where all intermediate pixels are stored on-chip and the bandwidth requirement is reduced. We choose one of the largest and most accurate networks, VGG16, and implement it on Xilinx Zynq ZC706 and Virtex VC707 boards, respectively. We achieve the top-5 accuracy of 86.25% using an equal distance non-uniform quantization method. It is estimated that the average performances are 316.23 GOP\/s under 172-MHz working frequency on Xilinx ZC706 and 1250.21 GOP\/s under 170-MHz working frequency on VC707, respectively. In brief, the proposed design outperforms the existing works significantly, in particular, surpassing related designs by more than two times in terms of resource efficiency.","keywords_author":["Convolutional neural network (CNN)","field programmable gate array (FPGA) platform","fast FIR algorithm","on-chip data storage scheme"],"keywords_other":["RECOGNITION"],"max_cite":0.0,"pub_year":2018.0,"sources":"['ieee', 'wos']","rawkeys":["recognition","convolutional neural network (cnn)","fast fir algorithm","on-chip data storage scheme","field programmable gate array (fpga) platform"],"tags":["recognition","convolutional neural network","fast fir algorithm","on-chip data storage scheme","field programmable gate array (fpga) platform"]},{"p_id":12129,"title":"Motor Fault Diagnosis Based on Short-time Fourier Transform and Convolutional Neural Network","abstract":"With the rapid development of mechanical equipment, the mechanical health monitoring field has entered the era of big data. However, the method of manual feature extraction has the disadvantages of low efficiency and poor accuracy, when handling big data. In this study, the research object was the asynchronous motor in the drivetrain diagnostics simulator system. The vibration signals of different fault motors were collected. The raw signal was pretreated using short time Fourier transform (STFT) to obtain the corresponding time-frequency map. Then, the feature of the time-frequency map was adaptively extracted by using a convolutional neural network (CNN). The effects of the pretreatment method, and the hyper parameters of network diagnostic accuracy, were investigated experimentally. The experimental results showed that the influence of the preprocessing method is small, and that the batch-size is the main factor affecting accuracy and training efficiency. By investigating feature visualization, it was shown that, in the case of big data, the extracted CNN features can represent complex mapping relationships between signal and health status, and can also overcome the prior knowledge and engineering experience requirement for feature extraction, which is used by traditional diagnosis methods. This paper proposes a new method, based on STFT and CNN, which can complete motor fault diagnosis tasks more intelligently and accurately.","keywords_author":["Big data","Convolutional neural network","Deep learning","Motor","Short-time Fourier transform","Big data","Deep learning","Short-time Fourier transform","Convolutional neural network","Motor"],"keywords_other":["WAVELET TRANSFORM","Time-frequency map","ROTATING MACHINERY","DEEP","Short time Fourier transforms","Mechanical equipment","RECOGNITION","Pre-processing method","Convolutional neural network","Network diagnostics","Pretreatment methods","Training efficiency","SIGNALS"],"max_cite":1.0,"pub_year":2017.0,"sources":"['ieee', 'scp', 'wos']","rawkeys":["signals","wavelet transform","network diagnostics","mechanical equipment","recognition","time-frequency map","big data","deep learning","deep","short-time fourier transform","training efficiency","pre-processing method","pretreatment methods","convolutional neural network","motor","rotating machinery","short time fourier transforms"],"tags":["signals","wavelet transform","network diagnostics","mechanical equipment","recognition","time-frequency map","big data","deep","machine learning","pre-processing method","pretreatment methods","training efficiency","convolutional neural network","motor","rotating machinery","short time fourier transforms"]},{"p_id":12134,"title":"A Practical and Highly Optimized Convolutional Neural Network for Classifying Traffic Signs in Real-Time","abstract":"Classifying traffic signs is an indispensable part of Advanced Driver Assistant Systems. This strictly requires that the traffic sign classification model accurately classifies the images and consumes as few CPU cycles as possible to immediately release the CPU for other tasks. In this paper, we first propose a new ConvNet architecture. Then, we propose a new method for creating an optimal ensemble of ConvNets with highest possible accuracy and lowest number of ConvNets. Our experiments show that the ensemble of our proposed ConvNets (the ensemble is also constructed using our method) reduces the number of arithmetic operations 88 and compared with two state-of-art ensemble of ConvNets. In addition, our ensemble is more accurate than one of the state-of-art ensembles and it is only less accurate than the other state-of-art ensemble when tested on the same dataset. Moreover, ensemble of our compact ConvNets reduces the number of the multiplications 95 and , yet, the classification accuracy drops only 0.2 and 0.4% compared with these two ensembles. Besides, we also evaluate the cross-dataset performance of our ConvNet and analyze its transferability power in different layers. We show that our network is easily scalable to new datasets with much more number of traffic sign classes and it only needs to fine-tune the weights starting from the last convolution layer. We also assess our ConvNet through different visualization techniques. Besides, we propose a new method for finding the minimum additive noise which causes the network to incorrectly classify the image by minimum difference compared with the highest score in the loss vector.","keywords_author":["Convolutional neural network","Traffic sign classification","Ensemble construction","Visualizing convolutional neural networks"],"keywords_other":["RECOGNITION","CLASSIFICATION","COLOR"],"max_cite":1.0,"pub_year":2016.0,"sources":"['ieee', 'wos']","rawkeys":["traffic sign classification","recognition","ensemble construction","visualizing convolutional neural networks","color","classification","convolutional neural network"],"tags":["traffic sign classification","recognition","ensemble construction","visualizing convolutional neural networks","color","classification","convolutional neural network"]},{"p_id":12137,"title":"SAR Image segmentation based on convolutional-wavelet neural network and markov random field","abstract":"Synthetic aperture radar (SAR) imaging system is usually an observation of the earths' surface. It means that rich structures exist in SAR images. Convolutional neural network (CNN) is good at learning features from raw data automatically, especially the structural features. Inspired by these, we propose a novel SAR image segmentation method based on convolutional-wavelet neural networks (CWNN) and Markov Random Field (MRF). In this approach, a wavelet constrained pooling layer is designed to replace the conventional pooling in CNN. The new architecture can suppress the noise and is better at keeping the structures of the learned features, which are crucial to the segmentation tasks. CWNN produces the segmentation map by patch-by-patch scanning. The segmentation result of CWNN will be used with two labeling strategies (i.e., a superpixel approach and a MRF approach) to produce the final segmentation map. The superpixel approach is used to enforce the smooth nature on the local region. On the other hand, the MRF approach is used to preserve the edges and the details of the SAR image. Specifically, two segmentation maps will be produced by applying the superpixel and MRF approaches. The first segmentation map is obtained by combining the segmentation map of CWNN and the superpixel approach, and the second segmentation map is obtained by applying the MRF approach on the original SAR image. Afterwards, these two segmentation maps are fused by using the sketch map of the SAR image to produce the final segmentation map. Experiments on the texture images demonstrate that the CWNN is effective for the segmentation tasks. Moreover, the experiments on the real SAR images show that our approach obtains the regions with labeling consistency and preserves the edges and details at the same time.","keywords_author":["Convolutional Neural Network","Markov Random Filed","SAR image segmentation","Wavelet transform","Convolutional Neural Network","Wavelet transform","Markov Random Filed","SAR image segmentation"],"keywords_other":["URBAN AREAS","Wavelet neural networks","TRANSFORM","FEATURES","Markov random filed","Segmentation results","CLASSIFICATION","MODEL","Labeling strategy","RECOGNITION","Structural feature","Convolutional neural network","Markov Random Fields","COMPLEX WAVELETS","TEXTURE","SAR image segmentation"],"max_cite":15.0,"pub_year":2017.0,"sources":"['ieee', 'scp', 'wos']","rawkeys":["urban areas","wavelet transform","labeling strategy","markov random fields","markov random filed","model","recognition","features","structural feature","complex wavelets","texture","classification","convolutional neural network","sar image segmentation","wavelet neural networks","transform","segmentation results"],"tags":["urban areas","wavelet transform","labeling strategy","markov random fields","markov random filed","model","recognition","features","structural feature","complex wavelets","texture","classification","convolutional neural network","sar image segmentation","wavelet neural networks","transform","segmentation results"]},{"p_id":85867,"title":"Martial Arts, Dancing and Sports dataset: A challenging stereo and multi-view dataset for 3D human pose estimation","abstract":"Human pose estimation is one of the most popular research topics in the past two decades, especially with the introduction of human pose datasets for benchmark evaluation. These datasets usually capture simple daily life actions. Here, we introduce a new dataset, the Martial Arts, Dancing and Sports (MADS), which consists of challenging martial arts actions (Tai-chi and Karate), dancing actions (hip-hop and jazz), and sports actions (basketball, volleyball, football, rugby, tennis and badminton). Two martial art masters, two dancers and an athlete performed these actions while being recorded with either multiple cameras or a stereo depth camera. In the multi-view or single-view setting, we provide three color views for 2D image-based human pose estimation algorithms. For depth-based human pose estimation, we provide stereo-based depth images from a single view. All videos have corresponding synchronized and calibrated ground-truth poses, which were captured using a Motion Capture system. We provide initial baseline results on our dataset using a variety of tracking frameworks, including a generative tracker based on the annealing particle filter and robust likelihood function, a discriminative tracker using twin Gaussian processes [1], and hybrid trackers, such as Personalized Depth Tracker [2]. The results of our evaluation suggest that discriminative approaches perform better than generative approaches when there are enough representative training samples, and that the generative methods are more robust to diversity of poses, but can fail to track when the motion is too quick for the effective search range of the particle filter. The data and the accompanying code will be made available to the research community. (C) 2017 Elsevier B.V. All rights reserved.","keywords_author":["Human pose estimation","Robust tracking","Evaluation","Martial arts","Dancing and sports"],"keywords_other":["RECOGNITION","HUMAN MOTION","MODELS","VISUAL TRACKING","IMAGES"],"max_cite":2.0,"pub_year":2017.0,"sources":"['wos']","rawkeys":["robust tracking","human pose estimation","martial arts","images","human motion","recognition","dancing and sports","evaluation","visual tracking","models"],"tags":["robust tracking","human pose estimations","martial arts","images","model","recognition","human motions","dancing and sports","evaluation","visual tracking"]},{"p_id":12140,"title":"Multi-Pixel Simultaneous Classification of PolSAR Image Using Convolutional Neural Networks","abstract":"Convolutional neural networks (CNN) have achieved great success in the optical image processing field. Because of the excellent performance of CNN, more and more methods based on CNN are applied to polarimetric synthetic aperture radar (PolSAR) image classification. Most CNN-based PolSAR image classification methods can only classify one pixel each time. Because all the pixels of a PolSAR image are classified independently, the inherent interrelation of different land covers is ignored. We use a fixed-feature-size CNN(FFS-CNN) to classify all pixels in a patch simultaneously. The proposed method has several advantages. First, FFS-CNN can classify all the pixels in a small patch simultaneously. When classifying a whole PolSAR image, it is faster than common CNNs. Second, FFS-CNN is trained to learn the interrelation of different land covers in a patch, so it can use the interrelation of land covers to improve the classification results. The experiments of FFS-CNN are evaluated on a Chinese Gaofen-3 PolSAR image and other two real PolSAR images. Experiment results show that FFS-CNN is comparable with the state-of-the-art PolSAR image classification methods.","keywords_author":["Gaofen-3","PolSAR image classification","convolutional neural networks","multi-pixel classification","fixed-feature-size"],"keywords_other":["RECOGNITION","DISPLACEMENT","POLARIMETRIC SAR"],"max_cite":0.0,"pub_year":2018.0,"sources":"['ieee', 'wos']","rawkeys":["recognition","displacement","convolutional neural networks","multi-pixel classification","polsar image classification","fixed-feature-size","polarimetric sar","gaofen-3"],"tags":["recognition","displacement","multi-pixel classification","polsar image classification","fixed-feature-size","polarimetric sar","gaofen-3","convolutional neural network"]},{"p_id":12139,"title":"End-to-End Waveform Utterance Enhancement for Direct Evaluation Metrics Optimization by Fully Convolutional Neural Networks","abstract":"Speech enhancement model is used to map a noisy speech to a clean speech. In the training stage, an objective function is often adopted to optimize the model parameters. However, in the existing literature, there is an inconsistency between the model optimization criterion and the evaluation criterion for the enhanced speech. For example, in measuring speech intelligibility, most of the evaluation metric is based on a short-time objective intelligibility (STOI) measure, while the frame based mean square error (MSE) between estimated and clean speech is widely used in optimizing the model. Due to the inconsistency, there is no guarantee that the trained model can provide optimal performance in applications. In this study, we propose an end-to-end utterance-based speech enhancement framework using fully convolutional neural networks (FCN) to reduce the gap between the model optimization and the evaluation criterion. Because of the utterance-based optimization, temporal correlation information of long speech segments, or even at the entire utterance level, can be considered to directly optimize perception-based objective functions. As an example, we implemented the proposed FCN enhancement framework to optimize the STOI measure. Experimental results show that the STOI of a test speech processed by the proposed approach is better than conventional MSE-optimized speech due to the consistency between the training and the evaluation targets. Moreover, by integrating the STOI into model optimization, the intelligibility of human subjects and automatic speech recognition system on the enhanced speech is also substantially improved compared to those generated based on the minimum MSE criterion.","keywords_author":["Automatic speech recognition","fully convolutional neural network","raw waveform","end-to-end speech enhancement","speech intelligibility"],"keywords_other":["SPEECH-ENHANCEMENT","MASKS","NOISE","RECOGNITION","ALGORITHMS","SOURCE SEPARATION","INTELLIGIBILITY","DEEP DENOISING AUTOENCODER"],"max_cite":0.0,"pub_year":2018.0,"sources":"['ieee', 'wos']","rawkeys":["fully convolutional neural network","recognition","noise","source separation","automatic speech recognition","raw waveform","deep denoising autoencoder","end-to-end speech enhancement","speech-enhancement","algorithms","speech intelligibility","intelligibility","masks"],"tags":["intelligence","noise","recognition","automatic speech recognition","raw waveform","source separation","masking","fully convolutional network","speech enhancement","end-to-end speech enhancement","deep denoising autoencoders","algorithms","speech intelligibility"]},{"p_id":12142,"title":"Deep Convolutional Neural Networks for mental load classification based on EEG data","abstract":"Electroencephalograph (EEG), the representation of the brain's electrical activity, is a widely used measure of brain activities such as working memory during cognitive tasks. Varying in complexity of cognitive tasks, mental load results in different EEG recordings. Classification of mental load is one of core issues in studies on working memory. Various machine learning methods have been introduced into this area, achieving competitive performance. Inspired by the recent breakthrough via deep recurrent convolutional neural networks (CNNs) on classifying mental load, we propose improved CNNs methods for this task. Specifically, our frameworks contain both single-model and double-model methods. With the help of our models, spatial, spectral, and temporal information of EEG data is taken into consideration. Meanwhile, a novel fusion strategy for utilizing different networks is introduced in this work. The proposed methods have been compared with state-of-the-art ones on the same EEG database. The comparison results show that both our single-model method and double-model method can achieve comparable or even better performance than the well-performed deep recurrent CNNs. Furthermore, our proposed CNNs models contain less parameters than state-of-the-art ones, making it be more competitive in further practical application. (C) 2017 Elsevier Ltd. All rights reserved.","keywords_author":["CNNs","Deep learning","EEG","Mental load classification","Deep learning","Mental load classification","CNNs","EEG"],"keywords_other":["SELECTION","Electrical activities","REPRESENTATION","CNNs","CAPACITY","Competitive performance","Mental loads","RECOGNITION","Temporal information","Convolutional neural network","Machine learning methods","Fusion strategies","MEMORY"],"max_cite":1.0,"pub_year":2018.0,"sources":"['ieee', 'scp', 'wos']","rawkeys":["cnns","competitive performance","capacity","machine learning methods","mental loads","recognition","memory","deep learning","eeg","representation","electrical activities","convolutional neural network","fusion strategies","temporal information","selection","mental load classification"],"tags":["competitive performance","machine learning methods","capacity","mental loads","recognition","memory","eeg","machine learning","representation","electrical activities","convolutional neural network","fusion strategies","temporal information","selection","mental load classification"]},{"p_id":12146,"title":"3D multi-resolution wavelet convolutional neural networks for hyperspectral image classification","abstract":"Hyperspectral images contain abundant spectral information, and three-dimensional (3D) feature extraction methods have been shown to be effective for classification. In this paper, we propose a hyperspectral image classification method that uses 3D multi-resolution wavelet convolutional network (3D MWCNNs) in which wavelets are first characterized by their time-frequency and multi-resolution. Then, the 3D-MWCNNs extract features from coarse to fine scales. In addition, 3D-MWCNNs work stably and effectively for approximation. In the conventional implementation of wavelets, empirical parameters must be determined in advance and the feature extraction process is not adaptive. Convolutional neural networks (CNNs) have strong adaptive learning capabilities and can extract features from low to high levels; however, they lack the theoretical underpinnings to perform multi resolution approximation for filter learning. Therefore, by combining the CNNs framework with multi-resolution analysis theory, a model called 3D MWCNNs is proposed to extract the 3D features from different scales and different depths adaptively. 3D MWCNNs model is better at feature representation and approximation from 3D cube data; therefore, they capture the spatial and spectral features more discriminatively to improve the classification accuracy. Experimental results on three well-known hyperspectral images demonstrate that the proposed framework achieves considerably higher classification accuracy than do several state-of-the-art algorithms. (C) 2017 Elsevier Inc. All rights reserved.","keywords_author":["Hyperspectral image classification","3D multi-resolution wavelet","Convolutional neural networks","Feature extraction"],"keywords_other":["PROFILES","TRANSFORM","FEATURES","REPRESENTATION","FRAMEWORK","DECOMPOSITION","RECOGNITION","ALGORITHMS","FUSION","FEATURE-EXTRACTION"],"max_cite":3.0,"pub_year":2017.0,"sources":"['ieee', 'wos']","rawkeys":["3d multi-resolution wavelet","recognition","convolutional neural networks","features","feature-extraction","framework","representation","hyperspectral image classification","feature extraction","algorithms","transform","fusion","profiles","decomposition"],"tags":["3d multi-resolution wavelet","recognition","features","framework","representation","hyperspectral image classification","feature extraction","convolutional neural network","algorithms","transform","fusion","profiles","decomposition"]},{"p_id":12148,"title":"Location Sensitive Deep Convolutional Neural Networks for Segmentation of White Matter Hyperintensities","abstract":"The anatomical location of imaging features is of crucial importance for accurate diagnosis in many medical tasks. Convolutional neural networks (CNN) have had huge successes in computer vision, but they lack the natural ability to incorporate the anatomical location in their decision making process, hindering success in some medical image analysis tasks. In this paper, to integrate the anatomical location information into the network, we propose several deep CNN architectures that consider multiscale patches or take explicit location features while training. We apply and compare the proposed architectures for segmentation of white matter hyperintensities in brain MR images on a large dataset. As a result, we observe that the CNNs that incorporate location information substantially outperform a conventional segmentation method with handcrafted features as well as CNNs that do not integrate location information. On a test set of 50 scans, the best configuration of our networks obtained a Dice score of 0.792, compared to 0.805 for an independent human observer. Performance levels of the machine and the independent human observer were not statistically significantly different (p-value = 0.06).","keywords_author":null,"keywords_other":["MR-IMAGES","QUANTIFICATION","RUN DMC","BRAIN IMAGES","AUTOMATIC SEGMENTATION","MULTIPLE-SCLEROSIS LESIONS","ROTTERDAM SCAN","MODEL","RECOGNITION","ALZHEIMERS-DISEASE"],"max_cite":6.0,"pub_year":2017.0,"sources":"['ieee', 'wos']","rawkeys":["rotterdam scan","recognition","model","alzheimers-disease","quantification","mr-images","multiple-sclerosis lesions","run dmc","brain images","automatic segmentation"],"tags":["rotterdam scan","recognition","model","alzheimers-disease","quantification","multiple sclerosis lesions","mr-images","brain imaging","run dmc","automatic segmentation"]},{"p_id":12152,"title":"Discovering similar Chinese characters in online handwriting with deep convolutional neural networks","abstract":"A primary reason for performance degradation in unconstrained online handwritten Chinese character recognition is the subtle differences between similar characters. Various methods have been proposed in previous works to address the problem of generating similar characters. These methods are basically comprised of two components-similar character discovery and cascaded classifiers. The goal of similar character discovery is to make similar character pairs\/sets cover as many misclassified samples as possible. It is observed that the confidence of convolutional neural network (CNN) is output by an end-to-end manner and it can be understood as one type of probability metric. In this paper, we propose an algorithm by leveraging CNN confidence for discovering similar character pairs\/sets. Specifically, a deep CNN is applied to output the top ranked candidates and the corresponding confidence scores, followed by an accumulating and averaging procedure. We experimentally found that the number of similar character pairs for each class is diverse and the confusion degree of similar character pairs is varied. To address these problems, we propose an entropy- based similarity measurement to rank these similar character pairs\/sets and reject those with low similarity. The experimental results indicate that by using 30,000 similar character pairs, our method achieves the hit rates of 98.44 and 98.05 % on CASIA-OLHWDB1.0 and CASIA-OLHWDB1.0-1.2 datasets, respectively, which are significantly higher than corresponding results produced by MQDF-based method (95.42 and 94.49 %). Furthermore, recognition of ten randomly selected similar character subsets with a two-stage classification scheme results in a relative error reduction of 30.11 % comparing with traditional single stage scheme, showing the potential usage of the proposed method.","keywords_author":["Similar character","Confidence","Similarity measurement","Convolutional neural network","Similar character pairs\/sets"],"keywords_other":["DISCRIMINATION","CONFIDENCE TRANSFORMATION","RECOGNITION"],"max_cite":2.0,"pub_year":2016.0,"sources":"['wos']","rawkeys":["recognition","similar character","similarity measurement","similar character pairs\/sets","confidence","convolutional neural network","discrimination","confidence transformation"],"tags":["recognition","similar character","similar character pairs\/sets","confidence","similarity measure","convolutional neural network","discrimination","confidence transformation"]},{"p_id":28537,"title":"Outcome measures based on classification performance fail to predict the intelligibility of binary-masked speech","abstract":"\u00a9 2016 Author(s).To date, the most commonly used outcome measure for assessing ideal binary mask estimation algorithms is based on the difference between the hit rate and the false alarm rate (H-FA). Recently, the error distribution has been shown to substantially affect intelligibility. However, H-FA treats each mask unit independently and does not take into account how errors are distributed. Alternatively, algorithms can be evaluated with the short-time objective intelligibility (STOI) metric using the reconstructed speech. This study investigates the ability of H-FA and STOI to predict intelligibility for binary-masked speech using masks with different error distributions. The results demonstrate the inability of H-FA to predict the behavioral intelligibility and also illustrate the limitations of STOI. Since every estimation algorithm will make errors that are distributed in different ways, performance evaluations should not be made solely on the basis of these metrics.","keywords_author":null,"keywords_other":["Hit rate","Humans","Voice Quality","Cochlear Implants","Audiometry, Speech","Outcome measures","Electric Stimulation","Acoustic Stimulation","Error distributions","Speech Intelligibility","Recognition (Psychology)","Speech Acoustics","Algorithms","Noise","False alarm rate","Ideal binary mask","Perceptual Masking","Estimation algorithm","Signal Processing, Computer-Assisted","Classification performance"],"max_cite":5.0,"pub_year":2016.0,"sources":"['scp', 'ieee']","rawkeys":["audiometry","speech acoustics","recognition (psychology)","speech","algorithms","false alarm rate","hit rate","perceptual masking","cochlear implants","classification performance","noise","outcome measures","humans","signal processing","electric stimulation","ideal binary mask","speech intelligibility","estimation algorithm","error distributions","acoustic stimulation","computer-assisted","voice quality"],"tags":["electrical-stimulation","audiometry","speech acoustics","speech","algorithms","false alarm rate","hit rate","perceptual masking","recognition","cochlear implants","classification performance","noise","outcome measures","humans","signal processing","ideal binary mask","speech intelligibility","estimation algorithm","error distributions","acoustic stimulation","computer-assisted","voice quality"]},{"p_id":85881,"title":"Distributed Newton Methods for Deep Neural Networks","abstract":"Deep learning involves a difficult nonconvex optimization problem with a large number of weights between any two adjacent layers of a deep structure. To handle large data sets or complicated networks, distributed training is needed, but the calculation of function, gradient, and Hessian is expensive. In particular, the communication and the synchronization cost may become a bottleneck. In this letter, we focus on situations where the model is distributedly stored and propose a novel distributed Newton method for training deep neural networks. By variable and feature-wise data partitions and some careful designs, we are able to explicitly use the Jacobian matrix for matrix-vector products in the Newton method. Some techniques are incorporated to reduce the running time as well as memory consumption. First, to reduce the communication cost, we propose a diagonalization method such that an approximate Newton direction can be obtained without communication between machines. Second, we consider subsampled Gauss-Newton matrices for reducing the running time as well as the communication cost. Third, to reduce the synchronization cost, we terminate the process of finding an approximate Newton direction even though some nodes have not finished their tasks. Details of some implementation issues in distributed environments are thoroughly investigated. Experiments demonstrate that the proposed method is effective for the distributed training of deep neural networks. Compared with stochastic gradient methods, it is more robust and may give better test accuracy.","keywords_author":null,"keywords_other":["RECOGNITION","OPTIMIZATION","GRADIENT DESCENT"],"max_cite":0.0,"pub_year":2018.0,"sources":"['wos']","rawkeys":["gradient descent","recognition","optimization"],"tags":["gradient descent","recognition","optimization"]},{"p_id":12153,"title":"A convolutional neural network VLSI architecture using sorting model for reducing multiply-and-accumulation operations","abstract":"Hierarchical convolutional neural networks are a well-known robust image-recognition model. In order to apply this model to robot vision or various intelligent real-time vision systems, its VLSI implementation is essential. This paper proposes a new algorithm for reducing multiply-and-accumulation operation by sorting neuron outputs by magnitude. We also propose a VLSI architecture based on this algorithm. We have designed and fabricated a sorting LSI by using a 0.35 mu m CMOS process. We have verified successful sorting operations at 100 MHz clock cycle by circuit simulation.","keywords_author":null,"keywords_other":["RECOGNITION"],"max_cite":0.0,"pub_year":2005.0,"sources":"['wos']","rawkeys":["recognition"],"tags":["recognition"]},{"p_id":12157,"title":"A Convolutional Neural Network Smartphone App for Real-Time Voice Activity Detection","abstract":"This paper presents a smartphone app that performs real-time voice activity detection based on convolutional neural network. Real-time implementation issues are discussed showing how the slow inference time associated with convolutional neural networks is addressed. The developed smartphone app is meant to act as a switch for noise reduction in the signal processing pipelines of hearing devices, enabling noise estimation or classification to be conducted in noise-only parts of noisy speech signals. The developed smartphone app is compared with a previously developed voice activity detection app as well as with two highly cited voice activity detection algorithms. The experimental results indicate that the developed app using convolutional neural network outperforms the previously developed smartphone app.","keywords_author":["Smartphone app for real-time voice activity detection","convolutional neural network voice activity detector","real-time implementation of convolutional neural network"],"keywords_other":["MODEL","RECOGNITION"],"max_cite":0.0,"pub_year":2018.0,"sources":"['ieee', 'wos']","rawkeys":["recognition","real-time implementation of convolutional neural network","model","convolutional neural network voice activity detector","smartphone app for real-time voice activity detection"],"tags":["recognition","real-time implementation of convolutional neural network","model","convolutional neural network voice activity detector","smartphone app for real-time voice activity detection"]},{"p_id":12158,"title":"A dyadic multi-resolution deep convolutional neural wavelet network for image classification","abstract":"For almost the past four decades, image classification has gained a lot of attention in the field of pattern recognition due to its application in various fields. Given its importance, several approaches have been proposed up to now. In this paper, we will present a dyadic multi-resolution deep convolutional neural wavelets' network approach for image classification. This approach consists of performing the classification of one class versus all the other classes of the dataset by the reconstruction of a Deep Convolutional Neural Wavelet Network (DCNWN). This network is based on the Neural Network (NN) architecture, the Fast Wavelet Transform (FWT) and the Adaboost algorithm. It consists, first, of extracting features using the FWT based on the Multi-Resolution Analysis (MRA). These features are used to calculate the inputs of the hidden layer. Second, those inputs are filtered by using the Adaboost algorithm to select the best ones corresponding to each image. Third, we create an AutoEncoder (AE) using wavelet networks of all images. Finally, we apply a pooling for each hidden layer of the wavelet network to obtain a DCNWN that permits the classification of one class and rejects all other classes of the dataset. Classification rates given by our approach show a clear improvement compared to those cited in this article.","keywords_author":["Image classification","Pattern recognition","Multi-resolution","Deep convolutional neural wavelet network","FastWavelet Transform","Adaboost"],"keywords_other":["SHAPE","REPRESENTATION","SCENE","RECOGNITION","FAST LEARNING ALGORITHM","CATEGORIES"],"max_cite":0.0,"pub_year":2017.0,"sources":"['ieee', 'wos']","rawkeys":["recognition","deep convolutional neural wavelet network","multi-resolution","fastwavelet transform","image classification","representation","scene","categories","shape","pattern recognition","fast learning algorithm","adaboost"],"tags":["recognition","deep convolutional neural wavelet network","fast wavelet transform","image classification","representation","scene","multiresolution","categories","shape","pattern recognition","fast learning algorithm","adaboost"]},{"p_id":85888,"title":"Sparse online feature maps","abstract":"Online kernel methods suffer from computational and memory complexity in large-scale problems. Due to these drawbacks, budget online kernel learning and kernel approximation (low-dimensional feature map approximation) methods are widely used to speed up time and to reduce memory usage of kernel approaches. In this paper, orthogonal Gram-Schmidt explicit feature maps are applied to online kernel methods. The main advantage of these feature maps come from their orthogonality property. Utilization of these feature maps leads to mutually linearly independent dimensions of feature space, hence, reduce the redundancy in this space. These feature maps can be applied to single-pass online learning methods with l(2)- and l(0)-norm regularization to reduce the computational and memory complexity. In this paper, the proposed methods are named: 1) Online Feature Maps (OFEMs) and 2) Sparse Online Feature Maps (SOFEMs). These methods are examined for binary and multiclass single-label classification problems. Extensive experiments are compared with the results of other state-of-the-art methods on standard and real-world datasets. The experimental results show that OFEMs and SOFEMs outperform other methods in the literature. (C) 2018 Elsevier B.V. All rights reserved.","keywords_author":["Explicit feature map","Kernel methods","Single-pass online learning","Gram-Schmidt orthogonalization process"],"keywords_other":["BUDGET","CLASSIFICATION","RECOGNITION","ALGORITHMS","PERCEPTRON","GRADIENT","KERNELS"],"max_cite":0.0,"pub_year":2018.0,"sources":"['wos']","rawkeys":["recognition","kernel methods","kernels","single-pass online learning","gram-schmidt orthogonalization process","perceptron","budget","gradient","classification","algorithms","explicit feature map"],"tags":["recognition","kernel methods","single-pass online learning","gram-schmidt orthogonalization process","explicit feature mapping","perceptron","budget","gradient","classification","kernel","algorithms"]},{"p_id":3977,"title":"Constrained Convolutional Neural Networks: A New Approach Towards General Purpose Image Manipulation Detection","abstract":"Identifying the authenticity and processing history of an image is an important task in multimedia forensics. By analyzing traces left by different image manipulations, researchers have been able to develop several algorithms capable of detecting targeted editing operations. While this approach has led to the development of several successful forensic algorithms, an important problem remains: creating forensic detectors for different image manipulations is a difficult and time consuming process. Furthermore, forensic analysts need general purpose forensic algorithms capable of detecting multiple different image manipulations. In this paper, we address both of these problems by proposing a new general purpose forensic approach using convolutional neural networks (CNNs). While CNNs are capable of learning classification features directly from data, in their existing form they tend to learn features representative of an image's content. To overcome this issue, we have developed a new type of CNN layer, called a constrained convolutional layer, that is able to jointly suppress an image's content and adaptively learn manipulation detection features. Through a series of experiments, we show that our proposed constrained CNN is able to learn manipulation detection features directly from data. Our experimental results demonstrate that our CNN can detect multiple different editing operations with up to 99.97% accuracy and outperform the existing state-of-the-art general purpose manipulation detector. Furthermore, our constrained CNN can still accurately detect image manipulations in realistic scenarios where there is a source camera model mismatch between the training and testing data.","keywords_author":["convolutional neural networks","deep convolutional features","deep learning","Image forensics","Image forensics","deep learning","convolutional neural networks","deep convolutional features"],"keywords_other":["Image forensics","Forensics","DIGITAL IMAGES","AUTHENTICATION","JPEG COMPRESSION","RECOGNITION","FORENSIC DETECTION","Convolutional neural network","Task analysis","deep convolutional features"],"max_cite":0.0,"pub_year":2018.0,"sources":"['scp', 'wos', 'ieee']","rawkeys":["task analysis","recognition","convolutional neural networks","authentication","jpeg compression","deep learning","forensic detection","image forensics","convolutional neural network","forensics","digital images","deep convolutional features"],"tags":["task analysis","recognition","authentication","jpeg compression","machine learning","forensic detection","forensics","image forensics","convolutional neural network","deep convolutional features","digital image"]},{"p_id":3979,"title":"Classification of Breast Cancer Based on Histology Images Using Convolutional Neural Networks","abstract":"In recent years, the classification of breast cancer has been the topic of interest in the field of Healthcare informatics, because it is the second main cause of cancer-related deaths in women. Breast cancer can be identified using a biopsy where tissue is removed and studied under microscope. The diagnosis is based on the qualification of the histopathologist, who will look for abnormal cells. However, if the histopathologist is not well-trained, this may lead to wrong diagnosis. With the recent advances in image processing and machine learning, there is an interest in attempting to develop a reliable pattern recognition based systems to improve the quality of diagnosis. In this paper, we compare two machine learning approaches for the automatic classification of breast cancer histology images into benign and malignant and into benign and malignant sub-classes. The first approach is based on the extraction of a set of handcrafted features encoded by two coding models (bag of words and locality constrained linear coding) and trained by support vector machines, while the second approach is based on the design of convolutional neural networks. We have also experimentally tested dataset augmentation techniques to enhance the accuracy of the convolutional neural network as well as \"handcrafted features + convolutional neural network\" and \"convolutional neural network features + classifier\" configurations. The results show convolutional neural networks outperformed the handcrafted feature based classifier, where we achieved accuracy between 96.15% and 98.33% for the binary classification and 83.31% and 88.23% for the multi-class classification.","keywords_author":["Histology images","convolutional neural networks","engineered features","bag of words","locality constrained linear coding"],"keywords_other":["DIAGNOSIS","CYTOLOGICAL IMAGES","SYSTEM","MODEL","RECOGNITION","ROBUST FEATURES","SURF"],"max_cite":0.0,"pub_year":2018.0,"sources":"['ieee', 'wos']","rawkeys":["diagnosis","bag of words","robust features","recognition","convolutional neural networks","model","histology images","locality constrained linear coding","system","surf","cytological images","engineered features"],"tags":["diagnosis","bag of words","robust features","recognition","model","histology images","system","surf","convolutional neural network","cytological images","locality-constrained linear coding","engineered features"]},{"p_id":3991,"title":"Multi-Organ Plant Classification Based on Convolutional and Recurrent Neural Networks","abstract":"Classification of plants based on a multi-organ approach is very challenging. Although additional data provide more information that might help to disambiguate between species, the variability in shape and appearance in plant organs also raises the degree of complexity of the problem. Despite promising solutions built using deep learning enable representative features to be learned for plant images, the existing approaches focus mainly on generic features for species classification, disregarding the features representing plant organs. In fact, plants are complex living organisms sustained by a number of organ systems. In our approach, we introduce a hybrid generic-organ convolutional neural network (HGO-CNN), which takes into account both organ and generic information, combining them using a new feature fusion scheme for species classification. Next, instead of using a CNN-based method to operate on one image with a single organ, we extend our approach. We propose a new framework for plant structural learning using the recurrent neural network-based method. This novel approach supports classification based on a varying number of plant views, capturing one or more organs of a plant, by optimizing the contextual dependencies between them. We also present the qualitative results of our proposed models based on feature visualization techniques and show that the outcomes of visualizations depict our hypothesis and expectation. Finally, we show that by leveraging and combining the aforementioned techniques, our best network outperforms the state of the art on the PlantClef2015 benchmark. The source code and models are available at https:\/\/github. com\/cs-chan\/Deep-Plant.","keywords_author":["deep learning","Plant classification","Plant classification","deep learning"],"keywords_other":["Structural learning","Species classification","FEATURES","Recurrent neural network (RNN)","Shape","Plant classification","RECOGNITION","Degree of complexity","Convolutional neural network","SPECIES IDENTIFICATION","Task analysis","CLASSIFIERS"],"max_cite":0.0,"pub_year":2018.0,"sources":"['scp', 'wos', 'ieee']","rawkeys":["task analysis","species identification","recognition","species classification","features","deep learning","classifiers","degree of complexity","shape","convolutional neural network","plant classification","recurrent neural network (rnn)","structural learning"],"tags":["task analysis","species identification","recognition","species classification","features","neural networks","machine learning","degree of complexity","shape","classifier","convolutional neural network","plant classification","structure-learning"]},{"p_id":12189,"title":"Clustering-Oriented Multiple Convolutional Neural Networks for Single Image Super-Resolution","abstract":"In contrast to the human visual system (HVS) that applies different processing schemes to visual information of different textural categories, most existing deep learning models for image super-resolution tend to exploit an indiscriminate scheme for processing one whole image. Inspired by the human cognitive mechanism, we propose a multiple convolutional neural network framework trained based on different textural clusters of image local patches. To this end, we commence by grouping patches into K clusters via K-means, which enables each cluster center to encode image priors of a certain texture category. We then train K convolutional neural networks for super-resolution based on the K clusters of patches separately, such that the multiple convolutional neural networks comprehensively capture the patch textural variability. Furthermore, each convolutional neural network characterizes one specific texture category and is used for restoring patches belonging to the cluster. In this way, the texture variation within a whole image is characterized by assigning local patches to their closest cluster centers, and the super-resolution of each local patch is conducted via the convolutional neural network trained by its cluster. Our proposed framework not only exploits the deep learning capability of convolutional neural networks but also adapts them to depict texture diversities for super-resolution. Experimental super-resolution evaluations on benchmark image datasets validate that our framework achieves state-of-the-art performance in terms of peak signal-to-noise ratio and structural similarity. Our multiple convolutional neural network framework provides an enhanced image super-resolution strategy over existing single-mode deep learning models.","keywords_author":["Clustering","Convolutional neural networks","Single image super-resolution"],"keywords_other":["MODEL","RECOGNITION","DEEP","REGRESSION"],"max_cite":0.0,"pub_year":2017.0,"sources":"['ieee', 'wos']","rawkeys":["recognition","model","convolutional neural networks","deep","clustering","single image super-resolution","regression"],"tags":["recognition","model","deep","sparse representation","clustering","convolutional neural network","regression"]},{"p_id":3999,"title":"Selective CS: An Energy-Efficient Sensing Architecture for Wireless Implantable Neural Decoding","abstract":"The spike classification is a critical step in the implantable neural decoding. The energy efficiency issue in the sensor node is a big challenge for the entire system. Compressive sensing (CS) theory provides a potential way to tackle this problem by reducing the data volume on the communication channel. However, the constant transmission of the compressed data is still energy-hungry. On the other hand, the feasibility of direct analysis in compression domain is mathematically demonstrated. This advance empowers the in-sensor light-weight signal analysis on the compressed data. In this paper, we propose a novel selective CS architecture for energy-efficient wireless implantable neural decoding based on compression analysis and deep learning. Specifically, we develop a two-stage classification procedure, including a light-weight coarse-grained screening module in the sensor and an accurate fine-grained analysis module in the server. To achieve better energy efficiency, the screening module is designed by the Softmax regression, which can complete the low-effort classification task at the sensor end and screen the high-effort task to transmit their compressed measurements to the remote server. The fine-grained analysis located in server end is constructed by the customized deep residual neural network. It can not only promote the spike classification accuracy, but also benefit the model quality of in-sensor Softmax model. The extensive experimental results indicate that our proposed selective CS architecture can gain more than 60% energy savings than the conventional CS architecture, yet even improve the accuracy of state-of-the-art CS architectures.","keywords_author":["Compressed sensing","deep learning","energy-efficient architecture","Compressed sensing","energy-efficient architecture","deep learning"],"keywords_other":["RECORDINGS","Energy-efficient architectures","Classification procedure","Classification tasks","Compressive sensing","CLASSIFICATION","SYSTEM","Fine-grained analysis","RECOGNITION","Wireless communications","Classification accuracy","Energy-efficient sensing"],"max_cite":0.0,"pub_year":2018.0,"sources":"['scp', 'wos', 'ieee']","rawkeys":["fine-grained analysis","energy-efficient architectures","recognition","deep learning","classification accuracy","system","energy-efficient sensing","classification tasks","recordings","wireless communications","classification","classification procedure","compressive sensing","compressed sensing","energy-efficient architecture"],"tags":["fine-grained analysis","energy-efficient architectures","recognition","classification accuracy","machine learning","system","energy-efficient sensing","classification tasks","recordings","wireless communications","classification","classification procedure","compressive sensing"]},{"p_id":4002,"title":"Soft Memory Box: A Virtual Shared Memory Framework for Fast Deep Neural Network Training in Distributed High Performance Computing","abstract":"\u00a9 2013 IEEE. Deep learning is one of the major promising machine learning methodologies. Deep learning is widely used in various application domains, e.g., image recognition, voice recognition, and natural language processing. In order to improve learning accuracy, deep neural networks have evolved by: 1) increasing the number of layers and 2) increasing the number of parameters in massive models. This implies that distributed deep learning platforms need to evolve to: 1) deal with huge\/complex deep neural networks and 2) process with high-performance computing resources for massive training data. This paper proposes a new virtual shared memory framework, called Soft Memory Box (SMB), which enables sharing the memory of remote node among distributed processes in the nodes so as to improve communication performance via parameter sharing. According to data-intensive performance evaluation results, the communication time of deep learning using the proposed SMB is 2.1 times faster than that using the massage passing interface (MPI). In addition, the communication time of the SMB-based asynchronous parameter update becomes 2-7 times faster than that using the MPI depending on deep learning models and the number of deep learning workers.","keywords_author":["deep neural network","distributed computing","distributed deep learning","High performance computing","shared memory","soft memory box","High performance computing","distributed computing","soft memory box","shared memory","deep neural network","distributed deep learning"],"keywords_other":["Training data","Computational model","Soft Memory Box","Memory management","High performance computing","RECOGNITION","Shared memory"],"max_cite":0.0,"pub_year":2018.0,"sources":"['scp', 'wos', 'ieee']","rawkeys":["shared memory","distributed computing","distributed deep learning","recognition","deep neural network","training data","soft memory box","high performance computing","computational model","memory management"],"tags":["computational modeling","shared memory","distributed computing","distributed deep learning","recognition","soft memory box","convolutional neural network","high performance computing","training data","memory management"]},{"p_id":4004,"title":"Palmprint gender classification by convolutional neural network","abstract":"Palmprint gender classification can revolutionise the performance of authentication systems, reduce searching space and speed up matching rate. However, to the best of their knowledge, there is no literature addressing this issue. The authors design a new convolutional neural network (CNN) structure, fine-tuning Visual Geometry Group Network, up to 19 layers to achieve a 20-layer network, for palmprint gender classification. Experimental results show that the proposed structure could achieve good performance for gender classification. They also investigate palmprint images with 15 different kinds of spectra. They empirically find that a palmprint image acquired by the Blue spectrum could achieve 89.2% correct classification and could be considered as a suitable spectrum for gender classification. The neural network is able to classify a 224x224x3-pixel palmprint image in <23ms, verifying that the proposed CNN is an effective real-time solution.","keywords_author":["palmprint recognition","neural nets","geometry","palmprint gender classification","convolutional neural network","CNN","fine-tuning visual geometry group network","Blue spectrum"],"keywords_other":["FUSION","PRINCIPAL COMPONENT ANALYSIS","RECOGNITION","IDENTIFICATION"],"max_cite":0.0,"pub_year":2018.0,"sources":"['ieee', 'wos']","rawkeys":["fine-tuning visual geometry group network","principal component analysis","identification","recognition","blue spectrum","geometry","palmprint gender classification","cnn","palmprint recognition","convolutional neural network","neural nets","fusion"],"tags":["fine-tuning visual geometry group network","principal component analysis","identification","recognition","geometry","blue spectrum","neural networks","palmprint gender classification","palmprint recognition","convolutional neural network","fusion"]},{"p_id":12196,"title":"Do deep convolutional neural networks really need to be deep when applied for remote scene classification?","abstract":"Deep convolutional neural networks (CNNs) have been widely used to obtain highlevel representation in various computer vision tasks. However, for remote scene classification, there are not sufficient images to train a very deep CNN from scratch. From two viewpoints of generalization power, we propose two promising kinds of deep CNNs for remote scenes and try to find whether deep CNNs need to be deep for remote scene classification. First, we transfer successful pretrained deep CNNs to remote scenes based on the theory that depth of CNNs brings the generalization power by learning available hypothesis for finite data samples. Second, according to the opposite viewpoint that generalization power of deep CNNs comes from massive memorization and shallow CNNs with enough neural nodes have perfect finite sample expressivity, we design a lightweight deep CNN (LDCNN) for remote scene classification. With five well-known pretrained deep CNNs, experimental results on two independent remote-sensing datasets demonstrate that transferred deep CNNs can achieve state-of-the-art results in an unsupervised setting. However, because of its shallow architecture, LDCNN cannot obtain satisfactory performance, regardless of whether in an unsupervised, semisupervised, or supervised setting. CNNs really need depth to obtain general features for remote scenes. This paper also provides baseline for applying deep CNNs to other remote sensing tasks. (C) 2017 Society of Photo-Optical Instrumentation Engineers (SPIE)","keywords_author":["convolutional neural network","remote sensing","scene classification","deep learning","generalization power"],"keywords_other":["SENSING IMAGERY","FEATURES","MODEL","RECOGNITION","T-SNE","MECHANISM","SCALE"],"max_cite":0.0,"pub_year":2017.0,"sources":"['wos']","rawkeys":["generalization power","model","recognition","mechanism","deep learning","features","remote sensing","t-sne","scene classification","convolutional neural network","scale","sensing imagery"],"tags":["generalization power","model","recognition","features","remote sensing","mechanisms","t-sne","machine learning","scene classification","sensed imagery","convolutional neural network","scale"]},{"p_id":12198,"title":"Transfer Learning Using Convolutional Neural Networks For Object Classification Within X-Ray Baggage Security Imagery","abstract":"We consider the use of transfer learning, via the use of deep Convolutional Neural Networks (CNN) for the image classification problem posed within the context of X-ray baggage security screening. The use of a deep multi-layer CNN approach, traditionally requires large amounts of training data, in order to facilitate construction of a complex complete endto-end feature extraction, representation and classification process. Within the context of X-ray security screening, limited availability of training for particular items of interest can thus pose a problem. To overcome this issue, we employ a transfer learning paradigm such that a pre-trained CNN, primarily trained for generalized image classification tasks where sufficient training data exists, can be specifically optimized as a later secondary process that targets specific this application domain. For the classical handgun detection problem we achieve 98.92% detection accuracy outperforming prior work in the field and furthermore extend our evaluation to a multiple object classification task within this context.","keywords_author":["Convolutional neural networks","transfer learning","image classification","baggage X - ray security"],"keywords_other":["RECOGNITION"],"max_cite":0.0,"pub_year":2016.0,"sources":"['ieee', 'wos']","rawkeys":["recognition","convolutional neural networks","transfer learning","image classification","baggage x - ray security"],"tags":["recognition","transfer learning","convolutional neural network","image classification","baggage x - ray security"]},{"p_id":12202,"title":"Direct Density Ratio Estimation with Convolutional Neural Networks with Application in Outlier Detection","abstract":"Recently, the ratio of probability density functions was demonstrated to be useful in solving various machine learning tasks such as outlier detection, non-stationarity adaptation, feature selection, and clustering. The key idea of this density ratio approach is that the ratio is directly estimated so that difficult density estimation is avoided. So far, parametric and non-parametric direct density ratio estimators with various loss functions have been developed, and the kernel least-squares method was demonstrated to be highly useful both in terms of accuracy and computational efficiency. On the other hand, recent study in pattern recognition exhibited that deep architectures such as a convolutional neural network can significantly outperform kernel methods. In this paper, we propose to use the convolutional neural network in density ratio estimation, and experimentally show that the proposed method tends to outperform the kernel-based method in outlying image detection.","keywords_author":["density ratio estimation","convolutional neural network","outlier detection"],"keywords_other":["RECOGNITION"],"max_cite":1.0,"pub_year":2015.0,"sources":"['wos']","rawkeys":["outlier detection","recognition","convolutional neural network","density ratio estimation"],"tags":["outlier detection","recognition","convolutional neural network","density ratio estimation"]},{"p_id":53163,"title":"Efficiency of classification methods based on empirical risk minimization","abstract":"A binary classification problem is reduced to the minimization of convex regularized empirical risk functionals in a reproducing kernel Hilbert space. The solution is searched for in the form of a finite linear combination of kernel support functions (Vapnik's support vector machines). Risk estimates for a misclassification as a function of the training sample size and other model parameters are obtained. \u00a9 2009 Springer Science+Business Media, Inc.","keywords_author":["Classification","Consistency","Empirical risk minimization","Machine learning","Rate of convergence","Recognition","Support vector machine (SVM)"],"keywords_other":["Rate of convergence","Recognition","Support vector machine (SVM)","Machine learning","Classification","Consistency","Empirical risk minimization"],"max_cite":0.0,"pub_year":2009.0,"sources":"['scp']","rawkeys":["recognition","machine learning","consistency","support vector machine (svm)","classification","empirical risk minimization","rate of convergence"],"tags":["recognition","machine learning","consistency","classification","empirical risk minimization","rate of convergence"]},{"p_id":85934,"title":"System recognizing Bahamian license plate with touching characters","abstract":"Various methods are proposed for license plate recognition, but none of them are universal. Some common methods for license plate localization, character extraction, and recognition are analyzed. Then a system is proposed to recognize the Bahamian license plate with touching characters. A vertical edge-based method with a modified sliding window technique is used to locate the license plate, and a machine learning process is used to trim the region. The located license plate is rectified by using the minimum enclosing box and the stroke width value. Then the vertical projection and pairs of extreme points are combined to segment the characters. Finally, a deep learning method is used to recognize the characters. 2996 images are experimented on and the total recognition accuracy achieves 83.29%. Typical methods of each stage are implemented to compare with the proposed methods. In addition, the proposed system is experimented on a public dataset to show the generalization ability of the system. The experimental results show that the proposed system performs better than the other methods and is able to be used in a real-time application. (C) 2016 SPIE and IS&T.","keywords_author":["license plate recognition","modified sliding window technique","license plate rectification","touching character segmentation"],"keywords_other":["BINARIZATION","LOCALIZATION","ALGORITHM","RECOGNITION","SEGMENTATION","NETWORK","IMAGES"],"max_cite":0.0,"pub_year":2016.0,"sources":"['wos']","rawkeys":["algorithm","network","recognition","images","localization","segmentation","modified sliding window technique","binarization","touching character segmentation","license plate rectification","license plate recognition"],"tags":["recognition","images","binarizations","localization","segmentation","modified sliding window technique","networks","touching character segmentation","algorithms","license plate rectification","license plate recognition"]},{"p_id":12207,"title":"Enhancing Hi-C data resolution with deep convolutional neural network HiCPlus","abstract":"Although Hi-C technology is one of the most popular tools for studying 3D genome organization, due to sequencing cost, the resolution of most Hi-C datasets are coarse and cannot be used to link distal regulatory elements to their target genes. Here we develop HiCPlus, a computational approach based on deep convolutional neural network, to infer high-resolution Hi-C interaction matrices from low-resolution Hi-C data. We demonstrate that HiCPlus can impute interaction matrices highly similar to the original ones, while only using 1\/16 of the original sequencing reads. We show that the models learned from one cell type can be applied to make predictions in other cell or tissue types. Our work not only provides a computational framework to enhance Hi-C data resolution but also reveals features underlying the formation of 3D chromatin interactions.","keywords_author":null,"keywords_other":["MAP","PRINCIPLES","MODEL","RECOGNITION","LIGATION","BIASES","CHROMATIN INTERACTIONS","BINDING","HUMAN GENOME","ARCHITECTURE"],"max_cite":0.0,"pub_year":2018.0,"sources":"['ieee', 'wos']","rawkeys":["biases","recognition","model","principles","map","human genome","ligation","binding","chromatin interactions","architecture"],"tags":["recognition","model","principles","map","epidemiology","human genome","ligation","binding","chromatin interactions","architecture"]},{"p_id":12210,"title":"Application of structured support vector machine backpropagation to a convolutional neural network for human pose estimation","abstract":"In this study, for the first time, we show how to formulate a structured support vector machine (SSVM) as two layers in a convolutional neural network, where the top layer is a loss augmented inference layer and the bottom layer is the normal convolutional layer. We show that a deformable part model can be learned with the proposed structured SVM neural network by backpropagating the error of the deformable part model to the convolutional neural network. The forward propagation calculates the loss augmented inference and the backpropagation calculates the gradient from the loss augmented inference layer to the convolutional layer. Thus, we obtain a new type of convolutional neural network called an Structured SVM convolutional neural network, which we applied to the human pose estimation problem. This new neural network can be used as the final layers in deep learning. Our method jointly learns the structural model parameters and the appearance model parameters. We implemented our method as a new layer in the existing Caffe library. (C) 2017 Elsevier Ltd. All rights reserved.","keywords_author":["Back propagation","Convolutional neural network","Deformable part model","Human pose estimation","Structured support vector machine"],"keywords_other":["RECOGNITION","PARTS"],"max_cite":0.0,"pub_year":2017.0,"sources":"['wos']","rawkeys":["human pose estimation","recognition","back propagation","structured support vector machine","parts","convolutional neural network","deformable part model"],"tags":["human pose estimations","recognition","structured support vector machine","parts","convolutional neural network","deformable part models","backpropagation"]},{"p_id":36787,"title":"Rolling bearing fault severity identification using deep sparse auto-encoder network with noise added sample expansion","abstract":"\u00a9 2017, \u00a9 IMechE 2017. This article presents a rolling bearing fault severity identification methodology that aims to adaptively extract fault severity features and intelligently identify the fault severity. The presented method is developed based on a deep sparse auto-encoder network trained with noise added sample expansion. A sparse auto-encoder is a learning algorithm that is capable of performing unsupervised learning of the inner structure and characters of source data. However, even though the original shallow structure of the sparse auto-encoders is capable of extracting the features, they do not have the capability for fault and severity classification. By stacking multiple sparse auto-encoders with a classifier layer, a deep sparse auto-encoder network with the ability of fault severity feature extraction and intelligent severity identification can be obtained. After unsupervised layer-wise self-learning and supervised fine-tuning, the designed deep sparse auto-encoder network can perform severity identification with automatically extracted fault features. Also, to overcome the issue of overfitting caused by limited number of training samples and stacked structure with numerous neurons and layers in a deep sparse auto-encoder network, Gaussian noises are added into the training samples to train the deep sparse auto-encoder network. Using deep sparse auto-encoder network for rolling bearing fault severity identification, the overfitting phenomenon can be retrained to increase the robustness of the network. The performance of the presented method is validated using bearing fault datasets that contain different levels of bearing severity and bearing life stages and compared with other methods. In comparison with recent solutions developed using deep learning approaches, the advantages of the presented method are that it does not require any pre-processing of the vibration data and provides a more robust diagnostic performance.","keywords_author":["deep learning","fault severity identification","noise added samples","Rolling bearing","sparse auto-encoder","Rolling bearing","fault severity identification","deep learning","sparse auto-encoder","noise added samples"],"keywords_other":["Rolling bearings","DIAGNOSIS","Stacked structure","Training sample","MACHINERY","AUTOENCODERS","Fault severities","RECOGNITION","Auto encoders","ENTROPY","Learning approach","Shallow structure","Robust diagnostics"],"max_cite":1.0,"pub_year":2017.0,"sources":"['scp', 'wos']","rawkeys":["diagnosis","training sample","recognition","sparse auto-encoder","noise added samples","deep learning","shallow structure","auto encoders","fault severities","autoencoders","stacked structure","rolling bearings","machinery","robust diagnostics","entropy","learning approach","fault severity identification","rolling bearing"],"tags":["diagnosis","training sample","recognition","noise added samples","shallow structure","auto encoders","machine learning","stacked autoencoders","fault severity","stacked structure","machinery","robust diagnostics","entropy","learning approach","fault severity identification","rolling bearing"]},{"p_id":85945,"title":"Subspace clustering using a low-rank constrained autoencoder","abstract":"The performance of subspace clustering is affected by data representation. Data representation for subspace clustering maps data from the original space into another space with the property of better separability. Many data representation methods have been developed in recent years. Typical among them are low-rank representation (LRR) and an autoencoder. LRR is a linear representation method that captures the global structure of data with low rank constraint. Alternatively, an autoencoder nonlinearly maps data into a latent space using a neural network by minimizing the difference between the reconstruction and input. To combine the advantages of an LRR (globality) and autoencoder (self-supervision based locality), we propose a novel data representation method for subspace clustering. The proposed method, called low-rank constrained autoencoder (LRAE), forces the latent representation of the neural network to be of low rank, and the low-rank constraint is computed as a prior from the input space. One major advantage of the LRAE is that the learned data representation not only maintains the local features of the data, but also preserves the underlying low-rank global structure. Extensive experiments on several datasets for subspace clustering were conducted. They demonstrated that the proposed LRAE substantially outperformed state-of-the-art subspace clustering methods. (C) 2017 Elsevier Inc. All rights reserved.","keywords_author":["Deep neural networks","Subspace clustering","Autoencoder","Low-rank representation"],"keywords_other":["NETWORKS","RECOGNITION"],"max_cite":0.0,"pub_year":2017.0,"sources":"['ieee', 'wos']","rawkeys":["recognition","deep neural networks","autoencoder","subspace clustering","low-rank representation","networks"],"tags":["recognition","auto encoders","subspace clustering","low-rank representation","networks","convolutional neural network"]},{"p_id":12218,"title":"Multi-National Banknote Classification Based on Visible-light Line Sensor and Convolutional Neural Network","abstract":"Automatic recognition of banknotes is applied in payment facilities, such as automated teller machines (ATMs) and banknote counters. Besides the popular approaches that focus on studying the methods applied to various individual types of currencies, there have been studies conducted on simultaneous classification of banknotes from multiple countries. However, their methods were conducted with limited numbers of banknote images, national currencies, and denominations. To address this issue, we propose a multi-national banknote classification method based on visible-light banknote images captured by a one-dimensional line sensor and classified by a convolutional neural network (CNN) considering the size information of each denomination. Experiments conducted on the combined banknote image database of six countries with 62 denominations gave a classification accuracy of 100%, and results show that our proposed algorithm outperforms previous methods.","keywords_author":["multi-national banknote classification","visible-light banknote images","one-dimensional line sensor","convolutional neural network"],"keywords_other":["RECOGNITION"],"max_cite":1.0,"pub_year":2017.0,"sources":"['wos']","rawkeys":["recognition","visible-light banknote images","multi-national banknote classification","one-dimensional line sensor","convolutional neural network"],"tags":["recognition","visible-light banknote images","multi-national banknote classification","one-dimensional line sensor","convolutional neural network"]},{"p_id":85948,"title":"SPARSE REPRESENTATION LEARNING OF DATA BY AUTOENCODERS WITH L-1\/2 REGULARIZATION","abstract":"Autoencoder networks have been demonstrated to be efficient for unsupervised learning of representation of images, documents and time series. Sparse representation can improve the interpretability of the input data and the generalization of a model by eliminating redundant features and extracting the latent structure of data. In this paper, we use L-1\/2 regularization method to enforce sparsity on the hidden representation of an autoencoder for achieving sparse representation of data. The performance of our approach in terms of unsupervised feature learning and supervised classification is assessed on the MNIST digit data set, the ORL face database and the Reuters-21578 text corpus. The results demonstrate that the proposed autoencoder can produce sparser representation and better reconstruction performance than the Sparse Autoencoder and the L-1 regularization Autoencoder. The new representation is also illustrated to be useful for a deep network to improve the classification performance.","keywords_author":["autoencoder","sparse representation","unsupervised feature learning","deep network","L-1\/2 regularization"],"keywords_other":["CONSTRAINTS","ATOMIC DECOMPOSITION","ALGORITHM","CONVERGENCE","NEURAL-NETWORKS","RECOGNITION","LASSO"],"max_cite":0.0,"pub_year":2018.0,"sources":"['wos']","rawkeys":["convergence","algorithm","neural-networks","constraints","recognition","atomic decomposition","unsupervised feature learning","sparse representation","autoencoder","lasso","deep network","l-1\/2 regularization"],"tags":["constraints","recognition","atomic decomposition","unsupervised feature learning","neural networks","sparse representation","auto encoders","lasso","l-1\/2 regularization","mathematics","algorithms","deep networks"]},{"p_id":4030,"title":"Cascaded Subpatch Networks for Effective CNNs","abstract":"Conventional convolutional neural networks use either a linear or a nonlinear filter to extract features from an image patch (region) of spatial size H x W (typically, H is small and is equal to W, e.g., H is 5 or 7). Generally, the size of the filter is equal to the size H x W of the input patch. We argue that the representational ability of equal-size strategy is not strong enough. To overcome the drawback, we propose to use subpatch filter whose spatial size h x w is smaller than H x W. The proposed subpatch filter consists of two subsequent filters. The first one is a linear filter of spatial size h x w and is aimed at extracting features from spatial domain. The second one is of spatial size 1 x 1 and is used for strengthening the connection between different input feature channels and for reducing the number of parameters. The subpatch filter convolves with the input patch and the resulting network is called a subpatch network. Taking the output of one subpatch network as input, we further repeat constructing subpatch networks until the output contains only one neuron in spatial domain. These subpatch networks form a new network called the cascaded subpatch network (CSNet). The feature layer generated by CSNet is called the csconv layer. For the whole input image, we construct a deep neural network by stacking a sequence of csconv layers. Experimental results on five benchmark data sets demonstrate the effectiveness and compactness of the proposed CSNet. For example, our CSNet reaches a test error of 5.68% on the CIFAR10 data set without model averaging. To the best of our knowledge, this is the best result ever obtained on the CIFAR10 data set.","keywords_author":["Cascaded subpatch networks (CSNet)","convolutional neural network (CNN)","feature extraction","subpatch filter"],"keywords_other":["RECOGNITION","DEEP"],"max_cite":1.0,"pub_year":2018.0,"sources":"['wos', 'ieee']","rawkeys":["recognition","deep","convolutional neural network (cnn)","cascaded subpatch networks (csnet)","feature extraction","subpatch filter"],"tags":["recognition","deep","cascaded subpatch networks (csnet)","feature extraction","convolutional neural network","subpatch filter"]},{"p_id":85959,"title":"Multilayer Convolutional Sparse Modeling: Pursuit and Dictionary Learning","abstract":"The recently proposed multilayer convolutional sparse coding (ML-CSC) model, consisting of a cascade of convolutional sparse layers, provides a new interpretation of convolutional neural networks (CNNs). Under this framework, the forward pass in a CNN is equivalent to a pursuit algorithm aiming to estimate the nested sparse representation vectors from a given input signal. Despite having served as a pivotal connection between CNNs and sparse modeling, a deeper understanding of the ML-CSC is still lacking. In this paper, we propose a sound pursuit algorithm for the ML-CSC model by adopting a projection approach. We provide new and improved bounds on the stability of the solution of such pursuit and we analyze different practical alternatives to implement this in practice. We show that the training of the filters is essential to allow for nontrivial signals in the model, and we derive an online algorithm to learn the dictionaries from real data, effectively resulting in cascaded sparse convolutional layers. Last, but not least, we demonstrate the applicability of the ML-CSC model for several applications in an unsupervised setting, providing competitive results. Our work represents a bridge between matrix factorization, sparse dictionary learning, and sparse autoencoders, and we analyze these connections in detail.","keywords_author":["Convolutional sparse coding","multilayer pursuit","convolutional neural networks","dictionary learning","sparse convolutional filters"],"keywords_other":["IMAGE","NETWORKS","REPRESENTATIONS","RECOGNITION","APPROXIMATIONS"],"max_cite":0.0,"pub_year":2018.0,"sources":"['ieee', 'wos']","rawkeys":["sparse convolutional filters","multilayer pursuit","recognition","convolutional neural networks","approximations","convolutional sparse coding","image","dictionary learning","networks","representations"],"tags":["sparse convolutional filters","multilayer pursuit","recognition","images","representation","convolutional sparse coding","approximation","networks","dictionary learning","convolutional neural network"]},{"p_id":45008,"title":"Gesture recognition with a low power FMCW radar and a deep convolutional neural network","abstract":"\u00a9 2017 European Microwave Association. Gesture recognition with radar enables remote control of consumer devices such as audio equipment, television sets and gaming consoles. In this paper, experimental results of hand gesture recognition with a low power FMCW radar and a deep convolutional neural network (CNN) are presented. The FMCW radar operates in the 24 GHz ISM frequency band and has an effective isotropic radiated power level of 0 dBm. Since low power consumption is a key aspect for application in consumer devices, the FMCW radar has only one receive channel which is different from other FMCW radars with multiple receive channels that have been described in literature. The recognition of gestures is performed with a deep convolutional neural network that is trained and tested with micro-Doppler spectrograms yielding excellent recognition performance in a simple test case consisting of 3 different gestures. A comparison of the training and test results for an amplitude spectrogram and a complex-valued spectrogram as the CNN input shows that in this test case there is no major benefit of using the phase information in the spectrogram.","keywords_author":["24 GHz","convolutional neural network","deep learning","FMCW","gesture","low power radar device","radar","recognition"],"keywords_other":["recognition","Low Power","FMCW","24 GHz","Convolutional neural network","gesture"],"max_cite":0.0,"pub_year":2018.0,"sources":"['scp']","rawkeys":["24 ghz","low power","fmcw","low power radar device","recognition","deep learning","convolutional neural network","gesture","radar"],"tags":["24 ghz","fmcw","low power radar device","recognition","machine learning","gestures","convolutional neural network","low power","radar"]},{"p_id":12241,"title":"DeepFix: A Fully Convolutional Neural Network for Predicting Human Eye Fixations","abstract":"Understanding and predicting the human visual attention mechanism is an active area of research in the fields of neuroscience and computer vision. In this paper, we propose DeepFix, a fully convolutional neural network, which models the bottom-up mechanism of visual attention via saliency prediction. Unlike classical works, which characterize the saliency map using various hand-crafted features, our model automatically learns features in a hierarchical fashion and predicts the saliency map in an end-to-end manner. DeepFix is designed to capture semantics at multiple scales while taking global context into account, by using network layers with very large receptive fields. Generally, fully convolutional nets are spatially invariant-this prevents them from modeling location-dependent patterns (e.g., centre-bias). Our network handles this by incorporating a novel location-biased convolutional layer. We evaluate our model on multiple challenging saliency data sets and show that it achieves the state-of-the-art results.","keywords_author":["convolutional neural network","deep learning","eye fixations","Saliency prediction","Saliency prediction","eye fixations","convolutional neural network","deep learning"],"keywords_other":["Modeling location","VISUAL-ATTENTION","Receptive fields","TOP-DOWN","Spatially invariants","BOTTOM-UP","Human visual attention","State of the art","MODEL","RECOGNITION","SALIENCY DETECTION","IMAGES","Convolutional neural network","Eye fixations","Visual Attention","OBJECT DETECTION"],"max_cite":10.0,"pub_year":2017.0,"sources":"['ieee', 'scp', 'wos']","rawkeys":["saliency prediction","spatially invariants","recognition","images","model","visual-attention","deep learning","bottom-up","human visual attention","modeling location","eye fixations","saliency detection","state of the art","object detection","convolutional neural network","receptive fields","visual attention","top-down"],"tags":["saliency prediction","spatially invariants","recognition","images","model","topdown","bottom-up","human visual attention","machine learning","eye fixations","modeling location","random forests","saliency detection","state of the art","object detection","convolutional neural network","visual attention"]},{"p_id":12242,"title":"Automated detection of arrhythmias using different intervals of tachycardia ECG segments with convolutional neural network","abstract":"Our cardiovascular system weakens and is more prone to arrhythmia as we age. An arrhythmia is an abnormal heartbeat rhythm which can be life-threatening. Atrial fibrillation (A(fib)), atrial flutter (A(fl)), and ventricular fibrillation (V-fib) are the recurring life-threatening arrhythmias that affect the elderly population. An electrocardiogram (ECG) is the principal diagnostic tool employed to record and interpret ECG signals. These signals contain information about the different types of arrhythmias. However, due to the complexity and non-linearity of ECG signals, it is difficult to manually analyze these signals. Moreover, the interpretation of ECG signals is subjective and might vary between the experts. Hence, a computer-aided diagnosis (CAD) system is proposed. The CAD system will ensure that the assessment of ECG signals is objective and accurate. In this work, we present a convolutional neural network (CNN) technique to automatically detect the different ECG segments. Our algorithm consists of an eleven-layer deep CNN with the output layer of four neurons, each representing the normal (N-sr), A(fib), A(fl), and V-fib ECG class. In this work, we have used ECG signals of two seconds and five seconds' durations without QRS detection. We achieved an accuracy, sensitivity, and specificity of 92.50%, 98.09%, and 93.13% respectively for two seconds of ECG segments. We obtained an accuracy of 94.90%, the sensitivity of 99.13%, and specificity of 81.44% for five seconds of ECG duration. This proposed algorithm can serve as an adjunct tool to assist clinicians in confirming their diagnosis. (C) 2017 Elsevier Inc. All rights reserved.","keywords_author":["Arrhythmia","Atrial fibrillation","Atrial flutter","Convolution neural network","Deep learning","Electrocardiogram signals","Ventricular fibrillation","Arrhythmia","Atrial fibrillation","Atrial flutter","Convolution neural network","Deep learning","Electrocardiogram signals","Ventricular fibrillation"],"keywords_other":["Arrhythmia","Ventricular fibrillation","CLASSIFICATION","Electrocardiogram signal","Atrial fibrillation","MODEL","RECOGNITION","Convolution neural network","Atrial flutter","IMAGES"],"max_cite":43.0,"pub_year":2017.0,"sources":"['ieee', 'scp', 'wos']","rawkeys":["recognition","images","model","convolution neural network","deep learning","atrial flutter","arrhythmia","ventricular fibrillation","classification","electrocardiogram signals","atrial fibrillation","electrocardiogram signal"],"tags":["recognition","images","model","machine learning","atrial flutter","arrhythmia","ventricular fibrillation","classification","convolutional neural network","atrial fibrillation","electrocardiogram signal"]},{"p_id":69586,"title":"Residual LSTM Attention Network for Object Tracking","abstract":"In this letter, we propose an attention network for object tracking. To construct the proposed attention network for sequential data, we combine long-short term memory (LSTM) and a residual framework into a residual LSTM (RLSTM). The LSTM, which learns temporal correlation, is used for a temporal learning of object tracking. In the proposed RLSTM method, the residual framework, which achieves the highest accuracy in ImageNet large scale visual recognition competition (ILSVRC) 2016, learns the variations of spatial inputs and thus achieves the spatio-temporal attention of the target object. Also, a rule-based RLSTM learning is used for robust attention. Experimental results on large tracking benchmark datasets object tracking benchmark (OTB)-2013, OTB-100, and OTB-50 show that the proposed RLSTM tracker achieves the highest performance among existing trackers including the Siamese trackers, attention trackers, and correlation trackers, and also has comparable performance with the state-of-the-art deep trackers.","keywords_author":["Attention network","attention tracker","deep tracker","object tracking","residual long-short term memory (RLSTM)","Siamese network","spatio-temporal attention","visual tracking"],"keywords_other":["RECOGNITION"],"max_cite":0.0,"pub_year":2018.0,"sources":"['wos']","rawkeys":["siamese network","attention network","recognition","object tracking","spatio-temporal attention","visual tracking","deep tracker","attention tracker","residual long-short term memory (rlstm)"],"tags":["siamese network","attention network","recognition","object tracking","spatio-temporal attention","visual tracking","deep tracker","attention tracker","residual long-short term memory (rlstm)"]},{"p_id":12244,"title":"A convolutional neural network approach to calibrating the rotation axis for X-ray computed tomography","abstract":"This paper presents an algorithm to calibrate the center-of-rotation for X-ray tomography by using a machine learning approach, the Convolutional Neural Network (CNN). The algorithm shows excellent accuracy from the evaluation of synthetic data with various noise ratios. It is further validated with experimental data of four different shale samples measured at the Advanced Photon Source and at the Swiss Light Source. The results are as good as those determined by visual inspection and show better robustness than conventional methods. CNN has also great potential for reducing or removing other artifacts caused by instrument instability, detector non-linearity, etc. An open-source toolbox, which integrates the CNN methods described in this paper, is freely available through GitHub at tomography\/xlearn and can be easily integrated into existing computational pipelines available at various synchrotron facilities. Source code, documentation and information on how to contribute are also provided.","keywords_author":["tomography reconstruction","rotation axis","convolutional neural network","open-source","Python"],"keywords_other":["RECOGNITION","ROBUST FACE DETECTION","REGISTRATION"],"max_cite":6.0,"pub_year":2017.0,"sources":"['wos']","rawkeys":["robust face detection","recognition","python","open-source","tomography reconstruction","convolutional neural network","registration","rotation axis"],"tags":["robust face detection","recognition","python","open sources","tomography reconstruction","convolutional neural network","registration","rotation axis"]},{"p_id":12246,"title":"Using Convolutional Neural Network Filters to Measure Left-Right Mirror Symmetry in Images","abstract":"We propose a method for measuring symmetry in images by using filter responses from Convolutional Neural Networks (CNNs). The aim of the method is to model human perception of left\/right symmetry as closely as possible. Using the Convolutional Neural Network (CNN) approach has two main advantages: First, CNN filter responses closely match the responses of neurons in the human visual system; they take information on color, edges and texture into account simultaneously. Second, we can measure higher-order symmetry, which relies not only on color, edges and texture, but also on the shapes and objects that are depicted in images. We validated our algorithm on a dataset of 300 music album covers, which were rated according to their symmetry by 20 human observers, and compared results with those from a previously proposed method. With our method, human perception of symmetry can be predicted with high accuracy. Moreover, we demonstrate that the inclusion of features from higher CNN layers, which encode more abstract image content, increases the performance further. In conclusion, we introduce a model of left\/right symmetry that closely models human perception of symmetry in CD album covers.","keywords_author":["symmetry perception","continuous symmetry","convolutional neural networks","aesthetics"],"keywords_other":["RECOGNITION"],"max_cite":2.0,"pub_year":2016.0,"sources":"['wos']","rawkeys":["recognition","convolutional neural networks","aesthetics","symmetry perception","continuous symmetry"],"tags":["recognition","aesthetics","symmetry perception","continuous symmetry","convolutional neural network"]},{"p_id":12247,"title":"A convolutional neural network VLSI architecture using thresholding and weight decomposition","abstract":"Hierarchical convolutional neural networks are a well-known robust image-recognition model. In order to apply this model to robot vision or various intelligent real-time vision systems, its VLSI implementation is essential. This paper proposes anew algorithm for reducing multiply-and-accumulation operation by thresholding in a projection field and by performing weight decomposition in a 2-D neuron array. We also propose a VLSI architecture based on the proposed algorithm, and estimate its operation performance.","keywords_author":null,"keywords_other":["RECOGNITION"],"max_cite":0.0,"pub_year":2004.0,"sources":"['wos']","rawkeys":["recognition"],"tags":["recognition"]},{"p_id":12255,"title":"Breast cancer diagnosis in DCE-MRI using mixture ensemble of convolutional neural networks","abstract":"This work addresses a novel computer-aided diagnosis (CAD) system in breast dynamic contrast-enhanced magnetic resonance imaging (DCE-MRI). The CAD system is designed based on a mixture ensemble of convolutional neural networks (ME-CNN) to discriminate between benign and malignant breast tumors. The ME-CNN is a modular and image-based ensemble, which can stochastically partition the high dimensional image space through simultaneous and competitive learning of its modules. The proposed system was assessed on our database of 112 DCE-MRI studies including solid breast masses, using a wide range of classification measures. The ME-CNN model composed of three CNN experts and one convolutional gating network achieves an accuracy of 96.39%, a sensitivity of 97.73% and a specificity of 94.87%. The experimental results also show that it has competitive classification performances compared to three existing single-classifier methods and two convolutional ensemble methods. The proposed ME CNN model could provide an effective tool for radiologists to analyse breast DCE-MRI images. (C) 2017 Elsevier Ltd. All rights reserved.","keywords_author":["Breast cancer","CAD systems","Convolutional neural networks","DCE-MRI","Mixture ensemble of experts","Breast cancer","DCE-MRI","Convolutional neural networks","Mixture ensemble of experts","CAD systems"],"keywords_other":["PREDICTION","Classification performance","MORPHOLOGY","Breast cancer diagnosis","FEATURES","Breast Cancer","Computer Aided Diagnosis(CAD)","COMPUTER-AIDED-DIAGNOSIS","DCE-MRI","CLASSIFICATION","CAD system","SEGMENTATION","RECOGNITION","Convolutional neural network","LESION","NUCLEI","Dynamic contrast enhanced magnetic resonance imaging","IMAGES"],"max_cite":8.0,"pub_year":2017.0,"sources":"['ieee', 'scp', 'wos']","rawkeys":["morphology","computer-aided-diagnosis","classification","convolutional neural network","computer aided diagnosis(cad)","mixture ensemble of experts","lesion","features","segmentation","cad system","nuclei","recognition","classification performance","convolutional neural networks","images","dynamic contrast enhanced magnetic resonance imaging","breast cancer diagnosis","breast cancer","dce-mri","prediction","cad systems"],"tags":["mixture ensemble of experts","recognition","classification performance","images","features","segmentation","computer-aided diagnosis","dce-mri","prediction","morphology","classification","convolutional neural network","lesions","cad system","breast cancer diagnosis","nuclei","breast cancer"]},{"p_id":12264,"title":"Adaptive pedestrian detection using convolutional neural network with dynamically adjusted classifier","abstract":"How to transfer the trained detector into the target scenarios has been an important topic for a long time in the field of computer vision. Unfortunately, most of the existing transfer methods need to keep source samples or label target samples in the detection phase. Therefore, they are difficult to apply to real applications. For this problem, we propose a framework that consists of a controlled convolutional neural network (CCNN) and a modulating neural network (MNN). In a CCNN, the parameters of the last layer, i.e., the classifier, are dynamically adjusted by a MNN. For each target sample, the CCNN adaptively generates a proprietary classifier. Our contributions include (1) the first detector-based unsupervised transfer method that is very suitable for real applications and (2) a new scheme of a dynamically adjusting classifier in which a new object function is invented. Experimental results confirm that our method can achieve state-of-the-art results on two pedestrian datasets. (C) 2017 SPIE and IS&T","keywords_author":["transferring","adaptive pedestrian detection","convolutional neural network","dynamical classifier"],"keywords_other":null,"max_cite":0.0,"pub_year":2017.0,"sources":"['wos']","rawkeys":["dynamical classifier","adaptive pedestrian detection","convolutional neural network","transferring"],"tags":["recognition","dynamical classifier","adaptive pedestrian detection","convolutional neural network"]},{"p_id":12267,"title":"Object detection and feature base learning with sparse convolutional neural networks","abstract":"A new convolutional neural network model termed sparse convolutional neural network (SCNN) is presented and its usefulness for real-time object detection in gray-valued, monocular video sequences is demonstrated. SCNNs are trained on \"raw\" gray values and are intended to perform feature selection as a part of regular neural network training. For this purpose, the learning rule is extended by an unsupervised component which performs a local nonlinear principal components analysis: in this way, meaningful and diverse properties can be computed from local image patches. The SCNN model can be used to train classifiers for different object classes which share a common first layer, i.e., a common preprocessing. This is of advantage since the information needs only to be calculated once for all classifiers. It is further demonstrated how SCNNs can be implemented by successive convolutions of the input image: scanning an image for objects at all possible locations is shown to be possible in real-time using this technique.","keywords_author":null,"keywords_other":["RECOGNITION","ALGORITHMS"],"max_cite":2.0,"pub_year":2006.0,"sources":"['wos']","rawkeys":["recognition","algorithms"],"tags":["recognition","algorithms"]},{"p_id":110585,"title":"Multi-Scale Residual Convolutional Neural Network for Haze Removal of Remote Sensing Images","abstract":"Haze removal is a pre-processing step that operates on at-sensor radiance data prior to the physically based image correction step to enhance hazy imagery visually. Most current haze removal methods focus on point-to-point operations and utilize information in the spectral domain, without taking consideration of the multi-scale spatial information of haze. In this paper, we propose a multi-scale residual convolutional neural network (MRCNN) for haze removal of remote sensing images. MRCNN utilizes 3D convolutional kernels to extract spatial-spectral correlation information and abstract features from surrounding neighborhoods for haze transmission estimation. It takes advantage of dilated convolution to aggregate multi-scale contextual information for the purpose of improving its prediction accuracy. Meanwhile, residual learning is utilized to avoid the loss of weak information while deepening the network. Our experiments indicate that MRCNN performs accurately, achieving an extremely low validation error and testing error. The haze removal results of several scenes of Landsat 8 Operational Land Imager (OLI) data show that the visibility of the dehazed images is significantly improved, and the color of recovered surface is consistent with the actual scene. Quantitative analysis proves that the dehazed results of MRCNN are superior to the traditional methods and other networks. Additionally, a comparison to haze-free data illustrates the spectral consistency after haze removal and reveals the changes in the vegetation index.","keywords_author":["haze removal","multi-scale context aggregation","residual learning","convolutional neural network","Landsat 8 OLI"],"keywords_other":["LAND-SURFACE IMAGERY","SATELLITE IMAGERY","VALIDATION","QUALITY","ALGORITHM","RECOGNITION","DETECT","ATMOSPHERIC CORRECTION","MULTISPECTRAL IMAGERY","SINGLE IMAGE"],"max_cite":0.0,"pub_year":2018.0,"sources":"['ieee', 'wos']","rawkeys":["algorithm","residual learning","quality","recognition","single image","detect","multi-scale context aggregation","landsat 8 oli","land-surface imagery","validation","multispectral imagery","atmospheric correction","convolutional neural network","satellite imagery","haze removal"],"tags":["residual learning","single images","quality","recognition","multi-scale context aggregation","landsat 8 oli","land-surface imagery","validation","multispectral imagery","detection","atmospheric correction","convolutional neural network","algorithms","satellite imagery","haze removal"]},{"p_id":12282,"title":"Convolutional neural network on three orthogonal planes for dynamic texture classification","abstract":"Dynamic Textures (DTs) are sequences of images of moving scenes that exhibit certain stationarity properties in time such as smoke, vegetation and fire. The analysis of DT is important for recognition, segmentation, synthesis or retrieval for a range of applications including surveillance, medical imaging and remote sensing. Convolutional Neural Networks (CNNs) have recently proven to be well suited for texture analysis with a design similar to filter banks. We develop a new DT analysis method based on a CNN method applied on three orthogonal planes. We train CNNs on spatial frames and temporal slices extracted from the DT sequences and combine their outputs to obtain a competitive DT classifier trained end-to-end. Our results on a wide range of commonly used DT classification benchmark datasets prove the robustness of our approach. Significant improvement of the state of the art is shown on the larger datasets. (C) 2017 Elsevier Ltd. All rights reserved.","keywords_author":["Dynamic texture","Image recognition","Convolutional neural network","Filter banks","Spatiotemporal analysis"],"keywords_other":["DATABASE","RECOGNITION","FEATURES"],"max_cite":0.0,"pub_year":2017.0,"sources":"['ieee', 'wos']","rawkeys":["spatiotemporal analysis","recognition","dynamic texture","features","filter banks","database","convolutional neural network","image recognition"],"tags":["spatiotemporal analysis","dynamic textures","recognition","features","databases","filter banks","convolutional neural network","image recognition"]},{"p_id":12285,"title":"EEG-based prediction of driver's cognitive performance by deep convolutional neural network","abstract":"We considered the prediction of driver's cognitive states related to driving performance using EEG signals. We proposed a novel channel-wise convolutional neural network (CCNN) whose architecture considers the unique characteristics of EEG data. We also discussed CCNN-R, a CCNN variation that uses Restricted Boltzmann Machine to replace the convolutional filter, and derived the detailed algorithm. To test the performance of CCNN and CCNN-R, we assembled a large EEG dataset from 3 studies of driver fatigue that includes samples from 37 subjects. Using this dataset, we investigated the new CCNN and CCNN-R on raw EEG data and also Independent Component Analysis (ICA) decomposition. We tested both within-subject and cross-subject predictions and the results showed CCNN and CCNN-R achieved robust and improved performance over conventional DNN and CNN as well as other non-DL algorithms. (C) 2016 Elsevier B.V. All rights reserved.","keywords_author":["Deep neural network","Convolutional neural network","Cognitive states"],"keywords_other":["DROWSINESS","RECOGNITION","INDEPENDENT COMPONENT ANALYSIS"],"max_cite":11.0,"pub_year":2016.0,"sources":"['ieee', 'wos']","rawkeys":["cognitive states","deep neural network","recognition","independent component analysis","convolutional neural network","drowsiness"],"tags":["recognition","independent component analysis","cognitive state","convolutional neural network","drowsiness"]},{"p_id":4096,"title":"Design of memristor-based image convolution calculation in convolutional neural network","abstract":"In this paper, an architecture based on memristors is proposed to implement image convolution computation in convolutional neural networks. This architecture could extract different features of input images when using different convolutional kernels. Bipolar memristors with threshold are employed in this work, which vary their conductance values under different voltages. Various kernels are needed to extract information of input images, while different kernels contain different weights. The memristances of bipolar memristors with threshold are convenient to be varied and kept, which make them suitable to act as the weights of kernels. The performances of the design are verified by simulation results.","keywords_author":["Memristor","Convolutional neural network","Image convolution computation"],"keywords_other":["RECOGNITION"],"max_cite":0.0,"pub_year":2016.0,"sources":"['ieee', 'wos']","rawkeys":["memristor","image convolution computation","recognition","convolutional neural network"],"tags":["recognition","image convolution computation","resistive switching memory","convolutional neural network"]},{"p_id":12298,"title":"Exploiting multi-channels deep convolutional neural networks for multivariate time series classification","abstract":"Time series classification is related to many different domains, such as health informatics, finance, and bioinformatics. Due to its broad applications, researchers have developed many algorithms for this kind of tasks, e.g., multivariate time series classification. Among the classification algorithms, k-nearest neighbor (k-NN) classification (particularly 1-NN) combined with dynamic time warping (DTW) achieves the state of the art performance. The deficiency is that when the data set grows large, the time consumption of 1-NN with DTW will be very expensive. In contrast to 1-NN with DTW, it is more efficient but less effective for feature-based classification methods since their performance usually depends on the quality of hand-crafted features. In this paper, we aim to improve the performance of traditional feature-based approaches through the feature learning techniques. Specifically, we propose a novel deep learning framework, multi-channels deep convolutional neural networks (MC-DCNN), for multivariate time series classification. This model first learns features from individual univariate time series in each channel, and combines information from all channels as feature representation at the final layer. Then, the learnt features are applied into a multilayer perceptron (MLP) for classification. Finally, the extensive experiments on real-world data sets show that our model is not only more efficient than the state of the art but also competitive in accuracy. This study implies that feature learning is worth to be investigated for the problem of time series classification.","keywords_author":["convolutional neural networks","deep learning","feature learning","time series classification","convolutional neural networks","time series classification","feature learning","deep learning"],"keywords_other":["Deep learning","Feature-based classification","Multivariate time series classifications","State-of-the-art performance","Feature based approaches","Time series classifications","RECOGNITION","Convolutional neural network","Feature learning"],"max_cite":8.0,"pub_year":2016.0,"sources":"['ieee', 'scp', 'wos']","rawkeys":["recognition","convolutional neural networks","feature based approaches","deep learning","multivariate time series classifications","time series classification","convolutional neural network","feature-based classification","feature learning","state-of-the-art performance","time series classifications"],"tags":["recognition","feature based approaches","multivariate time series classifications","machine learning","convolutional neural network","feature-based classification","feature learning","state-of-the-art performance","time series classifications"]},{"p_id":4113,"title":"Patch Autocorrelation Features: a translation and rotation invariant approach for image classification","abstract":"The autocorrelation is often used in signal processing as a tool for finding repeating patterns in a signal. In image processing, there are various image analysis techniques that use the autocorrelation of an image in a broad range of applications from texture analysis to grain density estimation. This paper provides an extensive review of two recently introduced and related frameworks for image representation based on autocorrelation, namely Patch Autocorrelation Features (PAF) and Translation and Rotation Invariant Patch Autocorrelation Features (TRIPAF). The PAF approach stores a set of features obtained by comparing pairs of patches from an image. More precisely, each feature is the euclidean distance between a particular pair of patches. The proposed approach is successfully evaluated in a series of handwritten digit recognition experiments on the popular MNIST data set. However, the PAF approach has limited applications, because it is not invariant to affine transformations. More recently, the PAF approach was extended to become invariant to image transformations, including (but not limited to) translation and rotation changes. In the TRIPAF framework, several features are extracted from each image patch. Based on these features, a vector of similarity values is computed between each pair of patches. Then, the similarity vectors are clustered together such that the spatial offset between the patches of each pair is roughly the same. Finally, the mean and the standard deviation of each similarity value are computed for each group of similarity vectors. These statistics are concatenated to obtain the TRIPAF feature vector. The TRIPAF vector essentially records information about the repeating patterns within an image at various spatial offsets. After presenting the two approaches, several optical character recognition and texture classification experiments are conducted to evaluate the two approaches. Results are reported on the MNIST (98.93%), the Brodatz (96.51%), and the UIUCTex (98.31%) data sets. Both PAF and TRIPAF are fast to compute and produce compact representations in practice, while reaching accuracy levels similar to other state-of-the-art methods.","keywords_author":["Patch Autocorrelation Features","Image autocorrelation","Patch-based method","Optical character recognition","Texture classification","Rotation invariant method","Translation invariant method","MNIST","Brodatz","UIUCTex"],"keywords_other":["HISTOGRAMS","SUPPORT VECTOR MACHINES","TEXTURE CLASSIFICATION","RECOGNITION","KERNELS"],"max_cite":0.0,"pub_year":2016.0,"sources":"['ieee', 'wos']","rawkeys":["patch autocorrelation features","image autocorrelation","recognition","mnist","histograms","kernels","uiuctex","brodatz","rotation invariant method","optical character recognition","support vector machines","patch-based method","texture classification","translation invariant method"],"tags":["patch autocorrelation features","image autocorrelation","recognition","mnist","histograms","machine learning","uiuctex","brodatz","kernel","rotation invariant method","optical character recognition","patch-based method","texture classification","translation invariant method"]},{"p_id":28691,"title":"SVC-based multivariate control charts for automatic anomaly detection in computer networks","abstract":"The design of multivariate control charts for automatic anomaly detection in computer networks is a challenging research issue due to the complexity of the data structure of the network operational data. In general, the design of statistical multivariate control charts is limited to a Gaussian distribution assumption or a pre-known probability distribution model, which is hardly applicable to the computer operation data. The paper is motivated by this timely need to develop SVC (support vector clustering) based multivariate control charts, which do not require the data to have a pre-known probability distribution model. The proposed method is validated through the simulations by comparing with the popularly used statistical T2 multivariate control charts. The effectiveness of the method is also demonstrated through automatic anomaly detection of typical computer intrusions. \u00a9 2007 IEEE.","keywords_author":["Control chart","Intrusion detection","Machine learning"],"keywords_other":["Operational data","Anomaly detections","international conferences","Multivariate control charts","probability distribution model","Computer operation","Autonomous System (AS)","Support vector clustering","General (CO)","(algorithmic) complexity","gaussian"],"max_cite":5.0,"pub_year":2007.0,"sources":"['scp']","rawkeys":["autonomous system (as)","operational data","control chart","gaussian","intrusion detection","international conferences","general (co)","machine learning","computer operation","support vector clustering","probability distribution model","multivariate control charts","(algorithmic) complexity","anomaly detections"],"tags":["recognition","operational data","anomaly detection","autonomous systems","control chart","intrusion detection systems","international conferences","complexity","machine learning","computer operation","gaussians","support vector clustering","probability distribution model","multivariate control charts"]},{"p_id":12310,"title":"An eye detection method based on convolutional neural networks and support vector machines","abstract":"Eye detection plays an important role in many fields, because eyes provide prominent facial feature information. However, changes in face pose, illumination variation, with glasses, and eye occlusions can make it difficult to detect eyes well from facial images. This paper proposes a hybrid model for eye detection. The model is an integration of two classifiers: Convolutional Neural Networks (CNN) and Support Vector Machines (SVM). In order to improve the speed of detection in the system, an eye variance filter (EVF) is constructed for eliminating most of noneye images to keep less candidate eye images. The CNN then works as a trainable feature extractor to explicitly extract various latent eye features. Finally, the trained SVM classifier is employed for eye verification instead of using the CNN classification function. Experiments applying the model have been conducted on the BioID, IMM, FERET and ORL face databases. Comparisons with other methods on the same databases indicate that this hybrid model has achieved a higher detection accuracy. Extensive experiments demonstrate the robustness and efficiency of our method by testing it on different facial images with varying eye conditions.","keywords_author":["Eye variance filter","convolutional neural networks","support vector machines","eye detection"],"keywords_other":["RECOGNITION","IMAGE","FACE","SYSTEM"],"max_cite":0.0,"pub_year":2018.0,"sources":"['wos']","rawkeys":["recognition","eye detection","convolutional neural networks","eye variance filter","system","image","support vector machines","face"],"tags":["recognition","images","eye detection","eye variance filter","machine learning","system","convolutional neural network","face"]},{"p_id":4119,"title":"Molecular graph convolutions: moving beyond fingerprints","abstract":"\u00a9 2016, Springer International Publishing Switzerland.Molecular \u201cfingerprints\u201d encoding structural information are the workhorse of cheminformatics and machine learning in drug discovery applications. However, fingerprint representations necessarily emphasize particular aspects of the molecular structure while ignoring others, rather than allowing the model to make data-driven decisions. We describe molecular graph convolutions, a machine learning architecture for learning from undirected graphs, specifically small molecules. Graph convolutions use a simple encoding of the molecular graph\u2014atoms, bonds, distances, etc.\u2014which allows the model to take greater advantage of information in the graph structure. Although graph convolutions do not outperform all fingerprint-based methods, they (along with other graph-based methods) represent a new paradigm in ligand-based virtual screening with exciting opportunities for future improvement.","keywords_author":["Artificial neural networks","Deep learning","Machine learning","Molecular descriptors","Virtual screening","Machine learning","Virtual screening","Deep learning","Artificial neural networks","Molecular descriptors"],"keywords_other":["Computer-Aided Design","Neural Networks (Computer)","Computer Graphics","Molecular Structure","Machine Learning","Pharmaceutical Preparations","RECOGNITION","SHAPE","NEURAL-NETWORK","Ligands","Drug Design"],"max_cite":41.0,"pub_year":2016.0,"sources":"['scp', 'wos', 'ieee']","rawkeys":["drug design","neural networks (computer)","recognition","computer-aided design","deep learning","molecular structure","ligands","machine learning","pharmaceutical preparations","virtual screening","shape","artificial neural networks","computer graphics","neural-network","molecular descriptors"],"tags":["drug design","recognition","features","neural networks","computer-aided diagnosis","molecular structure","machine learning","ligands","pharmaceutical preparations","virtual screening","shape","computer graphics"]},{"p_id":12311,"title":"Classification of CITES-listed and other neotropical Meliaceae wood images using convolutional neural networks","abstract":"Background: The current state-of-the-art for field wood identification to combat illegal logging relies on experienced practitioners using hand lenses, specialized identification keys, atlases of woods, and field manuals. Accumulation of this expertise is time-consuming and access to training is relatively rare compared to the international demand for field wood identification. A reliable, consistent and cost effective field screening method is necessary for effective global scale enforcement of international treaties such as the Convention on the International Trade in Endagered Species (CITES) or national laws (e.g. the US Lacey Act) governing timber trade and imports.","keywords_author":["Wood identification","Illegal logging","CITES","Forensic wood anatomy","Deep learning","Transfer learning","Convolutional neural networks"],"keywords_other":["RECOGNITION","TIMBER TRADE","IDENTIFICATION"],"max_cite":0.0,"pub_year":2018.0,"sources":"['wos']","rawkeys":["identification","convolutional neural networks","recognition","deep learning","forensic wood anatomy","illegal logging","transfer learning","wood identification","cites","timber trade"],"tags":["identification","recognition","transfer learning","forensic wood anatomy","illegal logging","machine learning","wood identification","convolutional neural network","cites","timber trade"]},{"p_id":12315,"title":"Machining vibration states monitoring based on image representation using convolutional neural networks","abstract":"Measured signals are usually fed into filters or signal decomposers to extract useful features to assist making identification in state monitoring or fault diagnosis. But what is routinely ignored is that an experienced expert can realize what is happening just by watching the signals presented on the oscilloscope even without the analyzing report. The vision image input and the experience feedback are the two keys in this identification process by the brain. The experience can be easily quantified, like 1 for \"good\" and 0 for \"bad\", and used for identification model construction, while there has been no attempt to use pictured signal as the model input. For closed-loop control system, it is necessary to acquire signal feedback point by point to adjust the system in real time. But for state monitoring and fault diagnosis, the pattern hiding among the signal points is usually more important, which is exactly one of the special fields of image representation to indicate complex interrelationship. Taking machining state monitoring as example, this paper explore the possibility to use the pictured signals as input to construct identification model without traditional feature engineering based on signal analysis. Convolutional neural networks (CNN) is introduced to connect pictured signals to different vibration states with experience feedback. Results validate the proposed method with excellent modeling performance. Time complexity analysis proves this pictured signal image representation based CNN method to be capable to be real-time. Two dimensional image representation is a powerful way to exhibit and fuse information. With high flexibility, the proposed method may be a promising framework for monitoring or fault diagnosis tasks. (C) 2017 Elsevier Ltd. All rights reserved.","keywords_author":["Intelligent manufacturing","State monitoring","Image representation","Convolutional neural networks"],"keywords_other":["DIAGNOSIS","TOOL","SUPPORT VECTOR MACHINE","WEAR","CLASSIFICATION","IDENTIFICATION","RECOGNITION","END MILLING PROCESS","ONLINE CHATTER DETECTION","WAVELET"],"max_cite":1.0,"pub_year":2017.0,"sources":"['ieee', 'wos']","rawkeys":["diagnosis","wear","online chatter detection","identification","convolutional neural networks","recognition","state monitoring","tool","wavelet","end milling process","classification","support vector machine","intelligent manufacturing","image representation"],"tags":["diagnosis","wear","online chatter detection","identification","recognition","state monitoring","machine learning","tool","wavelet","end milling process","classification","convolutional neural network","intelligent manufacturing","image representation"]},{"p_id":4125,"title":"Detecting Uyghur text in complex background images with convolutional neural network","abstract":"Uyghur text detection is crucial to a variety of real-world applications, while little researches put their attention on it. In this paper, we develop an effective and efficient region-based convolutional neural network for Uyghur text detection in complex background images. The characteristics of the network include: (1) Three region proposal networks are used to improve the recall, which simultaneously utilize feature maps from different convolutional layers. (2) The overall architecture of our network is in the form of fully convolutional network, and global average pooling is applied to replace the fully connected layers in the classification and bounding box regression layers. (3) To fully utilize the baseline information, Uyghur text lines are detected directly by the network in an end-to-end fashion. Experiment results on benchmark dataset show that our method achieves an F-measure of 0.83 and detection time of 0.6 s for each image in a single K20c GPU, which is much faster than the state-of-the-art methods while keeps competitive accuracy.","keywords_author":["Uyghur","Text detection","Text localization","Convolutional neural network"],"keywords_other":["LOCALIZATION","RECOGNITION","NATURAL IMAGES","LINE DETECTION"],"max_cite":0.0,"pub_year":2017.0,"sources":"['ieee', 'wos']","rawkeys":["text localization","recognition","localization","text detection","uyghur","convolutional neural network","line detection","natural images"],"tags":["text localization","recognition","localization","text detection","uyghur","convolutional neural network","line detection","natural images"]},{"p_id":12318,"title":"Cascade convolutional neural networks for automatic detection of thyroid nodules in ultrasound images","abstract":"Purpose: It is very important for calculation of clinical indices and diagnosis to detect thyroid nodules from ultrasound images. However, this task is a challenge mainly due to heterogeneous thyroid nodules with distinct components are similar to background in ultrasound images. In this study, we employ cascade deep convolutional neural networks (CNNs) to develop and evaluate a fully automatic detection of thyroid nodules from 2D ultrasound images.","keywords_author":["convolutional neural network","detection","feature extraction","segmentation","thyroid nodule","ultrasound image"],"keywords_other":["VALIDATION","FROC","OBSERVER","FEATURE-SELECTION","DELINEATION","CLASSIFICATION","ROC CURVE","RECOGNITION","AREA","PERFORMANCE"],"max_cite":2.0,"pub_year":2017.0,"sources":"['wos']","rawkeys":["recognition","delineation","performance","segmentation","froc","validation","feature-selection","classification","convolutional neural network","detection","feature extraction","area","observer","roc curve","thyroid nodule","ultrasound image"],"tags":["recognition","delineation","performance","segmentation","froc","validation","observers","feature selection","classification","convolutional neural network","detection","feature extraction","area","ultrasound images","roc curve","thyroid nodule"]},{"p_id":4128,"title":"Learning laparoscopic video shot classification for gynecological surgery","abstract":"\u00a9 2017, The Author(s). Videos of endoscopic surgery are used for education of medical experts, analysis in medical research, and documentation for everyday clinical life. Hand-crafted image descriptors lack the capabilities of a semantic classification of surgical actions and video shots of anatomical structures. In this work, we investigate how well single-frame convolutional neural networks (CNN) for semantic shot classification in gynecologic surgery work. Together with medical experts, we manually annotate hours of raw endoscopic gynecologic surgery videos showing endometriosis treatment and myoma resection of over 100 patients. The cleaned ground truth dataset comprises 9 h of annotated video material (from 111 different recordings). We use the well-known CNN architectures AlexNet and GoogLeNet and train these architectures for both, surgical actions and anatomy, from scratch. Furthermore, we extract high-level features from AlexNet with weights from a pre-trained model from the Caffe model zoo and feed them to an SVM classifier. Our evaluation shows that we reach an average recall of.697 and.515 for classification of anatomical structures and surgical actions respectively using off-the-shelf CNN features. Using GoogLeNet, we achieve a mean recall of.782 and.617 for classification of anatomical structures and surgical actions respectively. With AlexNet the achieved recall is.615 for anatomical structures and.469 for surgical action classification respectively. The main conclusion of our work is that advances in general image classification methods transfer to the domain of endoscopic surgery videos in gynecology. This is relevant as this domain is different from natural images, e.g. it is distinguished by smoke, reflections, or a limited amount of colors.","keywords_author":["Convolutional Neural Network","Deep learning","Video classification","Video classification","Deep learning","Convolutional Neural Network"],"keywords_other":["Action classifications","Classification methods","DEEP","Video classification","Anatomical structures","RECOGNITION","Video shot classification","TASKS","Convolutional neural network","Gynecologic surgeries","Semantic classification","CONVOLUTIONAL NEURAL-NETWORKS"],"max_cite":3.0,"pub_year":2018.0,"sources":"['wos', 'scp', 'ieee']","rawkeys":["recognition","classification methods","deep learning","deep","semantic classification","action classifications","anatomical structures","video shot classification","tasks","convolutional neural-networks","convolutional neural network","gynecologic surgeries","video classification"],"tags":["recognition","classification methods","semantic classification","deep","machine learning","action classifications","anatomical structures","video shot classification","convolutional neural network","gynecologic surgeries","task","video classification"]},{"p_id":61475,"title":"Theoretical Analysis of the Optimal Free Responses of Graph-Based SFA for the Design of Training Graphs","abstract":"Slow feature analysis (SFA) is an unsupervised learning algorithm that extracts slowly varying features from a multi-dimensional time series. Graph-based SFA (GSFA) is an extension to SFA for supervised learning that can be used to successfully solve regression problems if combined with a simple supervised post-processing step on a small number of slow features. The objective function of GSFA minimizes the squared output differences between pairs of samples specified by the edges of a structure called training graph. The edges of current training graphs, however, are derived only from the relative order of the labels. Exploiting the exact numerical value of the labels enables further improvements in label estimation accuracy.","keywords_author":["slow feature analysis","nonlinear regression","image analysis","pattern recognition","many classes"],"keywords_other":["SLOW FEATURE ANALYSIS","LAPLACIAN EIGENMAPS","RECOGNITION"],"max_cite":0.0,"pub_year":2016.0,"sources":"['wos']","rawkeys":["recognition","slow feature analysis","many classes","laplacian eigenmaps","pattern recognition","nonlinear regression","image analysis"],"tags":["non-linear regression","recognition","slow feature analysis","many classes","laplacian eigenmaps","pattern recognition","image analysis"]},{"p_id":4132,"title":"Deep multi-instance learning for end-to-end person re-identification","abstract":"In this paper, we introduce a deep multi-instance learning framework to boost the instance-level person re-identification performance. Motivated by the observation of considerably dramatic and complex varieties of visual appearances in many current person re-identification datasets, we explicitly represent a deep feature representation learning method for the final person re-identification task. However, most public datasets for person re-identification are usually small, that usually make deep learning model suffer from seriously over-fitting problem. To alleviate this matter, we formulate the problem of person re-identification as a deep multi-instance learning (DMIL) task. More specifically, We build a novel end-to-end person re-identification system by unifying DMIL with the convolutional feature learning. For well capturing these intra-class diversities and inter-class ambiguities of input visual samples across cameras, a multi-scale convolutional feature learning method is proposed by optimizing the Contrastive Loss function. Comprehensive evaluations over three public benchmark datasets (including VIPeR, ETHZ, and CUHK01 datasets) well demonstrate the encouraging performance of our proposed person re-identification framework on small datasets.","keywords_author":["Person re-identification","Deep multi-instance learning","Contrastive Loss function"],"keywords_other":["VERIFICATION","FEATURES","IMAGE CLASSIFICATION","RECOGNITION","RETRIEVAL"],"max_cite":0.0,"pub_year":2017.0,"sources":"['ieee', 'wos']","rawkeys":["recognition","features","deep multi-instance learning","person re-identification","verification","retrieval","contrastive loss function","image classification"],"tags":["recognition","features","deep multi-instance learning","person re-identification","verification","retrieval","contrastive loss function","image classification"]},{"p_id":12324,"title":"Image classification based on convolutional neural networks with cross-level strategy","abstract":"In the past few years, convolutional neural networks (CNNs) have exhibited great potential in the field of image classification. In this paper, we present a novel strategy named cross-level to improve the existing networks' architecture in which different levels of feature representation in a network are merely connected in series. The basic idea of cross-level is to establish a convolutional layer between two nonadjacent levels, aiming to extract more sufficient features with multiple scales at each feature representation level. The proposed cross-level strategy can be naturally integrated into an existing network without any change on its original architecture, which makes it very practical and convenient. Four popular convolutional networks for image classification are employed to illustrate its implementation in detail. Experimental results on the dataset adopted by the ImageNet Large-Scale Visual Recognition Challenge (ILSVRC) verify the effectiveness of the cross-level strategy on image classification. Furthermore, a new convolutional network with cross-level architecture is presented to demonstrate the potential of the proposed strategy in future network design.","keywords_author":["Convolutional neural networks (CNNs)","Deep learning","Feature representation","Image classification","Network architecture","Convolutional neural networks (CNNs)","Image classification","Network architecture","Feature representation","Deep learning"],"keywords_other":["Deep learning","Convolutional networks","Visual recognition","FEATURES","REPRESENTATION","Feature representation","Multiple scale","RECOGNITION","Convolutional neural network","SCALE","Future networks","Novel strategies"],"max_cite":0.0,"pub_year":2017.0,"sources":"['ieee', 'scp', 'wos']","rawkeys":["novel strategies","visual recognition","recognition","features","deep learning","network architecture","representation","convolutional networks","scale","feature representation","future networks","convolutional neural network","convolutional neural networks (cnns)","multiple scale","image classification"],"tags":["novel strategies","visual recognition","recognition","features","network architecture","machine learning","representation","scale","future networks","convolutional neural network","feature representation","multiple scale","image classification"]},{"p_id":61478,"title":"A sparse-response deep belief network based on rate distortion theory","abstract":"Deep belief networks (DBNs) are currently the dominant technique for modeling the architectural depth of brain, and can be trained efficiently in a greedy layer-wise unsupervised learning manner. However, DBNs without a narrow hidden bottleneck typically produce redundant, continuous-valued codes and unstructured weight patterns. Taking inspiration from rate distortion (RD) theory, which encodes original data using as few bits as possible, we introduce in this paper a variant of DBN, referred to as sparse-response DBN (SR-DBN). In this approach, Kullback-Leibler divergence between the distribution of data and the equilibrium distribution defined by the building block of DBN is considered as a distortion function, and the sparse response regularization induced by L-1-norm of codes is used to achieve a small code rate. Several experiments by extracting features from different scale image datasets show that our approach SR-DBN learns codes with small rate, extracts features at multiple levels of abstraction mimicking computations in the cortical hierarchy, and obtains more discriminative representation than PCA and several basic algorithms of DBNs. (C) 2014 Elsevier Ltd. All rights reserved.","keywords_author":["Deep belief network","Kullback-Leibler divergence","Information entropy","Rate distortion theory","Unsupervised feature learning"],"keywords_other":["V2","VISION","VISUAL-CORTEX","LEARN","NEURAL-NETWORKS","RECOGNITION","STIMULI"],"max_cite":30.0,"pub_year":2014.0,"sources":"['ieee', 'wos']","rawkeys":["neural-networks","recognition","unsupervised feature learning","visual-cortex","v2","learn","information entropy","vision","kullback-leibler divergence","deep belief network","rate distortion theory","stimuli"],"tags":["recognition","unsupervised feature learning","visual-cortex","neural networks","v2","machine learning","information entropy","vision","kullback-leibler divergence","rate-distortion theory","stimuli","deep belief networks"]},{"p_id":28711,"title":"Natural scene text detection with MC\u2013MR candidate extraction and coarse-to-fine filtering","abstract":"\u00a9 2017 Elsevier B.V.A novel natural scene text detection method is proposed in this paper. In the proposed method, first, we extract MSERs as text candidates with a proper multi-channel and multi-resolution Maximally Stable Extremal Regions (MC\u2013MR MSER) strategy. Then, we design a coarse-to-fine character classifier to discard false-positive candidates, where the coarse filter is based on morphological features and the fine filter is well-trained by convolutional neural network. Finally, text strings are formed with a graph model on detected characters. The proposed method is evaluated on ICDAR 2013 Robust Reading Competition benchmark database and the practical challenging multi-orientation scene text database (USTB) with standard rules. Experimental results show our method is efficient and effective. It achieves F-Score at 83.84% on ICDAR 2013 database and 51.15% on the more challenging USTB database, which are superior over several state-of-the-art text detection methods.","keywords_author":["Coarse-to-fine character classifier","Maximally stable extremal regions","Multi-channel and multi-resolution","Scene text detection","Scene text detection","Multi-channel and multi-resolution","Maximally stable extremal regions","Coarse-to-fine character classifier"],"keywords_other":["Benchmark database","Morphological features","State of the art","Multi channel","Scene Text","RECOGNITION","Convolutional neural network","Coarse to fine","Maximally Stable Extremal Regions"],"max_cite":5.0,"pub_year":2017.0,"sources":"['ieee', 'scp', 'wos']","rawkeys":["recognition","maximally stable extremal regions","multi-channel and multi-resolution","multi channel","state of the art","scene text detection","coarse to fine","convolutional neural network","scene text","coarse-to-fine character classifier","morphological features","benchmark database"],"tags":["recognition","maximally stable extremal regions","multi-channel and multi-resolution","state of the art","scene text detection","coarse to fine","convolutional neural network","multi-channel","scene text","coarse-to-fine character classifier","morphological features","benchmark database"]},{"p_id":12328,"title":"Kinetic Energy of Hydrocarbons as a Function of Electron Density and Convolutional Neural Networks","abstract":"We demonstrate a convolutional neural network trained to reproduce the Kohn-Sham kinetic energy of hydrocarbons from an input electron density. The output of the network is used as a nonlocal correction to conventional local and semilocal kinetic functionals. We show that this approximation qualitatively reproduces Kohn-Sham potential energy surfaces when used with conventional exchange correlation functionals. The density which minimizes the total energy given by the functional is examined in detail. We identify several avenues to improve on this exploratory work, by reducing numerical noise and changing the structure of our functional. Finally we examine the features in the density learned by the neural network to anticipate the prospects of generalizing these models.","keywords_author":null,"keywords_other":["MOLECULES","SIMULATIONS","SURFACES","APPROXIMATION","ORBITAL-FREE DFT","RECOGNITION","POTENTIALS","MECHANISM","EXCHANGE","ALUMINUM"],"max_cite":17.0,"pub_year":2016.0,"sources":"['wos']","rawkeys":["aluminum","recognition","surfaces","mechanism","potentials","simulations","molecules","approximation","exchange","orbital-free dft"],"tags":["aluminum","recognition","surfaces","potentials","mechanisms","molecules","approximation","exchange","orbital-free dft","simulation"]},{"p_id":12332,"title":"Convolutional neural network based amphibian sound classification using covariance and modulogram","abstract":"In this paper, a covariance matrix and modulogram are proposed for realizing amphibian sound classification using CNN (Convolutional Neural Network). First of all, a database is established by collecting amphibians sounds including endangered species in natural environment. In order to apply the database to CNN, it is necessary to standardize acoustic signals with different lengths. To standardize the acoustic signals, covariance matrix that gives distribution information and modulogram that contains the information about change over time are extracted and used as input to CNN. The experiment is conducted by varying the number of a convolutional layer and a fully-connected layer. For performance assessment, several conventional methods are considered representing various feature extraction and classification approaches. From the results, it is confirmed that convolutional layer has a greater impact on performance than the fully-connected layer. Also, the performance based on CNN shows attaining the highest recognition rate with 99.07 % among the considered methods.","keywords_author":["Acoustic event recognition","Environmental sound classificaiton","Deep learning","CNN (Convolutional Neural Network)"],"keywords_other":["RECOGNITION","FILTERBANK"],"max_cite":0.0,"pub_year":2018.0,"sources":"['wos']","rawkeys":["recognition","deep learning","cnn (convolutional neural network)","acoustic event recognition","filterbank","environmental sound classificaiton"],"tags":["recognition","filter banks","machine learning","convolutional neural network","acoustic event recognition","environmental sound classificaiton"]},{"p_id":61487,"title":"Editorial: Hierarchical Object Representations in the Visual Cortex and Computer Vision","abstract":null,"keywords_author":["computer model","neurophysiology","computer vision","visual cortex","computational neurosciences"],"keywords_other":["INFORMATION","FEEDFORWARD","ATTENTION","RECOGNITION","MODELS","ARCHITECTURE"],"max_cite":0.0,"pub_year":2015.0,"sources":"['ieee', 'wos']","rawkeys":["computational neurosciences","recognition","neurophysiology","feedforward","visual cortex","attention","computer model","information","models","computer vision","architecture"],"tags":["computational modeling","recognition","model","visual-cortex","computational neuroscience","neurophysiology","attention","feed-forward","information","computer vision","architecture"]},{"p_id":36914,"title":"Hand Tremor Based Biometric Recognition Using Leap Motion Device","abstract":"\u00a9 2013 IEEE. In this paper, the applicability of hand tremor-based biometric recognition via leap motion device is investigated. The hypothesis is that the hand tremor is unique for humans and can be utilized as a biometric identification. In order to verify our hypothesis, spatiotemporal hand tremor signals are acquired from subjects. The objective is to establish a live and secure identification system to avoid mimic and cloning of password by attackers. Various feature extraction methods, including statistical, fast Fourier transform, discrete wavelet transform, and 1-D local binary pattern are used. For evaluating recognition performance, Na\u00efve Bayes and Multi-Layer Perceptron are utilized as linear-simple and nonlinear-complex classifiers, respectively. Since the conducted experiments produced promising results (above 95% of classification accuracy rate), it is considered that the proposed approach has the potential to be used as a new biometric identification manner in the field of security.","keywords_author":["Authentication","biometrics","hand tremors","human computer interaction","leap motion","machine learning","neurophysiology","neuroscience","recognition","security"],"keywords_other":["Neuroscience","Leap Motion","Recognition","Hand tremors","Security"],"max_cite":1.0,"pub_year":2017.0,"sources":"['scp']","rawkeys":["neuroscience","recognition","leap motion","authentication","machine learning","human computer interaction","security","biometrics","neurophysiology","hand tremors"],"tags":["neuroscience","recognition","leap motion","authentication","machine learning","human-computer interaction","security","biometrics","neurophysiology","hand tremors"]},{"p_id":61491,"title":"Self-Organization of Spatio-Temporal Hierarchy via Learning of Dynamic Visual Image Patterns on Action Sequences","abstract":"It is well known that the visual cortex efficiently processes high-dimensional spatial information by using a hierarchical structure. Recently, computational models that were inspired by the spatial hierarchy of the visual cortex have shown remarkable performance in image recognition. Up to now, however, most biological and computational modeling studies have mainly focused on the spatial domain and do not discuss temporal domain processing of the visual cortex. Several studies on the visual cortex and other brain areas associated with motor control support that the brain also uses its hierarchical structure as a processing mechanism for temporal information. Based on the success of previous computational models using spatial hierarchy and temporal hierarchy observed in the brain, the current report introduces a novel neural network model for the recognition of dynamic visual image patterns based solely on the learning of exemplars. This model is characterized by the application of both spatial and temporal constraints on local neural activities, resulting in the self-organization of a spatio-temporal hierarchy necessary for the recognition of complex dynamic visual image patterns. The evaluation with the Weizmann dataset in recognition of a set of prototypical human movement patterns showed that the proposed model is significantly robust in recognizing dynamically occluded visual patterns compared to other baseline models. Furthermore, an evaluation test for the recognition of concatenated sequences of those prototypical movement patterns indicated that the model is endowed with a remarkable capability for the contextual recognition of long-range dynamic visual image patterns.","keywords_author":null,"keywords_other":["MECHANISMS","MOVEMENTS","NETWORKS","VISION","RECOGNITION","CORTEX","GRADIENT"],"max_cite":9.0,"pub_year":2015.0,"sources":"['ieee', 'wos']","rawkeys":["recognition","mechanisms","movements","vision","networks","gradient","cortex"],"tags":["recognition","mechanisms","movement","vision","networks","gradient","cortex"]},{"p_id":61498,"title":"Learning and surface boundary feedbacks for colour natural scene perception","abstract":"Boundary detection and segmentation are essential stages in object recognition and scene understanding. In this paper, we present a bio-inspired neural model of the ventral pathway for colour contour and surface perception, called LPREEN (Learning and Perceptual boundaRy rEcurrent dEtection Neural architecture). LPREEN models colour opponent processes and feedback interactions between cortical areas VI, V2, V4, and IT, which produce top-down and bottom-up information fusion. We suggest three feedback interactions that enhance and complete boundaries. Our proposed neural model contains a contour learning feedback that enhances the most probable contour positions in V1 according to a previous experience, and generates a surface perception in V4 through diffusion processes. We compared the proposed model with another bio-inspired model and two well-known contour extraction methods, using the Berkeley Segmentation Benchmark. LPREEN showed better performance than two methods and slightly worse performance than another one. (C) 2017 Elsevier B.V. All rights reserved.","keywords_author":["Computer vision","Contour learning","Boundary detection","Neural networks","Colour image processing","Bio-inspired models"],"keywords_other":["VISUAL-CORTEX","HIERARCHIES","ATTENTION","SEGREGATION","COMPLETION","RECOGNITION","NEURAL-NETWORK","DYNAMICS","ARCHITECTURE","IMAGES"],"max_cite":0.0,"pub_year":2017.0,"sources":"['ieee', 'wos']","rawkeys":["dynamics","recognition","images","visual-cortex","neural networks","colour image processing","hierarchies","segregation","boundary detection","completion","attention","bio-inspired models","neural-network","contour learning","computer vision","architecture"],"tags":["dynamics","recognition","images","visual-cortex","neural networks","colour image processing","segregation","boundary detection","completion","attention","bio-inspired models","contour learning","computer vision","hierarchy","architecture"]},{"p_id":118843,"title":"Survey of Image Segmentation Algorithm for Medical Images: Challenges and Methodologies","abstract":"Due to the tremendous increase in the usage of computer technologies, image-processing techniques have become one among the most important and rapidly used technique in a wide variety of applications. The basic idea of the medical images is to improve the different imaging content with respect to the images. Image segmentation is the basic step for any image processing technique in medical field. Hence several image segmentation techniques were introduced to segment an image before recognition which is the only small part that is more useful out of the whole image. A typical and medical imaging system is composed of four main processing steps such as image acquisition, segmentation, feature extraction and classification. In this paper the survey on existing medical image segmentation algorithms is given. In addition the paper addressed several challenges of the existing medical image segmentation techniques that a researcher can face during the implementation and outline of the strength and weaknesses of the existing segmentation algorithm.","keywords_author":["Image processing","segmentation","recognition","feature extraction","classification"],"keywords_other":["MR-IMAGES","FEATURES","TUMOR","MAGNETIC-RESONANCE IMAGES","BRAIN"],"max_cite":0.0,"pub_year":2017.0,"sources":"['wos']","rawkeys":["recognition","segmentation","features","image processing","tumor","brain","mr-images","classification","feature extraction","magnetic-resonance images"],"tags":["recognition","segmentation","features","image processing","tumor","brain","mr-images","classification","feature extraction","magnetic resonance imaging"]},{"p_id":4156,"title":"Capturing Temporal Structures for Video Captioning by Spatio-temporal Contexts and Channel Attention Mechanism","abstract":"To generate a natural language description for videos, there has been tremendous interest in developing deep neural networks with the integration of temporal structures in different categories. Considering the spatial and temporal domains inherent in video frames, we contend that the video dynamics and the spatio-temporal contexts are both important for captioning, which correspond to two different temporal structures. However, while the video dynamics is well investigated, the spatio-temporal contexts have not been given sufficient attention. In this paper, we take both structures into account and propose a novel recurrent convolution model for captioning. Firstly, for a comprehensive and detailed representation, we propose to aggregate the local and global spatio-temporal contexts in the recurrent convolution networks. Secondly, to capture much subtler temporal dynamics, the channel attention mechanism is introduced and it helps to understand the involvement of the frame feature maps with the captioning process. Finally, a qualitative comparison with several variants of our model demonstrates the effectiveness of incorporating these two structures. Moreover, experiments on YouTube2Text dataset have shown that the proposed method achieves competitive performance to other state-of-the-art methods.","keywords_author":["Video captioning","Recurrent convolution networks","Spatio-temporal contexts","Channel attention mechanism"],"keywords_other":["RECOGNITION"],"max_cite":0.0,"pub_year":2017.0,"sources":"['wos', 'ieee']","rawkeys":["recurrent convolution networks","recognition","channel attention mechanism","spatio-temporal contexts","video captioning"],"tags":["recurrent convolution networks","spatio-temporal context","recognition","channel attention mechanism","video captioning"]},{"p_id":12355,"title":"Toward Fast and Accurate Vehicle Detection in Aerial Images Using Coupled Region-Based Convolutional Neural Networks","abstract":"Vehicle detection in aerial images, being an interesting but challenging problem, plays an important role for a wide range of applications. Traditional methods are based on sliding-window search and handcrafted or shallow-learning-based features with heavy computational costs and limited representation power. Recently, deep learning algorithms, especially region-based convolutional neural networks (R-CNNs), have achieved state-of-the-art detection performance in computer vision. However, several challenges limit the applications of R-CNNs in vehicle detection from aerial images: 1) vehicles in large-scale aerial images are relatively small in size, and R-CNNs have poor localization performance with small objects; 2) R-CNNs are particularly designed for detecting the bounding box of the targets without extracting attributes; 3) manual annotation is generally expensive and the available manual annotation of vehicles for training R-CNNs are not sufficient in number. To address these problems, this paper proposes a fast and accurate vehicle detection framework. On one hand, to accurately extract vehicle-like targets, we developed an accurate-vehicle-proposal-network (AVPN) based on hyper feature map which combines hierarchical feature maps that are more accurate for small object detection. On the other hand, we propose a coupled R-CNN method, which combines an AVPN and a vehicle attribute learning network to extract the vehicle's location and attributes simultaneously. For original large-scale aerial images with limited manual annotations, we use cropped image blocks for training with data augmentation to avoid overfitting. Comprehensive evaluations on the public Munich vehicle dataset and the collected vehicle dataset demonstrate the accuracy and effectiveness of the proposed method.","keywords_author":["Attribute learning","convolutional neural networks","vehicle detection","vehicle proposal network"],"keywords_other":["REMOTE-SENSING IMAGES","FEATURES","CARS","CLASSIFICATION","RECOGNITION","SATELLITE IMAGES","UAV IMAGES","OBJECT DETECTION"],"max_cite":10.0,"pub_year":2017.0,"sources":"['ieee', 'wos']","rawkeys":["recognition","convolutional neural networks","attribute learning","features","satellite images","cars","remote-sensing images","classification","object detection","uav images","vehicle detection","vehicle proposal network"],"tags":["recognition","attribute learning","features","satellite images","vehicle detection","uav images","classification","convolutional neural network","object detection","car","remote sensing images","vehicle proposal network"]},{"p_id":61508,"title":"Hierarchical Latent Concept Discovery for Video Event Detection","abstract":"Semantic information is important for video event detection. How to automatically discover, model, and utilize semantic information to facilitate video event detection has been a challenging problem. In this paper, we propose a novel hierarchical video event detection model, which deliberately unifies the processes of underlying semantics discovery and event modeling from video data. Specially, different from most of the approaches based on manually pre-defined concepts, we devise an effective model to automatically uncover video semantics by hierarchically capturing latent static-visual concepts in frame-level and latent activity concepts (i.e., temporal sequence relationships of static-visual concepts) in segment-level. The unified model not only enables a discriminative and descriptive representation for videos, but also alleviates error propagation problem from video representation to event modeling existing in previous methods. A max-margin framework is employed to learn the model. Extensive experiments on four challenging video event datasets, i.e., MED11, CCV, UQE50, and FCVID, have been conducted to demonstrate the effectiveness of the proposed method.","keywords_author":["Event detection","latent concepts","semantic information"],"keywords_other":["MODEL","RECOGNITION","CLASSIFICATION"],"max_cite":2.0,"pub_year":2017.0,"sources":"['ieee', 'wos']","rawkeys":["latent concepts","recognition","model","classification","semantic information","event detection"],"tags":["latent concepts","recognition","model","classification","semantic information","event detection"]},{"p_id":12357,"title":"A Joint Convolutional Neural Networks and Context Transfer for Street Scenes Labeling","abstract":"Street scene understanding is an essential task for autonomous driving. One important step toward this direction is scene labeling, which annotates each pixel in the images with a correct class label. Although many approaches have been developed, there are still some weak points. First, many methods are based on the hand-crafted features whose image representation ability is limited. Second, they cannot label foreground objects accurately due to the data set bias. Third, in the refinement stage, the traditional Markov random filed inference is prone to over smoothness. For improving the above problems, this paper proposes a joint method of priori convolutional neural networks at superpixel level (called as \"priori s-CNNs\") and soft restricted context transfer. Our contributions are threefold: 1) a priori s-CNNs model that learns priori location information at superpixel level is proposed to describe various objects discriminatingly; 2) a hierarchical data augmentation method is presented to alleviate data set bias in the priori s-CNNs training stage, which improves foreground objects labeling significantly; and 3) a soft restricted MRF energy function is defined to improve the priori s-CNNs model's labeling performance and reduce the over smoothness at the same time. The proposed approach is verified on CamVid data set (11 classes) and SIFT Flow Street data set (16 classes) and achieves a competitive performance.","keywords_author":["convolutional neural networks","data augmentation","deep learning","label transfer","Scene labeling","street scenes","Scene labeling","convolutional neural networks","deep learning","label transfer","street scenes","data augmentation"],"keywords_other":["GRAPH CUTS","Three-dimensional display","Computational model","SEMANTIC SEGMENTATION","Competitive performance","Data augmentation","Street scene understanding","Image representations","RECOGNITION","Location information","Convolutional neural network","ENERGY MINIMIZATION"],"max_cite":10.0,"pub_year":2018.0,"sources":"['ieee', 'scp', 'wos']","rawkeys":["competitive performance","three-dimensional display","recognition","convolutional neural networks","energy minimization","deep learning","location information","street scene understanding","data augmentation","scene labeling","street scenes","convolutional neural network","semantic segmentation","graph cuts","computational model","image representations","label transfer"],"tags":["competitive performance","computational modeling","recognition","energy minimization","location information","street scene understanding","machine learning","data augmentation","label transfer","scene labeling","street scenes","three-dimensional displays","convolutional neural network","semantic segmentation","graph cuts","image representation"]},{"p_id":118853,"title":"A Multiscale Deeply Described Correlatons-Based Model for Land-Use Scene Classification","abstract":"Research efforts in land-use scene classification is growing alongside the popular use of High-Resolution Satellite (HRS) images. The complex background and multiple land-cover classes or objects, however, make the classification tasks difficult and challenging. This article presents a Multiscale Deeply Described Correlatons (MDDC)-based algorithm which incorporates appearance and spatial information jointly at multiple scales for land-use scene classification to tackle these problems. Specifically, we introduce a convolutional neural network to learn and characterize the dense convolutional descriptors at different scales. The resulting multiscale descriptors are used to generate visual words by a general mapping strategy and produce multiscale correlograms of visual words. Then, an adaptive vector quantization of multiscale correlograms, termed multiscale correlatons, are applied to encode the spatial arrangement of visual words at different scales. Experiments with two publicly available land-use scene datasets demonstrate that our MDDC model is discriminative for efficient representation of land-use scene images, and achieves competitive classification results with state-of-the-art methods.","keywords_author":["convolutional neural network","spatial information","multiple scales","feature representation"],"keywords_other":["VISUAL-WORDS","FEATURES","NETWORKS","RECOGNITION","REMOTE-SENSING IMAGERY","LEVEL"],"max_cite":3.0,"pub_year":2017.0,"sources":"['wos']","rawkeys":["remote-sensing imagery","recognition","features","level","visual-words","multiple scales","spatial information","networks","convolutional neural network","feature representation"],"tags":["remote-sensing imagery","recognition","features","level","visual word","networks","convolutional neural network","feature representation","multiple scale","spatial informations"]},{"p_id":118859,"title":"Land use and land cover classification for rural residential areas in China using soft-probability cascading of multifeatures","abstract":"A multifeature soft-probability cascading scheme to solve the problem of land use and land cover (LULC) classification using high-spatial-resolution images to map rural residential areas in China is proposed. The proposed method is used to build midlevel LULC features. Local features are frequently considered as low-level feature descriptors in a midlevel feature learning method. However, spectral and textural features, which are very effective low-level features, are neglected. The acquisition of the dictionary of sparse coding is unsupervised, and this phenomenon reduces the discriminative power of the midlevel feature. Thus, we propose to learn supervised features based on sparse coding, a support vector machine (SVM) classifier, and a conditional random field (CRF) model to utilize the different effective low-level features and improve the discriminability of midlevel feature descriptors. First, three kinds of typical low-level features, namely, dense scale-invariant feature transform, gray-level co-occurrence matrix, and spectral features, are extracted separately. Second, combined with sparse coding and the SVM classifier, the probabilities of the different LULC classes are inferred to build supervised feature descriptors. Finally, the CRF model, which consists of two parts: unary potential and pairwise potential, is employed to construct an LULC classification map. Experimental results show that the proposed classification scheme can achieve impressive performance when the total accuracy reached about 87%. (C) The Authors. Published by SPIE under a Creative Commons Attribution 3.0 Unported License. Distribution or reproduction of this work in whole or in part requires full attribution of the original publication, including its DOI.","keywords_author":["high-spatial-resolution remote sensing","image classification","midlevel feature learning","conditional random field"],"keywords_other":["MARKOV RANDOM-FIELD","FEATURES","SYSTEMS","MODEL","RECOGNITION","SEGMENTATION","REMOTE-SENSING IMAGERY","RESOLUTION","SCENE CLASSIFICATION","URBAN"],"max_cite":0.0,"pub_year":2017.0,"sources":"['wos']","rawkeys":["remote-sensing imagery","recognition","model","segmentation","features","urban","conditional random field","scene classification","markov random-field","high-spatial-resolution remote sensing","systems","image classification","resolution","midlevel feature learning"],"tags":["markov random fields","remote-sensing imagery","recognition","model","segmentation","features","urban","conditional random field","scene classification","system","high-spatial-resolution remote sensing","image classification","resolution","midlevel feature learning"]},{"p_id":4190,"title":"Face verification based on deep Bayesian convolutional neural network in unconstrained environment","abstract":"Unconstrained face verification aims to verify whether two specify images contain the same person. In this paper, we propose a deep Bayesian convolutional neural network (DBCNN) framework to extract facial features and measure their similarity for face verification in unconstrained conditions. Specifically, we design a deep convolutional neural network and construct a Bayesian probabilistic model by transferring the Bayesian likelihood ratio function into linear decision function. By training a decision line rather than finding a suitable threshold, we further enlarge the distances between inter-class and intra-class in unconstrained environment. Finally, we comprehensively evaluate our method on LFW, CACD-VS and MegaFace datasets. The test results on LFW and CACD-VS datasets show that our method can shrink intra-class variations significantly. The performance of our DBCNN model on MegaFace dataset proves that our model can achieve comparable performance to state-of-the-art methods on face verification with relative small training data and only one single network.","keywords_author":["Face verification","Bayesian inference","Convolutional neural network","Intra-class variations"],"keywords_other":["RECOGNITION"],"max_cite":0.0,"pub_year":2017.0,"sources":"['ieee', 'wos']","rawkeys":["intra-class variations","recognition","bayesian inference","face verification","convolutional neural network"],"tags":["recognition","bayesian inference","face verification","convolutional neural network","intra-class variation"]},{"p_id":36959,"title":"Recognition of multiple plant leaf diseases based on improved convolutional neural network","abstract":"\u00a9 2017, Editorial Department of the Transactions of the Chinese Society of Agricultural Engineering. All right reserved.Plant leaf diseases are a serious problem in agricultural production. To solve this problem and prevent diseases deterioration, accurate identification of diseases types is of great significance. In this paper, we proposed a recognition model of plant leaf diseases based on convolutional neural network (CNN), which combines the batch normalization and global pooling methods. The parameters of the traditional CNN model are large and have difficulty to converge. The proposed model was modified in the traditional structure of the CNN, which could optimize the training time and achieve the higher accuracy, and also reduce the size of model. In order to speed up the training convergence, we used the batch normalization layers. We put the input of every convolutional layer in batch, calculated the mean and variance of the batch, and then normalized this batch. We reduced some feature maps of some layers and removed the last full connect layer, with the global pooling layer instead. The proposed model has 5 convolutional layers and 4 pooling layers. In the last pooling layer pool5, the same kernel size of convolutional layer Conv5 was used to take advantage of the information of Conv5's feature map comprehensively. For the image preprocessing, we had zoomed, flipped and rotated the original pictures of dataset randomly to get the augmented dataset, and used the 80% of pictures as the train dataset and the rest as the test dataset. These pictures were quantized to 256\u00d7256 dpi for CNN training, and the original dataset and augmented dataset were used to train models. To look for the best size of the first layer kernel, in the first convolutional layer, different kernel sizes i.e. 11\u00d711, 9\u00d79 and 7\u00d77 dpi were used respectively. Furthermore, we chose the type of global pooling layer, like max pooling and average pooling. Then we designed 8 models with different Conv1 kernel sizes or global pooling types. To further improve the efficiency of this model, besides using the Gaussian initialization, we used the other common type of convolutional initialization such as Xavier initialization, and also used the PRelu activation function for each convolution layer. So the optimal model could be selected to recognize the 26 kinds of leaf diseases which involved 14 kinds of plants, and then we analyzed the model's convergence rate, memory usage and robustness. After the experiment, we compared the test accuracy between the traditional model and the proposed model based on original dataset and augmented dataset. The proposed model could accelerate the training convergence, and the test accuracy could achieve about 90% while the traditional model was only about 77% after 3 training epochs. Different kernel sizes of Conv1 had little impact on the accuracy but small kernel was proved to be more beneficial to the recognition of plant diseases, which could get more texture features than the big kernel size filter, and average pooling also made better results than max pooling. We got the best performance model which used the 9\u00d79 dpi kernel size and global average pooling layer. To show the proposed model's performance, we tested the accuracy on each class, and the mean accuracy of augmented test dataset was 99.56%, and the weighted average score of recall and precision rate achieved 99.41%. The proposed model had the size of only 2.6 MB. In addition, compared with the traditional methods, the change of the spatial position of the pictures had little effect on the performance of the improved model, and the proposed model could identify different diseases of various plant leaves. The results show that the model has higher recognition accuracy and stronger robustness, and can be used for the identification of plant leaf diseases.","keywords_author":["Batch normalization","Convolutional neural network","Deep learning","Diseases","Global pooling","Image processing","Plants","Recognition"],"keywords_other":["Recognition","Plants","Convolutional neural network","Batch normalization","Global pooling"],"max_cite":1.0,"pub_year":2017.0,"sources":"['scp']","rawkeys":["recognition","deep learning","image processing","plants","global pooling","batch normalization","convolutional neural network","diseases"],"tags":["recognition","image processing","disease","machine learning","global pooling","convolutional neural network","bayesian networks","biological"]},{"p_id":12384,"title":"Real-Time Motor Fault Detection by 1-D Convolutional Neural Networks","abstract":"Early detection of the motor faults is essential and artificial neural networks are widely used for this purpose. The typical systems usually encapsulate two distinct blocks: feature extraction and classification. Such fixed and hand-crafted features may be a suboptimal choice and require a significant computational cost that will prevent their usage for real-time applications. In this paper, we propose a fast and accurate motor condition monitoring and early fault-detection system using 1-D convolutional neural networks that has an inherent adaptive design to fuse the feature extraction and classification phases of the motor fault detection into a single learning body. The proposed approach is directly applicable to the raw data (signal), and, thus, eliminates the need for a separate feature extraction algorithm resulting in more efficient systems in terms of both speed and hardware. Experimental results obtained using real motor data demonstrate the effectiveness of the proposed method for real-time motor condition monitoring.","keywords_author":["Convolutional neural networks (CNNs)","motor current signature analysis (MCSA)","Convolutional neural networks (CNNs)","motor current signature analysis (MCSA)"],"keywords_other":["DIAGNOSIS","Real-time application","Feature extraction and classification","Feature extraction algorithms","WAVELET PACKET DECOMPOSITION","SIGNAL","BEARING DAMAGE DETECTION","Sub-optimal choices","INDUCTION-MOTOR","MODEL","Adaptive designs","RECOGNITION","SENSORLESS","Convolutional neural network","Motor current signature analysis","Computational costs","STATOR CURRENT","MACHINES"],"max_cite":43.0,"pub_year":2016.0,"sources":"['ieee', 'scp', 'wos']","rawkeys":["sub-optimal choices","motor current signature analysis (mcsa)","computational costs","convolutional neural network","bearing damage detection","feature extraction and classification","convolutional neural networks (cnns)","motor current signature analysis","machines","wavelet packet decomposition","diagnosis","recognition","sensorless","real-time application","stator current","feature extraction algorithms","signal","model","induction-motor","adaptive designs"],"tags":["diagnosis","signals","induction motor","motor current signature analysis","recognition","model","sensorless","stator current","sub-optimal choices","real-time application","wavelet packet decomposition","machine","computational costs","convolutional neural network","bearing damage detection","feature extraction and classification","adaptive designs","feature extraction algorithms"]},{"p_id":12385,"title":"Unsupervised feature selection for multi-class object detection using convolutional neural networks","abstract":"Convolutional Neural Networks (CNN) have proven to be useful tools for object detection and object recognition. They act like feature extractor and classifier at the same time. In this study we present an unsupervised feature selection procedure for constructing a training set for the CNN and analyze in detail the learnt receptive fields. We then introduce, for the first time, a figural alphabet to be used for low-level feature detection with CNN. This alphabet turned out to be useful in detecting a vocabulary set of intermediate level features and considerably reduces the complexity of the CNN. Moreover we propose an optimal high-level feature selection procedure and apply this to the challenging problem of car detection. We demonstrate promising results for multi-class object detection using obtained figural alphabet to detect considerably different categories of objects (e.g., faces and cars).","keywords_author":null,"keywords_other":["RECOGNITION"],"max_cite":4.0,"pub_year":2004.0,"sources":"['wos']","rawkeys":["recognition"],"tags":["recognition"]},{"p_id":12386,"title":"Real-Time Patient-Specific ECG Classification by 1-D Convolutional Neural Networks","abstract":"Goal: This paper presents a fast and accurate patient-specific electrocardiogram (ECG) classification and monitoring system. Methods: An adaptive implementation of 1-D convolutional neural networks (CNNs) is inherently used to fuse the two major blocks of the ECG classification into a single learning body: feature extraction and classification. Therefore, for each patient, an individual and simple CNN will be trained by using relatively small common and patient-specific training data, and thus, such patient-specific feature extraction ability can further improve the classification performance. Since this also negates the necessity to extract hand-crafted manual features, once a dedicated CNN is trained for a particular patient, it can solely be used to classify possibly long ECG data stream in a fast and accurate manner or alternatively, such a solution can conveniently be used for real-time ECG monitoring and early alert system on a light-weight wearable device. Results: The results over the MIT-BIH arrhythmia benchmark database demonstrate that the proposed solution achieves a superior classification performance than most of the state-of-the-art methods for the detection of ventricular ectopic beats and supraventricular ectopic beats. Conclusion: Besides the speed and computational efficiency achieved, once a dedicated CNN is trained for an individual patient, it can solely be used to classify his\/her long ECG records such as Holter registers in a fast and accurate manner. Significance: Due to its simple and parameter invariant nature, the proposed system is highly generic, and, thus, applicable to any ECG dataset.","keywords_author":["Convolutional Neural Networks","Patient-specific ECG classification","real-time heart monitoring","Convolutional neural networks (CNNs)","patient-specific ECG classification","real-time heart monitoring"],"keywords_other":["Signal Processing, Computer-Assisted","Classification performance","Feature extraction and classification","State-of-the-art methods","Algorithms","Adaptive implementation","Humans","Neural Networks (Computer)","HEARTBEAT INTERVAL FEATURES","MORPHOLOGY","Ventricular ectopic beats","Databases, Factual","Precision Medicine","RECOGNITION","Convolutional neural network","Electrocardiography","Ecg classifications","Heart monitoring"],"max_cite":73.0,"pub_year":2016.0,"sources":"['ieee', 'scp', 'wos']","rawkeys":["state-of-the-art methods","databases","heart monitoring","morphology","convolutional neural network","feature extraction and classification","electrocardiography","ecg classifications","convolutional neural networks (cnns)","real-time heart monitoring","algorithms","neural networks (computer)","classification performance","convolutional neural networks","recognition","humans","signal processing","ventricular ectopic beats","heartbeat interval features","factual","precision medicine","computer-assisted","adaptive implementation","patient-specific ecg classification"],"tags":["state-of-the-art methods","ecg","databases","heart monitoring","morphology","convolutional neural network","feature extraction and classification","ecg classifications","real-time heart monitoring","algorithms","recognition","classification performance","neural networks","humans","signal processing","ventricular ectopic beats","heartbeat interval features","factual","precision medicine","computer-assisted","adaptive implementation","patient-specific ecg classification"]},{"p_id":4194,"title":"Reducing and Stretching Deep Convolutional Activation Features for Accurate Image Classification","abstract":"In order to extract effective representations of data using deep learning models, deep convolutional activation feature (DeCAF) is usually considered. However, since the deep models for learning DeCAF are generally pre-trained, the dimensionality of DeCAF is simply fixed to a constant number (e.g., 4096D). In this case, one may ask whether DeCAF is good enough for image classification and whether we can further improve its performance? In this paper, to answer these two challenging questions, we propose a new model called RS-DeCAF based on \"reducing\" and \"stretching\" the dimensionality of DeCAF. In the implementation of RS-DeCAF, we reduce the dimensionality of DeCAF using dimensionality reduction methods and increase its dimensionality by stretching the weight matrix between successive layers. To improve the performance of RS-DeCAF, we also present a modified version of RS-DeCAF by applying the fine-tuning operation. Extensive experiments on several image classification tasks show that RS-DeCAF not only improves DeCAF but also outperforms previous \"stretching\" approaches. More importantly, from the results, we find that RS-DeCAF can generally achieve the highest classification accuracy when its dimensionality is two to four times of that of DeCAF.","keywords_author":["Image classification","Feature learning","Deep convolutional neural network","DeCAF","Stretching"],"keywords_other":["NEURAL-NETWORKS","RECOGNITION"],"max_cite":1.0,"pub_year":2017.0,"sources":"['wos', 'ieee']","rawkeys":["neural-networks","feature learning","recognition","deep convolutional neural network","stretching","decaf","image classification"],"tags":["feature learning","recognition","neural networks","stretching","convolutional neural network","decaf","image classification"]},{"p_id":12388,"title":"Spatial and Time Domain Feature of ERP Speller System Extracted via Convolutional Neural Network","abstract":"Feature of event-related potential (ERP) has not been completely understood and illiteracy problem remains unsolved. To this end, P300 peak has been used as the feature of ERP in most brain-computer interface applications, but subjects who do not show such peak are common. Recent development of convolutional neural network provides a way to analyze spatial and temporal features of ERP. Here, we train the convolutional neural network with 2 convolutional layers whose feature maps represented spatial and temporal features of event-related potential. We have found that nonilliterate subjects' ERP show high correlation between occipital lobe and parietal lobe, whereas illiterate subjects only show correlation between neural activities from frontal lobe and central lobe. The nonilliterates showed peaks in P300, P500, and P700, whereas illiterates mostly showed peaks in around P700. P700 was strong in both subjects. We found that P700 peak may be the key feature of ERP as it appears in both illiterate and nonilliterate subjects.","keywords_author":null,"keywords_other":["RECOGNITION","BCI"],"max_cite":0.0,"pub_year":2018.0,"sources":"['wos']","rawkeys":["recognition","bci"],"tags":["brain-computer interfaces","recognition"]},{"p_id":12394,"title":"Single Image Super-Resolution Based on Multi-Scale Competitive Convolutional Neural Network","abstract":"Deep convolutional neural networks (CNNs) are successful in single-image super-resolution. Traditional CNNs are limited to exploit multi-scale contextual information for image reconstruction due to the fixed convolutional kernel in their building modules. To restore various scales of image details, we enhance the multi-scale inference capability of CNNs by introducing competition among multi-scale convolutional filters, and build up a shallow network under limited computational resources. The proposed network has the following two advantages: (1) the multi-scale convolutional kernel provides the multi-context for image super-resolution, and (2) the maximum competitive strategy adaptively chooses the optimal scale of information for image reconstruction. Our experimental results on image super-resolution show that the performance of the proposed network outperforms the state-of-the-art methods.","keywords_author":["multi-scale","convolutional neural network","image super-resolution"],"keywords_other":["INTERPOLATION","RECOGNITION"],"max_cite":0.0,"pub_year":2018.0,"sources":"['ieee', 'wos']","rawkeys":["multi-scale","recognition","image super-resolution","interpolation","convolutional neural network"],"tags":["multi-scale","recognition","sparse representation","interpolation","convolutional neural network"]},{"p_id":12397,"title":"Robust Topological Navigation via Convolutional Neural Network Feature and Sharpness Measure","abstract":"Visual navigation for mobile robots has emerged in recent years. Among the various methods, topological navigation using visual information provides a scalable map representation for large-scale mapping and navigation. A topological map is essentially a graph with keyframes as its nodes and adjacency relations as its edges. Previous topological mapping uses local feature descriptors, such as scale-invariant feature transform or Speeded-Up Robust Features, to select keyframes in mapping, localization, and estimate relative pose. In practice, local features are not robust for severe motion blur or large illumination change. In this paper, we improve topological mapping to make it more efficient and robust. First, we use a convolutional neural network (CNN) feature as the holistic image representation. The CNN feature can be used to effectively retrieve keyframes that have similar appearance from a topological map, and it is robust to motion blur and illumination change. Thus, it improves the performance for place recognition and robot relocalization. Second, we use sharpness measure to select high-quality keyframes and avoid selecting blurry ones. Third, an efficient and robust non-rigid matching method, vector field consensus, is used for efficient geometric verification and to retrieve the most similar keyframe. The qualitative and quantitative experimental results demonstrate that our method is satisfactory.","keywords_author":["Topological navigation","visual navigation","convolutional neural network","sharpness measure"],"keywords_other":["SUPERRESOLUTION","REPRESENTATION","PERCEPTION","MODEL","RECOGNITION","CONSENSUS","SIMULTANEOUS LOCALIZATION","POINT SET REGISTRATION"],"max_cite":1.0,"pub_year":2017.0,"sources":"['ieee', 'wos']","rawkeys":["recognition","model","superresolution","consensus","point set registration","representation","simultaneous localization","perception","visual navigation","sharpness measure","convolutional neural network","topological navigation"],"tags":["recognition","model","consensus","point set registration","sparse representation","representation","simultaneous localization","visual navigation","perceptions","convolutional neural network","sharpness measure","topological navigation"]},{"p_id":4212,"title":"Fast Deep Convolutional Face Detection in the Wild Exploiting Hard Sample Mining","abstract":"\u00a9 2017 Elsevier Inc. Face detection constitutes a key visual information analysis task in Machine Learning. The rise of Big Data has resulted in the accumulation of a massive volume of visual data which requires proper and fast analysis. Deep Learning methods are powerful approaches towards this task as training with large amounts of data exhibiting high variability has been shown to significantly enhance their effectiveness, but often requires expensive computations and leads to models of high complexity. When the objective is to analyze visual content in massive datasets, the complexity of the model becomes crucial to the success of the model. In this paper, a lightweight deep Convolutional Neural Network (CNN) is introduced for the purpose of face detection, designed with a view to minimize training and testing time, and outperforms previously published deep convolutional networks in this task, in terms of both effectiveness and efficiency. To train this lightweight deep network without compromising its efficiency, a new training method of progressive positive and hard negative sample mining is introduced and shown to drastically improve training speed and accuracy. Additionally, a separate deep network was trained to detect individual facial features and a model that combines the outputs of the two networks was created and evaluated. Both methods are capable of detecting faces under severe occlusion and unconstrained pose variation and meet the difficulties of large scale real-world, real-time face detection, and are suitable for deployment even in mobile environments such as Unmanned Aerial Vehicles (UAVs).","keywords_author":["Convolutional Neural Networks","Deep learning","Face detection","Deep learning","Convolutional Neural Networks","Face detection"],"keywords_other":["CHALLENGES","FEATURES","NEURAL-NETWORKS","RECOGNITION","SURF"],"max_cite":0.0,"pub_year":2018.0,"sources":"['scp', 'wos', 'ieee']","rawkeys":["neural-networks","recognition","convolutional neural networks","features","deep learning","challenges","face detection","surf"],"tags":["recognition","features","neural networks","machine learning","challenges","face detection","surf","convolutional neural network"]},{"p_id":4214,"title":"Active and semi-supervised learning for object detection with imperfect data","abstract":"\u00a9 2017 Elsevier B.V.In this paper, we address the combination of the active learning (AL) and semi-supervised (SSL) learnings, called ASSL, to leverage the strong points of the both learning paradigms for improving the performance of object detection. Considering the pros and cons of the AL and SSL learning methods, ASSL where SSL method provides the incremental improvement of semi-supervised detection performance by combining the concept of diversity imported from AL methods. The proposed method demonstrates outstanding performance compared with state-of-art methods on the challenging Caltech pedestrian detection dataset, reducing the miss rate to 12.2%, which is significantly smaller than current state-of-art. In addition, extensive experiments have been carried out using ILSVRC detection dataset and online evaluation for activity recognition.","keywords_author":["Active Learning (AL)","Convolutional Neural Network (CNN)","Deep learning","Object detection","Semi-Supervised Learning (SSL)","Active Learning (AL)","Semi-Supervised Learning (SSL)","Convolutional Neural Network (CNN)","Deep learning","Object detection"],"keywords_other":["Activity recognition","NETWORKS","Incremental improvements","Semi- supervised learning","State-of-art methods","Detection performance","Semi-supervised learning (SSL)","RECOGNITION","Convolutional neural network","Active Learning"],"max_cite":0.0,"pub_year":2017.0,"sources":"['wos', 'scp', 'ieee']","rawkeys":["recognition","activity recognition","semi- supervised learning","deep learning","detection performance","active learning","convolutional neural network (cnn)","semi-supervised learning (ssl)","networks","object detection","convolutional neural network","state-of-art methods","active learning (al)","incremental improvements"],"tags":["recognition","activity recognition","detection performance","machine learning","semi-supervised learning","networks","object detection","convolutional neural network","state-of-art methods","incremental improvements"]},{"p_id":4220,"title":"Deep learning models for plant disease detection and diagnosis","abstract":"In this paper, convolutional neural network models were developed to perform plant disease detection and diagnosis using simple leaves images of healthy and diseased plants, through deep learning methodologies. Training of the models was performed with the use of an open database of 87,848 images, containing 25 different plants in a set of 58 distinct classes of [plant, disease] combinations, including healthy plants. Several model architectures were trained, with the best performance reaching a 99.53% success rate in identifying the corresponding [plant, disease] combination (or healthy plant). The significantly high success rate makes the model a very useful advisory or early warning tool, and an approach that could be further expanded to support an integrated plant disease identification system to operate in real cultivation conditions.","keywords_author":["Artificial intelligence","Convolutional neural networks","Machine learning","Pattern recognition","Plant disease identification","Convolutional neural networks","Machine learning","Artificial intelligence","Plant disease identification","Pattern recognition"],"keywords_other":["Cultivation conditions","Learning models","Early warning","Integrated plants","Plant disease","RECOGNITION","Convolutional neural network","Model architecture"],"max_cite":2.0,"pub_year":2018.0,"sources":"['ieee', 'scp', 'wos']","rawkeys":["learning models","artificial intelligence","early warning","recognition","convolutional neural networks","model architecture","machine learning","pattern recognition","plant disease","integrated plants","plant disease identification","convolutional neural network","cultivation conditions"],"tags":["learning models","early warning","recognition","model architecture","machine learning","pattern recognition","plant disease","integrated plants","plant disease identification","convolutional neural network","cultivation conditions"]},{"p_id":4227,"title":"Two-phase deep convolutional neural network for reducing class skewness in histopathological images based breast cancer detection","abstract":"Different types of breast cancer are affecting lives of women across the world. Common types include Ductal carcinoma in situ (DCIS), Invasive ductal carcinoma (IDC), Tubular carcinoma, Medullary carcinoma, and Invasive lobular carcinoma (ILC). While detecting cancer, one important factor is mitotic count showing how rapidly the cells are dividing. But the class imbalance problem, due to the small number of mitotic nuclei in comparison to the overwhelming number of non-mitotic nuclei, affects the performance of classification models.","keywords_author":["Breast cancer","Class imbalance","Convolutional neural networks","Deep learning","Histopathology","Mitosis count","Convolutional neural networks","Mitosis count","Deep learning","Histopathology","Class imbalance","Breast cancer"],"keywords_other":["Breast Cancer","Humans","Machine Learning","Mitosis count","ROC Curve","Female","Algorithms","Cell Nucleus","Breast Neoplasms","FEATURES","RECOGNITION","Histocytochemistry","Mitosis","Class imbalance","Neural Networks (Computer)","CLASSIFICATION","Image Interpretation, Computer-Assisted","Histopathology","BIOPSY IMAGES","SEGMENTATION","Convolutional neural network"],"max_cite":6.0,"pub_year":2017.0,"sources":"['ieee', 'scp', 'wos']","rawkeys":["breast neoplasms","mitosis","classification","convolutional neural network","image interpretation","segmentation","features","machine learning","algorithms","roc curve","neural networks (computer)","convolutional neural networks","recognition","cell nucleus","deep learning","humans","class imbalance","breast cancer","computer-assisted","histopathology","biopsy images","mitosis count","histocytochemistry","female"],"tags":["breast neoplasms","mitosis","classification","convolutional neural network","image interpretation","segmentation","features","machine learning","algorithms","roc curve","recognition","cell nucleus","neural networks","humans","class imbalance","breast cancer","computer-assisted","histopathology","biopsy images","mitosis count","histocytochemistry","female"]},{"p_id":69766,"title":"Multimodal Similarity Gaussian Process Latent Variable Model","abstract":"Data from real applications involve multiple modalities representing content with the same semantics from complementary aspects. However, relations among heterogeneous modalities are simply treated as observation-to-fit by existing work, and the parameterized modality specific mapping functions lack flexibility in directly adapting to the content divergence and semantic complicacy in multimodal data. In this paper, we build our work based on the Gaussian process latent variable model (GPLVM) to learn the non-parametric mapping functions and transform heterogeneous modalities into a shared latent space. We propose multimodal Similarity Gaussian Process latent variable model (m-SimGP), which learns the mapping functions between the intra-modal similarities and latent representation. We further propose multimodal distance-preserved similarity GPLVM (m-DSimGP) to preserve the intra-modal global similarity structure, and multimodal regularized similarity GPLVM (m-RSimGP) by encouraging similar\/dissimilar points to be similar\/dissimilar in the latent space. We propose m-DRSimGP, which combines the distance preservation in m-DSimGP and semantic preservation in m-RSimGP to learn the latent representation. The overall objective functions of the four models are solved by simple and scalable gradient decent techniques. They can be applied to various tasks to discover the nonlinear correlations and to obtain the comparable low-dimensional representation for heterogeneous modalities. On five widely used real-world data sets, our approaches outperform existing models on cross-modal content retrieval and multimodal classification.","keywords_author":["Multimodal learning","Gaussian processes","similarity preservation"],"keywords_other":["RECOGNITION","MULTIVIEW","NONLINEAR DIMENSIONALITY REDUCTION"],"max_cite":2.0,"pub_year":2017.0,"sources":"['ieee', 'wos']","rawkeys":["recognition","multiview","similarity preservation","multimodal learning","gaussian processes","nonlinear dimensionality reduction"],"tags":["recognition","similarity preserving","multimodal learning","gaussian processes","nonlinear dimensionality reduction","multi-views"]},{"p_id":4231,"title":"Deep Learning and Its Applications in Biomedicine","abstract":"Advances in biological and medical technologies have been providing us explosive volumes of biological and physiological data, such as medical images, electroencephalography, genomic and protein sequences. Learning from these data facilitates the understanding of human health and disease. Developed from artificial neural networks, deep learning-based algorithms show great promise in extracting features and learning patterns from complex data. The aim of this paper is to provide an overview of deep learning techniques and some of the state-of-the-art applications in the biomedical field. We first introduce the development of artificial neural network and deep learning. We then describe two main components of deep learning, i.e., deep learning architectures and model optimization. Subsequently, some examples are demonstrated for deep learning applications, including medical image classification, genomic sequence analysis, as well as protein structure classification and prediction. Finally, we offer our perspectives for the future directions in the field of deep learning.","keywords_author":["Deep learning","Big data","Bioinformatics","Biomedical informatics","Medical image","High-throughput sequencing"],"keywords_other":["CONVOLUTIONAL NEURAL-NETWORK","EEG","BRAIN-TUMOR SEGMENTATION","PROTEIN SECONDARY STRUCTURE","ARCHITECTURES","CANCER HISTOLOGY IMAGES","RECEPTIVE FIELDS","RECOGNITION","MINIMIZATION","STRUCTURE PREDICTION"],"max_cite":0.0,"pub_year":2018.0,"sources":"['ieee', 'wos']","rawkeys":["structure prediction","recognition","architectures","big data","deep learning","minimization","protein secondary structure","eeg","biomedical informatics","high-throughput sequencing","receptive fields","cancer histology images","brain-tumor segmentation","bioinformatics","medical image","convolutional neural-network"],"tags":["structure prediction","recognition","minimization","big data","protein secondary structure","medical imaging","brain tumor segmentation","machine learning","eeg","random forests","biomedical informatics","high-throughput sequencing","cancer histology images","bioinformatics","convolutional neural network","architecture"]},{"p_id":12423,"title":"A Robust System for Noisy Image Classification Combining Denoising Autoencoder and Convolutional Neural Network","abstract":"Image classification, a complex perceptual task with many real life important applications, faces a major challenge in presence of noise. Noise degrades the performance of the classifiers and makes them less suitable in real life scenarios. To solve this issue, several researches have been conducted utilizing denoising autoencoder (DAE) to restore original images from noisy images and then Convolutional Neural Network (CNN) is used for classification. The existing models perform well only when the noise level present in the training set and test set are same or differs only a little. To fit a model in real life applications, it should be independent to level of noise. The aim of this study is to develop a robust image classification system which performs well at regular to massive noise levels. The proposed method first trains a DAE with low-level noise-injected images and a CNN with noiseless native images independently. Then it arranges these two trained models in three different combinational structures: CNN, DAE-CNN, and DAE-DAE-CNN to classify images corrupted with zero, regular and massive noises, accordingly. Final system outcome is chosen by applying the winner-takes-all combination on individual outcomes of the three structures. Although proposed system consists of three DAEs and three CNNs in different structure layers, the DAEs and CNNs are the copy of same DAE and CNN trained initially which makes it computationally efficient as well. In DAE-DAE-CNN, two identical DAEs are arranged in a cascaded structure to make the structure well suited for classifying massive noisy data while the DAE is trained with low noisy image data. The proposed method is tested with MNIST handwritten numeral dataset with different noise levels. Experimental results revealed the effectiveness of the proposed method showing better results than individual structures as well as the other related methods.","keywords_author":["Image denoising","denoising autoencoder","cascaded denoising autoencoder","convolutional neural network"],"keywords_other":["SPARSE","RECOGNITION","REPRESENTATIONS","ENSEMBLES"],"max_cite":0.0,"pub_year":2018.0,"sources":"['wos']","rawkeys":["recognition","cascaded denoising autoencoder","denoising autoencoder","ensembles","image denoising","representations","sparse","convolutional neural network"],"tags":["recognition","cascaded denoising autoencoder","denoising autoencoder","representation","ensemble","image denoising","sparse","convolutional neural network"]},{"p_id":12427,"title":"An adaptive deep convolutional neural network for rolling bearing fault diagnosis","abstract":"The working conditions of rolling bearings usually is very complex, which makes it difficult to diagnose rolling bearing faults. In this paper, a novel method called the adaptive deep convolutional neural network (CNN) is proposed for rolling bearing fault diagnosis. Firstly, to get rid of manual feature extraction, the deep CNN model is initialized for automatic feature learning. Secondly, to adapt to different signal characteristics, the main parameters of the deep CNN model are determined with a particle swarm optimization method. Thirdly, to evaluate the feature learning ability of the proposed method, t-distributed stochastic neighbor embedding (t-SNE) is further adopted to visualize the hierarchical feature learning process. The proposed method is applied to diagnose rolling bearing faults, and the results confirm that the proposed method is more effective and robust than other intelligent methods.","keywords_author":["adaptive deep convolutional neural network","fault diagnosis","feature learning","particle swarm optimization","rolling bearing","rolling bearing","adaptive deep convolutional neural network","feature learning","particle swarm optimization","fault diagnosis"],"keywords_other":["Rolling bearings","WAVELET TRANSFORM","ROTATING MACHINERY","SUPPORT VECTOR MACHINE","Signal characteristic","RECOGNITION","Main parameters","Intelligent method","Convolutional neural network","Feature learning","Stochastic neighbor embedding","Hierarchical features"],"max_cite":4.0,"pub_year":2017.0,"sources":"['ieee', 'scp', 'wos']","rawkeys":["wavelet transform","recognition","stochastic neighbor embedding","signal characteristic","hierarchical features","intelligent method","main parameters","rolling bearing","adaptive deep convolutional neural network","convolutional neural network","support vector machine","feature learning","rolling bearings","rotating machinery","fault diagnosis","particle swarm optimization"],"tags":["wavelet transform","recognition","stochastic neighbor embedding","signal characteristic","hierarchical features","intelligent method","main parameters","machine learning","rolling bearing","adaptive deep convolutional neural network","convolutional neural network","feature learning","rotating machinery","fault diagnosis","particle swarm optimization"]},{"p_id":12437,"title":"Mixtures of Lightweight Deep Convolutional Neural Networks: Applied to Agricultural Robotics","abstract":"We propose a novel approach for training deep convolutional neural networks (DCNNs) that allows us to tradeoff complexity and accuracy to learn lightweight models suitable for robotic platforms such as AgBot II (which performs automated weed management). Our approach consists of three stages, the first is to adapt a pre-trained model to the task at hand. This provides state-of-the-art performance but at the cost of high computational complexity resulting in a low frame rate of just 0.12 frames per second (fps). Second, we use the adapted model and employ model compression techniques to learn a lightweight DCNN that is less accurate but has two orders of magnitude fewer parameters. Third, K lightweight models are combined as a mixture model to further enhance the performance of the lightweight models. Applied to the challenging task of weed segmentation, we improve the accuracy from 85.9%, using a traditional approach, to 93.9% by adapting a complicated pre-trained DCNN with 25M parameters (Inception-v3). The downside to this adapted model, Adapted-IV3, is that it can only process 0.12 fps. To make this approach fast while still retaining accuracy, we learn lightweight DCNNs which when combined can achieve accuracy greater than 90% while using considerably fewer parameters capable of processing between 1.07 and 1.83 fps, up to an order of magnitude faster and up to an order of magnitude fewer parameters.","keywords_author":["Agricultural automation","computer vision for automation","recognition"],"keywords_other":null,"max_cite":3.0,"pub_year":2017.0,"sources":"['ieee', 'wos']","rawkeys":["agricultural automation","computer vision for automation","recognition"],"tags":["agricultural automation","computer vision for automation","recognition"]},{"p_id":69782,"title":"Ensemble dropout extreme learning machine via fuzzy integral for data classification","abstract":"Extreme learning machine (ELM) is a simple but efficient algorithm for training single hidden layer feed-forward neural networks (SLFNs) with fast speed and good generalization ability. ELM has been successfully applied to many fields, such as pattern recognition, computer vision, biological information processing, etc. However, there are two problems in ELM. the first one is architecture selection, the second one is prediction instability. In order to deal with the two problems, based on dropout technique, an ensemble learning method is proposed in this paper. The proposed method can solve the first problem and can improve prediction stability. Our experimental results and statistical analysis on 14 data sets confirm this conclusion. Furthermore, our experimental results also show that the proposed approach outperforms the original ELM on prediction stability and classification accuracy. (C) 2017 Elsevier B.V. All rights reserved.","keywords_author":["Extreme learning machine","Dropout","Ensemble learning","Fuzzy integral","Data classification"],"keywords_other":["REGRESSION","BIG DATA","MULTICLASS CLASSIFICATION","RECOGNITION","MAPREDUCE","FUSION","GENETIC ALGORITHMS","CONVOLUTIONAL NEURAL-NETWORKS"],"max_cite":0.0,"pub_year":2017.0,"sources":"['ieee', 'wos']","rawkeys":["multiclass classification","dropout","recognition","big data","data classification","mapreduce","convolutional neural-networks","genetic algorithms","ensemble learning","extreme learning machine","fusion","fuzzy integral","regression"],"tags":["dropout","recognition","regression","big data","data classification","genetic algorithm","map-reduce","convolutional neural network","ensemble learning","extreme learning machine","fusion","fuzzy integral","multi-class classification"]},{"p_id":4250,"title":"Froth image analysis by use of transfer learning and convolutional neural networks","abstract":"Deep learning constitutes a significant recent advance in machine learning and has been particularly successful in applications related to image processing, where it can already surpass human accuracy in some cases. In this paper, the use of a convolutional neural network, AlexNet, pretrained on a database of images of common objects was used as is to extract features from flotation froth images. These features could subsequently be used to predict the conditions or performance of the flotation systems. Two case studies are considered. In the first, froth regimes in an industrial flotation plant could be identified significantly more reliably with the features generated by AlexNet than with previous state-of-the-art approaches, such as wavelets, grey level co-occurrence matrices or local binary patterns. In the second case study, the arsenic concentration in the batch flotation of realgar-orpiment-quartz mixtures could be predicted more accurately than was possible with features extracted by wavelets, grey level co-occurrence matrices, local binary patterns or by use of colour. These results suggest that feature extraction with convolutional neural networks trained on complex data sets from other domains can serve as more reliable methods than previous state-of-the-art approaches to froth image analysis.","keywords_author":["AlexNet","Convolution Neural Networks","Deep learning","Froth flotation","Image analysis","Machine vision","Froth flotation","Image analysis","Convolution Neural Networks","AlexNet","Deep learning","Machine vision"],"keywords_other":["PREDICTION","Transfer learning","FEATURES","GRADE","FLOTATION PLANTS","Arsenic concentration","CLASSIFICATION","State-of-the-art approach","RECOGNITION","MACHINE-VISION","Convolutional neural network","PERFORMANCE","Convolution neural network","Co-occurrence-matrix","FEATURE-EXTRACTION","AlexNet","Local binary patterns","DISTRIBUTIONS"],"max_cite":2.0,"pub_year":2018.0,"sources":"['ieee', 'scp', 'wos']","rawkeys":["performance","machine vision","classification","convolutional neural network","froth flotation","state-of-the-art approach","features","alexnet","transfer learning","recognition","deep learning","feature-extraction","convolution neural networks","distributions","machine-vision","arsenic concentration","grade","image analysis","convolution neural network","prediction","flotation plants","local binary patterns","co-occurrence-matrix"],"tags":["performance","machine vision","classification","convolutional neural network","froth flotation","state-of-the-art approach","features","alexnet","transfer learning","machine learning","recognition","distributions","arsenic concentration","grade","image analysis","prediction","flotation plants","feature extraction","local binary patterns","co-occurrence-matrix"]},{"p_id":4254,"title":"Color image classification via quaternion principal component analysis network","abstract":"\u00a9 2016 Elsevier B.V.The principal component analysis network (PCANet), which is one of the recently proposed deep learning architectures, achieves the state-of-the-art classification accuracy in various datasets and reveals a simple baseline for deep learning networks. However, the performance of PCANet may be degraded when dealing with color images due to the fact that the architecture of PCANet cannot properly utilize the spatial relationship between each color channel in three dimensional color image. In this paper, a quaternion principal component analysis network (QPCANet), which extends PCANet by using quaternion theory, is proposed for color image classification. Compared to PCANet, the proposed QPCANet takes into account the spatial distribution information of RGB channels in color images and ensures larger amount of intra-class invariance by using quaternion domain representation for color images. Experiments conducted on different color image datasets such as UC Merced Land Use, Georgia Tech face, CURet and Caltech-101 have revealed that the proposed QPCANet generally achieves higher classification accuracy than PCANet in color image classification task. The experimental results also verify that QPCANet has much better rotation invariance than PCANet when color image dataset contains lots of rotation information and demonstrate even a simple one-layer QPCANet may obtain satisfactory accuracy when compared with two-layer PCANet.","keywords_author":["Color image classification","Convolutional neural network","Deep learning","PCANet","QPCANet","Quaternion","Deep learning","Convolutional neural network","Quaternion","QPCANet","PCANet","Color image classification"],"keywords_other":["Deep learning","QPCANet","INVARIANT SCATTERING","PCANet","Quaternion","RECOGNITION","LEARNING ALGORITHM","Convolutional neural network","SCALE","Color images"],"max_cite":6.0,"pub_year":2016.0,"sources":"['scp', 'wos', 'ieee']","rawkeys":["invariant scattering","recognition","quaternion","deep learning","pcanet","scale","color image classification","convolutional neural network","learning algorithm","color images","qpcanet"],"tags":["invariant scattering","recognition","quaternion","pcanet","machine learning","scale","color image classification","convolutional neural network","learning algorithm","color images","qpcanet"]},{"p_id":12447,"title":"A new image classification method based on modified condensed nearest neighbor and convolutional neural networks","abstract":"As a typical model of deep learning, convolutional neural networks (CNN) has a state of art result on the large-scale images classification. However, with the constantly increasing of digit images, there contains more and more redundant, relevant and noisy samples which cause CNN running slowly and its classification accuracy also decreasing at the same time. In this paper, we provide an effective sample selection method for large-scale images based on the improved condensed nearest neighbor rule (called Condensed NN) by the k-means clustering algorithm. Condensed NN can condense a large quantity of original samples, and then the k-means clustering algorithm is used to further optimize and select the high quality samples that will be set as the new original inputs of CNN according to the distribution. Based on the selection of new samples, the training process of CNN can be speeded up dramatically while the classification accuracy is not inferior to the traditional CNN trained by all of samples. Experimental results show that the proposed method can effectively reduce most of useless samples and has a better generalization performance. (C) 2017 Elsevier B.V. All rights reserved.","keywords_author":["Large-scale image classification","Sample selection","Condensed nearest neighbor","Convolutional neural networks"],"keywords_other":["ALGORITHMS","RECOGNITION","SAMPLE SELECTION","RULE"],"max_cite":0.0,"pub_year":2017.0,"sources":"['wos']","rawkeys":["recognition","convolutional neural networks","large-scale image classification","rule","condensed nearest neighbor","algorithms","sample selection"],"tags":["rules","recognition","large-scale image classification","convolutional neural network","condensed nearest neighbor","algorithms","sample selection"]},{"p_id":12449,"title":"Micro-Doppler Based Classification of Human Aquatic Activities via Transfer Learning of Convolutional Neural Networks","abstract":"Accurate classification of human aquatic activities using radar has a variety of potential applications such as rescue operations and border patrols. Nevertheless, the classification of activities on water using radar has not been extensively studied, unlike the case on dry ground, due to its unique challenge. Namely, not only is the radar cross section of a human on water small, but the micro-Doppler signatures are much noisier due to water drops and waves. In this paper, we first investigate whether discriminative signatures could be obtained for activities on water through a simulation study. Then, we show how we can effectively achieve high classification accuracy by applying deep convolutional neural networks (DCNN) directly to the spectrogram of real measurement data. From the five-fold cross-validation on our dataset, which consists of five aquatic activities, we report that the conventional feature-based scheme only achieves an accuracy of 45.1%. In contrast, the DCNN trained using only the collected data attains 66.7%, and the transfer learned DCNN, which takes a DCNN pre-trained on a RGB image dataset and fine-tunes the parameters using the collected data, achieves a much higher 80.3%, which is a significant performance boost.","keywords_author":["radar","micro-Doppler signatures","aquatic activity classification","convolutional neural networks","transfer learning"],"keywords_other":["FEATURES","GROUND MOVING TARGETS","MODEL","RECOGNITION","SIGNATURES","RADAR"],"max_cite":2.0,"pub_year":2016.0,"sources":"['ieee', 'wos']","rawkeys":["recognition","convolutional neural networks","model","features","transfer learning","signatures","aquatic activity classification","micro-doppler signatures","radar","ground moving targets"],"tags":["recognition","model","features","transfer learning","aquatic activity classification","signature","micro-doppler signatures","convolutional neural network","radar","ground moving targets"]},{"p_id":4258,"title":"Convolutional neural networks for hyperspectral image classification","abstract":"As a powerful visual model, convolutional neural networks (CNNs) have demonstrated remarkable performance in various visual recognition problems, and attracted considerable attention in recent years. However, due to the highly correlated bands and insufficient training samples of hyperspectral image data, it still remains a challenging problem to effectively apply the CNN models on hyperspectral images. In this paper, an efficient CNN architecture has been proposed to boost its discriminative capability for hyperspectral image classification, in which the original data is used as the input and the final CNN outputs are the predicted class-related results. The proposed CNN infrastructure has several distinct advantages. Firstly, different from traditional classification methods those need hand-crafted features, the CNN model used here is designed to deal with the problem of hyperspectral image analysis in an end-to-end way. Secondly, the parameters of the CNN model are optimized from a small training set, while the over-fitting problem of the neural network has been alleviated to some extent. Finally, in order to better deal with the hyperspectral image information, 1 x 1 convolutional layers have been adopted, and an average pooling layer and larger dropout rates have also been employed in the whole CNN procedure. The experiments on three benchmark data sets have demonstrated that the proposed CNN architecture considerably outperforms other state-of-the-art methods. (C) 2016 Elsevier B.V. All rights reserved.","keywords_author":["Hyperspectral image classification","Convolutional neural networks","Deep learning"],"keywords_other":["SPECIAL-ISSUE","REPRESENTATION","FOREWORD","RECOGNITION","BAND SELECTION"],"max_cite":34.0,"pub_year":2016.0,"sources":"['ieee', 'wos']","rawkeys":["band selection","recognition","convolutional neural networks","foreword","deep learning","representation","hyperspectral image classification","special-issue"],"tags":["band selection","recognition","foreword","machine learning","hyperspectral image classification","representation","convolutional neural network","special-issue"]},{"p_id":4264,"title":"Video pornography detection through deep learning techniques and motion information","abstract":"Recent literature has explored automated pornographic detection a bold move to replace humans in the tedious task of moderating online content. Unfortunately, on scenes with high skin exposure, such as people sunbathing and wrestling, the state of the art can have many false alarms. This paper is based on the premise that incorporating motion information in the models can alleviate the problem of mapping skin exposure to pornographic content, and advances the bar on automated pornography detection with the use of motion information and deep learning architectures. Deep Learning, especially in the form of Convolutional Neural Networks, have striking results on computer vision, but their potential for pornography detection is yet to be fully explored through the use of motion information. We propose novel ways for combining static (picture) and dynamic (motion) information using optical flow and MPEG motion vectors. We show that both methods provide equivalent accuracies, but that MPEG motion vectors allow a more efficient implementation. The best proposed method yields a classification accuracy of 97.9% an error reduction of 64.4% when compared to the state of the art on a dataset of 800 challenging test cases. Finally, we present and discuss results on a larger, and more challenging, dataset.","keywords_author":["Deep learning and motion information","MPEG motion vectors","Optical flow","Pornography classification","Sensitive video classification","Pornography classification","Deep learning and motion information","Optical flow","MPEG motion vectors","Sensitive video classification"],"keywords_other":["On-line contents","REPRESENTATION","SPACE-TIME","Motion Vectors","Pornography detections","CLASSIFICATION","SYSTEM","RECOGNITION","Efficient implementation","Convolutional neural network","Video classification","Classification accuracy","Motion information"],"max_cite":7.0,"pub_year":2017.0,"sources":"['ieee', 'scp', 'wos']","rawkeys":["pornography detections","motion information","pornography classification","recognition","classification accuracy","space-time","mpeg motion vectors","deep learning and motion information","on-line contents","representation","system","classification","convolutional neural network","sensitive video classification","optical flow","efficient implementation","video classification","motion vectors"],"tags":["pornography detections","motion information","pornography classification","recognition","classification accuracy","mpeg motion vectors","deep learning and motion information","on-line contents","representation","system","classification","convolutional neural network","sensitive video classification","optical flow","video classification","efficient implementation","space time","motion vectors"]},{"p_id":12457,"title":"Unconstrained Still\/Video-Based Face Verification with Deep Convolutional Neural Networks","abstract":"Over the last 5 years, methods based on Deep Convolutional Neural Networks (DCNNs) have shown impressive performance improvements for object detection and recognition problems. This has been made possible due to the availability of large annotated datasets, a better understanding of the non-linear mapping between input images and class labels as well as the affordability of GPUs. In this paper, we present the design details of a deep learning system for unconstrained face recognition, including modules for face detection, association, alignment and face verification. The quantitative performance evaluation is conducted using the IARPA Janus Benchmark A (IJB-A), the JANUS Challenge Set 2 (JANUS CS2), and the Labeled Faces in the Wild (LFW) dataset. The IJB-A dataset includes real-world unconstrained faces of 500 subjects with significant pose and illumination variations which are much harder than the LFW and Youtube Face datasets. JANUS CS2 is the extended version of IJB-A which contains not only all the images\/frames of IJB-A but also includes the original videos. Some open issues regarding DCNNs for face verification problems are then discussed.","keywords_author":["Deep learning","Face detection\/association","Face verification","Fiducial detection","Metric learning","Deep learning","Face detection\/association","Fiducial detection","Face verification","Metric learning"],"keywords_other":["Face Verification","TRACKING","CASCADE","Metric learning","Pose and illumination variations","ALIGNMENT","RECOGNITION","WILD","Convolutional neural network","Nonlinear mappings","MODELS","Object detection and recognition","Labeled faces in the wilds (LFW)","Annotated datasets"],"max_cite":0.0,"pub_year":2018.0,"sources":"['ieee', 'scp', 'wos']","rawkeys":["fiducial detection","metric learning","face detection\/association","object detection and recognition","recognition","pose and illumination variations","deep learning","annotated datasets","labeled faces in the wilds (lfw)","tracking","face verification","nonlinear mappings","convolutional neural network","models","alignment","cascade","wild"],"tags":["fiducial detection","metric learning","face detection\/association","object detection and recognition","recognition","model","pose and illumination variations","annotated datasets","machine learning","labeled faces in the wilds (lfw)","tracking","face verification","nonlinear mappings","convolutional neural network","alignment","cascade","wild"]},{"p_id":12458,"title":"On the use of convolutional neural networks for robust classification of multiple fingerprint captures","abstract":"Fingerprint classification is one of the most common approaches to accelerate the identification in large databases of fingerprints. Fingerprints are grouped into disjoint classes, so that an input fingerprint is compared only with those belonging to the predicted class, reducing the penetration rate of the search. The classification procedure usually starts by the extraction of features from the fingerprint image, frequently based on visual characteristics. In this work, we propose an approach to fingerprint classification using convolutional neural networks, which avoid the necessity of an explicit feature extraction process by incorporating the image processing within the training of the classifier. Furthermore, such an approach is able to predict a class even for low-quality fingerprints that are rejected by commonly used algorithms, such as FingerCode. The study gives special importance to the robustness of the classification for different impressions of the same fingerprint, aiming to minimize the penetration in the database. In our experiments, convolutional neural networks yielded better accuracy and penetration rate than state-of-the-art classifiers based on explicit feature extraction. The tested networks also improved on the runtime, as a result of the joint optimization of both feature extraction and classification.","keywords_author":["Convolutional neural networks","deep learning","deep neural networks","fingerprint classification","Convolutional neural networks","fingerprint classification","deep learning","deep neural networks"],"keywords_other":["Fingerprint classification","SINGULAR POINTS","PATTERN-CLASSIFICATION","VERIFICATION","Feature extraction and classification","Classification procedure","Fingerprint images","Joint optimization","SUPPORT VECTOR MACHINES","IDENTIFICATION","DATABASE","RECOGNITION","Convolutional neural network","Penetration rates","Robust classification","LEVEL FUSION","CLASSIFIERS","FEATURE-EXTRACTION"],"max_cite":2.0,"pub_year":2018.0,"sources":"['ieee', 'scp', 'wos']","rawkeys":["identification","robust classification","classifiers","verification","convolutional neural network","feature extraction and classification","joint optimization","fingerprint images","classification procedure","level fusion","convolutional neural networks","recognition","penetration rates","deep learning","feature-extraction","database","fingerprint classification","deep neural networks","singular points","support vector machines","pattern-classification"],"tags":["identification","level fusion","fingerprint classification","databases","penetration rates","recognition","fingerprint images","machine learning","robust classification","singular points","verification","classifier","classification procedure","convolutional neural network","feature extraction and classification","feature extraction","joint optimization","pattern classification"]},{"p_id":12460,"title":"Training Deep Convolutional Neural Networks for Land-Cover Classification of High-Resolution Imagery","abstract":"Deep convolutional neural networks (DCNNs) have recently emerged as a dominant paradigm for machine learning in a variety of domains. However, acquiring a suitably large data set for training DCNN is often a significant challenge. This is a major issue in the remote sensing domain, where we have extremely large collections of satellite and aerial imagery, but lack the rich label information that is often readily available for other image modalities. In this letter, we investigate the use of DCNN for land-cover classification in high-resolution remote sensing imagery. To overcome the lack of massive labeled remote-sensing image data sets, we employ two techniques in conjunction with DCNN: transfer learning (TL) with fine-tuning and data augmentation tailored specifically for remote sensing imagery. TL allows one to bootstrap a DCNN while preserving the deep visual feature extraction learned over an image corpus from a different image domain. Data augmentation exploits various aspects of remote sensing imagery to dramatically expand small training image data sets and improve DCNN robustness for remote sensing image data. Here, we apply these techniques to the well-known UC Merced data set to achieve the land-cover classification accuracies of 97.8 +\/- 2.3%, 97.6 +\/- 2.6%, and 98.5 +\/- 1.4% with CaffeNet, GoogLeNet, and ResNet, respectively.","keywords_author":["Deep convolutional neural network (DCNN)","deep learning","high-resolution remote sensing imagery","land-cover classification","Transfer learning (TL)","Deep convolutional neural network (DCNN)","deep learning","high-resolution remote sensing imagery","land-cover classification","transfer learning (TL)"],"keywords_other":["Transfer learning","High resolution imagery","High resolution remote sensing imagery","RECOGNITION","Remote sensing imagery","Convolutional neural network","Remote sensing images","Visual feature extraction","SCENE CLASSIFICATION","Land cover classification"],"max_cite":18.0,"pub_year":2017.0,"sources":"['scp', 'wos']","rawkeys":["high resolution imagery","remote sensing imagery","transfer learning (tl)","recognition","deep learning","land cover classification","remote sensing images","transfer learning","scene classification","visual feature extraction","high-resolution remote sensing imagery","land-cover classification","convolutional neural network","high resolution remote sensing imagery","deep convolutional neural network (dcnn)"],"tags":["high resolution imagery","remote-sensing imagery","recognition","land cover classification","transfer learning","remote sensing images","visual feature extraction","machine learning","scene classification","convolutional neural network","high resolution remote sensing imagery"]},{"p_id":4270,"title":"Learning in the machine: The symmetries of the deep learning channel","abstract":"In a physical neural system, learning rules must be local both in space and time. In order for learning to occur, non-local information must be communicated to the deep synapses through a communication channel, the deep learning channel. We identify several possible architectures for this learning channel (Bidirectional, Conjoined, Twin, Distinct) and six symmetry challenges: (1) symmetry of architectures; (2) symmetry of weights; (3) symmetry of neurons; (4) symmetry of derivatives; (5) symmetry of processing; and (6) symmetry of learning rules. Random backpropagation (RBP) addresses the second and third symmetry, and some of its variations, such as skipped RBP (SRBP) address the first and the fourth symmetry. Here we address the last two desirable symmetries showing through simulations that they can be achieved and that the learning channel is particularly robust to symmetry variations. Specifically, random backpropagation and its variations can be performed with the same non-linear neurons used in the main input-output forward channel, and the connections in the learning channel can be adapted using the same algorithm used in the forward channel, removing the need for any specialized hardware in the learning channel. Finally, we provide mathematical results in simple cases showing that the learning equations in the forward and backward channels converge to fixed points, for almost any initial conditions. In symmetric architectures, if the weights in both channels are small at initialization, adaptation in both channels leads to weights that are essentially symmetric during and after learning. Biological connections are discussed. (C) 2017 Elsevier Ltd. All rights reserved.","keywords_author":["Backpropagation","Deep learning","Learning channel","Learning dynamics","Local learning","Neural networks","Neural networks","Deep learning","Backpropagation","Local learning","Learning channel","Learning dynamics"],"keywords_other":["CEREBRAL-CORTEX","ARCHITECTURES","ALGORITHM","Machine Learning","Non-linear neurons","Forward channels","Specialized hardware","ENDOCANNABINOIDS","Initial conditions","BACKPROPAGATION","Neural systems","Local learning","Forward-and-backward","NEURONS","RECOGNITION","GENERATIVE MODELS","Learning channel","SYNAPTIC PLASTICITY","Neural Networks (Computer)","NEURAL-NETWORKS"],"max_cite":3.0,"pub_year":2017.0,"sources":"['ieee', 'scp', 'wos']","rawkeys":["initial conditions","neural-networks","machine learning","cerebral-cortex","learning dynamics","neurons","algorithm","neural networks (computer)","recognition","specialized hardware","deep learning","neural networks","forward channels","forward-and-backward","local learning","neural systems","generative models","non-linear neurons","endocannabinoids","architectures","synaptic plasticity","learning channel","backpropagation"],"tags":["nonlinear neuron","initial conditions","architecture","machine learning","cerebral-cortex","algorithms","neurons","generative model","recognition","specialized hardware","neural networks","forward channels","forward-and-backward","local learning","neural systems","endocannabinoids","synaptic plasticity","learning dynamically","learning channel","backpropagation"]},{"p_id":4272,"title":"GXNOR-Net: Training deep neural networks with ternary weights and activations without full-precision memory under a unified discretization framework","abstract":"Although deep neural networks (DNNs) are being a revolutionary power to open up the AI era, the notoriously huge hardware overhead has challenged their applications. Recently, several binary and ternary networks, in which the costly multiply-accumulate operations can be replaced by accumulations or even binary logic operations, make the on-chip training of DNNs quite promising. Therefore there is a pressing need to build an architecture that could subsume these networks under a unified framework that achieves both higher performance and less overhead. To this end, two fundamental issues are yet to be addressed. The first one is how to implement the back propagation when neuronal activations are discrete. The second one is how to remove the full-precision hidden weights in the training phase to break the bottlenecks of memory\/computation consumption. To address the first issue, we present a multi-step neuronal activation discretization method and a derivative approximation technique that enable the implementing the back propagation algorithm on discrete DNNs. While for the second issue, we propose a discrete state transition (DST) methodology to constrain the weights in a discrete space without saving the hidden weights. Through this way, we build a unified framework that subsumes the binary or ternary networks as its special cases, and under which a heuristic algorithm is provided at the website https:\/\/github.com\/AcrossV\/Gated-XNOR. More particularly, we find that when both the weights and activations become ternary values, the DNNs can be reduced to sparse binary networks, termed as gated XNOR networks (GXNOR-Nets) since only the event of non-zero weight and non-zero activation enables the control gate to start the XNOR logic operations in the original binary networks. This promises the event-driven hardware design for efficient mobile intelligence. We achieve advanced performance compared with state-of-the-art algorithms. Furthermore, the computational sparsity and the number of states in the discrete space can be flexibly modified to make it suitable for various hardware platforms. (c) 2018 Elsevier Ltd. All rights reserved.","keywords_author":["GXNOR-Net","Discrete state transition","Ternary neural networks","Sparse binary networks"],"keywords_other":["RECOGNITION"],"max_cite":0.0,"pub_year":2018.0,"sources":"['wos', 'ieee']","rawkeys":["recognition","discrete state transition","sparse binary networks","ternary neural networks","gxnor-net"],"tags":["recognition","discrete state transition","sparse binary networks","ternary neural networks","gxnor-net"]},{"p_id":12466,"title":"Radar-ID: human identification based on radar micro-Doppler signatures using deep convolutional neural networks","abstract":"Human identification is crucial in various applications, including terrorist attack preventing, criminal seeking, defence and so on. Traditional human identification methods are usually based on vision, biological features, radio-frequency identification cards and so on. In this study, the authors propose an identification method based on radar micro-Doppler signatures using deep convolutional neural networks (DCNNs) for the first time, which can identify human in non-contact, remote and no lighting status. They employ a K-band Doppler radar to acquire the raw signals due to its stationary clutter rejection and movement detection ability as well as its short wavelength which can generate larger Doppler shift. Then short-time Fourier transform is applied to the raw signals to characterise micro-Doppler signatures. They adopt the DCNNs to deal with the spectrograms for human identification problem. The DCNNs can learn the necessary features and classification conditions from raw micro-Doppler spectrograms without employing any explicit features. While the traditional supervised learning techniques relying on the extracted features require domain knowledge of each problem. It is shown that this method can achieve average accuracy approximate to 97.1% for 4 people, 90.9% for 6 people, 89.1% for 8 people, 85.6% for 10 people, 77.4% for 12 people, 72.6% for 16 people and 68.9% for 20 people.","keywords_author":["neural nets","image classification","pattern classification","learning (artificial intelligence)","feature extraction","Fourier transforms","Doppler radar","Doppler shift","radar imaging","radar-ID","radar microDoppler signatures","deep convolutional neural networks","biological features","radio-frequency identification cards","DCNNs","K-band Doppler radar","stationary clutter rejection","movement detection ability","larger Doppler shift","short-time Fourier transform","human identification problem","classification conditions","microDoppler spectrograms","human identification methods"],"keywords_other":["RECOGNITION","FEATURES"],"max_cite":0.0,"pub_year":2018.0,"sources":"['wos']","rawkeys":["microdoppler spectrograms","classification conditions","human identification methods","radar-id","image classification","radar imaging","features","movement detection ability","fourier transforms","doppler shift","learning (artificial intelligence)","doppler radar","recognition","dcnns","radio-frequency identification cards","neural nets","pattern classification","k-band doppler radar","stationary clutter rejection","short-time fourier transform","deep convolutional neural networks","larger doppler shift","biological features","radar microdoppler signatures","feature extraction","human identification problem"],"tags":["microdoppler spectrograms","classification conditions","human identification methods","convolutional neural network","radar-id","image classification","radar imaging","features","machine learning","movement detection ability","doppler shift","fourier transform","recognition","doppler radar","neural networks","radio-frequency identification cards","pattern classification","k-band doppler radar","stationary clutter rejection","larger doppler shift","biological features","radar microdoppler signatures","feature extraction","human identification problem","short time fourier transforms"]},{"p_id":12468,"title":"Multilevel Cloud Detection for High-Resolution Remote Sensing Imagery Using Multiple Convolutional Neural Networks","abstract":"In high-resolution image data, multilevel cloud detection is a key task for remote sensing data processing. Generally, it is difficult to obtain high accuracy for multilevel cloud detection when using satellite imagery which only contains visible and near-infrared spectral bands. So, multilevel cloud detection for high-resolution remote sensing imagery is challenging. In this paper, a new multilevel cloud detection technique is proposed based on the multiple convolutional neural networks for high-resolution remote sensing imagery. In order to avoid input the entire image into the network for cloud detection, the adaptive simple linear iterative clustering (A-SCLI) algorithm was applied to the segmentation of the satellite image to obtain good-quality superpixels. After that, a new multiple convolutional neural networks (MCNNs) architecture is designed to extract multiscale features from each superpixel, and the superpixels are marked as thin cloud, thick cloud, cloud shadow, and non-cloud. The results suggest that the proposed method can detect multilevel clouds and obtain a high accuracy for high-resolution remote sensing imagery.","keywords_author":["multiple convolutional neural networks","cloud detection","superpixel","high-resolution remote sensing imagery"],"keywords_other":["SVM ENSEMBLE APPROACH","SLIC SUPERPIXELS","CLASSIFICATION","RECOGNITION","SEGMENTATION"],"max_cite":0.0,"pub_year":2018.0,"sources":"['wos']","rawkeys":["recognition","slic superpixels","segmentation","cloud detection","high-resolution remote sensing imagery","svm ensemble approach","classification","multiple convolutional neural networks","superpixel"],"tags":["recognition","slic superpixels","segmentation","cloud detection","svm ensemble approach","classification","high resolution remote sensing imagery","multiple convolutional neural networks","superpixel"]},{"p_id":12471,"title":"A snapshot of image pre-processing for convolutional neural networks: case study of MNIST","abstract":"In the last five years, deep learning methods and particularly Convolutional Neural Networks (CNNs) have exhibited excellent accuracies in many pattern classification problems. Most of the state-of-the-art models apply data-augmentation techniques at the training stage. This paper provides a brief tutorial on data preprocessing and shows its benefits by using the competitive MNIST handwritten digits classification problem. We show and analyze the impact of different preprocessing techniques on the performance of three CNNs, LeNet, Network3 and DropConnect, together with their ensembles. The analyzed transformations are, centering, elastic deformation, translation, rotation and different combinations of them. Our analysis demonstrates that data-preprocessing techniques, such as the combination of elastic deformation and rotation, together with ensembles have a high potential to further improve the state-of-the-art accuracy in MNIST classification.","keywords_author":["Classification","Deep learning","Convolutional Neural Networks (CNNs)","preprocessing","handwritten digits","data augmentation"],"keywords_other":["RECOGNITION"],"max_cite":4.0,"pub_year":2017.0,"sources":"['wos']","rawkeys":["handwritten digits","recognition","deep learning","preprocessing","data augmentation","classification","convolutional neural networks (cnns)"],"tags":["pre-processing","recognition","machine learning","data augmentation","classification","convolutional neural network","handwritten digit"]},{"p_id":69816,"title":"Scene parsing using inference Embedded Deep Networks","abstract":"Effective features and graphical model are two key points for realizing high performance scene parsing. Recently, Convolutional Neural Networks (CNNs) have shown great ability of learning features and attained remarkable performance. However, most researches use CNNs and graphical model separately, and do not exploit full advantages of both methods. In order to achieve better performance, this work aims to design a novel neural network architecture called Inference Embedded Deep Networks (IEDNs), which incorporates a novel designed inference layer based on graphical model. Through the IEDNs, the network can learn hybrid features, the advantages of which are that they not only provide a powerful representation capturing hierarchical information, but also encapsulate spatial relationship information among adjacent objects. We apply the proposed networks to scene labeling, and several experiments are conducted on SIFT Flow and PASCAL VOC Dataset. The results demonstrate that the proposed IEDNs can achieve better performance. (C) 2016 Elsevier Ltd. All rights reserved.","keywords_author":["Convolutional Neural Networks (CNNs)","Conditional Random Fields (CRFs)","Inference Embedded Deep Networks (IEDNs)","Hybrid Features"],"keywords_other":["SUPERPIXELS","SALIENCY","FEATURES","FRAMEWORK","3D SHAPE","SEGMENTATION","RECOGNITION","ENERGY MINIMIZATION","HIGH-LEVEL FEATURE","OBJECT DETECTION"],"max_cite":1.0,"pub_year":2016.0,"sources":"['ieee', 'wos']","rawkeys":["recognition","energy minimization","inference embedded deep networks (iedns)","features","segmentation","framework","conditional random fields (crfs)","3d shape","high-level feature","object detection","saliency","superpixels","convolutional neural networks (cnns)","hybrid features"],"tags":["high-level features","recognition","energy minimization","inference embedded deep networks (iedns)","features","segmentation","conditional random field","framework","3-d shape","object detection","convolutional neural network","saliency","superpixel","hybrid features"]},{"p_id":110777,"title":"Detection of abnormal heart conditions based on characteristics of ECG signals","abstract":"Heart diseases are one of the most important death causes across the globe. Therefore, early detection of heart diseases is crucial to reduce the rising death rate. Electrocardiogram (ECG) is widely used to diagnose many types of heart diseases such as abnormal heartbeat rhythm (arrhythmia). However, the non-linearity and the complexity of the abnormal ECG signals make it very difficult to detect its characteristics. Besides, it may be time-consuming to check these ECG signals manually. To overcome these limitations, we have proposed fast and accurate classifier that simulates the diagnosis of the cardiologist to classify the ECG signals into normal and abnormal from a single lead ECG signal and better than other well-known classifiers. First, an accurate algorithm is used for correcting the ECG signals from noise and extracting the major features of each ECG signal. After that, we simulated the characteristics of the ECG signals and created the proposed classifier from these characteristics. Two Neural Network (NN) classifiers, four Support Vector Machine (SVM) classifiers and K-Nearest Neighbor (KNN) classifier are employed to classify the ECG signals and compared with the proposed classifier. The total 13 features extracted from each ECG signal used in the proposed algorithm and set as input to the other classifiers. Our algorithm is validated using all records of MIT-BIH arrhythmia database. Experimental results show that the proposed classifier demonstrates better performance than other classifiers and yielded the highest average classification accuracy of 99%. Thus, our algorithm has the possibility to be implemented in clinical settings.","keywords_author":["ECG signals","Characteristics of ECG","NN","SVM","KNN"],"keywords_other":["GENETIC ALGORITHM","FEATURES","AUTOMATIC DETECTION","DECISION-MAKING","CLASSIFICATION","DECOMPOSITION","RECOGNITION","QRS COMPLEX","ELECTROCARDIOGRAM","MULTIRESOLUTION WAVELET TRANSFORM"],"max_cite":0.0,"pub_year":2018.0,"sources":"['wos']","rawkeys":["recognition","features","knn","decision-making","svm","characteristics of ecg","genetic algorithm","ecg signals","electrocardiogram","classification","multiresolution wavelet transform","nn","automatic detection","qrs complex","decomposition"],"tags":["decision making","recognition","ecg","features","neural networks","machine learning","characteristics of ecg","genetic algorithm","ecg signals","multiresolution wavelet transform","classification","qrs complexes","automatic detection","decomposition","k-nearest neighbors"]},{"p_id":12474,"title":"Deep convolutional neural networks for multi-modality isointense infant brain image segmentation","abstract":"The segmentation of infant brain tissue images into white matter (WM), gray matter (GM), and cerebrospinal fluid (CSF) plays an important role in studying early brain development in health and disease. In the isointense stage (approximately 6-8 months of age), WM and GM exhibit similar levels of intensity in both T1 and T2 MR images, making the tissue segmentation very challenging. Only a small number of existing methods have been designed for tissue segmentation in this isointense stage; however, they only used a single T1 or T2 images, or the combination of T1 and T2 images. In this paper, we propose to use deep convolutional neural networks (CNNs) for segmenting isointense stage brain tissues using multi-modality MR images. CNNs are a type of deep models in which trainable filters and local neighborhood pooling operations are applied alternatingly on the raw input images, resulting in a hierarchy of increasingly complex features. Specifically, we used multi-modality information from T1, T2, and fractional anisotropy (FA) images as inputs and then generated the segmentation maps as outputs. The multiple intermediate layers applied convolution, pooling, normalization, and other operations to capture the highly nonlinear mappings between inputs and outputs. We compared the performance of our approach with that of the commonly used segmentation methods on a set of manually segmented isointense stage brain images. Results showed that our proposed model significantly outperformed prior methods on infant brain tissue segmentation. In addition, our results indicated that integration of multi-modality images led to significant performance improvement. (C) 2014 Elsevier Inc. All rights reserved.","keywords_author":["Image segmentation","Multi-modality data","Infant brain image","Convolutional neural networks","Deep learning"],"keywords_other":["MR-IMAGES","TISSUE SEGMENTATION","SUBCORTICAL GRAY-MATTER","LEVEL SETS","NEONATAL BRAIN","AUTOMATIC SEGMENTATION","MAGNETIC-RESONANCE","RECOGNITION","CONSTRAINT","RECONSTRUCTION"],"max_cite":126.0,"pub_year":2015.0,"sources":"['ieee', 'wos']","rawkeys":["constraint","recognition","convolutional neural networks","infant brain image","deep learning","multi-modality data","tissue segmentation","subcortical gray-matter","magnetic-resonance","mr-images","reconstruction","neonatal brain","automatic segmentation","image segmentation","level sets"],"tags":["constraints","recognition","infant brain image","machine learning","tissue segmentation","subcortical gray-matter","multiresolution","mr-images","reconstruction","multi-modal data","convolutional neural network","neonatal brain","automatic segmentation","image segmentation","level set"]},{"p_id":69819,"title":"An efficient and effective convolutional auto-encoder extreme learning machine network for 3d feature learning","abstract":"3D shape features play a crucial role in graphics applications, such as 3D shape matching, recognition, and retrieval. Various 3D shape descriptors have been developed over the last two decades; however, existing descriptors are handcrafted features that are labor-intensively designed and cannot extract discriminative information for a large set of data. In this paper, we propose a rapid 3D feature learning method, namely, a convolutional auto-encoder extreme learning machine (CAE-ELM) that combines the advantages of the convolutional neuron network, auto-encoder, and extreme learning machine (ELM). This method performs better and faster than other methods. In addition, we define a novel architecture based on CAE-ELM. The architecture accepts two types of 3D shape representation, namely, voxel data and signed distance field data (SDF), as inputs to extract the global and local features of 3D shapes. Voxel data describe structural information, whereas SDF data contain details on 3D shapes. Moreover, the proposed CAE-ELM can be used in practical graphics applications, such as 3D shape completion. Experiments show that the features extracted by CAE-ELM are superior to existing hand-crafted features and other deep learning methods or ELM models. Moreover, the classification accuracy of the proposed architecture is superior to that of other methods on ModelNet10 (91.4%) and ModelNet40 (8435%). The training process also runs faster than existing deep learning methods by approximately two orders of magnitude. (C) 2015 Elsevier B.V. All rights reserved.","keywords_author":["Convolutional","Extreme learning machine","Auto-encoder","Feature learning"],"keywords_other":["FEEDFORWARD NETWORKS","REGRESSION","ALGORITHM","RETRIEVAL","RECOGNITION","MECHANISM","MAPREDUCE","SHAPE","ELMS"],"max_cite":15.0,"pub_year":2016.0,"sources":"['ieee', 'wos']","rawkeys":["elms","algorithm","auto-encoder","recognition","mechanism","convolutional","mapreduce","shape","retrieval","feedforward networks","feature learning","extreme learning machine","regression"],"tags":["recognition","mechanisms","convolution","auto encoders","feed-forward network","shape","map-reduce","retrieval","algorithms","feature learning","extreme learning machine","regression"]},{"p_id":53435,"title":"Features to determine clause-dependency in syntactic analysis","abstract":"The longer the input sentences, the worse the syntactic analysis results. Therefore, a long sentence is first segmented into several clauses. The syntactic analysis for each clause is performed, and finally all the analysis results are merged into one. In the merging process, it is difficult to determine the dependency among clauses. To handle such syntactic ambiguity among clauses, this paper proposes boosting-based clause-dependency determination method. We extract various features from clauses, and also analyze which features are effective on the clause-dependency determination. Our boositng-based method outperformed the previous methods, and showed that the most important feature was the surface form of the ending of a clause.","keywords_author":["Boosting","Clause","Dependency","Features","Machine learning","Syntactic analysis"],"keywords_other":["Clause-dependency","Merging process"],"max_cite":0.0,"pub_year":2007.0,"sources":"['scp']","rawkeys":["clause","features","clause-dependency","syntactic analysis","dependency","machine learning","merging process","boosting"],"tags":["recognition","clause","features","clause-dependency","syntactic analysis","machine learning","merging process","boosting"]},{"p_id":69821,"title":"Comparative analysis of shape descriptors for 3D objects","abstract":"One of the basic characteristics of an object is its shape. Several research areas in mathematics and computer science have taken an interest in object representation in both 2D images and 3D models, where shape descriptors are a powerful mechanism enabling the processes of classification, retrieval and comparison for object matching. In this paper, we present a literature survey of this broad field, including a comparative analysis based on the above shape descriptor processes. In view of their significance, we identified the shape descriptors implemented using the concept of visual salience. This paper gives an overview of this topic.","keywords_author":["Shape descriptors","Matching and similarity","Voxelization","Pose normalization and visual salience"],"keywords_other":["MODEL RETRIEVAL","TRANSFORM","FEATURES","REPRESENTATION","SEARCH","CLASSIFICATION","SYSTEM","RECOGNITION","SIMILARITY","IMAGES"],"max_cite":1.0,"pub_year":2017.0,"sources":"['wos']","rawkeys":["recognition","images","search","features","model retrieval","similarity","voxelization","representation","shape descriptors","matching and similarity","pose normalization and visual salience","system","classification","transform"],"tags":["recognition","images","search","features","model retrieval","similarity","voxel","representation","shape descriptors","matching and similarity","pose normalization and visual salience","system","classification","transform"]},{"p_id":69822,"title":"Semi-direct tracking and mapping with RGB-D camera for MAV","abstract":"In this paper we present a novel semi-direct tracking and mapping (SDTAM) approach for RGB-D cameras which inherits the advantages of both direct and feature based methods, and consequently it achieves high efficiency, accuracy, and robustness. The input RGB-D frames are tracked with a direct method and keyframes are refined by minimizing a proposed measurement residual function which takes both geometric and depth information into account. A local optimization is performed to refine the local map while global optimization detects and corrects loops with the appearance based bag of words and a co-visibility weighted pose graph. Our method has higher accuracy on both trajectory tracking and surface reconstruction compared to state-of-the-art frame-to-frame or frame-model approaches. We test our system in challenging sequences with motion blur, fast pure rotation, and large moving objects, the results demonstrate it can still successfully obtain results with high accuracy. Furthermore, the proposed approach achieves real-time speed which only uses part of the CPU computation power, and it can be applied to embedded devices such as phones, tablets, or micro aerial vehicles (MAVs).","keywords_author":["RGB-D SLAM","Localization","Tracking","Mapping","Reconstruction","Real-time"],"keywords_other":["FRAMEWORK","RETRIEVAL","3D SHAPE","RECOGNITION","EFFICIENT","FUSION","HIGH-LEVEL FEATURE"],"max_cite":0.0,"pub_year":2016.0,"sources":"['ieee', 'wos']","rawkeys":["efficient","recognition","localization","rgb-d slam","framework","mapping","real-time","tracking","3d shape","reconstruction","high-level feature","retrieval","fusion"],"tags":["high-level features","recognition","localization","rgb-d slam","framework","real time","map","efficiency","3-d shape","tracking","reconstruction","retrieval","fusion"]},{"p_id":12479,"title":"Deep Convolutional Neural Networks Enable Discrimination of Heterogeneous Digital Pathology Images","abstract":"Pathological evaluation of tumor tissue is pivotal for diagnosis in cancer patients and automated image analysis approaches have great potential to increase precision of diagnosis and help reduce human error. In this study, we utilize several computational methods based on convolutional neural networks (CNN) and build a stand-alone pipeline to effectively classify different histopathology images across different types of cancer.","keywords_author":["Biomarkers","Classification","Convolutional Neural Network","Deep learning","Digital pathology imaging","Tumor heterogeneity"],"keywords_other":["MICROSCOPY","CARCINOMA","ACCURACY","LUNG ADENOCARCINOMA","BREAST-CANCER","INFORMATICS","RECOGNITION","LEARNING ALGORITHM","SPECIMENS","COMPUTATIONAL PATHOLOGY"],"max_cite":0.0,"pub_year":2017.0,"sources":"['ieee', 'wos']","rawkeys":["accuracy","informatics","carcinoma","recognition","biomarkers","deep learning","computational pathology","specimens","microscopy","lung adenocarcinoma","digital pathology imaging","classification","convolutional neural network","learning algorithm","breast-cancer","tumor heterogeneity"],"tags":["accuracy","informatics","carcinoma","recognition","biomarkers","computational pathology","specimens","machine learning","lung adenocarcinoma","digital pathology imaging","classification","convolutional neural network","learning algorithm","microscopy","tumor heterogeneity","breast cancer"]},{"p_id":12480,"title":"Robust vehicle detection in aerial images based on cascaded convolutional neural networks","abstract":"Vehicle detection in aerial images is an important and challenging task. Traditionally, many target detection models based on sliding-window fashion were developed and achieved acceptable performance, but these models are time-consuming in the detection phase. Recently, with the great success of convolutional neural networks (CNNs) in computer vision, many state-of-the-art detectors have been designed based on deep CNNs. However, these CNN-based detectors are inefficient when applied in aerial image data due to the fact that the existing CNN-based models struggle with small-size object detection and precise localization. To improve the detection accuracy without decreasing speed, we propose a CNN-based detection model combining two independent convolutional neural networks, where the first network is applied to generate a set of vehicle-like regions from multi-feature maps of different hierarchies and scales. Because the multi-feature maps combine the advantage of the deep and shallow convolutional layer, the first network performs well on locating the small targets in aerial image data. Then, the generated candidate regions are fed into the second network for feature extraction and decision making. Comprehensive experiments are conducted on the Vehicle Detection in Aerial Imagery (VEDAI) dataset and Munich vehicle dataset. The proposed cascaded detection model yields high performance, not only in detection accuracy but also in detection speed.","keywords_author":["Aerial image","Convolutional neural network","Deep learning","Vehicle detection","vehicle detection","convolutional neural network","aerial image","deep learning"],"keywords_other":["Aerial images","GRADIENTS","FEATURES","SYSTEM","State of the art","RECOGNITION","Vehicle detection","Aerial image data","Convolutional neural network","Acceptable performance","Detection models","OBJECT DETECTION","Detection accuracy"],"max_cite":1.0,"pub_year":2017.0,"sources":"['scp', 'wos']","rawkeys":["recognition","features","deep learning","detection accuracy","state of the art","system","vehicle detection","aerial image","gradients","detection models","convolutional neural network","acceptable performance","aerial images","object detection","aerial image data"],"tags":["recognition","features","detection accuracy","machine learning","state of the art","system","vehicle detection","gradient","detection models","convolutional neural network","acceptable performance","aerial images","object detection","aerial image data"]},{"p_id":69820,"title":"An Improved EMD-Based Dissimilarity Metric for Unsupervised Linear Subspace Learning","abstract":"We investigate a novel way of robust face image feature extraction by adopting the methods based on Unsupervised Linear Subspace Learning to extract a small number of good features. Firstly, the face image is divided into blocks with the specified size, and then we propose and extract pooled Histogram of Oriented Gradient (pHOG) over each block. Secondly, an improved Earth Mover's Distance (EMD) metric is adopted to measure the dissimilarity between blocks of one face image and the corresponding blocks from the rest of face images. Thirdly, considering the limitations of the original Locality Preserving Projections (LPP), we proposed the Block Structure LPP (BSLPP), which effectively preserves the structural information of face images. Finally, an adjacency graph is constructed and a small number of good features of a face image are obtained by methods based on Unsupervised Linear Subspace Learning. A series of experiments have been conducted on several well-known face databases to evaluate the effectiveness of the proposed algorithm. In addition, we construct the noise, geometric distortion, slight translation, slight rotation AR, and Extended Yale B face databases, and we verify the robustness of the proposed algorithm when faced with a certain degree of these disturbances.","keywords_author":null,"keywords_other":["DISCRIMINANT-ANALYSIS","QUALITY","FRAMEWORK","RECOGNITION","HISTOGRAM","DIMENSIONALITY REDUCTION","CAMERA CALIBRATION"],"max_cite":0.0,"pub_year":2018.0,"sources":"['wos']","rawkeys":["camera calibration","quality","recognition","framework","discriminant-analysis","histogram","dimensionality reduction"],"tags":["camera calibration","quality","recognition","denoising autoencoder","framework","histograms","dimensionality reduction"]},{"p_id":12483,"title":"Improved Audio Scene Classification Based on Label-Tree Embeddings and Convolutional Neural Networks","abstract":"In this paper, we present an efficient approach for audio scene classification. We aim at learning representations for scene examples by exploring the structure of their class labels. A category taxonomy is automatically learned by collectively optimizing a tree-structured clustering of the given labels into multiple metaclasses. A scene recording is then transformed into a label-tree embedding image. Elements of the image represent the likelihoods that the scene instance belongs to the metaclasses. We investigate classification with label-tree embedding features learned from different low-level features as well as their fusion. We show that the combination of multiple features is essential to obtain good performance. While averaging label-tree embedding images over time yields good performance, we argue that average pooling possesses an intrinsic shortcoming. We alternatively propose an improved classification scheme to bypass this limitation. We aim at automatically learning common templates that are useful for the classification task from these images using simple but tailored convolutional neural networks. The trained networks are then employed as a feature extractor that matches the learned templates across a label-tree embedding image and produce the maximum matching scores as features for classification. Since audio scenes exhibit rich content, template learning and matching on low-level features would be inefficient. With label-tree embedding features, we have quantized and reduced the low-level features into the likelihoods of the metaclasses, on which the template learning and matching are efficient. We study both training convolutional neural networks on stacked label-tree embedding images and multistream networks. Experimental results on the DCASE2016 and LITIS Rouen datasets demonstrate the efficiency of the proposed methods.","keywords_author":["Audio scene classification","label tree embedding","convolutional neural network","multi-stream","template matching"],"keywords_other":["EVENTS","FORESTS","RECOGNITION","FEATURES"],"max_cite":1.0,"pub_year":2017.0,"sources":"['ieee', 'wos']","rawkeys":["recognition","multi-stream","features","events","forests","convolutional neural network","template matching","label tree embedding","audio scene classification"],"tags":["recognition","multi-stream","features","events","forests","convolutional neural network","template matching","label tree embedding","audio scene classification"]},{"p_id":12485,"title":"Multisource Transfer Learning With Convolutional Neural Networks for Lung Pattern Analysis","abstract":"Early diagnosis of interstitial lung diseases is crucial for their treatment, but even experienced physicians find it difficult, as their clinical manifestations are similar. In order to assist with the diagnosis, computer-aided diagnosis systems have been developed. These commonly rely on a fixed scale classifier that scans CT images, recognizes textural lung patterns, and generates a map of pathologies. In a previous study, we proposed a method for classifying lung tissue patterns using a deep convolutional neural network (CNN), with an architecture designed for the specific problem. In this study, we present an improved method for training the proposed network by transferring knowledge from the similar domain of general texture classification. Six publicly available texture databases are used to pretrain networks with the proposed architecture, which are then fine-tuned on the lung tissue data. The resulting CNNs are combined in an ensemble and their fused knowledge is compressed back to a network with the original architecture. The proposed approach resulted in an absolute increase of about 2% in the performance of the proposed CNN. The results demonstrate the potential of transfer learning in the field of medical image analysis, indicate the textural nature of the problem and show that the method used for training a network can be as important as designing its architecture.","keywords_author":["Convolutional neural networks (CNNs)","interstitial lung diseases (ILDs)","knowledge distillation","model compression","model ensemble","texture classification","transfer learning"],"keywords_other":["COMPUTED-TOMOGRAPHY","CLASSIFICATION","DISEASES","RECOGNITION","TISSUE-ANALYSIS","CT"],"max_cite":7.0,"pub_year":2017.0,"sources":"['ieee', 'wos']","rawkeys":["texture classification","model compression","recognition","transfer learning","tissue-analysis","knowledge distillation","model ensemble","classification","ct","convolutional neural networks (cnns)","interstitial lung diseases (ilds)","diseases","computed-tomography"],"tags":["model compression","recognition","transfer learning","tissue-analysis","disease","interstitial lung disease","knowledge distillation","model ensemble","classification","computed tomography","convolutional neural network","texture classification"]},{"p_id":12487,"title":"A practical approach for detection and classification of traffic signs using Convolutional Neural Networks","abstract":"Automatic detection and classification of traffic signs is an important task in smart and autonomous cars. Convolutional Neural Networks has shown a great success in classification of traffic signs and they have surpassed human performance on a challenging dataset called the German Traffic Sign Benchmark. However, these ConvNets suffer from two important issues. They are not computationally suitable for real-time applications in practice. Moreover, they cannot be used for detecting traffic signs for the same reason. In this paper, we propose a lightweight and accurate ConvNet for detecting traffic signs and explain how to implement the sliding window technique within the ConvNet using dilated convolutions. Then, we further optimize our previously proposed real-time ConvNet for the task of traffic sign classification and make it faster and more accurate. Our experiments on the German Traffic Sign Benchmark datasets show that the detection ConvNet locates the traffic signs with average precision equal to 99.89%. Using our sliding window implementation, it is possible to process 37.72 high-resolution images per second in a multi-scale fashion and locate traffic signs. Moreover, single ConvNet proposed for the task of classification is able to classify 99.55% of the test samples, correctly. Finally, our stability analysis reveals that the ConvNet is tolerant against Gaussian noise when sigma < 10. (C) 2016 Elsevier B.V. All rights reserved.","keywords_author":["Convolutional Neural Networks","Traffic sign detection","Traffic sign classification","Sliding window detection","Dense prediction"],"keywords_other":["RECOGNITION"],"max_cite":10.0,"pub_year":2016.0,"sources":"['wos']","rawkeys":["traffic sign classification","recognition","convolutional neural networks","dense prediction","traffic sign detection","sliding window detection"],"tags":["traffic sign classification","recognition","dense prediction","convolutional neural network","traffic sign detection","sliding window detection"]},{"p_id":4302,"title":"Deep learning for single-molecule science","abstract":"Exploring and making predictions based on single-molecule data can be challenging, not only due to the sheer size of the datasets, but also because a priori knowledge about the signal characteristics is typically limited and poor signal-to-noise ratio. For example, hypothesisdriven data exploration, informed by an expectation of the signal characteristics, can lead to interpretation bias or loss of information. Equally, even when the different data categories are known, e.g., the four bases in DNA sequencing, it is often difficult to know how to make best use of the available information content. The latest developments in machine learning (ML), so-called deep learning (DL) offer interesting, new avenues to address such challenges. In some applications, such as speech and image recognition, DL has been able to outperform conventional ML strategies and even human performance. However, to date DL has not been applied much in single-molecule science, presumably in part because relatively little is known about the 'internal workings' of such DL tools within single-molecule science as a field. In this Tutorial, we make an attempt to illustrate in a step-by-step guide how one of those, a convolutional neural network (CNN), may be used for base calling in DNA sequencing applications. We compare it with a SVM as a more conventional ML method, and discuss some of the strengths and weaknesses of the approach. In particular, a 'deep' neural network has many features of a 'black box', which has important implications on how we look at and interpret data.","keywords_author":["data analysis","deep learning","machine learning","nanoscience","nanotechnology","single-molecule","deep learning","single-molecule","data analysis","nanoscience","nanotechnology","machine learning"],"keywords_other":["Latest development","BREAK JUNCTION","Single molecule","TRANSPORT","ALGORITHM","GOLD","REPRESENTATIONS","Signal characteristic","NEURAL-NETWORKS","Human performance","RECOGNITION","Convolutional neural network","Data exploration","Priori knowledge","Information contents"],"max_cite":3.0,"pub_year":2017.0,"sources":"['ieee', 'scp', 'wos']","rawkeys":["information contents","break junction","convolutional neural network","transport","nanotechnology","nanoscience","neural-networks","signal characteristic","machine learning","data analysis","algorithm","recognition","deep learning","priori knowledge","gold","single-molecule","data exploration","representations","single molecule","latest development","human performance"],"tags":["information contents","break junction","convolutional neural network","nanotechnology","nanoscience","signal characteristic","machine learning","data analysis","algorithms","recognition","neural networks","priori knowledge","communication","gold","data exploration","representation","single molecule","latest development","human performance"]},{"p_id":12498,"title":"CNNH_PSS: protein 8-class secondary structure prediction by convolutional neural network with highway","abstract":"Background: Protein secondary structure is the three dimensional form of local segments of proteins and its prediction is an important problem in protein tertiary structure prediction. Developing computational approaches for protein secondary structure prediction is becoming increasingly urgent.","keywords_author":["Protein secondary structure","Convolutional neural network","Highway","Local context","Long-range interdependency"],"keywords_other":["PROFILES","ACCURACY","FEATURES","IDENTIFICATION","SEQUENCE","SUPPORT VECTOR MACHINES","EVOLUTIONARY INFORMATION","RECOGNITION"],"max_cite":0.0,"pub_year":2018.0,"sources":"['wos']","rawkeys":["accuracy","identification","recognition","highway","features","protein secondary structure","long-range interdependency","evolutionary information","sequence","convolutional neural network","support vector machines","profiles","local context"],"tags":["accuracy","identification","recognition","highway","features","protein secondary structure","long-range interdependency","machine learning","evolutionary information","sequence","convolutional neural network","local contexts","profiles"]},{"p_id":69852,"title":"Automatic music genre classification based on musical instrument track separation","abstract":"The aim of this article is to investigate whether separating music tracks at the pre-processing phase and extending feature vector by parameters related to the specific musical instruments that are characteristic for the given musical genre allow for efficient automatic musical genre classification in case of database containing thousands of music excerpts and a dozen of genres. Results of extensive experiments show that the approach proposed for music genre classification is promising. Overall, conglomerating parameters derived from both an original audio and a mixture of separated tracks improve classification effectiveness measures, demonstrating that the proposed feature vector and the Support Vector Machine (SVM) with Co-training mechanism are applicable to a large dataset.","keywords_author":["Music information retrieval (MIR)","Automatic music genre classification","Automatic separation of music tracks","Support vector machine (SVM)"],"keywords_other":["NONNEGATIVE MATRIX FACTORIZATION","FEATURES","AUDIO","SYSTEMS","RECOGNITION","SIGNALS"],"max_cite":0.0,"pub_year":2018.0,"sources":"['wos']","rawkeys":["signals","recognition","features","nonnegative matrix factorization","music information retrieval (mir)","automatic separation of music tracks","support vector machine (svm)","audio","systems","automatic music genre classification"],"tags":["signals","recognition","features","nonnegative matrix factorization","machine learning","system","automatic separation of music tracks","audio","retrieval","automatic music genre classification"]},{"p_id":69855,"title":"Relation Enhanced Neural Model for Type Classification of Entity Mentions with a Fine-Grained Taxonomy","abstract":"Inferring semantic types of the entity mentions in a sentence is a necessary yet challenging task. Most of existing methods employ a very coarse-grained type taxonomy, which is too general and not exact enough for many tasks. However, the performances of the methods drop sharply when we extend the type taxonomy to a fine-grained one with several hundreds of types. In this paper, we introduce a hybrid neural network model for type classification of entity mentions with a fine-grained taxonomy. There are four components in our model, namely, the entity mention component, the context component, the relation component, the already known type component, which are used to extract features from the target entity mention, context, relations and already known types of the entity mentions in surrounding context respectively. The learned features by the four components are concatenated and fed into a softmax layer to predict the type distribution. We carried out extensive experiments to evaluate our proposed model. Experimental results demonstrate that our model achieves state-of-the-art performance on the FIGER dataset. Moreover, we extracted larger datasets from Wikipedia and DBpedia. On the larger datasets, our model achieves the comparable performance to the state-of-the-art methods with the coarse-grained type taxonomy, but performs much better than those methods with the fine-grained type taxonomy in terms of micro-F1, macro-F1 and weighted-F1.","keywords_author":["entity mention classification","entity mention relation","fine-grained taxonomy"],"keywords_other":["RECOGNITION"],"max_cite":1.0,"pub_year":2017.0,"sources":"['wos']","rawkeys":["recognition","fine-grained taxonomy","entity mention classification","entity mention relation"],"tags":["recognition","fine-grained taxonomy","entity mention classification","entity mention relation"]},{"p_id":69857,"title":"Large vocabulary automatic chord estimation using bidirectional long short-term memory recurrent neural network with even chance training","abstract":"This paper presents an argument for the necessity of a large vocabulary in automatic chord recognition systems, on the grounds of the requirements of machine musicianship. It proposes a system framework with a skewed class-sensitive training scheme that leads to a preliminary solution to large vocabulary automatic chord estimation. This framework applies a bidirectional long short-term memory recurrent neural network architecture, which employs an 'even chance' training scheme to make up for the lack of uncommon chords' exposure. The main drawback of this approach is the low segmentation quality, which inevitably lowers the upper bound of chord estimation accuracy. Under a large vocabulary evaluation, the proposed system can significantly outperform the baseline system in terms of the overall weighted chord symbol recall, and there is no significant difference between them in terms of average chord quality accuracy. The results demonstrate preliminary success in our approach, and also prove the even chance training scheme to be effective in boosting uncommon chord symbol recalls as well as the average chord quality accuracy.","keywords_author":["Music information retrieval","automatic chord estimation","deep learning","recurrent neural network","large vocabulary"],"keywords_other":["AUDIO","MUSIC","RECOGNITION","TIME","DEEP ARCHITECTURES"],"max_cite":0.0,"pub_year":2018.0,"sources":"['ieee', 'wos']","rawkeys":["recognition","deep learning","music information retrieval","deep architectures","recurrent neural network","time","automatic chord estimation","large vocabulary","audio","music"],"tags":["recognition","neural networks","deep architectures","machine learning","time","automatic chord estimation","large vocabulary","audio","retrieval","music"]},{"p_id":4336,"title":"Joint Estimation of Age and Expression by Combining Scattering and Convolutional Networks","abstract":"This article tackles the problem of joint estimation of human age and facial expression. This is an important yet challenging problem because expressions can alter face appearances in a similar manner to human aging. Different from previous approaches that deal with the two tasks independently, our approach trains a convolutional neural network (CNN) model that unifies ordinal regression and multi-class classification in a single framework. We demonstrate experimentally that our method performs more favorably against state-of-the-art approaches.","keywords_author":["Age estimation","deep learning","convolutional networks","expression recognition","multi-task learning","multi-level regression","scattering network","transfer learning"],"keywords_other":["APPEARANCE","FEATURES","DATABASE","RECOGNITION","RANK ESTIMATION","FACIAL EXPRESSIONS","PATTERNS","SINGLE IMAGE"],"max_cite":0.0,"pub_year":2018.0,"sources":"['wos', 'ieee']","rawkeys":["single image","expression recognition","multi-task learning","recognition","features","deep learning","transfer learning","facial expressions","convolutional networks","appearance","age estimation","database","patterns","rank estimation","multi-level regression","scattering network"],"tags":["single images","expression recognition","recognition","databases","features","transfer learning","facial expressions","machine learning","patterns","appearance","age estimation","rank estimation","multitask learning","scattering networks","multi-level regression","convolutional neural network"]},{"p_id":12530,"title":"Fast Automatic Airport Detection in Remote Sensing Images Using Convolutional Neural Networks","abstract":"Fast and automatic detection of airports from remote sensing images is useful for many military and civilian applications. In this paper, a fast automatic detection method is proposed to detect airports from remote sensing images based on convolutional neural networks using the Faster R-CNN algorithm. This method first applies a convolutional neural network to generate candidate airport regions. Based on the features extracted from these proposals, it then uses another convolutional neural network to perform airport detection. By taking the typical elongated linear geometric shape of airports into consideration, some specific improvements to the method are proposed. These approaches successfully improve the quality of positive samples and achieve a better accuracy in the final detection results. Experimental results on an airport dataset, Landsat 8 images, and a Gaofen-1 satellite scene demonstrate the effectiveness and efficiency of the proposed method.","keywords_author":["airport detection","convolutional neural network","region proposal network"],"keywords_other":["RECOGNITION"],"max_cite":1.0,"pub_year":2018.0,"sources":"['wos']","rawkeys":["region proposal network","airport detection","recognition","convolutional neural network"],"tags":["region proposal network","airport detection","recognition","convolutional neural network"]},{"p_id":4340,"title":"Understanding Innovation Engines: Automated Creativity and Improved Stochastic Optimization via Deep Learning","abstract":"The Achilles Heel of stochastic optimization algorithms is getting trapped on local optima. Novelty Search mitigates this problem by encouraging exploration in all interesting directions by replacing the performance objective with a reward for novel behaviors. This reward for novel behaviors has traditionally required a human-crafted, behavioral distance function. While Novelty Search is a major conceptual breakthrough and outperforms traditional stochastic optimization on certain problems, it is not clear how to apply it to challenging, high-dimensional problems where specifying a useful behavioral distance function is difficult. For example, in the space of images, how do you encourage novelty to produce hawks and heroes instead of endless pixel static? Here we propose a new algorithm, the Innovation Engine, that builds on Novelty Search by replacing the human-crafted behavioral distance with a Deep Neural Network (DNN) that can recognize interesting differences between phenotypes. The key insight is that DNNs can recognize similarities and differences between phenotypes at an abstract level, wherein novelty means interesting novelty. For example, a DNN-based novelty search in the image space does not explore in the low-level pixel space, but instead creates a pressure to create new types of images (e.g., churches, mosques, obelisks, etc.). Here, we describe the long-term vision for the Innovation Engine algorithm, which involves many technical challenges that remain to be solved. We then implement a simplified version of the algorithm that enables us to explore some of the algorithm's key motivations. Our initial results, in the domain of images, suggest that Innovation Engines could ultimately automate the production of endless streams of interesting solutions in any domain: for example, producing intelligent software, robot controllers, optimized physical components, and art.","keywords_author":["Genetic algorithms","deep neural networks","CPPNs","MAP-Elites"],"keywords_other":["MODULARITY","DESIGN","NEURAL-NETWORKS","RECOGNITION","ALGORITHMS","ROBOTS","EVOLUTION"],"max_cite":5.0,"pub_year":2016.0,"sources":"['ieee', 'wos']","rawkeys":["neural-networks","design","map-elites","modularity","recognition","robots","deep neural networks","genetic algorithms","algorithms","evolution","cppns"],"tags":["design","map-elites","modularity","recognition","neural networks","genetic algorithm","robotics","convolutional neural network","algorithms","biological","cppns"]},{"p_id":4341,"title":"Deep Convolutional Neural Networks for Image Classification: A Comprehensive Review","abstract":"Convolutional neural networks (CNNs) have been applied to visual tasks since the late 1980s. However, despite a few scattered applications, they were dormant until the mid-2000s when developments in computing power and the advent of large amounts of labeled data, supplemented by improved algorithms, contributed to their advancement and brought them to the forefront of a neural network renaissance that has seen rapid progression since 2012. In this review, which focuses on the application of CNNs to image classification tasks, we cover their development, from their predecessors up to recent state-of-the-art deep learning systems. Along the way, we analyze (1) their early successes, (2) their role in the deep learning renaissance, (3) selected symbolic works that have contributed to their recent popularity, and (4) several improvement attempts by reviewing contributions and challenges of over 300 publications. We also introduce some of their current trends and remaining challenges.","keywords_author":null,"keywords_other":["DIMENSIONALITY","REGULARIZATION","REPRESENTATION","GRADIENT DESCENT","RECEPTIVE FIELDS","MODEL","RECOGNITION","LEARNING ALGORITHM","TERM","FACE VERIFICATION"],"max_cite":7.0,"pub_year":2017.0,"sources":"['wos', 'ieee']","rawkeys":["dimensionality","recognition","model","term","representation","face verification","gradient descent","receptive fields","learning algorithm","regularization"],"tags":["dimensionality","recognition","model","term","representation","random forests","face verification","gradient descent","learning algorithm","regularization"]},{"p_id":4342,"title":"Statistics of Visual Responses to Image Object Stimuli from Primate AIT Neurons to DNN Neurons","abstract":"Under the goal-driven paradigm, Yamins etal. (2014; Yamins & DiCarlo, 2016) have shown that by optimizing only the final eight-way categorization performance of a four-layer hierarchical network, not only can its top output layer quantitatively predict IT neuron responses but its penultimate layer can also automatically predict V4 neuron responses. Currently, deep neural networks (DNNs) in the field of computer vision have reached image object categorization performance comparable to that of human beings on ImageNet, a data set that contains 1.3 million training images of 1000 categories. We explore whether the DNN neurons (units in DNNs) possess image object representational statistics similar to monkey IT neurons, particularly when the network becomes deeper and the number of image categories becomes larger, using VGG19, a typical and widely used deep network of 19 layers in the computer vision field. Following Lehky, Kiani, Esteky, and Tanaka (2011, 2014), where the response statistics of 674 IT neurons to 806 image stimuli are analyzed using three measures (kurtosis, Pareto tail index, and intrinsic dimensionality), we investigate the three issues in this letter using the same three measures: (1) the similarities and differences of the neural response statistics between VGG19 and primate IT cortex, (2) the variation trends of the response statistics of VGG19 neurons at different layers from low to high, and (3) the variation trends of the response statistics of VGG19 neurons when the numbers of stimuli and neurons increase. We find that the response statistics on both single-neuron selectivity and population sparseness of VGG19 neurons are fundamentally different from those of IT neurons in most cases; by increasing the number of neurons in different layers and the number of stimuli, the response statistics of neurons at different layers from low to high do not substantially change; and the estimated intrinsic dimensionality values at the low convolutional layers of VGG19 are considerably larger than the value of approximately 100 reported for IT neurons in Lehky etal. (2014), whereas those at the high fully connected layers are close to or lower than 100. To the best of our knowledge, this work is the first attempt to analyze the response statistics of DNN neurons with respect to primate IT neurons in image object representation.","keywords_author":null,"keywords_other":["DIMENSIONALITY","SPACE","INFEROTEMPORAL CORTEX","RECOGNITION","SELECTIVITY","MODELS","SPARSENESS","TEMPORAL CORTEX"],"max_cite":1.0,"pub_year":2018.0,"sources":"['wos', 'ieee']","rawkeys":["dimensionality","recognition","sparseness","space","temporal cortex","inferotemporal cortex","models","selectivity"],"tags":["dimensionality","recognition","model","space","temporal cortex","sparse","inferotemporal cortex","selection"]},{"p_id":61689,"title":"Visual Grasp Affordance Localization in Point Clouds Using Curved Contact Patches","abstract":"Detecting affordances on objects is one of the main open problems in robotic manipulation. This paper presents a new method to represent and localize grasp affordances as bounded curved contact patches (paraboloids) of the size of the robotic hand. In particular, given a three-dimensional (3D) point cloud from a range sensor, a set of potential grasps is localized on a detected object by a fast contact patch fitting and validation process. For the object detection, three standard methods from the literature are used and compared. The potential grasps on the object are then re fined to a single affordance using their shape (size and curvature) and pose (reachability and minimum torque effort) properties, with respect to the robot and the manipulation task. We apply the proposed method to a circular valve turning task, verifying the ability to accurately and rapidly localize grasp affordances, under significant uncertainty in the environment. We experimentally validate the method with the humanoid robot COMAN on 10 circular control valves fixed on a wall, from five different viewpoints and robot poses for each valve. We compare the reliability of the introduced local grasp affordances method to the baseline that relies only on object detection, illustrating the superiority of ours for the valve turning task.","keywords_author":["3D perception","manipulation","grasp affordances","bounded curved patches","contacts","humanoids"],"keywords_other":["OBJECTS","RECOGNITION","TASK"],"max_cite":0.0,"pub_year":2017.0,"sources":"['wos']","rawkeys":["humanoids","bounded curved patches","3d perception","recognition","contacts","objects","grasp affordances","task","manipulation"],"tags":["contact","humanoids","bounded curved patches","3d perception","recognition","objects","grasp affordances","task","manipulation"]},{"p_id":4345,"title":"End-to-end, sequence-to-sequence probabilistic visual odometry through deep neural networks","abstract":"\u00a9 2017, \u00a9 The Author(s) 2017.This paper studies visual odometry (VO) from the perspective of deep learning. After tremendous efforts in the robotics and computer vision communities over the past few decades, state-of-the-art VO algorithms have demonstrated incredible performance. However, since the VO problem is typically formulated as a pure geometric problem, one of the key features still missing from current VO systems is the capability to automatically gain knowledge and improve performance through learning. In this paper, we investigate whether deep neural networks can be effective and beneficial to the VO problem. An end-to-end, sequence-to-sequence probabilistic visual odometry (ESP-VO) framework is proposed for the monocular VO based on deep recurrent convolutional neural networks. It is trained and deployed in an end-to-end manner, that is, directly inferring poses and uncertainties from a sequence of raw images (video) without adopting any modules from the conventional VO pipeline. It can not only automatically learn effective feature representation encapsulating geometric information through convolutional neural networks, but also implicitly model sequential dynamics and relation for VO using deep recurrent neural networks. Uncertainty is also derived along with the VO estimation without introducing much extra computation. Extensive experiments on several datasets representing driving, flying and walking scenarios show competitive performance of the proposed ESP-VO to the state-of-the-art methods, demonstrating a promising potential of the deep learning technique for VO and verifying that it can be a viable complement to current VO systems.","keywords_author":["deep learning","pose estimation","recurrent convolutional neural networks","uncertainty","Visual odometry","Visual odometry","pose estimation","uncertainty","deep learning","recurrent convolutional neural networks"],"keywords_other":["State-of-the-art methods","REPRESENTATION","Geometric information","Competitive performance","uncertainty","DATASET","Visual odometry","SLAM","VISION","Feature representation","RECOGNITION","STEREO","Convolutional neural network","MEMORY","CAMERA","Pose estimation"],"max_cite":0.0,"pub_year":2018.0,"sources":"['scp', 'wos', 'ieee']","rawkeys":["competitive performance","pose estimation","state-of-the-art methods","visual odometry","geometric information","dataset","memory","recognition","deep learning","recurrent convolutional neural networks","uncertainty","representation","slam","vision","camera","convolutional neural network","feature representation","stereo"],"tags":["competitive performance","pose estimation","state-of-the-art methods","visual odometry","geometric information","recognition","memory","recurrent convolutional neural network","uncertainty","machine learning","representation","vision","cameras","data sets","robotics","convolutional neural network","feature representation","stereo"]},{"p_id":61691,"title":"Teaching robots to do object assembly using multi-modal 3D vision","abstract":"The motivation of this paper is to develop an intelligent robot assembly system using multi-modal vision for next-generation industrial assembly. The system includes two phases where in the first phase human beings demonstrate assembly to robots and in the second phase robots detect objects, plan grasps, and assemble objects following human demonstration using Al searching. A notorious difficulty to implement such a system is the bad precision of 3D visual detection. This paper presents multi-modal approaches to overcome the difficulty: It uses AR markers in the teaching phase to detect human operation, and uses point clouds and geometric constraints in the robot execution phase to avoid unexpected occlusion and noises. The paper presents several experiments to examine the precision and correctness of the approaches. It demonstrates the applicability of the approaches by integrating them with graph model-based motion planning, and by executing the results on industrial robots in real-world scenarios. (C) 2017 Elsevier B.V. All rights reserved.","keywords_author":["3D visual detection","Robot manipulation","Motion planning"],"keywords_other":["POSE ESTIMATION","RECOGNITION","TRACKING"],"max_cite":1.0,"pub_year":2017.0,"sources":"['wos']","rawkeys":["pose estimation","3d visual detection","recognition","robot manipulation","tracking","motion planning"],"tags":["pose estimation","3d visual detection","recognition","robotic manipulators","tracking","motion planning"]},{"p_id":53500,"title":"Indoor navigation and workpiece outline recognition for autonomous construction machinery","abstract":"We study basic functions for autonomous construction machines, such as indoor navigation, workpiece outline recognition and machine learning method. The indoor navigation is based on laser scanners and 2D map matching of the surrounding walls. The positioning results are accurate in spite of obstacles. We simplify object recognition problem -without any visual markers by focusing on the workpieces with straight edges and a single chromatic color. The silhouettes of the workpieces are separated by a saturation filter and symbolized as the groups of line segments. Finally, new machine learning method is proposed. We focus on an improvement of the Q-Learning reward system. In our proposed method, machines can obtain the rules and surrounding maps with less parameter and action rules known in advance.","keywords_author":["Autonomous","Indoor navigation","Machine learning","Recognition","Vision system"],"keywords_other":["Map matching","Object recognition problem","Straight edge","Basic functions","Construction machinery","Visual markers","Reward systems","Action rules","Indoor navigation","Vision systems","Line segment","Q-learning","Autonomous","Laser scanner","Work pieces","Machine learning methods","Recognition","Chromatic colors","Construction machines"],"max_cite":0.0,"pub_year":2006.0,"sources":"['scp']","rawkeys":["chromatic colors","q-learning","basic functions","object recognition problem","indoor navigation","visual markers","reward systems","machine learning","construction machinery","straight edge","vision systems","recognition","vision system","laser scanner","machine learning methods","action rules","line segment","work pieces","autonomous","construction machines","map matching"],"tags":["chromatic colors","q-learning","basic functions","object recognition problem","visual markers","machine learning","rewarding system","construction machinery","straight edge","vision systems","recognition","least square","machine learning methods","action rules","line segment","work pieces","autonomous","construction machines","in-door navigations","map matching"]},{"p_id":61693,"title":"A gesture-based telemanipulation control for a robotic arm with biofeedback-based grasp","abstract":"Purpose - The idea is to exploit the natural stability and performance of the human arm during movement, execution and manipulation. The purpose of this paper is to remotely control a handling robot with a low cost but effective solution.","keywords_author":["Control","Robotics","Teleoperation","Kinect sensor","Gesture control","Telemanipulation","Human-Robot-Interface","EMG signal-based control"],"keywords_other":["INTERFACE","UPPER-LIMB","CLASSIFICATION","EMG SIGNALS","RECOGNITION","KINECT SENSOR"],"max_cite":0.0,"pub_year":2017.0,"sources":"['wos']","rawkeys":["gesture control","telemanipulation","recognition","human-robot-interface","interface","control","kinect sensor","teleoperation","robotics","upper-limb","classification","emg signal-based control","emg signals"],"tags":["gesture control","human-robot interface","recognition","telemanipulation","interface","upper limbs","control","kinect sensor","teleoperation","emg signal","robotics","classification","emg signal-based control"]},{"p_id":12545,"title":"Effective training of convolutional neural networks for face-based gender and age prediction","abstract":"Convolutional Neural Networks (CNNs) have been proven very effective for human demographics estimation by a number of recent studies. However, the proposed solutions significantly vary in different aspects leaving many open questions on how to choose an optimal CNN architecture and which training strategy to use. In this work, we shed light on some of these questions improving the existing CNN-based approaches for gender and age prediction and providing practical hints for future studies. In particular, we analyse four important factors of the CNN training for gender recognition and age estimation: (1) the target age encoding and loss function, (2) the CNN depth, (3) the need for pretraining, and (4) the training strategy: mono-task or multi-task. As a result, we design the state-of-the-art gender recognition and age estimation models according to three popular benchmarks: LFW, MORPH-II and FG-NET. Moreover, our best model won the ChaLearn Apparent Age Estimation Challenge 2016 significantly outperforming, the solutions of other participants. (C) 2017 Elsevier Ltd. All rights reserved.","keywords_author":["Age estimation","Convolutional neural network","Deep learning","Gender recognition","Soft biometrics","Gender recognition","Age estimation","Convolutional neural network","Soft biometrics","Deep learning"],"keywords_other":["REGRESSION","Training strategy","INFORMATION","Gender recognition","Soft biometrics","Age predictions","Loss functions","State of the art","CLASSIFICATION","DATABASE","RECOGNITION","MODEL","HISTOGRAM","Convolutional neural network","PATTERNS","Age estimation","IMAGES"],"max_cite":1.0,"pub_year":2017.0,"sources":"['ieee', 'scp', 'wos']","rawkeys":["gender recognition","loss functions","images","model","recognition","deep learning","training strategy","state of the art","patterns","age estimation","database","histogram","classification","convolutional neural network","information","soft biometrics","age predictions","regression"],"tags":["gender recognition","loss functions","images","model","databases","recognition","training strategy","state of the art","machine learning","histograms","age estimation","patterns","age prediction","classification","convolutional neural network","information","soft biometrics","regression"]},{"p_id":69892,"title":"Incremental Learning by Message Passing in Hierarchical Temporal Memory","abstract":"Hierarchical temporal memory (HTM) is a biologically inspired framework that can be used to learn invariant representations of patterns in a wide range of applications. Classical HTM learning is mainly unsupervised, and once training is completed, the network structure is frozen, thus making further training (i.e., incremental learning) quite critical. In this letter, we develop a novel technique for HTM (incremental) supervised learning based on gradient descent error minimization. We prove that error backpropagation can be naturally and elegantly implemented through native HTM message passing based on belief propagation. Our experimental results demonstrate that a two-stage training approach composed of unsupervised pretraining and supervised refinement is very effective (both accurate and efficient). This is in line with recent findings on other deep architectures.","keywords_author":null,"keywords_other":["NETWORKS","SLOW FEATURE ANALYSIS","LAPLACIAN EIGENMAPS","RECOGNITION"],"max_cite":2.0,"pub_year":2014.0,"sources":"['ieee', 'wos']","rawkeys":["slow feature analysis","networks","recognition","laplacian eigenmaps"],"tags":["slow feature analysis","networks","recognition","laplacian eigenmaps"]},{"p_id":12551,"title":"String representations and distances in deep Convolutional Neural Networks for image classification","abstract":"Recent advances in image classification mostly rely on the use of powerful local features combined with an adapted image representation. Although Convolutional Neural Network (CNN) features learned from ImageNet. were shown to be generic and very efficient, they still lack of flexibility to take into account variations in the spatial layout of visual elements. In this paper, we investigate the use of structural representations on top of pretrained CNN features to improve image classification. Images are represented as strings of CNN features. Similarities between such representations are computed using two new edit distance variants adapted to the image classification domain. Our algorithms have been implemented and tested on several challenging datasets, 15Scenes, Caltech101, Pascal VOC 2007 and MIT indoor. The results show that our idea of using structural string representations and distances clearly improves the classification performance over standard approaches based on CNN and SVM with linear kernel, as well as other recognized methods of the literature. (C) 2016 Elsevier Ltd. All rights reserved.","keywords_author":["Convolutional Neural Network","Edit distance","Image classification","String representation","Convolutional Neural Network","String representation","Edit distance","Image classification"],"keywords_other":["Spatial layout","Classification performance","Structural representation","EDIT DISTANCE","Edit distance","Image representations","String representation","RECOGNITION","Convolutional neural network","Visual elements"],"max_cite":22.0,"pub_year":2016.0,"sources":"['scp', 'wos']","rawkeys":["edit distance","spatial layout","recognition","classification performance","structural representation","string representation","convolutional neural network","image classification","visual elements","image representations"],"tags":["edit distance","spatial layout","recognition","classification performance","structural representation","string representation","convolutional neural network","image classification","visual elements","image representation"]},{"p_id":28937,"title":"Model generalization and its implications on intrusion detection","abstract":"To make up for the incompleteness of the known behaviors of a computing resource, model generalization is utilized to infer more behaviors in the behavior model besides the known behaviors. In principle, model generalization can improve the detection rate but may also degrade the detection performance. Therefore, the relation between model generalization and detection performance is critical for intrusion detection. However, most of past research only evaluates the overall efficiency of an intrusion detection technique via detection rate and false alarm\/positive rate, rather than the usefulness of model generalization for intrusion detection. In this paper, we try to do such evaluation, and then to find the implications of model generalization on intrusion detection. Within our proposed methodology, model generalization can be achieved in three levels. In this paper, we evaluate the first level model generalization. The experimental results show that the first level model generalization is useful mostly to enhance the detection performance of intrusion detection. However, its implications for intrusion detection are different with respect to different detection techniques. Our studies show that in general, though it is useful to generalize the normal behavior model so that more normal behaviors can be identified as such, the same is not advisable for the intrusive behavior model. Therefore, the intrusion signatures should be built compactly without first level generalization. \u00a9 Springer-Verlag Berlin Heidelberg 2005.","keywords_author":["Generalization","Intrusion","Intrusion Detection","Machine Learning","Security","Security Infrastructure"],"keywords_other":["Intrusion detection","Generalization","Intrusion","Security infrastructure"],"max_cite":5.0,"pub_year":2005.0,"sources":"['scp']","rawkeys":["intrusion detection","intrusion","machine learning","security","generalization","security infrastructure"],"tags":["recognition","intrusion","intrusion detection systems","machine learning","security","security infrastructure"]},{"p_id":12553,"title":"An optimized convolutional neural network with bottleneck and spatial pyramid pooling layers for classification of foods","abstract":"Keeping record of daily meal intake is an effective solution for tackling with obesity and overweight. This can be done by developing apps on smartphones that are able to automatically recommend a short list of most probable foods by analyzing the photo taken from food. Then, the user chooses the correct answer from the short list. Hence, the automatic food recognition system must be able to recommend an accurate list. In other words, it is not essential for these apps to have a very high top-1 accuracy. Considering that the app will show the list of 5 most probable foods, the food recognition system must have a high top-5 accuracy. A food recognition system is usually developed by adapting knowledge of state-of-the-art networks such as GoogleNet and ResNet to the domain of food. However, these networks have high number of parameters. In this paper, we propose a 23-layer architecture which has 99.14% and 96.63% fewer parameter compared with ResNet and GoogleNet. Our experiment on Food101 and UECFood-256 datasets shows that although our network reduces the number of parameters dramatically, it produces more accurate results than GoogleNet and its accuracy is comparable with ResNet. (c) 2017 Elsevier B.V. All rights reserved.","keywords_author":["Convolutional neural networks","Deep learning","Food classification","Neural network visualization","Spatial pyramid pooling","Food classification","Convolutional neural networks","Neural network visualization","Deep learning","Spatial pyramid pooling"],"keywords_other":["65D17","Spatial pyramids","41A05","65D05","RECOGNITION","Convolutional neural network","Network visualization","41A10"],"max_cite":1.0,"pub_year":2018.0,"sources":"['scp', 'wos']","rawkeys":["network visualization","41a10","recognition","convolutional neural networks","deep learning","41a05","65d05","65d17","convolutional neural network","spatial pyramid pooling","food classification","spatial pyramids","neural network visualization"],"tags":["network visualization","41a10","recognition","machine learning","41a05","65d05","65d17","convolutional neural network","spatial pyramid pooling","food classification","spatial pyramids","neural network visualization"]},{"p_id":12555,"title":"Variety Identification of Single Rice Seed Using Hyperspectral Imaging Combined with Convolutional Neural Network","abstract":"The feasibility of using hyperspectral imaging with convolutional neural network (CNN) to identify rice seed varieties was studied. Hyperspectral images of 4 rice seed varieties at two different spectral ranges (380-1030 nm and 874-1734 nm) were acquired. The spectral data at the ranges of 441-948 nm (Spectral range 1) and 975-1646 nm (Spectral range 2) were extracted. K nearest neighbors (KNN), support vector machine (SVM) and CNN models were built using different number of training samples (100, 200, 300, 400, 500, 600, 700, 800, 900, 1000, 1500, 2000, 2500 and 3000). KNN, SVM and CNN models in the Spectral range 2 performed slightly better than those in the Spectral range 1. The model performances improved with the increase in the number of training samples. The improvements were not significant when the number of training samples was large. CNN model performed better than the corresponding KNN and SVM models in most cases, which indicated the effectiveness of using CNN to analyze spectral data. The results of this study showed that CNN could be adopted in spectral data analysis with promising results. More varieties of rice need to be studied in future research to extend the use of CNNs in spectral data analysis.","keywords_author":["hyperspectral imaging","variety identification","rice seed","convolutional neural network"],"keywords_other":["DISCRIMINATION","CLASSIFICATION","MAIZE SEEDS","NEAR-INFRARED SPECTROSCOPY","RECOGNITION","MODELS","VARIABLE SELECTION"],"max_cite":2.0,"pub_year":2018.0,"sources":"['wos']","rawkeys":["variable selection","maize seeds","near-infrared spectroscopy","recognition","variety identification","hyperspectral imaging","classification","convolutional neural network","models","discrimination","rice seed"],"tags":["variable selection","maize seeds","near-infrared spectroscopy","model","recognition","variety identification","hyperspectral imaging","classification","convolutional neural network","discrimination","rice seed"]},{"p_id":12556,"title":"DeepSF: deep convolutional neural network for mapping protein sequences to folds","abstract":"Motivation: Protein fold recognition is an important problem in structural bioinformatics. Almost all traditional fold recognition methods use sequence (homology) comparison to indirectly predict the fold of a target protein based on the fold of a template protein with known structure, which cannot explain the relationship between sequence and fold. Only a few methods had been developed to classify protein sequences into a small number of folds due to methodological limitations, which are not generally useful in practice.","keywords_author":null,"keywords_other":["TARGET CLASSIFICATION","SECONDARY STRUCTURE","FSSP","CATH","SIMILARITY","DATABASE","RECOGNITION","SCOP","HOMOLOGY DETECTION","STRUCTURE PREDICTION"],"max_cite":0.0,"pub_year":2017.0,"sources":"['ieee', 'wos']","rawkeys":["structure prediction","recognition","homology detection","scop","similarity","database","cath","target classification","secondary structure","fssp"],"tags":["structure prediction","recognition","databases","homology detection","scop","similarity","cath","target classification","secondary structure","fssp"]},{"p_id":12562,"title":"Injurious or Noninjurious Defect Identification From MFL Images in Pipeline Inspection Using Convolutional Neural Network","abstract":"This paper proposes an injurious or noninjurious defect identification method from magnetic flux leakage (MFL) images based on convolutional neural network. Different from previous approaches, this method is fed by the MFL images instead of the features of the MFL measurements, and thus it can skip the procedure of feature extraction. Moreover, for convenience, a normalization layer is added to the front of model. In the convolution layers, the rectified linear units are employed as the activation functions to shorten the training period and improve the performance. In addition, two local response normalization layers are also embedded into the proposed structure. We demonstrate the performance of the proposed model using real MFL data collected from experimental pipelines. Benefited from the special structure of the proposed model, this method is robust for shift, scale, and distortion variances of input MFL images. We also present a comparative result of the proposed model and other methods. The results prove that the proposed method can achieve higher accuracy than the traditional approaches.","keywords_author":["Convolutional neural network (CNN)","defect identification","injurious or noninjurious","magnetic flux leakage (MFL)","nondestructive testing"],"keywords_other":["NONDESTRUCTIVE EVALUATION","RECOGNITION","INVERSION","SIGNALS"],"max_cite":2.0,"pub_year":2017.0,"sources":"['ieee', 'wos']","rawkeys":["signals","magnetic flux leakage (mfl)","recognition","nondestructive testing","injurious or noninjurious","defect identification","convolutional neural network (cnn)","inversion","nondestructive evaluation"],"tags":["signals","recognition","injurious or noninjurious","non destructive evaluation","defect identification","magnetic flux leakage","convolutional neural network","inversion","non destructive testing"]},{"p_id":12563,"title":"A Convolutional Neural Network Approach for Assisting Avalanche Search and Rescue Operations with UAV Imagery","abstract":"Following an avalanche, one of the factors that affect victims' chance of survival is the speed with which they are located and dug out. Rescue teams use techniques like trained rescue dogs and electronic transceivers to locate victims. However, the resources and time required to deploy rescue teams are major bottlenecks that decrease a victim's chance of survival. Advances in the field of Unmanned Aerial Vehicles (UAVs) have enabled the use of flying robots equipped with sensors like optical cameras to assess the damage caused by natural or manmade disasters and locate victims in the debris. In this paper, we propose assisting avalanche search and rescue (SAR) operations with UAVs fitted with vision cameras. The sequence of images of the avalanche debris captured by the UAV is processed with a pre-trained Convolutional Neural Network (CNN) to extract discriminative features. A trained linear Support Vector Machine (SVM) is integrated at the top of the CNN to detect objects of interest. Moreover, we introduce a pre-processing method to increase the detection rate and a post-processing method based on a Hidden Markov Model to improve the prediction performance of the classifier. Experimental results conducted on two different datasets at different levels of resolution show that the detection performance increases with an increase in resolution, while the computation time increases. Additionally, they also suggest that a significant decrease in processing time can be achieved thanks to the pre-processing step.","keywords_author":["avalanche","convolutional neural network (CNN)","deep learning","hidden Markov model (HMM)","object detection","search and rescue operation","support vector machine (SVM)","unmanned aerial vehicle (UAV)"],"keywords_other":["RECOGNITION","GRADIENTS","CLASSIFICATION","REGRESSION"],"max_cite":8.0,"pub_year":2017.0,"sources":"['wos']","rawkeys":["recognition","deep learning","avalanche","convolutional neural network (cnn)","support vector machine (svm)","search and rescue operation","gradients","hidden markov model (hmm)","classification","object detection","regression","unmanned aerial vehicle (uav)"],"tags":["unmanned aerial vehicle","hidden markov models","recognition","machine learning","avalanche","gradient","classification","convolutional neural network","object detection","regression","search and rescue operations"]},{"p_id":110871,"title":"L-Tree: A Local-Area-Learning-Based Tree Induction Algorithm for Image Classification","abstract":"The decision tree is one of the most effective tools for deriving meaningful outcomes from image data acquired from the visual sensors. Owing to its reliability, superior generalization abilities, and easy implementation, the tree model has been widely used in various applications. However, in image classification problems, conventional tree methods use only a few sparse attributes as the splitting criterion. Consequently, they suffer from several drawbacks in terms of performance and environmental sensitivity. To overcome these limitations, this paper introduces a new tree induction algorithm that classifies images on the basis of local area learning. To train our predictive model, we extract a random local area within the image and use it as a feature for classification. In addition, the self-organizing map, which is a clustering technique, is used for node learning. We also adopt a random sampled optimization technique to search for the optimal node. Finally, each trained node stores the weights that represent the training data and class probabilities. Thus, a recursively trained tree classifies the data hierarchically based on the local similarity at each node. The proposed tree is a type of predictive model that offers benefits in terms of image's semantic energy conservation compared with conventional tree methods. Consequently, it exhibits improved performance under various conditions, such as noise and illumination changes. Moreover, the proposed algorithm can improve the generalization ability owing to its randomness. In addition, it can be easily applied to ensemble techniques. To evaluate the performance of the proposed algorithm, we perform quantitative and qualitative comparisons with various tree-based methods using four image datasets. The results show that our algorithm not only involves a lower classification error than the conventional methods but also exhibits stable performance even under unfavorable conditions such as noise and illumination changes.","keywords_author":["decision tree","ensemble tree","image classification","self-organizing map"],"keywords_other":["RANDOM FOREST","RECOGNITION","SELF-ORGANIZING MAP"],"max_cite":0.0,"pub_year":2018.0,"sources":"['wos']","rawkeys":["recognition","ensemble tree","self-organizing map","decision tree","image classification","random forest"],"tags":["recognition","ensemble trees","random forests","self-organizing map","image classification","decision trees"]},{"p_id":4381,"title":"Regional parallel structure based CNN for thermal infrared face identification","abstract":"The convolutional neural network, based on multi-scale features, is introduced to thermal infrared face identification in this paper. A novel CNN structure is proposed based on characteristics of thermal infrared faces. To enhance and extract inconspicuous thermal infrared facial features for identification, convoluted edges are taken as the initial features. A regional parallel structured CNN algorithm (RPS net) is proposed to obtain multi-scale features based on edge information. Extensive experiments are conducted and analyzed, including statistical test with various classifiers, feature vector property, accuracies of each class and robustness against various noise. The experimental result indicates that RPS net overtakes algorithms based on traditional features (HoG, Fisherface and LBP) and some CNN algorithms (Alex net, VGG net, DeepID net and TFR net), with high quality features. Therefore, RPS net is effective and robust for thermal infrared face identification.","keywords_author":["Thermal infrared face identification","convolutional neural network","regional parallel structure"],"keywords_other":["CLASSIFICATION","NEURAL-NETWORKS","RECOGNITION","ALGORITHMS","EIGENFACE"],"max_cite":0.0,"pub_year":2018.0,"sources":"['wos', 'ieee']","rawkeys":["neural-networks","eigenface","recognition","regional parallel structure","classification","convolutional neural network","algorithms","thermal infrared face identification"],"tags":["recognition","eigenfaces","neural networks","regional parallel structure","classification","convolutional neural network","algorithms","thermal infrared face identification"]},{"p_id":4383,"title":"Training Deep Convolutional Neural Networks with Resistive Cross-Point Devices","abstract":"In a previous work we have detailed the requirements for obtaining maximal deep learning performance benefit by implementing fully connected deep neural networks (DNN) in the form of arrays of resistive devices. Here we extend the concept of Resistive Processing Unit (RPU) devices to convolutional neural networks (CNNs). We show how to map the convolutional layers to fully connected RPU arrays such that the parallelism of the hardware can be fully utilized in all three cycles of the backpropagation algorithm. We find that the noise and bound limitations imposed by the analog nature of the computations performed on the arrays significantly affect the training accuracy of the CNNs. Noise and bound management techniques are presented that mitigate these problems without introducing any additional complexity in the analog circuits and that can be addressed by the digital circuits. In addition, we discuss digitally programmable update management and device variability reduction techniques that can be used selectively for some of the layers in a CNN. We show that a combination of all those techniques enables a successful application of the RPU concept for training CNNs. The techniques discussed here are more general and can be applied beyond CNN architectures and therefore enables applicability of the RPU approach to a large class of neural network architectures.","keywords_author":["convolutional neural networks (CNN)","resistive processing unit (RPU)","deep neural network","deep learning","resistive switching","resistive random access memory (RRAM)","resistive memories"],"keywords_other":["RECOGNITION","CHIP","ARRAY"],"max_cite":2.0,"pub_year":2017.0,"sources":"['ieee', 'wos']","rawkeys":["array","recognition","deep neural network","chip","deep learning","resistive switching","resistive memories","resistive processing unit (rpu)","resistive random access memory (rram)","convolutional neural networks (cnn)"],"tags":["arrays","recognition","resistive memory","chip","machine learning","convolutional neural network","resistive switching","resistive switching memory","resistive processing unit (rpu)"]},{"p_id":4384,"title":"Conversion of Continuous-Valued Deep Networks to Efficient Event-Driven Networks for Image Classification","abstract":"Spiking neural networks (SNNs) can potentially offer an efficient way of doing inference because the neurons in the networks are sparsely activated and computations are event-driven. Previous work showed that simple continuous-valued deep Convolutional Neural Networks (CNNs) can be converted into accurate spiking equivalents. These networks did not include certain common operations such as max-pooling, softmax, batch-normalization and Inception-modules. This paper presents spiking equivalents of these operations therefore allowing conversion of nearly arbitrary CNN architectures. We show conversion of popular CNN architectures, including VGG-16 and lnception-v3, into SNNs that produce the best results reported to date on MNIST, CIFAR-10 and the challenging ImageNet dataset. SNNs can trade off classification error rate against the number of available operations whereas deep continuous-valued neural networks require a fixed number of operations to achieve their classification error rate. From the examples of LeNet for MNIST and BinaryNet for CIFAR-10, we show that with an increase in error rate of a few percentage points, the SNNs can achieve more than 2x reductions in operations compared to the original CNNs. This highlights the potential of SNNs in particular when deployed on power-efficient neuromorphic spiking neuron chips, for use in embedded applications.","keywords_author":["artificial neural network","spiking neural network","deep learning","object classification","deep networks","spiking network conversion"],"keywords_other":["RECOGNITION","SPIKING"],"max_cite":2.0,"pub_year":2017.0,"sources":"['wos', 'ieee']","rawkeys":["spiking network conversion","recognition","deep learning","spiking","spiking neural network","object classification","deep networks","artificial neural network"],"tags":["spiking network conversion","recognition","spiking neural networks","neural networks","spiking","machine learning","object classification","deep networks"]},{"p_id":12575,"title":"Automated red blood cells extraction from holographic images using fully convolutional neural networks","abstract":"In this paper, we present two models for automatically extracting red blood cells (RBCs) from RBCs holographic images based on a deep learning fully convolutional neural network (FCN) algorithm. The first model, called FCN-1, only uses the FCN algorithm to carry out RBCs prediction, whereas the second model, called FCN-2, combines the FCN approach with the marker-controlled watershed transform segmentation scheme to achieve RBCs extraction. Both models achieve good segmentation accuracy. In addition, the second model has much better performance in terms of cell separation than traditional segmentation methods. In the proposed methods, the RBCs phase images are first numerically reconstructed from RBCs holograms recorded with off-axis digital holographic microscopy. Then, some RBCs phase images are manually segmented and used as training data to fine-tune the FCN. Finally, each pixel in new input RBCs phase images is predicted into either foreground or background using the trained FCN models. The RBCs prediction result from the first model is the final segmentation result, whereas the result from the second model is used as the internal markers of the marker-controlled transform algorithm for further segmentation. Experimental results show that the given schemes can automatically extract RBCs from RBCs phase images and much better RBCs separation results are obtained when the FCN technique is combined with the marker-controlled watershed segmentation algorithm. (C) 2017 Optical Society of America","keywords_author":null,"keywords_other":["DIGITAL HOLOGRAPHY","CONTRAST","VISUALIZATION","RECOGNITION","SEGMENTATION","PHASE MICROSCOPY","3-DIMENSIONAL IDENTIFICATION","TRACKING"],"max_cite":1.0,"pub_year":2017.0,"sources":"['wos']","rawkeys":["recognition","segmentation","digital holography","3-dimensional identification","tracking","phase microscopy","contrast","visualization"],"tags":["recognition","segmentation","digital holography","3-dimensional identification","tracking","phase microscopy","contrast","visualization"]},{"p_id":4385,"title":"A Configurable Event-Driven Convolutional Node with Rate Saturation Mechanism for Modular ConvNet Systems Implementation","abstract":"Convolutional Neural Networks (ConvNets) are a particular type of neural network often used for many applications like image recognition, video analysis or natural language processing. They are inspired by the human brain, following a specific organization of the connectivity pattern between layers of neurons known as receptive field. These networks have been traditionally implemented in software, but they are becoming more computationally expensive as they scale up, having limitations for real-time processing of high-speed stimuli. On the other hand, hardware implementations show difficulties to be used for different applications, due to their reduced flexibility. In this paper, we propose a fully configurable event-driven convolutional node with rate saturation mechanism that can be used to implement arbitrary ConvNets on FPGAs. This node includes a convolutional processing unit and a routing element which allows to build large 2D arrays where any multilayer structure can be implemented. The rate saturation mechanism emulates the refractory behavior in biological neurons, guaranteeing a minimum separation in time between consecutive events. A 4-layer ConvNet with 22 convolutional nodes trained for poker card symbol recognition has been implemented in a Spartan6 FPGA. This network has been tested with a stimulus where 40 poker cards were observed by a Dynamic Vision Sensor (DVS) in 1 s time. Different slow-down factors were applied to characterize the behavior of the system for high speed processing. For slow stimulus play-back, a 96% recognition rate is obtained with a power consumption of 0.85mW. At maximum play-back speed, a traffic control mechanism downsamples the input stimulus, obtaining a recognition rate above 63% when less than 20% of the input events are processed, demonstrating the robustness of the network.","keywords_author":["convolutional neural networks","neuromorphic vision","Address Event Representation (AER)","event-drivenprocessing","neural network hardware","Reconfigurable Networks"],"keywords_other":["RECOGNITION","NETWORK","SENSOR"],"max_cite":0.0,"pub_year":2018.0,"sources":"['wos', 'ieee']","rawkeys":["address event representation (aer)","neural network hardware","network","convolutional neural networks","recognition","sensor","neuromorphic vision","event-drivenprocessing","reconfigurable networks"],"tags":["neural network hardware","recognition","sensors","neuromorphic vision","event driven processing","networks","convolutional neural network","address-event representation (aer)","reconfigurable network"]},{"p_id":4387,"title":"X-FIDO: An Effective Application for Detecting Olive Quick Decline Syndrome with Deep Learning and Data Fusion","abstract":"We have developed a vision-based program to detect symptoms of Olive Quick Decline Syndrome (OQDS) on leaves of Olea europaea L. infected by Xylella fastidiosa, named X-FIDO (Xylella FastIdiosa Detector for O. europaea L.). Previous work predicted disease from leaf images with deep learning but required a vast amount of data which was obtained via crowd sourcing such as the PlantVillage project. This approach has limited applicability when samples need to be tested with traditional methods (i.e., PCR) to avoid incorrect training input or for quarantine pests which manipulation is restricted. In this paper, we demonstrate that transfer learning can be leveraged when it is not possible to collect thousands of new leaf images. Transfer learning is the re-application of an already trained deep learner to a new problem. We present a novel algorithm for fusing data at different levels of abstraction to improve performance of the system. The algorithm discovers low-level features from raw data to automatically detect veins and colors that lead to symptomatic leaves. The experiment included images of 100 healthy leaves, 99 X. fastidiosa-positive leaves and 100 X. fastidiosa-negative leaves with symptoms related to other stress factors (i.e., abiotic factors such as water stress or others diseases). The program detects OQDS with a true positive rate of 98.60 +\/- 1.47% in testing, showing great potential for image analysis for this disease. Results were obtained with a convolutional neural network trained with the stochastic gradient descent method, and ten trials with a 75\/25 split of training and testing data. This work shows potential for massive screening of plants with reduced diagnosis time and cost.","keywords_author":["convolutional neural networks","deep learning","machine vision","transfer learning","Olea europaea","Xylella fastidiosa"],"keywords_other":["REAL-TIME PCR","TISSUE","QUANTIFICATION","XYLELLA-FASTIDIOSA","INFORMATION","FEATURES","RAPID DETECTION","RECOGNITION","PLANT","TREES"],"max_cite":1.0,"pub_year":2017.0,"sources":"['ieee', 'wos']","rawkeys":["rapid detection","tissue","olea europaea","xylella-fastidiosa","plant","convolutional neural networks","recognition","features","deep learning","transfer learning","trees","quantification","machine vision","xylella fastidiosa","information","real-time pcr"],"tags":["rapid detection","tissue","olea europaea","recognition","features","transfer learning","machine learning","machine vision","quantification","xylella fastidiosa","information","convolutional neural network","real-time pcr","biological","mathematics"]},{"p_id":12582,"title":"Efficient training algorithms for a class of shunting inhibitory convolutional neural networks","abstract":"This article presents some efficient training algorithms, based on first-order, second-order, and conjugate gradient optimization methods, for a class of convolutional neural networks (CoNNs), known as shunting inhibitory convolution neural networks. Furthermore, a new hybrid method is proposed, which is derived from the principles of Quickprop, Rprop, SuperSAB, and least squares (LS). Experimental results show that the new hybrid method can perform as well as the Levenberg-Marquardt (LM) algorithm, but at a much lower computational cost and less memory storage. For comparison sake, the visual pattern recognition task of face\/nonface discrimination is chosen as a classification problem to evaluate the performance of the training algorithms. Sixteen training algorithms are implemented for the three different variants of the proposed CoNN architecture: binary-, Toeplitz- and fully connected architectures. All implemented algorithms can train the three network architectures successfully, but their convergence speed vary markedly. In particular, the combination of LS with the new hybrid method and LS with the LM method achieve the best convergence rates in terms of number of training epochs. In addition, the classification accuracies of all three architectures are assessed using ten-fold cross validation. The results show that the binary- and Toeplitz-connected architectures outperform slightly the fully connected architecture: the lowest error rates across all training algorithms are 1.95% for Toeplitz-connected, 2.10% for the binary-connected, and 2.20% for the fully connected network. In general, the modified Broyden-Fletcher-Goldfarb-Shanno (BFGS) methods, the three variants of LM algorithm, and the new hybrid\/LS method perform consistently well, achieving error rates of less than 3% averaged across all three architectures.","keywords_author":["convolutional neural network (CoNN)","first- and second-order training methods","shunting inhibitory neuron"],"keywords_other":["FEEDFORWARD NETWORKS","LATERAL INHIBITION","CLASSIFICATION","RECOGNITION","MINIMIZATION","MECHANISM","CONJUGATE-GRADIENT METHOD","ADAPTATION","MULTILAYER PERCEPTRONS"],"max_cite":31.0,"pub_year":2005.0,"sources":"['wos']","rawkeys":["multilayer perceptrons","conjugate-gradient method","recognition","minimization","convolutional neural network (conn)","mechanism","lateral inhibition","first- and second-order training methods","classification","feedforward networks","adaptation","shunting inhibitory neuron"],"tags":["recognition","minimization","mechanisms","feed-forward network","lateral inhibition","first- and second-order training methods","classification","conjugate gradient method","convolutional neural network","multi layer perceptron","adaptation","shunting inhibitory neuron"]},{"p_id":12586,"title":"Automatic ear detection and feature extraction using Geometric Morphometrics and convolutional neural networks","abstract":"Accurate gathering of phenotypic information is a key aspect in several subject matters, including biometrics, biomedical analysis, forensics, and many other. Automatic identification of anatomical structures of biometric interest, such as fingerprints, iris patterns, or facial traits, are extensively used in applications like access control and anthropological research, all having in common the drawback of requiring intrusive means for acquiring the required information. In this regard, the ear structure has multiple advantages. Not only the ear's biometric markers can be easily captured from the distance with non intrusive methods, but also they experiment almost no changes over time, and are not influenced by facial expressions. Here we present a new method based on Geometric Morphometrics and Deep Learning for automatic ear detection and feature extraction in the form of landmarks. A convolutional neural network was trained with a set of manually landmarked examples. The network is able to provide morphometric landmarks on ears' images automatically, with a performance that matches human landmarking. The feasibility of using ear landmarks as feature vectors opens a novel spectrum of biometrics applications.","keywords_author":["learning (artificial intelligence)","neural nets","feature extraction","computational geometry","image matching","biometrics (access control)","feature extraction","geometric morphometrics","phenotypic information","anatomical structure identification","fingerprints","iris patterns","facial traits","ear structure","ear biometric markers","nonintrusive method","facial expressions","phenotypic attributes","deep-learning algorithms","automatic ear detection","2D landmarks","convolutional neural network training","morphometric landmarks","human-assisted landmark matching","feature vectors","people identification"],"keywords_other":["Biomedical analysis","Automatic identification","Subject matters","SHAPE","POSITION","IDENTIFICATION","Anatomical structures","MODEL","RECOGNITION","Geometric morphometrics","Convolutional neural network","PATTERN","Non-intrusive method","Facial Expressions"],"max_cite":3.0,"pub_year":2017.0,"sources":"['ieee', 'scp', 'wos']","rawkeys":["identification","biomedical analysis","facial traits","human-assisted landmark matching","convolutional neural network","iris patterns","ear biometric markers","pattern","nonintrusive method","automatic ear detection","people identification","position","feature vectors","image matching","convolutional neural network training","deep-learning algorithms","morphometric landmarks","automatic identification","biometrics (access control)","non-intrusive method","geometric morphometrics","learning (artificial intelligence)","phenotypic attributes","computational geometry","anatomical structure identification","recognition","facial expressions","neural nets","ear structure","subject matters","2d landmarks","model","fingerprints","phenotypic information","shape","anatomical structures","feature extraction"],"tags":["identification","biomedical analysis","facial traits","human-assisted landmark matching","patterns","convolutional neural network","iris patterns","ear biometric markers","automatic ear detection","people identification","machine learning","position","feature vectors","image matching","convolutional neural network training","morphometric landmarks","automatic identification","non-intrusive method","geometric morphometrics","deep learning algorithm","phenotypic attributes","recognition","computational geometry","anatomical structure identification","neural networks","facial expressions","biometrics","fingerprint","ear structure","2d landmarks","model","phenotypic information","subject matter","shape","anatomical structures","feature extraction"]},{"p_id":12590,"title":"Spatial-temporal convolutional neural networks for anomaly detection and localization in crowded scenes","abstract":"Abnormal behavior detection in crowded scenes is extremely challenging in the field of computer vision due to severe inter-object occlusions, varying crowd densities and the complex mechanics of a human crowd. We propose a method for detecting and locating anomalous activities in video sequences of crowded scenes. The key novelty of our method is the coupling of anomaly detection with a spatial temporal Convolutional Neural Networks (CNN), which to the best of our knowledge has not been previously done. This architecture allows us to capture features from both spatial and temporal dimensions by performing spatial-temporal convolutions, thereby, both the appearance and motion information encoded in continuous frames are extracted. The spatial-temporal convolutions are only performed within spatial-temporal volumes of moving pixels to ensure robustness to local noise, and increase detection accuracy. We experimentally evaluate our model on benchmark datasets containing various situations with human crowds, and the results demonstrate that the proposed approach surpass state-of-the-art methods. (C) 2016 Elsevier B.V. All rights reserved.","keywords_author":["Anomaly detection","Crowded scene","Spatial-temporal CNN","Surveillance","Spatial-temporal CNN","Anomaly detection","Crowded scene","Surveillance"],"keywords_other":["State-of-the-art methods","Temporal dimensions","Spatial temporals","Anomaly detection","SOCIAL FORCE MODEL","RECOGNITION","Convolutional neural network","MOTION","EVENT DETECTION","Abnormal behavior detections","Crowded scene","Anomalous activity"],"max_cite":19.0,"pub_year":2016.0,"sources":"['ieee', 'scp', 'wos']","rawkeys":["spatial temporals","state-of-the-art methods","temporal dimensions","crowded scene","anomalous activity","recognition","social force model","anomaly detection","motion","convolutional neural network","spatial-temporal cnn","surveillance","event detection","abnormal behavior detections"],"tags":["spatial temporals","state-of-the-art methods","temporal dimensions","crowded scenes","anomalous activity","recognition","anomaly detection","motion","social force models","convolutional neural network","spatial-temporal cnn","surveillance","event detection","abnormal behavior detections"]},{"p_id":78128,"title":"Scene classification based on single-layer SAE and SVM","abstract":"Scene classification aims to group images into semantic categories. It is a challenging problem in computer vision due to the difficulties of intra-class variability and inter-class similarity. In this paper, a scene classification approach based on single-layer sparse autoencoder (SAE) and support vector machine (SVM) is proposed. This approach consists of two steps: SAE-based feature learning step and SVM-based classification step. In the first step, a single-layer SAE network is constructed and trained by the patches which are sampled randomly from the source images. The feature representation of images is learned by the trained single-layer SAE network. Meanwhile, a pooling operation is used to reduce the dimension of the learned feature vectors. In the second step, in order to improve the classification accuracy, the parameters of SVM are optimized by a particle swarm optimization (PSO) based algorithm. The one-versus-one strategy is employed for the multiple scene classification problem. To show the efficiency of the proposed approach, several public data sets are employed. The results reveal that the proposed approach achieves better classification accuracy than the existing state-of-the-art methods. (C) 2014 Elsevier Ltd. All rights reserved.","keywords_author":["Scene classification","Feature learning","Single-layer SAE","SVM","PSO"],"keywords_other":["FEATURES","REPRESENTATION","SUPPORT VECTOR MACHINES","MODEL","IMAGE CLASSIFICATION","RECOGNITION","OPTIMIZATION"],"max_cite":19.0,"pub_year":2015.0,"sources":"['ieee', 'wos']","rawkeys":["pso","recognition","model","features","representation","scene classification","svm","support vector machines","optimization","single-layer sae","feature learning","image classification"],"tags":["recognition","model","features","representation","scene classification","machine learning","optimization","single-layer sae","feature learning","image classification","particle swarm optimization"]},{"p_id":94517,"title":"An Improved Bacterial-Foraging Optimization-Based Machine Learning Framework for Predicting the Severity of Somatization Disorder","abstract":"It is of great clinical significance to establish an accurate intelligent model to diagnose the somatization disorder of community correctional personnel. In this study, a novel machine learning framework is proposed to predict the severity of somatization disorder in community correction personnel. The core of this framework is to adopt the improved bacterial foraging optimization (IBFO) to optimize two key parameters (penalty coefficient and the kernel width) of a kernel extreme learning machine (KELM) and build an IBFO-based KELM (IBFO-KELM) for the diagnosis of somatization disorder patients. The main innovation point of the IBFO-KELM model is the introduction of opposition-based learning strategies in traditional bacteria foraging optimization, which increases the diversity of bacterial species, keeps a uniform distribution of individuals of initial population, and improves the convergence rate of the BFO optimization process as well as the probability of escaping from the local optimal solution. In order to verify the effectiveness of the method proposed in this study, a 10-fold cross-validation method based on data from a symptom self-assessment scale (SCL-90) is used to make comparison among IBFO-KELM, BFO-KELM (model based on the original bacterial foraging optimization model), GA-KELM (model based on genetic algorithm), PSO-KELM (model based on particle swarm optimization algorithm) and Grid-KELM (model based on grid search method). The experimental results show that the proposed IBFO-KELM prediction model has better performance than other methods in terms of classification accuracy, Matthews correlation coefficient (MCC), sensitivity and specificity. It can distinguish very well between severe somatization disorder and mild somatization and assist the psychological doctor with clinical diagnosis.","keywords_author":["kernel extreme learning machine","somatization disorder","parameter optimization","bacterial foraging optimization","opposition-based learning"],"keywords_other":["DIAGNOSIS","NETWORKS","ALGORITHM","CLASSIFICATION","RECOGNITION","CANCER","ELM"],"max_cite":1.0,"pub_year":2018.0,"sources":"['wos']","rawkeys":["diagnosis","algorithm","recognition","opposition-based learning","cancer","somatization disorder","bacterial foraging optimization","networks","classification","kernel extreme learning machine","parameter optimization","elm"],"tags":["diagnosis","recognition","opposition-based learning","cancer","somatization disorder","bacterial foraging optimization","networks","classification","kernel extreme learning machine","parameter optimization","algorithms","extreme learning machine"]},{"p_id":110904,"title":"Decomposition approach to the stability of recurrent neural networks with asynchronous time delays in quaternion field","abstract":"In this paper, the global exponential stability for recurrent neural networks (QVNNs) with asynchronous time delays is investigated in quaternion field. Due to the non-commutativity of quaternion multiplication resulting from Hamilton rules: ij = -ji = k, jk = -kj = i, ki = -ik = j, ijk = i(2) = j(2) = k(2) = -1, the QVNN is decomposed into four real-valued systems, which are studied separately. The exponential convergence is proved directly accompanied with the existence and uniqueness of the equilibrium point to the consider systems. Combining with the generalized infinity-norm and Cauchy convergence property in the quaternion field, some sufficient conditions to guarantee the stability are established without using any Lyapunov-Krasovskii functional and linear matrix inequality. Finally, a numerical example is given to demonstrate the effectiveness of the results. (C) 2017 Elsevier Ltd. All rights reserved.","keywords_author":["Global exponential stability","Quaternion-valued neural network","Asynchronous time delay","Linear matrix inequality"],"keywords_other":["VARYING DELAYS","SYSTEMS","COMPLEX","GLOBAL EXPONENTIAL STABILITY","RECOGNITION","INEQUALITY"],"max_cite":6.0,"pub_year":2017.0,"sources":"['wos']","rawkeys":["inequality","recognition","varying delays","asynchronous time delay","global exponential stability","linear matrix inequality","systems","complex","quaternion-valued neural network"],"tags":["inequality","quaternion-valued neural networks","recognition","varying delays","asynchronous time delay","global exponential stability","complexity","system","linear matrix inequality"]},{"p_id":12600,"title":"CNNdel: Calling Structural Variations on Low Coverage Data Based on Convolutional Neural Networks","abstract":"Many structural variations (SVs) detection methods have been proposed due to the popularization of next-generation sequencing (NGS). These SV calling methods use different SV-property-dependent features; however, they all suffer from poor accuracy when running on low coverage sequences. The union of results from these tools achieves fairly high sensitivity but still produces low accuracy on low coverage sequence data. That is, these methods contain many false positives. In this paper, we present CNNdel, an approach for calling deletions from paired-end reads. CNNdel gathers SV candidates reported by multiple tools and then extracts features from aligned BAM files at the positions of candidates. With labeled feature-expressed candidates as a training set, CNNdel trains convolutional neural networks (CNNs) to distinguish true unlabeled candidates from false ones. Results show that CNNdel works well with NGS reads from 26 low coverage genomes of the 1000 Genomes Project. The paper demonstrates that convolutional neural networks can automatically assign the priority of SV features and reduce the false positives efficaciously.","keywords_author":null,"keywords_other":["DISEASE","RECOGNITION","ACCURATE","PAIRED-END","HUMAN GENOME"],"max_cite":0.0,"pub_year":2017.0,"sources":"['wos']","rawkeys":["recognition","disease","paired-end","human genome","accurate"],"tags":["recognition","disease","paired-end","human genome","accurate"]},{"p_id":78138,"title":"Correcting Instrumental Variation and Time-Varying Drift: A Transfer Learning Approach With Autoencoders","abstract":"Electronic noses (e-noses) are instruments that can be used to measure gas samples conveniently. Based on the measured signal, the type and concentration of the gas can be predicted by pattern recognition algorithms. However, e-noses are often affected by influential factors, such as instrumental variation and time-varying drift. From the viewpoint of pattern recognition, the factors make the posterior distribution of the test data drift from that of the training data, thus will degrade the accuracy of the prediction models. In this paper, we propose drift correction autoencoder (DCAE) to address this problem. DCAE learns to model and correct the influential factors explicitly with the help of transfer samples. It generates drift-corrected and discriminative representation of the original data, which can then be applied to various prediction algorithms. We evaluate DCAE on data sets with instrumental variation and complex time-varying drift. Prediction models are trained on samples collected with one device or in the initial time period, then tested on other devices or time periods. Experimental results show that the DCAE outperforms typical drift correction algorithms and autoencoder-based transfer learning methods. It can improve the robustness of e-nose systems and greatly enhance their performance in real-world applications.","keywords_author":["Autoencoder","calibration transfer","drift correction","electronic nose (e-nose)","spectroscopy","transfer learning"],"keywords_other":["PREDICTION","CALIBRATION TRANSFER","ELECTRONIC NOSE","RECOGNITION","COMPENSATION"],"max_cite":6.0,"pub_year":2016.0,"sources":"['ieee', 'wos']","rawkeys":["recognition","transfer learning","prediction","calibration transfer","autoencoder","electronic nose (e-nose)","spectroscopy","drift correction","compensation","electronic nose"],"tags":["recognition","transfer learning","prediction","calibration transfer","auto encoders","spectroscopy","drift correction","compensation","electronic nose"]},{"p_id":12603,"title":"Text\/non-text image classification in the wild with convolutional neural networks","abstract":"Text in natural images is an important source of information, which can be utilized for many real-world applications. This work focuses on a new problem: distinguishing images that contain text from a large volume of natural images. To address this problem, we propose a novel convolutional neural network variant, called multi-scale spatial partition network (MSP-Net). The network classifies images that contain text or not, by predicting text existence in all image blocks, which are spatial partitions at multiple scales on an input image. The whole image is classified as a text image (an image containing text) as long as one of the blocks is predicted to contain text. The network classifies images very efficiently by predicting all blocks simultaneously in a single forward propagation. Through experimental evaluations and comparisons on public datasets, we demonstrate the effectiveness and robustness of the proposed method.","keywords_author":["Natural images","Text\/non-text image classification","Convolutional neural network","Multi-scale spatial partition"],"keywords_other":["VIDEO","RECOGNITION","ALGORITHMS","HANDWRITTEN DOCUMENTS"],"max_cite":7.0,"pub_year":2016.0,"sources":"['ieee', 'wos']","rawkeys":["recognition","multi-scale spatial partition","handwritten documents","video","convolutional neural network","natural images","algorithms","text\/non-text image classification"],"tags":["recognition","multi-scale spatial partition","handwritten document","video","convolutional neural network","natural images","algorithms","text\/non-text image classification"]},{"p_id":12606,"title":"Monitoring tool usage in surgery videos using boosted convolutional and recurrent neural networks","abstract":"This paper investigates the automatic monitoring of tool usage during a surgery, with potential applications in report generation, surgical training and real-time decision support. Two surgeries are considered: cataract surgery, the most common surgical procedure, and cholecystectomy, one of the most common digestive surgeries. Tool usage is monitored in videos recorded either through a microscope (cataract surgery) or an endoscope (cholecystectomy). Following state-of-the-art video analysis solutions, each frame of the video is analyzed by convolutional neural networks (CNNs) whose outputs are fed to recurrent neural networks (RNNs) in order to take temporal relationships between events into account. Novelty lies in the way those CNNs and RNNs are trained. Computational complexity prevents the endto-end training of \"CNN+RNN\" systems. Therefore, CNNs are usually trained first, independently from the RNNs. This approach is clearly suboptimal for surgical tool analysis: many tools are very similar to one another, but they can generally be differentiated based on past events. CNNs should be trained to extract the most useful visual features in combination with the temporal context. A novel boosting strategy is proposed to achieve this goal: the CNN and RNN parts of the system are simultaneously enriched by progressively adding weak classifiers (either CNNs or RNNs) trained to improve the overall classification accuracy. Experiments were performed in a dataset of 50 cataract surgery videos, where the usage of 21 surgical tools was manually annotated, and a dataset of 80 cholecystectomy videos, where the usage of 7 tools was manually annotated. Very good classification performance are achieved in both datasets: tool usage could be labeled with an average area under the ROC curve of A(z) = 0.9961 and A(z) = 0.9939, respectively, in offline mode (using past, present and future information), and A(z) = 0.9957 and A(z) = 0.9936, respectively, in online mode (using past and present information only). (C) 2018 Elsevier B.V. All rights reserved.","keywords_author":["Cataract and cholecystectomy surgeries","Tool usage monitoring","Video analysis","Convolutional and recurrent neural networks","Boosting"],"keywords_other":["RECOGNITION","TASKS","CATARACT-SURGERY","REAL-TIME SEGMENTATION"],"max_cite":0.0,"pub_year":2018.0,"sources":"['wos']","rawkeys":["recognition","tool usage monitoring","cataract and cholecystectomy surgeries","video analysis","convolutional and recurrent neural networks","tasks","real-time segmentation","cataract-surgery","boosting"],"tags":["recognition","tool usage monitoring","cataract and cholecystectomy surgeries","video analysis","convolutional and recurrent neural networks","real-time segmentation","task","cataract-surgery","boosting"]},{"p_id":12611,"title":"Double-stream Convolutional Neural Networks for Machine Vision Inspection of Natural Products","abstract":"There are known applications of convolutional neural networks to vision inspection of natural products. For many products it is sufficient to acquire and process a single image, but some might require imaging from two sides. Human experts performing quality inspection of malting barley typically only observe one side of each grain, but in doubtful cases look at both sides, intrinsically combining the information. In this paper, we make two contributions. We present a method for determining whether imaging objects from two sides yields performance benefits over single-sided imaging. Then we introduce a double-stream convolutional network for reasoning from two images simultaneously and analyze several methods of combining information from two streams. We find that when orientation of the object is unpredictable and the streams are not specialized to process a particular view, a fully shared architecture combining information on the prediction level yields best performance (98.7% accuracy on our dataset).","keywords_author":["Convolutional neural networks","machine vision","natural products","information fusion"],"keywords_other":["COMPUTER VISION","CLASSIFICATION","IDENTIFICATION","BARLEY","RECOGNITION","FUSION"],"max_cite":0.0,"pub_year":2017.0,"sources":"['ieee', 'wos']","rawkeys":["identification","convolutional neural networks","recognition","machine vision","classification","natural products","barley","computer vision","fusion","information fusion"],"tags":["natural-products","identification","recognition","machine vision","classification","convolutional neural network","barley","computer vision","fusion","information fusion"]},{"p_id":12612,"title":"A Mathematical Theory of Deep Convolutional Neural Networks for Feature Extraction","abstract":"Deep convolutional neural networks (DCNNs) have led to breakthrough results in numerous practical machine learning tasks, such as classification of images in the ImageNet data set, control-policy-learning to play Atari games or the board game Go, and image captioning. Many of these applications first perform feature extraction and then feed the results thereof into a classifier. The mathematical analysis of DCNNs for feature extraction was initiated by Mallat, 2012. Specifically, Mallat considered so-called scattering networks based on a wavelet transform followed by the modulus non-linearity in each network layer, and proved translation invariance (asymptotically in the wavelet scale parameter) and deformation stability of the corresponding feature extractor. This paper complements Mallat's results by developing a theory that encompasses general convolutional transforms, or in more technical parlance, general semi-discrete frames (including Weyl-Heisenberg filters, curvelets, shearlets, ridgelets, wavelets, and learned filters), general Lipschitz-continuous non-linearities (e.g., rectified linear units, shifted logistic sigmoids, hyperbolic tangents, and modulus functions), and general Lipschitz-continuous pooling operators emulating, e.g., sub-sampling and averaging. In addition, all of these elements can be different in different network layers. For the resulting feature extractor, we prove a translation invariance result of vertical nature in the sense of the features becoming progressively more translation-invariant with increasing network depth, and we establish deformation sensitivity bounds that apply to signal classes such as, e.g., band-limited functions, cartoon functions, and Lipschitz functions.","keywords_author":["deep convolutional neural networks","feature extraction","frame theory","Machine learning","scattering networks","Machine learning","deep convolutional neural networks","scattering networks","feature extraction","frame theory"],"keywords_other":["RIDGELET","INVARIANT SCATTERING","Translation invariants","Translation invariance","Band-limited functions","REPRESENTATIONS","Deformation sensitivities","TEXTURE CLASSIFICATION","RECOGNITION","Convolutional neural network","Frame theory","FRAMES","DECOMPOSITIONS","Scattering networks","ALGORITHMS","Sensitivity","CURVELET TRANSFORM","WAVELET"],"max_cite":5.0,"pub_year":2018.0,"sources":"['ieee', 'scp', 'wos']","rawkeys":["invariant scattering","band-limited functions","translation invariants","scattering networks","convolutional neural network","frame theory","frames","machine learning","curvelet transform","algorithms","texture classification","ridgelet","deformation sensitivities","recognition","translation invariance","decompositions","wavelet","deep convolutional neural networks","sensitivity","representations","feature extraction"],"tags":["invariant scattering","band-limited functions","scattering networks","convolutional neural network","frame theory","frames","machine learning","curvelet transform","algorithms","texture classification","deformation sensitivities","recognition","translation invariance","ridgelets","wavelet","decomposition","representation","sensitivity","feature extraction"]},{"p_id":28998,"title":"Improving patch-based scene text script identification with ensembles of conjoined networks","abstract":"\u00a9 2017 Elsevier LtdThis paper focuses on the problem of script identification in scene text images. Facing this problem with state of the art CNN classifiers is not straightforward, as they fail to address a key characteristic of scene text instances: their extremely variable aspect ratio. Instead of resizing input images to a fixed aspect ratio as in the typical use of holistic CNN classifiers, we propose here a patch-based classification framework in order to preserve discriminative parts of the image that are characteristic of its class. We describe a novel method based on the use of ensembles of conjoined networks to jointly learn discriminative stroke-parts representations and their relative importance in a patch-based classification scheme. Our experiments with this learning procedure demonstrate state-of-the-art results in two public script identification datasets. In addition, we propose a new public benchmark dataset for the evaluation of multi-lingual scene text end-to-end reading systems. Experiments done in this dataset demonstrate the key role of script identification in a complete end-to-end system that combines our script identification method with a previously published text detector and an off-the-shelf OCR engine.","keywords_author":["Convolutional neural networks","Ensemble of conjoined networks","Multi-language OCR","Scene text understanding","Script identification","Script identification","Scene text understanding","Multi-language OCR","Convolutional neural networks","Ensemble of conjoined networks"],"keywords_other":["Classification framework","Learning procedures","FEATURES","Classification scheme","CLASSIFICATION","PAGE SEGMENTATION","Scene Text","RECOGNITION","Convolutional neural network","Script identification","Key characteristics","DOCUMENT IMAGES","Multi languages"],"max_cite":5.0,"pub_year":2017.0,"sources":"['ieee', 'scp', 'wos']","rawkeys":["classification framework","recognition","convolutional neural networks","features","multi-language ocr","document images","multi languages","script identification","page segmentation","ensemble of conjoined networks","classification","convolutional neural network","scene text","key characteristics","classification scheme","scene text understanding","learning procedures"],"tags":["classification framework","recognition","features","multi-language ocr","document images","multi languages","script identification","page segmentation","ensemble of conjoined networks","classification","convolutional neural network","scene text","key characteristics","classification scheme","scene text understanding","learning procedures"]},{"p_id":12617,"title":"Deep Convolutional Neural Networks and Data Augmentation for Environmental Sound Classification","abstract":"The ability of deep convolutional neural networks (CNNs) to learn discriminative spectro-temporal patterns makes them well suited to environmental sound classification. However, the relative scarcity of labeled data has impeded the exploitation of this family of high-capacity models. This study has two primary contributions: first, we propose a deep CNN architecture for environmental sound classification. Second, we propose the use of audio data augmentation for overcoming the problem of data scarcity and explore the influence of different augmentations on the performance of the proposed CNN architecture. Combined with data augmentation, the proposed model produces state-of-theart results for environmental sound classification. We show that the improved performance stems from the combination of a deep, highcapacity model and an augmented training set: this combination outperforms both the proposed CNN without augmentation and a \"shallow\" dictionary learning model with augmentation. Finally, we examine the influence of each augmentation on the model's classification accuracy for each class, and observe that the accuracy for each class is influenced differently by each augmentation, suggesting that the performance of the model could be improved further by applying class-conditional data augmentation.","keywords_author":["Deep convolutional neural networks (CNNs)","deep learning","environmental sound classification","urban sound dataset","Deepconvolutional neural networks (CNNs)","deep learning","environmental sound classification","urban sound dataset"],"keywords_other":["Dictionary learning","Environmental sound classifications","Data augmentation","urban sound dataset","RECOGNITION","Temporal pattern","Primary contribution","Convolutional neural network","Classification accuracy"],"max_cite":29.0,"pub_year":2017.0,"sources":"['ieee', 'scp', 'wos']","rawkeys":["primary contribution","environmental sound classification","recognition","environmental sound classifications","deep learning","classification accuracy","data augmentation","urban sound dataset","deep convolutional neural networks (cnns)","dictionary learning","temporal pattern","convolutional neural network","deepconvolutional neural networks (cnns)"],"tags":["primary contribution","environmental sound classification","recognition","classification accuracy","machine learning","data augmentation","urban sound dataset","dictionary learning","temporal pattern","convolutional neural network"]},{"p_id":12618,"title":"Representation learning for mammography mass lesion classification with convolutional neural networks","abstract":"Background and objective: The automatic classification of breast imaging lesions is currently an unsolved problem. This paper describes an innovative representation learning framework for breast cancer diagnosis in mammography that integrates deep learning techniques to automatically learn discriminative features avoiding the design of specific hand-crafted image-based feature detectors.","keywords_author":["Breast cancer","Computer-aided diagnosis","Convolutional neural networks","Feature learning","Mammography","Breast cancer","Feature learning","Convolutional neural networks","Computer-aided diagnosis","Mammography"],"keywords_other":["Area under the ROC curve","DIAGNOSIS","Automatic classification","Neural Networks (Computer)","Breast cancer diagnosis","Breast Neoplasms","Breast Cancer","Humans","Histogram of oriented gradients (HOG)","BREAST-CANCER","Mammography","Machine Learning","RECOGNITION","Convolutional neural network","Biopsy","Feature learning","Discriminative features","Female"],"max_cite":56.0,"pub_year":2016.0,"sources":"['ieee', 'scp', 'wos']","rawkeys":["breast neoplasms","automatic classification","convolutional neural network","feature learning","area under the roc curve","computer-aided diagnosis","machine learning","histogram of oriented gradients (hog)","discriminative features","biopsy","diagnosis","neural networks (computer)","convolutional neural networks","recognition","humans","mammography","breast cancer diagnosis","breast-cancer","breast cancer","female"],"tags":["diagnosis","feature learning","breast neoplasms","recognition","neural networks","automatic classification","computer-aided diagnosis","female","histogram of oriented gradients","machine learning","humans","discriminative features","convolutional neural network","mammography","biopsy","breast cancer diagnosis","roc curve","breast cancer"]},{"p_id":78155,"title":"Predicting 3D lip shapes using facial surface EMG","abstract":"Aim","keywords_author":null,"keywords_other":["SURGERY","MODEL","RECOGNITION","CANCER","ELECTROMYOGRAPHY"],"max_cite":2.0,"pub_year":2017.0,"sources":"['wos']","rawkeys":["recognition","model","electromyography","cancer","surgery"],"tags":["recognition","model","electromyography","cancer","surgery"]},{"p_id":12622,"title":"Gender and Handedness Prediction from Offline Handwriting Using Convolutional Neural Networks","abstract":"Demographic handwriting-based classification problems, such as gender and handedness categorizations, present interesting applications in disciplines like Forensic Biometrics. This work describes an experimental study on the suitability of deep neural networks to three automatic demographic problems: gender, handedness, and combined gender-and-handedness classifications, respectively. Our research was carried out on two public handwriting databases: the IAM dataset containing English texts and the KHATT one with Arabic texts. The considered problems present a high intrinsic difficulty when extracting specific relevant features for discriminating the involved subclasses. Our solution is based on convolutional neural networks since these models had proven better capabilities to extract good features when compared to hand-crafted ones. Our work also describes the first approach to the combined gender-and-handedness prediction, which has not been addressed before by other researchers. Moreover, the proposed solutions have been designed using a unique network configuration for the three considered demographic problems, which has the advantage of simplifying the design complexity and debugging of these deep architectures when handling related handwriting problems. Finally, the comparison of achieved results to those presented in related works revealed the best average accuracy in the gender classification problem for the considered datasets.","keywords_author":null,"keywords_other":["ENGLISH","WRITER IDENTIFICATION","CLASSIFICATION","DATABASE","RECOGNITION"],"max_cite":0.0,"pub_year":2018.0,"sources":"['wos']","rawkeys":["recognition","english","database","writer identification","classification"],"tags":["recognition","databases","english","writer identification","classification"]},{"p_id":12633,"title":"Comparing fully convolutional networks, random forest, support vector machine, and patch-based deep convolutional neural networks for object-based wetland mapping using images from small unmanned aircraft system","abstract":"Deep learning networks have shown great success in several computer vision applications, but its implementation in natural land cover mapping in the context of object-based image analysis (OBIA) is rarely explored area especially in terms of the impact of training sample size on the performance comparison. In this study, two representatives of deep learning networks including fully convolutional networks (FCN) and patch-based deep convolutional neural networks (DCNN), and two conventional classifiers including random forest and support vector machine were implemented within the framework of OBIA to classify seven natural land cover types. We assessed the deep learning classifiers using different training sample sizes and compared their performance with traditional classifiers. FCN was implemented using two types of training samples to investigate its ability to utilize object surrounding information.Our results indicate that DCNN may produce inferior performance compared to conventional classifiers when the training sample size is small, but it tends to show substantially higher accuracy than the conventional classifiers when the training sample size becomes large. The results also imply that FCN is more efficient in utilizing the information in the training sample than DCNN and conventional classifiers, with higher if not similar achieved accuracy regardless of sample size. DCNN and FCN tend to show similar performance for the large sample size when the training samples used for training the FCN do not contain object surrounding label information. However, with the ability of utilizing surrounding label information, FCN always achieved much higher accuracy than all the other classification methods regardless of the number of training samples.","keywords_author":["FCN","deep learning","convolutional neural network","OBIA","UAS"],"keywords_other":["FEATURE REPRESENTATION","LAND-COVER","LIDAR DATA","EXTRACTION","SPECIES CLASSIFICATION","ECOSYSTEM SERVICES","RECOGNITION","REMOTE-SENSING IMAGERY","SPATIAL-RESOLUTION IMAGERY","AERIAL VEHICLE"],"max_cite":3.0,"pub_year":2018.0,"sources":"['ieee', 'wos']","rawkeys":["remote-sensing imagery","uas","lidar data","obia","recognition","species classification","deep learning","spatial-resolution imagery","ecosystem services","fcn","aerial vehicle","convolutional neural network","feature representation","land-cover","extraction"],"tags":["remote-sensing imagery","uas","recognition","species classification","spatial-resolution imagery","land cover","ecosystem services","machine learning","fully convolutional network","aerial vehicle","convolutional neural network","object based image analysis","feature representation","lidar data","extraction"]},{"p_id":12637,"title":"Automatic recognition of holistic functional brain networks using iteratively optimized convolutional neural networks (IO-CNN) with weak label initialization","abstract":"fMRI data decomposition techniques have advanced significantly from shallow models such as Independent Component Analysis (ICA) and Sparse Coding and Dictionary Learning (SCDL) to deep learning models such Deep Belief Networks (DBN) and Convolutional Autoencoder (DCAE). However, interpretations of those decomposed networks are still open questions due to the lack of functional brain atlases, no correspondence across decomposed or reconstructed networks across different subjects, and significant individual variabilities. Recent studies showed that deep learning, especially deep convolutional neural networks (CNN), has extraordinary ability of accommodating spatial object patterns, e.g., our recent works using 3D CNN for fMRI-derived network classifications achieved high accuracy with a remarkable tolerance for mistakenly labelled training brain networks. However, the training data preparation is one of the biggest obstacles in these supervised deep learning models for functional brain network map recognitions, since manual labelling requires tedious and time-consuming labours which will sometimes even introduce label mistakes. Especially for mapping functional networks in large scale datasets such as hundreds of thousands of brain networks used in this paper, the manual labelling method will become almost infeasible. In response, in this work, we tackled both the network recognition and training data labelling tasks by proposing a new iteratively optimized deep learning CNN (IO-CNN) framework with an automatic weak label initialization, which enables the functional brain networks recognition task to a fully automatic large-scale classification procedure. Our extensive experiments based on ABIDE-II 1099 brains' fMRI data showed the great promise of our IO-CNN framework. (C) 2018 Elsevier B.V. All rights reserved.","keywords_author":["Convolutional neural networks","Deep learning","fMRI","Functional brain networks","Recognition","Weak label initialization","fMRI","Functional brain networks","Deep learning","Convolutional neural networks","Recognition","Weak label initialization"],"keywords_other":["Weak labels","Recognition","ATLASES","Brain networks","fMRI","Convolutional neural network","FMRI","SPARSE REPRESENTATION","ARCHITECTURE"],"max_cite":0.0,"pub_year":2018.0,"sources":"['scp', 'wos']","rawkeys":["functional brain networks","recognition","convolutional neural networks","deep learning","atlases","sparse representation","weak labels","fmri","convolutional neural network","weak label initialization","brain networks","architecture"],"tags":["functional brain networks","recognition","sparse representation","machine learning","atlas","weak labels","fmri","convolutional neural network","weak label initialization","brain networks","architecture"]},{"p_id":12642,"title":"Automated Detection of Obstructive Sleep Apnea Events from a Single-Lead Electrocardiogram Using a Convolutional Neural Network","abstract":"In this study, we propose a method for the automated detection of obstructive sleep apnea (OSA) from a single-lead electrocardiogram (ECG) using a convolutional neural network (CNN). A CNN model was designed with six optimized convolution layers including activation, pooling, and dropout layers. One-dimensional (1D) convolution, rectified linear units (ReLU), and max pooling were applied to the convolution, activation, and pooling layers, respectively. For training and evaluation of the CNN model, a single-lead ECG dataset was collected from 82 subjects with OSA and was divided into training (including data from 63 patients with 34,281 events) and testing (including data from 19 patients with 8571 events) datasets. Using this CNN model, a precision of 0.99%, a recall of 0.99%, and an F-1-score of 0.99% were attained with the training dataset; these values were all 0.96% when the CNN was applied to the testing dataset. These results show that the proposed CNN model can be used to detect OSA accurately on the basis of a single-lead ECG. Ultimately, this CNN model may be used as a screening tool for those suspected to suffer from OSA.","keywords_author":["Obstructive sleep apnea","Single-lead ECG","Convolutional neural network"],"keywords_other":["RECOGNITION","RECORDINGS","CLASSIFICATION","HYPOPNEA EVENTS"],"max_cite":0.0,"pub_year":2018.0,"sources":"['wos']","rawkeys":["recognition","hypopnea events","single-lead ecg","recordings","obstructive sleep apnea","classification","convolutional neural network"],"tags":["recognition","hypopnea events","single-lead ecg","recordings","obstructive sleep apnea","classification","convolutional neural network"]},{"p_id":12644,"title":"Learning distributed representations of RNA sequences and its application for predicting RNA-protein binding sites with a convolutional neural network","abstract":"RNA-binding proteins (RBPs) play a crucial role in gene regulation. Unfortunately, experimental approaches for detecting the RNA-protein binding sites on RNAs are still high-cost and time-consuming. Computer algorithms are therefore wanted for automatic detection of the binding sites from sequences. This usually requires efficient representations of sequences of various length into vectors of the same size. For example, the k-mers representation as one-hot encoding is a simple widely used approach in the RBP binding sites prediction field. However, the k-mer feature representation will lead to extremely high-dimensional and sparse problems. Furthermore, the k-mer feature representation ignores the positional information within the sequences, which could negatively impact its predictive power.","keywords_author":["RNA-binding protein","k-mers","Distributed representation","Convolutional neural network"],"keywords_other":["REGRESSION","CLIP","AMINO-ACIDS","TRANSCRIPTOME-WIDE IDENTIFICATION","SUITE","RECOGNITION","SPECIFICITIES"],"max_cite":1.0,"pub_year":2018.0,"sources":"['wos']","rawkeys":["specificities","amino-acids","suite","recognition","k-mers","distributed representation","convolutional neural network","rna-binding protein","regression","clip","transcriptome-wide identification"],"tags":["suite","recognition","specificity","k-mers","rna-binding proteins","amino acids","distributed representation","convolutional neural network","regression","clip","transcriptome-wide identification"]},{"p_id":12669,"title":"Emergence of Convolutional Neural Network in Future Medicine: Why and How. A Review on Brain Tumor Segmentation","abstract":"Manual analysis of brain tumors magnetic resonance images is usually accompanied by some problem. Several techniques have been proposed for the brain tumor segmentation. This study will be focused on searching popular databases for related studies, theoretical and practical aspects of Convolutional Neural Network surveyed in brain tumor segmentation. Based on our findings, details about related studies including the datasets used, evaluation parameters, preferred architectures and complementary steps analyzed. Deep learning as a revolutionary idea in image processing, achieved brilliant results in brain tumor segmentation too. This can be continuing until the next revolutionary idea emerging.","keywords_author":["convolution neural network","brain tumor","segmentation","deep learning"],"keywords_other":["MODEL","RECOGNITION","EPIDEMIOLOGY","IMAGES"],"max_cite":0.0,"pub_year":2018.0,"sources":"['wos']","rawkeys":["recognition","images","model","segmentation","convolution neural network","deep learning","epidemiology","brain tumor"],"tags":["recognition","images","model","segmentation","epidemiology","machine learning","convolutional neural network","brain tumors"]},{"p_id":12676,"title":"A deep convolutional neural network with new training methods for bearing fault diagnosis under noisy environment and different working load","abstract":"In recent years, intelligent fault diagnosis algorithms using machine learning technique have achieved much success. However, due to the fact that in real world industrial applications, the working load is changing all the time and noise from the working environment is inevitable, degradation of the performance of intelligent fault diagnosis methods is very serious. In this paper, a new model based on deep learning is proposed to address the problem. Our contributions of include: First, we proposed an end-to-end method that takes raw temporal signals as inputs and thus doesn't need any time consuming denoising preprocessing. The model can achieve pretty high accuracy under noisy environment. Second, the model does not rely on any domain adaptation algorithm or require information of the target domain. It can achieve high accuracy when working load is changed. To understand the proposed model, we will visualize the learned features, and try to analyze the reasons behind the high performance of the model. (C) 2017 Published by Elsevier Ltd.","keywords_author":["Anti-noise","Convolutional neural networks","End-to-end","Intelligent fault diagnosis","Load domain adaptation","Intelligent fault diagnosis","Convolutional neural networks","Load domain adaptation","Anti-noise","End-to-end"],"keywords_other":["Anti noise","ROTATING MACHINERY","Intelligent fault diagnosis","End to end","RECOGNITION","Convolutional neural network","Domain adaptation"],"max_cite":13.0,"pub_year":2018.0,"sources":"['ieee', 'scp', 'wos']","rawkeys":["anti noise","load domain adaptation","recognition","convolutional neural networks","end to end","end-to-end","anti-noise","convolutional neural network","domain adaptation","intelligent fault diagnosis","rotating machinery"],"tags":["load domain adaptation","recognition","denoising autoencoder","end to end","anti-noise","convolutional neural network","intelligent fault diagnosis","rotating machinery"]},{"p_id":12682,"title":"Sea Ice Concentration Estimation During Melt From Dual-Pol SAR Scenes Using Deep Convolutional Neural Networks: A Case Study","abstract":"High-resolution ice concentration maps are of great interest for ship navigation and ice hazard forecasting. In this case study, a convolutional neural network (CNN) has been used to estimate ice concentration using synthetic aperture radar (SAR) scenes captured during the melt season. These dual-pol RADARSAT-2 satellite images are used as input, and the ice concentration is the direct output from the CNN. With no feature extraction or segmentation postprocessing, the absolute mean errors of the generated ice concentration maps are less than 10% on average when compared with manual interpretation of the ice state by ice experts. The CNN is demonstrated to produce ice concentration maps with more detail than produced operationally. Reasonable ice concentration estimations are made in melt regions and in regions of low ice concentration.","keywords_author":["Convolutional neural network (CNN)","ice concentration","synthetic aperture radar (SAR)"],"keywords_other":["BALTIC SEA","RECOGNITION","ALGORITHM","IMAGES"],"max_cite":16.0,"pub_year":2016.0,"sources":"['ieee', 'wos']","rawkeys":["algorithm","recognition","images","baltic sea","synthetic aperture radar (sar)","convolutional neural network (cnn)","ice concentration"],"tags":["recognition","images","baltic sea","synthetic aperture radar","ice concentration","convolutional neural network","algorithms"]},{"p_id":12688,"title":"Characterization and identification of asphalt mixtures based on Convolutional Neural Network methods using X-ray scanning images","abstract":"Accurate identification of asphalt mixtures based on images is a difficult task due to the complex components of materials under random mixing processes. This study explores the application of a Convolutional Neural Network (CNN) in classifying and identifying asphalt mixtures using the sectional images obtained from the X-ray computed tomography (CT) method. Images of 11 asphalt mixtures with two mixing types (hot mix and cold recycled mix), three nominal maximum aggregate sizes (13, 20 and 25 mm), and two compaction methods (Marshall and Superpave Gyratory) were obtained and segmented into three parts, including air voids, mastics, and aggregates, based on digital image processing (DIP) methods. The CNN models were trained on different combinations of image sets, and data augmentation technique was applied to the train set. Test results showed that CNN models from binary images of mastics and aggregates could success classify different asphalt mixtures with higher reliability than CNN model from images of air voids. CNN model from mastic could better distinguished different compaction methods while CNN model from aggregates could recognize different gradation types better. (C) 2018 Elsevier Ltd. All rights reserved.","keywords_author":["Asphalt mixtures","Image processing","X-ray CT scanning","Convolutional Neural Network","Image classification"],"keywords_other":["IMAGING ANALYSIS","INTERNAL STRUCTURE","COMPUTED-TOMOGRAPHY","RUTTING PERFORMANCE","VOIDS","RECOGNITION","CONTACT","HOT-MIX ASPHALT"],"max_cite":0.0,"pub_year":2018.0,"sources":"['wos']","rawkeys":["contact","imaging analysis","recognition","asphalt mixtures","voids","image processing","hot-mix asphalt","internal structure","convolutional neural network","x-ray ct scanning","rutting performance","image classification","computed-tomography"],"tags":["contact","recognition","voids","image processing","asphalt mixture","hot-mix asphalt","internal structure","computed tomography","convolutional neural network","x-ray ct scanning","rutting performance","image classification","image analysis"]},{"p_id":12694,"title":"Towards automatic wild animal monitoring: Identification of animal species in camera-trap images using very deep convolutional neural networks","abstract":"Non-intrusive monitoring of animals in the wild is possible using camera trapping networks. The cameras are triggered by sensors in order to disturb the animals as little as possible. This approach produces a high volume of data (in the order of thousands or millions of images) that demands laborious work to analyze both useless (incorrect detections, which are the most) and useful (images with presence of animals). In this work, we show that as soon as some obstacles are overcome, deep neural networks can cope with the problem of the automated species classification appropriately. As case of study, the most common 26 of 48 species from the Snapshot Serengeti (SSe) dataset were selected and the potential of the Very Deep Convolutional neural networks framework for the species identification task was analyzed. In the worst-case scenario (unbalanced training dataset containing empty images) the method reached 35.4% Top-1 and 60.4% Top-5 accuracy. For the best scenario (balanced dataset, images containing foreground animals only, and manually segmented) the accuracy reached a 88.9% Top-1 and 98.1% Top-5, respectively. To the best of our knowledge, this is the first published attempt on solving the automatic species recognition on the SSe dataset. In addition, a comparison with other approaches on a different dataset was carried out, showing that the architectures used in this work outperformed previous approaches. The limitations of the method, drawbacks, as well as new challenges in automatic camera-trap species classification are widely discussed.","keywords_author":["Animal species recognition","Deep convolutional neural networks","Camera-trap","Snapshot Serengeti"],"keywords_other":["RECOGNITION"],"max_cite":1.0,"pub_year":2017.0,"sources":"['wos']","rawkeys":["recognition","animal species recognition","snapshot serengeti","deep convolutional neural networks","camera-trap"],"tags":["recognition","animal species recognition","snapshot serengeti","camera traps","convolutional neural network"]},{"p_id":12720,"title":"Using Deep Convolutional Neural Network Architectures for Object Classification and Detection Within X-Ray Baggage Security Imagery","abstract":"We consider the use of deep convolutional neural networks (CNNs) with transfer learning for the image classification and detection problems posed within the context of X-ray baggage security imagery. The use of the CNN approach requires large amounts of data to facilitate a complex end-to-end feature extraction and classification process. Within the context of X-ray security screening, limited availability of object of interest data examples can thus pose a problem. To overcome this issue, we employ a transfer learning paradigm such that a pre-trained CNN, primarily trained for generalized image classification tasks where sufficient training data exists, can be optimized explicitly as a later secondary process towards this application domain. To provide a consistent feature-space comparison between this approach and traditional feature space representations, we also train support vector machine (SVM) classifier on CNN features. We empirically show that fine-tuned CNN features yield superior performance to conventional hand-crafted features on object classification tasks within this context. Overall we achieve 0.994 accuracy based on AlexNet features trained with SVM classifier. In addition to classification, we also explore the applicability of multiple CNN driven detection paradigms, such as sliding window-based CNN (SW-CNN), Faster region-based CNNs (F-RCNNs), region-based fully convolutional networks (R-FCN), and YOLOv2. We train numerous networks tackling both single and multiple detections over SW-CNN\/F-RCNN\/R-FCN\/YOLOv2 variants. YOLOv2, Faster-RCNN, and R-FCN provide superior results to the more traditional SW-CNN approaches. With the use of YOLOv2, using input images of size 544 x 544, we achieve 0.885 mean average precision (mAP) for a six-class object detection problem. The same approach with an input of size 416 x 416 yields 0.974 mAP for the two-class firearm detection problem and requires approximately 100 ms per image. Overall we illustrate the comparative performance of these techniques and show that object localization strategies cope well with cluttered X-ray security imagery, where classification techniques fail.","keywords_author":["Deep convolutional neural networks","transfer learning","image classification","detection","X-ray baggage security"],"keywords_other":["RECOGNITION"],"max_cite":0.0,"pub_year":2018.0,"sources":"['ieee', 'wos']","rawkeys":["recognition","transfer learning","deep convolutional neural networks","x-ray baggage security","detection","image classification"],"tags":["recognition","transfer learning","x-ray baggage security","detection","convolutional neural network","image classification"]},{"p_id":37300,"title":"Improving Right Whale recognition by fine-tuning alignment and using wide localization network","abstract":"\u00a9 2017 IEEE. Right Whales can be recognized by the callosities pattern on their heads. They are an endangered species with an estimated 450 whales remaining. Marine biologists regularly perform manual recognition of the whales while monitoring the population but the process is slow and time consuming. Deep learning methods achieved state-of-the-art results on several visual recognition tasks. However, training deep learning models on this task is very difficult because the number of training images is low. We propose a wide localization network which can be used to localize the region of interest in image. Once the region of interest is localized, a deep learning model can be used to classify the whales. The solution we describe in this paper achieves an accuracy score of 78.7% and ranks as one of the best 3 solutions on this dataset.","keywords_author":["Convolutional Neural Network","Deep Learning","Detection","Image Classification","Localization","Recognition","Whale Detection","Whale Localization","Whale Recognition"],"keywords_other":["Recognition","Localization","Convolutional neural network","Whale Localization","Whale Recognition"],"max_cite":1.0,"pub_year":2017.0,"sources":"['scp']","rawkeys":["recognition","localization","deep learning","whale localization","detection","convolutional neural network","whale recognition","whale detection","image classification"],"tags":["recognition","localization","machine learning","whale localization","detection","convolutional neural network","whale recognition","whale detection","image classification"]},{"p_id":12731,"title":"Human Activity Classification with Transmission and Reflection Coefficients of On-Body Antennas Through Deep Convolutional Neural Networks","abstract":"We propose to classify human activities based on transmission coefficient (S-21) and reflection coefficient (S-11) of on-body antennas with deep convolutional neural networks (DCNNs). It is shown that spectrograms of S-21 and S-11 exhibit unique time-varying signatures for different body motion activities that can be used for classification purposes. DCNN, a deep learning approach, is applied to spectrograms to learn the necessary features and classification boundaries. It is found that DCNN can achieve classification accuracies of 98.8% using S-21 and 97.1% using S-11. The effects of operating frequency and antenna location on the accuracy have been investigated.","keywords_author":["Convolutional neural network (CNN)","deep learning","human activity classification","joint time-frequency transform","on-body channel","Convolutional neural network (CNN)","deep learning","human activity classification","joint time-frequency transform","on-body channel"],"keywords_other":["AREA NETWORKS","Transmission and reflection coefficient","MICRO-DOPPLER SIGNATURES","Classification boundary","On-body channels","Time frequency transform","Transmission coefficients","RECOGNITION","Convolutional neural network","PROPAGATION","OUTLOOK","Human activities","Classification accuracy"],"max_cite":2.0,"pub_year":2017.0,"sources":"['ieee', 'scp', 'wos']","rawkeys":["classification boundary","time frequency transform","recognition","deep learning","on-body channel","classification accuracy","human activities","transmission coefficients","transmission and reflection coefficient","convolutional neural network (cnn)","joint time-frequency transform","micro-doppler signatures","outlook","convolutional neural network","on-body channels","propagation","area networks","human activity classification"],"tags":["classification boundary","recognition","on-body channel","classification accuracy","human activities","machine learning","time-frequency transformation","transmission and reflection coefficient","transmission coefficients","joint time-frequency transform","micro-doppler signatures","outlook","convolutional neural network","propagation","area networks","human activity classification"]},{"p_id":111048,"title":"Artificial Urdu Text Detection and Localization from Individual Video Frames","abstract":"In current era of technology, information acquisition from images and videos become most important task due to the rapid development of data mining and machine learning.The information can be either textual, visual, or combination of these. Text appearing in images or videos is a significant source of information and plays a vital role to perceive it. Developing a unified method to detect the text is hard, as textual properties (i.e. font, size, color, illumination, orientation, etc.) may vary with the complex background. So far, multimedia and computer vision community unable yet to standardize any ideal approach to extract the text smoothly. In this paper, a novel method is proposed to detect and localize artificial Urdu text in individual video frames. Firstly, Sobel and Canny edge detection operators are applied to input frame and are merged with MSER (Maximally Stable Extremal Region) detected regions. Next, geometric constraints are applied to eliminate obvious non-text regions with large and small variations. Further refining of non-text regions is achieved by stroke width transform. SVM (Support Vector Machine) classifier is trained to classify text and non-text objects. Finally, bounding boxes are used to localize the text.Experimental results show that the proposed method is robust and efficient than state-of-the-art methods.","keywords_author":["Text Detection","Artificial Urdu Text","Video Images","Maximally Stable Extremal Region"],"keywords_other":["RECOGNITION","IMAGES"],"max_cite":0.0,"pub_year":2018.0,"sources":"['wos']","rawkeys":["recognition","images","video images","text detection","maximally stable extremal region","artificial urdu text"],"tags":["recognition","maximally stable extremal regions","images","text detection","artificial urdu text","video image"]},{"p_id":111049,"title":"Visual and textual information fusion using Kernel method for content based image retrieval","abstract":"In computer vision, each region of an image has an equal and important value that is either an object in an image or the text. The appearance of text within images is certainly a rich information for humans as well as for machines. So far, the art methods have explored either visual features or the social tags to retrieve similar images. Another possibility for image retrieval is to generate fully automatic tags\/keywords by extracting embedded and scene text in images along with low-level visual features and fuse them together. Considering this, we have investigated a novel approach to retrieve similar textual images by exploiting visual and textual characteristics of the image. The method extracts visual salient features in first step, and the text is detected and recognized in next step. The method allocates two feature vectors each for visual and textual, and fused them together using Kernel method. The method supports three modes of search: Image query, Keywords, and Combination of both. The experimental results on benchmark datasets shows the textual features can be as effective as visual features for CBIR applications.","keywords_author":["Visual contents","Textual contents","Information fusion","Image retrieval","CBIR"],"keywords_other":["LOCALIZATION","FEATURES","WORDS","CLASSIFICATION","RECOGNITION","SCENE IMAGES"],"max_cite":0.0,"pub_year":2018.0,"sources":"['wos']","rawkeys":["recognition","localization","features","image retrieval","visual contents","textual contents","classification","cbir","scene images","words","information fusion"],"tags":["scene image","recognition","localization","features","visual content","image retrieval","classification","textual content","content-based image retrieval","words","information fusion"]},{"p_id":111051,"title":"Watch, attend and parse: An end-to-end neural network based approach to handwritten mathematical expression recognition","abstract":"Machine recognition of a handwritten mathematical expression (HME) is challenging due to the ambiguities of handwritten symbols and the two-dimensional structure of mathematical expressions. Inspired by recent work in deep learning, we present Watch, Attend and Parse (WAP), a novel end-to-end approach based on neural network that learns to recognize HMEs in a two-dimensional layout and outputs them as one-dimensional character sequences in LaTeX format. Inherently unlike traditional methods, our proposed model avoids problems that stem from symbol segmentation, and it does not require a predefined expression grammar. Meanwhile, the problems of symbol recognition and structural analysis are handled, respectively, using a watcher and a parser. We employ a convolutional neural network encoder that takes HME images as input as the watcher and employ a recurrent neural network decoder equipped with an attention mechanism as the parser to generate LaTeX sequences. Moreover, the correspondence between the input expressions and the output LaTeX sequences is learned automatically by the attention mechanism. We validate the proposed approach on a benchmark published by the CROHME international competition. Using the official training dataset, WAP significantly outperformed the state-of-the-art method with an expression recognition accuracy of 46.55% on CROHME 2014 and 44.55% on CROHME 2016. (C) 2017 Elsevier Ltd. All rights reserved.","keywords_author":["Handwritten mathematical expression","recognition","Neural network","Attention"],"keywords_other":null,"max_cite":1.0,"pub_year":2017.0,"sources":"['wos']","rawkeys":["attention","neural network","recognition","handwritten mathematical expression"],"tags":["attention","recognition","handwritten mathematical expression","neural networks"]},{"p_id":37323,"title":"Deepmap+: Recognizing high-level indoor semantics using virtual features and samples based on a multi-length window framework","abstract":"\u00a9 2017 by the authors. Licensee MDPI, Basel, Switzerland.Existing indoor semantic recognition schemes are mostly capable of discovering patterns through smartphone sensing, but it is hard to recognize rich enough high-level indoor semantics for map enhancement. In this work we present DeepMap+, an automatical inference system for recognizing high-level indoor semantics using complex human activities with wrist-worn sensing. DeepMap+ is the first deep computation system using deep learning (DL) based on a multi-length window framework to enrich the data source. Furthermore, we propose novel methods of increasing virtual features and virtual samples for DeepMap+ to better discover hidden patterns of complex hand gestures. We have performed 23 high-level indoor semantics (including public facilities and functional zones) and collected wrist-worn data at a Wal-Mart supermarket. The experimental results show that our proposed methods can effectively improve the classification accuracy.","keywords_author":["Activity recognition","Deep learning","Indoor semantic inference","Multi-length windows","Virtual features","Virtual samples","indoor semantic inference","activity recognition","multi-length windows","virtual samples","virtual features","deep learning"],"keywords_other":["Semantic recognition","Activity recognition","Virtual sample","Smartphone sensing","Virtual features","Computation systems","RECOGNITION","Semantic inference","Classification accuracy"],"max_cite":1.0,"pub_year":2017.0,"sources":"['scp', 'wos']","rawkeys":["multi-length windows","semantic recognition","virtual samples","recognition","activity recognition","deep learning","computation systems","classification accuracy","virtual sample","virtual features","indoor semantic inference","smartphone sensing","semantic inference"],"tags":["multi-length windows","semantic recognition","virtual samples","recognition","activity recognition","classification accuracy","computational system","machine learning","virtual features","indoor semantic inference","smartphone sensing","semantic inference"]},{"p_id":12748,"title":"Automatic diagnosis of abnormal macula in retinal optical coherence tomography images using wavelet-based convolutional neural network features and random forests classifier","abstract":"The present research intends to propose a fully automatic algorithm for the classification of three-dimensional (3-D) optical coherence tomography (OCT) scans of patients suffering from abnormal macula from normal candidates. The method proposed does not require any denoising, segmentation, retinal alignment processes to assess the intraretinal layers, as well as abnormalities or lesion structures. To classify abnormal cases from the control group, a two-stage scheme was utilized, which consists of automatic subsystems for adaptive feature learning and diagnostic scoring. In the first stage, a wavelet-based convolutional neural network (CNN) model was introduced and exploited to generate B-scan representative CNN codes in the spatial-frequency domain, and the cumulative features of 3-D volumes were extracted. In the second stage, the presence of abnormalities in 3-D OCTs was scored over the extracted features. Two different retinal SD-OCT datasets are used for evaluation of the algorithm based on the unbiased fivefold cross-validation (CV) approach. The first set constitutes 3-D OCT images of 30 normal subjects and 30 diabetic macular edema (DME) patients captured from the Topcon device. The second publicly available set consists of 45 subjects with a distribution of 15 patients in age-related macular degeneration, DME, and normal classes from the Heidelberg device. With the application of the algorithm on overall OCT volumes and 10 repetitions of the fivefold CV, the proposed scheme obtained an average precision of 99.33% on dataset1 as a two-class classification problem and 98.67% on dataset2 as a three-class classification task. (C) 2018 Society of Photo-Optical Instrumentation Engineers (SPIE)","keywords_author":["classification","convolutional neural networks","feature learning","macular disease","retinal optical coherence tomography","spatial-frequency information"],"keywords_other":["BOUNDARIES","EDEMA","LAYER SEGMENTATION","DEGENERATION","MACHINE","RECOGNITION","AMD","OCT IMAGES"],"max_cite":0.0,"pub_year":2018.0,"sources":"['ieee', 'wos']","rawkeys":["retinal optical coherence tomography","recognition","convolutional neural networks","boundaries","degeneration","machine","oct images","spatial-frequency information","amd","edema","classification","macular disease","feature learning","layer segmentation"],"tags":["retinal optical coherence tomography","recognition","boundaries","degeneration","machine","oct images","spatial-frequency information","amd","edema","classification","convolutional neural network","macular disease","feature learning","layer segmentation"]},{"p_id":111055,"title":"Text box proposals for handwritten word spotting from documents","abstract":"In this article, we propose a new approach to segmentation-free word spotting that is based on the combination of three different contributions. Firstly, inspired by the success of bounding box proposal algorithms in object recognition, we propose a scheme to generate a set of word-independent text box proposals. For that, we generate a set of atomic bounding boxes based on simple connected component analysis that are combined using a set of spatial constraints in order to generate the final set of text box proposals. Secondly, an attribute representation based on the Pyramidal Histogram of Characters (PHOC) is encoded in an integral image and used to efficiently evaluate text box proposals for retrieval. Thirdly, we also propose an indexing scheme for fast retrieval based on character n-grams. For the generation of the index a similar attribute space based on a Pyramidal Histogram of Character N-grams (PHON) is used. All attribute models are learned using linear SVMs over the Fisher Vector representation of the word images along with the PHOC or PHON labels of the corresponding words. We show the performance of the proposed approach in both tasks of query-by-string and query-by-example in standard single- and multi-writer data sets, reporting state-of-the-art results.","keywords_author":["Word spotting","Segmentation-free","Bounding box proposals","Word attributes","Pyramidal Histogram of Characters"],"keywords_other":["HISTORICAL DOCUMENTS","LINE","RETRIEVAL","RECOGNITION","SEGMENTATION"],"max_cite":0.0,"pub_year":2018.0,"sources":"['wos']","rawkeys":["recognition","segmentation","line","historical documents","segmentation-free","pyramidal histogram of characters","retrieval","word spotting","bounding box proposals","word attributes"],"tags":["recognition","segmentation","line","historical documents","segmentation-free","pyramidal histogram of characters","retrieval","word spotting","bounding box proposals","word attributes"]},{"p_id":29142,"title":"Deep Visual Attention Prediction","abstract":"\u00a9 1992-2012 IEEE. In this paper, we aim to predict human eye fixation with view-free scenes based on an end-to-end deep learning architecture. Although convolutional neural networks (CNNs) have made substantial improvement on human attention prediction, it is still needed to improve the CNN-based attention models by efficiently leveraging multi-scale features. Our visual attention network is proposed to capture hierarchical saliency information from deep, coarse layers with global saliency information to shallow, fine layers with local saliency response. Our model is based on a skip-layer network structure, which predicts human attention from multiple convolutional layers with various reception fields. Final saliency prediction is achieved via the cooperation of those global and local predictions. Our model is learned in a deep supervision manner, where supervision is directly fed into multi-level layers, instead of previous approaches of providing supervision only at the output layer and propagating this supervision back to earlier layers. Our model thus incorporates multi-level saliency predictions within a single network, which significantly decreases the redundancy of previous approaches of learning multiple network streams with different input scales. Extensive experimental analysis on various challenging benchmark data sets demonstrate our method yields the state-of-the-art performance with competitive inference time.1 1Our source code is available at https:\/\/github.com\/wenguanwang\/deepattention.","keywords_author":["convolutional neural network","deep learning","human eye fixation","saliency detection","Visual attention","Visual attention","convolutional neural network","saliency detection","deep learning","human eye fixation"],"keywords_other":["Human eye","Saliency detection","Predictive models","Computational model","NETWORKS","MODEL","RECOGNITION","SALIENCY DETECTION","Convolutional neural network","OPTIMIZATION","Visual Attention"],"max_cite":4.0,"pub_year":2018.0,"sources":"['ieee', 'scp', 'wos']","rawkeys":["recognition","model","deep learning","predictive models","saliency detection","human eye","networks","convolutional neural network","optimization","visual attention","human eye fixation","computational model"],"tags":["computational modeling","recognition","model","human eye fixations","predictive models","machine learning","saliency detection","human eye","networks","convolutional neural network","optimization","visual attention"]},{"p_id":12766,"title":"Robust classification approach for segmentation of blood defects in cod fillets based on deep convolutional neural networks and support vector machines and calculation of gripper vectors for robotic processing","abstract":"Despite advances in computer vision and segmentation techniques, the segmentation of food defects such as blood spots, exhibiting a high degree of randomness and biological variation in size and coloration degree, has proven to be extremely challenging and it is not successfully resolved. Therefore, in this paper, we propose an approach for robust automated pixel-wise classification for segmentation of blood spots, focusing specifically on challenging texture-uniform cod fish fillets. A multimodal vision system, described in this paper, enables perfectly aligned RGB and D-depth images for localization of segmented blood spots in 3D. Classification models based on (1) Convolutional Neural Networks - CNN and (2) Support Vector Machines - SVM for the classification of defective fillets were developed. A colour based, pixel-wise and SVM-based model was developed for accurate segmentation and localisation of blood spots resulting in 96% overall accuracy when tested on whole fillet images. Classification between normal and defective fillets based on GPU (Graphical Processing Unit)- accelerated CNN classification model achieved 100% accuracy, versus the SVM-based model achieving 99%. We present a novel data augmentation approach that desensitizes the CNN towards shape features and makes the CNN to focus more on colour. We show how pixel-wise classification is,used for an accurate localization of blood spots in 3D space and calculation of resulting 3D gripper vectors, as an input to robotic processing. (C) 2017 Elsevier B.V. All rights reserved.","keywords_author":["Image segmentation","Industrial application","RGB-D image","Robotics","Support vector machines","Deep convolutional neural networks"],"keywords_other":["MULTICLASS","IMAGE-ANALYSIS","VISION","COLOR","TEXTURE FEATURES","RECOGNITION"],"max_cite":2.0,"pub_year":2017.0,"sources":"['wos']","rawkeys":["industrial application","recognition","multiclass","rgb-d image","texture features","deep convolutional neural networks","vision","robotics","color","support vector machines","image-analysis","image segmentation"],"tags":["industrial application","recognition","texture features","machine learning","vision","rgb-d images","robotics","color","convolutional neural network","multi-class","image segmentation","image analysis"]},{"p_id":53731,"title":"PmBc : Conception et performances d'un nouvel algorithme d'induction d'hyperrectangles","abstract":"This paper presents PmBc, a new hyperrectangle induction algorithm for numerical spaces. It is based on successive generalizations, guided by examples similarity. We discuss the main design decisions underlying PmBc and compare them with similar approaches in hyperrectangle induction. We then provide experimental comparisons between PmBc, BNGE, C4.5 and CN2 on various learning tasks. PmBc exhibits higher classification accuracy than the three other algorithms while spending more time for computation than C4.5 and less than BNGE and CN2.","keywords_author":["Generalization","Hyperrectangle induction","Machine learning","Similarity assumption"],"keywords_other":null,"max_cite":0.0,"pub_year":1998.0,"sources":"['scp']","rawkeys":["hyperrectangle induction","machine learning","similarity assumption","generalization"],"tags":["hyperrectangle induction","recognition","similarity assumption","machine learning"]},{"p_id":94692,"title":"Bearing Fault Diagnosis Based on Improved Locality-Constrained Linear Coding and Adaptive PSO-Optimized SVM","abstract":"A novel bearing fault diagnosis method based on improved locality-constrained linear coding (LLC) and adaptive PSO-optimized support vector machine (SVM) is proposed. In traditional LLC, each feature is encoded by using a fixed number of bases without considering the distribution of the features and the weight of the bases. To address these problems, an improved LLC algorithm based on adaptive and weighted bases is proposed. Firstly, preliminary features are obtained by wavelet packet node energy. Then, dictionary learning with class-wise K-SVD algorithm is implemented. Subsequently, based on the learned dictionary the LLC codes can be solved using the improved LLC algorithm. Finally, SVM optimized by adaptive particle swarm optimization (PSO) is utilized to classify the discriminative LLC codes and thus bearing fault diagnosis is realized. In the dictionary leaning stage, other methods such as selecting the samples themselves as dictionary and K-means are also conducted for comparison. The experiment results show that the LLC codes can effectively extract the bearing fault characteristics and the improved LLC outperforms traditional LLC. The dictionary learned by class-wise K-SVD achieves the best performance. Additionally, adaptive PSO-optimized SVM can greatly enhance the classification accuracy comparing with SVM using default parameters and linear SVM.","keywords_author":null,"keywords_other":["DICTIONARIES","COMPRESSION","ROLLING ELEMENT BEARINGS","VIBRATION SIGNALS","ATOMIC DECOMPOSITION","K-SVD","RECOGNITION","ROLLER-BEARINGS","SPARSE REPRESENTATION","FEATURE-EXTRACTION"],"max_cite":1.0,"pub_year":2017.0,"sources":"['wos']","rawkeys":["compression","recognition","atomic decomposition","feature-extraction","sparse representation","rolling element bearings","vibration signals","roller-bearings","dictionaries","k-svd"],"tags":["recognition","atomic decomposition","codes","sparse representation","vibration signal","feature extraction","roller-bearings","dictionaries","rolling element bearing","k-svd"]},{"p_id":53742,"title":"Integrated model - A proposal to handle noise","abstract":"\u00a9 Springer-Verlag Berlin Heidelberg 1995. We present the theoretical background of the Integrated Model, a new induction algorithm proposed and implemented by the authors. The algorithm relies on a bottom-up strategy, from particular to general, feature less common that the usual top-down strategy founded in a great number of induction tools. We introduce a method for finding the values of the basic probability assigmnent according to Theory of Evidence, called probabilistic mass. With this notion, we propose a generalisation of the algorithm capable of handle noisy data.","keywords_author":["Classification rules","Decision trees","Generalization","Induction","Machine learning","Noise","Theory of evidence","Uncertainty measures"],"keywords_other":["Classification rules","Noise","Uncertainty measures","Theory of evidence","Generalization","Induction"],"max_cite":0.0,"pub_year":1995.0,"sources":"['scp']","rawkeys":["noise","induction","uncertainty measures","classification rules","machine learning","generalization","theory of evidence","decision trees"],"tags":["recognition","noise","induction","uncertainty measures","classification rules","machine learning","theory of evidence","decision trees"]},{"p_id":53747,"title":"An application of machine learning in the domain of loan analysis","abstract":"\u00a9 Springer-Verlag Berlin Heidelberg 1993. Making decisions on whether to give or not a financial support to an industrial project is a very common and yet very complex task. Financial institutions need much expertise to deal with the large amount of information that has to be considered for this process. An expert system based approach seems to be an interesting solution to the problems raised by this type of application. Knowledge acquisition is, however, a very time consuming task. We have used APT, a multistrategy learning system, as a knowledge elicitation tool in the domain of loan decision. We describe the process of building and refining a knowledge base and compare the results of our approach to a conventional expert system.","keywords_author":["Generalization","Knowledge acquisition","Machine learning applications"],"keywords_other":["Generalization","Financial institution","Making decision","Multi-strategy learning","Industrial projects","Time-consuming tasks","Machine learning applications","Financial support"],"max_cite":0.0,"pub_year":1993.0,"sources":"['scp']","rawkeys":["making decision","knowledge acquisition","time-consuming tasks","machine learning applications","financial support","financial institution","generalization","multi-strategy learning","industrial projects"],"tags":["making decision","knowledge acquisition","recognition","time-consuming tasks","machine learning applications","financial support","financial institution","multi-strategy learning","industrial projects"]},{"p_id":29173,"title":"DeepCloud: Ground-Based Cloud Image Categorization Using Deep Convolutional Features","abstract":"\u00a9 2017 IEEE. Accurate ground-based cloud image categorization is a critical but challenging task that has not been well addressed. One of the essential issues that affect the performance is to extract the representative visual features. Nearly all of the existing methods rely on the hand-crafted descriptors (e.g., local binary patterns, CENsus TRsansform hISTogram, and scale-invariant feature transform). Their limited discriminative power indeed leads to the unsatisfactory performance. To alleviate this, we propose \"DeepCloud\" as a novel cloud image feature extraction approach by resorting to the deep convolutional visual features. In the recent years, the deep convolutional neural network (CNN) has achieved the promising results in lots of computer vision and image understanding fields. Nevertheless, it has not been applied to cloud image classification yet. Thus, we actually pay the first effort to fill this blank. Since cloud image classification can be attributed to a multi-instance learning problem, simply employing the convolutional features within CNN cannot achieve the promising result. To address this, Fisher vector encoding is applied to executing the spatial feature aggregation and high-dimensional feature mapping on the raw deep convolutional features. Moreover, the hierarchical convolutional layers are used simultaneously to capture the fine textural characteristics and high-level semantic information in the unified manner. To further leverage the performance, a cloud pattern mining and selection method are also proposed. It targets at finding the discriminative local patterns to better distinguish the different kinds of clouds. The experiments on a challenging ground-based cloud image data set demonstrate the superiority of the proposition over the state-of-the-art methods.","keywords_author":["Convolutional neural network (CNN)","deep learning","Fisher vector (FV)","ground-based cloud image categorization","pattern mining","Convolutional neural network (CNN)","deep learning","Fisher vector (FV)","ground-based cloud image categorization","pattern mining"],"keywords_other":["Pattern mining","State-of-the-art methods","SKY IMAGES","Scale invariant feature transforms","Textural characteristic","Fisher vectors","CLASSIFICATION","High dimensional feature","HEIGHT","RECOGNITION","ALGORITHMS","Convolutional neural network","Cloud image","FEATURE-EXTRACTION","SUPPORT"],"max_cite":4.0,"pub_year":2017.0,"sources":"['ieee', 'scp', 'wos']","rawkeys":["state-of-the-art methods","high dimensional feature","classification","convolutional neural network","height","sky images","support","fisher vectors","algorithms","scale invariant feature transforms","recognition","deep learning","feature-extraction","convolutional neural network (cnn)","cloud image","textural characteristic","pattern mining","fisher vector (fv)","ground-based cloud image categorization"],"tags":["state-of-the-art methods","height","scale invariant feature transforms","sky images","recognition","support","machine learning","high dimensional feature","texture characteristics","classification","convolutional neural network","feature extraction","cloud image","fisher vectors","algorithms","ground-based cloud image categorization","pattern mining"]},{"p_id":29186,"title":"Bass net: Band-adaptive spectral-spatial feature learning neural network for hyperspectral image classification","abstract":"\u00a9 1980-2012 IEEE. Deep learning based land cover classification algorithms have recently been proposed in the literature. In hyperspectral images (HSIs), they face the challenges of large dimensionality, spatial variability of spectral signatures, and scarcity of labeled data. In this paper, we propose an end-to-end deep learning architecture that extracts band specific spectral-spatial features and performs land cover classification. The architecture has fewer independent connection weights and thus requires fewer training samples. The method is found to outperform the highest reported accuracies on popular HSI data sets.","keywords_author":["Convolutional neural network (CNN)","deep learning","feature extraction","hyperspectral imagery","landcover classification","pattern classification","Convolutional neural network (CNN)","deep learning","feature extraction","hyperspectral imagery","landcover classification","pattern classification"],"keywords_other":["REMOTE-SENSING IMAGES","Labeled data","Spatial features","Training sample","Spectral signature","Learning architectures","SUPPORT VECTOR MACHINES","RECOGNITION","Connection weights","Spatial variability","Land cover classification"],"max_cite":4.0,"pub_year":2017.0,"sources":"['ieee', 'scp', 'wos']","rawkeys":["training sample","learning architectures","recognition","connection weights","deep learning","land cover classification","landcover classification","spatial features","convolutional neural network (cnn)","hyperspectral imagery","spatial variability","remote-sensing images","feature extraction","support vector machines","labeled data","spectral signature","pattern classification"],"tags":["training sample","learning architectures","recognition","connection weights","land cover classification","machine learning","landcover classification","spatial features","spatial variability","hyperspectral imaging","feature extraction","convolutional neural network","labeled data","remote sensing images","spectral signature","pattern classification"]},{"p_id":29191,"title":"Monitoring stress with a wrist device using context","abstract":"\u00a9 2017 Elsevier Inc. Being able to detect stress as it occurs can greatly contribute to dealing with its negative health and economic consequences. However, detecting stress in real life with an unobtrusive wrist device is a challenging task. The objective of this study is to develop a method for stress detection that can accurately, continuously and unobtrusively monitor psychological stress in real life. First, we explore the problem of stress detection using machine learning and signal processing techniques in laboratory conditions, and then we apply the extracted laboratory knowledge to real-life data. We propose a novel context-based stress-detection method. The method consists of three machine-learning components: a laboratory stress detector that is trained on laboratory data and detects short-term stress every 2 min; an activity recognizer that continuously recognizes the user's activity and thus provides context information; and a context-based stress detector that uses the outputs of the laboratory stress detector, activity recognizer and other contexts, in order to provide the final decision on 20-min intervals. Experiments on 55 days of real-life data showed that the method detects (recalls) 70% of the stress events with a precision of 95%.","keywords_author":["Context","Healthcare","Machine learning","Real life","Stress detection","Wrist device"],"keywords_other":["Signal Processing, Computer-Assisted","Laboratory conditions","Economic consequences","Wrist device","Humans","Stress, Psychological","Stress detection","Wrist","Machine Learning","Signal processing technique","Context","Monitoring, Physiologic","Real life","Life Change Events","Psychological stress"],"max_cite":4.0,"pub_year":2017.0,"sources":"['scp']","rawkeys":["psychological","monitoring","laboratory conditions","economic consequences","healthcare","stress","machine learning","real life","stress detection","wrist","humans","signal processing","computer-assisted","physiologic","life change events","psychological stress","signal processing technique","context","wrist device"],"tags":["monitoring","laboratory conditions","economic consequences","healthcare","stress","machine learning","real life","recognition","stress detection","wrist","humans","signal processing","computer-assisted","physiology","life change events","psychological stress","signal processing technique","context","wrist device"]},{"p_id":45583,"title":"Data-centric method for object observation through scattering media","abstract":"\u00a9 2018 SPIE. A data-centric method is introduced for object observation through scattering media. A large number of training pairs are used to characterize the relation between the object and the observation signals based on machine learning. Using the method object information can be retrieved even from strongly-disturbed signals. As potential applications, object recognition, imaging, and focusing through scattering media were demonstrated.","keywords_author":["data-centric method","focusing","imaging","machine learning","recognition","scattering"],"keywords_other":["On-machines","recognition","Data centric","Scattering media","Object information"],"max_cite":0.0,"pub_year":2018.0,"sources":"['scp']","rawkeys":["object information","data-centric method","recognition","scattering media","on-machines","machine learning","scattering","focusing","data centric","imaging"],"tags":["object information","data-centric method","recognition","images","on-machines","scattering media","machine learning","scattering","focus","data centric"]},{"p_id":29209,"title":"Machine learning framework for the detection of mental stress at multiple levels","abstract":"\u00a9 2013 IEEE.Mental stress has become a social issue and could become a cause of functional disability during routine work. In addition, chronic stress could implicate several psychophysiological disorders. For example, stress increases the likelihood of depression, stroke, heart attack, and cardiac arrest. The latest neuroscience reveals that the human brain is the primary target of mental stress, because the perception of the human brain determines a situation that is threatening and stressful. In this context, an objective measure for identifying the levels of stress while considering the human brain could considerably improve the associated harmful effects. Therefore, in this paper, a machine learning (ML) framework involving electroencephalogram (EEG) signal analysis of stressed participants is proposed. In the experimental setting, stress was induced by adopting a well-known experimental paradigm based on the montreal imaging stress task. The induction of stress was validated by the task performance and subjective feedback. The proposed ML framework involved EEG feature extraction, feature selection (receiver operating characteristic curve, t-test and the Bhattacharya distance), classification (logistic regression, support vector machine and na\u00efve Bayes classifiers) and tenfold cross validation. The results showed that the proposed framework produced 94.6% accuracy for two-level identification of stress and 83.4% accuracy for multiple level identification. In conclusion, the proposed EEG-based ML framework has the potential to quantify stress objectively into multiple levels. The proposed method could help in developing a computer-aided diagnostic tool for stress detection.","keywords_author":["Absolute power","amplitude asymmetry","coherence","EEG","machine learning","mental stress levels","phase lag","relative power","support vector machine","t-test"],"keywords_other":["Mental stress","T-tests","Phase lags","Absolute power","Psychology","Cardiac arrest","amplitude asymmetry","relative power"],"max_cite":4.0,"pub_year":2017.0,"sources":"['scp']","rawkeys":["mental stress levels","coherence","eeg","machine learning","phase lags","t-test","absolute power","amplitude asymmetry","phase lag","cardiac arrest","relative power","support vector machine","psychology","t-tests","mental stress"],"tags":["recognition","mental stress levels","coherence","eeg","machine learning","phase lags","t-test","absolute power","amplitude asymmetry","relative power","cardiac arrest","mental stress"]},{"p_id":111144,"title":"Rotation-invariant object detection using Sector-ring HOG and boosted random ferns","abstract":"The histogram of oriented gradients (HOG) is widely used for image description and has proven to be very effective. In some practical applications that lack an assumption of the object's orientation, rotation-invariant detection is of vital significance. To address this problem, this paper presents a new visual feature, Sector-ring HOG (SRHOG), which is obtained by improving the gradient binning and spatial binning based on HOG. The new feature can convert planar image rotations into cyclic shifts of the final descriptor and thereby facilitate rotated object detection. After modifying boosted random ferns in SRHOG feature domain, we further propose two strategies for rotation-invariant object detection: one depends completely on the new feature's characteristic, and the other introduces an orientation estimation step. The former is more suitable to 'finding objects' and the latter can provide the higher orientation estimation accuracy. Both the use of supervised learning and working in the gradient space make our approaches effective and robust. We show these properties by thorough testing on the public Freestyle Motocross dataset and our dataset for victim detection in post-disaster rescue efforts.","keywords_author":["Rotation-invariant detection","Sector-ring HOG","HOG","Boosted random ferns (BRFs)"],"keywords_other":["DESCRIPTORS","RECOGNITION","IMAGE FEATURES"],"max_cite":0.0,"pub_year":2017.0,"sources":"['ieee', 'wos']","rawkeys":["image features","recognition","rotation-invariant detection","sector-ring hog","boosted random ferns (brfs)","hog","descriptors"],"tags":["image features","recognition","rotation-invariant detection","sector-ring hog","histogram of oriented gradients","boosted random ferns (brfs)","descriptors"]},{"p_id":4684,"title":"A performance evaluation of local descriptors","abstract":"In this paper, we compare the performance of descriptors computed for local interest regions, as, for example, extracted by the Harris-Affine detector [32]. Many different descriptors have been proposed in the literature. It is unclear which descriptors are more appropriate and how their performance depends on the interest region detector. The descriptors should be distinctive and at the same time robust to changes in viewing conditions as well as to errors of the detector. Our evaluation uses as criterion recall with respect to precision and is carried out for different image transformations. We compare shape context [3], steerable filters [12], PCA-SIFT [19], differential invariants [20], spin images [21], SIFT [26], complex filters [37], moment invariants [43], and cross-correlation for different types of interest regions. We also propose an extension of the SIFT descriptor and show that it outperforms the original method. Furthermore, we observe that the ranking of the descriptors is mostly independent of the interest region detector and that the SIFT-based descriptors perform best. Moments and steerable filters show the best performance among the low dimensional descriptors. \u00a9 2005 IEEE.","keywords_author":["Interest points","Interest regions","Invariance","Local descriptors","Matching","Recognition"],"keywords_other":["Interest regions","Local descriptors","Interest points"],"max_cite":4597.0,"pub_year":2005.0,"sources":"['scp', 'wos']","rawkeys":["recognition","interest points","local descriptors","interest regions","matching","invariance"],"tags":["recognition","interest points","local descriptors","interest regions","matching","invariance"]},{"p_id":53838,"title":"A New CNN-Based Method for Multi-Directional Car License Plate Detection","abstract":"\u00a9 2000-2011 IEEE.This paper presents a novel convolutional neural network (CNN)-based method for high-Accuracy real-Time car license plate detection. Many contemporary methods for car license plate detection are reasonably effective under the specific conditions or strong assumptions only. However, they exhibit poor performance when the assessed car license plate images have a degree of rotation, as a result of manual capture by traffic police or deviation of the camera. Therefore, we propose the a CNN-based MD-YOLO framework for multi-directional car license plate detection. Using accurate rotation angle prediction and a fast intersection-over-union evaluation strategy, our proposed method can elegantly manage rotational problems in real-Time scenarios. A series of experiments have been carried out to establish that the proposed method outperforms over other existing state-of-The-Art methods in terms of better accuracy and lower computational cost.","keywords_author":["convolutional neural network","intersection over union","License plate detection","MD-YOLO","multi-direction","License plate detection","convolutional neural network","MD-YOLO","intersection over union","multi-direction"],"keywords_other":["Degree of rotation","State-of-the-art methods","License plate detection","RECOGNITION","multi-direction","Convolutional neural network","Evaluation strategies","License plate images","Computational costs","VIDEO","TRACKING"],"max_cite":0.0,"pub_year":2018.0,"sources":"['scp', 'wos']","rawkeys":["license plate detection","state-of-the-art methods","evaluation strategies","recognition","degree of rotation","tracking","video","license plate images","intersection over union","computational costs","convolutional neural network","multi-direction","md-yolo"],"tags":["license plate detection","state-of-the-art methods","evaluation strategies","recognition","degree of rotation","tracking","video","license plate images","intersection over union","computational costs","convolutional neural network","multi-directional","md-yolo"]},{"p_id":4689,"title":"Deep learning","abstract":"\u00a9 2015 Macmillan Publishers Limited. All rights reserved.Deep learning allows computational models that are composed of multiple processing layers to learn representations of data with multiple levels of abstraction. These methods have dramatically improved the state-of-the-art in speech recognition, visual object recognition, object detection and many other domains such as drug discovery and genomics. Deep learning discovers intricate structure in large data sets by using the backpropagation algorithm to indicate how a machine should change its internal parameters that are used to compute the representation in each layer from the representation in the previous layer. Deep convolutional nets have brought about breakthroughs in processing images, video, speech and audio, whereas recurrent nets have shone light on sequential data such as text and speech.","keywords_author":null,"keywords_other":["Computers","Algorithms","Neural Networks (Computer)","NETS","ALGORITHM","NEURAL-NETWORKS","RECOGNITION","Artificial Intelligence","Language","MODELS","CORTEX","IMAGES"],"max_cite":4373.0,"pub_year":2015.0,"sources":"['wos', 'scp', 'ieee']","rawkeys":["neural-networks","algorithm","artificial intelligence","neural networks (computer)","images","recognition","language","models","nets","computers","algorithms","cortex"],"tags":["recognition","images","model","language","neural networks","machine learning","nets","algorithms","cortex"]},{"p_id":21073,"title":"What is a 'good' periocular region for recognition?","abstract":"In challenging image acquisition settings where the performance of iris recognition algorithms degrades due to poor segmentation of the iris, image blur, specular reflections, and occlusions from eye lids and eye lashes, the periocular region has been shown to offer better recognition rates. However, the definition of a periocular region is subject to interpretation. This paper investigates the question of what is the best periocular region for recognition by identifying sub-regions of the ocular image when using near-infrared (NIR) or visible light (VL) sensors. To determine the best periocular region, we test two fundamentally different algorithms on challenging periocular datasets of contrasting build on four different periocular regions. Our results indicate that system performance does not necessarily improve as the ocular region becomes larger. Rather in NIR images the eye shape is more important than the brow or cheek as the image has little to no skin texture (leading to a smaller accepted region), while in VL images the brow is very important (requiring a larger region). \u00a9 2013 IEEE.","keywords_author":["biometrics","ocular","periocular","recognition"],"keywords_other":["Periocular","Near infra red","recognition","Ocular images","ocular","Iris recognition algorithm","Specular reflections","Visible light"],"max_cite":15.0,"pub_year":2013.0,"sources":"['scp']","rawkeys":["recognition","near infra red","iris recognition algorithm","periocular","ocular images","visible light","ocular","biometrics","specular reflections"],"tags":["recognition","near infra red","iris recognition algorithm","periocular","ocular images","visible light","ocular","biometrics","specular reflections"]},{"p_id":21078,"title":"Perceptual annotation: Measuring human vision to improve computer vision","abstract":"For many problems in computer vision, human learners are considerably better than machines. Humans possess highly accurate internal recognition and learning mechanisms that are not yet understood, and they frequently have access to more extensive training data through a lifetime of unbiased experience with the visual world. We propose to use visual psychophysics to directly leverage the abilities of human subjects to build better machine learning systems. First, we use an advanced online psychometric testing platform to make new kinds of annotation data available for learning. Second, we develop a technique for harnessing these new kinds of information -'perceptual annotations' -for support vector machines. A key intuition for this approach is that while it may remain infeasible to dramatically increase the amount of data and high-quality labels available for the training of a given system, measuring the exemplar-by-exemplar difficulty and pattern of errors of human annotators can provide important information for regularizing the solution of the system at hand. A case study for the problem face detection demonstrates that this approach yields state-of-the-art results on the challenging FDDB data set. \u00a9 1979-2012 IEEE.","keywords_author":["citizen science","face detection","Machine learning","psychology","psychometrics","psychophysics","regularization","support vector machines","visual recognition"],"keywords_other":["Pattern Recognition, Visual","Male","Visual recognition","psychophysics","Humans","Pattern Recognition, Automated","Female","Citizen science","Databases, Factual","Machine Learning","psychometrics","Support Vector Machine","Face","Psychophysics","psychology","Data Curation","regularization"],"max_cite":15.0,"pub_year":2014.0,"sources":"['scp', 'wos']","rawkeys":["visual recognition","citizen science","psychophysics","automated","databases","psychometrics","support vector machine","machine learning","face detection","visual","psychology","regularization","data curation","humans","face","factual","male","support vector machines","pattern recognition","female"],"tags":["visual recognition","citizen science","recognition","automated","databases","male","psychophysics","machine learning","visualization","humans","face detection","psychometrics","pattern recognition","face","female","factual","regularization","data curation"]},{"p_id":45676,"title":"Comparative analysis of raw images and meta feature based Urdu OCR using CNN and LSTM","abstract":"\u00a9 2015 The Science and Information (SAI) Organization Limited.Urdu language uses cursive script which results in connected characters constituting ligatures. For identifying characters within ligatures of different scales (font sizes), Convolution Neural Network (CNN) and Long Short Term Memory (LSTM) Network are used. Both network models are trained on formerly extracted ligature thickness graphs, from which models extract Meta features. These thickness graphs provide consistent information across different font sizes. LSTM and CNN are also trained on raw images to compare performance on both forms of inputs. For this research, two corpora, i.e. Urdu Printed Text Images (UPTI) and Centre for Language Engineering (CLE) Text Images are used. Overall performance of networks ranges between 90% and 99.8%. Average accuracy on Meta features is 98.08% while using raw images, 97.07% average accuracy is achieved.","keywords_author":["Convolution Neural Network (CNN)","Deep learning","Ligature","Long Short Term Memory (LSTM)","OCR","Scale invariance","Long Short Term Memory (LSTM)","Convolution Neural Network (CNN)","OCR","scale invariance","deep learning","ligature"],"keywords_other":["RECOGNITION","RECURRENT NEURAL-NETWORKS"],"max_cite":0.0,"pub_year":2018.0,"sources":"['scp', 'wos']","rawkeys":["long short term memory (lstm)","recognition","recurrent neural-networks","ocr","deep learning","ligature","convolution neural network (cnn)","scale invariance"],"tags":["recognition","neural networks","long short-term memory","machine learning","ligature","convolutional neural network","optical character recognition","scale invariant"]},{"p_id":53873,"title":"Robust 3D Hand Pose Estimation from Single Depth Images Using Multi-View CNNs","abstract":"\u00a9 1992-2012 IEEE. Articulated hand pose estimation is one of core technologies in human-computer interaction. Despite the recent progress, most existing methods still cannot achieve satisfactory performance, partly due to the difficulty of the embedded high-dimensional nonlinear regression problem. Most existing data-driven methods directly regress 3D hand pose from 2D depth image, which cannot fully utilize the depth information. In this paper, we propose a novel multi-view convolutional neural network (CNN)-based approach for 3D hand pose estimation. To better exploit 3D information in the depth image, we project the point cloud generated from the query depth image onto multiple views of two projection settings and integrate them for more robust estimation. Multi-view CNNs are trained to learn the mapping from projected images to heat-maps, which reflect probability distributions of joints on each view. These multi-view heat-maps are then fused to estimate the optimal 3D hand pose with learned pose priors, and the unreliable information in multi-view heat-maps is suppressed using a view selection method. Experimental results show that the proposed method is superior to the state-of-the-art methods on two challenging data sets. Furthermore, a cross-data set experiment also validates that our proposed approach has good generalization ability.","keywords_author":["3D hand pose estimation","convolutional neural networks","multi-view CNNs","3D hand pose estimation","convolutional neural networks","multi-view CNNs"],"keywords_other":["Heating system","Multi-views","Two-dimensional displays","REGRESSION FORESTS","RECOGNITION","3D hand pose estimations","Convolutional neural network","Solid model","Pose estimation","TRACKING"],"max_cite":0.0,"pub_year":2018.0,"sources":"['ieee', 'scp', 'wos']","rawkeys":["pose estimation","heating system","3d hand pose estimations","multi-view cnns","convolutional neural networks","recognition","3d hand pose estimation","tracking","two-dimensional displays","regression forests","convolutional neural network","solid model","multi-views"],"tags":["pose estimation","heating system","3d hand pose estimations","multi-view cnns","recognition","tracking","solid modeling","regression forests","convolutional neural network","two dimensional displays","multi-views"]},{"p_id":45721,"title":"Continuous dropout strategy for deep learning network","abstract":"\u00a9 Springer Nature Singapore Pte Ltd. 2018. Recent years, more and more attractive results are achieved by deep learning. However, large numbers of parameters generally cause overfitting in the training stage. Hinton [17] proposed dropout to address this problem in 2012. During our research, we find that there is a balance between generalization and accuracy. Dropout can increase generalization, decrease overfitting and thus increase accuracy by using appropriate dropout rate. However, too high generalization may lead to relatively low accuracy. So, we propose a continuous dropout rate strategy that we gradually decrease the dropout rate during training instead of a constant one. In this way, we can obtain high generalization in the beginning and high accuracy in the end. Experiment results show that our proposed strategy can achieve higher accuracy compared to the traditional dropout.","keywords_author":["Deep learning","Dropout","Generalization","Overfitting"],"keywords_other":["Overfitting","Learning network","Dropout","High-accuracy","Generalization"],"max_cite":0.0,"pub_year":2018.0,"sources":"['scp', 'ieee']","rawkeys":["dropout","deep learning","learning network","generalization","high-accuracy","overfitting"],"tags":["dropout","recognition","learning network","machine learning","high-accuracy","overfitting"]},{"p_id":45725,"title":"Improving the histogram of oriented gradient feature for threat detection in ground penetrating radar by implementing it as a trainable convolutional neural network","abstract":"\u00a9 2018 SPIE. A large number of algorithms have been proposed for automatic buried threat detection (BTD) in ground penetrating radar (GPR) data. Convolutional neural networks (CNNs) have recently achieved groundbreaking results on many recognition tasks. This success is due, in part, to their ability to automatically infer effective data representations (i.e., features) using training data. This capability however results in a high capacity model (i.e., many free parameters) that is difficult to train, and more prone to overfitting, than models employing hand-crafted feature designs. This drawback is pronounced when training data is relatively scarce, as is the case with GPR BTD. In this work we propose to combine the relative advantages of hand-crafted features, and CNNs, by constructing CNN architectures that closely emulate successful hand-crafted feature designs for GPR BTD. This makes it possible to apply supervised training to traditional hand-crafted features, allowing them to adapt to the unique characteristics of the GPR BTD problem. Simultaneously, this approach yields a much lower capacity CNN model that incorporates substantial prior research knowledge, making the model much easier to train. We demonstrate the feasibility and effectiveness of this approach by designing a \"neural\" implementation of the popular histogram of oriented gradient (HOG) feature. The resulting neural HOG (NHOG) implementation is much smaller and easier to train than standard CNN architectures, and achieves superior detection performance compared to the un-trained HOG feature. In theory, neural implementations can be developed for many existing successful GPR BTD algorithms, potentially yielding similar benefits.","keywords_author":["buried threat detection","convolutional neural networks","deep learning","generalization","ground penetrating radar","histogram of oriented gradients","regularization"],"keywords_other":["Threat detection","Histogram of oriented gradients","generalization","Convolutional neural network","Ground Penetrating Radar","regularization"],"max_cite":0.0,"pub_year":2018.0,"sources":"['scp']","rawkeys":["threat detection","convolutional neural networks","buried threat detection","deep learning","histogram of oriented gradients","generalization","convolutional neural network","ground penetrating radar","regularization"],"tags":["threat detection","recognition","buried threat detection","machine learning","histogram of oriented gradients","convolutional neural network","gaussian process regression","regularization"]},{"p_id":45726,"title":"Deep temporal multimodal fusion for medical procedure monitoring using wearable sensors","abstract":"\u00a9 2017 IEEE. Personal use is permitted. Processmonitoring and verification have a wide range of uses in the medical and healthcare fields. Currently, such tasks are often carried out by a trained specialist, which makes them expensive, inefficient, and time-consuming. Recent advances in automated video- and multimodal-data-based action and activity recognition have made it possible to reduce the extent of manual intervention required to effectively carry out process supervision tasks. In this paper, we propose algorithms for automated egocentric human action and activity recognition frommultimodal data, with a target application of monitoring and assisting a user perform a multistep medical procedure. We propose a supervised deep multimodal fusion framework that relies on concurrent processing of motion data acquired with wearable sensors and video data acquired with an egocentric or body-mounted camera. We demonstrate the effectiveness of the algorithm on a public multimodal dataset and conclude that automated process monitoring via the use of multiple heterogeneous sensors is a viable alternative to its manual counterpart. Furthermore, we demonstrate that the application of previously proposed adaptive sampling schemes to the video processing branch of the multimodal framework results in significant performance improvements.","keywords_author":["Action and activity recognition","Deep learning","Deep temporal fusion","Egocentric vision","Hand localization","Medical procedures","Multimodal fusion","Wearable sensors","Action and activity recognition","deep learning","deep temporal fusion","egocentric vision","hand localization","medical procedures","multimodal fusion","wearable sensors"],"keywords_other":["EGOCENTRIC VIDEO","Biomedical monitoring","Activity recognition","Multi-modal fusion","Medical services","HOSPITAL PHARMACY","Medical procedures","RECOGNITION","OBJECTS","hand localization"],"max_cite":0.0,"pub_year":2018.0,"sources":"['ieee', 'scp', 'wos']","rawkeys":["biomedical monitoring","medical procedures","recognition","activity recognition","action and activity recognition","wearable sensors","deep learning","multi-modal fusion","objects","egocentric vision","multimodal fusion","deep temporal fusion","egocentric video","medical services","hospital pharmacy","hand localization"],"tags":["biomedical monitoring","medical procedures","recognition","activity recognition","action and activity recognition","wearable sensors","multi-modal fusion","machine learning","objects","egocentric vision","multimodal fusion","deep temporal fusion","egocentric video","medical services","hospital pharmacy","hand localization"]},{"p_id":4768,"title":"Basic concepts and taxonomy of dependable and secure computing","abstract":"This paper gives the main definitions relating to dependability, a generic concept including as special case such attributes as reliability, availability, safety, integrity, maintainability, etc. Security brings in concerns for confidentiality, in addition to availability and integrity. Basic definitions are given first. They are then commented upon, and supplemented by additional definitions, which address the threats to dependability and security (faults, errors, failures), their attributes, and the means for their achievement (fault prevention, fault tolerance, fault removal, fault forecasting). The aim is to explicate a set of general concepts, of relevance across a wide range of situations and, therefore, helping communication and cooperation among a number of scientific and technical communities, including ones that are concentrating on particular types of system, of system failures, or of causes of system failures.","keywords_author":["Attacks","Dependability","Errors","Failures","Fault forecasting","Fault removal","Fault tolerance","Faults","Security","Trust","Vulnerabilities"],"keywords_other":["Trust","Vulnerabilities","Attacks","Security","Faults","Fault removal","Failures","Fault forecasting","Dependability"],"max_cite":2572.0,"pub_year":2004.0,"sources":"['scp']","rawkeys":["errors","faults","fault removal","trust","security","fault forecasting","attacks","dependability","vulnerabilities","fault tolerance","failures"],"tags":["diagnosis","error","recognition","fault removal","trust","security","fault forecasting","vulnerability","attacks","failure","fault tolerance"]},{"p_id":111268,"title":"A 32 x 32 Pixel Convolution Processor Chip for Address Event Vision Sensors With 155 ns Event Latency and 20 Meps Throughput","abstract":"This paper describes a convolution chip for event-driven vision sensing and processing systems. As opposed to conventional frame-constraint vision systems, in event-driven vision there is no need for frames. In frame-free event-based vision, information is represented by a continuous flow of self-timed asynchronous events. Such events can be processed on the fly by event-based convolution chips, providing at their output a continuous event flow representing the 2-D filtered version of the input flow. In this paper we present a 32 x 32 pixel 2-D convolution event processor whose kernel can have arbitrary shape and size up to 32 x 32. Arrays of such chips can be assembled to process larger pixel arrays. Event latency between input and output event flows can be as low as 155 ns. Input event throughput can reach 20 Meps (mega events per second), and output peak event rate can reach 45 Meps. The chip can be configured to discriminate between two simulated propeller-like shapes rotating simultaneously in the field of view at a speed as high as 9400 rps (revolutions per second). Achieving this with a frame-constraint system would require a sensing and processing capability of about 100 K frames per second. The prototype chip has been built in 0.35 mu m CMOS technology, occupies 4.3 x 5.4 mm(2) and consumes a peak power of 200 mW at maximum kernel size at maximum input event rate.","keywords_author":["Address event representation","convolution processing","event-based processing","feature extraction","frame-less vision","neuromorphic circuits and systems","object recognition","vision processing"],"keywords_other":["NEUROMORPHIC CHIPS","CONTRAST","OPTIC-NERVE SIGNALS","SYSTEMS","NEURAL-NETWORKS","RECOGNITION","FACE DETECTION","MODELS","IMAGE SENSOR","RANGE VISION"],"max_cite":24.0,"pub_year":2011.0,"sources":"['wos']","rawkeys":["neural-networks","convolution processing","recognition","neuromorphic circuits and systems","frame-less vision","neuromorphic chips","optic-nerve signals","image sensor","object recognition","face detection","address event representation","vision processing","contrast","feature extraction","models","systems","range vision","event-based processing"],"tags":["convolution processing","image sensors","model","frame-less vision","event based processing","neural networks","neuromorphic chips","neuromorphic circuits and systems","optic-nerve signals","recognition","object recognition","face detection","system","vision processing","contrast","feature extraction","address-event representation (aer)","range vision"]},{"p_id":111272,"title":"On real-time AER 2-D convolutions hardware for neuromorphic spike-based cortical processing","abstract":"In this paper, a chip that performs real-time image convolutions with programmable kernels of arbitrary shape is presented. The chip is a first experimental prototype of reduced size to validate the implemented circuits and system level techniques. The convolution processing is based on the address-event-representation (AER) technique, which is a spike-based biologically inspired image and video representation technique that favors communication bandwidth for pixels with more information. As a first test prototype, a pixel array of 16 x 16 has been implemented with programmable kernel size of up to 16 x 16. The chip has been fabricated in a standard 0.35-mu m complimentary metal-oxide-semiconductor (CMOS) process. The technique also allows to process larger size images by assembling 2-D arrays of such chips. Pixel operation exploits low-power mixed analog-digital circuit techniques. Because of the low currents involved (down to nanoamperes or even picoamperes), an important amount of pixel area is devoted to mismatch calibration. The rest of the chip uses digital circuit techniques, both synchronous and asynchronous. The fabricated chip has been thoroughly tested, both at the pixel level and at the system level. Specific computer interfaces have been developed for generating AER streams from conventional computers and feeding them as inputs to the convolution chip, and for grabbing AER streams coming out of the convolution chip and storing and analyzing them on computers. Extensive experimental results are provided. At the end of this paper, we provide discussions and results on scaling up the approach for larger pixel arrays and multilayer cortical AER systems.","keywords_author":["address-event representation (AER)","analog circuits","asynchronous circuits","bioinspired systems","cortical layer processing","image convolutions","image processing","low power circuits","mixed-signal circuits","spike-based processing"],"keywords_other":["CIRCUITS","REPRESENTATION","OPTIC-NERVE SIGNALS","NEOCOGNITRON","NEURAL-NETWORKS","RECOGNITION","RETINA","VISION SYSTEMS","CALIBRATION","CHIP"],"max_cite":47.0,"pub_year":2008.0,"sources":"['wos']","rawkeys":["spike-based processing","asynchronous circuits","neural-networks","low power circuits","analog circuits","vision systems","image convolutions","recognition","neocognitron","retina","address-event representation (aer)","bioinspired systems","calibration","optic-nerve signals","chip","image processing","representation","cortical layer processing","circuits","mixed-signal circuits"],"tags":["spike-based processing","asynchronous circuits","low power circuits","analog circuits","vision systems","image convolutions","recognition","neocognitron","neural networks","retina","address-event representation (aer)","bioinspired systems","calibration","optic-nerve signals","chip","image processing","representation","cortical layer processing","circuits","mixed-signal circuits"]},{"p_id":12969,"title":"The Affective Computing Approach to Affect Measurement","abstract":"Affective computing (AC) adopts a computational approach to study affect. We highlight the AC approach towards automated affect measures that jointly model machine-readable physiological\/behavioral signals with affect estimates as reported by humans or experimentally elicited. We describe the conceptual and computational foundations of the approach followed by two case studies: one on discrimination between genuine and faked expressions of pain in the lab, and the second on measuring nonbasic affect in the wild. We discuss applications of the measures, analyze measurement accuracy and generalizability, and highlight advances afforded by computational tipping points, such as big data, wearable sensing, crowdsourcing, and deep learning. We conclude by advocating for increasing synergies between AC and affective science and offer suggestions toward that direction.","keywords_author":["affect detection","affective measurement","multimodal sensing","supervised classification"],"keywords_other":["METAANALYSIS","UNIVERSALITY","CLASSIFICATION","EXPRESSION","CULTURAL SPECIFICITY","RESPONSES","RECOGNITION","MODELS","EMOTION"],"max_cite":0.0,"pub_year":2018.0,"sources":"['wos']","rawkeys":["supervised classification","cultural specificity","recognition","emotion","responses","universality","metaanalysis","affect detection","affective measurement","classification","expression","models","multimodal sensing"],"tags":["supervised classification","cultural specificity","recognition","model","emotion","responses","metaanalysis","affect detection","affective measurement","classification","expression","multimodal sensing","university"]},{"p_id":111277,"title":"Uncertainty analysis of a neural network used for fatigue lifetime prediction","abstract":"The application of interval set techniques to the quantification of uncertainty in a neural network regression model of fatigue lifetime is considered. Bayesian evidence training was implemented to train a series of multi-layer perceptron networks on experimental fatigue life measurements in glass fibre composite sandwich materials. A set of independent measurements conducted 2 months after the training session, and at intermediate fatigue loading levels, was used to provide a rigorous test of the generalisation capacity of the networks. The robustness of the networks to uncertainty in the input data was investigated using an interval-based technique. It is demonstrated that the interval approach allowed for an alternative to probabilistic-based confidence bounds of prediction accuracy. In addition, the technique provided an alternative network selection tool, and also allowed for an alternative to estimating the lifetime prediction error that was found to be a significant improvement over the Bayesian-derived estimate of confidence bound. (C) 2007 Elsevier Ltd. All rights reserved.","keywords_author":["lifetime prediction","fatigue","neural networks","uncertainty","interval analysis"],"keywords_other":["CLASSIFICATION","RELIABILITY","METHODOLOGY","RECOGNITION","MODELS","COMPOSITE-MATERIALS"],"max_cite":24.0,"pub_year":2008.0,"sources":"['ieee', 'wos']","rawkeys":["interval analysis","recognition","composite-materials","neural networks","uncertainty","methodology","reliability","lifetime prediction","classification","fatigue","models"],"tags":["interval analysis","recognition","model","neural networks","uncertainty","composite materials","methodology","reliability","lifetime prediction","classification","fatigue"]},{"p_id":12975,"title":"Recent developments in human gait research: parameters, approaches, applications, machine learning techniques, datasets and challenges","abstract":"Human gait provides a way of locomotion by combined efforts of the brain, nerves, and muscles. Conventionally, the human gait has been considered subjectively through visual observations but now with advanced technology, human gait analysis can be done objectively and empirically for the better quality of life. In this paper, the literature of the past survey on gait analysis has been discussed. This is followed by discussion on gait analysis methods. Vision-based human motion analysis has the potential to provide an inexpensive, non-obtrusive solution for the estimation of body poses. Data parameters for gait analysis have been discussed followed by preprocessing steps. Then the implemented machine learning techniques have been discussed in detail. The objective of this survey paper is to present a comprehensive analysis of contemporary gait analysis. This paper presents a framework (parameters, techniques, available database, machine learning techniques, etc.) for researchers in identifying the infertile areas of gait analysis. The authors expect that the overview presented in this paper will help advance the research in the field of gait analysis. Introduction to basic taxonomies of human gait is presented. Applications in clinical diagnosis, geriatric care, sports, biometrics, rehabilitation, and industrial area are summarized separately. Available machine learning techniques are also presented with available datasets for gait analysis. Future prospective in gait analysis are discussed in the end.","keywords_author":["Human gait analysis","Machine learning techniques","Gait Datasets","Gait Approaches","Application","Survey"],"keywords_other":["ARTIFICIAL-INTELLIGENCE","WEARABLE SENSORS","CLUSTER-ANALYSIS","LOWER-EXTREMITY","HUMAN MOTION ANALYSIS","HUMAN LOCOMOTION","RECOGNITION","PERFORMANCE EVALUATION","NEURAL-NETWORK","PATTERNS"],"max_cite":0.0,"pub_year":2018.0,"sources":"['wos']","rawkeys":["application","recognition","survey","wearable sensors","gait datasets","machine learning techniques","patterns","gait approaches","human locomotion","cluster-analysis","neural-network","performance evaluation","artificial-intelligence","human motion analysis","lower-extremity","human gait analysis"],"tags":["recognition","wearable sensors","survey","neural networks","gait datasets","machine learning techniques","machine learning","patterns","cluster analysis","gait approaches","lower extremity","human locomotions","applications","performance evaluation","human motion analysis","human gait analysis"]},{"p_id":111281,"title":"Training higher order Gaussian synapses","abstract":"In this article we present an algorithm that permits training networks that include gaussian type higher order synapses. This algorithm is an extension of the classical backpropagation algorithm. higher order synapses permit carrying out tasks using simpler networks than traditionally employed. The key to this simplicity is in the structure of the synapses: a gaussian with three trainable parameters. The fact that it is a function and consequently presents a variable output depending on its inputs and that it possesses more than one trainable parameter that allows it to implement non linear processing functions on its inputs, endows the networks with a large capacity for learning and generalization. We present two examples where these capacities are shown. The first one is a target tracking module for a the visual system of a real robot and the second one is an image classification system working on real images.","keywords_author":null,"keywords_other":["NEURAL NETWORKS","RECOGNITION"],"max_cite":14.0,"pub_year":1999.0,"sources":"['wos']","rawkeys":["neural networks","recognition"],"tags":["neural networks","recognition"]},{"p_id":12989,"title":"Neural Semantic Personalized Ranking for item cold-start recommendation","abstract":"Recommender systems help users deal with information overload and enjoy a personalized experience on the Web. One of the main challenges in these systems is the item cold-start problem which is very common in practice since modern online platforms have thousands of new items published every day. Furthermore, in many real-world scenarios, the item recommendation tasks are based on users' implicit preference feedback such as whether a user has interacted with an item. To address the above challenges, we propose a probabilistic modeling approach called Neural Semantic Personalized Ranking (NSPR) to unify the strengths of deep neural network and pairwise learning. Specifically, NSPR tightly couples a latent factor model with a deep neural network to learn a robust feature representation from both implicit feedback and item content, consequently allowing our model to generalize to unseen items. We demonstrate NSPR's versatility to integrate various pairwise probability functions and propose two variants based on the Logistic and Probit functions. We conduct a comprehensive set of experiments on two real-world public datasets and demonstrate that NSPR significantly outperforms the state-of-the-art baselines.","keywords_author":["Recommender systems","Deep neural network","Implicit feedback","Pairwise learning"],"keywords_other":["RECOGNITION"],"max_cite":3.0,"pub_year":2017.0,"sources":"['ieee', 'wos']","rawkeys":["pairwise learning","recognition","deep neural network","recommender systems","implicit feedback"],"tags":["pairwise learning","recognition","recommender systems","convolutional neural network","implicit feedback"]},{"p_id":45759,"title":"A multiaxial data-based machine learning model for exercise motion recognition","abstract":"\u00a9 2018 SERSC Australia. Wearable devices using only accelerometers cannot correctly recognize the detailed motions of exercise performed by patients and cannot count the correct number of repetitions. Therefore, in this paper, we suggest a method to improve the recognition accuracy of detailed motions using machine learning technology simultaneously using an accelerometer, and magnetometer in a wearable device. We particularly improved the recognition accuracy of detailed exercise motions through machine learning using data measured on 9 axes. To verify its effectiveness, we conducted experiments for the recognition of each motion when the patients performed exercise consisting of nine motions with the 9-axis accelerometer attached to their wrists. Accuracy of 81.8% was recorded when only acceleration data was used, whereas accuracy of 91% was recorded in the 9- axis data obtained using three sensors, thereby improving accuracy by 9.2%.","keywords_author":["Exercise","IoT","Machine learning","Motion sensor","Recognition"],"keywords_other":null,"max_cite":0.0,"pub_year":2018.0,"sources":"['scp']","rawkeys":["recognition","machine learning","motion sensor","iot","exercise"],"tags":["recognition","motion sensors","machine learning","exercise","internet of things (iot)"]},{"p_id":12991,"title":"Regularization of neural network model with distance metric learning for i-vector based spoken language identification","abstract":"The i-vector representation and modeling technique has been successfully applied in spoken language identification (SLI). The advantage of using the i-vector representation is that any speech utterance with a variable duration length can be represented as a fixed length vector. In modeling, a discriminative transform or classifier must be applied to emphasize the variations correlated to language identity since the i-vector representation encodes several types of the acoustic variations (e.g., speaker variation, transmission channel variation, etc.). Owing to the strong nonlinear discriminative power, the neural network model has been directly used to learn the mapping function between the i-vector representation and the language identity labels. In most studies, only the point-wise feature-label information is fed to the model for parameter learning that may result in model overfitting, particularly when with limited training data. In this study, we propose to integrate pair-wise distance metric learning as the regularization of model parameter optimization. In the representation space of nonlinear transforms in the hidden layers, a distance metric learning is explicitly designed to minimize the pair-wise intra-class variation and maximize the inter-class variation. Using the pair-wise distance metric learning, the i-vectors are transformed to a new feature space, wherein they are much more discriminative for samples belonging to different languages while being much more similar for samples belonging to the same language. We tested the algorithm on an SLI task, and obtained promising results, which outperformed conventional regularization methods. (C) 2017 Elsevier Ltd. All rights reserved.","keywords_author":["Neural network model","Cross entropy","Pair-wise distance metric learning","Spoken language identification"],"keywords_other":["RECOGNITION","VERIFICATION"],"max_cite":0.0,"pub_year":2017.0,"sources":"['ieee', 'wos']","rawkeys":["pair-wise distance metric learning","cross entropy","recognition","spoken language identification","verification","neural network model"],"tags":["pair-wise distance metric learning","cross entropy","recognition","spoken language identification","verification","neural network model"]},{"p_id":12998,"title":"Deep vs. shallow networks: An approximation theory perspective","abstract":"The paper briefly reviews several recent results on hierarchical architectures for learning from examples, that may formally explain the conditions under which Deep Convolutional Neural Networks perform much better in function approximation problems than shallow, one-hidden layer architectures. The paper announces new results for a non-smooth activation function - the ReLU function - used in present-day neural networks, as well as for the Gaussian networks. We propose a new definition of relative dimension to encapsulate different notions of sparsity of a function class that can possibly be exploited by deep networks but not by shallow ones to drastically reduce the complexity required for approximation and learning.","keywords_author":["Deep and shallow networks","Gaussian networks","ReLU networks","blessed representation"],"keywords_other":["NEURAL-NETWORKS","RECOGNITION"],"max_cite":6.0,"pub_year":2016.0,"sources":"['wos']","rawkeys":["neural-networks","recognition","deep and shallow networks","blessed representation","relu networks","gaussian networks"],"tags":["recognition","neural networks","deep and shallow networks","blessed representation","relu networks","gaussian networks"]},{"p_id":29387,"title":"MapReduce based distributed learning algorithm for Restricted Boltzmann Machine","abstract":"\u00a9 2016 Elsevier B.V.Deep learning is recently regarded as the closest artificial intelligence model to human brain. It is about learning multiple levels of representation and abstraction that help to make sense of data such as images, sound, and text. One deep model often consists of a hierarchical architecture that has the capability to model super non-linear and stochastic problems. Restricted Boltzmann Machine (RBM) is the main constructing block of current deep networks, as most of deep architectures are built with it. Based on MapReduce framework and Hadoop distributed file system, this paper proposes a distributed algorithm for training the RBM model. Its implementation and performance are evaluated on Big Data platform-Hadoop. The main contribution of the new learning algorithm is that it solves the scalability problem that limits the development of deep learning. The intelligence growing process of human brain requires learning from Big Data. The distributed learning mechanism for RBM makes it possible to abstract sophisticated and informative features from Big Data to achieve high-level intelligence. The evaluations of the proposed learning algorithm are carried out on image inpainting and classification problems based on the BAS dataset and MNIST hand-written digits dataset.","keywords_author":["Big data","Deep learning","MapReduce","Restricted boltzmann machine","Deep learning","Restricted Boltzmann Machine","Big data","MapReduce"],"keywords_other":["Deep learning","Map-reduce","Restricted boltzmann machine","Distributed learning algorithms","DEEP","Scalability problems","Hadoop distributed file systems","Distributed learning","NEURAL-NETWORKS","RECOGNITION","Hierarchical architectures"],"max_cite":4.0,"pub_year":2016.0,"sources":"['ieee', 'scp', 'wos']","rawkeys":["neural-networks","distributed learning algorithms","recognition","big data","deep learning","deep","hadoop distributed file systems","mapreduce","hierarchical architectures","map-reduce","distributed learning","scalability problems","restricted boltzmann machine"],"tags":["distributed learning algorithms","recognition","big data","neural networks","deep","machine learning","hierarchical architectures","map-reduce","hdfs","distributed learning","scalability problems","restricted boltzmann machine"]},{"p_id":86734,"title":"DeepBag: Recognizing Handbag Models","abstract":"In this paper, we address the problem of branded handbag recognition. It is a challenging problem due to the non-rigid deformation, illumination changes, and inter-class similarity. We propose a novel framework based on deep convolutional neural network (CNN). Concretely, we propose a new CNN model, called feature selective joint classification-regression CNN (FSCR-CNN). Its advantages lie in two folds: 1) it alleviates the illumination changes by a feature selection strategy to focus on the color-nondiscriminative features in the network learning, and 2) rather than only targeting on the hard label (i.e., the handbag model), it also incorporates a soft label (i.e., a distribution measuring the similarity between the ground truth model and all the models to be trained) to construct the loss function for training CNN, which leads to a better classifier for handbags with large inter-class similarity. We evaluate the performance of our framework on a newly built branded handbag dataset. The results show that it performs favorably for recognizing handbags with 94.48% in accuracy. We also apply the proposed FSCR-CNN model in recognizing other fine-grained objects with state-of-the-art CNN architectures, which is able to achieve over 5% improvement in accuracy.","keywords_author":["Convolutional neural networks","feature selection","handbag recognition","soft label"],"keywords_other":["GRADIENTS","FEATURES","NETWORKS","CLASSIFICATION","RETRIEVAL","RECOGNITION","SCALE","CATEGORIZATION"],"max_cite":4.0,"pub_year":2015.0,"sources":"['ieee', 'wos']","rawkeys":["recognition","convolutional neural networks","features","soft label","categorization","feature selection","gradients","classification","handbag recognition","networks","retrieval","scale"],"tags":["recognition","features","soft label","categorization","feature selection","gradient","classification","convolutional neural network","handbag recognition","networks","retrieval","scale"]},{"p_id":13017,"title":"Artificial neural network-based internal leakage fault detection for hydraulic actuators: An experimental investigation","abstract":"Internal leakage is a typical fault in the hydraulic systems, which may be caused by seal damage, and result in deteriorated performance of the system. To study this issue, this article carries out an experimental investigation of artificial neural network-based detection method for internal leakage fault. A period of pressure signal at one chamber of the actuator was taken in response to sinusoidal-like inputs for the closed-loop controlled system as a basic signal unit, and totally, 1000 periodic signal units are obtained from the experiments. The above experimental measurements are repetitively implemented with 11 different active exerted internal leakage levels, that is, totally 11,000 basic signal units are obtained. For signal processing, the pressure signal in the operation condition without active exerted leakage is chosen to generate a baseline with suitable pre-proceed, and the relative values of the other basic signal units (D-value between the baseline and other original signals) act as the global samples of the following artificial neural networks, traditional back propagation neural network, deep neural network, convolution neural network and auto-encoder neural network, separately; 8800 samples by random extraction as train samples to train the above neural networks and the other samples different from the train samples act as test samples to examine the detection accuracy of the proposed method. It is shown that the deep neural network with five layers can obtain a best detection accuracy (92.23%) of the above-mentioned neural networks. In addition, the methods based on wavelet transform and Hilbert-Huang transform are also applied, and a comparison of these methods is provided at last. From the comparison, it is shown that the proposed detection method obtains a good result without a need to model the internal leakage or a complicated signal processing.","keywords_author":["Internal leakage","artificial neural networks","hydraulic actuators","fault detection"],"keywords_other":["DIAGNOSIS","WAVELET-BASED APPROACH","SYSTEMS","DISSIPATIVITY ANALYSIS","IDENTIFICATION","RECOGNITION","ELECTROHYDRAULIC CYLINDER"],"max_cite":0.0,"pub_year":2017.0,"sources":"['ieee', 'wos']","rawkeys":["diagnosis","internal leakage","wavelet-based approach","identification","recognition","electrohydraulic cylinder","fault detection","dissipativity analysis","artificial neural networks","hydraulic actuators","systems"],"tags":["diagnosis","internal leakage","wavelet-based approach","identification","recognition","electrohydraulic cylinder","neural networks","system","fault detection","dissipativity analysis","hydraulic actuators"]},{"p_id":4863,"title":"A survey of advances in vision-based human motion capture and analysis","abstract":"This survey reviews advances in human motion capture and analysis from 2000 to 2006, following a previous survey of papers up to 2000 [T.B. Moeslund, E. Granum, A survey of computer vision-based human motion capture, Computer Vision and Image Understanding, 81(3) (2001) 231-268.]. Human motion capture continues to be an increasingly active research area in computer vision with over 350 publications over this period. A number of significant research advances are identified together with novel methodologies for automatic initialization, tracking, pose estimation, and movement recognition. Recent research has addressed reliable tracking and pose estimation in natural scenes. Progress has also been made towards automatic understanding of human actions and behavior. This survey reviews recent trends in video-based human capture and analysis, as well as discussing open problems for future research to achieve automatic visual analysis of human movement. \u00a9 2006 Elsevier Inc. All rights reserved.","keywords_author":["Human motion","Initialization","Pose estimation","Recognition","Review","Tracking"],"keywords_other":["Human motion","Pose estimation","Initialization","Visual analysis"],"max_cite":1618.0,"pub_year":2006.0,"sources":"['ieee', 'scp', 'wos']","rawkeys":["pose estimation","recognition","visual analysis","human motion","initialization","tracking","review"],"tags":["pose estimation","recognition","visual analysis","human motions","initialization","tracking","review"]},{"p_id":45825,"title":"Face Alignment with Deep Regression","abstract":"\u00a9 2016 IEEE. In this paper, we present a deep regression approach for face alignment. The deep regressor is a neural network that consists of a global layer and multistage local layers. The global layer estimates the initial face shape from the whole image, while the following local layers iteratively update the shape with local image observations. Combining standard derivations and numerical approximations, we make all layers able to backpropagate error differentials, so that we can apply the standard backpropagation to jointly learn the parameters from all layers. We show that the resulting deep regressor gradually and evenly approaches the true facial landmarks stage by stage, avoiding the tendency that often occurs in the cascaded regression methods and deteriorates the overall performance: yielding early stage regressors with high alignment accuracy gains but later stage regressors with low alignment accuracy gains. Experimental results on standard benchmarks demonstrate that our approach brings significant improvements over previous cascaded regression algorithms.","keywords_author":["Backpropagation","cascaded regression","deep learning","face alignment","Backpropagation","cascaded regression","deep learning","face alignment"],"keywords_other":["Face shapes","Face alignment","FEATURES","REPRESENTATION","Facial landmark","Numerical approximations","ACTIVE APPEARANCE MODELS","RECOGNITION","Regression algorithms","Alignment accuracy","Regression method"],"max_cite":0.0,"pub_year":2018.0,"sources":"['ieee', 'scp', 'wos']","rawkeys":["active appearance models","facial landmark","face alignment","recognition","features","deep learning","face shapes","regression method","representation","numerical approximations","alignment accuracy","cascaded regression","regression algorithms","backpropagation"],"tags":["active appearance models","facial landmark","face alignment","recognition","features","face shapes","regression method","machine learning","representation","numerical approximations","alignment accuracy","cascaded regression","regression algorithms","backpropagation"]},{"p_id":21257,"title":"Automatic sleep stage classification based on sparse deep belief net and combination of multiple classifiers","abstract":"\u00a9 The Author(s) 2015.This paper presents an automatic sleep stage method combining a sparse deep belief net and combination of multiple classifiers for electroencephalogram, electrooculogram and electromyogram. The sparse deep belief net was applied to extract features from these signals automatically, and the combination of multiple classifiers, utilizing the extracted features, assigned each 30-s epoch to one of the five possible sleep stages. More importantly, we proposed a new voting principle based on classification entropy to enhance the classification performance further by harnessing the complementary information provided by the individual classifier. Differently from existing methods, our method used unsupervised feature learning to extract features automatically from raw sleep data and classification based on the learned features. The results of automatic and manual scorings were compared on an epoch-by-epoch basis. The accuracies for wake, S1, S2, SWS and REM were 98.49%, 80.05%, 91.2%, 98.22% and 95.31%, respectively, and the total accuracy of sleep stage was 91.31%. The results demonstrated that the sparse deep belief net was an efficient feature extraction method for sleep data, and the combination of multiple classifiers based on classification entropy performed well on sleep stages.","keywords_author":["Combination of multiple classifiers","feature extraction","sleep stage","sparse deep belief net","Combination of multiple classifiers","feature extraction","sleep stage","sparse deep belief net"],"keywords_other":["EEG","SELECTION","Classification performance","Combination of multiple classifiers","Feature extraction methods","Individual classifiers","STRIATE CORTEX","FEATURES","Sleep stage","ALGORITHM","SUPPORT VECTOR MACHINES","SYSTEM","sparse deep belief net","Unsupervised feature learning","RECOGNITION","Electro-oculogram","SIGNALS","RECEPTIVE-FIELDS"],"max_cite":15.0,"pub_year":2016.0,"sources":"['ieee', 'scp', 'wos']","rawkeys":["feature extraction methods","combination of multiple classifiers","features","sleep stage","electro-oculogram","system","algorithm","receptive-fields","classification performance","recognition","eeg","striate cortex","sparse deep belief net","selection","individual classifiers","signals","unsupervised feature learning","feature extraction","support vector machines"],"tags":["feature extraction methods","random forests","combination of multiple classifiers","features","sleep stage","electro-oculogram","machine learning","system","algorithms","recognition","classification performance","eeg","striate cortex","sparse deep belief net","selection","individual classifiers","signals","unsupervised feature learning","feature extraction"]},{"p_id":21281,"title":"Parallel vision for perception and understanding of complex scenes: methods, framework, and perspectives","abstract":"\u00a9 2017, Springer Science+Business Media B.V.In the study of image and vision computing, the generalization capability of an algorithm often determines whether it is able to work well in complex scenes. The goal of this review article is to survey the use of photorealistic image synthesis methods in addressing the problems of visual perception and understanding. Currently, the ACP Methodology comprising artificial systems, computational experiments, and parallel execution is playing an essential role in modeling and control of complex systems. This paper extends the ACP Methodology into the computer vision field, by proposing the concept and basic framework of Parallel Vision. In this paper, we first review previous works related to Parallel Vision, in terms of synthetic data generation and utilization. We detail the utility of synthetic data for feature analysis, object analysis, scene analysis, and other analyses. Then we propose the basic framework of Parallel Vision, which is composed of an ACP trilogy (artificial scenes, computational experiments, and parallel execution). We also present some in-depth thoughts and perspectives on Parallel Vision. This paper emphasizes the significance of synthetic data to vision system design and suggests a novel research methodology for perception and understanding of complex scenes.","keywords_author":["ACP Methodology","Complex scenes","Computer graphics","Image synthesis","Parallel Vision","Visual perception","Visual perception","Complex scenes","Parallel Vision","ACP Methodology","Computer graphics","Image synthesis"],"keywords_other":["Image synthesis","COMPUTER VISION","VIRTUAL WORLDS","Complex scenes","CAMERA NETWORKS","Visual perception","Parallel vision","VIDEO SURVEILLANCE","RECOGNITION","IMAGES","ALGORITHMS","ACP Methodology","DOMAIN ADAPTATION","MACHINES","PEDESTRIAN DETECTION"],"max_cite":15.0,"pub_year":2017.0,"sources":"['ieee', 'scp', 'wos']","rawkeys":["pedestrian detection","machines","images","recognition","video surveillance","virtual worlds","visual perception","acp methodology","domain adaptation","complex scenes","computer graphics","image synthesis","algorithms","computer vision","parallel vision","camera networks"],"tags":["pedestrian detection","recognition","images","denoising autoencoder","video surveillance","virtual worlds","machine","visual perception","acp methodology","complex scenes","computer graphics","image synthesis","algorithms","computer vision","parallel vision","camera networks"]},{"p_id":78651,"title":"The Effects of Practice Schedule and Critical Thinking Prompts on Learning and Transfer of a Complex Judgment Task","abstract":"Many instructional strategies that appear to improve learners' performance during training may not realize adequate posttest performance or transfer to a job. The converse has been found lobe true as well: Instructional strategies that appear to slow the learner's progress during training often lead to better posttraining or transfer performance. For example, many studies have shown beneficial effects of random over blocked practice on transfer of learning, even though blocked practice often leads to better performance during the training session. In a 2 X 3 factorial experiment (N = 120), with the factors practice schedule (random, blocked) and critical thinking prompts (before task, after task, none), this study investigates whether this also applies to complex judgment tasks and whether critical thinking prompts can enhance the effectiveness of particular practice schedules. It is hypothesized that prompts provided after task execution yield best transfer in a random practice schedule, whereas prompts provided before task execution yield best transfer in a blocked schedule. In line with this hypothesis, a blocked schedule led to better performance than random practice during training but not on the transfer test, where a random schedule was beneficial. The hypothesized interaction effect was also found: Critical thinking prompts after task execution significantly benefit transfer performance of participants following a random schedule, and transfer performance following a blocked schedule can be a little enhanced through providing critical thinking prompts before task execution. These results warrant instruction in critical thinking processes to teach complex judgment tasks, using random practice schedules combined with critical thinking prompts provided after task execution.","keywords_author":["contextual interference","critical thinking","complex judgment","learning","transfer"],"keywords_other":["COGNITIVE-LOAD APPROACH","SELF-EXPLANATIONS","MIDDLE SCHOOL STUDENTS","TRANSFER PERFORMANCE","WORKED EXAMPLES","MULTIPLE-CUE PROBABILITY","FEEDBACK","MOTOR-SKILL ACQUISITION","CONTEXTUAL INTERFERENCE","REFLECTION"],"max_cite":14.0,"pub_year":2011.0,"sources":"['wos']","rawkeys":["complex judgment","transfer","transfer performance","worked examples","cognitive-load approach","learning","contextual interference","motor-skill acquisition","feedback","critical thinking","self-explanations","middle school students","reflection","multiple-cue probability"],"tags":["complex judgment","transfer performance","recognition","worked examples","cognitive-load approach","skills","contextual interference","machine learning","motor-skill acquisition","feedback","self-explanations","middle school students","reflection","multiple-cue probability"]},{"p_id":37691,"title":"Security perspective of Biometric recognition and machine learning techniques","abstract":"Biometric systems may be used to create a remote access model on devices, ensure personal data protection, personalize and facilitate the access security. Biometric systems are generally used to increase the security level in addition to the previous authentication methods and they seen as a good solution. Biometry occupies an important place between the areas of daily life of the machine learning. In this study; the techniques, methods, technologies used in biometric systems are researched, machine learning techniques used biometric aplications are investigated for the security perspective, the advantages and disadvantages that these tecniques provide are given. The studies in the literature between 2010-2016 years, used algorithms, technologies, metrics, usage areas, the machine learning techniques used for different biometric systems such as face, palm prints, iris, voice, fingerprint recognition are researched and the studies made are evaluated. The level of security provided by the use of biometric systems by developed using machine learning and disadvantages that arise in the use of these systems are stated in detail in the study. Also, impact on people of biometric methods in terms of ease of use, security and usages areas are examined.","keywords_author":["Biometric","Face","Fingerprint","Iris","Machine learning","Recognition","Security","Teeth","Voice"],"keywords_other":["Recognition","Security","Fingerprint","Face","Teeth","Iris"],"max_cite":1.0,"pub_year":2017.0,"sources":"['scp']","rawkeys":["iris","recognition","machine learning","biometric","security","teeth","face","fingerprint","voice"],"tags":["iris","recognition","machine learning","security","teeth","biometrics","face","fingerprint","voice"]},{"p_id":21354,"title":"Hypergraph regularized autoencoder for image-based 3D human pose recovery","abstract":"\u00a9 2015 Elsevier B.V. All rights reserved.Image-based human pose recovery is usually conducted by retrieving relevant poses with image features. However, semantic gap exists for current feature extractors, which limits recovery performance. In this paper, we propose a novel feature extractor with deep learning. It is based on denoising autoencoder and improves traditional methods by adopting locality preserved restriction. To impose this restriction, we introduce manifold regularization with hypergraph Laplacian. Hypergraph Laplacian matrix is constructed with patch alignment framework. In this way, an automatic feature extractor for silhouettes is achieved. Experimental results on two datasets show that the recovery error has been reduced by 10% to 20%, which demonstrates the effectiveness of the proposed method.","keywords_author":["Deep learning","Human pose recovery","Hypergraph","Manifold regularization","Patch alignment framework","Human pose recovery","Deep learning","Manifold regularization","Hypergraph","Patch alignment framework"],"keywords_other":["Deep learning","SHAPE CONTEXTS","Manifold regularizations","FEATURES","RANKING","Hypergraph","RETRIEVAL","RECOGNITION","Human pose","Patch alignment framework","TRACKING"],"max_cite":14.0,"pub_year":2016.0,"sources":"['ieee', 'scp', 'wos']","rawkeys":["human pose","patch alignment framework","recognition","features","deep learning","shape contexts","manifold regularizations","tracking","manifold regularization","retrieval","ranking","human pose recovery","hypergraph"],"tags":["human pose","patch alignment framework","recognition","features","standards","machine learning","shape contexts","tracking","manifold regularization","retrieval","human pose recovery","hypergraph"]},{"p_id":4973,"title":"Large-scale video classification with convolutional neural networks","abstract":"\u00a9 2014 IEEE.Convolutional Neural Networks (CNNs) have been established as a powerful class of models for image recognition problems. Encouraged by these results, we provide an extensive empirical evaluation of CNNs on large-scale video classification using a new dataset of 1 million YouTube videos belonging to 487 classes. We study multiple approaches for extending the connectivity of a CNN in time domain to take advantage of local spatio-temporal information and suggest a multiresolution, foveated architecture as a promising way of speeding up the training. Our best spatio-temporal networks display significant performance improvements compared to strong feature-based baselines (55.3% to 63.9%), but only a surprisingly modest improvement compared to single-frame models (59.3% to 60.9%). We further study the generalization performance of our best model by retraining the top layers on the UCF-101 Action Recognition dataset and observe significant performance improvements compared to the UCF-101 baseline model (63.3% up from 43.9%).","keywords_author":["action","classification","convolutional","dataset","large-scale","network","neural","recognition","sports","video"],"keywords_other":["recognition","dataset","convolutional","sports","large-scale","video","neural","action"],"max_cite":1156.0,"pub_year":2014.0,"sources":"['ieee', 'scp', 'wos']","rawkeys":["network","recognition","dataset","convolutional","sports","large-scale","video","neural","classification","action"],"tags":["recognition","convolution","sports","large-scale","video","data sets","networks","neural","classification","action"]},{"p_id":95121,"title":"Crowdsourcing the character of a place: Character-level convolutional networks for multilingual geographic text classification","abstract":"This article presents a new character-level convolutional neural network model that can classify multilingual text written using any character set that can be encoded with UTF-8, a standard and widely used 8-bit character encoding. For geographic classification of text, we demonstrate that this approach is competitive with state-of-the-art word-based text classification methods. The model was tested on four crowdsourced data sets made up of Wikipedia articles, online travel blogs, Geonames toponyms, and Twitter posts. Unlike word-based methods, which require data cleaning and pre-processing, the proposed model works for any language without modification and with classification accuracy comparable to existing methods. Using a synthetic data set with introduced character-level errors, we show it is more robust to noise than word-level classification algorithms. The results indicate that UTF-8 character-level convolutional neural networks are a promising technique for georeferencing noisy text, such as found in colloquial social media posts and texts scanned with optical character recognition. However, word-based methods currently require less computation time to train, so currently are preferable for classifying well-formatted and cleaned texts in single languages.","keywords_author":null,"keywords_other":["DOCUMENTS","NEURAL-NETWORKS","RECOGNITION","WEB"],"max_cite":1.0,"pub_year":2018.0,"sources":"['ieee', 'wos']","rawkeys":["documents","neural-networks","recognition","web"],"tags":["neural networks","documentation","recognition","web"]},{"p_id":5015,"title":"Human-level control through deep reinforcement learning","abstract":"\u00a9 2015 Macmillan Publishers Limited. All rights reserved. The theory of reinforcement learning provides a normative account, deeply rooted in psychological and neuroscientific perspectives on animal behaviour, of how agents may optimize their control of an environment. To use reinforcement learning successfully in situations approaching real-world complexity, however, agents are confronted with a difficult task: they must derive efficient representations of the environment from high-dimensional sensory inputs, and use these to generalize past experience to new situations. Remarkably, humans and other animals seem to solve this problem through a harmonious combination of reinforcement learning and hierarchical sensory processing systems, the former evidenced by a wealth of neural data revealing notable parallels between the phasic signals emitted by dopaminergic neurons and temporal difference reinforcement learning algorithms. While reinforcement learning agents have achieved some successes in a variety of domains, their applicability has previously been limited to domains in which useful features can be handcrafted, or to domains with fully observed, low-dimensional state spaces. Here we use recent advances in training deep neural networks to develop a novel artificial agent, termed a deep Q-network, that can learn successful policies directly from high-dimensional sensory inputs using end-to-end reinforcement learning. We tested this agent on the challenging domain of classic Atari 2600 games. We demonstrate that the deep Q-network agent, receiving only the pixels and the game score as inputs, was able to surpass the performance of all previous algorithms and achieve a level comparable to that of a professional human games tester across a set of 49 games, using the same algorithm, network architecture and hyperparameters. This work bridges the divide between high-dimensional sensory inputs and actions, resulting in the first artificial agent that is capable of learning to excel at a diverse array of challenging tasks.","keywords_author":null,"keywords_other":["Algorithms","Neural Networks (Computer)","Humans","Models, Psychological","RECOGNITION","Artificial Intelligence","Reinforcement (Psychology)","CORTEX","Video Games","Reward"],"max_cite":1046.0,"pub_year":2015.0,"sources":"['ieee', 'scp', 'wos']","rawkeys":["artificial intelligence","neural networks (computer)","recognition","reinforcement (psychology)","video games","humans","psychological","reward","models","algorithms","cortex"],"tags":["recognition","model","video games","neural networks","machine learning","humans","reward","algorithms","cortex"]},{"p_id":37788,"title":"The design of a psychotherapy remote intelligent system","abstract":"\u00a9 2005 - 2016 JATIT & LLS. All rights reserved.Generally speaking, scientific research aims to develop humanity and makes it be better. Meanwhile, the research we did is involved within this trend because it holds a great importance as it combines many flags that occupy a high profile, such as computer science, psychology and philosophy. Through this research, we will let the coming generations be blessed with an integrated and balanced psychological health. Besides, people in many countries suffer from psychological problems that turn them into frustrated and make them unable to engage in various activities of their countries. Those psychological disorders make them remain silent and unable to share their concerns with others; they make them unable to talk about their psychological difficulties that prevent their integration into their communities and it is also known that psychological problems have a cure and a solution. For instance, psychological social scientists are able to edit and correct those behaviors and psychological disorders. Meanwhile, following an analysis of the field work findings, it becomes clear that the problem of citizens is reflected in the lack of courage to visit specialists in the field of psychology. Waiting for citizens to be convinced that it\u2019s not shame to visit a psychologist, and that the doctors or psychologist advisors are just like other medicine specialists, the solution was the psychological remote therapy. Thus, to land this solution on the ground and to solve this problem, we adopted modern technological ways such as the expert system and machine learning.","keywords_author":["Awareness","Expert system","Intelligent system","Machine learning","Psychology"],"keywords_other":null,"max_cite":1.0,"pub_year":2016.0,"sources":"['scp']","rawkeys":["machine learning","psychology","awareness","expert system","intelligent system"],"tags":["recognition","machine learning","intelligent systems","awareness","expert system"]},{"p_id":45982,"title":"Deep construction of an affective latent space via multimodal enactment","abstract":"IEEE We draw on a simulationist approach to the analysis of facially displayed emotions - e.g., in the course of a face-to-face interaction between an expresser and an observer. At the heart of such perspective lies the enactment of the perceived emotion in the observer. We propose a novel probabilistic framework based on a deep latent representation of a continuous affect space, which can be exploited for both the estimation and the enactment of affective states in a multimodal space (visible facial expressions and physiological signals). The rationale behind the approach lies in the large body of evidence from affective neuroscience showing that when we observe emotional facial expressions, we react with congruent facial mimicry. Further, in more complex situations, affect understanding is likely to rely on a comprehensive representation grounding the reconstruction of the state of the body associated with the displayed emotion. We show that our approach can address such problems in a unified and principled perspective, thus avoiding ad hoc heuristics while minimising learning efforts.","keywords_author":["Bayesian models.","Computational modeling","deep learning","Emotion","Face","human-agent interaction","Manganese","Mirrors","Observers","Psychology","simulation","Visualization"],"keywords_other":["Human agent interactions","Computational model","Emotion","Psychology","Face","Bayesian model","Observers","simulation"],"max_cite":0.0,"pub_year":2017.0,"sources":"['scp']","rawkeys":["computational modeling","bayesian model","emotion","deep learning","bayesian models","human agent interactions","mirrors","manganese","observers","human-agent interaction","face","psychology","computational model","visualization","simulation"],"tags":["computational modeling","bayesian model","recognition","emotion","human agent interactions","machine learning","mirrors","manganese","observers","face","visualization","simulation"]},{"p_id":62368,"title":"The effect of texture and grain size on improving the mechanical properties of Mg-Al-Zn alloys by friction stir processing","abstract":"Friction stir processing (FSP) was used to achieve grain refinement on Mg-Al-Zn alloys, which also brought in significant texture modification. The different micro-texture characteristics were found to cause irregular micro-hardness distribution in FSPed region. The effects of texture and grain size were investigated by comparative analyses with strongly textured rolling sheet. Grain refinement improved both strength and elongation in condition of a basal texture while such led to an increment in yield stress and a drop in elongation and ultimate stress when the basal texture was modified by FSP.","keywords_author":null,"keywords_other":["FRACTURE-BEHAVIOR","SHEET","MICROSTRUCTURE","SLIP","TENSILE BEHAVIOR","HIGH-PRESSURE TORSION","AZ31 MAGNESIUM ALLOY","DEPENDENCE","SEVERE PLASTIC-DEFORMATION","EVOLUTION"],"max_cite":0.0,"pub_year":2018.0,"sources":"['wos']","rawkeys":["az31 magnesium alloy","high-pressure torsion","fracture-behavior","microstructure","tensile behavior","sheet","slip","evolution","dependence","severe plastic-deformation"],"tags":["recognition","az31 magnesium alloy","high-pressure torsion","fracture-behavior","microstructure","tensile behavior","sheet","slip","biological","severe plastic-deformation"]},{"p_id":13221,"title":"Diagnosis of stator faults of the single-phase induction motor using acoustic signals","abstract":"An early diagnosis of faults prevents financial loss and downtimes in the industry. In this paper the authors presented the early fault diagnostic technique of stator faults of the single-phase induction motor. The proposed technique was based on recognition of acoustic signals. The authors measured and analysed 3 states of the single-phase induction motor: a healthy single-phase induction motor, a single-phase induction motor with shorted coils of auxiliary winding, a single-phase induction motor with shorted coils of auxiliary winding and main winding. In this paper an original method of feature extraction called MSAF-RATIO30-MULTIEXPANDED (Method of Selection of Amplitudes of Frequency Ratio 30% of maximum of amplitude Multiexpanded) was described. This method was used to form feature vectors. A classification of obtained vectors was performed by the KNN (K-Nearest Neighbour classifier), the K-Means clustering and the Linear Perceptron. The early fault diagnostic technique can find application for protection of the single-phase induction motors. It can be also used for other rotating electrical machines. (C) 2016 Elsevier Ltd. All rights reserved.","keywords_author":["Fault","Acoustic signal","Single phase induction motor","Diagnosis","Recognition"],"keywords_other":["COMBUSTION ENGINE","SUPPORT VECTOR MACHINE","TECHNICAL CONDITION","VIBRATION","CLASSIFICATION","IDENTIFICATION","GEAR","BEARING","CLASSIFIERS","ELECTRICAL MACHINES"],"max_cite":26.0,"pub_year":2017.0,"sources":"['wos']","rawkeys":["diagnosis","bearing","fault","identification","recognition","acoustic signal","single phase induction motor","classifiers","technical condition","classification","combustion engine","gear","support vector machine","electrical machines","vibration"],"tags":["diagnosis","bearing","vibration","identification","recognition","single phase induction motor","machine learning","technical condition","classifier","classification","combustion engine","gear","acoustic signals","electric machines"]},{"p_id":103336,"title":"Facilitation of picture-naming in anomic subjects: Sound vs mouth shape","abstract":"Background: Phonological cueing is often applied to assist spoken word retrieval in anomia. Phonological cues usually comprise auditory and visual information. Yet while the auditory channel provides full information about a cued phoneme, the mouth shape only contains information about certain features of a segment, e.g., place of articulation. Aims: This study aimed to investigate the immediate effectiveness of visual phonological information in anomic participants, as compared with auditory cueing, in assisting spoken word retrieval. Methods Procedures: In a group of 16 anomic participants, spoken picture-naming tasks were administered using mouth shapes of initial segments of target words as visual cues, and initial phonemes as auditory cues. Outcomes Results: Reaction time analyses revealed significant facilitation effects for both the visual and the auditory cues. Single-case analyses revealed selective effects: participants benefited either from auditory or from visual phonological information. Conclusions: Visually based phonetic feature information specifying the initial phoneme of a word can be sufficient to facilitate word retrieval. The cognitive neurolinguistic factors predicting the effectiveness of visual vs auditory-segmental cueing remain unresolved.","keywords_author":["Anomia","Spoken naming","Auditory phonological cues","Visual phonological cues"],"keywords_other":["WORD-RETRIEVAL","INFORMATION","REPRESENTATIONS","HEARING","APHASIA","RECOGNITION","DIFFICULTIES","CLASSICAL ANEMIA","CONSONANTS","SPEECH"],"max_cite":4.0,"pub_year":2011.0,"sources":"['wos']","rawkeys":["difficulties","recognition","anomia","spoken naming","hearing","representations","visual phonological cues","word-retrieval","information","classical anemia","speech","consonants","auditory phonological cues","aphasia"],"tags":["recognition","anomia","representation","hearing","spoken naming","difficulty","visual phonological cues","word-retrieval","information","classical anemia","speech","consonants","auditory phonological cues","aphasia"]},{"p_id":78776,"title":"The Socratic Note Taking Technique: Addressing the Problem of Students Not Engaging With Assigned Readings before Class","abstract":"The notion of Socratic Note Taking (SNT) is introduced to enhance students' learning from assigned readings. SNT features students asking questions and answering their own questions while doing the readings. To test the effectiveness of SNT, half the students from two sections of a philosophy course were assigned SNT on alternating weeks. Quizzes each week alternated between the two classes as either high or low stakes in a counterbalanced format. The design was a 2 (Notes: SNT or not) x 2 (Stakes: high or low) x 2 (Replication: first or second replication of a Notes x Stakes cell) within-participants factorial. On ten-point quizzes, SNT made an average difference of 1.22 points (more than a letter grade). Furthermore, the results indicate that SNT is particularly effective with weaker students, e.g., we found a nearly three-point increase on ten-point quizzes for the weakest students.","keywords_author":null,"keywords_other":["COMPREHENSION","AIDS","PERFORMANCE","QUIZZES","PSYCHOLOGY","SOCIOLOGY"],"max_cite":1.0,"pub_year":2017.0,"sources":"['wos']","rawkeys":["comprehension","performance","sociology","aids","quizzes","psychology"],"tags":["comprehension","recognition","performance","sociology","quizzes","automatic incident detection"]},{"p_id":78786,"title":"Describing Typical Capstone Course Experiences From a National Random Sample","abstract":"The pedagogical value of capstones has been regularly discussed within psychology. This study presents results from an examination of a national random sample of department webpages and an online survey that characterized the typical capstone course in terms of classroom activities and course administration. The department webpages provide an estimate for the presence of capstone courses within the major and some basic characteristics of the course. The survey results offer a more in-depth profile of the purpose and make up of typical capstone courses in psychology. Results suggest that institutions have multiple approaches to the capstone course, but that these differences are rarely associated with institutional size, type of funding, or degree offered. These results could be useful for departments considering the place of capstone course in their own major. Finally, they suggest some areas of future research such as the type, quality, and final outcomes of capstone projects and their relationship to learning goals.","keywords_author":["capstone courses","undergraduate curriculum"],"keywords_other":["PSYCHOLOGY","SOCIOLOGY","UNDERGRADUATE RESEARCH"],"max_cite":4.0,"pub_year":2013.0,"sources":"['wos']","rawkeys":["undergraduate research","undergraduate curriculum","sociology","capstone courses","psychology"],"tags":["capstone course","undergraduate research","recognition","undergraduate curriculum","sociology"]},{"p_id":70609,"title":"Abnormal event detection and localization using level set based on hybrid features","abstract":"In this paper, a new method using level set is proposed for abnormal event detection and localization in video surveillance. From an input video, five image descriptors, namely the color moments, the edge histogram descriptors, the color and edge directivity descriptors, the color layout descriptors, and the scalable color descriptors, are extracted for robust detection. We employ the local binary fitting model as the statistical learning model to update by the input videos in real time. We performed experiments on the publicly available UCSD anomaly detection dataset and showed that our method has good performance for detecting and localizing abnormality compared to the state-of-the-art methods.","keywords_author":["Abnormality detection","Localization in video surveillance","Hybrid image features"],"keywords_other":["ANOMALY DETECTION","PARALLEL FRAMEWORK","SPARSE REPRESENTATION","IMAGE REPRESENTATION","MODEL","VIDEO SURVEILLANCE","RECOGNITION","MANY-CORE PROCESSORS","SCENES","CROWD BEHAVIOR DETECTION"],"max_cite":0.0,"pub_year":2017.0,"sources":"['ieee', 'wos']","rawkeys":["abnormality detection","parallel framework","scenes","many-core processors","model","recognition","hybrid image features","anomaly detection","localization in video surveillance","sparse representation","video surveillance","crowd behavior detection","image representation"],"tags":["abnormality detection","parallel framework","many-core processors","model","recognition","hybrid image features","anomaly detection","localization in video surveillance","sparse representation","scene","video surveillance","crowd behavior detection","image representation"]},{"p_id":62418,"title":"Robust sound event classification with bilinear multi-column ELM-AE and two-stage ensemble learning","abstract":"The automatic sound event classification (SEC) has attracted a growing attention in recent years. Feature extraction is a critical factor in SEC system, and the deep neural network (DNN) algorithms have achieved the state-of-the-art performance for SEC. The extreme learning machine-based auto-encoder (ELM-AE) is a new deep learning algorithm, which has both an excellent representation performance and very fast training procedure. However, ELM-AE suffers from the problem of unstability. In this work, a bilinear multi-column ELM-AE (B-MC-ELM-AE) algorithm is proposed to improve the robustness, stability, and feature representation of the original ELM-AE, which is then applied to learn feature representation of sound signals. Moreover, a B-MC-ELM-AE and two-stage ensemble learning (TsEL)-based feature learning and classification framework is then developed to perform the robust and effective SEC. The experimental results on the Real World Computing Partnership Sound Scene Database show that the proposed SEC framework outperforms the state-of-the-art DNN algorithm.","keywords_author":["Sound event classification","Multi-column","Bilinear","Extreme learning machine auto-encoder","Ensemble learning"],"keywords_other":["DEEP","REPRESENTATION","MACHINE","NEURAL-NETWORKS","RECOGNITION"],"max_cite":0.0,"pub_year":2017.0,"sources":"['ieee', 'wos']","rawkeys":["neural-networks","recognition","deep","extreme learning machine auto-encoder","machine","multi-column","bilinear","representation","sound event classification","ensemble learning"],"tags":["recognition","neural networks","deep","extreme learning machine auto-encoder","machine","multi-column","bilinear","representation","sound event classification","ensemble learning"]},{"p_id":62420,"title":"Manifold Ranking-Based Matrix Factorization for Saliency Detection","abstract":"Saliency detection is used to identify the most important and informative area in a scene, and it is widely used in various vision tasks, including image quality assessment, image matching, and object recognition. Manifold ranking (MR) has been used to great effect for the saliency detection, since it not only incorporates the local spatial information but also utilizes the labeling information from background queries. However, MR completely ignores the feature information extracted from each superpixel. In this paper, we propose an MR-based matrix factorization (MRMF) method to overcome this limitation. MRMF models the ranking problem in the matrix factorization framework and embeds query sample labels in the coefficients. By incorporating spatial information and embedding labels, MRMF enforces similar saliency values on neighboring superpixels and ranks superpixels according to the learned coefficients. We prove that the MRMF has good generalizability, and develops an efficient optimization algorithm based on the Nesterov method. Experiments using popular benchmark data sets illustrate the promise of MRMF compared with the other state-of-the-art saliency detection methods.","keywords_author":["Manifold ranking (MR)","matrix factorization","optimization algorithm","saliency detection"],"keywords_other":["IMAGE","VISUAL-ATTENTION","DEEP","FEATURES","MODEL","RECOGNITION","REGION DETECTION","VIDEO COMPRESSION","OBJECT DETECTION"],"max_cite":24.0,"pub_year":2016.0,"sources":"['ieee', 'wos']","rawkeys":["recognition","model","region detection","features","visual-attention","deep","video compression","matrix factorization","saliency detection","image","object detection","optimization algorithm","manifold ranking (mr)"],"tags":["recognition","images","model","features","region detection","deep","nonnegative matrix factorization","video compression","saliency detection","multiresolution","object detection","visual attention","optimization algorithms"]},{"p_id":62421,"title":"Large Sparse Cone Non-negative Matrix Factorization for Image Annotation","abstract":"Image annotation assigns relevant tags to query images based on their semantic contents. Since Non-negative Matrix Factorization (NMF) has the strong ability to learn parts-based representations, recently, a number of algorithms based on NMF have been proposed for image annotation and have achieved good performance. However, most of the efforts have focused on the representations of images and annotations. The properties of the semantic parts have not been well studied. In this article, we revisit the sparseness-constrained NMF (sNMF) proposed by Hoyer [ 2004]. By endowing the sparseness constraint with a geometric interpretation and sNMF with theoretical analyses of the generalization ability, we show that NMF with such a sparseness constraint has three advantages for image annotation tasks: (i) The sparseness constraint is more l(0)-norm oriented than the l(0)-norm-based sparseness, which significantly enhances the ability of NMF to robustly learn semantic parts. (ii) The sparseness constraint has a large cone interpretation and thus allows the reconstruction error of NMF to be smaller, which means that the learned semantic parts are more powerful to represent images for tagging. (iii) The learned semantic parts are less correlated, which increases the discriminative ability for annotating images. Moreover, we present a new efficient large sparse cone NMF (LsCNMF) algorithm to optimize the sNMF problem by employing the Nesterov's optimal gradient method. We conducted experiments on the PASCAL VOC07 dataset and demonstrated the effectiveness of LsCNMF for image annotation.","keywords_author":["Non-negative matrix factorization","image annotation","Nesterovs optimal gradient","sparseness constraint"],"keywords_other":["NETWORKS","RECOGNITION","SOCIAL MULTIMEDIA","WAVELET"],"max_cite":2.0,"pub_year":2017.0,"sources":"['ieee', 'wos']","rawkeys":["recognition","nesterovs optimal gradient","sparseness constraint","wavelet","networks","non-negative matrix factorization","social multimedia","image annotation"],"tags":["sparse constraint","recognition","nesterovs optimal gradient","nonnegative matrix factorization","wavelet","networks","social multimedia","image annotation"]},{"p_id":29654,"title":"Geodesic invariant feature: A local descriptor in depth","abstract":"\u00a9 2014 IEEE.Different from the photometric images, depth images resolve the distance ambiguity of the scene, while the properties, such as weak texture, high noise, and low resolution, may limit the representation ability of the well-developed descriptors, which are elaborately designed for the photometric images. In this paper, a novel depth descriptor, geodesic invariant feature (GIF), is presented for representing the parts of the articulate objects in depth images. GIF is a multilevel feature representation framework, which is proposed based on the nature of depth images. Low-level, geodesic gradient is introduced to obtain the invariance to the articulate motion, such as scale and rotation variation. Midlevel, superpixel clustering is applied to reduce depth image redundancy, resulting in faster processing speed and better robustness to noise. High-level, deep network is used to exploit the nonlinearity of the data, which further improves the classification accuracy. The proposed descriptor is capable of encoding the local structures in the depth data effectively and efficiently. Comparisons with the state-of-the-art methods reveal the superiority of the proposed method.","keywords_author":["Body parts recognition","deep learning","depth image","pose recognition","superpixel","Body parts recognition","pose recognition","depth image","deep learning","superpixel"],"keywords_other":["Deep learning","Pose recognition","Depth image","REPRESENTATION","Super pixels","RECOGNITION","BINARY PATTERNS","SCALE","IMAGE RETRIEVAL","Body parts","PCA","TRACKING"],"max_cite":4.0,"pub_year":2015.0,"sources":"['ieee', 'scp', 'wos']","rawkeys":["binary patterns","recognition","depth image","body parts","deep learning","image retrieval","representation","scale","superpixel","tracking","super pixels","pca","pose recognition","body parts recognition"],"tags":["binary patterns","principal component analysis","depth image","recognition","body parts","image retrieval","machine learning","representation","tracking","superpixel","scale","pose recognition","body parts recognition"]},{"p_id":62423,"title":"The ordinal relation preserving binary codes","abstract":"Hashing algorithm has been widely used for efficient approximate nearest neighbor (ANN) search. Learning optimal hashing functions has been given focus and it is still a challenge. This paper aims to effectively and efficiently generate relative similarity preserving binary codes. Most existing hashing methods try to preserve the locality similarity by preserving direct distance similarity, while ignoring the relative similarity which advantages in ANN search. In this paper, this issue is solved by proposing the relative error which emphasizes that the ordinal relations in Hamming space and Euclidean space should be consistent with each other. We learn hashing projection functions via two steps. The first step adopts the lookup-based mechanism to find the optimal binary codes of training data, which can preserve the relative similarity and simultaneously adapt to data distribution. The binary codes in the first step are considered as supervision information in the second step. The objective of the second step is to learn hashing projection functions which can efficiently regenerate the binary codes in the first step. Aim to be in accordance with the property of data distribution, the hyper internal tangent planes of two specified spheres are chosen as hashing projection functions. Assisted by these projection functions, the time complexity of encoding process is greatly reduced. Experimental results on four public data sets demonstrate that our method outperforms many other state-of-the-art methods. (C) 2015 Elsevier Ltd. All rights reserved.","keywords_author":["Hashing algorithm","Binary codes","Approximate nearest neighbor search","Image retrieval"],"keywords_other":["PRODUCT QUANTIZATION","SEARCH","SCENE","RECOGNITION","DICTIONARY","APPROXIMATE NEAREST-NEIGHBOR"],"max_cite":4.0,"pub_year":2015.0,"sources":"['ieee', 'wos']","rawkeys":["recognition","search","approximate nearest neighbor search","product quantization","hashing algorithm","image retrieval","scene","dictionary","approximate nearest-neighbor","binary codes"],"tags":["recognition","search","approximate nearest neighbor search","product quantization","neural networks","image retrieval","scene","hashing algorithms","dictionaries","binary codes"]},{"p_id":62424,"title":"Convolution in Convolution for Network in Network","abstract":"Network in network (NiN) is an effective instance and an important extension of deep convolutional neural network consisting of alternating convolutional layers and pooling layers. Instead of using a linear filter for convolution, NiN utilizes shallow multilayer perceptron (MLP), a nonlinear function, to replace the linear filter. Because of the powerfulness of MLP and 1 x 1 convolutions in spatial domain, NiN has stronger ability of feature representation and hence results in better recognition performance. However, MLP itself consists of fully connected layers that give rise to a large number of parameters. In this paper, we propose to replace dense shallow MLP with sparse shallow MLP. One or more layers of the sparse shallow MLP are sparely connected in the channel dimension or channel-spatial domain. The proposed method is implemented by applying unshared convolution across the channel dimension and applying shared convolution across the spatial dimension in some computational layers. The proposed method is called convolution in convolution (CiC). The experimental results on the CIFAR10 data set, augmented CIFAR10 data set, and CIFAR100 data set demonstrate the effectiveness of the proposed CiC method.","keywords_author":["Convolution in convolution (CiC)","convolutional neural networks (CNNs)","image recognition","network in network (NiN)"],"keywords_other":["LEARNING DEEP","RECOGNITION","REPRESENTATION","IMAGES"],"max_cite":3.0,"pub_year":2018.0,"sources":"['ieee', 'wos']","rawkeys":["recognition","images","convolution in convolution (cic)","representation","learning deep","image recognition","convolutional neural networks (cnns)","network in network (nin)"],"tags":["network in network","recognition","images","convolution in convolution (cic)","representation","learning deep","image recognition","convolutional neural network"]},{"p_id":62426,"title":"Nonnegative Multiresolution Representation-Based Texture Image Classification","abstract":"Effective representation of image texture is important for an image-classification task. Statistical modelling in wavelet domains has been widely used to image texture representation. However, due to the intraclass complexity and interclass diversity of textures, it is hard to use a predefined probability distribution function to fit adaptively all wavelet subband coefficients of different textures. In this article, we propose a novel modelling approach, Heterogeneous and Incrementally Generated Histogram (HIGH), to indirectly model the wavelet coefficients by use of four local features in wavelet subbands. By concatenating all the HIGHs in all wavelet subbands of a texture, we can construct a nonnegative multiresolution vector (NMV) to represent a texture image. Considering the NMV's high dimensionality and nonnegativity, we further propose a Hessian regularized discriminative nonnegative matrix factorization to compute a low-dimensional basis of the linear subspace of NMVs. Finally, we present a texture classification approach by projecting NMVs on the low-dimensional basis. Experimental results show that our proposed texture classification method outperforms seven representative approaches.","keywords_author":["Theory","Experimentation","Nonnegative matrix factorization","texture classification","histogram","manifold regularization","Hessian regularization"],"keywords_other":["SUBSPACE","TRANSFORM","FEATURES","GENERALIZED GAUSSIAN DENSITY","MODEL","LOCAL BINARY PATTERNS","RECOGNITION","RETRIEVAL","MATRIX FACTORIZATION","FEATURE-EXTRACTION"],"max_cite":6.0,"pub_year":2015.0,"sources":"['ieee', 'wos']","rawkeys":["recognition","hessian regularization","model","features","feature-extraction","transform","nonnegative matrix factorization","matrix factorization","histogram","manifold regularization","generalized gaussian density","local binary patterns","retrieval","theory","subspace","experimentation","texture classification"],"tags":["recognition","hessian regularization","model","features","transform","nonnegative matrix factorization","histograms","manifold regularization","theory","feature extraction","generalized gaussian density","local binary patterns","retrieval","subspace","experimentation","texture classification"]},{"p_id":62430,"title":"Boosted Cross-Domain Dictionary Learning for Visual Categorization","abstract":null,"keywords_author":null,"keywords_other":["IMAGE CLASSIFICATION","RECOGNITION","CONSISTENT K-SVD","DISCRIMINATIVE DICTIONARY"],"max_cite":1.0,"pub_year":2016.0,"sources":"['ieee', 'wos']","rawkeys":["consistent k-svd","image classification","recognition","discriminative dictionary"],"tags":["consistent k-svd","image classification","recognition","discriminative dictionaries"]},{"p_id":62432,"title":"Online primal-dual learning for a data-dependent multi-kernel combination model with multiclass visual categorization applications","abstract":"The task of visual categorization becomes very challenging as the number of samples or classes increases. This is mainly due to large memory requirements or a crowded semantic space, which causes class separation difficulties. In this paper, a new approach called OPrDuM2 (online primal-dual multiple kernel multiple class) is proposed. This new algorithm can significantly reduce the complexity of visual categorization. Under this approach, an elegant heterogeneous feature fusion machine is used to consolidate complementary class-discriminative information, and online learning is applied to deal with large sample sizes. To be specific, a data-dependent multi-kernel machine that fuses multiple heterogeneous features in a nonlinear manner is defined by extending the multi-kernel learning model, and multivariate hinge logs and conservative updating rules are used to increase the sample-level sparsity in the model. Given the intrinsic hierarchical structure of samples, features, and classes, an innovative triple mixed-norm regularizer with strong convexity is utilized to facilitate optimization. An iterative solution is derived using the language of duality, leading to a low regret bound of D(rr), where T is the number of online iterations. This paper details a proof of the algorithm's convergence, and describes extensive visual data classification experiments that demonstrate the effectiveness and robustness of this new approach. (C) 2015 Elsevier Inc. All rights reserved.","keywords_author":["Data-dependent multi-kernel combination model","Multiclass visual categorization","Online learning","Primal-dual learning"],"keywords_other":["DATA FUSION","CLASSIFICATION","RECOGNITION","ALGORITHMS","SCALE","KERNEL"],"max_cite":2.0,"pub_year":2015.0,"sources":"['ieee', 'wos']","rawkeys":["primal-dual learning","data fusion","recognition","kernel","data-dependent multi-kernel combination model","classification","multiclass visual categorization","algorithms","online learning","scale"],"tags":["primal-dual learning","data fusion","recognition","kernel","data-dependent multi-kernel combination model","classification","multiclass visual categorization","algorithms","online learning","scale"]},{"p_id":95202,"title":"A Biologically Inspired Appearance Model for Robust Visual Tracking","abstract":"In this paper, we propose a biologically inspired appearance model for robust visual tracking. Motivated in part by the success of the hierarchical organization of the primary visual cortex (area V1), we establish an architecture consisting of five layers: whitening, rectification, normalization, coding, and pooling. The first three layers stem from the models developed for object recognition. In this paper, our attention focuses on the coding and pooling layers. In particular, we use a discriminative sparse coding method in the coding layer along with spatial pyramid representation in the pooling layer, which makes it easier to distinguish the target to be tracked from its background in the presence of appearance variations. An extensive experimental study shows that the proposed method has higher tracking accuracy than several state-of-the-art trackers.","keywords_author":["Appearance modeling","biologically inspiration","sparse coding","visual tracking"],"keywords_other":["SELECTION","FEATURES","CLASSIFICATION","RECOGNITION","BENCHMARK","SPARSITY","CORTEX","SHIFT","OBJECT TRACKING","RECEPTIVE-FIELDS"],"max_cite":25.0,"pub_year":2017.0,"sources":"['wos']","rawkeys":["appearance modeling","benchmark","object tracking","receptive-fields","features","recognition","sparsity","shift","visual tracking","classification","biologically inspiration","selection","cortex","sparse coding"],"tags":["biologically inspired","appearance modeling","benchmark","object tracking","recognition","features","sparsity","random forests","shift","visual tracking","classification","selection","cortex","sparse coding"]},{"p_id":62434,"title":"Multi-task proximal support vector machine","abstract":"With the explosive growth of the use of imagery, visual recognition plays an important role in many applications and attracts increasing research attention. Given several related tasks, single-task learning learns each task separately and ignores the relationships among these tasks. Different from single-task learning, multi-task learning can explore more information to learn all tasks jointly by using relationships among these tasks. In this paper, we propose a novel multi-task learning model based on the proximal support vector machine. The proximal support vector machine uses the large-margin idea as does the standard support vector machines but with looser constraints and much lower computational cost. Our multi-task proximal support vector machine inherits the merits of the proximal support vector machine and achieves better performance compared with other popular multi-task learning models. Experiments are conducted on several multi-task learning datasets, including two classification datasets and one regression dataset. All results demonstrate the effectiveness and efficiency of our proposed multi-task proximal support vector machine. (C) 2015 Elsevier Ltd. All rights reserved.","keywords_author":["Multi-task learning","Support vector machines","Proximal classifiers"],"keywords_other":["MODEL","IMAGE CLASSIFICATION","RECOGNITION","SCENE CLASSIFICATION"],"max_cite":8.0,"pub_year":2015.0,"sources":"['ieee', 'wos']","rawkeys":["multi-task learning","recognition","model","scene classification","proximal classifiers","support vector machines","image classification"],"tags":["recognition","model","scene classification","machine learning","multitask learning","proximal classifiers","image classification"]},{"p_id":62436,"title":"Content-Based Visual Landmark Search via Multimodal Hypergraph Learning","abstract":"While content-based landmark image search has recently received a lot of attention and became a very active domain, it still remains a challenging problem. Among the various reasons, high diverse visual content is the most significant one. It is common that for the same landmark, images with a wide range of visual appearances can be found from different sources and different landmarks may share very similar sets of images. As a consequence, it is very hard to accurately estimate the similarities between the landmarks purely based on single type of visual feature. Moreover, the relationships between landmark images can be very complex and how to develop an effective modeling scheme to characterize the associations still remains an open question. Motivated by these concerns, we propose multimodal hypergraph (MMHG) to characterize the complex associations between landmark images. In MMHG, images are modeled as independent vertices and hyperedges contain several vertices corresponding to particular views. Multiple hypergraphs are firstly constructed independently based on different visual modalities to describe the hidden high-order relations from different aspects. Then, they are integrated together to involve discriminative information from heterogeneous sources. We also propose a novel content-based visual landmark search system based on MMHG to facilitate effective search. Distinguished from the existing approaches, we design a unified computational module to support query-specific combination weight learning. An extensive experiment study on a large-scale test collection demonstrates the effectiveness of our scheme over state-of-the-art approaches.","keywords_author":["Content-based visual landmark search","high-order relations","multimodal hypergraph (MMHG)","visual diversity"],"keywords_other":["RERANKING","GRAPH","IMAGE SEARCH","FEATURES","CLASSIFICATION","RETRIEVAL","RECOGNITION","EVENT DETECTION"],"max_cite":25.0,"pub_year":2015.0,"sources":"['ieee', 'wos']","rawkeys":["content-based visual landmark search","reranking","recognition","multimodal hypergraph (mmhg)","features","graph","visual diversity","classification","high-order relations","image search","retrieval","event detection"],"tags":["content-based visual landmark search","recognition","multimodal hypergraph (mmhg)","features","re-ranking","graph","visual diversity","classification","high-order relations","image search","retrieval","event detection"]},{"p_id":62435,"title":"Ordinal pyramid coding for rotation invariant feature extraction","abstract":"This paper proposes a novel rotation invariant feature for object recognition. Firstly, the local Fourier transform features of pixels in the described region are encoded by Fisher Vectors. Then, the encoded vectors are aggregated into a final representation by ordinal pyramid pooling, which hierarchically partitions the described region into sub-regions based on the orders of its pixels' rotation invariants. Since both the encoded Fisher Vectors and the ordinal pyramid pooling strategy are rotation invariant, the extracted feature is rotation invariant by nature. Two kinds of rotation invariants are investigated in this framework, one is the Radial Gradient Orientation and the other is the Radial Gradient Angle. Experiments on handwritten digit recognition and airplane\/car detection in aerial images demonstrate the effectiveness of the proposed method, which outperforms the state of the art. (C) 2017 Elsevier B.V. All rights reserved.","keywords_author":["Rotation invariant","Ordinal pyramid pooling","Fisher vector","Feature extraction"],"keywords_other":["REMOTE-SENSING IMAGES","HOG","DEEP","REPRESENTATION","NETWORKS","MODEL","RECOGNITION","EFFICIENT","SCENE CLASSIFICATION","OBJECT DETECTION"],"max_cite":0.0,"pub_year":2017.0,"sources":"['ieee', 'wos']","rawkeys":["efficient","recognition","model","ordinal pyramid pooling","rotation invariant","deep","representation","scene classification","fisher vector","hog","networks","object detection","feature extraction","remote-sensing images"],"tags":["recognition","model","ordinal pyramid pooling","deep","histogram of oriented gradients","representation","efficiency","scene classification","networks","rotation invariance","fisher vectors","feature extraction","object detection","remote sensing images"]},{"p_id":62448,"title":"Diverging receptive and expressive word processing mechanisms in a deep dyslexic reader","abstract":"We report on KJ, a patient with acquired dyslexia due to cerebral artery infarction. He represents an unusually clear case of an \"output\" deep dyslexic reader, with a distinct pattern of pure semantic reading. According to current neuropsychological models of reading, the severity of this condition is directly related to the degree of impairment in semantic and phonological representations and the resulting imbalance in the interaction between the two word processing pathways. The present work sought to examine whether an innovative eye movement supported intervention combining lexical and segmental therapy would strengthen phonological processing and lead to an attenuation of the extreme semantic over-involvement in KJ's word identification process. Reading performance was assessed before (T1) between (12) and after (T3) therapy using both analyses of linguistic errors and word viewing patterns. Therapy resulted in improved reading aloud accuracy along with a change in error distribution that suggested a return to more sequential reading. Interestingly, this was in contrast to the dynamics of moment-to-moment word processing, as eye movement analyses still suggested a predominantly holistic strategy, even at T3. So, in addition to documenting the success of the therapeutic intervention, our results call for a theoretically important conclusion: Real-time letter and word recognition routines should be considered separately from properties of the verbal output. Combining both perspectives may provide a promising strategy for future assessment and therapy evaluation. (C) 2016 Published by Elsevier Ltd.","keywords_author":["Deep dyslexia","Aphasia","Reading therapy","Eye movements","Reading strategy"],"keywords_other":["PURE ALEXIA","INFORMATION","READING TREATMENT","ROUTE","RECOGNITION","SURFACE DYSLEXIA","RECOVERY","SENTENCE COMPREHENSION","SEMANTIC ERRORS","EYE-MOVEMENTS"],"max_cite":0.0,"pub_year":2016.0,"sources":"['wos']","rawkeys":["sentence comprehension","deep dyslexia","recognition","semantic errors","route","recovery","pure alexia","reading strategy","surface dyslexia","information","reading therapy","reading treatment","eye-movements","eye movements","aphasia"],"tags":["sentence comprehension","deep dyslexia","recognition","semantic errors","routing","recovery","pure alexia","surface dyslexia","information","reading therapy","reading treatment","reading strategy","eye movements","aphasia"]},{"p_id":13297,"title":"Evaluation of Students' Perceptions Towards An Innovative Teaching-Learning Method During Pharmacology Revision Classes: Autobiography of Drugs","abstract":"Introduction: Various studies in medical education have shown that active learning strategies should be incorporated into the teaching-learning process to make learning more effective, efficient and meaningful.","keywords_author":["Active learning","Medical students' perception","Pharmacology education","Reinforcement"],"keywords_other":null,"max_cite":1.0,"pub_year":2015.0,"sources":"['wos']","rawkeys":["medical students' perception","reinforcement","pharmacology education","active learning"],"tags":["medical students' perception","recognition","pharmacology education","machine learning"]},{"p_id":46069,"title":"Chinese traditional visual cultural symbols recognition based on the combination of depth hierarchy features and SVM","abstract":"\u00a9 2017 IEEE. Chinese traditional visual culture symbols (CT-VCSs) is formed in the tradition and has the characteristic of Chinese unique ideological and cultural connotation. It is a visual cultural heritage of Chinese culture. So the research on CT-VCSs has important practical significance. In this paper, it is mainly about the recognition and classification of CT-VCSs based on machine learning. We make use of the advantages of both shallow learning and deep learning, and then we combine them together to achieve the purpose of recognition. In other words, we regard the traditional convolutional neural network as a feature extractor and extract the features of two full-connected layers as depth hierarchy features. At last, we take them into SVM with Histogram Intersection Kernel function for classification. Experimental results show that the recognition effect is very good.","keywords_author":["Chinese traditional visual culture symbol","classification","CNN","CT-VCSs","deep learning","depth hierarchy features","kernel function","recognition","shallow learning","SVM"],"keywords_other":["Chinese traditional visual culture symbol","recognition","Kernel function","depth hierarchy features","shallow learning"],"max_cite":0.0,"pub_year":2017.0,"sources":"['scp']","rawkeys":["recognition","deep learning","kernel function","depth hierarchy features","cnn","svm","ct-vcss","classification","chinese traditional visual culture symbol","shallow learning"],"tags":["recognition","kernel function","depth hierarchy features","machine learning","ct-vcss","classification","convolutional neural network","chinese traditional visual culture symbol","shallow learning"]},{"p_id":103442,"title":"Natural protein sequences are more intrinsically disordered than random sequences","abstract":"Most natural protein sequences have resulted from millions or even billions of years of evolution. How they differ from random sequences is not fully understood. Previous computational and experimental studies of random proteins generated from noncoding regions yielded inclusive results due to species-dependent codon biases and GC contents. Here, we approach this problem by investigating 10,000 sequences randomized at the amino acid level. Using well-established predictors for protein intrinsic disorder, we found that natural sequences have more long disordered regions than random sequences, even when random and natural sequences have the same overall composition of amino acid residues. We also showed that random sequences are as structured as natural sequences according to contents and length distributions of predicted secondary structure, although the structures from random sequences may be in a molten globular-like state, according to molecular dynamics simulations. The bias of natural sequences toward more intrinsic disorder suggests that natural sequences are created and evolved to avoid protein aggregation and increase functional diversity.","keywords_author":["Random sequence","Protein intrinsic disorder","Secondary structure","Molten globule","Molecular dynamics simulation"],"keywords_other":["FOLDED PROTEINS","PREDICTION","INFORMATION","CLASSIFICATION","AGGREGATION","LIBRARY","PARTICLE MESH EWALD","DATABASE","RECOGNITION","EVOLUTION"],"max_cite":9.0,"pub_year":2016.0,"sources":"['wos']","rawkeys":["library","folded proteins","protein intrinsic disorder","recognition","molten globule","prediction","aggregation","database","random sequence","classification","information","molecular dynamics simulation","evolution","secondary structure","particle mesh ewald"],"tags":["folded proteins","libraries","protein intrinsic disorder","recognition","databases","molten globule","prediction","aggregation","random sequence","classification","information","biological","secondary structure","molecular-dynamics simulations","particle mesh ewald"]},{"p_id":95273,"title":"Learning Representations for Nonspeech Audio Events Through Their Similarities to Speech Patterns","abstract":"The human auditory system is very well matched to both human speech and environmental sounds. Therefore, the question arises whether human speech material may provide useful information for training systems for analyzing nonspeech audio signals, e.g., in a classification task. In order to answer this question, we consider speech patterns as basic acoustic concepts, which embody and represent the target nonspeech signal. To find out how similar the nonspeech signal is to speech, we classify it with a classifier trained on the speech patterns and use the classification posteriors to represent the closeness to the speech bases. The speech similarities are finally employed as a descriptor to represent the target signal. We further show that a better descriptor can be obtained by learning to organize the speech categories hierarchically with a tree structure. Furthermore, these descriptors are generic. That is, once the speech classifier has been learned, it can be employed as a feature extractor for different datasets without retraining. Lastly, we propose an algorithm to select a sufficient subset, which provides an approximate representation capability of the entire set of available speech patterns. We conduct experiments for the application of audio event analysis. Phone triplets from the TIMIT dataset were used as speech patterns to learn the descriptors for audio events of three different datasets with different complexity, including UPC-TALP, Freiburg-106, and NAR. The experimental results on the event classification task show that a good performance can be easily obtained even if a simple linear classifier is used. Furthermore, fusion of the learned descriptors as an additional source leads to state-of-the-art performance on all the three target datasets.","keywords_author":["Feature learning","representation","nonspeech audio event","speech patterns","phone triplets"],"keywords_other":["IMAGE FEATURE","FEATURES","MACHINE","SYSTEMS","CLASSIFICATION","RECOGNITION","TIME","DEEP NEURAL-NETWORKS","FORESTS","FEATURE-EXTRACTION"],"max_cite":5.0,"pub_year":2016.0,"sources":"['ieee', 'wos']","rawkeys":["phone triplets","recognition","systems","features","feature-extraction","forests","nonspeech audio event","machine","representation","time","deep neural-networks","speech patterns","classification","image feature","feature learning"],"tags":["phone triplets","image features","recognition","features","forests","nonspeech audio event","machine","representation","system","time","speech patterns","classification","feature extraction","convolutional neural network","feature learning"]},{"p_id":103466,"title":"Protein Solvent-Accessibility Prediction by a Stacked Deep Bidirectional Recurrent Neural Network","abstract":"Residue solvent accessibility is closely related to the spatial arrangement and packing of residues. Predicting the solvent accessibility of a protein is an important step to understand its structure and function. In this work, we present a deep learning method to predict residue solvent accessibility, which is based on a stacked deep bidirectional recurrent neural network applied to sequence profiles. To capture more long-range sequence information, a merging operator was proposed when bidirectional information from hidden nodes was merged for outputs. Three types of merging operators were used in our improved model, with a long short-term memory network performing as a hidden computing node. The trained database was constructed from 7361 proteins extracted from the PISCES server using a cut-off of 25% sequence identity. Sequence-derived features including position-specific scoring matrix, physical properties, physicochemical characteristics, conservation score and protein coding were used to represent a residue. Using this method, predictive values of continuous relative solvent-accessible area were obtained, and then, these values were transformed into binary states with predefined thresholds. Our experimental results showed that our deep learning method improved prediction quality relative to current methods, with mean absolute error and Pearson's correlation coefficient values of 8.8% and 74.8%, respectively, on the CB502 dataset and 8.2% and 78%, respectively, on the Manesh215 dataset.","keywords_author":["solvent-accessibility prediction","bidirectional recurrent network","sequence profile","merging operator"],"keywords_other":["MULTIPLE SEQUENCE ALIGNMENT","REGRESSION","INFORMATION","SUPPORT VECTOR MACHINE","RECOGNITION","SECONDARY STRUCTURE PREDICTION","MEMBRANE-PROTEINS","REAL-VALUE PREDICTION","GLOBULAR-PROTEINS","SURFACE"],"max_cite":0.0,"pub_year":2018.0,"sources":"['wos']","rawkeys":["bidirectional recurrent network","recognition","globular-proteins","solvent-accessibility prediction","merging operator","surface","sequence profile","information","secondary structure prediction","real-value prediction","support vector machine","regression","multiple sequence alignment","membrane-proteins"],"tags":["bidirectional recurrent network","surfaces","recognition","globular-proteins","solvent-accessibility prediction","merging operator","machine learning","sequence profile","information","secondary structure prediction","real-value prediction","regression","multiple sequence alignment","membrane proteins"]},{"p_id":21548,"title":"Predicting web server crashes: A case study in comparing prediction algorithms","abstract":"Traditionally, performance has been the most important metrics when evaluating a system. However, in the last decades industry and academia have been paying increasing attention to another metric to evaluate servers: availability. A web server may serve many users when running, but if it is out of service too much time, it becomes useless and expensive. The industry has adopted several techniques to improve system availability, yet crashes still happen. In this paper, we propose a new framework to predict time-to-failure when the system is suffering transient failures that consume resources randomly. We study which machine learning algorithms build a more accurate model of the behavior of the anomaly system, and focus on Linear Regression and Decision Tree algorithms. Our preliminary results show that M5P (a Decision Tree algorithm) is the best option to model the behavior of the system under the random injection of memory leaks. \u00a9 2009 IEEE.","keywords_author":["Dependability","High-availability","Machine learning","Prediction algorithms"],"keywords_other":["System availability","Web servers","Time to failure","Decision-tree algorithm","Machine learning","Machine learning algorithms","High-availability","Prediction algorithms","Memory leaks","Dependability"],"max_cite":14.0,"pub_year":2009.0,"sources":"['scp']","rawkeys":["memory leaks","machine learning algorithms","system availability","machine learning","web servers","high-availability","prediction algorithms","decision-tree algorithm","time to failure","dependability"],"tags":["time to failure","recognition","software aging","machine learning algorithms","system availability","machine learning","prediction algorithms","decision-tree algorithm","high availability","web server"]},{"p_id":103468,"title":"Improving protein disorder prediction by deep bidirectional long short-term memory recurrent neural networks","abstract":"Motivation: Capturing long-range interactions between structural but not sequence neighbors of proteins is a long-standing challenging problem in bioinformatics. Recently, long short-term memory (LSTM) networks have significantly improved the accuracy of speech and image classification problems by remembering useful past information in long sequential events. Here, we have implemented deep bidirectional LSTM recurrent neural networks in the problem of protein intrinsic disorder prediction.","keywords_author":null,"keywords_other":["SECONDARY STRUCTURE","ACCURATE PREDICTION","SEQUENCE-BASED PREDICTION","ARCHITECTURES","REGION PREDICTIONS","DATABASE","RECOGNITION","BINDING","INTRINSICALLY UNSTRUCTURED PROTEINS","BACKPROPAGATION"],"max_cite":9.0,"pub_year":2016.0,"sources":"['ieee', 'wos']","rawkeys":["recognition","intrinsically unstructured proteins","architectures","region predictions","database","binding","secondary structure","accurate prediction","backpropagation","sequence-based prediction"],"tags":["recognition","intrinsically unstructured proteins","region predictions","databases","binding","secondary structure","accurate prediction","architecture","backpropagation","sequence-based prediction"]},{"p_id":103469,"title":"Structural vaccinology considerations for in silico designing of a multi-epitope vaccine","abstract":"Multi-epitope peptide vaccines, as a kind of fusion proteins, usually possess a string-of-beads structure, consisting of several peptidic epitopes, probably adjuvants and linkers. Very numerous options are possible in selecting the order of different segments and linkers. Such factors can affect the vaccine efficacy through impacting physicochemical characteristics and protein tertiary structure.","keywords_author":["Multi-epitope vaccine","String-of-beads vaccine","Structural vaccinology","Linker","HPV vaccine","Bioinformatics"],"keywords_other":["PROTEIN SECONDARY STRUCTURE","ESCHERICHIA-COLI","PEPTIDE VACCINE","WEB SERVER","ACCESSIBLE SURFACE-AREA","NEURAL-NETWORKS","RECOGNITION","IMMUNOINFORMATICS","LINKER LENGTH","STRUCTURE PREDICTION"],"max_cite":1.0,"pub_year":2018.0,"sources":"['wos']","rawkeys":["neural-networks","linker","immunoinformatics","hpv vaccine","recognition","multi-epitope vaccine","protein secondary structure","string-of-beads vaccine","structural vaccinology","structure prediction","escherichia-coli","bioinformatics","peptide vaccine","accessible surface-area","web server","linker length"],"tags":["linker","immunoinformatics","recognition","string-of-beads vaccine","multi-epitope vaccine","protein secondary structure","neural networks","structural vaccinology","structure prediction","escherichia-coli","bioinformatics","peptide vaccine","accessible surface-area","hpv vaccines","web server","linker length"]},{"p_id":29766,"title":"Hierarchical spatiotemporal feature extraction using recurrent online clustering","abstract":"Deep machine learning offers a comprehensive framework for extracting meaningful features from complex observations in an unsupervised manner. The majority of deep learning architectures described in the literature primarily focus on extracting spatial features. However, in real-world settings, capturing temporal dependencies in observations is critical for accurate inference. This paper introduces an enhancement to DeSTIN - a compositional deep learning architecture in which each layer consists of multiple instantiations of a common node - that learns to represent spatiotemporal patterns in data based on a novel recurrent clustering algorithm. Contrary to mainstream deep architectures, such as deep belief networks where layer-by-layer training is assumed, each of the nodes in the proposed architecture is trained independently and in parallel. Moreover, top-down and bottom-up information flows facilitate rich feature formation. A semi-supervised setting is demonstrated achieving state-of-the-art results on the MNIST classification benchmarks. A GPU implementation is discussed further accentuating the scalability properties of the proposed framework. \u00a9 2013 Elsevier Ltd. All rights reserved.","keywords_author":["Deep machine learning","Online clustering","Pattern recognition","Recurrent clustering","Spatiotemporal signals","Unsupervised feature extraction","Deep machine learning","Online clustering","Recurrent clustering","Unsupervised feature extraction","Spatiotemporal signals","Pattern recognition"],"keywords_other":["Online-clustering","Spatio temporal features","Spatiotemporal patterns","FACE","Deep architectures","Deep belief networks","Spatio-temporal signals","RECOGNITION","Recurrent clustering","Proposed architectures"],"max_cite":4.0,"pub_year":2014.0,"sources":"['ieee', 'scp', 'wos']","rawkeys":["deep machine learning","spatiotemporal signals","recognition","spatio-temporal signals","proposed architectures","unsupervised feature extraction","deep architectures","recurrent clustering","spatiotemporal patterns","online-clustering","pattern recognition","face","spatio temporal features","deep belief networks","online clustering"],"tags":["deep machine learning","recognition","spatio-temporal signals","proposed architectures","unsupervised feature extraction","deep architectures","recurrent clustering","spatiotemporal patterns","online-clustering","pattern recognition","face","spatio temporal features","deep belief networks"]},{"p_id":70730,"title":"Emotion detection from text and speech: a survey","abstract":"Emotion recognition has emerged as an important research area which may reveal some valuable input to a variety of purposes. People express their emotions directly or indirectly through their speech, facial expressions, gestures or writings. Many different sources of information, such as speech, text and visual can be used to analyze emotions. Nowadays, writings take many forms of social media posts, micro-blogs, news articles, etc., and the content of these posts can be useful resource for text mining to discover and unhide various aspects, including emotions. Extracting emotions behind these postings is an immense and complicated task. To tackle this problem, researchers from diverse fields are trying to find an efficient way to more precisely detect human emotions from various sources, including text and speech. In this sense, different word-based and sentence-based techniques, machine learning, natural language processing methods, etc., have been used to achieve better accuracy. Analyzing emotions can be helpful in many different domains. One such domain is human computer interaction. With the help of emotion recognition, computers can make better decisions to help users. With the increase in popularity of robotic research, emotion recognition will also help making human-robot interaction more natural. This survey covers existing emotion detection research efforts, emotion models, emotion datasets, emotion detection techniques, their features, limitations and some possible future directions. We focus on reviewing research efforts analyzing emotions based on text and speech. We investigated different feature sets that have been used in existing methodologies. We summarize basic achievements in the field and highlight possible extensions for better outcome.","keywords_author":["Emotion","Text","Emotion models","Emotion recognition","Emotion analysis","Speech","Classifiers"],"keywords_other":["FEATURES","EXTRACTION","MODEL","RECOGNITION","SENTIMENT","CLASSIFIERS","FACIAL EXPRESSION"],"max_cite":0.0,"pub_year":2018.0,"sources":"['ieee', 'wos']","rawkeys":["recognition","model","emotion","features","classifiers","emotion models","text","emotion recognition","speech","facial expression","sentiment","emotion analysis","extraction"],"tags":["recognition","emotion modeling","model","emotion","features","facial expressions","text","emotion recognition","classifier","speech","sentiment","emotion analysis","extraction"]},{"p_id":70731,"title":"Discriminative feature representation for image classification via multimodal multitask deep neural networks","abstract":"A good image feature representation is crucial for image classification tasks. Many traditional applications have attempted to design single-modal features for image classification; however, these may have difficulty extracting sufficient information, resulting in misjudgments for various categories. Recently, researchers have focused on designing multimodal features, which have been successfully employed in many situations. However, there are still some problems in this research area, including selecting efficient features for each modality, transforming them to the subspace feature domain, and removing the heterogeneities among different modalities. We propose an end-to-end multimodal deep neural network (MDNN) framework to automate the feature selection and transformation procedures for image classification. Furthermore, inspired by Fisher's theory of linear discriminant analysis, we improve the proposed MDNN by further proposing a multimodal multitask deep neural network (M2DNN) model. The motivation behind M2DNN is to improve the classification performance by incorporating an auxiliary discriminative constraint to the subspace representation. Experimental results on five representative datasets (NUS-WIDE, Scene-15, Texture-25, Indoor-67, and Caltech-101) demonstrate the effectiveness of the proposed MDNN and M2DNN models. In addition, experimental comparisons of the Fisher score criterion exhibit that M2DNN is more robust and has better discriminative power than other approaches. (C) 2017 SPIE and IS&T","keywords_author":["feature representation","feature selection","feature transformation","deep neural networks"],"keywords_other":["RECOGNITION"],"max_cite":1.0,"pub_year":2017.0,"sources":"['wos']","rawkeys":["recognition","feature transformation","deep neural networks","feature selection","feature representation"],"tags":["recognition","feature selection","convolutional neural network","feature transformations","feature representation"]},{"p_id":70733,"title":"Multimedia Big Data Analytics: A Survey","abstract":"With the proliferation of online services and mobile technologies, the world has stepped into a multimedia big data era. A vast amount of research work has been done in the multimedia area, targeting different aspects of big data analytics, such as the capture, storage, indexing, mining, and retrieval of multimedia big data. However, very few research work provides a complete survey of the whole pine-line of the multimedia big data analytics, including the management and analysis of the large amount of data, the challenges and opportunities, and the promising research directions. To serve this purpose, we present this survey, which conducts a comprehensive overview of the state-of-the-art research work on multimedia big data analytics. It also aims to bridge the gap between multimedia challenges and big data solutions by providing the current big data frameworks, their applications in multimedia analyses, the strengths and limitations of the existing methods, and the potential future directions in multimedia big data analytics. To the best of our knowledge, this is the first survey that targets the most recent multimedia management techniques for very large-scale data and also provides the research studies and technologies advancing the multimedia analyses in this big data era.","keywords_author":["Algorithms","Design","Management","Performance","Big data analytics","multimedia analysis","5V challenges","multimedia databases","indexing","retrieval","machine learning","data mining","mobile multimedia","survey"],"keywords_other":["AWARE","DATA-MANAGEMENT","CLOUD","DEEP FEATURE","FRAMEWORK","FEATURE-SELECTION","RECOGNITION","IMAGE RETRIEVAL","VIDEO","AFFINITY HYBRID TREE"],"max_cite":0.0,"pub_year":2018.0,"sources":"['wos']","rawkeys":["indexing","design","performance","feature-selection","multimedia analysis","aware","survey","mobile multimedia","machine learning","video","algorithms","affinity hybrid tree","data mining","recognition","framework","image retrieval","deep feature","multimedia databases","management","big data analytics","cloud","retrieval","5v challenges","data-management"],"tags":["design","performance","multimedia analysis","data management","multimedia database","survey","mobile multimedia","machine learning","index","video","feature selection","algorithms","affinity hybrid tree","data mining","recognition","framework","image retrieval","management","big data analytics","cloud","retrieval","deep features","awareness","5v challenges"]},{"p_id":70736,"title":"Real-Time Facial Features Detection from Low Resolution Thermal Images with Deep Classification Models","abstract":"Deep networks have already shown a spectacular success for object classification and detection for various applications from everyday use cases to advanced medical problems. The main advantage of the classification models over the detection models is less time and effort needed for dataset preparation, because classification networks do not require bounding box annotations, but labels at the image level only. Yet, after passing the image through a stack of convolutions followed by stride and pooling operations the full image is reduced into a single vector of class probabilities and the spatial arrangement of pixels is completely lost. Our proposed approach shows how to localize objects by restoring the spatial information about features distribution from the classification network. The presented modifications of architecture are limited to the inference phase only. As a result, we are able to combine the simplicity and short time required for dataset preparation with the results similar to the output from the detection model. Additionally, we showcased that it is possible to repurpose the existing network, originally trained on RGB images, to a novel task of facial features detection from low resolution thermal images, while preserving the high precision (>99%). The proposed facial features detection system can be potentially combined with wearable devices that will collect data and send it to the server for more computationally expensive analysis, e.g., calculation of the respiratory rate form the detected nostril area. Real time performance and small resource utilization proved that the presented approach can be used to serve multiple patients without any impact on the latency, e.g., as a centralized monitoring station for the home healthcare. The combination of wearable devices with machine learning algorithms run on the remote and more powerful platforms could revolutionize the practice of medicine by delivering healthcare to patients anywhere in the world.","keywords_author":["Deep Learning","Neural Network","Transfer Learning","Thermography","Object Detection","Remote Healthcare"],"keywords_other":["RECOGNITION","VIDEO","FACE DETECTION"],"max_cite":0.0,"pub_year":2018.0,"sources":"['wos']","rawkeys":["neural network","recognition","deep learning","transfer learning","remote healthcare","face detection","thermography","video","object detection"],"tags":["recognition","neural networks","transfer learning","machine learning","remote healthcare","face detection","thermography","video","object detection"]},{"p_id":70737,"title":"Random Deep Belief Networks for Recognizing Emotions from Speech Signals","abstract":"Now the human emotions can be recognized from speech signals using machine learning methods; however, they are challenged by the lower recognition accuracies in real applications due to lack of the rich representation ability. Deep belief networks (DBN) can automatically discover the multiple levels of representations in speech signals. To make full of its advantages, this paper presents an ensemble of random deep belief networks (RDBN) method for speech emotion recognition. It firstly extracts the low level features of the input speech signal and then applies them to construct lots of random subspaces. Each random subspace is then provided for DBN to yield the higher level features as the input of the classifier to output an emotion label. All outputted emotion labels are then fused through the majority voting to decide the final emotion label for the input speech signal. The conducted experimental results on benchmark speech emotion databases show that RDBN has better accuracy than the compared methods for speech emotion recognition.","keywords_author":null,"keywords_other":["ENSEMBLE","SPECTRAL FEATURES","CLASSIFICATION","SUPPORT VECTOR MACHINES","NEURAL-NETWORKS","RECOGNITION","CLASSIFIERS"],"max_cite":3.0,"pub_year":2017.0,"sources":"['ieee', 'wos']","rawkeys":["neural-networks","recognition","ensemble","classifiers","spectral features","classification","support vector machines"],"tags":["recognition","neural networks","machine learning","ensemble","classifier","classification","spectral feature"]},{"p_id":13392,"title":"Dictionary Learning with Low Computational Complexity for Classification of Human Micro-Dopplers Across Multiple Carrier Frequencies","abstract":"Recently, several machine learning algorithms have been applied for classifying micro Doppler signatures from different human motions. However, these algorithms must demonstrate versatility in handling diversity in test and training data to be used for real-life scenarios. For example, situations may arise where the propagation channel or the presence of interference sources in the test site will permit only specific frequency bands of radar operation. These bands may differ from those used previously while training. In this paper, we examine the performances of three sparsity driven dictionary learning algorithms synthesis, deep, and analysis for learning unique features extracted from training data gathered across multiple carrier frequencies. These features are subsequently used for classifying test data from another distinct carrier frequency. Our experimental results, from measurement data, show that the dictionary learning algorithms are capable of extracting meaningful representations of the micro-Dopplers despite the rich frequency diversity in the data. In particular, the deep dictionary learning algorithm yields a high classification accuracy of 91% with a very low computational time for testing.","keywords_author":["analysis dictionary learning","classification","deep learning","micro-Dopplers","Radar","sparse coding","synthesis dictionary learning","Radar","micro-Dopplers","sparse coding","synthesis dictionary learning","deep learning","analysis dictionary learning","classification"],"keywords_other":["SELECTION","Dictionary learning","Training data","ORTHOGONAL MATCHING PURSUIT","PERSONNEL","micro-Dopplers","SIGNAL RECOVERY","RADAR","RECOGNITION","Classification algorithm","SOURCE SEPARATION","SPARSE REPRESENTATION","SIGNATURES","EMPIRICAL MODE DECOMPOSITION","Sparse coding"],"max_cite":0.0,"pub_year":2018.0,"sources":"['scp', 'wos']","rawkeys":["classification","training data","signatures","orthogonal matching pursuit","signal recovery","empirical mode decomposition","recognition","analysis dictionary learning","deep learning","sparse representation","dictionary learning","selection","sparse coding","synthesis dictionary learning","source separation","micro-dopplers","personnel","radar","classification algorithm"],"tags":["classification","training data","machine learning","signature","orthogonal matching pursuit","signal recovery","micro-doppler","empirical mode decomposition","recognition","analysis dictionary learning","sparse representation","dictionary learning","selection","sparse coding","synthesis dictionary learning","source separation","personnel","radar","classification algorithm"]},{"p_id":111701,"title":"Novel Leakage Detection by Ensemble CNN-SVM and Graph-Based Localization in Water Distribution Systems","abstract":"In many water distribution systems, a significant amount of water is lost because of leakage during transit from the water treatment plant to consumers. As a result, water leakage detection and localization have been a consistent focus of research. Typically, diagnosis or detection systems based on sensor signals incur significant computational and time costs, whereas the system performance depends on the features selected as input to the classifier. In this paper, to solve this problem, we propose a novel, fast, and accurate water leakage detection system with an adaptive design that fuses a one-dimensional convolutional neural network and a support vector machine. We also propose a graph-based localization algorithm to determine the leakage location. An actual water pipeline network is represented by a graph network and it is assumed that leakage events occur at virtual points on the graph. The leakage location at which costs are minimized is estimated by comparing the actual measured signals with the virtually generated signals. The performance was validated on a wireless sensor network based test bed, deployed on an actual WDS. Our proposed methods achieved 99.3% leakage detection accuracy and a localization error of less than 3 m.","keywords_author":["Ensemble convolutional neural network (CNN) and support vector machine (SVM)","leakage detection","one-dimensional (1-D) CNNs","pipeline network localization"],"keywords_other":["RECOGNITION","CLASSIFICATION","CONVOLUTIONAL NEURAL-NETWORKS"],"max_cite":1.0,"pub_year":2018.0,"sources":"['ieee', 'wos']","rawkeys":["leakage detection","recognition","one-dimensional (1-d) cnns","ensemble convolutional neural network (cnn) and support vector machine (svm)","classification","convolutional neural-networks","pipeline network localization"],"tags":["leakage detection","recognition","one-dimensional (1-d) cnns","ensemble convolutional neural network (cnn) and support vector machine (svm)","classification","convolutional neural network","pipeline network localization"]},{"p_id":95328,"title":"Multi-task classification with sequential instances and tasks","abstract":"In this paper, we propose a novel multi-task classification framework, called Multi-Task classification with Sequential Instances and Tasks (MTSIT). Different from previous works, which treat all tasks and instances equally, MTSIT is inspired by the cognitive process of human brain that often learns from easier tasks to harder tasks. Specifically, the method attempts to jointly learn the task curriculum (learning order of tasks) and the instance curriculum (learning order of instances) by introducing a self-paced item for the instances of each task in the existing multi-task learning framework Sequential Multi-Task learning (SeqMT), which transfers information from the previously learned tasks to the next ones through shared task parameters. To effectively solve MTSIT, we also propose an optimization algorithm in which the instance curriculum and the task curriculum alternate between two paradigms, Tasks-to-Instances and Instances-to-Tasks (TILT). In the tasks-to-instances step, the learner conducts the instance curriculum when the task curriculum has been fixed, while in the instances-to tasks step, the task curriculum is learned when the instance curriculum in each task has been settled down. Our TIIT method is based on an error bound of the proposed MTSIT. Experimental results on three real world datasets demonstrate the effectiveness of our method.","keywords_author":["Classification","Multi-task learning","Curriculum learning","Self-paced learning"],"keywords_other":["RECOGNITION"],"max_cite":0.0,"pub_year":2018.0,"sources":"['wos']","rawkeys":["multi-task learning","curriculum learning","recognition","classification","self-paced learning"],"tags":["curriculum learning","recognition","multitask learning","classification","self-paced learning"]},{"p_id":21619,"title":"Ducklings imprint on the relational concept of \"same or different\"","abstract":"The ability to identify and retain logical relations between stimuli and apply themto novel stimuli is known as relational concept learning.This has been demonstrated in a few animal species after extensive reinforcement training, and it reveals the brain's ability to deal with abstract properties. Here we describe relational concept learning in newborn ducklings without reinforced training. Newly hatched domesticatedmallards that were briefly exposed to a pair of objects that were either the same or different in shape or color later preferred to follow pairs of new objects exhibiting the imprinted relation.Thus, even in a seemingly rigid and very rapid form of learning such as filial imprinting, the brain operates with abstract conceptual reasoning, a faculty often assumed to be reserved to highly intelligent organisms.","keywords_author":null,"keywords_other":["Concept Formation","Pattern Recognition, Visual","Animals, Newborn","Animals","Ducks","Color Perception","Reinforcement (Psychology)","Imprinting (Psychology)"],"max_cite":14.0,"pub_year":2016.0,"sources":"['scp']","rawkeys":["ducks","reinforcement (psychology)","concept formation","newborn","color perception","imprinting (psychology)","pattern recognition","visual","animals"],"tags":["ducks","recognition","concept formation","newborn","color perception","pattern recognition","visualization","animals"]},{"p_id":13428,"title":"Deep Haar scattering networks","abstract":"An orthogonal Haar scattering transform is a deep network computed with a hierarchy of additions, subtractions and absolute values over pairs of coefficients. Unsupervised learning optimizes Haar pairs to obtain sparse representations of training data with an algorithm of polynomial complexity. For signals defined on a graph, a Haar scattering is computed by cascading orthogonal Haar wavelet transforms on the graph, with Haar wavelets having connected supports. It defines a representation which is invariant to local displacements of signal values on the graph. When the graph connectivity is unknown, unsupervised Haar learning can provide a consistent estimation of connected wavelet supports. Classification results are given on image data bases, defined on regular grids or graphs, with a connectivity which may be known or unknown.","keywords_author":["deep learning","neural network","scattering transform","Haar wavelet","classification","images","graphs"],"keywords_other":["GRAPHS","INVARIANT SCATTERING","NEURAL-NETWORKS","RECOGNITION","LEARNING ALGORITHM","FIELD"],"max_cite":2.0,"pub_year":2016.0,"sources":"['ieee', 'wos']","rawkeys":["invariant scattering","neural-networks","neural network","haar wavelet","graphs","images","recognition","deep learning","field","scattering transform","classification","learning algorithm"],"tags":["invariant scattering","recognition","images","scattering transforms","neural networks","field","graph","machine learning","classification","haar wavelets","learning algorithm"]},{"p_id":70776,"title":"On using supervised clustering analysis to improve classification performance","abstract":"During the past decade, graph-based learning methods have proved to be an effective tool to make full use of both labeled and unlabeled data samples to improve learning performance. These methods try to discover the intrinsic structures and discriminative information embedded in the data, by building one or more graphs to model the relationship among the data samples. Consequently, how to build an effective graph is the core problem. In this paper we introduce a novel graph-based classification method, called Supervised clustering-based Regularized Least Squares Classification (SuperRLSC), in which local and global graphs of the data are built by supervised clustering. The motivation is that supervised clustering may discover more actual data structures compared to unsupervised clustering. In our algorithm, we firstly employ supervised k-means to partition the whole training dataset into several meaningful clusters in order to discover the intrinsic and discriminative structures. We then use the discovered structures to build local and global graphs of the data. The local graph reveals local geometric and discriminative structures, while the global graph reveals global discriminative information. Finally a hybrid local\/global graph-based regularization term is embedded into supervised classification (i.e., RLSC in this paper). To validate the effectiveness of our algorithm, a series of experiments are performed on several UCI benchmark datasets. The results show that our algorithm can achieve better or at least comparable performance to the other graph-based algorithms and the traditional state-of-the-art supervised classification methods. (C) 2018 Elsevier Inc. All rights reserved.","keywords_author":["Supervised classification","Supervised clustering","Regularized Least Squares Classification","k-means"],"keywords_other":["MANIFOLD REGULARIZATION","RECOGNITION","FRAMEWORK"],"max_cite":0.0,"pub_year":2018.0,"sources":"['wos']","rawkeys":["k-means","supervised classification","recognition","framework","regularized least squares classification","manifold regularization","supervised clustering"],"tags":["supervised classification","recognition","framework","manifold regularization","kernel methods","supervised clustering","regularized least-squares classification"]},{"p_id":70779,"title":"Case study of 3D fingerprints applications","abstract":"Human fingers are 3D objects. More information will be provided if three dimensional (3D) fingerprints are available compared with two dimensional (2D) fingerprints. Thus, this paper firstly collected 3D finger point cloud data by Structured-light Illumination method. Additional features from 3D fingerprint images are then studied and extracted. The applications of these features are finally discussed. A series of experiments are conducted to demonstrate the helpfulness of 3D information to fingerprint recognition. Results show that a quick alignment can be easily implemented under the guidance of 3D finger shape feature even though this feature does not work for fingerprint recognition directly. The newly defined distinctive 3D shape ridge feature can be used for personal authentication with Equal Error Rate (EER) of similar to 8.3%. Also, it is helpful to remove false core point. Furthermore, a promising of EER similar to 1.3% is realized by combining this feature with 2D features for fingerprint recognition which indicates the prospect of 3D fingerprint recognition.","keywords_author":null,"keywords_other":["REGRESSION","PROFILOMETRY","SYSTEM","ALIGNMENT","RECOGNITION","RECONSTRUCTION"],"max_cite":0.0,"pub_year":2017.0,"sources":"['ieee', 'wos']","rawkeys":["alignment","recognition","system","reconstruction","profilometry","regression"],"tags":["alignment","recognition","system","reconstruction","profilometry","regression"]},{"p_id":111750,"title":"Metal oxide resistive switching memory: Materials, properties and switching mechanisms","abstract":"With the continuously changing landscape of the computer technologies, a new memory type is needed that will be fast, energy efficient and long-lasting. It shall combine the speed of random access memory (RAM) and nonvolatile in the same time. Resistive RAM (RRAM) is one of the most promising candidates in this respect. RRAM has attracted a great deal of attention owing to its potential as a possible replacement for flash memory in next-generation nonvolatile memory (NVM) applications. A brief summary of binary metal oxide RRAM is given in this review. We discuss the RRAM technology development based on published papers, including the mechanism of resistive switching in transition metal oxides, resistive switching materials, device structure, properties, and reliability such as endurance and retention of the device. We also provide possible solutions through innovations in device materials, structures, and understanding the device physics.","keywords_author":["REAM","Conduction mechanism"],"keywords_other":["DEVICES","TOP ELECTRODE","POLYCRYSTALLINE NIO FILMS","LAYER","THIN-FILMS","DEPENDENCE","RRAM","NONVOLATILE MEMORY","TEMPERATURE"],"max_cite":6.0,"pub_year":2017.0,"sources":"['wos']","rawkeys":["devices","polycrystalline nio films","ream","top electrode","rram","conduction mechanism","layer","thin-films","dependence","temperature","nonvolatile memory"],"tags":["recognition","devices","polycrystalline nio films","ream","top electrode","non-volatile memory","conduction mechanism","layer","thin-films","temperature","resistive switching memory"]},{"p_id":111762,"title":"Neuromorphic computing with multi-memristive synapses","abstract":"Neuromorphic computing has emerged as a promising avenue towards building the next generation of intelligent computing systems. It has been proposed that memristive devices, which exhibit history-dependent conductivity modulation, could efficiently represent the synaptic weights in artificial neural networks. However, precise modulation of the device conductance over a wide dynamic range, necessary to maintain high network accuracy, is proving to be challenging. To address this, we present a multi-memristive synaptic architecture with an efficient global counter-based arbitration scheme. We focus on phase change memory devices, develop a comprehensive model and demonstrate via simulations the effectiveness of the concept for both spiking and non-spiking neural networks. Moreover, we present experimental results involving over a million phase change memory devices for unsupervised learning of temporal correlations using a spiking neural network. The work presents a significant step towards the realization of large-scale and energy-efficient neuromorphic computing systems.","keywords_author":null,"keywords_other":["PLASTICITY","DEVICES","NEURONS","PHASE","NEURAL-NETWORKS","RECOGNITION","LTP","LONG-TERM POTENTIATION","MEMORY"],"max_cite":0.0,"pub_year":2018.0,"sources":"['wos']","rawkeys":["neural-networks","recognition","memory","ltp","devices","plasticity","phase","long-term potentiation","neurons"],"tags":["recognition","memory","devices","neural networks","plasticity","phase","long-term potentiation","neurons"]},{"p_id":111788,"title":"Unsupervised Feature Learning With Winner-Takes-All Based STDP","abstract":"We present a novel strategy for unsupervised feature learning in image applications inspired by the Spike-Timing-Dependent-Plasticity (STDP) biological learning rule. We show equivalence between rank order coding Leaky-Integrate-and-Fire neurons and ReLU artificial neurons when applied to non-temporal data. We apply this to images using rank-order coding, which allows us to perform a full network simulation with a single feed-forward pass using GPU hardware. Next we introduce a binary STDP learning rule compatible with training on batches of images. Two mechanisms to stabilize the training are also presented : a Winner-Takes-All (WTA) framework which selects the most relevant patches to learn from along the spatial dimensions, and a simple feature-wise normalization as homeostatic process. This learning process allows us to train multi-layer architectures of convolutional sparse features. We apply our method to extract features from the MNIST, ETH80, CIFAR-10, and STL-10 datasets and show that these features are relevant for classification. We finally compare these results with several other state of the art unsupervised learning methods.","keywords_author":["Spike-Timing-Dependent-Pasticity","neural network","unsupervised learning","winner-takes-all","vision"],"keywords_other":["PLASTICITY","ONE SPIKE","RECOGNITION","OBJECT","TIME","CATEGORIZATION","NEURON"],"max_cite":0.0,"pub_year":2018.0,"sources":"['wos']","rawkeys":["neural network","recognition","object","neuron","winner-takes-all","spike-timing-dependent-pasticity","plasticity","categorization","time","vision","unsupervised learning","one spike"],"tags":["recognition","winner-take-all","neural networks","objects","plasticity","categorization","spike-timing-dependent-pasticity","time","unsupervised learning","vision","neurons","one spike"]},{"p_id":21678,"title":"High-Density Electromyography and Motor Skill Learning for Robust Long-Term Control of a 7-DoF Robot Arm","abstract":"\u00a9 2001-2011 IEEE. Myoelectric control offers a direct interface between human intent and various robotic applications through recorded muscle activity. Traditional control schemes realize this interface through direct mapping or pattern recognition techniques. The former approach provides reliable control at the expense of functionality, while the latter increases functionality at the expense of long-term reliability. An alternative approach, using concepts of motor learning, provides session-independent simultaneous control, but previously relied on consistent electrode placement over biomechanically independent muscles. This paper extends the functionality and practicality of the motor learning-based approach, using high-density electrode grids and muscle synergy-inspired decomposition to generate control inputs with reduced constraints on electrode placement. The method is demonstrated via real-time simultaneous and proportional control of a 4-DoF myoelectric interface over multiple days. Subjects showed learning trends consistent with typical motor skill learning without requiring any retraining or recalibration between sessions. Moreover, they adjusted to physical constraints of a robot arm after learning the control in a constraint-free virtual interface, demonstrating robust control as they performed precision tasks. The results demonstrate the efficacy of the proposed man-machine interface as a viable alternative to conventional control schemes for myoelectric interfaces designed for long-term use.","keywords_author":["Electromyography (EMG)","high-density electromyography (EMG)","human-robot interaction","motor learning","myoelectric control","prosthetics","real-time systems","simultaneous control"],"keywords_other":["Man machine interface","Learning","Humans","Proportional control","Movement","Computer Systems","Electromyography","Man-Machine Systems","Muscle, Skeletal","Muscle Contraction","Robotic applications","Motor learning","Algorithms","Myoelectric control","Robotics","Motor skill learning","Male","Young Adult","Motor Skills","Simultaneous control","Adult","Pattern recognition techniques","Biofeedback, Psychology"],"max_cite":14.0,"pub_year":2016.0,"sources":"['scp', 'wos']","rawkeys":["movement","human-robot interaction","young adult","adult","muscle","motor skill learning","electromyography","electromyography (emg)","real-time systems","muscle contraction","robotic applications","computer systems","algorithms","psychology","prosthetics","learning","humans","myoelectric control","high-density electromyography (emg)","man-machine systems","man machine interface","pattern recognition techniques","male","motor learning","motor skills","skeletal","robotics","simultaneous control","proportional control","biofeedback"],"tags":["man-machine interfaces","movement","human-robot interaction","young adult","adult","muscle","motor skill learning","electromyography","computational system","machine learning","real-time systems","muscle contraction","robotic applications","algorithms","prosthetics","recognition","humans","myoelectric control","high-density electromyography (emg)","man-machine systems","pattern recognition techniques","male","motor learning","motor skills","skeletal","robotics","simultaneous control","proportional control","biofeedback"]},{"p_id":46257,"title":"Stacked sparse auto-encoders (SSAE) based electronic nose for chinese liquors classification","abstract":"\u00a9 2017 by the authors. Licensee MDPI, Basel, Switzerland. This paper presents a stacked sparse auto-encoder (SSAE) based deep learning method for an electronic nose (e-nose) system to classify different brands of Chinese liquors. It is well known that preprocessing; feature extraction (generation and reduction) are necessary steps in traditional data-processing methods for e-noses. However, these steps are complicated and empirical because there is no uniform rule for choosing appropriate methods from many different options. The main advantage of SSAE is that it can automatically learn features from the original sensor data without the steps of preprocessing and feature extraction; which can greatly simplify data processing procedures for e-noses. To identify different brands of Chinese liquors; an SSAE based multi-layer back propagation neural network (BPNN) is constructed. Seven kinds of strong-flavor Chinese liquors were selected for a self-designed e-nose to test the performance of the proposed method. Experimental results show that the proposed method outperforms the traditional methods.","keywords_author":["Chinese liquors classification","Deep learning","Electronic nose","Stacked sparse auto-encoders","stacked sparse auto-encoders","electronic nose","deep learning","Chinese liquors classification"],"keywords_other":["Back-propagation neural networks","Learning methods","Processing procedures","ALGORITHM","RECOGNITION","Electronic nose (e-nose)","Auto encoders","Sensor data","DEEP NEURAL-NETWORK","Chinese liquors","Data processing methods"],"max_cite":0.0,"pub_year":2017.0,"sources":"['ieee', 'scp', 'wos']","rawkeys":["algorithm","data processing methods","chinese liquors classification","recognition","sensor data","stacked sparse auto-encoders","deep learning","learning methods","deep neural-network","auto encoders","chinese liquors","electronic nose (e-nose)","processing procedures","back-propagation neural networks","electronic nose"],"tags":["data processing methods","chinese liquors classification","recognition","sensor data","back propagation neural networks","stacked sparse autoencoder","learning methods","auto encoders","chinese liquors","machine learning","processing procedures","convolutional neural network","algorithms","electronic nose"]},{"p_id":111796,"title":"A 4-fJ\/Spike Artificial Neuron in 65 nm CMOS Technology","abstract":"As Moore's law reaches its end, traditional computing technology based on the Von Neumann architecture is facing fundamental limits. Among them is poor energy efficiency. This situation motivates the investigation of different processing information paradigms, such as the use of spiking neural networks (SNNs), which also introduce cognitive characteristics. As applications at very high scale are addressed, the energy dissipation needs to be minimized. This effort starts from the neuron cell. In this context, this paper presents the design of an original artificial neuron, in standard 65 nm CMOS technology with optimized energy efficiency. The neuron circuit response is designed as an approximation of the Morris-Lecar theoretical model. In order to implement the non-linear gating variables, which control the ionic channel currents, transistors operating in deep subthreshold are employed. Two different circuit variants describing the neuron model equations have been developed. The first one features spike characteristics, which correlate well with a biological neuron model. The second one is a simplification of the first, designed to exhibit higher spiking frequencies, targeting large scale bio-inspired information processing applications. The most important feature of the fabricated circuits is the energy efficiency of a few femtojoules per spike, which improves prior state-of-the-art by two to three orders of magnitude. This performance is achieved by minimizing two key parameters: the supply voltage and the related membrane capacitance. Meanwhile, the obtained standby power at a resting output does not exceed tens of picowatts. The two variants were sized to 200 and 35 mu m(2) with the latter reaching a spiking output frequency of 26 kHz. This performance level could address various contexts, such as highly integrated neuro-processors for robotics, neuroscience or medical applications.","keywords_author":["artificial neuron","Morris-Lecar neuron","CMOS","subthreshold","analog VLSI","spiking neural network"],"keywords_other":["SPIKING NEURONS","SILICON NEURON","NETWORKS","BEHAVIOR","DEPENDENCE","MODELS","SEIZURES","MEMBRANE CAPACITANCE","OSCILLATIONS","SYNAPSES"],"max_cite":2.0,"pub_year":2017.0,"sources":"['wos']","rawkeys":["morris-lecar neuron","artificial neuron","synapses","dependence","subthreshold","membrane capacitance","seizures","spiking neural network","cmos","spiking neurons","networks","models","silicon neuron","oscillations","analog vlsi","behavior"],"tags":["morris-lecar neuron","artificial neuron","recognition","model","spiking neural networks","subthreshold","synapses","membrane capacitance","seizures","cmos","spiking neurons","networks","silicon neuron","oscillations","analog vlsi","behavior"]},{"p_id":103667,"title":"Psychology Students' learning strategies: An exploratory study","abstract":"In this study we aim to characterize the learning strategies used by psychology undergraduates in their first to fourth year. The participants were 155 students who responded to the LASSI questionnaire and the Reading Strategies Scale. We analyzed the data using Analysis of variance (ANOVA), the Student's t test, and Pearson correlation. We observed that first-years had higher scores in categories regarding attitude, motivation, concentration, proof reading strategies and time management when compared to students of higher years. We identified significant positive association among all categories of study and reading strategies. We conclude that interventions on the use of motivational strategies to help students assess the value they attribute to their university, to their course, and to help them set goals, based on what the university and the course can provide them seems to be especially promising.","keywords_author":["Learning strategies","Study behavior","Higher education","Psychology"],"keywords_other":["UNIVERSITY-STUDENTS","ACHIEVEMENT"],"max_cite":0.0,"pub_year":2017.0,"sources":"['wos']","rawkeys":["study behavior","learning strategies","achievement","higher education","psychology","university-students"],"tags":["recognition","study behavior","achievement","higher-education","learning strategy","university-students"]},{"p_id":111869,"title":"Color pornographic image detection based on color-saliency preserved mixture deformable part model","abstract":"To utilize the rich semantic information of sexual organs, we propose a new framework for pornographic image detection based on sexual organ detectors. Traditional sexual organ detectors are built on shape features. Since the color distribution of sexual organ in same pose is consistent, color is an important visual clue to represent sexual organs. We use color attribute to describe the local color of sexual organs and concatenate it with histogram of oriented gradients based shape feature to represent sexual organs. Based on the concatenated feature, we train sexual organ detectors by the color-saliency preserved mixture deformable part model (CPMDPM). We detect pornographic images sequentially with sexual organ detectors. In experiments, the optimal part number of the deformable part model is chosen experimentally. We evaluate the performance of each CPMDPM based sexual organ detector, which is superior over the shape feature based detector. The proposed pornographic detection method is superior over methods based on low level features of skin regions, bag of words model and color incorporated SIFT features etc.","keywords_author":["Pornographic image detection","Deformable part model","Mixture model","Salient color distribution"],"keywords_other":["RECOGNITION"],"max_cite":0.0,"pub_year":2018.0,"sources":"['wos']","rawkeys":["pornographic image detection","recognition","salient color distribution","mixture model","deformable part model"],"tags":["pornographic image detection","recognition","salient color distribution","mixture models","deformable part models"]},{"p_id":38145,"title":"Inertia based recognition of daily activities with ANNs and spectrotemporal features","abstract":"\u00a9 2015 IEEE. As mobile and personal health devices gain in popularity, increasing amounts of data is collected via their embedded sensors such as heart rate monitors and accelerometers. Data analytics and more specifically machine learning algorithms can transform this data into actionable information to improve personal healthcare and quality of life. The main objective of this study is to develop an algorithmic classification framework using feed-forward multilayer perceptrons and statistically rich spectrotemporal features to recognize daily activities based on 3-axis acceleration data. A multitude of MLP topologies and setups, such as different numbers and sizes of hidden layers, supervised output structuring, etc. are tested to comprehensively analyze the clustering capabilities of the artificial neural network for a wide-range of settings. In addition, the contribution of subset of features to classification accuracy is studied to identify respective information potentials and further improve accuracy. Publicly available wrist-worn accelerometer dataset from University of California Irvine's machine learning repository is used for fair comparison with the most recent literature published using the same dataset. Results indicate a significant improvement in recognition rate where the overall accuracy over seven selected activity classes is 91% compared to 54% of the latest publication using the same dataset.","keywords_author":["Accelerometer","Classification","Daily activity","Health","Inertia","Machine learning","Recognition"],"keywords_other":["Classification framework","Recognition","Feed forward multilayers","Daily activity","University of California","Machine learning repository","Spectrotemporal features","Inertia"],"max_cite":1.0,"pub_year":2016.0,"sources":"['scp']","rawkeys":["inertia","spectrotemporal features","classification framework","recognition","university of california","daily activity","machine learning","machine learning repository","feed forward multilayers","classification","health","accelerometer"],"tags":["inertia","spectrotemporal features","classification framework","recognition","university of california","daily activity","machine learning","machine learning repository","feed forward multilayers","classification","health","accelerometer"]},{"p_id":13574,"title":"A novel wavelet sequences based on deep bidirectional LSTM network model for ECG signal classification","abstract":"Long-short term memory networks (LSTMs), which have recently emerged in sequential data analysis, are the most widely used type of recurrent neural networks (RNNs) architecture. Progress on the topic of deep learning includes successful adaptations of deep versions of these architectures. In this study, a new model for deep bidirectional LSTM network-based wavelet sequences called DBLSTM-WS was proposed for classifying electrocardiogram (ECG) signals. For this purpose, a new wavelet-based layer is implemented to generate ECG signal sequences. The ECG signals were decomposed into frequency sub-bands at different scales in this layer. These sub-bands are used as sequences for the input of LSTM networks. New network models that include unidirectional (ULSTM) and bidirectional (BLSTM) structures are designed for performance comparisons. Experimental studies have been performed for five different types of heartbeats obtained from the MIT-BIH arrhythmia database. These five types are Normal Sinus Rhythm (NSR), Ventricular Premature Contraction (VPC), Paced Beat (PB), Left Bundle Branch Block (LBBB), and Right Bundle Branch Block (RBBB). The results show that the DBLSTM-WS model gives a high recognition performance of 99.39%. It has been observed that the wavelet-based layer proposed in the study significantly improves the recognition performance of conventional networks. This proposed network structure is an important approach that can be applied to similar signal processing problems.","keywords_author":["Deep learning","ECG signals","Long-short term memory","Recurrent neural networks","Long-short term memory","Recurrent neural networks","Deep learning","ECG signals"],"keywords_other":["CONVOLUTIONAL NEURAL-NETWORK","AUTOMATED DETECTION","Ventricular premature contractions","TRANSFORM","FEATURES","Performance comparison","ARCHITECTURES","Sequential data analysis","Electrocardiogram signal","INDEPENDENT COMPONENT ANALYSIS","RECOGNITION","FEATURE-EXTRACTION","Normal sinus rhythm","ECG signals","Recurrent neural network (RNNs)","BEAT CLASSIFICATION","SHORT-TERM-MEMORY","Signal processing problems"],"max_cite":0.0,"pub_year":2018.0,"sources":"['scp', 'wos']","rawkeys":["short-term-memory","recurrent neural networks","sequential data analysis","electrocardiogram signal","recurrent neural network (rnns)","automated detection","features","transform","long-short term memory","recognition","deep learning","feature-extraction","ecg signals","ventricular premature contractions","performance comparison","beat classification","architectures","normal sinus rhythm","independent component analysis","signal processing problems","convolutional neural-network"],"tags":["short-term-memory","convolutional neural network","sequential data analysis","electrocardiogram signal","architecture","automated detection","features","machine learning","transform","recognition","neural networks","long short-term memory","ecg signals","ventricular premature contractions","performance comparison","beat classification","normal sinus rhythm","independent component analysis","signal processing problems","feature extraction"]},{"p_id":87312,"title":"Automatic Pixel-Level Pavement Crack Detection Using Information of Multi-Scale Neighborhoods","abstract":"Robust automatic pavement crack detection is critical to automated road condition evaluation. However, research on crack detection is still limited and pixel-level automatic crack detection remains a challenging problem, due to heterogeneous pixel intensity, complex crack topology, poor illumination condition, and noisy texture background. In this paper, we propose a novel approach for automatically detecting pavement cracks at pixel level, leveraging on multi-scale neighborhood information, and pixel intensity. Using pixel intensity information, a probabilistic generative model (PGM) based method is developed to calculate the probability of a crack for each pixel. This produces a probability map consisting of the probability of each pixel being part of the crack. We demonstrate that the neighborhoods of each pixel contain critical information for crack detection, and propose a support vector machine (SVM) based method to calculate the probability maps using information of multi-scale neighborhoods. We develop a fusion algorithm to merge the multiple probability maps, obtained from both PGM and SVM approaches, into a fused map, which can detect cracks with accuracy higher than any of the original probability maps. We also propose a weighted dilation operation that relies on the fused probability map to enhance the recognition of borderline pixels and improve the crack continuity without increasing the crack width improperly. Experimental results demonstrate that our algorithm achieves better performance in terms of precision, recall, f1-score, and receiver operating characteristic, in comparison with the state-of-the-art pavement crack detection algorithms.","keywords_author":["Pavement crack detection","probability map","multi-scale neighborhoods","probabilistic generative mode","support vector machine"],"keywords_other":["SELECTION","SURFACES","CLASSIFICATION","RECOGNITION","NEURAL-NETWORK","IMAGES"],"max_cite":0.0,"pub_year":2018.0,"sources":"['wos']","rawkeys":["surfaces","recognition","images","probabilistic generative mode","pavement crack detection","neural-network","classification","selection","support vector machine","multi-scale neighborhoods","probability map"],"tags":["surfaces","recognition","images","probabilistic generative mode","neural networks","pavement crack detection","selection","machine learning","classification","probability maps","multi-scale neighborhoods"]},{"p_id":29981,"title":"Building extensible frameworks for data processing: The case of MDP, Modular toolkit for Data Processing","abstract":"Data processing is a ubiquitous task in scientific research, and much energy is spent on the development of appropriate algorithms. It is thus relatively easy to find software implementations of the most common methods. On the other hand, when building concrete applications, developers are often confronted with several additional chores that need to be carried out beside the individual processing steps. These include for example training and executing a sequence of several algorithms, writing code that can be executed in parallel on several processors, or producing a visual description of the application. The Modular toolkit for Data Processing (MDP) is an open source Python library that provides an implementation of several widespread algorithms and offers a unified framework to combine them to build more complex data processing architectures. In this paper we concentrate on some of the newer features of MDP, focusing on the choices made to automatize repetitive tasks for users and developers. In particular, we describe the support for parallel computing and how this is implemented via a flexible extension mechanism. We also briefly discuss the support for algorithms that require bi-directional data flow. \u00a9 2011 Elsevier B.V.","keywords_author":["Computational neuroscience","Machine learning","Python","Scientific computing","Machine learning","Python","Scientific computing","Computational neuroscience"],"keywords_other":["SLOW FEATURE ANALYSIS","Python","Computational neuroscience","Scientific researches","Extension mechanisms","Unified framework","RECOGNITION","Concrete applications","Extensible framework","Software implementation"],"max_cite":4.0,"pub_year":2013.0,"sources":"['scp', 'wos']","rawkeys":["scientific researches","scientific computing","recognition","concrete applications","extensible framework","computational neuroscience","slow feature analysis","machine learning","python","unified framework","software implementation","extension mechanisms"],"tags":["scientific researches","scientific computing","recognition","concrete applications","extensible framework","computational neuroscience","slow feature analysis","machine learning","python","unified framework","software implementation","extension mechanisms"]},{"p_id":21851,"title":"Perception and automatic recognition of laughter from whole-body motion: Continuous and categorical perspectives","abstract":"\u00a9 2015 IEEE. Despite its importance in social interactions, laughter remains little studied in affective computing. Intelligent virtual agents are often blind to users' laughter and unable to produce convincing laughter themselves. Respiratory, auditory, and facial laughter signals have been investigated but laughter-related body movements have received less attention. The aim of this study is threefold. First, to probe human laughter perception by analyzing patterns of categorisations of natural laughter animated on a minimal avatar. Results reveal that a low dimensional space can describe perception of laughter \"types\". Second, to investigate observers' perception of laughter (hilarious, social, awkward, fake, and non-laughter) based on animated avatars generated from natural and acted motion-capture data. Significant differences in torso and limb movements are found between animations perceived as laughter and those perceived as non-laughter. Hilarious laughter also differs from social laughter. Different body movement features were indicative of laughter in sitting and standing avatar postures. Third, to investigate automatic recognition of laughter to the same level of certainty as observers' perceptions. Results show recognition rates of the Random Forest model approach human rating levels. Classification comparisons and feature importance analyses indicate an improvement in recognition of social laughter when localized features and nonlinear models are used.","keywords_author":["face and gesture recognition","machine learning","Miscellaneous","psychology"],"keywords_other":["Motion capture data","Miscellaneous","Automatic recognition","Face and gesture recognition","Social interactions","Random forest modeling","psychology","Low-dimensional spaces"],"max_cite":13.0,"pub_year":2015.0,"sources":"['scp']","rawkeys":["social interactions","machine learning","psychology","motion capture data","miscellaneous","automatic recognition","face and gesture recognition","low-dimensional spaces","random forest modeling"],"tags":["recognition","social interactions","machine learning","motion capture data","miscellaneous","automatic recognition","face and gesture recognition","low-dimensional spaces","random forest modeling"]},{"p_id":87412,"title":"Clinical judgement in the era of big data and predictive analytics","abstract":"Clinical judgement is a central and longstanding issue in the philosophy of medicine which has generated significant interest over the past few decades. In this article, we explore different approaches to clinical judgement articulated in the literature, focusing in particular on data-driven, mathematical approaches which we contrast with narrative, virtue-based approaches to clinical reasoning. We discuss the tension between these different clinical epistemologies and further explore the implications of big data and machine learning for a philosophy of clinical judgement. We argue for a pluralistic, integrative approach, and demonstrate how narrative, virtue-based clinical reasoning will remain indispensable in an era of big data and predictive analytics.","keywords_author":["artificial intelligence","big data","clinical epistemology","clinical judgement","evidence-based medicine","machine learning","medical education","narrative medicine","person-centred medicine","philosophy of medicine","predictive analytics"],"keywords_other":["PHRONESIS","NARRATIVE EVIDENCE","EVIDENCE-BASED MEDICINE","ADDITIONAL BASIC SCIENCE","MODEL","ARGUMENTATION","BIASES","EPISTEMOLOGY","PSYCHOLOGY","REFLECTION"],"max_cite":1.0,"pub_year":2018.0,"sources":"['wos']","rawkeys":["big data","person-centred medicine","argumentation","biases","evidence-based medicine","narrative medicine","machine learning","clinical epistemology","psychology","reflection","medical education","clinical judgement","philosophy of medicine","epistemology","phronesis","artificial intelligence","model","predictive analytics","narrative evidence","additional basic science"],"tags":["big data","person-centred medicine","argumentation","evidence-based medicine","narrative medicine","epidemiology","machine learning","clinical epistemology","reflection","recognition","medical education","clinical judgement","philosophy of medicine","epistemology","phronesis","model","predictive analytics","narrative evidence","additional basic science"]},{"p_id":30077,"title":"ZamAn and Raqm: Extracting temporal and numerical expressions in Arabic","abstract":"In this paper we investigate automatic identification of Arabic temporal and numerical expressions. The objectives of this paper are 1) to describe ZamAn, a machine learning method we have developed to label Arabic temporals, processing the functional dashtag -TMP used in the Arabic treebank to mark a temporal modifier which represents a reference to a point in time or a span of time, and 2) to present Raqm, a machine learning method applied to identify different forms of numerical expressions in order to normalise them into digits. We present a series of experiments evaluating how well ZamAn (resp. Raqm) copes with the enriched Arabic data achieving state-of-the-art results of F1-measure of 88.5% (resp. 96%) for bracketing and 73.1% (resp. 94.4%) for detection. \u00a9 2011 Springer-Verlag Berlin Heidelberg.","keywords_author":["Extraction","Labeller","Machine Learning","Numerical Expression","Recognition","Temporal Expression","Temporal Phrase"],"keywords_other":["Recognition","Machine-learning","Temporal Phrase","Labeller","Numerical Expression","Temporal expressions"],"max_cite":4.0,"pub_year":2011.0,"sources":"['scp']","rawkeys":["recognition","numerical expression","temporal expressions","machine learning","temporal phrase","machine-learning","labeller","extraction","temporal expression"],"tags":["recognition","numerical expression","temporal expressions","machine learning","temporal phrase","extraction","labeling"]},{"p_id":13703,"title":"Control of stair ascent and descent with a powered transfemoral prosthesis","abstract":"This paper presents a finite state-based control system for a powered transfemoral prosthesis that provides stair ascent and descent capability. The control system was implemented on a powered prosthesis and evaluated by a unilateral, transfemoral amputee subject. The ability of the powered prosthesis to provide stair ascent and descent capability was assessed by comparing the gait kinematics, as recorded by a motion capture system, with the kinematics provided by a passive prosthesis, in addition to those recorded from a set of healthy subjects. The results indicate that the powered prosthesis provides gait kinematics that are considerably more representative of healthy gait, relative to the passive prosthesis, for both stair ascent and descent. \u00a9 2001-2011 IEEE.","keywords_author":["Amputee","mechatronics","prosthesis","robotics","transfemoral"],"keywords_other":["Gait kinematics","Amputees","Humans","Amputation Stumps","Therapy, Computer-Assisted","Transfemoral","Equipment Failure Analysis","Equipment Design","Orthotic Devices","Motion capture system","Healthy subjects","Gait Disorders, Neurologic","Arthroplasty, Replacement, Ankle","Robotics","Amputee","Trans-femoral prosthesis","Artificial Limbs","Knee Prosthesis","Biofeedback, Psychology"],"max_cite":73.0,"pub_year":2013.0,"sources":"['scp']","rawkeys":["neurologic","equipment failure analysis","gait disorders","gait kinematics","ankle","amputee","amputees","trans-femoral prosthesis","psychology","artificial limbs","therapy","healthy subjects","motion capture system","arthroplasty","humans","mechatronics","prosthesis","orthotic devices","amputation stumps","equipment design","computer-assisted","replacement","transfemoral","robotics","knee prosthesis","biofeedback"],"tags":["equipment failure analysis","gait disorders","gait kinematics","ankle","amputees","trans-femoral prosthesis","neurological","artificial limbs","therapy","healthy subjects","recognition","motion capture system","arthroplasty","humans","mechatronics","prosthesis","orthotic devices","amputation stumps","equipment design","computer-assisted","replacement","transfemoral","robotics","knee prosthesis","biofeedback"]},{"p_id":54700,"title":"Batch-normalized Mlpconv-wise supervised pre-training network in network","abstract":"Deep multi-layered neural networks have nonlinear levels that allow them to represent highly varying nonlinear functions compactly. In this paper, we propose a new deep architecture with enhanced model discrimination ability that we refer to as mlpconv-wise supervised pre-training network in network (MPNIN). The process of information abstraction is facilitated within the receptive fields for MPNIN. The proposed architecture uses the framework of the recently developed NIN structure, which slides a universal approximator, such as a multilayer perceptron with rectifier units, across an image to extract features. However, the random initialization of NIN can produce poor solutions to gradient-based optimization. We use mlpconv-wise supervised pre-training to remedy this defect because this pre-training technique may contribute to overcoming the difficulties of training deep networks by better initializing the weights in all the layers. Moreover, batch normalization is applied to reduce internal covariate shift by pre-conditioning the model. Empirical investigations are conducted on the Mixed National Institute of Standards and Technology (MNIST), the Canadian Institute for Advanced Research (CIFAR-10), CIFAR-100, the Street View House Numbers (SVHN), the US Postal (USPS), Columbia University Image Library (COIL20), COIL100 and Olivetti Research Ltd (ORL) datasets, and the results verify the effectiveness of the proposed MPNIN architecture.","keywords_author":["Deep learning (DL)","Mlpconv-wise supervised pre-training network in network (MPNIN)","Network in network (NIN) structure","Mlpconv layer","Batch normalization"],"keywords_other":["NEURAL-NETWORKS","RECOGNITION","DEEP"],"max_cite":0.0,"pub_year":2017.0,"sources":"['ieee', 'wos']","rawkeys":["mlpconv layer","neural-networks","mlpconv-wise supervised pre-training network in network (mpnin)","network in network (nin) structure","recognition","deep","batch normalization","deep learning (dl)"],"tags":["mlpconv layer","mlpconv-wise supervised pre-training network in network (mpnin)","network in network (nin) structure","recognition","neural networks","deep","machine learning","bayesian networks"]},{"p_id":54701,"title":"Extreme learning machine based transfer learning algorithms: A survey","abstract":"Extreme learning machine (ELM) has been increasingly popular in the field of transfer learning (TL) due to its simplicity, training speed and ease of use in online sequential learning process. This paper critically examines transfer learning algorithms formulated with ELM technique and provides state of the art knowledge to expedite the learning process ELM based TL algorithms. As this article discusses available ELM based TL algorithm in detail, it provides a holistic overview of current literature, serves as a starting point for new researchers in ELM based TL algorithms and facilitates identification of future research direction in concise manner. (C) 2017 Elsevier B.V. All rights reserved.","keywords_author":["Transfer learning","Extreme learning machine"],"keywords_other":["FEEDFORWARD NETWORKS","CLASSIFICATION","NEURAL-NETWORKS","RECOGNITION","OPTIMIZATION"],"max_cite":2.0,"pub_year":2017.0,"sources":"['ieee', 'wos']","rawkeys":["neural-networks","recognition","transfer learning","classification","optimization","feedforward networks","extreme learning machine"],"tags":["recognition","neural networks","transfer learning","feed-forward network","classification","optimization","extreme learning machine"]},{"p_id":87469,"title":"Design of Vector Field for Different Subphases of Gait and Regeneration of Gait Pattern","abstract":"In this paper, we have designed the vector fields (VFs) for all the six joints (hip, knee, and ankle) of a bipedal walking model. The bipedal gait is the manifestation of temporal changes in the six joints angles, two each for hip, knee, and ankle values and it is a combination of seven different discrete subphases. Developing the correct joint trajectories for all the six joints was difficult from a purely mechanics-based model due to its inherent complexities. To get the correct and exact joint trajectories, it is very essential for a modern bipedal robot to walk stably. By designing the VF correctly, we are able to get the stable joint trajectory ranges and able to reproduce angle ranges from theses designed VFs. This is purely a data driven computational modeling approach, which is based on the hypothesis that morphologically similar structure (human-robot) can adopt similar gait patterns. To validate the correctness of the design, we have applied all the possible combination of joint trajectories to HOAP-2 bipedal robot, which could walk successfully maintaining its stability. The VF provides joint trajectories for a particular joint. The results show that our data driven computational model is able to provide the correct joints angle ranges, which are stable.","keywords_author":["Biometric","bipedal walk","gait","HOAP2","hybrid automata","stability","vector field (VF)"],"keywords_other":["CLASSIFICATION","IDENTIFICATION","MODEL","RECOGNITION","HUMANOID PUSH RECOVERY"],"max_cite":0.0,"pub_year":2018.0,"sources":"['wos']","rawkeys":["identification","model","recognition","hoap2","biometric","stability","classification","humanoid push recovery","hybrid automata","vector field (vf)","gait","bipedal walk"],"tags":["bipedal walking","identification","model","recognition","hoap2","stability","classification","biometrics","humanoid push recovery","hybrid automata","gait","vector field"]},{"p_id":87471,"title":"Hybrid classifier based life cycle stages analysis for malaria-infected erythrocyte using thin blood smear images","abstract":"Malaria, being a life-threatening disease caused by parasites, demands its rapid and accurate diagnosis. In this paper, we develop a computer-assisted malaria-infected life-cycle stages classification based on a hybrid classifier using thin blood smear images. The major issues are: feature extraction, feature selection and classification of erythrocytes infected with different life-cycle stages of malaria. Feature set (134 dimensional features) has been defined by the combination of the proposed features along with the existing features. Features such as prediction error, co-occurrence of linear binary pattern, chrominance channel histogram, R-G color channel difference histogram and Gabor features are the newly proposed features in our system. In the feature selection, a two-stage algorithm utilizing the filter method to rank the feature, along with the incremental feature selection technique, has been analyzed. Moreover, the performance of all the individual classifiers (Naive Bayes, support vector machine, k-nearest neighbors and artificial neural network) is evaluated. Finally, the three individual classifiers are combined to develop a hybrid classifier using different classifier combining techniques. From the experimental results, it may be concluded that hybrid classifier formed by the combination of SVM, k-NN and ANN with majority voting technique provides satisfactory results compared to other individual classifiers as well as other hybrid model. An accuracy of 96.54 +\/- 0.73% has been achieved on the collected clinical database. The results show an improvement in accuracy (11.62, 6.7, 3.39 and 2.39%) as compared to the state-of-the-art individual classifiers, i.e., Naive Bayes, SVM, k-NN and ANN, respectively.","keywords_author":["Malaria infection","Erythrocyte","Kruskal-Wallis with incremental feature selection (IFS) technique","Hybrid classifier","Classifier combination techniques"],"keywords_other":["DIAGNOSIS","PARASITES","FEATURES","FEATURE-SELECTION","REDUCTION","IDENTIFICATION","SYSTEM","RECOGNITION","HAND GESTURES","MICROSCOPIC IMAGES"],"max_cite":0.0,"pub_year":2018.0,"sources":"['wos']","rawkeys":["diagnosis","reduction","hand gestures","identification","kruskal-wallis with incremental feature selection (ifs) technique","features","classifier combination techniques","microscopic images","recognition","system","feature-selection","malaria infection","parasites","erythrocyte","hybrid classifier"],"tags":["diagnosis","reduction","identification","recognition","kruskal-wallis with incremental feature selection (ifs) technique","features","classifier combination techniques","hand gesture","microscopic image","system","erythrocytes","feature selection","malaria infection","parasite","hybrid classifier"]},{"p_id":13758,"title":"Transfer learning for visual categorization: A survey","abstract":"\u00a9 2012 IEEE.Regular machine learning and data mining techniques study the training data for future inferences under a major assumption that the future data are within the same feature space or have the same distribution as the training data. However, due to the limited availability of human labeled training data, training data that stay in the same feature space or have the same distribution as the future data cannot be guaranteed to be sufficient enough to avoid the over-fitting problem. In real-world applications, apart from data in the target domain, related data in a different domain can also be included to expand the availability of our prior knowledge about the target future data. Transfer learning addresses such cross-domain learning problems by extracting useful information from data in a related domain and transferring them for being used in target tasks. In recent years, with transfer learning being applied to visual categorization, some typical problems, e.g., view divergence in action recognition tasks and concept drifting in image classification tasks, can be efficiently solved. In this paper, we survey state-of-the-art transfer learning algorithms in visual categorization applications, such as object recognition, image classification, and human action recognition.","keywords_author":["Action recognition","image classification","machine learning","object recognition","survey","transfer learning","visual categorization.","Action recognition","image classification","machine learning","object recognition","survey","transfer learning","visual categorization"],"keywords_other":["HUMAN ACTION RECOGNITION","Humans","Machine Learning","MOTION","Transfer learning","INVARIANT ANALYSIS","Visual categorization","Transfer (Psychology)","VIEW ACTION RECOGNITION","Labeled training data","IMAGE CLASSIFICATION","Cross-domain learning","Action recognition","Different domains","Models, Theoretical","Over fitting problem","Algorithms","HISTOGRAMS","KERNEL","Neural Networks (Computer)","REPRESENTATION","Visual Perception","Knowledge","Surveys and Questionnaires","Human-action recognition","FUZZY SYSTEM","DOMAIN ADAPTATION"],"max_cite":71.0,"pub_year":2015.0,"sources":"['scp', 'wos']","rawkeys":["knowledge","transfer (psychology)","action recognition","fuzzy system","kernel","image classification","human action recognition","survey","transfer learning","motion","machine learning","visual perception","models","algorithms","domain adaptation","view action recognition","neural networks (computer)","histograms","humans","visual categorization","invariant analysis","over fitting problem","cross-domain learning","labeled training data","surveys and questionnaires","representation","different domains","human-action recognition","object recognition","theoretical"],"tags":["knowledge","action recognition","kernel","image classification","survey","transfer learning","motion","machine learning","visual perception","algorithms","view action recognition","recognition","denoising autoencoder","neural networks","histograms","human activity recognition","humans","fuzzy systems","visual categorization","invariant analysis","over fitting problem","cross-domain learning","labeled training data","model","surveys and questionnaires","representation","different domains","object recognition","theoretical"]},{"p_id":112072,"title":"Analysing comparative soft biometrics from crowdsourced annotations","abstract":"Soft biometrics enable human description and identification from low-quality surveillance footage. This study premises the design, collection and analysis of a novel crowdsourced dataset of comparative soft biometric body annotations, obtained from a richly diverse set of human annotators. The authors annotate 100 subject images to provide a coherent, in-depth appraisal of the collected annotations and inferred relative labels. The dataset includes gender as a comparative trait and the authors find that comparative labels characteristically contain additional discriminative information over traditional categorical annotations. Using the authors' pragmatic dataset, semantic recognition is performed by inferring relative biometric signatures using a RankSVM algorithm. This demonstrates a practical scenario, reproducing responses from a video surveillance operator searching for an individual. The approach can reliably return the correct match in the top 7% of results with ten comparisons, or top 13% of results using just five sets of subject comparisons.","keywords_author":["biometrics (access control)","video surveillance","support vector machines","comparative soft biometric analysis","crowdsourced annotations","human description","human identification","low-quality surveillance footage","comparative soft biometric body annotations","additional discriminative information","pragmatic dataset","semantic recognition","RankSVM algorithm","video surveillance operator searching"],"keywords_other":["RECOGNITION","CRIME","HUMAN IDENTIFICATION"],"max_cite":3.0,"pub_year":2016.0,"sources":"['wos']","rawkeys":["comparative soft biometric body annotations","semantic recognition","video surveillance operator searching","comparative soft biometric analysis","recognition","pragmatic dataset","video surveillance","low-quality surveillance footage","additional discriminative information","ranksvm algorithm","biometrics (access control)","crowdsourced annotations","human description","support vector machines","crime","human identification"],"tags":["comparative soft biometric body annotations","semantic recognition","video surveillance operator searching","comparative soft biometric analysis","recognition","pragmatic dataset","video surveillance","machine learning","low-quality surveillance footage","additional discriminative information","ranksvm algorithm","biometrics","crowdsourced annotations","human description","crime","human identification"]},{"p_id":71129,"title":"Automatic Fabric Defect Detection with a Multi-Scale Convolutional Denoising Autoencoder Network Model","abstract":"Fabric defect detection is a necessary and essential step of quality control in the textile manufacturing industry. Traditional fabric inspections are usually performed by manual visual methods, which are low in efficiency and poor in precision for long-term industrial applications. In this paper, we propose an unsupervised learning-based automated approach to detect and localize fabric defects without any manual intervention. This approach is used to reconstruct image patches with a convolutional denoising autoencoder network at multiple Gaussian pyramid levels and to synthesize detection results from the corresponding resolution channels. The reconstruction residual of each image patch is used as the indicator for direct pixel-wise prediction. By segmenting and synthesizing the reconstruction residual map at each resolution level, the final inspection result can be generated. This newly developed method has several prominent advantages for fabric defect detection. First, it can be trained with only a small amount of defect-free samples. This is especially important for situations in which collecting large amounts of defective samples is difficult and impracticable. Second, owing to the multi-modal integration strategy, it is relatively more robust and accurate compared to general inspection methods (the results at each resolution level can be viewed as a modality). Third, according to our results, it can address multiple types of textile fabrics, from simple to more complex. Experimental results demonstrate that the proposed model is robust and yields good overall performance with high precision and acceptable recall rates.","keywords_author":["fabric defect detection","unsupervised learning","deep neural network","convolutional denoising autoencoder","Gaussian pyramid"],"keywords_other":["INSPECTION","TRANSFORM","DECOMPOSITION","RECOGNITION","NEURAL-NETWORK","GABOR FILTERS"],"max_cite":0.0,"pub_year":2018.0,"sources":"['ieee', 'wos']","rawkeys":["gaussian pyramid","recognition","deep neural network","gabor filters","inspection","unsupervised learning","neural-network","transform","convolutional denoising autoencoder","fabric defect detection","decomposition"],"tags":["gaussian pyramid","recognition","gabor filter","neural networks","inspection","unsupervised learning","convolutional neural network","transform","convolutional denoising autoencoder","fabric defect detection","decomposition"]},{"p_id":71132,"title":"A 3D polar-radius-moment invariant as a shape circularity measure","abstract":"In this paper a novel and generalized circularity measure is proposed based on 3D polar-radius-moment invariant. We proved theoretically and verify experimentally that the proposed measure is invariant to scaling and rotation. Moreover, the proposed measure can be adapted to a fixed range for a suitable value of p according to degree of accuracy and is generalized to satisfy different requirements in actual application. The experimental results show that the proposed measure can accord with human visual perception and consistently performs best in terms of retrieval efficiency in test datasets than other compared methods. (C) 2017 Elsevier B.V. All rights reserved.","keywords_author":["Shape recognition","Circularity measure","3D polar-radius-moment","3D model retrieval"],"keywords_other":["ELLIPTICITY","CONVEXITY MEASURE","CUBENESS","REPRESENTATION","RECOGNITION","3-D OBJECT RETRIEVAL"],"max_cite":0.0,"pub_year":2017.0,"sources":"['wos']","rawkeys":["recognition","3d model retrieval","ellipticity","shape recognition","convexity measure","representation","cubeness","circularity measure","3-d object retrieval","3d polar-radius-moment"],"tags":["recognition","3d model retrieval","cube","ellipticity","shape recognition","convexity measure","representation","3d object retrieval","circularity measure","3d polar-radius-moment"]},{"p_id":71151,"title":"Tile-Based Semisupervised Classification of Large-Scale VHR Remote Sensing Images","abstract":"This paper deals with the problem of the classification of large-scale very high-resolution (VHR) remote sensing (RS) images in a semisupervised scenario, where we have a limited training set (less than ten training samples per class). Typical pixel-based classification methods are unfeasible for large-scale VHR images. Thus, as a practical and efficient solution, we propose to subdivide the large image into a grid of tiles and then classify the tiles instead of classifying pixels. Our proposed method uses the power of a pretrained convolutional neural network (CNN) to first extract descriptive features from each tile. Next, a neural network classifier (composed of 2 fully connected layers) is trained in a semisupervised fashion and used to classify all remaining tiles in the image. This basically presents a coarse classification of the image, which is sufficient for many RS application. The second contribution deals with the employment of the semisupervised learning to improve the classification accuracy. We present a novel semisupervised approach which exploits both the spectral and spatial relationships embedded in the remaining unlabelled tiles. In particular, we embed a spectral graph Laplacian in the hidden layer of the neural network. In addition, we apply regularization of the output labels using a spatial graph Laplacian and the random Walker algorithm. Experimental results obtained by testing the method on two large-scale images acquired by the IKONOS2 sensor reveal promising capabilities of this method in terms of classification accuracy even with less than ten training samples per class.","keywords_author":null,"keywords_other":["NEURAL-NETWORKS","RECOGNITION","SEGMENTATION","REGRESSION"],"max_cite":0.0,"pub_year":2018.0,"sources":"['ieee', 'wos']","rawkeys":["neural-networks","regression","recognition","segmentation"],"tags":["neural networks","regression","recognition","segmentation"]},{"p_id":87551,"title":"Automatic placental maturity grading via hybrid learning","abstract":"Fetal viability, gestational age, and complicated image processing have made evaluating placental maturity a tedious and time-consuming task. Despite various developments, automatic placental maturity still remains as a challenging issue. To address this issue, we propose a new method to automatically grade placental maturity from B-mode ultrasound (BUS) and color Doppler energy (CDE) images based on a hybrid learning architecture. We also apply an improved pyramidal shift invariant feature transform (IPSIFT) descriptor using a coarse-to-fine scale representation for visual feature extraction. These local features are then clustered by a generative Gaussian mixture model (GMM) to incorporate high order statistics. Next, the clustering representatives are encoded and aggregated via Fisher vector (FV). Instead of using traditional FV, an end to -end deep training strategy is developed to fine-tune the GMM parameters to boost evaluation performance. A multi-view fusion technique is also developed for feature complementarity exploration. Extensive experimental results demonstrate that our method delivers promising performance in placental maturity evaluation and outperforms competing methods.","keywords_author":["Placental maturity evaluation","Pyramidal descriptor","Deep feature training","Hybrid learning","Normalization"],"keywords_other":["FISHER VECTOR","IMAGE CLASSIFICATION","RECOGNITION","SEGMENTATION","GROWTH","DOPPLER ULTRASOUND","CATEGORIES","LOCAL FEATURES"],"max_cite":0.0,"pub_year":2016.0,"sources":"['ieee', 'wos']","rawkeys":["placental maturity evaluation","pyramidal descriptor","recognition","deep feature training","segmentation","local features","hybrid learning","categories","fisher vector","normalization","doppler ultrasound","image classification","growth"],"tags":["placental maturity evaluation","pyramidal descriptor","recognition","deep feature training","local feature","segmentation","hybrid learning","categories","normalization","fisher vectors","doppler ultrasound","image classification","growth"]},{"p_id":87558,"title":"A deeply supervised residual network for HEp-2 cell classification via cross-modal transfer learning","abstract":"Accurate Human Epithelial-2 (HEp-2) cell image classification plays an important role in the diagnosis of many autoimmune diseases and subsequent treatment. One of the key challenges is huge intra-class variations caused by inhomogeneous illumination. To address it, we propose a framework based on very deep supervised residual network (DSRN) to classify HEp-2 cell images. Specifically, we adopt a residual network of 50 layers (ResNet-50) that is substantially deep to extract rich and discriminative features. The deep supervision is imposed on the ResNet-based framework to further boost the classification performance by directly guiding the training of the lower and upper levels of the network. The proposed method is evaluated using two publicly available datasets (i.e., International Conference on Pattern Recognition (ICPR) 2012 and ICPR2016-Task1 cell classification contest datasets). Different from the previous deep learning models learned from scratch, a cross-modal transfer learning strategy is developed. Namely, we pretrain ICPR2012 dataset to fine-tune ICPR2016 dataset based on our DSRN model since both datasets are similar. Extensive experiments show that the proposed method delivers state-of-the-art performance and outperforms the traditional methods based on deep convolutional neural network (DCNN). (C) 2018 Elsevier Ltd. All rights reserved.","keywords_author":["HEp-2 cell classification","Residual network","Deeply supervised ResNet","Cross-modal transfer learning"],"keywords_other":["FEATURES","IDENTIFICATION","SPECTRAL FEATURE-SELECTION","AD DIAGNOSIS","RECOGNITION","JOINT REGRESSION","ALZHEIMERS-DISEASE","PATTERNS"],"max_cite":1.0,"pub_year":2018.0,"sources":"['ieee', 'wos']","rawkeys":["cross-modal transfer learning","identification","recognition","features","alzheimers-disease","spectral feature-selection","patterns","deeply supervised resnet","ad diagnosis","residual network","joint regression","hep-2 cell classification"],"tags":["cross-modal transfer learning","identification","recognition","features","alzheimers-disease","spectral feature-selection","patterns","deeply supervised resnet","ad diagnosis","residual network","joint regression","hep-2 cell classification"]},{"p_id":30216,"title":"Vowel recognition from continuous articulatory movements for speaker-dependent applications","abstract":"A novel approach was developed to recognize vowels from continuous tongue and lip movements. Vowels were classified based on movement patterns (rather than on derived articulatory features, e.g., lip opening) using a machine learning approach. Recognition accuracy on a single-speaker dataset was 94.02% with a very short latency. Recognition accuracy was better for high vowels than for low vowels. This finding parallels previous empirical findings on tongue movements during vowels. The recognition algorithm was then used to drive an articulation-to-acoustics synthesizer. The synthesizer recognizes vowels from continuous input stream of tongue and lip movements and plays the corresponding sound samples in near real-time. \u00a92010 IEEE.","keywords_author":["Articulation","Machine learning","Recognition","Support vector machine"],"keywords_other":["Continuous input","Recognition","Machine-learning","Articulatory features","Recognition algorithm","Data sets","Recognition accuracy","Support vector","Articulation","Vowel recognition","Sound sample","Lip movements","Movement pattern","Empirical findings"],"max_cite":4.0,"pub_year":2010.0,"sources":"['scp']","rawkeys":["recognition","movement pattern","support vector","vowel recognition","articulatory features","continuous input","articulation","machine learning","sound sample","lip movements","data sets","recognition accuracy","machine-learning","recognition algorithm","support vector machine","empirical findings"],"tags":["recognition","movement pattern","vowel recognition","articulatory features","continuous input","articulation","machine learning","sound sample","lip movements","data sets","recognition accuracy","support vector","recognition algorithm","empirical findings"]},{"p_id":87560,"title":"Automatic content understanding with cascaded spatial-temporal deep framework for capsule endoscopy videos","abstract":"Capsule endoscopy (CE) is the first-line diagnostic tool for inspecting gastrointestinal (GI) tract diseases. It is a tremendous task on examining and managing the CE videos by endoscopists. Therefore, a computer-aided diagnosis system is desired and urgent. In this paper, a general cascaded spatial temporal deep framework is proposed to understand the most commonly seen contents of whole GI tract videos. First, the noisy contents such as feces, bile, bubble, and low power images are detected and removed by a Convolutional Neural Network (CNN) model. The clear images are then classified into entrance, stomach, small intestine, and colon by the second CNN. Finally, the topographic segmentation of the whole video is performed with a global temporal integration strategy by Hidden Markov Model (HMM). Compared to existing methods, the proposed framework performs noise content detection and topographic segmentation at the same time, which significantly reduces the number of images to be checked by endoscopists and segments images of different organs more accurately. Experiments on a dataset with 630K images from 14 patients demonstrate that the proposed approach achieves a promising performance in terms of effectiveness and efficiency.","keywords_author":["Wireless capsule endoscopy","Convolutional neural network","Topographic segmentation","Content understanding","Hidden Markov model"],"keywords_other":["FEATURES","SYSTEM","RECOGNITION","SEGMENTATION","MODELS","TEXTURE","IMAGES"],"max_cite":2.0,"pub_year":2016.0,"sources":"['ieee', 'wos']","rawkeys":["content understanding","images","recognition","features","segmentation","topographic segmentation","wireless capsule endoscopy","system","texture","hidden markov model","convolutional neural network","models"],"tags":["hidden markov models","content understanding","images","model","features","recognition","segmentation","topographic segmentation","wireless capsule endoscopy","system","texture","convolutional neural network"]},{"p_id":5647,"title":"Unsupervised salience learning for person re-identification","abstract":"Human eyes can recognize person identities based on some small salient regions. However, such valuable salient information is often hidden when computing similarities of images with existing approaches. Moreover, many existing approaches learn discriminative features and handle drastic viewpoint change in a supervised way and require labeling new training data for a different pair of camera views. In this paper, we propose a novel perspective for person re-identification based on unsupervised salience learning. Distinctive features are extracted without requiring identity labels in the training procedure. First, we apply adjacency constrained patch matching to build dense correspondence between image pairs, which shows effectiveness in handling misalignment caused by large viewpoint and pose variations. Second, we learn human salience in an unsupervised manner. To improve the performance of person re-identification, human salience is incorporated in patch matching to find reliable and discriminative matched patches. The effectiveness of our approach is validated on the widely used VIPeR dataset and ETHZ dataset. \u00a9 2013 IEEE.","keywords_author":["person re-identification","recognition","Salience matching"],"keywords_other":["recognition","Identity labels","Dense correspondences","Person re identifications","Training procedures","Salience matching","Discriminative features","Salient regions"],"max_cite":433.0,"pub_year":2013.0,"sources":"['ieee', 'scp', 'wos']","rawkeys":["identity labels","recognition","person re identifications","dense correspondences","salience matching","training procedures","person re-identification","discriminative features","salient regions"],"tags":["identity labels","recognition","dense correspondences","salience matching","training procedures","person re-identification","discriminative features","salient regions"]},{"p_id":71187,"title":"Group sparse autoencoder","abstract":"Unsupervised feature extraction is gaining a lot of research attention following its success to represent any kind of noisy data. Owing to the presence of a lot of training parameters, these feature learning models are prone to overfitting. Different regularization methods have been explored in the literature to avoid overfitting in deep learning models. In this research, we consider autoencoder as the feature learning architecture and propose l(2.1)-norm based regularization to improve its learning capacity, called as Group Sparse AutoEncoder (GSAE). l(2.1) -norm is based on the postulate that the features from the same class will have a common sparsity pattern in the feature space. We present the learning algorithm for group sparse encoding using majorization-minimization approach. The performance of the proposed algorithm is also studied on three baseline image datasets: MNIST, CIFAR-10, and SVHN. Further, using GSAE, we propose a novel deep learning based image representation for minutia detection from latent fingerprints. Latent fingerprints contain only a partial finger region, very noisy ridge patterns, and depending on the surface it is deposited, contain significant background noise. We formulate the problem of minutia extraction as a two-class classification problem and learn the descriptor using the novel formulation of GSAE. Experimental results on two publicly available latent fingerprint datasets show that the proposed algorithm yields state-of-the-art results for automated minutia extraction. (C) 2017 Elsevier B.V. All rights reserved.","keywords_author":["Supervised autoencoder","Group sparsity","Latent fingerprint","Minutia extraction"],"keywords_other":["DEEP","FINGERPRINT DATABASE","NEURAL-NETWORKS","RECOGNITION","ALGORITHMS"],"max_cite":4.0,"pub_year":2017.0,"sources":"['ieee', 'wos']","rawkeys":["neural-networks","recognition","latent fingerprint","deep","fingerprint database","minutia extraction","supervised autoencoder","group sparsity","algorithms"],"tags":["recognition","latent fingerprint","neural networks","deep","fingerprint database","supervised autoencoder","group sparsity","minutiae extraction","algorithms"]},{"p_id":87573,"title":"Random forest classification based acoustic event detection utilizing contextual-information and bottleneck features","abstract":"The variety of event categories and event boundary information have resulted in limited success for acoustic event detection systems. To deal with this, we propose to utilize the long contextual information, low-dimensional discriminant global bottleneck features and category-specific bottleneck features. By concatenating several adjacent frames together, the use of contextual information makes it easier to cope with acoustic signals with long duration. Global and category-specific bottleneck features can extract the prior knowledge of the event category and boundary, which is ideally matched by the task of an event detection system. Evaluations on the UPC-TALP and ITC-IRST databases of highly variable acoustic events demonstrate the effectiveness of the proposed approaches by achieving a 5.30% and 4.44% absolute error rate improvement respectively compared to the state of art technique. (C) 2018 Elsevier Ltd. All rights reserved.","keywords_author":["Acoustic event detection","Contextual information","Global bottleneck features","Category-specific bottleneck features"],"keywords_other":["SELECTION","CONVERGENCE","NOISY ENVIRONMENTS","RECOGNITION","SOUNDS"],"max_cite":0.0,"pub_year":2018.0,"sources":"['wos']","rawkeys":["convergence","sounds","recognition","contextual information","category-specific bottleneck features","noisy environments","global bottleneck features","selection","acoustic event detection"],"tags":["sounds","recognition","contextual information","category-specific bottleneck features","mathematics","global bottleneck features","selection","acoustic event detection","noisy environment"]},{"p_id":30234,"title":"Why classifying search algorithms is essential","abstract":"In chemistry, the periodic table of elements was a huge leap of progress. It allowed elements to be placed in a table allowing their classification. More importantly, it allowed predictions to be made about their properties, and in some cases predictions about elements which had not even been discovered at the time the periodic table was proposed. We argue that the current state of search methodologies is analogous to the state of chemistry before the arrival of the periodic table, and such a classification system is well overdue. Many machine learning research papers are currently published on the premise that the proposed algorithm does better on a particular data set than another algorithm. This approach, while it does produce better results on the given data set, does not produce an understanding of why a particular algorithm performs well on a particular problem. The No Free Lunch Theorem, while stating this is impossible over all data sets, also provides a possible framework. We state why the performance table associated with No Free Lunch (which has rows and columns similar to the periodic table), which is exactly what we are looking for, is unworkable, as problems cannot be indexed or enumerated in practice. We believe the classification of algorithms and problems is the biggest issue facing machine learning today. Progress in science is often brought about by asking the right questions, before finding the answers, and it is this question we address in this paper. Therefore, the contribution of this paper is raising the profile of the challenge of classifying algorithms and problems. An underlying aim is to reduce the number of papers with titles of the form \"Algorithm X Applied to Problem Y\", or simply \"A New Algorithm\", as new algorithms should only be introduced with an intended class of problem instances in mind. \u00a92010 IEEE.","keywords_author":["Bias","Generalization","Induction","Machine learning","No free lunch theorems","Optimization","Search"],"keywords_other":["Bias","Search","Machine learning","No free lunch theorem","Generalization","Induction"],"max_cite":4.0,"pub_year":2010.0,"sources":"['scp']","rawkeys":["search","induction","no free lunch theorem","machine learning","generalization","bias","no free lunch theorems","optimization"],"tags":["recognition","search","induction","no free lunch theorem","epidemiology","machine learning","optimization"]},{"p_id":79418,"title":"Enhancing students' approaches to learning: the added value of gradually implementing case-based learning","abstract":"Previous research has shown the difficulty of enhancing students' approaches to learning, in particular the deep approach, through student-centred teaching methods such as problem- and case-based learning. This study investigates whether mixed instructional methods combining case-based learning and lectures have the power to enhance students' approaches to learning, compared to instructional methods using either case-based learning or lectures. A quasi-experimental research was set up using a pre-\/post-test design. Participants were 1,098 first-year student teachers taking a course on child development. Statistical analysis showed that students in a gradually implemented case-based setting, in which lectures gradually made way for case-based learning, scored significantly higher on the scales organised studying and effort management and significantly lower on the surface approach, compared to students in a completely case-based setting. Therefore, students in a gradually implemented case-based setting worked in a better organised way and spent more effort and concentration than students who experienced only case-based learning. Nevertheless, the gradually implemented case-based setting did not encourage students to apply deep approaches that aimed at understanding. Quantitative content analysis revealed that students in the gradually implemented case-based setting especially appreciated the variation in teaching methods and the specific combination of lectures and case-based learning.","keywords_author":["Case-based learning","Lectures","Approaches to learning","Perceptions"],"keywords_other":["PERSPECTIVE","CASE-BASED INSTRUCTION","OUTCOMES","CONSTRUCTIVIST","PERCEPTIONS","PERFORMANCE","DISCOVERY","INQUIRY","PSYCHOLOGY","ENVIRONMENT"],"max_cite":6.0,"pub_year":2013.0,"sources":"['wos']","rawkeys":["outcomes","performance","approaches to learning","discovery","case-based learning","inquiry","lectures","perspective","perceptions","environment","psychology","case-based instruction","constructivist"],"tags":["lecture","outcomes","performance","approaches to learning","recognition","discovery","case-based learning","inquiry","perspective","perceptions","environment","case-based instruction","constructivist"]},{"p_id":79448,"title":"Motivation and transfer in professional training: A meta-analysis of the moderating effects of knowledge type, instruction, and assessment conditions","abstract":"This meta-analysis (148 studies, k = 197, N = 31,718) examined the relationship between motivation and transfer in professional training. For this purpose, motivation was conceptualized in the following nine dimensions: motivation to learn, motivation to transfer, pre- and post-training self-efficacy, mastery orientation, performance orientation, avoidance orientation, expectancy, and instrumentality. Population correlation estimates ranged between -0.11 and 0.52. Three moderator effects were estimated. First, correlations were higher when the training focused on declarative and self-regulatory, rather than on procedural, knowledge. Second, learner-centered environments tended to show greater numbers of positive correlations than did knowledge-centered environments. Third, when compared with external, supervisory, or peer assessment, self-assessment of transfer produced upwardly biased population estimates irrespective of the transfer criterion. These findings are discussed in terms of their implications for theories of training effectiveness and their significance for the practice of training evaluation. (C) 2011 Elsevier Ltd. All rights reserved.","keywords_author":["Professional training","Training motivation","Transfer","Knowledge type","Assessment","Meta-analysis"],"keywords_other":["REGULATORY PROCESSES","HEARING PROTECTION","METACOGNITIVE ACTIVITY","GOAL ORIENTATION","INTERPERSONAL SKILLS","CREDIBILITY INTERVALS","VERBAL SELF-GUIDANCE","LEARNING OUTCOMES","SITUATIONAL CHARACTERISTICS","COMPLEX SKILL ACQUISITION"],"max_cite":53.0,"pub_year":2011.0,"sources":"['wos']","rawkeys":["transfer","situational characteristics","goal orientation","complex skill acquisition","metacognitive activity","professional training","training motivation","hearing protection","assessment","knowledge type","interpersonal skills","regulatory processes","learning outcomes","verbal self-guidance","credibility intervals","meta-analysis"],"tags":["knowledge types","goal orientation","situational characteristics","complex skill acquisition","metacognitive activity","professional training","training motivation","recognition","hearing protection","assessment","interpersonal skills","metaanalysis","credible interval","regulatory process","learning outcomes","verbal self-guidance"]},{"p_id":54897,"title":"Non-linear matrix completion","abstract":"Conventional matrix completion methods are generally linear because they assume that the given data are from linear transformations of lower-dimensional latent subspace and the matrix is of low-rank. Therefore, these methods are not effective in recovering incomplete matrices when the data are from non-linear transformations of lower-dimensional latent subspace. Matrices consisting of such nonlinear data are always of high-rank or even full-rank. In this paper, a novel method, called non-linear matrix completion (NLMC), is proposed to recover missing entries of data matrices with non-linear structures. NLMC minimizes the rank (approximated by Schatten p-norm) of a matrix in the feature space given by a non-linear mapping of the data (input) space, where kernel trick is used to avoid carrying out the unknown non-linear mapping explicitly. The proposed NLMC is compared with existing methods on a toy example of matrix completion and real problems including image inpainting and single-\/multi-label classification. The experimental results verify the effectiveness and superiority of the proposed method. In addition, the idea of NLMC can be extended to a non-linear rank-minimization framework applicable to other problems such as non-linear denoising. (C) 2017 Elsevier Ltd. All rights reserved.","keywords_author":["Matrix completion","Low-rank","Kernel","Schatten p-norm","Image inpainting","Single-\/multi-label classification","Non-linear denoising"],"keywords_other":["ALGORITHM","MODEL","RECOGNITION","MINIMIZATION","KERNEL","PCA","PRE-IMAGE","MULTILABEL IMAGE CLASSIFICATION","NUCLEAR NORM REGULARIZATION"],"max_cite":0.0,"pub_year":2018.0,"sources":"['wos']","rawkeys":["algorithm","recognition","model","image inpainting","minimization","multilabel image classification","single-\/multi-label classification","kernel","nuclear norm regularization","low-rank","matrix completion","schatten p-norm","pca","non-linear denoising","pre-image"],"tags":["principal component analysis","recognition","model","image inpainting","minimization","multilabel image classification","single-\/multi-label classification","kernel","nuclear norm regularization","low-rank","matrix completion","schatten p-norm","algorithms","non-linear denoising","pre-image"]},{"p_id":46746,"title":"Local Deep Field for Electrocardiogram Beat Classification","abstract":"IEEE To reduce the high mortality rate among heart patients, electrocardiogram (ECG) beat classification plays an important role in computer aided diagnosis system, but this issue is challenging because of the complex variations of data. Since ECG beat data lie on high-dimension manifold, we propose a novel method, named &#x201C;Local Deep Field&#x201D;, in purpose of capturing the devil in the details of this manifold. This method learns different deep architectures within the local manifold charts. Local regionalization can help tackle the particularity of local variations, while deep architecture can disentangle the hidden class information within local distributions. The advantage of the proposed method has been experimentally demonstrated in terms of MIT-BIH Arrhythmia database.","keywords_author":["Databases","ECG Beat Classification","Electrocardiography","Feature extraction","Heart beat","Local Deep Field","Machine learning","Manifold","Manifolds","Sensors","ECG beat classification","local deep field","data manifold"],"keywords_other":["Computer aided diagnosis systems","HEARTBEAT CLASSIFICATION","Class information","Deep architectures","Manifold","FEATURES","MACHINE","OPTIMIZATION","Local distributions","NEURAL-NETWORKS","RECOGNITION","Ecg beat classifications","Beat classification","ECG SIGNAL CLASSIFICATION","Local Deep Field"],"max_cite":0.0,"pub_year":2017.0,"sources":"['scp', 'wos']","rawkeys":["ecg beat classification","databases","sensors","manifolds","electrocardiography","neural-networks","features","local distributions","machine","machine learning","optimization","ecg beat classifications","computer aided diagnosis systems","recognition","deep architectures","class information","ecg signal classification","data manifold","heartbeat classification","beat classification","local deep field","feature extraction","heart beat","manifold"],"tags":["ecg","databases","sensors","manifolds","features","local distributions","machine","machine learning","heart beats","data manifolds","optimization","ecg beat classifications","computer aided diagnosis systems","recognition","neural networks","deep architectures","class information","ecg signal classification","heartbeat classification","beat classification","local deep field","feature extraction"]},{"p_id":22207,"title":"Generic decoding of seen and imagined objects using hierarchical visual features","abstract":"\u00a9 The Author(s) 2017. Object recognition is a key function in both human and machine vision. While brain decoding of seen and imagined objects has been achieved, the prediction is limited to training examples. We present a decoding approach for arbitrary objects using the machine vision principle that an object category is represented by a set of features rendered invariant through hierarchical processing. We show that visual features, including those derived from a deep convolutional neural network, can be predicted from fMRI patterns, and that greater accuracy is achieved for low-\/high-level features with lower-\/higher-level visual areas, respectively. Predicted features are used to identify seen\/imagined object categories (extending beyond decoder training) from a set of computed features for numerous object images. Furthermore, decoding of imagined objects reveals progressive recruitment of higher-to-lower visual representations. Our results demonstrate a homology between human and machine vision and its utility for brain-based information retrieval.","keywords_author":null,"keywords_other":["HUMAN BRAIN ACTIVITY","AREAS","RECONSTRUCTION","REPRESENTATIONS","RECOGNITION","NATURAL IMAGES","MODELS","CORTEX","FMRI","RECEPTIVE-FIELDS"],"max_cite":13.0,"pub_year":2017.0,"sources":"['scp', 'wos']","rawkeys":["receptive-fields","recognition","areas","human brain activity","reconstruction","representations","fmri","models","natural images","cortex"],"tags":["recognition","model","human brain activity","representation","random forests","reconstruction","fmri","natural images","area","cortex"]},{"p_id":38657,"title":"Learning from Few Samples with Memory Network","abstract":"\u00a9 2017, Springer Science+Business Media, LLC. Neural networks (NN) have achieved great successes in pattern recognition and machine learning. However, the success of a NN usually relies on the provision of a sufficiently large number of data samples as training data. When fed with a limited data set, a NN\u2019s performance may be degraded significantly. In this paper, a novel NN structure is proposed called a memory network. It is inspired by the cognitive mechanism of human beings, which can learn effectively, even from limited data. Taking advantage of the memory from previous samples, the new model achieves a remarkable improvement in performance when trained using limited data. The memory network is demonstrated here using the multi-layer perceptron (MLP) as a base model. However, it would be straightforward to extend the idea to other neural networks, e.g., convolutional neural networks (CNN). In this paper, the memory network structure is detailed, the training algorithm is presented, and a series of experiments are conducted to validate the proposed framework. Experimental results show that the proposed model outperforms traditional MLP-based models as well as other competitive algorithms in response to two real benchmark data sets.","keywords_author":["Memory","Multi-layer perceptron","Neural network","Prior knowledge","Recognition","Memory","Multi-layer perceptron","Neural network","Recognition","Prior knowledge"],"keywords_other":["Neural network (nn)","STYLE","Recognition","Training algorithms","Competitive algorithms","Prior knowledge","RECOGNITION","Convolutional neural network","Multi layer perceptron","Cognitive mechanisms"],"max_cite":1.0,"pub_year":2018.0,"sources":"['scp', 'wos']","rawkeys":["neural network","recognition","training algorithms","memory","competitive algorithms","multi-layer perceptron","cognitive mechanisms","convolutional neural network","multi layer perceptron","neural network (nn)","style","prior knowledge"],"tags":["recognition","training algorithms","memory","neural networks","competitive algorithms","cognitive mechanisms","convolutional neural network","multi layer perceptron","styles","prior knowledge"]},{"p_id":22293,"title":"Data driven articulatory synthesis with deep neural networks","abstract":"\u00a9 2015 Elsevier Ltd. All rights reserved.The conventional approach for data-driven articulatory synthesis consists of modeling the joint acoustic-articulatory distribution with a Gaussian mixture model (GMM), followed by a post-processing step that optimizes the resulting acoustic trajectories. This final step can significantly improve the accuracy of the GMM frame-by-frame mapping but is computationally intensive and requires that the entire utterance be synthesized beforehand, making it unsuited for real-time synthesis. To address this issue, we present a deep neural network (DNN) articulatory synthesizer that uses a tapped-delay input line, allowing the model to capture context information in the articulatory trajectory without the need for post-processing. We characterize the DNN as a function of the context size and number of hidden layers, and compare it against two GMM articulatory synthesizers, a baseline model that performs a simple frame-by-frame mapping, and a second model that also performs trajectory optimization. Our results show that a DNN with a 60-ms context window and two 512-neuron hidden layers can synthesize speech at four times the frame rate - comparable to frame-by-frame mappings, while improving the accuracy of trajectory optimization (a 9.8% reduction in Mel Cepstral distortion). Subjective evaluation through pairwise listening tests also shows a strong preference toward the DNN articulatory synthesizer when compared to GMM trajectory optimization.","keywords_author":["Articulatory synthesis","Deep learning","Electromagnetic articulography","Gaussian mixture models","Articulatory synthesis","Electromagnetic articulography","Deep learning","Gaussian mixture models"],"keywords_other":["Deep learning","Electromagnetic articulography","Subjective evaluations","Trajectory optimization","BOLTZMANN MACHINES","Articulatory synthesis","MOVEMENTS","ACOUSTICS","MODEL","CONVERSION","RECOGNITION","SPEECH PRODUCTION","Conventional approach","Deep neural networks","Gaussian Mixture Model"],"max_cite":12.0,"pub_year":2016.0,"sources":"['ieee', 'scp', 'wos']","rawkeys":["speech production","electromagnetic articulography","recognition","conversion","model","trajectory optimization","deep learning","deep neural networks","boltzmann machines","movements","articulatory synthesis","conventional approach","subjective evaluations","acoustics","gaussian mixture models","gaussian mixture model"],"tags":["speech production","electromagnetic articulography","recognition","conversion","model","trajectory optimization","movement","boltzmann machines","machine learning","articulatory synthesis","convolutional neural network","conventional approach","subjective evaluations","acoustics","gaussian mixture model"]},{"p_id":5912,"title":"Convolutional face finder: A neural architecture for fast and robust face detection","abstract":"In this paper, we present a novel face detection approach based on a convolutional neural architecture, designed to robustly detect highly variable face patterns, rotated up to \u00b120 degrees in image plane and turned up to \u00b160 degrees, in complex real world images. The proposed system automatically synthesizes simple problem-specific feature extractors from a training set of face and nonface patterns, without making any assumptions or using any hand-made design concerning the features to extract or the areas of the face pattern to analyze. The face detection procedure acts like a pipeline of simple convolution and subsampling modules that treat the raw input image as a whole. We therefore show that an efficient face detection system does not require any costly local preprocessing before classification of image areas. The proposed scheme provides very high detection rate with a particularly low level of false positives, demonstrated on difficult test sets, without requiring the use of multiple networks for handling difficult cases. We present extensive experimental results illustrating the efficiency of the proposed approach on difficult test sets and including an indepth sensitivity analysis with respect to the degrees of variability of the face patterns. \u00a9 2004 IEEE.","keywords_author":["Convolutional networks","Face detection","Machine learning","Neural networks","face detection","neural networks","machine learning","convolutional networks"],"keywords_other":["Convolutional networks","FEATURES","RECOGNITION","NETWORK","Face detection","IMAGES"],"max_cite":344.0,"pub_year":2004.0,"sources":"['ieee', 'scp', 'wos']","rawkeys":["network","recognition","images","features","neural networks","machine learning","convolutional networks","face detection"],"tags":["recognition","images","features","neural networks","machine learning","face detection","networks","convolutional neural network"]},{"p_id":30501,"title":"Will Computer-Aided Detection and Diagnosis Revolutionize Colonoscopy?","abstract":null,"keywords_author":null,"keywords_other":["RISK","Surgery, Computer-Assisted","Humans","Diagnosis, Computer-Assisted","QUALITY","SYSTEM","HISTOLOGY","Colonoscopy","RECOGNITION","SOCIETY","DIMINUTIVE COLORECTAL POLYPS","REAL-TIME FEEDBACK","CANCER","Colonic Polyps"],"max_cite":4.0,"pub_year":2017.0,"sources":"['ieee', 'scp', 'wos']","rawkeys":["colonoscopy","diagnosis","quality","recognition","computer-assisted","cancer","diminutive colorectal polyps","histology","society","system","humans","risk","surgery","colonic polyps","real-time feedback"],"tags":["colonoscopy","diagnosis","quality","recognition","computer-assisted","cancer","diminutive colorectal polyps","histology","system","humans","societies","risk","surgery","colonic polyps","real-time feedback"]},{"p_id":22317,"title":"The effect of whitening transformation on pooling operations in convolutional autoencoders","abstract":"\u00a9 2015, Li et al.; licensee Springer. Convolutional autoencoders (CAEs) are unsupervised feature extractors for high-resolution images. In the pre-processing step, whitening transformation has widely been adopted to remove redundancy by making adjacent pixels less correlated. Pooling is a biologically inspired operation to reduce the resolution of feature maps and achieve spatial invariance in convolutional neural networks. Conventionally, pooling methods are mainly determined empirically in most previous work. Therefore, our main purpose is to study the relationship between whitening processing and pooling operations in convolutional autoencoders for image classification. We propose an adaptive pooling approach based on the concepts of information entropy to test the effect of whitening on pooling in different conditions. Experimental results on benchmark datasets indicate that the performance of pooling strategies is associated with the distribution of feature activations, which can be affected by whitening processing. This provides guidance for the selection of pooling methods in convolutional autoencoders and other convolutional neural networks.","keywords_author":["Computer vision","Convolutional neural network","Deep learning","Image classification","Sparse autoencoder","Unsupervised learning","Convolutional neural network","Sparse autoencoder","Image classification","Computer vision","Unsupervised learning","Deep learning"],"keywords_other":["Deep learning","Whitening transformation","Biologically inspired","Information entropy","RECOGNITION","Convolutional neural network","Auto encoders","Pre-processing step","NETWORK","High resolution image"],"max_cite":12.0,"pub_year":2015.0,"sources":"['scp', 'wos']","rawkeys":["biologically inspired","network","recognition","deep learning","high resolution image","whitening transformation","auto encoders","information entropy","unsupervised learning","convolutional neural network","computer vision","pre-processing step","image classification","sparse autoencoder"],"tags":["biologically inspired","recognition","whitening transformation","high resolution image","auto encoders","machine learning","information entropy","stacked autoencoders","networks","unsupervised learning","convolutional neural network","computer vision","pre-processing step","image classification"]},{"p_id":112451,"title":"Vision-Based Human Action Classification Using Adaptive Boosting Algorithm","abstract":"Precise recognition of human action is a key enabler for the development of many applications, including autonomous robots for medical diagnosis and surveillance of elderly people in home environment. This paper addresses the human action recognition based on variation in body shape. Specifically, we divide the human body into five partitions that correspond to five partial occupancy areas. For each frame, we calculated area ratios and used them as input data for recognition stage. Here, we consider six classes of activities namely: walking, standing, bending, lying, squatting, and sitting. In this paper, we proposed an efficient human action recognition scheme, which takes advantages of the superior discrimination capacity of adaptive boosting algorithm. We validated the effectiveness of this approach by using experimental data from two publicly available databases fall detection databases from the University of Rzeszow's and the Universidad de Malaga fall detection data sets. We provided comparisons of the proposed approach with the state-of-the-art classifiers based on the neural network, K-nearest neighbor, support vector machine, and naive Bayes and showed that we achieve better results in discriminating human gestures.","keywords_author":["Fall detection","cascade classifier","gesture recognition","vision computing"],"keywords_other":["FALL DETECTION","FEATURES","SYSTEM","RECOGNITION","ACCELEROMETER","NETWORK","ADABOOST","SENSORS"],"max_cite":0.0,"pub_year":2018.0,"sources":"['wos']","rawkeys":["network","recognition","features","sensors","gesture recognition","system","cascade classifier","fall detection","adaboost","vision computing","accelerometer"],"tags":["recognition","features","sensors","gesture recognition","system","adaboost","fall detection","networks","vision computing","accelerometer","cascade classifiers"]},{"p_id":22342,"title":"Quality of social experience explains the relation between extraversion and positive affect","abstract":"\u00a9 2014 American Psychological Association. The personality trait extraversion is associated with higher positive affect, and individuals who behave in an extraverted way experience increased positive affect. Across 2 studies, we examine whether the positive affectivity of extraverts can be explained in terms of qualitative aspects of social experience resulting from extraverted (i.e., bold, assertive) behavior. In our first study (N = 225, 58% female), we found that social well-being, a broad measure of quality of social life (Keyes, 1998) was a significant mediator of the relation between trait extraversion and trait positive affect. This effect was specific to 1 aspect of social well-being-social contribution, one's sense of making an impact on one's social world. In our second study (N = 81, 75% female), we found that a momentary assessment of social well-being mediated the effect of experimentally manipulated extraverted behavior (in the context of 2 brief discussion tasks) on state positive affect. Furthermore, perceived contribution to the discussion tasks accounted for up to 70% of the effect of enacted extraversion on positive affect. This is the first identified mediator of the effect of enacted extraversion on positive affect. Implications and suggestions for extensions of this research are discussed.","keywords_author":["Counterdispositional behavior","Extraversion","Positive affect","Social contribution","Social well-being"],"keywords_other":["Social Behavior","Adolescent","Male","Young Adult","Humans","Affect","Interpersonal Relations","Social Skills","Female","Extraversion (Psychology)"],"max_cite":12.0,"pub_year":2015.0,"sources":"['scp']","rawkeys":["adolescent","social well-being","social behavior","male","social skills","humans","extraversion","young adult","positive affect","social contribution","interpersonal relations","counterdispositional behavior","extraversion (psychology)","female","affect"],"tags":["adolescent","social well-being","recognition","social behavior","male","social skills","humans","young adult","positive affect","social contribution","interpersonal relations","counterdispositional behavior","female","affect"]},{"p_id":112467,"title":"Efficient and self-adaptive in-situ learning in multilayer memristor neural networks","abstract":"Memristors with tunable resistance states are emerging building blocks of artificial neural networks. However, in situ learning on a large-scale multiple-layer memristor network has yet to be demonstrated because of challenges in device property engineering and circuit integration. Here we monolithically integrate hafnium oxide-based memristors with a foundrymade transistor array into a multiple-layer neural network. We experimentally demonstrate in situ learning capability and achieve competitive classification accuracy on a standard machine learning dataset, which further confirms that the training algorithm allows the network to adapt to hardware imperfections. Our simulation using the experimental parameters suggests that a larger network would further increase the classification accuracy. The memristor neural network is a promising hardware platform for artificial intelligence with high speed-energy efficiency.","keywords_author":null,"keywords_other":["CROSS-POINT DEVICES","CIRCUIT","CLASSIFICATION","DESIGN","RECOGNITION","SYNAPSES"],"max_cite":0.0,"pub_year":2018.0,"sources":"['ieee', 'wos']","rawkeys":["design","recognition","synapses","circuit","cross-point devices","classification"],"tags":["design","recognition","synapses","circuits","cross-point devices","classification"]},{"p_id":71519,"title":"Audio Surveillance of Roads: A System for Detecting Anomalous Sounds","abstract":"In the last decades, several systems based on video analysis have been proposed for automatically detecting accidents on roads to ensure a quick intervention of emergency teams. However, in some situations, the visual information is not sufficient or sufficiently reliable, whereas the use of microphones and audio event detectors can significantly improve the overall reliability of surveillance systems. In this paper, we propose a novel method for detecting road accidents by analyzing audio streams to identify hazardous situations such as tire skidding and car crashes. Our method is based on a two-layer representation of an audio stream: at a low level, the system extracts a set of features that is able to capture the discriminant properties of the events of interest, and at a high level, a representation based on a bag-of-words approach is then exploited in order to detect both short and sustained events. The deployment architecture for using the system in real environments is discussed, together with an experimental analysis carried out on a data set made publicly available for benchmarking purposes. The obtained results confirm the effectiveness of the proposed approach.","keywords_author":["Hazard detection","accident detection","audio events","audio detection","tire skidding","car crashes"],"keywords_other":["NOISE","RECOGNITION","MODELS","BEHAVIOR ANALYSIS","ISSUES"],"max_cite":12.0,"pub_year":2016.0,"sources":"['wos']","rawkeys":["recognition","audio detection","noise","car crashes","behavior analysis","accident detection","tire skidding","issues","models","audio events","hazard detection"],"tags":["accident detections","recognition","audio detection","model","noise","car crashes","behavior analysis","tire skidding","issues","audio events","hazard detection"]},{"p_id":14176,"title":"First steps toward an electronic field guide for plants","abstract":"We describe an ongoing project to digitize information about plant specimens and make it available to botanists in the field. This first requires digital images and models, and then effective retrieval and mobile computing mechanisms for accessing this information. We have almost completed a digital archive of the collection of type specimens at the Smithsonian Institution Department of Botany. Using these and additional images, we have also constructed prototype electronic field guides for the flora of Plummers Island. Our guides use a novel computer vision algorithm to compute leaf similarity. This algorithm is integrated into image browsers that assist a user in navigating a large collection of images to identify the species of a new specimen. For example, our systems allow a user to photograph a leaf and use this image to retrieve a set of leaves with similar shapes. We measured the effectiveness of one of these systems with recognition experiments on a large dataset of images, and with user studies of the complete retrieval system. In addition, we describe future directions for acquiring models of more complex, 3D specimens, and for using new methods in wearable computing to interact with data in the 3D environment in which it is acquired.","keywords_author":["Augmented reality","Computer vision","Content-based image retrieval","Electronic field guide","Inner-distance","Mobile computing","Recognition","Shape matching","Species identification","Type specimens","Wearable computing"],"keywords_other":null,"max_cite":64.0,"pub_year":2006.0,"sources":"['scp', 'wos']","rawkeys":["species identification","recognition","electronic field guide","mobile computing","type specimens","augmented reality","inner-distance","computer vision","content-based image retrieval","shape matching","wearable computing"],"tags":["species identification","recognition","electronic field guide","mobile computing","type specimens","augmented reality","inner-distance","computer vision","content-based image retrieval","shape matching","wearable computing"]},{"p_id":22368,"title":"Multimodal emotional state recognition using sequence-dependent deep hierarchical features","abstract":"\u00a9 2015 The Authors. Emotional state recognition has become an important topic for human-robot interaction in the past years. By determining emotion expressions, robots can identify important variables of human behavior and use these to communicate in a more human-like fashion and thereby extend the interaction possibilities. Human emotions are multimodal and spontaneous, which makes them hard to be recognized by robots. Each modality has its own restrictions and constraints which, together with the non-structured behavior of spontaneous expressions, create several difficulties for the approaches present in the literature, which are based on several explicit feature extraction techniques and manual modality fusion. Our model uses a hierarchical feature representation to deal with spontaneous emotions, and learns how to integrate multiple modalities for non-verbal emotion recognition, making it suitable to be used in an HRI scenario. Our experiments show that a significant improvement of recognition accuracy is achieved when we use hierarchical features and multimodal information, and our model improves the accuracy of state-of-theart approaches from 82.5% reported in the literature to 91.3% for a benchmark dataset on spontaneous emotion expressions.","keywords_author":["Convolutional Neural Networks","Deep learning","Emotion recognition","Hierarchical features","Human Robot Interaction","Emotion recognition","Deep learning","Convolutional Neural Networks","Hierarchical features","Human Robot Interaction"],"keywords_other":["Deep learning","Emotions","Learning","FACE","Humans","Recognition (Psychology)","Emotion recognition","Multi-modal information","BODY GESTURE","MODEL","Recognition accuracy","State-of-the-art approach","Convolutional neural network","Robotics","Feature extraction techniques","Hierarchical features"],"max_cite":12.0,"pub_year":2015.0,"sources":"['ieee', 'scp', 'wos']","rawkeys":["feature extraction techniques","convolutional neural networks","emotions","model","deep learning","recognition (psychology)","hierarchical features","learning","humans","human robot interaction","emotion recognition","multi-modal information","recognition accuracy","convolutional neural network","robotics","state-of-the-art approach","face","body gesture"],"tags":["feature extraction techniques","recognition","model","emotion","machine learning","hierarchical features","human-robot interaction","humans","recognition accuracy","emotion recognition","multi-modal information","robotics","convolutional neural network","state-of-the-art approach","face","body gesture"]},{"p_id":14189,"title":"Minimum spanning tree based one-class classifier","abstract":"In the problem of one-class classification one of the classes, called the target class, has to be distinguished from all other possible objects. These are considered as non-targets. The need for solving such a task arises in many practical applications, e.g. in machine fault detection, face recognition, authorship verification, fraud recognition or person identification based on biometric data. This paper proposes a new one-class classifier, the minimum spanning tree class descriptor (MST_CD). This classifier builds on the structure of the minimum spanning tree constructed on the target training set only. The classification of test objects relies on their distances to the closest edge of that tree, hence the proposed method is an example of a distance-based one-class classifier. Our experiments show that the MST_CD performs especially well in case of small sample size problems and in high-dimensional spaces. \u00a9 2008 Elsevier B.V. All rights reserved.","keywords_author":["Class oriented description","Minimum spanning tree","Novelty detection","One-class classification","Recognition"],"keywords_other":["One-class classification","Recognition","Class oriented description","Minimum spanning tree","Novelty detection"],"max_cite":63.0,"pub_year":2009.0,"sources":"['scp']","rawkeys":["recognition","novelty detection","minimum spanning tree","class oriented description","one-class classification"],"tags":["recognition","novelty detection","class oriented description","one-class classification","minimum spanning trees"]},{"p_id":71537,"title":"Short-term speed predictions exploiting big data on large urban road networks","abstract":"Big data from floating cars supply a frequent, ubiquitous sampling of traffic conditions on the road network and provide great opportunities for enhanced short-term traffic predictions based on real-time information on the whole network. Two network-based machine learning models, a Bayesian network and a neural network, are formulated with a double star framework that reflects time and space correlation among traffic variables and because of its modular structure is suitable for an automatic implementation on large road networks. Among different mono-dimensional time-series models, a seasonal autoregressive moving average model (SARMA) is selected for comparison. The time-series model is also used in a hybrid modeling framework to provide the Bayesian network with an a priori estimation of the predicted speed, which is then corrected exploiting the information collected on other links. A large floating car data set on a sub-area of the road network of Rome is used for validation. To account for the variable accuracy of the speed estimated from floating car data, a new error indicator is introduced that relates accuracy of prediction to accuracy of measure. Validation results highlighted that the spatial architecture of the Bayesian network is advantageous in standard conditions, where a priori knowledge is more significant, while mono-dimensional time series revealed to be more valuable in the few cases of non-recurrent congestion conditions observed in the data set. The results obtained suggested introducing a supervisor framework that selects the most suitable prediction depending on the detected traffic regimes. (C) 2016 Elsevier Ltd. All rights reserved.","keywords_author":["Short-term traffic predictions","Big data","Floating car data","Measure accuracy","Bayesian networks","Neural networks","SARMA models","Supervised learning"],"keywords_other":["HIGHWAY","CELL TRANSMISSION MODEL","TRAFFIC STATE ESTIMATION","UNCERTAINTY QUANTIFICATION","ALGORITHM","FREQUENCY","DEPENDENCE","KALMAN FILTER","NEURAL-NETWORK","FLOW PREDICTION"],"max_cite":10.0,"pub_year":2016.0,"sources":"['wos']","rawkeys":["algorithm","floating car data","short-term traffic predictions","supervised learning","highway","big data","cell transmission model","neural networks","frequency","measure accuracy","flow prediction","kalman filter","neural-network","uncertainty quantification","bayesian networks","traffic state estimation","dependence","sarma models"],"tags":["short-term traffic predictions","floating car data","supervised learning","recognition","measurement accuracy","highway","big data","cell transmission model","neural networks","frequency","flow prediction","kalman filter","uncertainty quantification","bayesian networks","algorithms","traffic state estimation","sarma models"]},{"p_id":71548,"title":"Traffic Analytics With Low-Frame-Rate Videos","abstract":"In this paper, we investigate the possibility of monitoring highway traffic based on videos whose frame rate is too low to accurately estimate motion features. The goal of the proposed method is to recognize traffic conditions instead of measuring them, as is usually the case. The main advantage of our approach comes from its ability to process low-frame-rate videos for which motion features cannot be estimated. Our method takes advantage of the highly redundant nature of traffic scenes that are pictured from a top-down perspective showing vehicles on a predominant asphalted road surrounded by background objects. Due to the limited variety of objects pictured in traffic scenes, our method gets to learn features that are specific to such images. With these features, our method is able to segment traffic images, classify traffic scenes, and estimate traffic density without requiring motion features. Different convolutional neural network models are proposed to segment traffic images in three different classes (Road, Car, and Background), classify traffic images into different categories (Empty, Fluid, Heavy, and Jam), and predict traffic density. We also propose a procedure to perform transfer learning of any of these models to new traffic scenes.","keywords_author":["Convolutional neural network (CNN)","traffic density","transfer learning","video surveillance"],"keywords_other":["CLASSIFICATION","SYSTEM","RECOGNITION","SURVEILLANCE","CONVOLUTIONAL NEURAL-NETWORKS"],"max_cite":0.0,"pub_year":2018.0,"sources":"['ieee', 'wos']","rawkeys":["recognition","transfer learning","system","convolutional neural network (cnn)","classification","convolutional neural-networks","traffic density","video surveillance","surveillance"],"tags":["recognition","traffic densities","transfer learning","system","classification","convolutional neural network","video surveillance","surveillance"]},{"p_id":14206,"title":"Data mining for occupational injuries in the Taiwan construction industry","abstract":"There is a higher rate of occupational injury in the construction industry than most other industries on average. However, steps can be taken to reduce worker risk through effective injury prevention strategies. In this article, association rule mining is employed in identifying the characteristics of occupational injuries in the construction industry. Accident reports during the period 1999-2004 are extracted from case reports of the Northern Region Inspection Office of the Council of Labor Affairs of Taiwan. In addition to general factors, several factors related to weather conditions are included in this article. The results show that there are some patterns of occupational injuries in the construction industry. The effect of rain on the occurrence of fatalities is of great significance. Proposed inspection plans should be in accordance with the type of construction and environmental evaluation. The findings identified in this article provide a direction for more effective inspection strategies and injury prevention programs. \u00a9 2007 Elsevier Ltd. All rights reserved.","keywords_author":["Association rule","Construction industry","Occupational fatalities"],"keywords_other":["inspection strategies","case reports","Injury prevention","Association rule mining (ARM)","Elsevier (CO)","Weather conditions","Occupational injuries","Environmental evaluation","Inspection planning","General (CO)"],"max_cite":63.0,"pub_year":2008.0,"sources":"['scp']","rawkeys":["association rule mining (arm)","occupational fatalities","injury prevention","case reports","association rule","inspection strategies","occupational injuries","environmental evaluation","general (co)","construction industry","inspection planning","weather conditions","elsevier (co)"],"tags":["occupational fatalities","recognition","association rule mining","case reports","injury prevention","inspection strategies","occupational injuries","environmental evaluation","construction industry","inspection planning","weather conditions","elsevier (co)","association rules"]},{"p_id":30605,"title":"Function approximation with evolved multilayer perceptions","abstract":"This paper presents the application of a method (G-Prop) based on an evolutionary algorithm (EA) and backpropagation (BP) to solve function approximation problems. The EA selects the multilayer perceptron (MLP) initial weights and learning rate, and changes the number of neurons in the hidden layer through the application of specific variation operators, one of which is BP training. The EA works on the initial weights and structure of the MLP, which is then trained using QuickProp in order to compute its fitness; thus G-Prop combines the advantages of the global search performed by the EA over the MLP parameter space and the local search of the BP algorithm. Besides, variation operators are directly applied to the MLP object, not needing any other kind of representation. G-Prop algorithm has been tested by applying it to two commonly used function approximation problems, and compared with several methods, proving that it obtains good results on approximation ability.","keywords_author":["Function Approximation","Generalization","Machine Learning","Neuro Evolutionary","Optimization"],"keywords_other":["Function approximation","Multilayer competitive learning","Optimal learning parameters"],"max_cite":4.0,"pub_year":2001.0,"sources":"['scp']","rawkeys":["function approximation","optimal learning parameters","machine learning","generalization","multilayer competitive learning","optimization","neuro evolutionary"],"tags":["function approximation","recognition","optimal learning parameters","machine learning","multilayer competitive learning","optimization","neuro evolutionary"]},{"p_id":63373,"title":"Correcting Instrumental Variation and Time-Varying Drift Using Parallel and Serial Multitask Learning","abstract":"When instruments and sensor systems are used to measure signals, the posterior distribution of test samples often drifts from that of the training ones, which invalidates the initially trained classification or regression models. This may be caused by instrumental variation, sensor aging, and environmental change. We introduce transfer-sample-based multitask learning (TMTL) to address this problem, with a special focus on applications in machine olfaction. Data collected with each device or in each time period define a domain. Transfer samples are the same group of samples measured in every domain. They are used by our method to share knowledge across domains. Two paradigms, parallel and serial transfer, are designed to deal with different types of drift. A dynamic model strategy is proposed to predict samples with known acquisition time. Experiments on three realworld data sets confirm the efficacy of the proposed methods. They achieve good accuracy compared with traditional featurelevel drift correction algorithms and typical labeled-sample-based MTL methods, with few transfer samples needed. TMTL is a practical algorithm framework which can greatly enhance the robustness of sensor systems with complex drift.","keywords_author":["Drift correction","machine olfaction","multitask learning (MTL)","transfer learning","transfer sample"],"keywords_other":["CALIBRATION TRANSFER","PREDICTION","SENSOR DRIFT","SIGNAL","DESIGN","RECOGNITION","ELECTRONIC-NOSE","COMPENSATION"],"max_cite":1.0,"pub_year":2017.0,"sources":"['wos']","rawkeys":["design","transfer sample","recognition","transfer learning","prediction","calibration transfer","machine olfaction","multitask learning (mtl)","electronic-nose","sensor drift","drift correction","compensation","signal"],"tags":["signals","design","transfer sample","recognition","transfer learning","prediction","calibration transfer","machine olfaction","multitask learning","sensor drift","drift correction","compensation","electronic nose"]},{"p_id":63375,"title":"A Novel Semi-Supervised Learning Approach in Artificial Olfaction for E-Nose Application","abstract":"Artificial olfaction data are usually represented by a sensor array embedded in an electronic nose system (E-Nose), such that each observation can be expressed as a feature vector for pattern recognition. The concerns of this paper are threefold: 1) each feature can be represented by multiple different modalities; 2) manual labeling of sensory data in real application is difficult and hardly impossible, which results in an issue of insufficient labeled data; and 3) classifier learning is generally independent of feature engineering, such that the recognition capability of E-Nose is restricted due to the unilateral suboptimum. Motivated by these concerns, in this paper, from a new perspective of multi-task learning, we aim at proposing a unified semi-supervised learning framework nominated as MFKS, and the merits are composed of three points. First, a multi-feature joint classifier learning with low-rank constraint is developed for exploiting the structural information of multiple feature modalities. The relatedness of sub-classifiers with respect to feature modalities is preserved by imposing a low-rank constraint on the group classifier. Second, with a manifold assumption, a Laplacian graph manifold regularization is incorporated for capturing the intrinsic geometry of unlabeled data. Third, the features and classifiers are learned simultaneously in a unified framework, such that the optimality and robustness are improved. Experiments on two data sets, including large-scale 16-sensor data with 36-month drift and small-scale temperature modulated sensory data, demonstrate that the proposed approach has 4% improvement in classification accuracy than others.","keywords_author":["Artificial olfactory system","electronic nose","multi-feature learning","semi-supervised learning"],"keywords_other":["MACHINE","FEATURE-SELECTION","ELECTRONIC NOSE","INDOOR AIR CONTAMINANTS","CLASSIFIER ENSEMBLE","GAS SENSOR ARRAY","RECOGNITION","COUNTERACTION","NONLINEAR DIMENSIONALITY REDUCTION","DRIFT COMPENSATION"],"max_cite":6.0,"pub_year":2016.0,"sources":"['wos']","rawkeys":["recognition","classifier ensemble","multi-feature learning","drift compensation","machine","artificial olfactory system","feature-selection","gas sensor array","semi-supervised learning","nonlinear dimensionality reduction","indoor air contaminants","electronic nose","counteraction"],"tags":["recognition","multi-feature learning","drift compensation","machine","artificial olfactory system","gas sensor array","semi-supervised learning","feature selection","nonlinear dimensionality reduction","indoor air contaminants","electronic nose","classifier ensembles","counteraction"]},{"p_id":79773,"title":"A new framework for mobile robot trajectory tracking using depth data and learning algorithms","abstract":"This paper proposes a framework for trajectory tracking of wheeled robots in indoor environment. Being robust against uncertainty and scalability to large environments are essential factors for this task. Here, it is supposed that the robot is only equipped with a vision system e.g. a Kinect camera. Generally, some challenges in this problem are: determining suitable control architecture, adjusting the parameters of this architecture according to the given purposes, extracting proper features from high-dimensional input images. In this paper, using deep learning methods the proper features are extracted. The controller is designed based on weighted sum of these features. A new method to combine supervised learning and reinforcement learning is introduced to adjust the proposed controller parameters. The mobile robot and the experimental environments are established on the WEBOTS and MATLAB co-simulation platform. The simulation experimental results indicate that the designed control is robust and effective for tracking trajectories in the indoor environment.","keywords_author":["Depth data","mobile robot","reinforcement learning","trajectory tracking"],"keywords_other":["REINFORCEMENT","REPRESENTATION","VISION","NEURAL-NETWORKS","NAVIGATION"],"max_cite":0.0,"pub_year":2018.0,"sources":"['ieee', 'wos']","rawkeys":["neural-networks","trajectory tracking","reinforcement","representation","mobile robot","vision","depth data","reinforcement learning","navigation"],"tags":["trajectory tracking","recognition","neural networks","representation","vision","depth data","mobile robots","reinforcement learning","navigation"]},{"p_id":63428,"title":"VPPAW penetration monitoring based on fusion of visual and acoustic signals using t-SNE and DBN model","abstract":"It is a big challenge to identify the joint penetration status for obtaining high-quality weld joints during variable polarity plasma arc welding (VPPAW).This paper addresses \"t-stochastic neighbor embedding\" (t-SNE) and deep belief network (DBN) to perform VPPAW process monitoring and penetration status identification. The multi source weld information under different conditions is simultaneously received by using visual and acoustic sensors. Time and frequency domains features extracting from the sensors constitute a fusing feature set to reflect the variation trend of weld penetration status. Using the obtained feature vectors, t-SNE method is proposed to acquire the intrinsic features corresponding to different penetration states and map them into a 3 dimensional space to realize visualization. The visualization result produced by t-SNE is significantly better than PCA or Isomap technique. Then the DBN classification model with the optimal structure is developed to guarantee effective identification of penetration status. Experimental verification and comparisons show that the\"classification performance of DBN can reach 97.62% which indicates DBN outperforms back-propagation neural network (BPNN) and support vector machine (SVM) models. The proposed methodology based on the combination of t-SNE and DBN might be regarded as a promising technique for VPPAW penetration status identification. (C) 2017 Elsevier Ltd. All rights reserved.","keywords_author":["VPPAW","Penetration monitoring","Deep belief network","Manifold learning","Feature extracting"],"keywords_other":["KEYHOLE","CLASSIFICATION","COMPONENT ANALYSIS","NEURAL-NETWORKS","RECOGNITION","NONLINEAR DIMENSIONALITY REDUCTION"],"max_cite":0.0,"pub_year":2017.0,"sources":"['ieee', 'wos']","rawkeys":["neural-networks","recognition","keyhole","feature extracting","vppaw","manifold learning","classification","deep belief network","component analysis","nonlinear dimensionality reduction","penetration monitoring"],"tags":["recognition","keyhole","neural networks","vppaw","manifold learning","classification","feature extraction","component analysis","nonlinear dimensionality reduction","deep belief networks","penetration monitoring"]},{"p_id":63430,"title":"Evaluation of vehicle interior sound quality using a continuous restricted Boltzmann machine-based DBN","abstract":"The perception of vehicle interior sound quality is important for passengers. In this paper, a feature fusion process for extracting the characteristics of vehicle interior noise is studied, and an improved deep belief network (DBN) that uses continuous restricted Boltzmann machines (CRBMs) to model continuous data is proposed. Six types of vehicles are used for recording interior noise under different working conditions, and a corresponding subjective evaluation is implemented. Psychoacoustic metrics and energy-based criteria using the wavelet transform (WT), wavelet packet transform (WPT), empirical mode decomposition (EMD), critical-band-based pass filter, and Mel-scale-based triangular filer approaches have been applied to extract interior noise features and then develop a fusing feature set combining psychoacoustic metrics and critical band energy based on comparisons. Using the obtained fusion feature set, a CRBM-based DBN (CRBM-DBN) model is developed through experiments. The newly developed model is verified by comparing its performance relative to multiple linear regression (MLR), backpropagation neural network (BPNN), and support vector machine (SVM) models. The results show that the proposed CRBM-DBN model has a lower prediction error and higher correlation coefficient with human perception compared to the other considered methods. In addition, CRBM-DBN outperforms BPNN and SVM in terms of stability and reliability. The presented approach may be regarded as a promising method for evaluating vehicle noise. (C) 2016 Elsevier Ltd. All rights reserved.","keywords_author":["Sound quality evaluation","Vehicle interior noise","Feature extraction","Continuous restricted Boltzmann machines","Deep belief networks"],"keywords_other":["ARTIFICIAL NEURAL-NETWORK","ROUGHNESS","PREDICTION","DIAGNOSIS","NOISE","ALGORITHM","DEEP BELIEF NETWORKS","MODEL","RECOGNITION","INDEX"],"max_cite":7.0,"pub_year":2017.0,"sources":"['ieee', 'wos']","rawkeys":["diagnosis","algorithm","roughness","recognition","model","noise","continuous restricted boltzmann machines","artificial neural-network","prediction","index","sound quality evaluation","feature extraction","vehicle interior noise","deep belief networks"],"tags":["diagnosis","roughness","recognition","model","noise","continuous restricted boltzmann machines","neural networks","prediction","vehicle interior noise","index","sound quality evaluation","feature extraction","algorithms","deep belief networks"]},{"p_id":63444,"title":"Finite-sensor fault-diagnosis simulation study of gas turbine engine using information entropy and deep belief networks","abstract":"Precise fault diagnosis is an important part of prognostics and health management. It can avoid accidents, extend the service life of the machine, and also reduce maintenance costs. For gas turbine engine fault diagnosis, we cannot install too many sensors in the engine because the operating environment of the engine is harsh and the sensors will not work in high temperature, at high rotation speed, or under high pressure. Thus, there is not enough sensory data from the working engine to diagnose potential failures using existing approaches. In this paper, we consider the problem of engine fault diagnosis using finite sensory data under complicated circumstances, and propose deep belief networks based on information entropy, IE-DBNs, for engine fault diagnosis. We first introduce several information entropies and propose joint complexity entropy based on single signal entropy. Second, the deep belief networks (DBNs) is analyzed and a logistic regression layer is added to the output of the DBNs. Then, information entropy is used in fault diagnosis and as the input for the DBNs. Comparison between the proposed IE-DBNs method and state-of-the-art machine learning approaches shows that the IE-DBNs method achieves higher accuracy.","keywords_author":["Deep belief networks (DBNs)","Fault diagnosis","Information entropy","Engine"],"keywords_other":["PACKET","ALGORITHM","CLASSIFICATION","KAISER ENERGY OPERATOR","SYSTEM","RECOGNITION","SPECTRUM","CONVOLUTIONAL NEURAL-NETWORKS"],"max_cite":3.0,"pub_year":2016.0,"sources":"['ieee', 'wos']","rawkeys":["algorithm","recognition","spectrum","kaiser energy operator","system","information entropy","deep belief networks (dbns)","classification","convolutional neural-networks","engine","packet","fault diagnosis"],"tags":["recognition","spectrum","kaiser energy operator","system","information entropy","engines","classification","convolutional neural network","algorithms","packet","deep belief networks","fault diagnosis"]},{"p_id":55254,"title":"Full-automatic computer aided system for stem cell clustering using content-based microscopic image analysis","abstract":"Stem cells are very original cells that can differentiate into other cells, tissues and organs, which play a very important role in biomedical treatments. Because of the importance of stem cells, in this paper we propose a full-automatic computer aided clustering system to assist scientists to explore potential co-occurrence relations between the cell differentiation and their morphological information in phenotype. In this proposed system, a multi-stage Content-based Microscopic Image Analysis (CBMIA) framework is applied, including image segmentation, feature extraction, feature selection, feature fusion and clustering techniques. First, an Improved Supervised Normalized Cuts (ISNC) segmentation algorithm is newly introduced to partition multiple stem cells into individual regions in an original microscopic image, which is the most important contribution in this paper. Then, based on the segmented stem cells, 11 different feature extraction approaches are applied to represent the morphological characteristics of them. Thirdly, by analysing the robustness and stability of the extracted features, Hu and Zernike moments are selected. Fourthly, these two selected features are combined by an early fusion approach to further enhance the properties of the feature representation of stem cells. Finally, k-means clustering algorithm is chosen to classify stem cells into different categories using the fused feature. Furthermore, in order to prove the effectiveness and usefulness of this proposed system, we carry out a series of experiments to evaluate our methods. Especially, our ISNC segmentation obtains 92.4% similarity, 96.0% specificity and 107.8% ration of accuracy, showing the potential of our work. (C) 2017 Nalecz Institute of Biocybernetics and Biomedical Engineering of the Polish Academy of Sciences. Published by Elsevier B.V. All rights reserved.","keywords_author":["Stem cell","Biomedical microscopic image","Content-based Microscopic Image","Analysis","Image Segmentation","Supervised Normalized Cuts","Cell clustering"],"keywords_other":["SELECTION","COMPUTATION","FEATURES","ALGORITHM","CLASSIFICATION","RECOGNITION","SEGMENTATION","MOMENTS","NUCLEI","TRACKING"],"max_cite":0.0,"pub_year":2017.0,"sources":"['wos']","rawkeys":["algorithm","biomedical microscopic image","moments","recognition","segmentation","features","nuclei","content-based microscopic image","stem cell","tracking","classification","analysis","cell clustering","computation","selection","supervised normalized cuts","image segmentation"],"tags":["biomedical microscopic image","moments","recognition","segmentation","features","neural networks","nuclei","content-based microscopic image","stem-cells","tracking","analysis","classification","algorithms","selection","cell clusters","supervised normalized cuts","image segmentation"]},{"p_id":63452,"title":"The early-warning model of equipment chain in gas pipeline based on DNN-HMM","abstract":"Since the operating state of the compressor unit could be influenced by several factors including connected pipeline, auxiliary system and other related equipment, it is necessary to treat the compressor unit as a sub-chain of the whole pipeline equipment chain. To deal with the indistinguishable phenomena in the compressor unit, including pipeline leakage, ice jam and auxiliary system failure, an innovative early-warning model based on analyses of characteristics of early-warning system and equipment chain is proposed in this thesis, which fully takes advantage of feature extraction of deep belief network (DNN) and hidden state analysis of hidden Markov model (HMM) to estimate the operating status of the compressor unit. Validated by field data, the model is demonstrated to be of preferable accuracy and generalization for early-warning of the equipment chain by results of experiments. Moreover, it is advantageous in terms of processing speed. (C) 2015 Elsevier B.V. All rights reserved.","keywords_author":["Early-warning","Equipment chain","Compressor unit","Deep belief networks","Hidden Markov model"],"keywords_other":["FAULT-DIAGNOSIS","ALGORITHM","CLASSIFICATION","DEEP BELIEF NETWORKS","IDENTIFICATION APPROACH","RECOGNITION","COMPRESSOR"],"max_cite":4.0,"pub_year":2015.0,"sources":"['ieee', 'wos']","rawkeys":["algorithm","early-warning","compressor","identification approach","recognition","equipment chain","compressor unit","classification","hidden markov model","fault-diagnosis","deep belief networks"],"tags":["hidden markov models","early warning","compressor","identification approach","recognition","equipment chain","compressor unit","classification","algorithms","deep belief networks","fault diagnosis"]},{"p_id":14302,"title":"Driver crash risk factors and prevalence evaluation using naturalistic driving data","abstract":"The accurate evaluation of crash causal factors can provide fundamental information for effective transportation policy, vehicle design, and driver education. Naturalistic driving (ND) data collected with multiple onboard video cameras and sensors provide a unique opportunity to evaluate risk factors during the seconds leading up to a crash. This paper uses a National Academy of Sciences-sponsored ND dataset comprising 905 injurious and property damage crash events, the magnitude of which allows the first direct analysis (to our knowledge) of causal factors using crashes only. The results show that crash causation has shifted dramatically in recent years, with driver-related factors (i.e., error, impairment, fatigue, and distraction) present in almost 90% of crashes. The results also definitively show that distraction is detrimental to driver safety, with handheld electronic devices having high use rates and risk.","keywords_author":["Crash risk","Driver distraction","Driver error","Driver impairment","Naturalistic driving"],"keywords_other":["Humans","Cities","Aged, 80 and over","Accidents, Traffic","Adolescent","Aged","Models, Theoretical","Algorithms","Odds Ratio","Databases, Factual","Automobile Driving","United States","Sleep Stages","Young Adult","Middle Aged","Stress, Psychological","Adult","Attention","Risk Factors","Fatigue"],"max_cite":61.0,"pub_year":2016.0,"sources":"['scp']","rawkeys":["automobile driving","databases","accidents","psychological","aged","young adult","united states","adult","adolescent","cities","driver distraction","middle aged","stress","models","algorithms","humans","attention","80 and over","driver error","crash risk","factual","driver impairment","risk factors","theoretical","sleep stages","odds ratio","traffic","fatigue","naturalistic driving"],"tags":["automobile driving","databases","accidents","aged","united-states","young adult","adult","adolescent","cities","sleep stage","middle aged","stress","algorithms","recognition","humans","attention","80 and over","driver error","crash risk","factual","driver impairment","model","risk factors","theoretical","traffic","odds ratios","fatigue","driver distractions","naturalistic driving"]},{"p_id":63455,"title":"Stacked Multilevel-Denoising Autoencoders: A New Representation Learning Approach for Wind Turbine Gearbox Fault Diagnosis","abstract":"Currently, vibration analysis has been widely considered as an effective way to fulfill the fault diagnosis task of gearboxes in wind turbines (WTs). However, vibration signals are usually with abundant noise and characterized as nonlinearity and nonstationarity. Therefore, it is quite challenging to extract robust and useful fault features from complex vibration signals to achieve an accurate and reliable diagnosis. This paper proposes a novel feature representation learning approach, named stacked multilevel-denoising autoencoders (SMLDAEs), with the aim to learn robust and discriminative fault feature representations through a deep network architecture for diagnosis accuracy improvement. In our proposed approach, we design an MLD training scheme, which uses multiple noise levels to train AEs. It enables to learn more general and detailed fault feature patterns simultaneously at different scales from the complex frequency spectra of the raw vibration data, and therefore helps enhance the feature learning and fault diagnosis capability. Furthermore, SMLDAE-based fault diagnosis is performed with an unsupervised representation learning procedure followed by a supervised fine-tuning process with label information for classification. Our approach is evaluated by using the field vibration data collected from a self-designed WT gearbox test rig. The results show that our proposed approach learned more robust and discriminative fault feature representations and achieved the best diagnosis accuracy compared with the traditional approaches.","keywords_author":["Fault diagnosis","multilevel-denoising (MLD) training","stacked denoising autoencoders (SDAEs)","vibration representation learning","wind turbine (WT) gearbox"],"keywords_other":["MACHINERY","CLASSIFICATION","RECOGNITION","SIGNALS","DEEP NEURAL-NETWORKS"],"max_cite":2.0,"pub_year":2017.0,"sources":"['ieee', 'wos']","rawkeys":["signals","vibration representation learning","stacked denoising autoencoders (sdaes)","recognition","deep neural-networks","fault diagnosis","classification","machinery","wind turbine (wt) gearbox","multilevel-denoising (mld) training"],"tags":["signals","multilevel-denoising (mld) training","vibration representation learning","recognition","classification","machinery","convolutional neural network","wind turbine (wt) gearbox","fault diagnosis","stacked denoising autoencoder"]},{"p_id":63458,"title":"Sound quality prediction of vehicle interior noise using deep belief networks","abstract":"The sound quality of vehicle interior noise strongly influences passengers' psychological and physiological perceptions. To predict the sound quality of interior noise, a vehicle road test with four compact cars has been conducted. All recorded interior noise signals have been denoised via a discrete wavelet transform (DWT) denoising procedure and subsequently evaluated subjectively through the anchor semantic differential (ASD) test by a jury. In addition, a novel prediction method, namely, regression-based deep belief networks (DBNs), which substitute the support vector regression (SVR) layer for the linear softmax classification layer at the top of the general DBN's structure, has been proposed to predict the interior sound quality. The parameter selection of the DBN model has been compared and studied using a grid search. In addition, four conventional machine-learning-based methods have been introduced to enable a comparison of the performance with the newly developed DBNs. Furthermore, the feature fusion ability of DBNs has been studied by varying the amount of information that the dataset offers. The results show the following: (1) The accuracy and robustness of the proposed DBN-based sound quality prediction approach are better than those of the 4 other referenced methods. (2) The multiple-feature fusing process can strongly affect the prediction performance. (3) Finally, the \"unsupervised pre-training process of the DBNs can enhance the information fusing ability. Finally, the newly proposed regression-based DBN approach may be extended to address other vehicle noises in the future. (C) 2016 Elsevier Ltd. All rights reserved.","keywords_author":["Sound quality","Vehicle interior noise","Deep belief networks","Machine learning","Feature fusion"],"keywords_other":["ARTIFICIAL NEURAL-NETWORK","REGRESSION","TRANSFORM","TIME-SERIES ANALYSIS","FAULT-DIAGNOSIS","VIBRATION","OBJECTIVE EVALUATION","RECOGNITION","WIGNER-VILLE DISTRIBUTION","EMPIRICAL MODE DECOMPOSITION"],"max_cite":11.0,"pub_year":2016.0,"sources":"['ieee', 'wos']","rawkeys":["recognition","wigner-ville distribution","artificial neural-network","objective evaluation","machine learning","vehicle interior noise","sound quality","deep belief networks","fault-diagnosis","time-series analysis","transform","regression","feature fusion","vibration","empirical mode decomposition"],"tags":["recognition","neural networks","wigner-ville distribution","objective evaluation","machine learning","time series analysis","feature fusion","sound quality","vehicle interior noise","transform","regression","deep belief networks","fault diagnosis","vibration","empirical mode decomposition"]},{"p_id":96235,"title":"Context-Associative Hierarchical Memory Model for Human Activity Recognition and Prediction","abstract":"Human activity recognition is a challenging high-level vision task, for which multiple factors, such as subject, object, and their diverse interactions, have to be considered and modeled. Current learning-based methods are limited in the capability to integrate human-level concepts into an easily extensible computational framework. Inspired by the existing human memory model, we present a context-associative approach to recognize activity with human-object interaction. The proposed system can recognize incoming visual content based on the previous experienced activities. The high-level activity is parsed into consecutive subactivities, and we build a context cluster to model the temporal relations. The semantic attributes of the subactivity are organized by a concept hierarchy. Based on the hierarchy, a series of similarity functions are defined to turn the recognition computing into retrievals over the contextual memory, similar to the auto-associative characteristics of human memory. Partially matching in retrieval and stored memory make the activity prediction possible. The dynamical evolution of the brain memory is mimicked to allow decay and reinforcement of the input information, providing a natural way to maintain data and save computational time. We evaluate our approach on three data sets: CAD-120, MHOI, and OPPORTUNITY. The proposed method demonstrates promising results comparedwith other state of-the-art techniques.","keywords_author":["Human activity","human memory","instance-based learning","one-shot learning","prediction","recognition"],"keywords_other":["VIDEOS"],"max_cite":1.0,"pub_year":2017.0,"sources":"['ieee', 'wos']","rawkeys":["recognition","prediction","human activity","instance-based learning","videos","human memory","one-shot learning"],"tags":["recognition","human activities","prediction","instance based learning","video","human memory","one-shot learning"]},{"p_id":14336,"title":"Adaptive BCI Based on Variational Bayesian Kalman Filtering: An Empirical Evaluation","abstract":"This paper proposes the use of variational Kalman filtering as an inference technique for adaptive classification in a brain computer interface (BCI). The proposed algorithm translates electroencephalogram segments adaptively into probabilities of cognitive states. It, thus, allows for nonstationarities in the joint process over cognitive state and generated EEG which may occur during a consecutive number of trials. Nonstationarities may have technical reasons (e.g., changes in impedance between scalp and electrodes) or be caused by learning effects in subjects. We compare the performance of the proposed method against an equivalent static classifier by estimating the generalization accuracy and the bit rate of the BCI. Using data from two studies with healthy subjects, we conclude that adaptive classification significantly improves BCI performance. Averaging over all subjects that participated in the respective study, we obtain, depending on the cognitive task pairing, an increase both in generalization accuracy and bit rate of up to 8%. We may, thus, conclude that adaptive inference can play a significant contribution in the quest of increasing bit rates and robustness of current BCI technology. This is especially true since the proposed algorithm can be applied in real time.","keywords_author":["Adaptive classification","Bayesian learning","BCI","Variational method"],"keywords_other":["Brain computer interface","Generalization","Robustness","Learning effects"],"max_cite":61.0,"pub_year":2004.0,"sources":"['scp']","rawkeys":["robustness","adaptive classification","brain computer interface","generalization","learning effects","bci","bayesian learning","variational method"],"tags":["robustness","brain-computer interfaces","adaptive classification","recognition","learning effectiveness","variational methods","bayesian learning"]},{"p_id":96266,"title":"A Direct and Fast Methodology for Ship Recognition in Sentinel-2 Multispectral Imagery","abstract":"The European Space Agency satellite Sentinel-2 provides multispectral images with pixel sizes down to 10 m. This high resolution allows for ship detection and recognition by determining a number of important ship parameters. We are able to show how a ship position, its heading, length and breadth can be determined down to a subpixel resolution. If the ship is moving, its velocity can also be determined from its Kelvin waves. The 13 spectrally different visual and infrared images taken using multispectral imagery (MSI) are fingerprints that allow for the recognition and identification of ships. Furthermore, the multispectral image profiles along the ship allow for discrimination between the ship, its turbulent wakes, and the Kelvin waves, such that the ship's length and breadth can be determined more accurately even when sailing. The ship's parameters are determined by using satellite imagery taken from several ships, which are then compared to known values from the automatic identification system. The agreement is on the order of the pixel resolution or better.","keywords_author":["Sentinel-2","multispectral","ship","recognition","identification","turbulent wake","Kelvin waves"],"keywords_other":null,"max_cite":3.0,"pub_year":2016.0,"sources":"['ieee', 'wos']","rawkeys":["recognition","identification","turbulent wake","kelvin waves","ship","sentinel-2","multispectral"],"tags":["recognition","identification","turbulent wake","kelvin waves","multiple sclerosis","ship","sentinel-2"]},{"p_id":96275,"title":"Remote sensing image classification using extreme learning machine-guided collaborative coding","abstract":"Remote sensing image classification is a very challenging problem and covariance descriptor can be introduced in the feature extraction and representation process for remote sensing image. However, due to the reason that covariance descriptor lies in non-Euclidean manifold, conventional extreme learning machine (ELM) cannot effectively deal with this problem. In this paper, we propose an improved ELM framework which incorporates the collaborative coding to tackle the covariance descriptor classification problem. First, a new ELM-guided dictionary learning and coding model is proposed. Then the iterative optimization algorithm is developed to solve the model. By evaluating the proposed approach on the UCMERCED high-resolution aerial image dataset, we show the effectiveness of the proposed strategy.","keywords_author":["Extreme learning machine","Collaborative coding","Covariance descriptor"],"keywords_other":["NETWORKS","APPROXIMATION","RECOGNITION","SPARSE REPRESENTATION","OPTIMIZATION","KERNELS"],"max_cite":0.0,"pub_year":2016.0,"sources":"['ieee', 'wos']","rawkeys":["covariance descriptor","recognition","sparse representation","kernels","approximation","collaborative coding","networks","optimization","extreme learning machine"],"tags":["covariance descriptor","recognition","sparse representation","approximation","collaborative coding","kernel","networks","optimization","extreme learning machine"]},{"p_id":96279,"title":"A Hierarchical Maritime Target Detection Method for Optical Remote Sensing Imagery","abstract":"Maritime target detection from optical remote sensing images plays an important role in related military and civil applications and its weakness lies in its compromised performance under complex uncertain conditions. In this paper, a novel hierarchical ship detection method is proposed to overcome this issue. In the ship detection stage, based on Entropy information, we construct a combined saliency model with self-adaptive weights to prescreen ship candidates from across the entire maritime domain. To characterize ship targets and further reduce the false alarms, we introduce a novel and practical descriptor based on gradient features, and this descriptor is robust against clutter introduced by heavy clouds, islands, ship wakes as well as variation in target size. Furthermore, the proposed method is effective for not only color images but also gray images. The experimental results obtained using real optical remote sensing images have demonstrated that the locations and the number of ships can be determined accurately and that the false alarm rate is greatly decreased. A comprehensive comparison is performed between the proposed method and the state-of-the-art methods, which shows that the proposed method achieves higher accuracy and outperforms all the competing methods. Furthermore, the proposed method is robust under various backgrounds of maritime images and has great potential for providing more accurate target detection in engineering applications.","keywords_author":["remote sensing","ship detection","visual saliency","Entropy information","gradient features"],"keywords_other":["SALIENCY","MACHINE","CLASSIFICATION","MODEL","RECOGNITION","SHIP DETECTION","SATELLITE IMAGES","SPARSE REPRESENTATION","MULTISPECTRAL IMAGERY","OBJECT DETECTION"],"max_cite":5.0,"pub_year":2017.0,"sources":"['ieee', 'wos']","rawkeys":["gradient features","entropy information","model","recognition","remote sensing","machine","ship detection","sparse representation","visual saliency","satellite images","multispectral imagery","classification","object detection","saliency"],"tags":["entropy information","model","recognition","remote sensing","machine","ship detection","sparse representation","visual saliency","satellite images","multispectral imagery","classification","object detection","saliency","gradient feature"]},{"p_id":63512,"title":"Big data analytics enabled by feature extraction based on partial independence","abstract":"Complex cells in primary visual cortex (V1) selectively respond to bars and edges at a particular location and orientation. Namely, they are relatively invariant to the phase as well as selective to the frequency and orientation emerging from natural images that are analogous to the characteristics of complex cells in V1 with the energy function of receptive fields (RFs) from tuning curve test with sinusoidal function in our related jobs. In this paper, we propose a feature learning algorithm based on the overcomplete AISA to apply on big data in parallel computing. In order to demonstrate the effectiveness of the overcomplete AISA features in the classification task, two feature representation architectures are evolved into the partial independent signal bases and partial independent factorial representation, respectively. Experiments on four datasets (Coil20, Extended YaleB, USPS, PIE), acquired conjunction with two classification architectures based on the overcomplete AISA features, show that the classification accuracy is mostly higher than those obtained from the other ICA related features and two other sparse representation features with a small number of training samples via nearest neighbor (NN) classification method. (C) 2018 Elsevier B.V. All rights reserved.","keywords_author":["Independent Component(IC)","Overcomplete features","Sparse representation","Big data"],"keywords_other":["DEEP ARCHITECTURES","VISION","PHASE","COMPONENT ANALYSIS","BLIND SEPARATION","RECOGNITION","NATURAL IMAGES","EMERGENCE","ALGORITHMS","SPARSE REPRESENTATION"],"max_cite":0.0,"pub_year":2018.0,"sources":"['wos']","rawkeys":["recognition","overcomplete features","big data","natural images","sparse representation","deep architectures","vision","emergence","phase","blind separation","independent component(ic)","component analysis","algorithms"],"tags":["recognition","overcomplete features","big data","natural images","sparse representation","deep architectures","vision","emergence","phase","blind separation","independent components","component analysis","algorithms"]},{"p_id":63514,"title":"Neural Bag-of-Features learning","abstract":"In this paper, a neural learning architecture for the well-known Bag-of-Features (BoF) model, called Neural Bag-of-Features, is proposed. The Neural BoF model is formulated in two neural layers: a Radial Basis Function (RBF) layer and an accumulation layer. The ability of the Neural BoF model to improve the classification performance is demonstrated using four datasets, including a large-scale dataset, and five different feature types. The gains are two-fold: the classification accuracy increases and, at the same time, smaller networks can be used, reducing the required training and testing time. Furthermore, the Neural BoF natively supports training and classifying from feature streams. This allows the proposed method to efficiently scale to large datasets. The streaming process can also be used to introduce noise and reduce the over-fitting of the network. Finally, the Neural BoF provides a framework that can model and extend the dictionary learning methodology.","keywords_author":["Bag-of-Features","RBF neural networks","Dictionary learning"],"keywords_other":["UNIVERSAL","NETWORKS","REPRESENTATION","CLASSIFICATION","RECOGNITION","DICTIONARY","SCALE","CATEGORIZATION","TEXTURE"],"max_cite":9.0,"pub_year":2016.0,"sources":"['ieee', 'wos']","rawkeys":["bag-of-features","recognition","rbf neural networks","representation","categorization","dictionary","dictionary learning","classification","networks","texture","universal","scale"],"tags":["bag-of-features","recognition","representation","categorization","radial basis function neural networks","networks","dictionary learning","classification","texture","university","scale","dictionaries"]},{"p_id":96283,"title":"A fast and efficient conformal regressor with regularized extreme learning machine","abstract":"A conformal regressor combines conformal prediction and a traditional regressor for point predictions. It produces a valid prediction interval for a new testing input such that the probability of the target output being not included in the prediction interval is not more than a preset significance level. Although conformal prediction is both theoretically and empirically valid, one main drawback of the existing conformal regressors is their computational inefficiency. This paper proposes a novel fast and efficient conformal regressor named LW-JP-RELM, with combination of the local-weighted jackknife prediction (LW-JP), a new variant of conformal prediction, and the regularized extreme learning machine (RELM). The development of our learning algorithm is important both for the applications of extreme learning machine and conformal prediction. On the one hand, LW-JP-RELM complements ELM with interval predictions that satisfy a given level of confidence. On the other hand, the underlying learning process and the outstanding learning ability of RELM make LW-JP-RELM a very fast and informationally efficient conformal regressor. In the experiments, the empirical validity and informational efficiency of our method were compared to those of the state-of-art on 20 public data sets and the results confirmed that LW-JP-RELM is a competitive and promising conformal regressor. (c) 2018 Elsevier B.V. All rights reserved.","keywords_author":["Conformal regressor","Jackknife prediction","Extreme learning machine","Interval prediction","Computational efficiency"],"keywords_other":["PREDICTION","NEURAL-NETWORKS","RECOGNITION","PERFORMANCE","KERNEL","ELM"],"max_cite":0.0,"pub_year":2018.0,"sources":"['wos']","rawkeys":["neural-networks","recognition","performance","prediction","computational efficiency","kernel","jackknife prediction","interval prediction","extreme learning machine","conformal regressor","elm"],"tags":["recognition","performance","neural networks","prediction","kernel","jackknife prediction","interval prediction","extreme learning machine","conformal regressor","computationally efficient"]},{"p_id":47138,"title":"Dominant feature based convolutional neural network for faces in videos","abstract":"\u00a9 2017 IEEE. Standard face recognition modules are fabricated for general-purpose applications while few have been designed with speed in mind. This paper proposes an efficient architecture for face recognition in which two self-contained Convolutional Neural Networks (CNNs) are used to detect and recognize faces in regions containing a dense grouping of Features from Accelerated Segment Test (FAST). This configuration proves to be practical for videos as it is selective in its analysis of an input frame. City surveillance and public safety is a critical issue in smart cities and the deployment of Smart Video Surveillance systems is the need of the hour. Typically, the problem at hand will be person identification which is the association of a biometric trait with a particular human being. FAST key points can be generated and analyzed in near real-time and that data can be used to extract and process faces in the background. The CNNs were trained using a combination of datasets of labelled faces, videos and trivial objects. The results obtained upon analyzing the performance of the system on the ChokePoint dataset proved very insightful. This configuration leads to a very effective face recognition system.","keywords_author":["Biometrics","Convolution","Deep learning","Detection","Neural network","Recognition"],"keywords_other":["Person identification","Critical issues","Efficient architecture","Recognition","Smart video surveillance systems","Face recognition systems","Convolutional neural network","Biometric traits"],"max_cite":0.0,"pub_year":2017.0,"sources":"['scp']","rawkeys":["biometric traits","neural network","person identification","recognition","deep learning","smart video surveillance systems","convolution","critical issues","detection","biometrics","convolutional neural network","efficient architecture","face recognition systems"],"tags":["biometric traits","person identification","recognition","neural networks","smart video surveillance systems","convolution","machine learning","critical issues","detection","biometrics","convolutional neural network","efficient architecture","face recognition systems"]},{"p_id":96296,"title":"Occluded Object Detection in High-Resolution Remote Sensing Images Using Partial Configuration Object Model","abstract":"Deformable-part-based model (DPM) has shown great success in object detection in recent years. However, its performance will degrade on partially occluded objects and is even worse on largely occluded objects in real remote sensing applications. To address this problem, a novel partial configuration object model (PCM) is developed in this paper. Compared to conventional single-layer DPMs, an extra partial configuration layer, which is composed of partial configurations defined according to possible occlusion patterns, is introduced in PCM to block the transmission of occlusion impact. During detection, each hypothesis from a partial configuration layer will infer the entire object based on spatial interrelationship and final detection results are obtained from the fusion of these possible entire objects using a weighted continuous clustering method. As PCM makes a better compromise between the deformation modeling flexibility of small parts and the discriminative shape-capturing capability of large DPM, its performance on occluded object detection will be improved. Moreover, occlusion states of detected objects can be inferred with the intermediate results of our model. Experimental results on multiple high-resolution remote sensing image datasets demonstrate the effectiveness of the proposed model.","keywords_author":["Object detection","occlusion inference","partial configuration object model (PCM)","remote sensing images"],"keywords_other":["VISUAL SALIENCY","INFORMATION","NETWORKS","RECOGNITION","SEGMENTATION","AIRPORT DETECTION","SHIP DETECTION","SATELLITE IMAGES","ORIENTED GRADIENTS","SIFT KEYPOINTS"],"max_cite":5.0,"pub_year":2017.0,"sources":"['ieee', 'wos']","rawkeys":["oriented gradients","recognition","segmentation","ship detection","occlusion inference","visual saliency","satellite images","networks","airport detection","object detection","information","partial configuration object model (pcm)","remote sensing images","sift keypoints"],"tags":["oriented gradients","recognition","segmentation","ship detection","occlusion inference","visual saliency","satellite images","networks","airport detection","object detection","information","partial configuration object model (pcm)","remote sensing images","sift keypoints"]},{"p_id":96299,"title":"Classification with boosting of extreme learning machine over arbitrarily partitioned data","abstract":"Machine learning-based computational intelligence methods are widely used to analyze large-scale data sets in this age of big data. Extracting useful predictive modeling from these types of data sets is a challenging problem due to their high complexity. Analyzing large amount of streaming data that can be leveraged to derive business value is another complex problem to solve. With high levels of data availability (i.e., Big Data), automatic classification of them has become an important and complex task. Hence, we explore the power of applying MapReduce-based distributed AdaBoosting of extreme learning machine (ELM) to build a predictive bag of classification models. Accordingly, (1) data set ensembles are created; (2) ELM algorithm is used to build weak learners (classifier functions); and (3) builds a strong learner from a set of weak learners. We applied this training model to the benchmark knowledge discovery and data mining data sets.","keywords_author":["Extreme learning machine","AdaBoost","Ensemble methods","MapReduce"],"keywords_other":["FEEDFORWARD NETWORKS","FRAMEWORK","HIDDEN NODES","ALGORITHM","SUPPORT VECTOR MACHINES","RECOGNITION","PATTERN","MAPREDUCE","ELM"],"max_cite":0.0,"pub_year":2017.0,"sources":"['ieee', 'wos']","rawkeys":["algorithm","hidden nodes","pattern","recognition","framework","ensemble methods","mapreduce","support vector machines","feedforward networks","extreme learning machine","elm","adaboost"],"tags":["hidden nodes","recognition","framework","feed-forward network","ensemble methods","patterns","machine learning","map-reduce","algorithms","extreme learning machine","adaboost"]},{"p_id":38988,"title":"Supervised machine learning algorithms to diagnose stress for vehicle drivers based on physiological sensor signals","abstract":"\u00a9 2015 The authors and IOS Press. All rights reserved. Machine learning algorithms play an important role in computer science research. Recent advancement in sensor data collection in clinical sciences lead to a complex, heterogeneous data processing, and analysis for patient diagnosis and prognosis. Diagnosis and treatment of patients based on manual analysis of these sensor data are difficult and time consuming. Therefore, development of Knowledge-based systems to support clinicians in decision-making is important. However, it is necessary to perform experimental work to compare performances of different machine learning methods to help to select appropriate method for a specific characteristic of data sets. This paper compares classification performance of three popular machine learning methods i.e., case-based reasoning, neutral networks and support vector machine to diagnose stress of vehicle drivers using finger temperature and heart rate variability. The experimental results show that case-based reasoning outperforms other two methods in terms of classification accuracy. Case-based reasoning has achieved 80% and 86% accuracy to classify stress using finger temperature and heart rate variability. On contrary, both neural network and support vector machine have achieved less than 80% accuracy by using both physiological signals.","keywords_author":["Case-based reasoning","Machine-learning","Physiological sensor signal","Stress"],"keywords_other":["Heart Rate","Sensitivity and Specificity","Algorithms","Neural Networks (Computer)","Humans","Stress, Psychological","Machine Learning","Artificial Intelligence","Support Vector Machine","Automobile Driving","Monitoring, Ambulatory","Body Temperature"],"max_cite":1.0,"pub_year":2015.0,"sources":"['scp']","rawkeys":["automobile driving","artificial intelligence","ambulatory","neural networks (computer)","sensitivity and specificity","stress","machine learning","humans","psychological","case-based reasoning","machine-learning","algorithms","physiological sensor signal","support vector machine","heart rate","monitoring","body temperature"],"tags":["automobile driving","ambulatory","recognition","neural networks","sensitivity and specificity","stress","machine learning","humans","case-based reasoning","physiological sensor signal","algorithms","heart rate","monitoring","body temperature"]},{"p_id":63567,"title":"An Optimization Approach for Localization Refinement of Candidate Traffic Signs","abstract":"We propose a localization refinement approach for candidate traffic signs. Previous traffic sign localization approaches, which place a bounding rectangle around the sign, do not always give a compact bounding box, making the subsequent classification task more difficult. We formulate localization as a segmentation problem, and incorporate prior knowledge concerning color and shape of traffic signs. To evaluate the effectiveness of our approach, we use it as an intermediate step between a standard traffic sign localizer and a classifier. Our experiments use the well-known German Traffic Sign Detection Benchmark (GTSDB) as well as our new Chinese Traffic Sign Detection Benchmark. This newly created benchmark is publicly available, 1 and goes beyond previous benchmark data sets: it has over 5000 high-resolution images containing more than 14 000 traffic signs taken in realistic driving conditions. Experimental results show that our localization approach significantly improves bounding boxes when compared with a standard localizer, thereby allowing a standard traffic sign classifier to generate more accurate classification results.","keywords_author":["Traffic sign localization","optimization","graph cut"],"keywords_other":["GRAPH CUTS","SHAPE PRIORS","RECOGNITION","SEGMENTATION","RADIAL SYMMETRY"],"max_cite":3.0,"pub_year":2017.0,"sources":"['wos']","rawkeys":["shape priors","recognition","segmentation","radial symmetry","traffic sign localization","optimization","graph cuts","graph cut"],"tags":["recognition","segmentation","radial symmetry","shape prior","optimization","graph cuts","traffic sign localization"]},{"p_id":63572,"title":"Towards Real-Time Traffic Sign Detection and Classification","abstract":"Traffic sign recognition plays an important role in driver assistant systems and intelligent autonomous vehicles. Its real-time performance is highly desirable in addition to its recognition performance. This paper aims to deal with real-time traffic sign recognition, i.e., localizing what type of traffic sign appears in which area of an input image at a fast processing time. To achieve this goal, we first propose an extremely fast detection module, which is 20 times faster than the existing best detection module. Our detection module is based on traffic sign proposal extraction and classification built upon a color probability model and a color HOG. Then, we harvest from a convolutional neural network to further classify the detected signs into their subclasses within each superclass. Experimental results on both German and Chinese roads show that both our detection and classification methods achieve comparable performance with the state-of-the-art methods, with significantly improved computational efficiency.","keywords_author":["Traffic sign detection","traffic sign recognition","real-time","color probability model"],"keywords_other":["RECOGNITION","ALGORITHMS"],"max_cite":14.0,"pub_year":2016.0,"sources":"['wos']","rawkeys":["recognition","real-time","traffic sign detection","algorithms","color probability model","traffic sign recognition"],"tags":["recognition","real time","traffic sign detection","algorithms","color probability model","traffic sign recognition"]},{"p_id":55392,"title":"Deep temporal models and active inference (vol 77, pg 388, 2017)","abstract":"How do we navigate a deeply structured world? Why are you reading this sentence first and did you actually look at the fifth word? This review offers some answers by appealing to active inference based on deep temporal models. It builds on previous formulations of active inference to simulate behavioural and electrophysiological responses under hierarchical generative models of state transitions. Inverting these models corresponds to sequential inference, such that the state at any hierarchical level entails a sequence of transitions in the level below. The deep temporal aspect of these models means that evidence is accumulated over nested time scales, enabling inferences about narratives (i.e., temporal scenes). We illustrate this behaviour with Bayesian belief updating and neuronal process theories to simulate the epistemic foraging seen in reading. These simulations reproduce perisaccadic delay period activity and local field potentials seen empirically. Finally, we exploit the deep structure of these models to simulate responses to local (e.g., font type) and global (e.g., semantic) violations; reproducing mismatch negativity and P300 responses respectively.","keywords_author":["Active inference","Bayesian","Hierarchical","Reading","Violation","Free energy","P300","MMN"],"keywords_other":["MISMATCH NEGATIVITY","COMPUTATIONAL PSYCHIATRY","EVIDENCE ACCUMULATION","RECOGNITION","SHORT-TERM-MEMORY","CORTEX","SCHIZOPHRENIA","NETWORK MODEL","ORGANIZATION","SERIAL ORDER"],"max_cite":0.0,"pub_year":2018.0,"sources":"['wos']","rawkeys":["active inference","p300","mmn","recognition","organization","reading","short-term-memory","violation","schizophrenia","serial order","network model","computational psychiatry","free energy","hierarchical","cortex","mismatch negativity","evidence accumulation","bayesian"],"tags":["active inference","p300","mmn","recognition","organization","reading","short-term-memory","violation","schizophrenia","network modeling","serial order","computational psychiatry","free energy","hierarchical","cortex","mismatch negativity","evidence accumulation","bayesian"]},{"p_id":55394,"title":"Spectral-Spatial Response for Hyperspectral Image Classification","abstract":"This paper presents a hierarchical deep framework called Spectral-Spatial Response (SSR) to jointly learn spectral and spatial features of Hyperspectral Images (HSIs) by iteratively abstracting neighboring regions. SSR forms a deep architecture and is able to learn discriminative spectral-spatial features of the input HSI at different scales. It includes several existing spectral-spatial-based methods as special scenarios within a single unified framework. Based on SSR, we further propose the Subspace Learning-based Networks (SLN) as an example of SSR for HSI classification. In SLN, the joint spectral and spatial features are learned using templates simply learned by Marginal Fisher Analysis (MFA) and Principal Component Analysis (PCA). An important contribution to the success of SLN is the exploitation of label information of training samples and the local spatial structure of HSI. Extensive experimental results on four challenging HSI datasets taken from the Airborne Visible-Infrared Imaging Spectrometer (AVIRIS) and Reflective Optics System Imaging Spectrometer (ROSIS) airborne sensors show the implementational simplicity of SLN and verify the superiority of SSR for HSI classification.","keywords_author":["hierarchical framework","hyperspectral image classification","spectral-spatial feature","joint feature learning","subspace learning"],"keywords_other":["APPROXIMATION","ALGORITHM","SYSTEM","TIME-SERIES PREDICTION","NEURAL-NETWORKS","RECOGNITION","EXTREME LEARNING-MACHINE","DIMENSIONALITY REDUCTION","SPARSE REPRESENTATION","FEATURE-EXTRACTION"],"max_cite":2.0,"pub_year":2017.0,"sources":"['ieee', 'wos']","rawkeys":["neural-networks","algorithm","hierarchical framework","spectral-spatial feature","recognition","feature-extraction","sparse representation","hyperspectral image classification","system","approximation","joint feature learning","subspace learning","time-series prediction","extreme learning-machine","dimensionality reduction"],"tags":["hierarchical framework","spectral-spatial feature","recognition","time series prediction","neural networks","sparse representation","hyperspectral image classification","system","approximation","joint feature learning","subspace learning","feature extraction","algorithms","extreme learning machine","dimensionality reduction"]},{"p_id":55398,"title":"Deep temporal models and active inference","abstract":"How do we navigate a deeply structured world? Why are you reading this sentence first - and did you actually look at the fifth word? This review offers some answers by appealing to active inference based on deep temporal models. It builds on previous formulations of active inference to simulate behavioural and electrophysiological responses under hierarchical generative models of state transitions. Inverting these models corresponds to sequential inference, such that the state at any hierarchical level entails a sequence of transitions in the level below. The deep temporal aspect of these models means that evidence is accumulated over nested time scales, enabling inferences about narratives (i.e., temporal scenes). We illustrate this behaviour with Bayesian belief updating - and neuronal process theories - to simulate the epistemic foraging seen in reading. These simulations reproduce perisaccadic delay period activity and local field potentials seen empirically. Finally, we exploit the deep structure of these models to simulate responses to local (e.g., font type) and global (e.g., semantic) violations; reproducing mismatch negativity and P300 responses respectively.","keywords_author":["Active inference","Bayesian","Hierarchical","Reading","Violation","Free energy","P300","MMN"],"keywords_other":["MISMATCH NEGATIVITY","COMPUTATIONAL PSYCHIATRY","EVIDENCE ACCUMULATION","RECOGNITION","SHORT-TERM-MEMORY","CORTEX","SCHIZOPHRENIA","NETWORK MODEL","ORGANIZATION","SERIAL ORDER"],"max_cite":13.0,"pub_year":2017.0,"sources":"['ieee', 'wos']","rawkeys":["active inference","p300","mmn","recognition","organization","reading","short-term-memory","violation","schizophrenia","serial order","network model","computational psychiatry","free energy","hierarchical","cortex","mismatch negativity","evidence accumulation","bayesian"],"tags":["active inference","p300","mmn","recognition","organization","reading","short-term-memory","violation","schizophrenia","network modeling","serial order","computational psychiatry","free energy","hierarchical","cortex","mismatch negativity","evidence accumulation","bayesian"]},{"p_id":55400,"title":"Difference representation learning using stacked restricted Boltzmann machines for change detection in SAR images","abstract":"In this paper, we establish a deep neural network using stacked Restricted Boltzmann Machines (RBMs) to analyze the difference images and detect changes between multitemporal synthetic aperture radar (SAR) images. Given the two multitemporal images, a difference image which shows difference degrees between corresponding pixels is generated. Then, RBMs are stacked to form a deep hierarchical neural network to learn to analyze the difference image and recognize the changed pixels and unchanged pixels. The learning process includes unsupervised layer-wise feature learning and supervised fine-tuning of network parameters. Unsupervised learning aims to learn the representation of the difference image. Supervised fine-tuning aims to learn to classify the changed and unchanged pixels. The network can learn from datasets that have few labeled data. The labeled data can be selected from the results obtained by other methods because there is no prior information in image change detection. The system learns to detect the changes instead of recognizing the changes by fixed equations as in traditional change detection algorithms. We test the network with real synthetic aperture radar datasets and the labeled samples are extracted from the results obtained, respectively, by several methods, including a thresholding method, a level set method and two clustering methods. The results achieved by the trained network outperform that of other methods.","keywords_author":["Image change detection","Deep neural network","Representation learning","Synthetic aperture radar"],"keywords_other":["DEEP","NETWORKS","NETS","UNSUPERVISED CHANGE DETECTION","CLASSIFICATION","RECOGNITION","MODELS"],"max_cite":6.0,"pub_year":2016.0,"sources":"['ieee', 'wos']","rawkeys":["recognition","deep neural network","synthetic aperture radar","deep","unsupervised change detection","representation learning","networks","classification","models","nets","image change detection"],"tags":["recognition","model","synthetic aperture radar","deep","unsupervised change detection","representation learning","networks","classification","convolutional neural network","nets","image change detection"]},{"p_id":55402,"title":"Learning with hidden variables","abstract":"Learning and inferring features that generate sensory input is a task continuously performed by cortex. In recent years, novel algorithms and learning rules have been proposed that allow neural network models to learn such features from natural images, written text, audio signals, etc. These networks usually involve deep architectures with many layers of hidden neurons. Here we review recent advancements in this area emphasizing, amongst other things, the processing of dynamical inputs by networks with hidden nodes and the role of single neuron models. These points and the questions they arise can provide conceptual advancements in understanding of learning in the cortex and the relationship between machine learning approaches to learning with hidden nodes and those in cortical circuits.","keywords_author":null,"keywords_other":["HELMHOLTZ MACHINE","BOLTZMANN MACHINES","FEATURES","GRADED-RESPONSE NEURONS","ALGORITHM","NEURAL-NETWORKS","RECOGNITION","MODELS","CORTEX","MEMORY"],"max_cite":7.0,"pub_year":2015.0,"sources":"['ieee', 'wos']","rawkeys":["neural-networks","algorithm","recognition","helmholtz machine","features","memory","boltzmann machines","models","cortex","graded-response neurons"],"tags":["recognition","model","helmholtz machine","features","memory","neural networks","boltzmann machines","algorithms","cortex","graded-response neurons"]},{"p_id":55407,"title":"Robust Deep Network with Maximum Correntropy Criterion for Seizure Detection","abstract":"Effective seizure detection from long-term EEG is highly important for seizure diagnosis. Existing methods usually design the feature and classifier individually, while little work has been done for the simultaneous optimization of the two parts. This work proposes a deep network to jointly learn a feature and a classifier so that they could help each other to make the whole system optimal. To deal with the challenge of the impulsive noises and outliers caused by EMG artifacts in EEG signals, we formulate a robust stacked autoencoder (R-SAE) as a part of the network to learn an effective feature. In R-SAE, the maximum correntropy criterion (MCC) is proposed to reduce the effect of noise\/outliers. Unlike the mean square error (MSE), the output of the new kernel MCC increases more slowly than that of MSE when the input goes away from the center. Thus, the effect of those noises\/outliers positioned far away from the center can be suppressed. The proposed method is evaluated on six patients of 33.6 hours of scalp EEG data. Our method achieves a sensitivity of 100% and a specificity of 99%, which is promising for clinical applications.","keywords_author":null,"keywords_other":["SCALP EEG","ALGORITHM","NEURAL-NETWORKS","RECOGNITION","EPILEPTIC SEIZURES","ONSET"],"max_cite":0.0,"pub_year":2014.0,"sources":"['ieee', 'wos']","rawkeys":["neural-networks","algorithm","recognition","epileptic seizures","scalp eeg","onset"],"tags":["recognition","neural networks","epileptic seizures","scalp eeg","onset","algorithms"]},{"p_id":55408,"title":"Robust Traffic-Sign Detection and Classification Using Mobile LiDAR Data With Digital Images","abstract":"This study aims at building a robust method for detecting and classifying traffic signs from mobile LiDAR point clouds and digital images. First, this method detects traffic signs from mobile LiDAR point clouds with regard to a prior knowledge of road width, pole height, reflectance, geometrical structure, and traffic-sign size. Then, traffic-sign images are segmented by projecting the detected traffic-sign points onto the digital images. Afterward, the segmented traffic-sign images are normalized for automatic classification with a given image size. Finally, a traffic-sign classifier is proposed based on a supervised Gaussian-Bernoulli deep Boltzmann machine model. We evaluated the proposed method using datasets acquired by a RIEGL VMX-450 system. The traffic-sign detection accuracy of 86.8% was achieved; through parameter sensitivity analysis, the overall performance of traffic-sign classification achieved a recognition rate of 93.3%. The computational performance showed that our method provides a promising solution to traffic-sign detection and classification using mobile LiDAR point clouds and digital images.","keywords_author":["Digital images","deep learning","geometrical features","intensity","mobile LiDAR point clouds","traffic signs"],"keywords_other":["BOLTZMANN MACHINES","HIERARCHICAL-DEEP MODELS","LASER-SCANNING DATA","RECOGNITION","INVENTORY","POINT-CLOUDS","OBJECT DETECTION"],"max_cite":0.0,"pub_year":2018.0,"sources":"['ieee', 'wos']","rawkeys":["hierarchical-deep models","point-clouds","recognition","deep learning","boltzmann machines","intensity","laser-scanning data","mobile lidar point clouds","geometrical features","inventory","object detection","traffic signs","digital images"],"tags":["point cloud","recognition","boltzmann machines","machine learning","geometric feature","intensity","laser-scanning data","mobile lidar point clouds","inventory","object detection","hierarchical-deep model","traffic signs","digital image"]},{"p_id":79988,"title":"Gabor Convolutional Networks","abstract":"In steerable filters, a filter of arbitrary orientation can be generated by a linear combination of a set of \"basis filters.\" Steerable properties dominate the design of the traditional filters, e.g., Gabor filters and endow features the capability of handling spatial transformations. However, such properties have not yet been well explored in the deep convolutional neural networks (DCNNs). In this paper, we develop a new deep model, namely, Gabor convolutional networks (GCNs or Gabor CNNs), with Gabor filters incorporated into DCNNs such that the robustness of learned features against the orientation and scale changes can be reinforced. By manipulating the basic element of DCNNs, i.e., the convolution operator, based on Gabor filters, GCNs can be easily implemented and are readily compatible with any popular deep learning architecture. We carry out extensive experiments to demonstrate the promising performance of our GCNs framework, and the results show its superiority in recognizing objects, especially when the scale and rotation changes take place frequently. Moreover, the proposed GCNs have much fewer network parameters to be learned and can effectively reduce the training complexity of the network, leading to a more compact deep learning model while still maintaining a high feature representation capacity.","keywords_author":["Gabor CNNs","Gabor filters","convolutional neural networks","orientation","kernel modulation"],"keywords_other":["RECOGNITION","SALIENCY DETECTION","FILTER"],"max_cite":0.0,"pub_year":2018.0,"sources":"['wos', 'ieee']","rawkeys":["recognition","kernel modulation","convolutional neural networks","gabor cnns","gabor filters","saliency detection","filter","orientation"],"tags":["recognition","gabor filter","kernel modulation","gabor cnns","saliency detection","convolutional neural network","filter","orientation"]},{"p_id":112768,"title":"A machine learning approach to galaxy-LSS classification - I. Imprints on halo merger trees","abstract":"The cosmic web plays a major role in the formation and evolution of galaxies and defines, to a large extent, their properties. However, the relation between galaxies and environment is still not well understood. Here, we present a machine learning approach to study imprints of environmental effects on the mass assembly of haloes. We present a galaxy-LSS machine learning classifier based on galaxy properties sensitive to the environment. We then use the classifier to assess the relevance of each property. Correlations between galaxy properties and their cosmic environment can be used to predict galaxy membership to void\/wall or filament\/cluster with an accuracy of 93 per cent. Our study unveils environmental information encoded in properties of haloes not normally considered directly dependent on the cosmic environment such as merger history and complexity. Understanding the physical mechanism by which the cosmic web is imprinted in a halo can lead to significant improvements in galaxy formation models. This is accomplished by extracting features from galaxy properties and merger trees, computing feature scores for each feature and then applying support vector machine (SVM) to different feature sets. To this end, we have discovered that the shape and depth of the merger tree, formation time, and density of the galaxy are strongly associated with the cosmic environment. We describe a significant improvement in the original classification algorithm by performing LU decomposition of the distance matrix computed by the feature vectors and then using the output of the decomposition as input vectors for SVM.","keywords_author":["methods: data analysis","galaxies: evolution","large-scale structure of Universe"],"keywords_other":["FILAMENTARY STRUCTURE","COSMOLOGICAL SIMULATIONS","MORPHOLOGY","PHOTOMETRIC REDSHIFTS","COSMIC WEB","NEURAL-NETWORKS","RICH CLUSTERS","DEPENDENCE","ALGEBRAIC CONNECTIVITY","EVOLUTION"],"max_cite":0.0,"pub_year":2018.0,"sources":"['wos']","rawkeys":["galaxies: evolution","neural-networks","cosmological simulations","algebraic connectivity","large-scale structure of universe","methods: data analysis","morphology","photometric redshifts","cosmic web","rich clusters","evolution","dependence","filamentary structure"],"tags":["galaxies: evolution","cosmological simulations","recognition","filamentary structures","algebraic connectivity","large-scale structure of universe","neural networks","morphology","photometric redshifts","cosmic web","rich clusters","biological","methods: data analysis"]},{"p_id":14471,"title":"Anticipating Human Activities Using Object Affordances for Reactive Robotic Response","abstract":"\u00a9 1979-2012 IEEE.An important aspect of human perception is anticipation, which we use extensively in our day-to-day activities when interacting with other humans as well as with our surroundings. Anticipating which activities will a human do next (and how) can enable an assistive robot to plan ahead for reactive responses. Furthermore, anticipation can even improve the detection accuracy of past activities. The challenge, however, is two-fold: We need to capture the rich context for modeling the activities and object affordances, and we need to anticipate the distribution over a large space of future human activities. In this work, we represent each possible future using an anticipatory temporal conditional random field (ATCRF) that models the rich spatial-temporal relations through object affordances. We then consider each ATCRF as a particle and represent the distribution over the potential futures using a set of particles. In extensive evaluation on CAD-120 human activity RGB-D dataset, we first show that anticipation improves the state-of-the-art detection results. We then show that for new subjects (not seen in the training set), we obtain an activity anticipation accuracy (defined as whether one of top three predictions actually happened) of 84.1, 74.4 and 62.2 percent for an anticipation time of 1, 3 and 10 seconds respectively. Finally, we also show a robot using our algorithm for performing a few reactive responses.","keywords_author":["3D Activity Understanding","Human Activity Anticipation","Machine Learning","RGBD Data","Robotics Perception"],"keywords_other":["Human Activities","Humans","Machine Learning","Movement","Perception","Anticipation, Psychological","Video Recording","Human activities","Algorithms","Reactive robotics","RGBD Data","Robotics","Models, Statistical","Detection accuracy","Spatial temporals","3D Activity Understanding","Possible futures","Conditional random field","Imaging, Three-Dimensional"],"max_cite":58.0,"pub_year":2016.0,"sources":"['scp', 'wos']","rawkeys":["statistical","anticipation","movement","psychological","robotics perception","human activity anticipation","spatial temporals","conditional random field","machine learning","rgbd data","models","algorithms","possible futures","3d activity understanding","human activities","humans","reactive robotics","three-dimensional","video recording","detection accuracy","perception","robotics","imaging"],"tags":["anticipation","movement","robotics perception","human activity anticipation","spatial temporals","conditional random field","rgb-d data","machine learning","algorithms","possible futures","3d activity understanding","recognition","images","human activities","humans","reactive robotics","perceptions","three-dimensional","statistics","video recording","model","detection accuracy","robotics"]},{"p_id":30875,"title":"Shape Retrieval of Non-rigid 3D Human Models","abstract":"\u00a9 2016, The Author(s). 3D models of humans are commonly used within computer graphics and vision, and so the ability to distinguish between body shapes is an important shape retrieval problem. We extend our recent paper which provided a benchmark for testing non-rigid 3D shape retrieval algorithms on 3D human models. This benchmark provided a far stricter challenge than previous shape benchmarks. We have added 145 new models for use as a separate training set, in order to standardise the training data used and provide a fairer comparison. We have also included experiments with the FAUST dataset of human scans. All participants of the previous benchmark study have taken part in the new tests reported here, many providing updated results using the new data. In addition, further participants have also taken part, and we provide extra analysis of the retrieval results. A total of 25 different shape retrieval methods are compared.","keywords_author":["3D humans","3D shape retrieval","Benchmark","Non-rigid 3D shape retrieval","Benchmark","3D shape retrieval","Non-rigid 3D shape retrieval","3D humans"],"keywords_other":["BAG-OF-FEATURES","Training data","Benchmark study","SURFACES","Shape retrieval","VORONOI DIAGRAMS","DISTANCE","3D shape retrieval","3D human models","RECOGNITION","3D humans","Training sets","DESCRIPTOR","POSE","SIGNATURES","Different shapes"],"max_cite":4.0,"pub_year":2016.0,"sources":"['ieee', 'scp', 'wos']","rawkeys":["bag-of-features","surfaces","benchmark","recognition","voronoi diagrams","shape retrieval","training sets","signatures","3d shape retrieval","distance","3d humans","benchmark study","non-rigid 3d shape retrieval","descriptor","different shapes","3d human models","training data","pose"],"tags":["bag-of-features","surfaces","benchmark","recognition","voronoi diagrams","shape retrieval","different shapes","training sets","3d shape retrieval","distance","signature","3d humans","benchmark study","non-rigid 3d shape retrieval","descriptors","3d human models","training data","pose"]},{"p_id":55478,"title":"Deep Hierarchical Representation from Classifying Logo-405","abstract":"We introduce a logo classification mechanism which combines a series of deep representations obtained by fine-tuning convolutional neural network (CNN) architectures and traditional pattern recognition algorithms. In order to evaluate the proposed mechanism, we build a middle-scale logo dataset (named Logo-405) and treat it as a benchmark for logo related research. Our experiments are carried out on both the Logo-405 dataset and the publicly available FlickrLogos-32 dataset. The experimental results demonstrate that the proposed mechanism outperforms two popular ways used for logo classification, including the strategies that integrate hand-crafted features and traditional pattern recognition algorithms and the models which employ deep CNNs.","keywords_author":null,"keywords_other":["RECOGNITION","CLASSIFICATION","FEATURES"],"max_cite":0.0,"pub_year":2017.0,"sources":"['ieee', 'wos']","rawkeys":["recognition","classification","features"],"tags":["recognition","classification","features"]},{"p_id":55479,"title":"The State of the Art Ten Years After a State of the Art: Future Research in Music Information Retrieval","abstract":"A decade has passed since the first review of research on a 'flagship application' of music information retrieval (MIR): the problem of music genre recognition (MGR). During this time, about 500 works addressing MGR have been published, and at least 10 campaigns have been run to evaluate MGR systems. This makes MGR one of the most researched areas of MIR. So, where does MGR now lie? We show that in spite of this massive amount of work, MGR does not lie far from where it began, and the paramount reason for this is that most evaluation in MGR lacks validity. We perform a case study of all published research using the most-used benchmark dataset in MGR during the past decade: GTZAN. We show that none of the evaluations in these many works is valid to produce conclusions with respect to recognizing genre, i.e. that a system is using criteria relevant for recognizing genre. In fact, the problems of validity in evaluation also affect research in music emotion recognition and autotagging. We conclude by discussing the implications of our work for MGR and MIR in the next ten years.","keywords_author":["information retrieval","machine learning","databases","music analysis"],"keywords_other":["DISCRIMINATIONS","FEATURES","AUDIO","EXPERIMENTAL COMPUTER-SCIENCE","DATABASES","CLASSIFICATION","SYSTEMS","RECOGNITION","INTER-GENRE SIMILARITY","SOCIAL TAGS"],"max_cite":14.0,"pub_year":2014.0,"sources":"['ieee', 'wos']","rawkeys":["recognition","social tags","databases","features","inter-genre similarity","machine learning","information retrieval","audio","classification","experimental computer-science","discriminations","systems","music analysis"],"tags":["recognition","social tagging","databases","features","machine learning","system","information retrieval","audio","classification","experimental computer-science","inter-genre similarity","discrimination","music analysis"]},{"p_id":30918,"title":"Neuro-Inspired Computing with Emerging Nonvolatile Memorys","abstract":"\u00a9 2018 IEEE. This comprehensive review summarizes state of the art, challenges, and prospects of the neuro-inspired computing with emerging nonvolatile memory devices. First, we discuss the demand for developing neuro-inspired architecture beyond today's von-Neumann architecture. Second, we summarize the various approaches to designing the neuromorphic hardware (digital versus analog, spiking versus nonspiking, online training versus offline training) and discuss why emerging nonvolatile memory is attractive for implementing the synapses in the neural network. Then, we discuss the desired device characteristics of the synaptic devices (e.g., multilevel states, weight update nonlinearity\/asymmetry, variation\/noise), and survey a few representative material systems and device prototypes reported in the literature that show the analog conductance tuning. These candidates include phase change memory, resistive memory, ferroelectric memory, floating-gate transistors, etc. Next, we introduce the crossbar array architecture to accelerate the weighted sum and weight update operations that are commonly used in the neuro-inspired machine learning algorithms, and review the recent progresses of array-level experimental demonstrations for pattern recognition tasks. In addition, we discuss the peripheral neuron circuit design issues and present a device-circuit-algorithm codesign methodology to evaluate the impact of nonideal device effects on the system-level performance (e.g., learning accuracy). Finally, we give an outlook on the customization of the learning algorithms for efficient hardware implementation.","keywords_author":["Hardware accelerator","machine learning","neural network","neuromorphic computing","nonvolatile memory","resistive memory","synaptic device","Hardware accelerator","machine learning","neuromorphic computing","neural network","nonvolatile memory","resistive memory","synaptic device"],"keywords_other":["SYNAPTIC PLASTICITY","MEMRISTOR","Random access memory","DEVICE","Non-volatile memory","SYSTEMS","Neuromorphic computing","RECOGNITION","Neuromorphics","Resistive memory","ELECTRONIC SYNAPSES","PHASE-CHANGE MEMORY","METAL-OXIDE RRAM","Synapses","NETWORK","Hardware accelerators","RANDOM-ACCESS MEMORY","synaptic device"],"max_cite":3.0,"pub_year":2018.0,"sources":"['ieee', 'scp', 'wos']","rawkeys":["neural network","random access memory","synapses","neuromorphics","memristor","hardware accelerator","resistive memory","machine learning","non-volatile memory","neuromorphic computing","metal-oxide rram","synaptic device","network","recognition","phase-change memory","hardware accelerators","device","random-access memory","synaptic plasticity","electronic synapses","systems","nonvolatile memory"],"tags":["random access memory","synapses","resistive memory","machine learning","system","non-volatile memory","neuromorphic computing","metal-oxide rram","synaptic device","recognition","devices","neural networks","networks","phase-change memory","resistive switching memory","hardware accelerators","synaptic plasticity","electronic synapses","neuromorphic"]},{"p_id":71885,"title":"The visual system supports online translation invariance for object identification","abstract":"The ability to recognize the same image projected to different retinal locations is critical for visual object recognition in natural contexts. According to many theories, the translation invariance for objects extends only to trained retinal locations, so that a familiar object projected to a nontrained location should not be identified. In another approach, invariance is achieved \"online,\" such that learning to identify an object in one location immediately affords generalization to other locations. We trained participants to name novel objects at one retinal location using eyetracking technology and then tested their ability to name the same images presented at novel retinal locations. Across three experiments, we found robust generalization. These findings provide a strong constraint for theories of vision.","keywords_author":["Translation invariance","Translation tolerance","Object identification","Vision","Human visual perception and categorization","Object recognition","Perceptual categorization"],"keywords_other":["DISCRIMINATION","RECOGNITION","RETINAL POSITION","HIERARCHY"],"max_cite":2.0,"pub_year":2016.0,"sources":"['wos']","rawkeys":["recognition","translation invariance","human visual perception and categorization","object recognition","perceptual categorization","retinal position","vision","translation tolerance","discrimination","hierarchy","object identification"],"tags":["recognition","translation invariance","human visual perception and categorization","object recognition","perceptual categorization","retinal position","vision","translation tolerance","discrimination","hierarchy","object identification"]},{"p_id":71887,"title":"Bridging the Gap between Brain Activity and Cognition: Beyond the Different Tales of fMRI Data Analysis","abstract":"The human brain is an extremely complex system of interacting physical and functional units, ranging from single neurons to complex networks. Cognition is a network phenomenon because it does not exist in isolated synapses, neurons, or even brain areas. In spite of that, a great amount of functional magnetic resonance imaging (fMRI) studies have explored what areas are involved in a variety of cognitive processes, merely localizing where in the brain those processes occur. Instead, the very notion of network phenomena requires understanding spatiotemporal dynamics, which, in turn, depends on the way fMRI data are analyzed. What are the mechanisms for simulating different cognitive functions and their spatiotemporal activity patterns? In order to bridge the gap between brain network activity and the emerging cognitive functions, we need more plausible computational models, which should reflect putative neural mechanisms and the properties of brain network dynamics.","keywords_author":["fMRI","multi-voxel pattern analysis","functional connectivity","effective connectivity","complex networks","graph-theoretical analysis","deep networks"],"keywords_other":["NETWORKS","SYSTEMS","HUMAN VISUAL-CORTEX","REPRESENTATIONS","NEUROIMAGING DATA","DYNAMIC CAUSAL-MODELS","BOLD HEMODYNAMIC-RESPONSES","RECOGNITION","CONNECTIVITY"],"max_cite":1.0,"pub_year":2017.0,"sources":"['wos']","rawkeys":["functional connectivity","graph-theoretical analysis","neuroimaging data","dynamic causal-models","recognition","systems","multi-voxel pattern analysis","effective connectivity","bold hemodynamic-responses","human visual-cortex","connectivity","networks","representations","fmri","complex networks","deep networks"],"tags":["functional connectivity","graph-theoretical analysis","neuroimaging data","recognition","multi-voxel pattern analysis","effective connectivity","bold hemodynamic-responses","human visual-cortex","connectivity","representation","system","networks","fmri","complex networks","dynamic causal modeling","deep networks"]},{"p_id":71894,"title":"A New Method for Automatic Sleep Stage Classification","abstract":"Traditionally, automatic sleep stage classification is quite a challenging task because of the difficulty in translating open-textured standards to mathematical models and the limitations of handcrafted features. In this paper, a new system for automatic sleep stage classification is presented. Compared with existing sleep stage methods, our method can capture the sleep information hidden inside electroencephalography (EEG) signals and automatically extract features from raw data. To translate open sleep stage standards into machine rules recognized by computers, a new model named fast discriminative complex-valued convolutional neural network (FDCCNN) is proposed to extract features from raw EEG data and classify sleep stages. The new model combines complex-valued backpropagation and the Fisher criterion. It can learn discriminative features and overcome the negative effect of imbalance dataset. More importantly, the orthogonal decision boundaries for the real and imaginary parts of a complex-valued convolutional neuron are proven. A speed-up algorithm is proposed to reduce computational workload and yield improvements of over an order of magnitude compared to the normal convolution algorithm. The classification performances of handcrafted features and different convolutional neural networks are compared with that of the FDCCNN. The total accuracy and kappa coefficient of the proposed method are 92% and 0.84, respectively. Experiment results demonstrated that the performance of our system is comparable to those of human experts.","keywords_author":["Convolutional neural network","electroen-cephalography","feature extraction","fisher criterion","sleep stage"],"keywords_other":["ARTIFICIAL NEURAL-NETWORK","FEATURES","COMBINATION","CLASSIFIERS","WAKE CLASSIFICATION","FEATURE-SELECTION","SYSTEM","RECOGNITION","SIGNALS","SINGLE-CHANNEL EEG"],"max_cite":0.0,"pub_year":2017.0,"sources":"['ieee', 'wos']","rawkeys":["signals","recognition","wake classification","features","combination","artificial neural-network","sleep stage","fisher criterion","classifiers","system","feature-selection","electroen-cephalography","convolutional neural network","feature extraction","single-channel eeg"],"tags":["signals","recognition","features","neural networks","combination","sleep stage","eeg","fisher criterion","system","feature selection","classifier","convolutional neural network","feature extraction","wake classification","single channel eeg"]},{"p_id":71895,"title":"Speech-Centric Information Processing: An Optimization-Oriented Approach","abstract":"Automatic speech recognition (ASR) is a central and common component of voice-driven information processing systems in human language technology, including spoken language translation (SLT), spoken language understanding (SLU), voice search, spoken document retrieval, and so on. Interfacing ASR with its downstream text-based processing tasks of translation, understanding, and information retrieval (IR) creates both challenges and opportunities in optimal design of the combined, speech-enabled systems. We present an optimization-oriented statistical framework for the overall system design where the interactions between the subsystems in tandem are fully incorporated and where design consistency is established between the optimization objectives and the end-to-end system performance metrics. Techniques for optimizing such objectives in both the decoding and learning phases of the speech-centric information processing (SCIP) system design are described, in which the uncertainty in speech recognition subsystem's outputs is fully considered and marginalized. This paper provides an overview of the past and current work in this area. Future challenges and new opportunities are also discussed and analyzed.","keywords_author":["Joint optimization","speech recognition","speech-centric information processing (SCIP)","spoken language translation (SLT)","spoken language understanding (SLU)","voice search"],"keywords_other":["STATISTICAL ESTIMATION","SEARCH","SIGNAL","CLASSIFICATION","ERROR","RECOGNITION","SPOKEN LANGUAGE TRANSLATION","TASK","MODELS","INEQUALITY"],"max_cite":10.0,"pub_year":2013.0,"sources":"['ieee', 'wos']","rawkeys":["inequality","error","recognition","search","spoken language understanding (slu)","spoken language translation","voice search","speech recognition","statistical estimation","classification","models","joint optimization","task","speech-centric information processing (scip)","spoken language translation (slt)","signal"],"tags":["signals","inequality","error","recognition","model","search","spoken language translation","voice search","speech recognition","statistical estimation","classification","joint optimization","task","speech-centric information processing (scip)","spoken language understanding"]},{"p_id":71896,"title":"Language Identification in Short Utterances Using Long Short-Term Memory (LSTM) Recurrent Neural Networks","abstract":"Long Short Term Memory (LSTM) Recurrent Neural Networks (RNNs) have recently outperformed other state-of-the-art approaches, such as i-vector and Deep Neural Networks (DNNs), in automatic Language Identification (LID), particularly when dealing with very short utterances (similar to 3s). In this contribution we present an open-source, end-to-end, LSTM RNN system running on limited computational resources (a single GPU) that outperforms a reference i-vector system on a subset of the NIST Language Recognition Evaluation (8 target languages, 3s task) by up to a 26%. This result is in line with previously published research using proprietary LSTM implementations and huge computational resources, which made these former results hardly reproducible. Further, we extend those previous experiments modeling unseen languages (out of set, OOS, modeling), which is crucial in real applications. Results show that a LSTM RNN with OOS modeling is able to detect these languages and generalizes robustly to unseen OOS languages. Finally, we also analyze the effect of even more limited test data (from 2.25s to 0.1s) proving that with as little as 0.5s an accuracy of over 50% can be achieved.","keywords_author":null,"keywords_other":["RECOGNITION","SPEAKER VERIFICATION"],"max_cite":14.0,"pub_year":2016.0,"sources":"['ieee', 'wos']","rawkeys":["recognition","speaker verification"],"tags":["recognition","speaker verification"]},{"p_id":71899,"title":"Frame-by-frame language identification in short utterances using deep neural networks","abstract":"This work addresses the use of deep neural networks (DNNs) in automatic language identification (LID) focused on short test utterances. Motivated by their recent success in acoustic modelling for speech recognition, we adapt DNNs to the problem of identifying the language in a given utterance from the short-term acoustic features. We show how DNNs are particularly suitable to perform LID in real-time applications, due to their capacity to emit a language identification posterior at each new frame of the test utterance. We then analyse different aspects of the system, such as the amount of required training data, the number of hidden layers, the relevance of contextual information and the effect of the test utterance duration. Finally, we propose several methods to combine frame-by-frame posteriors. Experiments are conducted on two different datasets: the public NIST Language Recognition Evaluation 2009 (3 s task) and a much larger corpus (of 5 million utterances) known as oogle 5M LID, obtained from different Google Services. Reported results show relative improvements of DNNs versus the i-vector system of 40% in LRE09 3 second task and 76% in Google 5M LID. (C) 2014 Elsevier Ltd. All rights reserved.","keywords_author":["DNNs","Real-time LID","i-vectors"],"keywords_other":["NIST LRE 2009","VARIABILITY","SPEAKER VERIFICATION","INFORMATION","RECOGNITION","SPEECH"],"max_cite":9.0,"pub_year":2015.0,"sources":"['ieee', 'wos']","rawkeys":["recognition","dnns","i-vectors","variability","speaker verification","information","speech","nist lre 2009","real-time lid"],"tags":["recognition","variability","speaker verification","information","convolutional neural network","speech","nist lre 2009","real-time lid","i-vector"]},{"p_id":71901,"title":"On the use of deep feedforward neural networks for automatic language identification","abstract":"In this work, we present a comprehensive study on the use of deep neural networks (DNNs) for automatic language identification (LID). Motivated by the recent success of using DNNs in acoustic modeling for speech recognition, we adapt DNNs to the problem of identifying the language in a given utterance from its short-term acoustic features. We propose two different DNN-based approaches. In the first one, the DNN acts as an end-to-end LID classifier, receiving as input the speech features and providing as output the estimated probabilities of the target languages. In the second approach, the DNN is used to extract bottleneck features that are then used as inputs for a state-of-the-art i-vector system. Experiments are conducted in two different scenarios: the complete NIST Language Recognition Evaluation dataset 2009 (LRE'09) and a subset of the Voice of America (VOA) data from LRE'09, in which all languages have the same amount of training data. Results for both datasets demonstrate that the DNN-based systems significantly outperform a state-of-art i-vector system when dealing with short-duration utterances. Furthermore, the combination of the DNN-based and the classical i-vector system leads to additional performance improvements (up to 45% of relative improvement in both EER and C-avg on 3s and 10s conditions, respectively). (C) 2016 The Authors. Published by Elsevier Ltd. This is an open access article under the CC BY license (http:\/\/creativecommons.org\/licenses\/by\/4.0\/).","keywords_author":["LID","DNN","Bottleneck","i-vectors"],"keywords_other":["RECOGNITION","SPEECH","SPEAKER VERIFICATION","INFORMATION"],"max_cite":8.0,"pub_year":2016.0,"sources":"['ieee', 'wos']","rawkeys":["recognition","i-vectors","speaker verification","dnn","information","speech","bottleneck","lid"],"tags":["recognition","language identification","speaker verification","information","convolutional neural network","bottleneck","speech","i-vector"]},{"p_id":71904,"title":"Head motion synthesis from speech using deep neural networks","abstract":"This paper presents a deep neural network (DNN) approach for head motion synthesis, which can automatically predict head movement of a speaker from his\/her speech. Specifically, we realize speech-to-head-motion mapping by learning a DNN from audio-visual broadcast news data. We first show that a generatively pre-trained neural network significantly outperforms a conventional randomly initialized network. We then demonstrate that filter bank (FBank) features outperform mel frequency cepstral coefficients (MFCC) and linear prediction coefficients (LPC) in head motion prediction. Finally, we discover that extra training data from other speakers used in the pre-training stage can improve the head motion prediction performance of a target speaker. Our promising results in speech-to-head-motion prediction can be used in talking avatar animation.","keywords_author":["Head motion synthesis","Deep neural network","Talking avatar","Computer animation"],"keywords_other":["PROSODIC FEATURES","NETS","ANIMATION","RECOGNITION","DRIVEN"],"max_cite":10.0,"pub_year":2015.0,"sources":"['ieee', 'wos']","rawkeys":["talking avatar","recognition","driven","deep neural network","nets","prosodic features","head motion synthesis","animation","computer animation"],"tags":["talking avatar","recognition","driven","animals","convolutional neural network","nets","prosodic features","head motion synthesis","computer animation"]},{"p_id":88292,"title":"Subspace learning by kernel dependence maximization for cross-modal retrieval","abstract":"Heterogeneity of multi-modal data is the key challenge for multimedia cross-modal retrieval. To solve this challenge, many approaches have been developed. As the mainstream, subspace learning based approaches focus on learning a latent shared subspace to measure similarities between cross-modal data, and have shown their remarkable performance in practical cross-modal retrieval tasks. However, most of the existing approaches are intrinsically identified with feature dimension reduction on different modalities in a shared subspace, unable to fundamentally resolve the heterogeneity issue well; therefore they often can not obtain satisfactory results as expected. As claimed in Hilbert space theory, different Hilbert spaces with the same dimension are isomorphic. Based on this premise, isomorphic mapping subspaces can be considered as a single space shared by multi-modal data. To this end, we in this paper propose a correlation-based cross-modal subspace learning model via kernel dependence maximization (KDM). Unlike most of the existing correlation-based subspace learning methods, the proposed KDM learns subspace representation for each modality by maximizing the kernel dependence (correlation) instead of directly maximizing the feature correlations between multi-modal data. Specifically, we first map multimodal data into different Hilbert spaces but with the same dimension individually, then we calculate kernel matrix in each Hilbert space and measure the correlations between multi-modalities based on kernels. Experimental results have shown the effectiveness and competitiveness of the proposed KDM against the compared classic subspace learning approaches. (C) 2018 Elsevier B.V. All rights reserved.","keywords_author":["Correlation\/dependence","Cross-modal retrieval","Subspace learning","Supervised learning"],"keywords_other":["RECOGNITION","CANONICAL CORRELATION-ANALYSIS","INFORMATION"],"max_cite":0.0,"pub_year":2018.0,"sources":"['wos']","rawkeys":["supervised learning","recognition","correlation\/dependence","canonical correlation-analysis","subspace learning","information","cross-modal retrieval"],"tags":["supervised learning","recognition","correlation\/dependence","subspace learning","information","cross-modal retrieval","canonical correlation analysis"]},{"p_id":39182,"title":"An efficient computational intelligence technique for affine- transformation-invariant image face detection, tracking, and recognition in a video stream","abstract":"While there are many current approaches to solving the difficulties that come with detecting, tracking, and recognizing a given face in a video sequence, the difficulties arising when there are differences in pose, facial expression, orientation, lighting, scaling, and location remain an open research problem. In this paper we present and perform the study and analysis of a computationally efficient approach for each of the three processes, namely a given template face detection, tracking, and recognition. The proposed algorithms are faster relatively to other existing iterative methods. In particular, we show that unlike such iterative methods, the proposed method does not estimate a given face rotation angle or scaling factor by looking into all possible face rotations or scaling factors. The proposed method looks into segmenting and aligning the distance between two eyes' pupils in a given face image with the image x-axis. Reference face images in a given database are normalized with respect to translation, rotation, and scaling. We show here how the proposed method to estimate a given face image template rotation and scaling factor leads to real-time template image rotation and scaling corrections. This allows the recognition algorithm to be less computationally complex than iterative methods. \u00a9 2014 IEEE.","keywords_author":["computational intelligence","detection","facial","machine learning","real-time","recognition","tracking","video"],"keywords_other":["Computational intelligence techniques","recognition","Facial Expressions","real-time","Recognition algorithm","facial","video","Computationally efficient"],"max_cite":1.0,"pub_year":2014.0,"sources":"['scp']","rawkeys":["recognition","recognition algorithm","computational intelligence techniques","real-time","facial expressions","machine learning","facial","tracking","video","detection","computational intelligence","computationally efficient"],"tags":["recognition","recognition algorithm","computational intelligence techniques","real time","facial expressions","machine learning","facial","tracking","video","detection","computational intelligence","computationally efficient"]},{"p_id":22817,"title":"Automatic Skin Lesion Segmentation Using Deep Fully Convolutional Networks with Jaccard Distance","abstract":"\u00a9 1982-2012 IEEE. Automatic skin lesion segmentation in dermoscopic images is a challenging task due to the low contrast between lesion and the surrounding skin, the irregular and fuzzy lesion borders, the existence of various artifacts, and various imaging acquisition conditions. In this paper, we present a fully automatic method for skin lesion segmentation by leveraging 19-layer deep convolutional neural networks that is trained end-to-end and does not rely on prior knowledge of the data. We propose a set of strategies to ensure effective and efficient learning with limited training data. Furthermore, we design a novel loss function based on Jaccard distance to eliminate the need of sample re-weighting, a typical procedure when using cross entropy as the loss function for image segmentation due to the strong imbalance between the number of foreground and background pixels. We evaluated the effectiveness, efficiency, as well as the generalization capability of the proposed framework on two publicly available databases. One is from ISBI 2016 skin lesion analysis towards melanoma detection challenge, and the other is the PH2 database. Experimental results showed that the proposed method outperformed other state-of-the-art algorithms on these two databases. Our method is general enough and only needs minimum pre- and post-processing, which allows its adoption in a variety of medical image segmentation tasks.","keywords_author":["Deep learning","dermoscopy","fully convolutional neural networks","image segmentation","jaccard distance","melanoma","Deep learning","fully convolutional neural networks","image segmentation","jaccard distance","melanoma","dermoscopy"],"keywords_other":["Generalization capability","Humans","Skin","MRI","CANCER","Limited training data","DERMOSCOPY IMAGES","DIAGNOSIS","Convolutional networks","Artifacts","MELANOMA","Melanoma","Algorithms","State-of-the-art algorithms","Jaccard distance","RECOGNITION","Neural Networks (Computer)","BORDER DETECTION","CLASSIFICATION","melanoma","NEURAL-NETWORKS","Dermoscopy","Convolutional neural network"],"max_cite":11.0,"pub_year":2017.0,"sources":"['ieee', 'scp', 'wos']","rawkeys":["dermoscopy images","mri","classification","border detection","convolutional neural network","neural-networks","limited training data","convolutional networks","artifacts","algorithms","diagnosis","neural networks (computer)","recognition","jaccard distance","deep learning","dermoscopy","humans","image segmentation","skin","generalization capability","state-of-the-art algorithms","cancer","fully convolutional neural networks","melanoma"],"tags":["dermoscopy images","classification","border detection","convolutional neural network","limited training data","machine learning","artifacts","algorithms","diagnosis","recognition","jaccard distance","neural networks","dermoscopy","humans","image segmentation","skin","generalization capability","state-of-the-art algorithms","cancer","fully convolutional network","melanoma","magnetic resonance imaging"]},{"p_id":31013,"title":"Deep Representation-Based Feature Extraction and Recovering for Finger-Vein Verification","abstract":"\u00a9 2017 IEEE.Finger-vein biometrics has been extensively investigated for personal verification. Despite recent advances in finger-vein verification, current solutions completely depend on domain knowledge and still lack the robustness to extract finger-vein features from raw images. This paper proposes a deep learning model to extract and recover vein features using limited a priori knowledge. First, based on a combination of the known state-of-the-art handcrafted finger-vein image segmentation techniques, we automatically identify two regions: A clear region with high separability between finger-vein patterns and background, and an ambiguous region with low separability between them. The first is associated with pixels on which all the above-mentioned segmentation techniques assign the same segmentation label (either foreground or background), while the second corresponds to all the remaining pixels. This scheme is used to automatically discard the ambiguous region and to label the pixels of the clear region as foreground or background. A training data set is constructed based on the patches centered on the labeled pixels. Second, a convolutional neural network (CNN) is trained on the resulting data set to predict the probability of each pixel of being foreground (i.e., vein pixel), given a patch centered on it. The CNN learns what a finger-vein pattern is by learning the difference between vein patterns and background ones. The pixels in any region of a test image can then be classified effectively. Third, we propose another new and original contribution by developing and investigating a fully convolutional network to recover missing finger-vein patterns in the segmented image. The experimental results on two public finger-vein databases show a significant improvement in terms of finger-vein verification accuracy.","keywords_author":["Convolutional autoencoder","Convolutional neural network","Deep learning","Finger-Vein verification","Hand biometrics","Representation learning","Hand biometrics","finger-vein verification","deep learning","convolutional neural network","convolutional autoencoder","representation learning"],"keywords_other":["PERSONAL IDENTIFICATION","Convolutional networks","Representation learning","Personal verification","CURVATURE","Segmentation techniques","SYSTEM","DISTANCE","RECOGNITION","Convolutional neural network","Auto encoders","IMAGE SEGMENTATION","FUSION","REPEATED LINE TRACKING","Training data sets","Finger vein","CONVOLUTIONAL NEURAL-NETWORKS"],"max_cite":3.0,"pub_year":2017.0,"sources":"['ieee', 'scp', 'wos']","rawkeys":["finger-vein verification","hand biometrics","convolutional neural-networks","convolutional neural network","personal verification","training data sets","convolutional networks","system","repeated line tracking","convolutional autoencoder","fusion","curvature","recognition","segmentation techniques","deep learning","distance","representation learning","finger vein","image segmentation","auto encoders","personal identification"],"tags":["person identification","person verification","hand biometrics","convolutional neural network","training data sets","machine learning","system","finger vein verification","repeated line tracking","convolutional autoencoder","fusion","curvature","recognition","segmentation techniques","distance","representation learning","finger vein","image segmentation","auto encoders"]},{"p_id":22822,"title":"EEG-Based Strategies to Detect Motor Imagery for Control and Rehabilitation","abstract":"\u00a9 2017 IEEE.Advances in brain-computer interface (BCI) technology have facilitated the detection of Motor Imagery (MI) from electroencephalography (EEG). First, we present three strategies of using BCI to detect MI from EEG: Operant conditioning that employed a fixed model, machine learning that employed a subject-specific model computed from calibration, and adaptive strategy that continuously compute the subject-specific model. Second, we review prevailing works that employed the operant conditioning and machine learning strategies. Third, we present our past work on six stroke patients who underwent a BCI rehabilitation clinical trial with averaged accuracies of 79.8% during calibration and 69.5% across 18 online feedback sessions. Finally, we perform an offline study in this paper on our work employing the adaptive strategy. The results yielded significant improvements of 12% (p < 0.001) and 9% (p < 0.001) using all the data and using limited preceding data respectively in the feedback accuracies. The results showed an increase in the amount of training data yielded improvements. Nevertheless, results of using limited preceding data showed a larger part of the improvement was due to the adaptive strategy and changing subject-specific models did not deteriorate the accuracies. Hence the adaptive strategy is effective in addressing the non-stationarity between calibration and feedback sessions.","keywords_author":["Adaptive","brain-computer interface (BCI)","electroenceptography (EEG)","machine learning","motor imagery (MI)","operant conditioning","stroke rehabilitation"],"keywords_other":["Sensitivity and Specificity","Humans","Reproducibility of Results","Motor imagery","Machine Learning","Movement","Evoked Potentials, Motor","Adaptive","Pattern Recognition, Automated","electroenceptography (EEG)","Electroencephalography","Brain","Neurological Rehabilitation","Algorithms","Brain-Computer Interfaces","Stroke rehabilitation","Imagination","Operant conditioning","Motor Cortex","Biofeedback, Psychology"],"max_cite":11.0,"pub_year":2017.0,"sources":"['scp']","rawkeys":["automated","movement","operant conditioning","brain","neurological rehabilitation","electroenceptography (eeg)","brain-computer interfaces","brain-computer interface (bci)","machine learning","algorithms","psychology","motor","sensitivity and specificity","evoked potentials","motor imagery","reproducibility of results","humans","stroke rehabilitation","adaptive","imagination","electroencephalography","motor cortex","motor imagery (mi)","pattern recognition","biofeedback"],"tags":["automated","movement","brain","adaptation","neurological rehabilitation","electroenceptography (eeg)","brain-computer interfaces","machine learning","algorithms","motor","sensitivity and specificity","evoked potentials","recognition","eeg","reproducibility of results","humans","stroke rehabilitation","operating condition","imagination","mutual information","motor cortex","pattern recognition","biofeedback"]},{"p_id":31016,"title":"Predicting user behavior in electronic markets based on personality-mining in large online social networks: A personality-based product recommender framework","abstract":"\u00a9 2016, The Author(s).Determining a user\u2019s preferences is an important condition for effectively operating automatic recommendation systems. Since personality theory claims that a user\u2019s personality substantially influences preference, I propose a personality-based product recommender (PBPR) framework to analyze social media data in order to predict a user\u2019s personality and to subsequently derive its personality-based product preferences. The PBRS framework will be evaluated as an IT-artefact with a unique online social network XING dataset and a unique coffeemaker preference dataset. My evaluation results show (a) the possibility of predicting a user\u2019s personality from social media data, as I reached a predictive gain between 23.2 and 41.8 percent and (b) the possibility of recommending products based on a user\u2019s personality, as I reached a predictive gain of 45.1 percent.","keywords_author":["Agreeableness","Big data analytics","Conscientiousness","Extraversion","Five factor model","Machine learning","Neuroticism","Online social networks","Openness to experience","Personality mining","Predictive analytics","Product recommender system"],"keywords_other":null,"max_cite":3.0,"pub_year":2017.0,"sources":"['scp']","rawkeys":["online social networks","conscientiousness","neuroticism","product recommender system","agreeableness","predictive analytics","machine learning","five factor model","personality mining","extraversion","openness to experience","big data analytics"],"tags":["online social networks","recognition","conscientiousness","neuroticism","agreeableness","predictive analytics","five-factor model","machine learning","product recommendation system","personality minings","openness to experience","big data analytics"]},{"p_id":71990,"title":"Biometric security system: a rigorous review of unimodal and multimodal biometrics techniques","abstract":"Biometric-based system is used for authentication of an individual and to counter the possible threats used for security purpose. A wide variety of systems require reliable personal authentication schemes to either confirm or determine the identity of individuals requesting their services. Typical scenarios are access control and authentication transaction. Examples of such systems are automatic teller machines (ATMs), criminal verification, national unique identifications, border crossing, airports, cellular phones, etc. In the absence of robust authentication schemes, these systems are vulnerable to the wiles of an impostor. The purpose of such schemes is to ensure that the rendered services are accessed by the legitimate user, and not anyone else. Many researchers developed biometric-based system despite that each system has its own limitations. The main aim of this paper is to give a qualitative and computational analysis of existing biometric-based system and describes various unimodal and multimodal systems.","keywords_author":["multimodal biometrics","unimodal biometric","feature fusion","classification","identification","recognition","texture","error rate","neural network","biometric system"],"keywords_other":["VERIFICATION","DECISION-LEVEL FUSION","AUTHENTICATION","IRIS","REPRESENTATION","RECOGNITION","IDENTIFICATION SYSTEM","NETWORK","IMAGES"],"max_cite":0.0,"pub_year":2018.0,"sources":"['wos']","rawkeys":["multimodal biometrics","neural network","network","identification","error rate","authentication","images","iris","recognition","unimodal biometric","representation","verification","texture","classification","identification system","decision-level fusion","biometric system","feature fusion"],"tags":["multimodal biometrics","identification","error rate","authentication","identification systems","images","iris","neural networks","recognition","representation","unimodal biometric","networks","texture","classification","verification","biometric systems","decision-level fusion","feature fusion"]},{"p_id":22841,"title":"Towards latent context-aware recommendation systems","abstract":"\u00a9 2016 Elsevier B.V. All rights reserved.The emergence and penetration of smart mobile devices has given rise to the development of context-aware systems that utilize sensors to collect available data about users in order to improve various user services. Recently, the use of context-aware recommender systems (CARS) aimed at recommending items to users has expanded, particularly those that consider user context. Adding context to recommendation systems is challenging, because the addition of various environmental contexts to the recommendation process results in the expansion of its dimensionality, and thus increases sparsity. Therefore, existing CARS tend to incorporate a small set of pre-defined explicit contexts which do not necessary represent user context or reflect the optimal set of features for the recommendation process. We suggest a novel approach centered on representing environmental features as low dimensional unsupervised latent contexts. We extract data from a rich set of mobile sensors in order to infer unexplored user contexts in an unsupervised manner. The latent contexts are hidden context patterns modeled as numeric vectors which are efficiently extracted from raw sensor data. The latent contexts are automatically learned for each user utilizing unsupervised deep learning techniques and PCA on the data collected from the user's mobile phone. Integrating the data extracted from high dimensional sensors into a new latent context-aware recommendation algorithm results in up to a 20% increase in recommendation accuracy.","keywords_author":["Context","Context-aware recommender systems","Deep learning","Matrix factorization","Recommendation","Recommender systems","Recommendation","Recommender systems","Context-aware recommender systems","Context","Matrix factorization","Deep learning"],"keywords_other":["Deep learning","Context-aware recommender systems","Recommendation","RECOGNITION","Context","Matrix factorizations"],"max_cite":11.0,"pub_year":2016.0,"sources":"['ieee', 'scp', 'wos']","rawkeys":["recommendation","recognition","deep learning","context-aware recommender systems","matrix factorization","recommender systems","matrix factorizations","context"],"tags":["recommendation","recognition","nonnegative matrix factorization","context-aware recommender systems","machine learning","recommender systems","context"]},{"p_id":31048,"title":"Automatic Recognition of fMRI-derived Functional Networks using 3D Convolutional Neural Networks","abstract":"IEEE Current fMRI data modeling techniques such as Independent Component Analysis (ICA) and Sparse Coding methods can effectively reconstruct dozens or hundreds of concurrent interacting functional brain networks simultaneously from the whole brain fMRI signals. However, such reconstructed networks have no correspondences across different subjects. Thus, automatic, effective and accurate classification and recognition of these large numbers of fMRI-derived functional brain networks are very important for subsequent steps of functional brain analysis in cognitive and clinical neuroscience applications. However, this task is still a challenging and open problem due to the tremendous variability of various types of functional brain networks and the presence of various sources of noises. In recognition of the fact that convolutional neural networks (CNN) has superior capability of representing spatial patterns with huge variability and dealing with large noises, in this paper, we design, apply and evaluate a deep 3D CNN framework for automatic, effective and accurate classification and recognition of large number of functional brain networks reconstructed by sparse representation of whole-brain fMRI signals. Our extensive experimental results based on the Human Connectome Project (HCP) fMRI data showed that the proposed deep 3D CNN can effectively and robustly perform functional networks classification and recognition tasks, while maintaining a high tolerance for mistakenly labelled training instances. Our work provides a new deep learning approach for modeling functional connectomes based on fMRI data.","keywords_author":["convolutional neural networks","deep learning","Dictionaries","fMRI","functional brain networks","Image reconstruction","recognition","Robustness","Sociology","Statistics","Three-dimensional displays","Training"],"keywords_other":["recognition","Three-dimensional display","Sociology","Brain networks","fMRI","Convolutional neural network"],"max_cite":3.0,"pub_year":2017.0,"sources":"['scp', 'wos']","rawkeys":["functional brain networks","image reconstruction","robustness","statistics","recognition","convolutional neural networks","three-dimensional display","deep learning","training","sociology","three-dimensional displays","fmri","convolutional neural network","brain networks","dictionaries"],"tags":["functional brain networks","image reconstruction","robustness","statistics","recognition","training","machine learning","sociology","three-dimensional displays","fmri","convolutional neural network","brain networks","dictionaries"]},{"p_id":96590,"title":"Dynamic Random Arching in the Flow Field of Top-Coal Caving Mining","abstract":"The large mining height fully mechanized top-coal caving mining technique has developed rapidly and become the most extensively used mining method for ultra-thick coal seams. The arching of coal\/gangue and the drawing out of gangue are peculiar phenomena in the process of fully mechanized top-coal caving mining, which not only affects the recovery of top-coal, but also affects the quality of the coal. This paper studies the arching phenomenon in top-coal caving mining process of ultra-thick coal seam. A series of laboratory granular material simulation experiments were performing and a top-coal arching model in the framework of mechanics was established to explore the formation characteristic of arches and their effects to top-coal loss. Then the countermeasures against the arches and technology path of intelligent mining based on improving top-coal recovery were put forward and performed in practice. The results show that the recovery ratio of top-coal has increased nearly 6%, and increased the production efficiency at the same time. The research on arching mechanism and removing strategies of dynamic random arches effectively improves the efficiency of fully mechanized top-coal caving mining in ultra-thick coal seams, and provides the foundation for the realization of intelligent top-coal caving mining technology.","keywords_author":["top-coal caving mining","arching mechanism","dynamic random arch","granular material","intelligent mining"],"keywords_other":["PREDICTION","FACE","MANAGEMENT","DESIGN","HEIGHT","MODEL","GRANULAR-MATERIALS","RECOGNITION","SEAMS","INTERFACE"],"max_cite":0.0,"pub_year":2018.0,"sources":"['wos']","rawkeys":["design","height","recognition","granular material","model","interface","prediction","top-coal caving mining","intelligent mining","arching mechanism","seams","management","face","dynamic random arch","granular-materials"],"tags":["design","height","recognition","granular material","model","interface","prediction","top-coal caving mining","intelligent mining","arching mechanism","seams","management","face","dynamic random arch"]},{"p_id":31062,"title":"Deep Video Hashing","abstract":"\u00a9 2017 IEEE. In this work, we propose a deep video hashing (DVH) method for scalable video search. Unlike most existing video hashing methods that first extract features for each single frame and then use conventional image hashing techniques, our DVH learns binary codes for the entire video with a deep learning framework so that both the temporal and discriminative information can be well exploited. Specifically, we fuse the temporal information across different frames within each video to learn the feature representation under two criteria: the distance between a feature pair obtained at the top layer is small if they are from the same class, and large if they are from different classes; and the quantization loss between the real-valued features and the binary codes is minimized. We exploit different deep architectures to utilize spatial-temporal information in different manners and compare them with single-frame-based deep models and state-of-the-art image hashing methods. Experimental results demonstrate the effectiveness of our proposed method.","keywords_author":["Deep learning","scalable video search","video hashing","Deep learning","scalable video search","video hashing"],"keywords_other":["Video hashing","VISUAL-SEARCH","IMAGE SEARCH","Deep architectures","Quantization loss","Spatial temporals","QUANTIZATION","CODES","Learning frameworks","MANIFOLDS","Feature representation","RETRIEVAL","RECOGNITION","Temporal information","Scalable video"],"max_cite":3.0,"pub_year":2017.0,"sources":"['ieee', 'scp', 'wos']","rawkeys":["spatial temporals","recognition","scalable video search","deep learning","codes","deep architectures","video hashing","manifolds","learning frameworks","retrieval","image search","temporal information","quantization loss","feature representation","quantization","visual-search","scalable video"],"tags":["signals","spatial temporals","recognition","scalable video search","codes","visual search","deep architectures","machine learning","video hashing","manifolds","learning frameworks","retrieval","image search","temporal information","quantization loss","feature representation","scalable video"]},{"p_id":55642,"title":"Neural class-specific regression for face verification","abstract":"Face verification is a problem approached in the literature mainly using non-linear class-specific subspace learning techniques. While it has been shown that kernel-based class-specific discriminant analysis is able to provide excellent performance in small- and medium-scale face verification problems, its application in today's large-scale problems is difficult due to its training space and computational requirements. In this study, generalising on kernel-based class-specific discriminant analysis, it is shown that class-specific subspace learning can be cast as a regression problem. This allows them to derive linear, (reduced) kernel and neural network-based class-specific discriminant analysis methods using efficient batch and\/or iterative training schemes, suited for large-scale learning problems. The authors test the performance of these methods in two datasets describing medium- and large-scale face verification problems.","keywords_author":null,"keywords_other":["NYSTROM METHOD","DISCRIMINANT-ANALYSIS","CLASSIFICATION","RECOGNITION","EXTREME LEARNING-MACHINE"],"max_cite":0.0,"pub_year":2018.0,"sources":"['ieee', 'wos']","rawkeys":["recognition","discriminant-analysis","nystrom method","classification","extreme learning-machine"],"tags":["recognition","denoising autoencoder","nystrom method","classification","extreme learning machine"]},{"p_id":6491,"title":"Predicting the sequence specificities of DNA- and RNA-binding proteins by deep learning","abstract":"\u00a9 2015 Nature America, Inc. All rights reserved.Knowing the sequence specificities of DNA- and RNA-binding proteins is essential for developing models of the regulatory processes in biological systems and for identifying causal disease variants. Here we show that sequence specificities can be ascertained from experimental data with 'deep learning' techniques, which offer a scalable, flexible and unified computational approach for pattern discovery. Using a diverse array of experimental data and evaluation metrics, we find that deep learning outperforms other state-of-the-art methods, even when training on in vitro data and testing on in vivo data. We call this approach DeepBind and have built a stand-alone software tool that is fully automatic and handles millions of sequences per experiment. Specificities determined by DeepBind are readily visualized as a weighted ensemble of position weight matrices or as a 'mutation map' that indicates how variations affect binding within a specific sequence.","keywords_author":null,"keywords_other":["COMPLEXITY","HUMAN TRANSCRIPTION FACTORS","DNA-Binding Proteins","Sequence specificity","CANCER","MICROARRAYS","RNA-binding protein","TERT PROMOTER MUTATIONS","MELANOMA","Specific sequences","Regulatory process","State-of-the-art methods","DISEASE","Stand-alone software","RECOGNITION","Sequence Analysis, Protein","Software","RNA-Binding Proteins","NETWORKS","Evaluation metrics","GENE-REGULATION","Position-Specific Scoring Matrices","Computational Biology","Computational approach"],"max_cite":229.0,"pub_year":2015.0,"sources":"['ieee', 'scp', 'wos']","rawkeys":["state-of-the-art methods","tert promoter mutations","complexity","rna-binding proteins","recognition","disease","microarrays","specific sequences","evaluation metrics","networks","rna-binding protein","software","sequence analysis","position-specific scoring matrices","dna-binding proteins","gene-regulation","cancer","computational biology","melanoma","protein","computational approach","regulatory process","human transcription factors","sequence specificity","stand-alone software"],"tags":["state-of-the-art methods","tert promoter mutations","complexity","microarray","rna-binding proteins","recognition","disease","specific sequences","evaluation metrics","gene regulation","networks","software","sequence analysis","position-specific scoring matrices","dna-binding proteins","cancer","proteins","computational biology","melanoma","computational approach","regulatory process","human transcription factors","sequence specificity","stand-alone software"]},{"p_id":55643,"title":"Improving CNN Performance Accuracies With Min-Max Objective","abstract":"We propose a novel method for improving performance accuracies of convolutional neural network (CNN) without the need to increase the network complexity. We accomplish the goal by applying the proposed Min-Max objective to a layer below the output layer of a CNN model in the course of training. The Min-Max objective explicitly ensures that the feature maps learned by a CNN model have the minimum within-manifold distance for each object manifold and the maximum between-manifold distances among different object manifolds. The Min-Max objective is general and able to be applied to different CNNs with insignificant increases in computation cost. Moreover, an incremental minibatch training procedure is also proposed in conjunction with the Min-Max objective to enable the handling of large-scale training data. Comprehensive experimental evaluations on several benchmark data sets with both the image classification and face verification tasks reveal that employing the proposed Min-Max objective in the training process can remarkably improve performance accuracies of a CNN model in comparison with the same model trained without using this objective.","keywords_author":["Convolutional neural network (CNN)","face verification","image classification","incremental minibatch training procedure","min-max objective"],"keywords_other":["NEURAL-NETWORKS","RECOGNITION","DEEP"],"max_cite":1.0,"pub_year":2018.0,"sources":"['wos']","rawkeys":["neural-networks","recognition","deep","min-max objective","convolutional neural network (cnn)","face verification","incremental minibatch training procedure","image classification"],"tags":["recognition","neural networks","deep","min-max objective","face verification","incremental minibatch training procedure","convolutional neural network","image classification"]},{"p_id":55646,"title":"Neural networks within multi-core optic fibers","abstract":"Hardware implementation of artificial neural networks facilitates real-time parallel processing of massive data sets. Optical neural networks offer low-volume 3D connectivity together with large bandwidth and minimal heat production in contrast to electronic implementation. Here, we present a conceptual design for in-fiber optical neural networks. Neurons and synapses are realized as individual silica cores in a multi-core fiber. Optical signals are transferred transversely between cores by means of optical coupling. Pump driven amplification in erbium-doped cores mimics synaptic interactions. We simulated three-layered feed-forward neural networks and explored their capabilities. Simulations suggest that networks can differentiate between given inputs depending on specific configurations of amplification; this implies classification and learning capabilities. Finally, we tested experimentally our basic neuronal elements using fibers, couplers, and amplifiers, and demonstrated that this configuration implements a neuron-like function. Therefore, devices similar to our proposed multicore fiber could potentially serve as building blocks for future large-scale small-volume optical artificial neural networks.","keywords_author":null,"keywords_other":["BEAM-PROPAGATION METHOD","IMPLEMENTATION","NETS","COUPLED-MODE THEORY","PHOTONIC CRYSTAL FIBERS","RECOGNITION","ASSOCIATIVE-MEMORY","PATTERN","EQUATIONS","SYNCHRONY"],"max_cite":2.0,"pub_year":2016.0,"sources":"['ieee', 'wos']","rawkeys":["equations","pattern","recognition","photonic crystal fibers","associative-memory","synchrony","implementation","coupled-mode theory","nets","beam-propagation method"],"tags":["associative memory","equations","recognition","photonic crystal fiber","patterns","synchrony","implementation","coupled-mode theory","nets","beam-propagation method"]},{"p_id":22907,"title":"Extraversion, neuroticism, attachment style and fear of missing out as predictors of social media use and addiction","abstract":"\u00a9 2017 Elsevier Ltd Social media use is prevalent in today's society and has contributed to problems with social media addiction. The goal of the study was to investigate whether extraversion, neuroticism, attachment style, and fear of missing out (FOMO) were predictors of social media use and addiction. Participants in the study (N = 207) volunteered to complete a brief survey measuring levels of extraversion, neuroticism, attachment styles, and FOMO. In the final model of a hierarchical regression, younger age, neuroticism, and fear of missing out predicted social media use. Only fear of missing out predicted social media addiction. Attachment anxiety and avoidance predicted social media addiction, but this relationship was no longer significant after the addition of FOMO.","keywords_author":["Attachment style","Extraversion","FOMO","Neuroticism","Social media addiction","Social media use"],"keywords_other":null,"max_cite":11.0,"pub_year":2017.0,"sources":"['scp']","rawkeys":["social media addiction","neuroticism","social media use","fomo","attachment style","extraversion"],"tags":["social media addiction","recognition","neuroticism","social media use","fomo","attachment style"]},{"p_id":31099,"title":"A skin segmentation algorithm based on stacked autoencoders","abstract":"\u00a9 1999-2012 IEEE.A good skin detector that is capable of capturing skin tones under different conditions is important for human-machine interaction applications. In a general situation, skin detectors, such as skin probability maps or Gaussian mixture models, achieve acceptable skin segmentation results. However, the false positive rate increases significantly when the skin tones are in shadow or when skin-like background objects are under similar illumination. In this paper, we propose a novel skin feature learning algorithm based on stacked autoencoders, which are deep neural networks. To overcome the problems encountered in skin segmentation that are caused by different ethnicities and varying illumination conditions, the stacked autoencoders are utilized to learn more discriminative representations of the skin area in both the RGB color space and the HSV color space. Unlike traditional machine learning methods, instead of predicting each pixel individually, our algorithm utilizes blocks to learn the representations and detect the skin areas. The algorithm exploits the learning ability of deep neural networks to learn high-level representations of skin tones. Experiments on test images show that the proposed algorithm achieves acceptable results on several publicly available data sets. To reduce the difficulty of detecting skin pixels in these data sets, the ground truths of these data sets are commonly focused on foreground skin area detection. Our skin detector is also able to detect background areas, as shown in our experiments.","keywords_author":["Color space","machine learning","skin detection","stacked autoencoders","Color space","machine learning","skin detection","stacked autoencoders"],"keywords_other":["Skin probability maps","Human machine interaction","CLASSIFICATION","Color space","RECOGNITION","Machine learning methods","MODELS","NEURAL-NETWORK","Skin Detection","Gaussian Mixture Model","Illumination conditions","Autoencoders"],"max_cite":3.0,"pub_year":2017.0,"sources":"['ieee', 'scp', 'wos']","rawkeys":["machine learning methods","recognition","illumination conditions","human machine interaction","machine learning","stacked autoencoders","skin probability maps","autoencoders","classification","models","neural-network","skin detection","color space","gaussian mixture model"],"tags":["machine learning methods","recognition","model","illumination conditions","neural networks","auto encoders","machine learning","stacked autoencoders","skin probability maps","human machine interface","classification","skin detection","color space","gaussian mixture model"]},{"p_id":47484,"title":"Coupled multiview autoencoders with locality sensitivity for three-dimensional human pose estimation","abstract":"\u00a9 2017 SPIE and IS&T. Estimating three-dimensional (3D) human poses from a single camera is usually implemented by searching pose candidates with image descriptors. Existing methods usually suppose that the mapping from feature space to pose space is linear, but in fact, their mapping relationship is highly nonlinear, which heavily degrades the performance of 3D pose estimation. We propose a method to recover 3D pose from a silhouette image. It is based on the multiview feature embedding (MFE) and the locality-sensitive autoencoders (LSAEs). On the one hand, we first depict the manifold regularized sparse low-rank approximation for MFE and then the input image is characterized by a fused feature descriptor. On the other hand, both the fused feature and its corresponding 3D pose are separately encoded by LSAEs. A two-layer back-propagation neural network is trained by parameter fine-tuning and then used to map the encoded 2D features to encoded 3D poses. Our LSAE ensures a good preservation of the local topology of data points. Experimental results demonstrate the effectiveness of our proposed method.","keywords_author":["autoencoder","deep learning","multiview fusion","three-dimensional human pose estimation","three-dimensional human pose estimation","deep learning","autoencoder","multiview fusion"],"keywords_other":["Mapping relationships","Threedimensional (3-d)","Multi-views","FEATURES","NETWORKS","Low rank approximations","3D HUMAN POSE","RECOGNITION","Auto encoders","Back propagation neural networks","Feature descriptors","Human pose estimations"],"max_cite":0.0,"pub_year":2017.0,"sources":"['scp', 'wos']","rawkeys":["human pose estimations","recognition","back propagation neural networks","feature descriptors","deep learning","features","threedimensional (3-d)","autoencoder","auto encoders","networks","3d human pose","three-dimensional human pose estimation","low rank approximations","multiview fusion","mapping relationships","multi-views"],"tags":["human pose estimations","recognition","back propagation neural networks","feature descriptors","features","machine learning","auto encoders","networks","3d human pose","three-dimensional human pose estimation","low rank approximations","multiview fusion","mapping relationships","three-dimensional","multi-views"]},{"p_id":47489,"title":"Vehicle recognition and its trajectory registration on the image sequence using deep convolutional neural network","abstract":"\u00a9 2017 IEEE. The article shows the methods of vehicle recognition on the image sequence and its trajectory registration. As a recognition algorithm authors used Viola-Jones method with optical flow filter and the deep convolutional neural network in combination with sliding window technique for vehicle detection task. Also authors analyze approaches to registration of detected vehicle trajectories on image sequence based on its linear and angular velocities and Kalman filter. The efficiency of vehicle detection is shown in terms of the precision and recall of recognition. Quality of vehicle registration on the image sequence is estimated by the standard deviation of results from sample values. The article also shows usage prospects of proposed algorithms as a part of driver assistance system and unmanned vehicle control system.","keywords_author":["convolutional neural network","deep learning","detection","driver assistance system","image sequence","recognition","registration","trajectory","vehicle","Viola-Jones method"],"keywords_other":["recognition","Image sequence","Viola jones","Convolutional neural network","registration","Driver assistance system"],"max_cite":0.0,"pub_year":2017.0,"sources":"['scp', 'ieee']","rawkeys":["recognition","deep learning","viola-jones method","viola jones","driver assistance system","image sequence","detection","convolutional neural network","registration","trajectory","vehicle"],"tags":["recognition","viola-jones method","machine learning","viola jones","image sequences","driver assistance system","trajectories","vehicles","detection","convolutional neural network","registration"]},{"p_id":104838,"title":"MiRduplexSVM: A High-Performing MiRNA-Duplex Prediction and Evaluation Methodology","abstract":"We address the problem of predicting the position of a miRNA duplex on a microRNA hairpin via the development and application of a novel SVM-based methodology. Our method combines a unique problem representation and an unbiased optimization protocol to learn from mirBase19.0 an accurate predictive model, termed MiRduplexSVM. This is the first model that provides precise information about all four ends of the miRNA duplex. We show that (a) our method outperforms four state-of-the-art tools, namely MaturePred, MiRPara, MatureBayes, MiRdup as well as a Simple Geometric Locator when applied on the same training datasets employed for each tool and evaluated on a common blind test set. (b) In all comparisons, MiRduplexSVM shows superior performance, achieving up to a 60% increase in prediction accuracy for mammalian hairpins and can generalize very well on plant hairpins, without any special optimization. (c) The tool has a number of important applications such as the ability to accurately predict the miRNA or the miRNA*, given the opposite strand of a duplex. Its performance on this task is superior to the 2nts overhang rule commonly used in computational studies and similar to that of a comparative genomic approach, without the need for prior knowledge or the complexity of performing multiple alignments. Finally, it is able to evaluate novel, potential miRNAs found either computationally or experimentally. In relation with recent confidence evaluation methods used in miRBase, MiRduplexSVM was successful in identifying high confidence potential miRNAs.","keywords_author":null,"keywords_other":["BIOGENESIS","IDENTIFICATION","MODEL","CANCER","RECOGNITION","DEEP SEQUENCING DATA","GENE PREDICTION","MICRORNAS","DROSHA","SPECIFICITY"],"max_cite":5.0,"pub_year":2015.0,"sources":"['wos']","rawkeys":["drosha","identification","model","recognition","cancer","specificity","gene prediction","biogenesis","deep sequencing data","micrornas"],"tags":["drosha","identification","model","recognition","cancer","specificity","gene prediction","biogenesis","deep sequencing data","micrornas"]},{"p_id":22926,"title":"Learning a hyperplane classifier by minimizing an exact bound on the VC dimension","abstract":"\u00a9 2014 Elsevier B.V. The VC dimension measures the complexity of a learning machine, and a low VC dimension leads to good generalization. While SVMs produce state-of-the-art learning performance, it is well known that the VC dimension of a SVM can be unbounded; despite good results in practice, there is no guarantee of good generalization. In this paper, we show how to learn a hyperplane classifier by minimizing an exact, or \u0398 bound on its VC dimension. The proposed approach, termed as the Minimal Complexity Machine (MCM), involves solving a simple linear programming problem. Experimental results show, that on a number of benchmark datasets, the proposed approach learns classifiers with error rates much less than conventional SVMs, while often using fewer support vectors. On many benchmark datasets, the number of support vectors is less than one-tenth the number used by SVMs, indicating that the MCM does indeed learn simpler representations.","keywords_author":["Complexity","Generalization","Machine learning","Sparse","Support vector machines","VC dimension"],"keywords_other":["Learning performance","Complexity","Benchmark datasets","Learning machines","Linear programming problem","Sparse","VC dimension","Generalization"],"max_cite":11.0,"pub_year":2015.0,"sources":"['scp']","rawkeys":["benchmark datasets","machine learning","complexity","generalization","sparse","linear programming problem","learning machines","support vector machines","vc dimension","learning performance"],"tags":["benchmark datasets","recognition","machine learning","complexity","sparse","linear programming problem","learning machines","vc dimension","learning performance"]},{"p_id":47510,"title":"Nonredundant sparse feature extraction using autoencoders with receptive fields clustering","abstract":"\u00a9 2017 Elsevier LtdThis paper proposes new techniques for data representation in the context of deep learning using agglomerative clustering. Existing autoencoder-based data representation techniques tend to produce a number of encoding and decoding receptive fields of layered autoencoders that are duplicative, thereby leading to extraction of similar features, thus resulting in filtering redundancy. We propose a way to address this problem and show that such redundancy can be eliminated. This yields smaller networks and produces unique receptive fields that extract distinct features. It is also shown that autoencoders with nonnegativity constraints on weights are capable of extracting fewer redundant features than conventional sparse autoencoders. The concept is illustrated using conventional sparse autoencoder and nonnegativity-constrained autoencoders with MNIST digits recognition, NORB normalized-uniform object data and Yale face dataset.","keywords_author":["Agglomerative clustering","Autoencoder","Deep learning","Filter clustering","Receptive fields","Autoencoder","Agglomerative clustering","Deep learning","Filter clustering","Receptive fields"],"keywords_other":["Data representations","Receptive fields","Cluster Analysis","Filter clustering","Learning","Pattern Recognition, Automated","REPRESENTATION","CONSTRAINTS","ALGORITHM","Databases, Factual","Non-negativity constraints","NEURAL-NETWORKS","Information Storage and Retrieval","RECOGNITION","Agglomerative clustering","Auto encoders","Encoding and decoding","Redundant features"],"max_cite":0.0,"pub_year":2017.0,"sources":"['ieee', 'scp', 'wos']","rawkeys":["automated","agglomerative clustering","databases","autoencoder","non-negativity constraints","encoding and decoding","data representations","neural-networks","constraints","cluster analysis","filter clustering","information storage and retrieval","algorithm","recognition","redundant features","deep learning","learning","receptive fields","factual","auto encoders","representation","pattern recognition"],"tags":["automated","agglomerative clustering","databases","random forests","non-negativity constraints","encoding and decoding","data representations","constraints","machine learning","cluster analysis","filter clustering","algorithms","information storage and retrieval","recognition","redundant features","neural networks","factual","auto encoders","representation","pattern recognition"]},{"p_id":39320,"title":"Performance analysis of classification algorithms applied to Caltech101 image database","abstract":"Identifying the wide range of applications, machine learning algorithms proved its ability to learn without being explicitly programmed. Classifying the images through machine learning algorithms is getting wide range of acceptability nowadays. Being a branch of Artificial Intelligence, machine learning implies the study of systems which has the capability to learn from data. Machine learning involves two parts-representation and generalization. Representation implies labeling seen data instances and generalization determines whether the system can perform well on unlabelled data instances. In this article, we focused on the performance of machine learning algorithms [1]. A CBIR (Content Based Image Retrieval) frame work has been developed and obtained a reduced texture feature data set using Caltech101 image database [2]. We highlight the top five algorithms such as Logistic, Bagging, LMT, Multiclass classifier and Attribute selection classifier which can be used for image classification. In introduction, an overview of the selected techniques is presented. We have extracted 2037 feature vectors from Caltech101 image database. These data are used to distinguish the performance of machine learning algorithms. Having checked all machine learning algorithms supported, we identified top five algorithms that have a better performance compared to other machine learning algorithms. The software used for testing is WEKA [3], which is an open source software developed by University of Waikato, New Zealand. \u00a9 2014 IEEE.","keywords_author":["Artificial Intelligence","Caltech 101","Generalization","Machine Learning","Representation"],"keywords_other":["Performance analysis","Open Source Software","Classification algorithm","Content based image retrieval","Representation","Multi-class classifier","Caltech","Generalization"],"max_cite":1.0,"pub_year":2014.0,"sources":"['scp']","rawkeys":["performance analysis","artificial intelligence","open source software","caltech","machine learning","representation","generalization","multi-class classifier","content based image retrieval","classification algorithm","caltech 101"],"tags":["performance analysis","recognition","open source software","caltech","machine learning","representation","multi-class classifier","content-based image retrieval","classification algorithm","caltech-101"]},{"p_id":96674,"title":"How automated image analysis techniques help scientists in species identification and classification?","abstract":"Identification of taxonomy at a specific level is time consuming and reliant upon expert ecologists. Hence the demand for automated species identification increased over the last two decades. Automation of data classification is primarily focussed on images while incorporating and analysing image data has recently become easier due to developments in computational technology. Research efforts on identification of species include specimens' image processing, extraction of identical features, followed by classifying them into correct categories. In this paper, we discuss recent automated species identification systems, mainly for categorising and evaluating their methods. We reviewed and compared different methods in step by step scheme of automated identification and classification systems of species images. The selection of methods is influenced by many variables such as level of classification, number of training data and complexity of images. The aim of writing this paper is to provide researchers and scientists an extensive background study on work related to automated species identification, focusing on pattern recognition techniques in building such systems for biodiversity studies.","keywords_author":["automated image recognition","digital image processing","species images","species classification","life data technology"],"keywords_other":["ARTIFICIAL NEURAL-NETWORK","DISCRIMINANT-ANALYSIS","FEATURES","FEATURE-SELECTION","SYSTEM","LOCAL BINARY PATTERNS","RECOGNITION","RETRIEVAL","GYRODACTYLUS","SCALE"],"max_cite":0.0,"pub_year":2018.0,"sources":"['wos']","rawkeys":["automated image recognition","recognition","species classification","features","life data technology","artificial neural-network","discriminant-analysis","digital image processing","system","feature-selection","species images","local binary patterns","retrieval","gyrodactylus","scale"],"tags":["automated image recognition","recognition","species classification","features","neural networks","denoising autoencoder","life data technology","system","digital image processing","species images","feature selection","local binary patterns","retrieval","gyrodactylus","scale"]},{"p_id":14779,"title":"Optimizing classifiers for imbalanced training sets","abstract":"Following recent results [9, 8] showing the importance of the fat-shattering dimension in explaining the beneficial effect of a large margin on generalization performance, the current paper investigates the implications of these results for the case of imbalanced datasets and develops two approaches to setting the threshold. The approaches are incorporated into ThetaBoost, a boosting algorithm for dealing with unequal loss functions. The performance of ThetaBoost and the two approaches are tested experimentally.","keywords_author":["Computational Learning Theory","Fat-shattering","Generalization","Imbalanced datasets","Large margin","Pac estimates","Unequal loss"],"keywords_other":["Pac estimates","Fat-shattering","Imbalanced Data-sets","Computational learning theory","Large margins","Generalization"],"max_cite":54.0,"pub_year":1999.0,"sources":"['scp', 'wos']","rawkeys":["computational learning theory","pac estimates","unequal loss","large margins","fat-shattering","generalization","imbalanced datasets","large margin","imbalanced data-sets"],"tags":["computational learning theory","pac estimates","recognition","unequal loss","large margins","fat-shattering","imbalanced data-sets"]},{"p_id":113138,"title":"Deep Convolutional Activations-Based Features for Ground-Based Cloud Classification","abstract":"Ground-based cloud classification is crucial for meteorological research and has received great concern in recent years. However, it is very challenging due to the extreme appearance variations under different atmospheric conditions. Although the convolutional neural networks have achieved remarkable performance in image classification, no one has evaluated their suitability for cloud classification. In this letter, we propose to use the deep convolutional activations-based features (DCAFs) for ground-based cloud classification. Considering the unique characteristic of cloud, we believe the local rich texture information might be more important than the global layout information and, thus, give a comprehensive evaluation of using both shallow convolutional layers-based features and DCAFs. Experimental results on two challenging public data sets demonstrate that although the realization of DCAF is quite straightforward without any use-dependent tricks, it outperforms conventional hand-crafted features considerably.","keywords_author":["Cloud classification","convolutional activations","convolutional neural network (CNN)","fine-tune","max pooling","sum pooling"],"keywords_other":["RECOGNITION","IMAGES"],"max_cite":3.0,"pub_year":2017.0,"sources":"['ieee', 'wos']","rawkeys":["convolutional activations","fine-tune","recognition","images","cloud classification","convolutional neural network (cnn)","sum pooling","max pooling"],"tags":["convolutional activations","fine tuning","recognition","images","cloud classification","convolutional neural network","max-pooling","sum pooling"]},{"p_id":88566,"title":"Hybrid-feature-guided lung nodule type classification on CT images","abstract":"In this paper, we propose a novel classification method for lung nodules from CT images based on hybrid features. Towards nodules of different types, including well-circumscribed, vascularized, juxtapleural, pleural-tail, as well as ground glass optical (GGO) and non-nodule from CT scans, our method has achieved promising classification results. The proposed method utilizes hybrid descriptors consisting of statistical features from multi-view multi-scale convolutional neural networks (CNNs) and geometrical features from Fisher vector (FV) encodings based on scale-invariant feature transform (SIFT). First, we approximate the nodule radii based on icosahedron sampling and intensity analysis. Then, we apply high frequency content measure analysis to obtain sampling views with more abundant information. After that, based on re-sampled views, we train multi-view multi-scale CNNs to extract statistical features and calculate FV encodings as geometrical features. Finally, we achieve hybrid features by merging statistical and geometrical features based on multiple kernel learning (MKL) and classify nodule types through a multi-class support vector machine. The experiments on LIDC-IDRI and ELCAP have shown that our method has achieved promising results and can be of great assistance for radiologists' diagnosis of lung cancer in clinical practice. (C) 2017 Elsevier Ltd. All rights reserved.","keywords_author":["Computer tomography","Lung nodule","CNNs","Hybrid features"],"keywords_other":["DIAGNOSIS","PULMONARY NODULES","MODEL","RECOGNITION","SEGMENTATION","CANCER","SCANS","LESIONS","COMPUTED-TOMOGRAPHY IMAGES"],"max_cite":3.0,"pub_year":2017.0,"sources":"['ieee', 'wos']","rawkeys":["cnns","diagnosis","recognition","model","computer tomography","cancer","segmentation","lung nodule","pulmonary nodules","computed-tomography images","lesions","scans","hybrid features"],"tags":["diagnosis","recognition","model","segmentation","cancer","pulmonary nodules","computed-tomography images","lung nodules","computed tomography","convolutional neural network","lesions","scans","hybrid features"]},{"p_id":113203,"title":"An Electricity Price Forecasting Model by Hybrid Structured Deep Neural Networks","abstract":"Electricity price is a key influencer in the electricity market. Electricity market trades by each participant are based on electricity price. The electricity price adjusted with the change in supply and demand relationship can reflect the real value of electricity in the transaction process. However, for the power generating party, bidding strategy determines the level of profit, and the accurate prediction of electricity price could make it possible to determine a more accurate bidding price. This cannot only reduce transaction risk, but also seize opportunities in the electricity market. In order to effectively estimate electricity price, this paper proposes an electricity price forecasting system based on the combination of 2 deep neural networks, the Convolutional Neural Network (CNN) and the Long Short Term Memory (LSTM). In order to compare the overall performance of each algorithm, the Mean Absolute Error (MAE) and Root-Mean-Square error (RMSE) evaluating measures were applied in the experiments of this paper. Experiment results show that compared with other traditional machine learning methods, the prediction performance of the estimating model proposed in this paper is proven to be the best. By combining the CNN and LSTM models, the feasibility and practicality of electricity price prediction is also confirmed in this paper.","keywords_author":["electricity price forecasting","hybrid structured model","convolutional neural network","long short term memory"],"keywords_other":["PREDICTION","SUPPORT VECTOR MACHINE","FEATURE-SELECTION","ALGORITHM","SYSTEM","RECOGNITION","SUSTAINABILITY","DECISION TREE CLASSIFIER"],"max_cite":0.0,"pub_year":2018.0,"sources":"['wos']","rawkeys":["hybrid structured model","algorithm","recognition","prediction","electricity price forecasting","system","feature-selection","convolutional neural network","support vector machine","decision tree classifier","sustainability","long short term memory"],"tags":["hybrid structured model","recognition","long short-term memory","prediction","machine learning","electricity price forecasting","system","feature selection","convolutional neural network","algorithms","sustainability","decision tree classifiers"]},{"p_id":72258,"title":"Methane Gas Density Monitoring and Predicting Based on RFID Sensor Tag and CNN Algorithm","abstract":"According to the advantages of integrating wireless sensors networks (WSN) and radio frequency identification (RFID), this paper proposes a novel method for methane gas density monitoring and predicting based on a passive RFID sensor tag and a convolutional neural networks (CNN) algorithm. The proposed wireless sensor is based on electronic product code (EPC) generation2 (G2) protocol and the sensor data is embedded into the identification (ID) information of the RFID chip. The wireless sensor consists of a communication section, radio-frequency (RF) front-end section, and digital section. The communication section is used to perform the transmission and reception of wireless signals, modulation, and demodulation. The RF front-end section is adopted to provide the stable supply voltage for other parts. The digital section is employed to achieve sensor data and control the overall operation of the wireless sensor based on EPC protocol. Because the miscellaneous noises will decrease the accuracy during the process of data wireless transmission, the CNN algorithm is adopted to extract the robust feature from raw data. The measurement results show that the exploited RFID sensor can realize a maximum communication distance of 10.3 m and can accurately measure and predict the methane gas density in an underground mine. The RFID sensor technology is a beneficial supplement to the current underground WSN monitoring system.","keywords_author":["methane gas density","RFID sensor tag","CNN algorithm"],"keywords_other":["ZIGBEE","COAL-MINES","FAULT-DIAGNOSIS","DESIGN","SYSTEM","NEURAL-NETWORKS","RECOGNITION","ENVIRONMENT"],"max_cite":0.0,"pub_year":2018.0,"sources":"['wos']","rawkeys":["neural-networks","design","rfid sensor tag","recognition","methane gas density","coal-mines","system","zigbee","environment","fault-diagnosis","cnn algorithm"],"tags":["rfid sensor tag","design","recognition","methane gas density","neural networks","coal-mines","system","zigbee","environment","fault diagnosis","cnn algorithm"]},{"p_id":72259,"title":"Image analysis method for crack distribution and width estimation for reinforced concrete structures","abstract":"Crack observation is important for evaluating the structural performance and safety of reinforced concrete (RC) structures. Most of the existing image-based crack detection methods are based on edge detection algorithms, which detect cracks that are wide enough to present dark areas in the obtained images. Cracks initiate as thin cracks, generally having width less than the width of a pixel in images; such cracks are generally undetectable by edge detection-based methods.","keywords_author":["Image analysis","Shear crack","Thin crack observation","RC structure","Structural-health monitoring"],"keywords_other":["OPTICAL-FLOW","ELEMENT","DAMAGE","INSPECTION","MODEL","RECOGNITION","CAMERA"],"max_cite":0.0,"pub_year":2018.0,"sources":"['wos']","rawkeys":["damage","element","optical-flow","shear crack","recognition","model","rc structure","thin crack observation","inspection","camera","structural-health monitoring","image analysis"],"tags":["damage","shear crack","recognition","model","rc structures","thin crack observation","inspection","cameras","structural health monitoring","optical flow","elements","image analysis"]},{"p_id":14921,"title":"A comparison of linear and neural network ARX models applied to a prediction of the indoor temperature of a building","abstract":"A neural network auto regressive with exogenous input (NNARX) model is used to predict the indoor temperature of a residential building. Firstly, the optimal regresser of a linear ARX model is identified by minimising Akaike's final prediction error (FPE). This regressor is then used as the input vector of a fully connected feedforward neural network with one hidden layer often units and one output unit. Results show that the NNARX model outperforms the linear model considerably: the sum of the squared error (SSE) is 15.0479 with the ARX model and 2.0632 with the NNARX model. The optimal network topology is subsequently determined by pruning the fully connected network according to the optimal brain surgeon (OBS) strategy. With this procedure near 73% of connections were removed and, as a result, the performance of the network has been improved: the SSE is equal to 0.9060.","keywords_author":["Feedforward network","Generalisation","Neural network ARX model","Optimal brain surgeon strategy","Overfitting","Pruned neural network"],"keywords_other":["Neural network ARX model","Overfitting","Pruned neural network","Generalization","Optimal brain surgeon strategy"],"max_cite":52.0,"pub_year":2004.0,"sources":"['scp']","rawkeys":["neural network arx model","generalisation","pruned neural network","generalization","optimal brain surgeon strategy","overfitting","feedforward network"],"tags":["neural network arx model","pruned neural network","recognition","feed-forward network","machine learning","optimal brain surgeon strategy","overfitting"]},{"p_id":23113,"title":"Flash flood forecasting by statistical learning in the absence of rainfall forecast: A case study","abstract":"The feasibility of flash flood forecasting without making use of rainfall predictions is investigated. After a presentation of the \"cevenol flash floods\", which caused 1.2 billion Euros of economical damages and 22 fatalities in 2002, the difficulties incurred in the forecasting of such events are analyzed, with emphasis on the nature of the database and the origins of measurement noise. The high level of noise in water level measurements raises a real challenge. For this reason, two regularization methods have been investigated and compared: early stopping and weight decay. It appears that regularization by early stopping provides networks with lower complexity and more accurate predicted hydrographs than regularization by weight decay. Satisfactory results can thus be obtained up to a forecasting horizon of three hours, thereby allowing an early warning of the populations. \u00a9 2009 Springer-Verlag.","keywords_author":["early stopping","Forecasting","generalization","identification","machine learning","neural network","weight decay"],"keywords_other":["identification","Machine-learning","Early stopping","generalization","Weight decay"],"max_cite":11.0,"pub_year":2009.0,"sources":"['scp']","rawkeys":["neural network","identification","forecasting","machine learning","early stopping","generalization","weight decay","machine-learning"],"tags":["identification","recognition","neural networks","forecasting","machine learning","early stopping","weight decay"]},{"p_id":47697,"title":"Unsupervised t-Distributed Video Hashing and its Deep Hashing Extension","abstract":"IEEE In this work, a novel unsupervised hashing algorithm, referred to as t-USMVH, and its extension to unsupervised deep hashing, referred to as t-UDH, are proposed to support large-scale video-to-video retrieval. To improve robustness of the unsupervised learning, t-USMVH combines multiple types of feature representations and effectively fuses them by examining a continuous relevance score based on a Gaussian estimation over pairwise distances, and also a discrete neighbor score based on the cardinality of reciprocal neighbors. To reduce sensitivity to scale changes for mapping objects that are far apart from each other, Student t-distribution is used to estimate the similarity between the relaxed hash code vectors for keyframes. This results in more accurate preservation of the desired unsupervised similarity structure in the hash code space. By adapting the corresponding optimization objective and constructing the hash mapping function via a deep neural network, we develop a robust unsupervised training strategy for a deep hashing network. The efficiency and effectiveness of the proposed methods are evaluated on two public video collections via comparisons against multiple classical and state-of-the-art methods.","keywords_author":["Computational modeling","deep neural network","Feature extraction","hashing","Machine learning","multi-view learning","Neural networks","Robustness","Student t-distribution","Training","unsupervised learning","Video retrieval","Visualization","Video retrieval","hashing","deep neural network","multi-view learning","unsupervised learning","Student t-distribution"],"keywords_other":["Multi-view learning","LOCALIZATION","Computational model","REPRESENTATION","SEARCH","MANIFOLDS","RECOGNITION","hashing","IMAGE RETRIEVAL","MULTI-FEATURE FUSION","Student-t distribution","Video retrieval"],"max_cite":1.0,"pub_year":2017.0,"sources":"['ieee', 'scp', 'wos']","rawkeys":["robustness","multi-view learning","search","localization","manifolds","student-t distribution","visualization","machine learning","computational modeling","recognition","neural networks","multi-feature fusion","training","image retrieval","video retrieval","student t-distribution","unsupervised learning","computational model","deep neural network","representation","hashing","feature extraction"],"tags":["robustness","multi-view learning","search","localization","manifolds","convolutional neural network","visualization","machine learning","computational modeling","recognition","neural networks","multi-feature fusion","training","image retrieval","video retrieval","unsupervised learning","student's-t distribution","representation","hashing","feature extraction"]},{"p_id":55960,"title":"Weakly supervised target detection in remote sensing images based on transferred deep features and negative bootstrapping","abstract":"Target detection in remote sensing images (RSIs) is a fundamental yet challenging problem faced for remote sensing images analysis. More recently, weakly supervised learning, in which training sets require only binary labels indicating whether an image contains the object or not, has attracted considerable attention owing to its obvious advantages such as alleviating the tedious and time consuming work of human annotation. Inspired by its impressive success in computer vision field, in this paper, we propose a novel and effective framework for weakly supervised target detection in RSIs based on transferred deep features and negative bootstrapping. On one hand, to effectively mine information from RSIs and improve the performance of target detection, we develop a transferred deep model to extract high-level features from RSIs, which can be achieved by pre-training a convolutional neural network model on a large-scale annotated dataset (e.g. ImageNet) and then transferring it to our task by domain-specifically fine-tuning it on RSI datasets. On the other hand, we integrate negative bootstrapping scheme into detector training process to make the detector converge more stably and faster by exploiting the most discriminative training samples. Comprehensive evaluations on three RSI datasets and comparisons with state-of-the-art weakly supervised target detection approaches demonstrate the effectiveness and superiority of the proposed method.","keywords_author":["Target detection","Weakly supervised learning","Transferred deep features","Negative bootstrapping","Remote sensing images"],"keywords_other":["SUPPORT VECTOR MACHINES","RECOGNITION","KEYPOINTS","EFFICIENT","SCENE CLASSIFICATION","OBJECT DETECTION"],"max_cite":23.0,"pub_year":2016.0,"sources":"['ieee', 'wos']","rawkeys":["efficient","recognition","transferred deep features","keypoints","scene classification","target detection","object detection","negative bootstrapping","support vector machines","remote sensing images","weakly supervised learning"],"tags":["recognition","transferred deep features","keypoints","scene classification","machine learning","efficiency","target detection","object detection","negative bootstrapping","remote sensing images","weakly supervised learning"]},{"p_id":72344,"title":"Face alignment in-the-wild: A Survey","abstract":"Over the last two decades, face alignment or localizing fiducial facial points on 2D images has received increasing attention owing to its comprehensive applications in automatic face analysis. However, such a task has proven extremely challenging in unconstrained environments due to many confounding factors, such as pose, occlusions, expression and illumination. While numerous techniques have been developed to address these challenges, this problem is still far away from being solved. In this survey, we present an up-to-date critical review of the existing literatures on face alignment, focusing on those methods addressing overall difficulties and challenges of this topic under uncontrolled conditions. Specifically, we categorize existing face alignment techniques, present detailed descriptions of the prominent algorithms within each category, and discuss their advantages and disadvantages. Furthermore, we organize special discussions on the practical aspects of face alignment in-the-wild, towards the development of a robust face alignment system. In addition, we show performance statistics of the state of the art, and conclude this paper with several promising directions for future research. (C) 2017 Elsevier Inc. All rights reserved.","keywords_author":["Face alignment","Active appearance model","Constrained local model","Cascaded regression","Deep convolutional neural networks"],"keywords_other":["REGRESSION","MORPHABLE MODEL","LANDMARK LOCALIZATION","REPRESENTATION","CLASSIFICATION","ACTIVE APPEARANCE MODELS","RECOGNITION","ROBUST"],"max_cite":1.0,"pub_year":2017.0,"sources":"['ieee', 'wos']","rawkeys":["active appearance models","landmark localization","face alignment","recognition","active appearance model","deep convolutional neural networks","constrained local model","representation","classification","robust","cascaded regression","regression","morphable model"],"tags":["active appearance models","landmark localization","face alignment","robustness","recognition","representation","classification","convolutional neural network","cascaded regression","constrained local models","regression","morphable model"]},{"p_id":55967,"title":"Discriminant deep belief network for high-resolution SAR image classification","abstract":"Classification plays an important role in many fields of synthetic aperture radar (SAR) image understanding and interpretation. Many scholars have devoted to design features to characterize the content of SAR images. However, it is still a challenge to design discriminative and robust features for SAR image classification. Recently, the deep learning has attracted much attention and has been successfully applied in many fields of computer vision. In this paper, a novel feature learning approach that is called discriminant deep belief network (DisDBN) is proposed to learning high-level features for SAR image classification, in which the discriminant features are learned by combining ensemble learning with a deep belief network in an unsupervised manner. Firstly, some subsets of SAR image patches are selected and marked with pseudo-labels to train weak classifiers. Secondly, the specific SAR image patch is characterized by a set of projection vectors that are obtained by projecting the SAR image patch onto each weak decision space spanned by each weak classifier. Finally, the discriminant features are generated by feeding the projection vectors to a DBN for SAR image classification. Experimental results demonstrate that better classification performance can be achieved by the proposed approach than the other state-of-the-art approaches. (C) 2016 Elsevier Ltd. All rights reserved.","keywords_author":["Discriminant feature learning","Deep belief network","SAR image classification","Ensemble learning","Similarity measurement"],"keywords_other":["BOLTZMANN MACHINES","INFORMATION","REPRESENTATION","NEURAL-NETWORKS","RECOGNITION","SEGMENTATION","BAND","FEATURE-EXTRACTION"],"max_cite":15.0,"pub_year":2016.0,"sources":"['ieee', 'wos']","rawkeys":["neural-networks","recognition","similarity measurement","segmentation","feature-extraction","boltzmann machines","discriminant feature learning","representation","sar image classification","band","deep belief network","information","ensemble learning"],"tags":["recognition","segmentation","neural networks","boltzmann machines","representation","sar image classification","similarity measure","band","feature extraction","information","ensemble learning","discriminative feature learning","deep belief networks"]},{"p_id":96928,"title":"Proffering a discourse in different communicative contexts","abstract":"Communicative contexts may affect how the speaker proffers a discourse. In particular, we assumed that uni-directional (as compared with bi-directional) and audio (as compared with audio-visual) contexts induce the speaker to elaborate and then use an articulated mental model of the discourse because they do not allow the exploitation of all the communicative means. Unidirectional contexts do not allow recovery of communicative failures, and audio contexts do not allow access to extralinguistic communication. The results of an experiment involving 84 adult participants confirmed the predictions deriving from these assumptions: linguistic indices of the exploitation of an articulated mental model of the discourse are greater in uni-directional and audio contexts as compared with bi-directional and audio-visual contexts, respectively. (C) 2009 Elsevier B.V. All rights reserved.","keywords_author":["Discourse production","Communicative contexts","Levels-of-processing","Mental models"],"keywords_other":["TEXT COMPREHENSION","FRAMEWORK","INFERENCES","MODALITY CONFUSION","RECOGNITION","MEMORY RESEARCH","GESTURES"],"max_cite":0.0,"pub_year":2010.0,"sources":"['wos']","rawkeys":["inferences","levels-of-processing","recognition","framework","mental models","modality confusion","gestures","memory research","text comprehension","communicative contexts","discourse production"],"tags":["levels-of-processing","recognition","framework","mental models","modality confusion","gestures","memory research","text comprehension","inference","communicative contexts","discourse production"]},{"p_id":96940,"title":"The beneficial effect of a speaker's gestures on the listener's memory for action phrases: The pivotal role of the listener's premotor cortex","abstract":"Memory for action phrases improves in the listeners when the speaker accompanies them with gestures compared to when the speaker stays still. Since behavioral studies revealed a pivotal role of the listeners' motor system, we aimed to disentangle the role of primary motor and premotor cortices. Participants had to recall phrases uttered by a speaker in two conditions: in the gesture condition, the speaker performed gestures congruent with the action; in the no-gesture condition, the speaker stayed still. In Experiment 1, half of the participants underwent inhibitory rTMS over the hand\/arm region of the left premotor cortex (PMC) and the other half over the hand\/arm region of the left primary motor cortex (M1). The enactment effect disappeared only following rTMS over PMC. In Experiment 2, we detected the usual enactment effect after rTMS over vertex, thereby excluding possible nonspecific rTMS effects. These findings suggest that the information encoded in the premotor cortex is a crucial part of the memory trace.","keywords_author":["Enactment","Motor system","Gesture observation","Memory for actions"],"keywords_other":["METAANALYSIS","MIRROR NEURON","MOTOR INFORMATION","STIMULATION","AREAS","PATIENT","RECOGNITION","REACTIVATION","ACTION EVENTS","BRAIN"],"max_cite":0.0,"pub_year":2018.0,"sources":"['wos']","rawkeys":["reactivation","memory for actions","motor system","recognition","enactment","areas","stimulation","gesture observation","brain","metaanalysis","mirror neuron","motor information","action events","patient"],"tags":["memory for actions","motor system","reactivity","recognition","enactment","stimulation","gesture observation","brain","metaanalysis","mirror neuron","motor information","action events","area","patient"]},{"p_id":72370,"title":"Generation of action description from classification of motion and object","abstract":"This paper presents a novel approach to learning of relations among motions, objects, and language, and to generating sentences that describe human actions. Our approach categorizes human motions and the objects acted on those motions, and subsequently integrates the motion categories and object categories with their descriptive sentences. The integration consists of two steps. The first step stochastically learns the relations among the motions, objects, and words in the sentences. The second step stochastically learns the order of words in the sentences as the sentence structures. The model derived in the first step is referred to as \"action language\" model and that derived in the second step as \"natural language\" model. This framework for integrating an action language model with a natural language model can be applied to generating descriptive sentences from human actions, where each action is recognized as a pair containing a motion category and an object category, the words relevant to the action are generated via the contained motion and object categories, and the words to be arranged result in a descriptive sentence. More theoretically, our approach searches for multiple words likely to be generated from the motion and object categories by using the action language model; and subsequently searches for a sequence of these words that is likely to be generated from the obtained words, using the natural language model. We tested our proposed approach for sentence generation by applying it to human action data captured by an RGB-D sensor, and demonstrated its validity. (C) 2017 The Authors. Published by Elsevier B.V.","keywords_author":["Action description","Human motion","Object recognition"],"keywords_other":["HIDDEN MARKOV-MODELS","NETWORKS","RECOGNITION","ROBOTS","IMITATE"],"max_cite":1.0,"pub_year":2017.0,"sources":"['wos']","rawkeys":["recognition","human motion","action description","robots","imitate","object recognition","networks","hidden markov-models"],"tags":["hidden markov models","recognition","action description","human motions","object recognition","networks","robotics","imitation"]},{"p_id":31424,"title":"A holistic approach for inspection of civil infrastructures based on computer vision techniques","abstract":"In this work, it is examined the 2D recognition and 3D modelling of concrete tunnel cracks, through visual cues. At the time being, the structural integrity inspection of large-scale infrastructures is mainly performed through visual observations by human inspectors, who identify structural defects, rate them and, then, categorize their severity. The described approach targets at minimum human intervention, for autonomous inspection of civil infrastructures. The shortfalls of existing approaches in crack assessment are being addressed by proposing a novel detection scheme. Although efforts have been made in the field, synergies among proposed techniques are still missing. The holistic approach of this paper exploits the state of the art techniques of pattern recognition and stereo-matching, in order to build accurate 3D crack models. The innovation lies in the hybrid approach for the CNN detector initialization, and the use of the modified census transformation for stereo matching along with a binary fusion of two state-of- The- Art optimization schemes. The described approach manages to deal with images of harsh radiometry, along with severe radiometric differences in the stereo pair. The effectiveness of this workflow is evaluated on a real dataset gathered in highway and railway tunnels. What is promising is that the computer vision workflow described in this work can be transferred, with adaptations of course, to other infrastructure such as pipelines, bridges and large industrial facilities that are in the need of continuous state assessment during their operational life cycle.","keywords_author":["Automation","Cracks","Deep learning","Inspection","Matching","Recognition","Reconstruction"],"keywords_other":["Deep learning","Civil infrastructures","Recognition","Computer vision techniques","Matching","Large scale infrastructures","Industrial facilities","State-of-the-art techniques"],"max_cite":3.0,"pub_year":2016.0,"sources":"['scp']","rawkeys":["recognition","cracks","large scale infrastructures","deep learning","inspection","industrial facilities","automation","reconstruction","civil infrastructures","computer vision techniques","matching","state-of-the-art techniques"],"tags":["cracking","recognition","large scale infrastructures","automated","machine learning","inspection","industrial facilities","reconstruction","civil infrastructures","computer vision techniques","matching","state-of-the-art techniques"]},{"p_id":56010,"title":"Robots That Think Fast and Slow: An Example of Throwing the Ball Into the Basket","abstract":"Can a robot think like a human being? Scientists in recent years have been trying to achieve this dream, and we are also committed to this same goal. In this paper, we use an example of throwing the ball into the basket to make the robots process with human-like thinking behavior. Such thinking behavior adopted in this paper is divided into two modes: fast and slow. The fast mode belongs to the intuitional reaction, and the slow mode represents the complicated cogitation in human brain. This fascinating human thinking concept is inspired by the book, Thinking, Fast and Slow, which explains the process of the human brain. In addition, the psychology theories proposed in this book are also adopted to realize the thinking algorithms, and our experiments verify that the thinking mode of human beings is reasonable and effective in robots.","keywords_author":["Anchoring effect","fast and slow systems","FIRA","humanoid robot","learning algorithm","peak-end rule","psychology"],"keywords_other":["ARTIFICIAL BEE COLONY","GENETIC ALGORITHM","QUALITY","MACHINE","SYSTEM","SCIENCE","SWARM","MANUFACTURER","ANT COLONY OPTIMIZATION"],"max_cite":1.0,"pub_year":2016.0,"sources":"['ieee', 'wos']","rawkeys":["manufacturer","quality","machine","ant colony optimization","fast and slow systems","artificial bee colony","system","genetic algorithm","science","swarm","peak-end rule","humanoid robot","learning algorithm","psychology","anchoring effect","fira"],"tags":["swarms","quality","recognition","machine","ant colony optimization","fast and slow systems","system","artificial bee colonies","genetic algorithm","manufacturing","science","peak-end rule","humanoid robot","learning algorithm","anchoring effect","fira"]},{"p_id":97037,"title":"Multi-instance multi-label image classification: A neural approach","abstract":"In this paper, a multi-instance multi-label algorithm based on neural networks is proposed for image classification. The proposed algorithm, termed multi-instance multi-label neural network (MIMLNN), consists of two stages of MultiLayer Perceptrons (MLP). For multi-instance multi-label image classification, all the regional features are fed to the first-stage MLP, with one MLP copy processing one image region. After that, the MLP in the second stage incorporates the outputs of the first-stage MLPs to produce the final labels for the input image. The first-stage MLP is expected to model the relationship between regions and labels, while the second-stage MLP aims at capturing the label correlation for classification refinement. Error Back-Propagation (BP) approach is adopted to tune the parameters of MIMLNN. In view of that traditional gradient descent algorithm suffers from long-term dependency problem, a refined BP algorithm named Rprop is extended to effectively train MIMLNN. The experiments are conducted on a synthetic dataset and the Corel dataset. Experimental results demonstrate the superior performance of MIMLNN comparing with state-of-the-art algorithms for multi-instance multi-label image classification. (c) 2012 Elsevier B.V. All rights reserved.","keywords_author":["Multi-instance multi-label learning","Image classification","Neural networks","Synthetic data"],"keywords_other":["MACHINE","ALGORITHM","RECOGNITION","SEGMENTATION","REGIONS","ANNOTATION","NETWORK"],"max_cite":15.0,"pub_year":2013.0,"sources":"['ieee', 'wos']","rawkeys":["algorithm","network","recognition","segmentation","neural networks","machine","regions","multi-instance multi-label learning","synthetic data","annotation","image classification"],"tags":["recognition","segmentation","neural networks","machine","networks","regions","multi-instance multi-label learning","algorithms","synthetic data","annotation","image classification"]},{"p_id":23310,"title":"Towards unsupervised learning for arabic handwritten recognition using deep architectures","abstract":"\u00a9 Springer International Publishing Switzerland 2015.In the pattern recognition field and especially in the Handwriting recognition one, the Deep learning is becoming the new trend in Artificial Intelligence with the sheer size of raw data available nowadays. In this paper, we highlights how Deep Learning techniques can be effectively applied for recognizing Arabic handwritten script, our field of interest, and this by investigating two deep architectures: Deep Belief Network (DBN) and Convolutional Neural Networks (CNN). The two proposed architectures take the raw data as input and proceed with a greedy layer-wise unsupervised learning algorithm. The experimental study has proved promising results which are comparable or even superior to the standard classifiers with an efficiency of DBN over CNN architecture.","keywords_author":["Arabic handwritten script","CNN","DBN","Recognition","Unsupervised learning"],"keywords_other":["Arabic handwritten recognition","Recognition","Arabic handwritten script","CNN","Convolutional neural network","DBN","Handwriting recognition","Deep belief network (DBN)"],"max_cite":11.0,"pub_year":2015.0,"sources":"['scp', 'ieee']","rawkeys":["recognition","deep belief network (dbn)","dbn","cnn","arabic handwritten recognition","unsupervised learning","convolutional neural network","arabic handwritten script","handwriting recognition"],"tags":["recognition","arabic handwritten recognition","unsupervised learning","convolutional neural network","handwriting recognition","arabic handwritten script","deep belief networks"]},{"p_id":23325,"title":"An efficient and robust integrated geospatial object detection framework for high spatial resolution remote sensing imagery","abstract":"\u00a9 2017 by the authors.Geospatial object detection from high spatial resolution (HSR) remote sensing imagery is a significant and challenging problem when further analyzing object-related information for civil and engineering applications. However, the computational efficiency and the separate region generation and localization steps are two big obstacles for the performance improvement of the traditional convolutional neural network (CNN)-based object detection methods. Although recent object detection methods based on CNN can extract features automatically, these methods still separate the feature extraction and detection stages, resulting in high time consumption and low efficiency. As a significant influencing factor, the acquisition of a large quantity of manually annotated samples for HSR remote sensing imagery objects requires expert experience, which is expensive and unreliable. Despite the progress made in natural image object detection fields, the complex object distribution makes it difficult to directly deal with the HSR remote sensing imagery object detection task. To solve the above problems, a highly efficient and robust integrated geospatial object detection framework based on faster region-based convolutional neural network (Faster R-CNN) is proposed in this paper. The proposed method realizes the integrated procedure by sharing features between the region proposal generation stage and the object detection stage. In addition, a pre-training mechanism is utilized to improve the efficiency of the multi-class geospatial object detection by transfer learning from the natural imagery domain to the HSR remote sensing imagery domain. Extensive experiments and comprehensive evaluations on a publicly available 10-class object detection dataset were conducted to evaluate the proposed method.","keywords_author":["Feature sharing","Geospatial object detection","High spatial resolution (HSR) remote sensing imagery","Integration","Pre-training mechanism","geospatial object detection","high spatial resolution (HSR) remote sensing imagery","integration","pre-training mechanism","feature sharing"],"keywords_other":["Comprehensive evaluation","CONVOLUTIONAL NETWORKS","TARGET DETECTION","EXTRACTION","Engineering applications","Pre-training","Feature Sharing","MODEL","RECOGNITION","SEGMENTATION","ROTATION-INVARIANT","Convolutional neural network","Remote sensing imagery","SATELLITE IMAGES","SCENE CLASSIFICATION","High spatial resolution","Geo-spatial objects"],"max_cite":11.0,"pub_year":2017.0,"sources":"['ieee', 'scp', 'wos']","rawkeys":["high spatial resolution","rotation-invariant","scene classification","satellite images","convolutional neural network","extraction","segmentation","convolutional networks","engineering applications","remote sensing imagery","geospatial object detection","integration","recognition","pre-training","high spatial resolution (hsr) remote sensing imagery","feature sharing","model","comprehensive evaluation","geo-spatial objects","pre-training mechanism","target detection"],"tags":["high spatial resolution","scene classification","satellite images","convolutional neural network","extraction","segmentation","rotation invariance","engineering applications","remote-sensing imagery","geospatial object detection","integration","recognition","pre-training","high spatial resolution (hsr) remote sensing imagery","feature sharing","model","comprehensive evaluation","geo-spatial objects","pre-training mechanism","target detection"]},{"p_id":47909,"title":"Exploiting knowledge composition to improve real-life hand prosthetic control","abstract":"\u00a9 2001-2011 IEEE.In myoelectric prosthesis control, one of the hottest topics nowadays is enforcing simultaneous and proportional (s\/p) control over several degrees of freedom. This problem is particularly hard and the scientific community has so far failed to provide a stable and reliable s\/p control, effective in daily-life activities. In order to improve the reliability of this form of control, in this paper we propose on-the-fly knowledge composition, thereby reducing the burden of matching several patterns at the same time, and simplifying the task of the system. In particular, we show that using our method it is possible to dynamically compose a model by juxtaposing subsets of previously gathered (sample, target) pairs in real-time, rather than composing a single model in the beginning and then hoping it can reliably distinguish all patterns. Fourteen intact subjects participated in an experiment, where repetitive daily-life tasks (e.g. ironing a cloth) were performed using a commercially available dexterous prosthetic hand mounted on a splint and wirelessly controlled using a machine learning method. During the experiment, the subjects performed these tasks using myocontrol with and without knowledge composition and the results demonstrate that employing knowledge composition allowed better performance, i.e. reducing the overall task completion time by 30%.","keywords_author":["hand prosthetics","Human-machine interfaces","knowledge composition","machine learning"],"keywords_other":["Sensitivity and Specificity","Hand","Human Machine Interface","Humans","Reproducibility of Results","Machine Learning","Movement","Computer Simulation","Pattern Recognition, Automated","Scientific community","Prosthetic hands","Task completion time","Algorithms","Machine learning methods","Robotics","Male","Young Adult","Models, Biological","Prosthetic controls","Myoelectric prosthesis controls","Artificial Limbs","Daily life activities","Biofeedback, Psychology"],"max_cite":0.0,"pub_year":2017.0,"sources":"['scp']","rawkeys":["automated","movement","computer simulation","scientific community","young adult","daily life activities","prosthetic hands","task completion time","machine learning","models","algorithms","biological","psychology","sensitivity and specificity","artificial limbs","human-machine interfaces","reproducibility of results","humans","myoelectric prosthesis controls","machine learning methods","hand","prosthetic controls","knowledge composition","male","robotics","human machine interface","hand prosthetics","pattern recognition","biofeedback"],"tags":["automated","movement","computer simulation","scientific community","young adult","daily life activities","prosthetic hands","task completion time","machine learning","algorithms","biological","sensitivity and specificity","artificial limbs","recognition","reproducibility of results","humans","myoelectric prosthesis controls","machine learning methods","hand","prosthetic controls","model","knowledge composition","male","robotics","human machine interface","hand prosthetics","pattern recognition","biofeedback"]},{"p_id":113472,"title":"Ship Classification Based on MSHOG Feature and Task-Driven Dictionary Learning with Structured Incoherent Constraints in SAR Images","abstract":"In this paper, we present a novel method for ship classification in synthetic aperture radar (SAR) images. The proposed method consists of feature extraction and classifier training. Inspired by SAR-HOG feature in automatic target recognition, we first design a novel feature named MSHOG by improving SAR-HOG, adapting it to ship classification, and employing manifold learning to achieve dimensionality reduction. Then, we train the classifier and dictionary jointly in task-driven dictionary learning (TDDL) framework. To further improve the performance of TDDL, we enforce structured incoherent constraints on it and develop an efficient algorithm for solving corresponding optimization problem. Extensive experiments performed on two datasets with TerraSAR-X images demonstrate that the proposed method, MSHOG feature and TDDL with structured incoherent constraints, outperforms other existing methods and achieves state-of-art performance.","keywords_author":["ship classification","task-driven dictionary learning","structured incoherent constraints","sparse representation","manifold learning","histogram of oriented gradients (HOG)"],"keywords_other":["TERRASAR-X","REGULARIZATION","MANIFOLDS","COMPONENT ANALYSIS","RECOGNITION","NONLINEAR DIMENSIONALITY REDUCTION","SPARSE REPRESENTATION","EIGENMAPS"],"max_cite":1.0,"pub_year":2018.0,"sources":"['wos']","rawkeys":["task-driven dictionary learning","recognition","eigenmaps","ship classification","sparse representation","histogram of oriented gradients (hog)","structured incoherent constraints","manifolds","manifold learning","component analysis","nonlinear dimensionality reduction","terrasar-x","regularization"],"tags":["task-driven dictionary learning","recognition","eigenmaps","ship classification","sparse representation","histogram of oriented gradients","structured incoherent constraints","manifolds","manifold learning","component analysis","nonlinear dimensionality reduction","terrasar-x","regularization"]},{"p_id":6984,"title":"Narcissism, extraversion and adolescents' self-presentation on Facebook","abstract":"Social Networking Sites (e.g. Facebook), which afford self-presentation, are gaining popularity amongst adolescents. This study examined the relationship of narcissism and extraversion on adolescents' self-presentation in four Facebook profile features (profile picture, status updates, social network size, photo count), as reported by Grade 7-Grade 9 adolescents. After accounting for extraversion, narcissism predicted features presenting self-generated content (profile picture rating, status update frequency), but not features presenting system-generated content (social network size, photo count). \u00a9 2010 Elsevier Ltd.","keywords_author":["Extraversion","Facebook","Narcissism","Self-presentation"],"keywords_other":null,"max_cite":177.0,"pub_year":2011.0,"sources":"['scp']","rawkeys":["self-presentation","facebook","extraversion","narcissism"],"tags":["self-presentation","facebook","recognition","narcissism"]},{"p_id":88907,"title":"Domain adaptation with low-rank alignment for weakly supervised hand pose recovery","abstract":"Human hand pose recovery (HPR) in depth images is usually conducted by constructing mappings between 2D depth images and 3D hand poses. It is a challenging task since the feature spaces of 2D images and 3D poses are different. Therefore, a large number of labeled data is required for training, especially for popular frameworks such as deep learning. In this paper, we propose an HPR method with weak supervision. It is based on neural network and domain adaptation is introduced to enhance the trained model. To achieve domain adaptation, we propose low-rank alignment, which aligns the testing samples to the distribution of labeled samples. In this process, autoencoders are used to extract 2D image features and low-rank representation is used to describe this feature space. Therefore, the proposed method is named as Domain Adaptation with Low-Rank Alignment (DALA). In this way, we obtain a robust and non-linear mapping from 2D images to 3D poses. Experiments are conducted on two challenging benchmark datasets MSRA and ICVL. Both the results on a single dataset and across datasets show the outstanding performance of DALA. (C) 2017 Elsevier B.V. All rights reserved.","keywords_author":["Human hand pose recovery","Neural network","Domain adaptation","Low-rank representation"],"keywords_other":["REGRESSION","IMAGE CLASSIFICATION","3D HUMAN POSE","RECOGNITION","DEPTH","TRACKING"],"max_cite":0.0,"pub_year":2018.0,"sources":"['ieee', 'wos']","rawkeys":["neural network","recognition","tracking","depth","low-rank representation","3d human pose","domain adaptation","regression","image classification","human hand pose recovery"],"tags":["recognition","denoising autoencoder","neural networks","tracking","depth","low-rank representation","3d human pose","regression","image classification","human hand pose recovery"]},{"p_id":64346,"title":"Healthy human sitting posture estimation in RGB-D scenes using object context","abstract":"Unhealthy sitting posture leads to cervical spondylosis and other related cumulative trauma disorders (CTDs). Unfortunately, the research on the investigation of heathy sitting posture is rare. The current research is to estimate heathy sitting posture based on a computer workstation ergonomics perspective. A novel RGB-D scene healthy human sitting posture estimation framework was developed to estimate the sitting posture, in which a human posture is represented by 15 skeletal joints. A healthy human sitting posture configuration is defined from the view of ergonomics, a Na < ve Bayes classifier was used to learn the health-constrained spatial and context relationships between objects and the human skeletal joints in the RGB-D scene. At the estimation stage, the object spatial features (e.g., coordinate, distance, height and angle) in the RGB-D scene were obtained through conducting the scene labeling. 15 human skeletal joints were extracted simultaneously from Kinect as primary inputs, and then algorithms were developed to generate and to classify the candidate healthy skeleton joints. Through skeleton refinement, the skeleton joints distribution of a healthy sitting posture was produced. The framework was tested on a dataset comprised of RGB-D scenes, which were collected from 3 subjects (3 types of sitting postures, each in 3 different offices). The experiment results indicate that the framework is feasible and reliable.","keywords_author":["Sitting posture estimation","Health","RGB-D scene","Ergonomics"],"keywords_other":["IMAGE","SENSOR","INTERVENTION","WORKPLACE","WORKSTATION DESIGN","MODEL","RECOGNITION","ERGONOMICS"],"max_cite":0.0,"pub_year":2017.0,"sources":"['ieee', 'wos']","rawkeys":["workplace","recognition","model","workstation design","sitting posture estimation","intervention","sensor","image","rgb-d scene","health","ergonomics"],"tags":["workplace","recognition","images","model","sensors","sitting posture estimation","workstation design","intervention","rgb-d scene","health","ergonomics"]},{"p_id":64350,"title":"Monocular scene flow estimation via variational method","abstract":"Scene flow provides the 3D motion field of point clouds, which correspond to image pixels. Current algorithms usually need complex stereo calibration before estimating flow, which has strong restrictions on the position of the camera. This paper proposes a monocular camera scene flow estimation algorithm. Firstly, an energy functional is constructed, where three important assumptions are turned into data terms derivation: a brightness constancy assumption, a gradient constancy assumption, and a short time object velocity constancy assumption. Two smooth operators are used as regularization terms. Then, an occluded map computation algorithm is used to ensure estimating scene flow only on un-occluded points. After that, the energy functional is solved with a coarse-to-fine variational equation on Gaussian pyramid, which can prevent the iteration from converging to a local minimum value. The experiment results show that the algorithm can use three sequential frames at least to get scene flow in world coordinate, without optical flow or disparity inputting.","keywords_author":["Scene flow","Monocular camera","Time-space consistency","Energy functional","Nonlinear iteration","Intelligent healthy drivings"],"keywords_other":["ENVIRONMENTS","RECOGNITION","SLAM"],"max_cite":1.0,"pub_year":2017.0,"sources":"['ieee', 'wos']","rawkeys":["time-space consistency","recognition","scene flow","nonlinear iteration","intelligent healthy drivings","slam","environments","monocular camera","energy functional"],"tags":["time-space consistency","recognition","monocular cameras","scene flow","intelligent healthy drivings","nonlinear iteration","energy functions","robotics","environment"]},{"p_id":64352,"title":"Topic categorization and representation of health community generated data","abstract":"The representation and categorization of professional health provider released data have been well investigated and practically implemented. These have facilitated browsing, search and high-order learning of health information. On the other hand, there has been little corresponding studies on the representation and categorization of health community generated data. It is usually more complex, inconsistent and ambiguous, and consequently raises challenges for data access and analytics. This paper explores various representations for health community generated data and categorizes these data in terms of health topics. In addition, this work utilizes pseudo-labeled data to train the supervised topic categorization models, and this makes the whole categorization process unsupervised and extendable to handle large-scale data. The extensive experiments on two real-world datasets reveal our interesting findings of the informative representation approaches and effective categorization models for health community generated data.","keywords_author":["Health community generated data","Learning model","Semantic representation","Health topic categorization"],"keywords_other":["MEDLINE","IMAGE","CLASSIFICATION","RECOGNITION","VOCABULARY"],"max_cite":1.0,"pub_year":2017.0,"sources":"['ieee', 'wos']","rawkeys":["medline","health topic categorization","recognition","vocabulary","learning model","semantic representation","image","classification","health community generated data"],"tags":["medline","learning models","health topic categorization","recognition","images","vocabulary","semantic representation","classification","health community generated data"]},{"p_id":64357,"title":"E-health monitoring system enhancement with Gaussian mixture model","abstract":"In order to enhance the healthcare system, we have designed and developed a system prototype which remotely monitors patient's vital parameters by using mobile based android application. Proposed E-health care system collects patient's biological and personal information with the corresponding vital parameters and stores this Meta data information into the health care database servers. The distributed servers are connected with GSP system. So the extracted information from the server is directly feed to the doctor's mobile device as well as to the patient's mobile devices in a presentable format. This system also uses Frontline SMS as an SMS service which is used to send SMS to the doctor's mobile device automatically, when any one of the patient's vital parameter goes out of normal range. In this paper, we present the GMM (Gaussian mixture model) based on extracted features of the patient information and assign it to the specialized doctor. In this work, we have shown that by GMM based algorithm efficiently balances the patient load to the doctor. This novel approach enhances the E-health monitoring system for normal situations as well as in the case of Natural disaster. The proposed load balancing approach gives relief to the patient for unnecessary long delay to receive medical advice. The presented result in this work shown that, the doctors from all category and specialization are loaded rationally and uniformly. According to our knowledge GMM based approach is the new additional component to enhance the E-health care system.","keywords_author":["Android","Database server","Frontline SMS","GPS","GMM","Healthcare system","Vital parameters"],"keywords_other":["RECOGNITION","CARE"],"max_cite":1.0,"pub_year":2017.0,"sources":"['ieee', 'wos']","rawkeys":["database server","recognition","healthcare system","android","gmm","care","frontline sms","gps","vital parameters"],"tags":["database server","recognition","neural networks","healthcare system","care","frontline sms","gaussian processes","vital parameters","gaussian mixture model"]},{"p_id":23402,"title":"Localization based stereo speech source separation using probabilistic time-frequency masking and deep neural networks","abstract":"\u00a9 2016, Yu et al.Time-frequency (T-F) masking is an effective method for stereo speech source separation. However, reliable estimation of the T-F mask from sound mixtures is a challenging task, especially when room reverberations are present in the mixtures. In this paper, we propose a new stereo speech separation system where deep neural networks are used to generate soft T-F mask for separation. More specifically, the deep neural network, which is composed of two sparse autoencoders and a softmax regression, is used to estimate the orientations of the dominant source at each T-F unit, based on low-level features, such as mixing vector (MV), interaural level, and phase difference (IPD\/ILD). The dataset for training the networks was generated by the convolution of binaural room impulse responses (RIRs) and clean speech signals positioned in different angles with respect to the sensors. With the training dataset, we use unsupervised learning to extract high-level features from low-level features and use supervised learning to find the nonlinear functions between high-level features and the orientations of dominant source. By using the trained networks, the probability that each T-F unit belongs to different sources (target and interferers) can be estimated based on the localization cues which is further used to generate the soft mask for source separation. Experiments based on real binaural RIRs and TIMIT dataset are provided to show the performance of the proposed system for reverberant speech mixtures, as compared with a model-based T-F masking technique proposed recently.","keywords_author":["Deep learning","Deep neural networks","Soft mask","Source separation","Deep learning","Deep neural networks","Source separation","Soft mask"],"keywords_other":["Deep learning","BLIND SOURCE SEPARATION","Time-Frequency Masking","Room reverberations","RECOGNITION","Softmax regressions","SEGREGATION","Time-frequency masking (T-F)","Binaural room impulse response","Nonlinear functions","Deep neural networks"],"max_cite":37.0,"pub_year":2016.0,"sources":"['ieee', 'scp', 'wos']","rawkeys":["soft mask","blind source separation","recognition","source separation","deep learning","deep neural networks","room reverberations","time-frequency masking","binaural room impulse response","nonlinear functions","softmax regressions","segregation","time-frequency masking (t-f)"],"tags":["soft mask","blind source separation","recognition","source separation","time-frequency masking","sparse representation","binaural room impulse response","machine learning","nonlinear functions","convolutional neural network","segregation","room reverberation"]},{"p_id":56183,"title":"Connecting with learning: Motivation, affect and cognition in interest processes","abstract":"In this paper we draw on our research on interest to explore the questions posed for this special issue. Interest is conceptualized as an affective state that represents students' subjective experience of learning; the state that arises from either situational triggers or a well-developed individual interest. Drawing on the broad research literature on interest, and using our own findings in relation to the state of interest, we consider how interest represents an integration of affect, motivation and cognition. In particular, how the state of interest brings together motivation in the form of prior goals and interests and focuses them into on-task behavior. We illustrate ways that our research monitoring on-task sequences of affect and behavior, is confronting some of the methodological concerns posed in relation to measurement of affective states. Finally, we examine some of the paths by which triggered states of interest can contribute to productive student engagement with learning.","keywords_author":["interest","engagement","on-task measures"],"keywords_other":["EMOTIONS","STUDENTS","VALIDATION","SITUATIONAL INTEREST","ANALOG MOOD SCALES","INQUIRY","ACHIEVEMENT GOAL THEORY","PERSONALITY","PSYCHOLOGY","CLASSROOM"],"max_cite":117.0,"pub_year":2006.0,"sources":"['wos']","rawkeys":["students","emotions","interest","on-task measures","situational interest","engagement","inquiry","validation","achievement goal theory","personality","analog mood scales","classroom","psychology"],"tags":["students","recognition","emotion","interest","on-task measures","situational interest","engagement","inquiry","validation","achievement goal theory","analog mood scales","classroom","personalizations"]},{"p_id":88965,"title":"Recognition of Cursive Arabic Handwritten Text Using Embedded Training Based on Hidden Markov Models","abstract":"This paper presents a system for offline recognition of cursive Arabic handwritten text based on Hidden Markov Models (HMMs). The proposed work reports an effective method taking into account the context of character by applying an embedded training-based HMMs to perform and enhance the character models. The system is analytical without explicit segmentation; extracted features preceded by baseline estimation are statistical and structural to integrate both the peculiarities of the text and the pixel distribution characteristics of the word image. The experiments are done on benchmark IFN\/ENIT database. The proposed work shows the effectiveness of using embedded training-based HMMs for enhancing the recognition rate, and the obtained results are promising and encouraging.","keywords_author":["Recognition","handwriting","Arabic text","HMMs","embedded training"],"keywords_other":["ATTRIBUTES","CHARACTER"],"max_cite":0.0,"pub_year":2018.0,"sources":"['wos']","rawkeys":["handwriting","recognition","hmms","attributes","character","arabic text","embedded training"],"tags":["hidden markov models","handwriting","recognition","attributes","character","arabic texts","embedded training"]},{"p_id":7053,"title":"Reading Text in the Wild with Convolutional Neural Networks","abstract":"\u00a9 2015, Springer Science+Business Media New York.In this work we present an end-to-end system for text spotting\u2014localising and recognising text in natural scene images\u2014and text based image retrieval. This system is based on a region proposal mechanism for detection and deep convolutional neural networks for recognition. Our pipeline uses a novel combination of complementary proposal generation techniques to ensure high recall, and a fast subsequent filtering stage for improving precision. For the recognition and ranking of proposals, we train very large convolutional neural networks to perform word recognition on the whole proposal region at the same time, departing from the character classifier based systems of the past. These networks are trained solely on data produced by a synthetic text generation engine, requiring no human labelled data. Analysing the stages of our pipeline, we show state-of-the-art performance throughout. We perform rigorous experiments across a number of standard end-to-end text spotting benchmarks and text-based image retrieval datasets, showing a large improvement over all previous methods. Finally, we demonstrate a real-world application of our text spotting system to allow thousands of hours of news footage to be instantly searchable via a text query.","keywords_author":["Convolutional neural networks","Deep learning","Synthetic data","Text detection","Text recognition","Text retrieval","Text spotting","Text spotting","Text recognition","Text detection","Deep learning","Convolutional neural networks","Synthetic data","Text retrieval"],"keywords_other":["Deep learning","Text detection","Text spotting","WORLD","RECOGNITION","IMAGES","Text recognition","Convolutional neural network","Synthetic data","Text retrieval","SCENE TEXT"],"max_cite":171.0,"pub_year":2016.0,"sources":"['ieee', 'scp', 'wos']","rawkeys":["text retrieval","recognition","convolutional neural networks","images","deep learning","text detection","world","text spotting","text recognition","convolutional neural network","scene text","synthetic data"],"tags":["text retrieval","recognition","images","text detection","machine learning","world","text spotting","text recognition","convolutional neural network","scene text","synthetic data"]},{"p_id":7056,"title":"Pareto-based multiobjective machine learning: An overview and case studies","abstract":"Machine learning is inherently a multiobjective task. Traditionally, however, either only one of the objectives is adopted as the cost function or multiple objectives are aggregated to a scalar cost function. This can be mainly attributed to the fact that most conventional learning algorithms can only deal with a scalar cost function. Over the last decade, efforts on solving machine learning problems using the Pareto-based multiobjective optimization methodology have gained increasing impetus, particularly due to the great success of multiobjective optimization using evolutionary algorithms and other population-based stochastic search methods. It has been shown that Pareto-based multiobjective learning approaches are more powerful compared to learning algorithms with a scalar cost function in addressing various topics of machine learning, such as clustering, feature selection, improvement of generalization ability, knowledge extraction, and ensemble generation. One common benefit of the different multiobjective learning approaches is that a deeper insight into the learning problem can be gained by analyzing the Pareto front composed of multiple Pareto-optimal solutions. This paper provides an overview of the existing research on multiobjective machine learning, focusing on supervised learning. In addition, a number of case studies are provided to illustrate the major benefits of the Pareto-based approach to machine learning, e.g., how to identify interpretable models and models that can generalize on unseen data from the obtained Pareto-optimal solutions. Three approaches to Pareto-based multiobjective ensemble generation are compared and discussed in detail. Finally, potentially interesting topics in multiobjective machine learning are suggested. \u00a9 2008 IEEE.","keywords_author":["Ensemble","Evolutionary multiobjective optimization","Generalization","Machine learning","Multiobjective learning","Multiobjective optimization","Neural networks","Pareto optimization"],"keywords_other":["Population-based stochastic search methods","Pareto-optimal solutions"],"max_cite":171.0,"pub_year":2008.0,"sources":"['scp']","rawkeys":["multiobjective optimization","neural networks","multiobjective learning","population-based stochastic search methods","machine learning","ensemble","generalization","pareto optimization","pareto-optimal solutions","evolutionary multiobjective optimization"],"tags":["recognition","pareto-optimal","neural networks","population-based stochastic search methods","machine learning","multi-objective learning","ensemble","pareto optimal solutions","evolutionary multiobjective optimization","multi-objective optimization"]},{"p_id":7062,"title":"Transferring deep convolutional neural networks for the scene classification of high-resolution remote sensing imagery","abstract":"\u00a9 2015 by the authors.Learning efficient image representations is at the core of the scene classification task of remote sensing imagery. The existing methods for solving the scene classification task, based on either feature coding approaches with low-level hand-engineered features or unsupervised feature learning, can only generate mid-level image features with limited representative ability, which essentially prevents them from achieving better performance. Recently, the deep convolutional neural networks (CNNs), which are hierarchical architectures trained on large-scale datasets, have shown astounding performance in object recognition and detection. However, it is still not clear how to use these deep convolutional neural networks for high-resolution remote sensing (HRRS) scene classification. In this paper, we investigate how to transfer features from these successfully pre-trained CNNs for HRRS scene classification. We propose two scenarios for generating image features via extracting CNN features from different layers. In the first scenario, the activation vectors extracted from fully-connected layers are regarded as the final image features; in the second scenario, we extract dense features from the last convolutional layer at multiple scales and then encode the dense features into global image features through commonly used feature coding approaches. Extensive experiments on two public scene classification datasets demonstrate that the image features obtained by the two proposed scenarios, even with a simple linear classifier, can result in remarkable performance and improve the state-of-the-art by a significant margin. The results reveal that the features from pre-trained CNNs generalize well to HRRS datasets and are more expressive than the low- and mid-level features. Moreover, we tentatively combine features extracted from different CNN models for better performance.","keywords_author":["CNN","Convolutional layer","Feature coding","Feature representation","Fully-connected layer","Scene classification","CNN","scene classification","feature representation","feature coding","convolutional layer","fully-connected layer"],"keywords_other":["FEATURES","Feature coding","Fully-connected layer","CNN","MODEL","Feature representation","RECOGNITION","Scene classification","SCALE","PATTERNS","Convolutional layer"],"max_cite":171.0,"pub_year":2015.0,"sources":"['wos', 'scp', 'ieee']","rawkeys":["convolutional layer","recognition","model","features","scene classification","patterns","cnn","fully-connected layer","feature coding","feature representation","scale"],"tags":["recognition","model","features","convolutional layers","scene classification","patterns","convolutional neural network","fully-connected layers","feature coding","feature representation","scale"]},{"p_id":56219,"title":"Facilitating the comparison of multiple visual items on screen: The example of electronic architectural plan correction","abstract":"This paper describes two experiments designed to (1) ascertain whether the way in which architectural plans are displayed on a computer screen influences the quality of their correction by humans, and (2) identify the visual exploration strategies adopted in this type of task. Results of the first \"spot the difference\" experiment showed that superimposing the plans yielded better error correction performances than displaying them side by side. Furthermore, a sequential display mode, where the second plan only gradually appeared on the screen, improved error search effectiveness. In the second experiment, eye movement recordings revealed that superimposition increased plan comparison efficiency by making it easier to establish coreference between the two sources of information. The improvement in effectiveness in the sequential condition was shown to be linked to the attentional guidance afforded by this display mode, which helped users to make a more thorough exploration of the plans. (C) 2013 Elsevier Ltd and The Ergonomics Society. All rights reserved.","keywords_author":["Plan correction","Spatial integration","Attentional guidance"],"keywords_other":["OCULOMOTOR CAPTURE","ATTENTION","LOAD","RECOGNITION","SPATIAL CONTIGUITY","MOTION","SKETCH","EYE-MOVEMENTS"],"max_cite":2.0,"pub_year":2014.0,"sources":"['wos']","rawkeys":["recognition","spatial contiguity","load","motion","spatial integration","attentional guidance","plan correction","attention","sketch","eye-movements","oculomotor capture"],"tags":["recognition","spatial contiguity","load","motion","spatial integration","attentional guidance","plan correction","attention","sketch","eye movements","oculomotor capture"]},{"p_id":113573,"title":"Texture and art with deep neural networks","abstract":"Although the study of biological vision and computer vision attempt to understand powerful visual information processing from different angles, they have a long history of informing each other. Recent advances in texture synthesis that were motivated by visual neuroscience have led to a substantial advance in image synthesis and manipulation in computer vision using convolutional neural networks (CNNs). Here, we review these recent advances and discuss how they can in turn inspire new research in visual perception and computational neuroscience.","keywords_author":null,"keywords_other":["VISUAL TEXTURES","MODEL","RECOGNITION","CORTEX","STATISTICS"],"max_cite":0.0,"pub_year":2017.0,"sources":"['wos']","rawkeys":["statistics","recognition","model","cortex","visual textures"],"tags":["statistics","recognition","model","visual texture","cortex"]},{"p_id":56233,"title":"THE TIME COURSE OF INFORMATION EXTRACTION FROM INSTRUCTIONAL DIAGRAMS","abstract":"This study investigated which information is extracted from a brief glance at an instructional diagram to assess its possible contribution for learning with text and diagrams. An experimental paradigm from scene perception research was used to study diagrams. University students (N = 20) saw pictures showing a scene or instructional diagrams for four different presentation times (50 msec. vs 250 msec. vs 1,000 msec. vs 3,000 msec.). Following presentation of a picture or diagram, respectively, participants were asked to verify a statement about its gist, details, and the functioning (for diagrams only). Repeated-measures analyses of variance (ANOVAs) were used to analyze verification accuracy for statements about gist, details, and the functioning as well as the eye movements (i.e., fixation durations and saccade amplitudes) during picture inspection. In both scenes and instructional diagrams, gist but not details were accurately identified from a first glance at the picture (i.e., at 50 msec. and 250 msec.). In contrast, verification accuracy for gist and details increased at a slower rate in instructional diagrams than in scene pictures over presentation times. Moreover, the characteristic function of increasing fixation durations with increasing inspection time was found in scenes, but not in instructional diagrams. Taken together, results suggest that both types of illustrations are processed differently at longer inspection times; however, patterns of early information extraction are similar, namely that the gist but far less information about details is extracted. Results imply people are able to extract an instructional diagram's global spatial structure from a first glance, which may be helpful to learning from text.","keywords_author":null,"keywords_other":["SCENE PERCEPTION","NATURAL SCENES","VISUAL-SEARCH","FEATURES","GIST","REPRESENTATIONS","RECOGNITION","INATTENTIONAL BLINDNESS","PICTURES","EYE-MOVEMENTS"],"max_cite":10.0,"pub_year":2012.0,"sources":"['wos']","rawkeys":["natural scenes","gist","recognition","features","pictures","representations","scene perception","inattentional blindness","eye-movements","visual-search"],"tags":["natural scenes","gist","recognition","features","pictures","visual search","representation","scene perception","inattentional blindness","eye movements"]},{"p_id":31664,"title":"Generic Feature Learning in Computer Vision","abstract":"\u00a9 2015 The Authors.Current Machine learning algorithms are highly dependent on manually designing features and the Performance of such algo- rithms predominantly depend on how good our representations are. Manually we might never be able to produce best and diverse set of features that closely describe all the variations that occur in our data. Understanding this, vision community is moving towards learning the optimum features itself instead of learning from the features. Traditional hand engineered features lack in generalizing well to other domains\/Problems, are time consuming, expensive, requires expert knowledge on the problem domain and doesn't facilitate learning from previous learnings\/Representations(Transfer learning). All these issues are resolved in learning deep representations. Since 2006 a wide range of representation learning algorithms has been proposed but by the recent success and breakthroughs of few deep learning models, the representation learning algorithms have gained the spotlight. This paper aims to give short overview of deep learning approaches available for vision tasks. We also discuss their applicability (With respect to their properties) in vision field.","keywords_author":["Autoencoders","Convolution Neural networks","Deep learning","Feature learning","Generic features","RBM","Recognition","Supervised learning","Unsupervised learning","Vision"],"keywords_other":null,"max_cite":3.0,"pub_year":2015.0,"sources":"['scp']","rawkeys":["generic features","supervised learning","recognition","rbm","deep learning","vision","convolution neural networks","autoencoders","unsupervised learning","feature learning"],"tags":["generic features","supervised learning","recognition","auto encoders","machine learning","vision","unsupervised learning","convolutional neural network","feature learning","restricted boltzmann machine"]},{"p_id":39856,"title":"Autonomous surveillance tolerant to interference","abstract":"Autonomous recognition of human activities from video streams is an important aspect of surveillance. A key challenge is to learn an appropriate representation or model of each activity. This paper presents a novel solution for recognizing a set of predefined actions in video streams of variable durations, even in the presence of interference, such as noise and gaps caused by occlusions or intermittent data loss. The most significant contribution of this solution is learning the number of states required to represent an action, in a short period of time, without exhaustive testing of all state spaces. It works by using Surprise-Based Learning (SBL) to reason on data (object tracks) provided by a vision module. SBL autonomously learns a set of rules which capture the essential information required to disambiguate each action. These rules are then grouped together to form states and a corresponding Markov chain which can detect actions with varying time duration. Several experiments on the publicly available visint.org video corpora have yielded favorable results. \u00a9 2012 Springer-Verlag.","keywords_author":["Development Learning","Gap Filling","Machine Learning","Predictive Modeling","Recognition","Temporal and Sequential Learning"],"keywords_other":["Sequential learning","Recognition","Predictive modeling","Development Learning","Gap filling"],"max_cite":1.0,"pub_year":2012.0,"sources":"['scp', 'ieee']","rawkeys":["sequential learning","recognition","predictive modeling","machine learning","gap filling","temporal and sequential learning","development learning"],"tags":["sequential learning","recognition","predictive models","machine learning","gap filling","temporal and sequential learning","development learning"]},{"p_id":113606,"title":"Convex Multiview Semi-Supervised Classification","abstract":"In many practical applications, there are a great number of unlabeled samples available, while labeling them is a costly and tedious process. Therefore, how to utilize unlabeled samples to assist digging out potential information about the problem is very important. In this paper, we study a multiclass semi-supervised classification task in the context of multiview data. First, an optimization method named Parametric multiview semi-supervised classification (PMSSC) is proposed, where the built classifier for each individual view is explicitly combined with a weight factor. By analyzing the weakness of it, a new adapted weight learning strategy is further formulated, and we come to the convex multiview semi-supervised classification (CMSSC) method. Comparing with the PMSSC, this method has two significant properties. First, without too much loss in performance, the newly used weight learning technique achieves eliminating a hyperparameter, and thus it becomes more compact in form and practical to use. Second, as its name implies, the CMSSC models a convex problem, which avoids the local-minimum problem. Experimental results on several multiview data sets demonstrate that the proposed methods achieve better performances than recent representative methods and the CMSSC is preferred due to its good traits.","keywords_author":["Multiview data","semi-supervised classification","weight learning"],"keywords_other":["INTEGRATION","IMAGE CLASSIFICATION","RECOGNITION"],"max_cite":0.0,"pub_year":2017.0,"sources":"['ieee', 'wos']","rawkeys":["integration","recognition","weight learning","semi-supervised classification","multiview data","image classification"],"tags":["integration","recognition","weighted learning","semi-supervised classification","multiview data","image classification"]},{"p_id":113610,"title":"Driver drowsiness detection using facial dynamic fusion information and a DBN","abstract":"Driver drowsiness is a frequent cause of traffic accidents. Research on driver drowsiness detection methods is important to improve road traffic safety. Previous driving fatigue detection methods frequently extracted single features such as eye or mouth changes and trained shallow classifiers, which limit the generalisation capability of these methods. This study proposes a framework for recognising driver drowsiness expression by using facial dynamic fusion information and a deep belief network (DBN) to address the aforementioned problem. First, the landmarks and textures of the facial region are extracted from videos captured using a high-definition camera. Then, a DBN is built to classify facial drowsiness expressions. Finally, the authors' method is tested on a driver drowsiness dataset, which includes different genders, ages, head poses and illuminations. Certain experiments are also carried out to investigate the effects of different facial subregions and temporal resolutions on the accuracy of driver fatigue recognition. Results demonstrate the validity of the proposed method, which has an average accuracy of 96.7%.","keywords_author":["driver information systems","road traffic","road safety","feature extraction","emotion recognition","face recognition","object detection","cameras","video signal processing","belief networks","image fusion","driver drowsiness detection methods","facial dynamic fusion information","DBN","deep belief network","traffic accidents","road traffic safety improvement","driver drowsiness expression recognition","landmark extraction","texture extraction","high-definition camera","facial drowsiness expression classification","driver drowsiness dataset","genders","ages","head poses","illuminations","facial subregions","temporal resolutions","driver fatigue recognition"],"keywords_other":["FEATURES","FATIGUE EXPRESSIONS","CLASSIFICATION","SYSTEM","RANDOM SUBSPACE ENSEMBLES","RECOGNITION","GRADIENT","MACHINES"],"max_cite":0.0,"pub_year":2018.0,"sources":"['ieee', 'wos']","rawkeys":["ages","driver drowsiness detection methods","high-definition camera","driver information systems","genders","driver fatigue recognition","temporal resolutions","gradient","classification","traffic accidents","illuminations","facial subregions","facial drowsiness expression classification","head poses","machines","features","dbn","facial dynamic fusion information","landmark extraction","system","driver drowsiness dataset","cameras","driver drowsiness expression recognition","face recognition","deep belief network","object detection","random subspace ensembles","fatigue expressions","texture extraction","recognition","image fusion","video signal processing","road traffic","road safety","belief networks","road traffic safety improvement","emotion recognition","feature extraction"],"tags":["driver drowsiness detection methods","high-definition camera","driver information systems","gender","driver fatigue recognition","aged","gradient","classification","traffic accidents","facial subregions","facial drowsiness expression classification","features","landmark extraction","facial dynamic fusion information","machine","system","driver drowsiness dataset","cameras","driver drowsiness expression recognition","face recognition","object detection","random subspace ensembles","fatigue expressions","texture extraction","recognition","image fusion","video signal processing","road traffic","road safety","belief networks","head pose","illumination","road traffic safety improvement","emotion recognition","feature extraction","deep belief networks","temporal resolution"]},{"p_id":113611,"title":"Soli: Ubiquitous Gesture Sensing with Millimeter Wave Radar","abstract":"This paper presents Soli, a new, robust, high-resolution, low-power, miniature gesture sensing technology for human-computer interaction based on millimeter-wave radar. We describe a new approach to developing a radar-based sensor optimized for human-computer interaction, building the sensor architecture from the ground up with the inclusion of radar design principles, high temporal resolution gesture tracking, a hardware abstraction layer (HAL), a solid-state radar chip and system architecture, interaction models and gesture vocabularies, and gesture recognition. We demonstrate that Soli can be used for robust gesture recognition and can track gestures with sub-millimeter accuracy, running at over 10,000 frames per second on embedded hardware.","keywords_author":["sensors","interaction","gestures","RF","radar"],"keywords_other":["HUMAN GAIT","ACCURACY","WIRELESS COMMUNICATIONS","PHASE","RECOGNITION","IMPULSE RADAR","60 GHZ BAND","SENSORS"],"max_cite":17.0,"pub_year":2016.0,"sources":"['wos']","rawkeys":["accuracy","interaction","recognition","60 ghz band","sensors","impulse radar","human gait","gestures","phase","wireless communications","rf","radar"],"tags":["accuracy","recognition","60 ghz band","sensors","interactivity","impulse radar","random forests","human gait","gestures","phase","wireless communications","radar"]},{"p_id":113622,"title":"On-Line Detection and Segmentation of Sports Motions Using a Wearable Sensor","abstract":"In sports motion analysis, observation is a prerequisite for understanding the quality of motions. This paper introduces a novel approach to detect and segment sports motions using a wearable sensor for supporting systematic observation. The main goal is, for convenient analysis, to automatically provide motion data, which are temporally classified according to the phase definition. For explicit segmentation, a motion model is defined as a sequence of sub-motions with boundary states. A sequence classifier based on deep neural networks is designed to detect sports motions from continuous sensor inputs. The evaluation on two types of motions (soccer kicking and two-handed ball throwing) verifies that the proposed method is successful for the accurate detection and segmentation of sports motions. By developing a sports motion analysis system using the motion model and the sequence classifier, we show that the proposed method is useful for observation of sports motions by automatically providing relevant motion data for analysis.","keywords_author":["sports motion","detection","segmentation","wearable sensor","deep neural networks"],"keywords_other":["NEURAL-NETWORKS","RECOGNITION","TIME"],"max_cite":0.0,"pub_year":2018.0,"sources":"['ieee', 'wos']","rawkeys":["neural-networks","wearable sensor","recognition","segmentation","deep neural networks","time","detection","sports motion"],"tags":["recognition","wearable sensors","segmentation","neural networks","time","detection","convolutional neural network","sports motion"]},{"p_id":113623,"title":"Semantic Feature Mining for Video Event Understanding","abstract":"Content-based video understanding is extremely difficult due to the semantic gap between low-level vision signals and the various semantic concepts (object, action, and scene) in videos. Though feature extraction from videos has achieved significant progress, most of the previous methods rely only on low-level features, such as the appearance and motion features. Recently, visual-feature extraction has been improved significantly with machine-learning algorithms, especially deep learning. However, there is still not enough work focusing on extracting semantic features from videos directly. The goal of this article is to adopt unlabeled videos with the help of text descriptions to learn an embedding function, which can be used to extract more effective semantic features from videos when only a few labeled samples are available for video recognition. To achieve this goal, we propose a novel embedding convolutional neural network (ECNN). We evaluate our algorithm by comparing its performance on three challenging benchmarks with several popular state-of-the-art methods. Extensive experimental results show that the proposed ECNN consistently and significantly outperforms the existing methods.","keywords_author":["Video recognition","event"],"keywords_other":["RECOGNITION","IMAGES"],"max_cite":2.0,"pub_year":2016.0,"sources":"['ieee', 'wos']","rawkeys":["video recognition","recognition","images","event"],"tags":["events","video recognition","recognition","images"]},{"p_id":80867,"title":"Predicting scientific imagination from the joint influences of intrinsic motivation, self-efficacy, agreeableness, and extraversion","abstract":"Previous research has indicated that intrinsic motivation and self-efficacy influence the human imagination, and the personality traits of agreeableness and extraversion are significant predictors of students' attitudes toward science. The purpose of this study was to analyze the impacts of intrinsic motivation, self-efficacy, agreeableness, and extraversion on the imagination of science majors. The moderating roles of agreeableness and extraversion were also examined, and the mediating influence of self-efficacy was subsequently tested. A total of 402 science majors from 6 universities across Taiwan participated in the study. Structural equation modeling was used to test all the proposed hypotheses. The results showed that self-efficacy, agreeableness, and extraversion directly affected student imagination. Both intrinsic motivation and agreeableness indirectly affected student imagination through self-efficacy. In addition, both agreeableness and extraversion moderated the influence of intrinsic motivation on the self-efficacy of science majors, and these moderating effects continually influenced student imagination through self-efficacy. (C) 2013 Elsevier Inc. All rights reserved.","keywords_author":["Agreeableness","Extraversion","Intrinsic motivation","Scientific imagination","Self-efficacy"],"keywords_other":["MEDIATOR","STUDENTS","VALIDATION","PERSONALITY-TRAITS","CREATIVITY","PERFORMANCE","SCIENCE-EDUCATION","PHYSICS","INTERESTS","ACHIEVEMENT"],"max_cite":3.0,"pub_year":2014.0,"sources":"['wos']","rawkeys":["intrinsic motivation","students","physics","performance","mediator","personality-traits","science-education","agreeableness","scientific imagination","creativity","interests","validation","achievement","extraversion","self-efficacy"],"tags":["intrinsic motivation","students","recognition","performance","physics","interest","scientific imagination","agreeableness","creativity","validation","science education","achievement","self-efficacy","personality traits","mediation"]},{"p_id":80879,"title":"Transfer of Life Skills in Sport-Based Youth Development Programs: A Conceptual Framework Bridging Learning to Application","abstract":"Research has demonstrated that many quality sport-based youth development programs promote life skill acquisition (e.g., leadership, self-control) with the ultimate goal of facilitating positive outcomes in youth participants' social and academic environments. Researchers call this \"transfer of life skills\" (i.e., the idea that physical, behavioral, and cognitive skills youth learn in sport can be applied in non-sport settings to promote healthy development). However, research surrounding this topic has been mixed, as many studies found evidence of transfer. In this article, a variety of learning theories were integrated to propose a conceptual framework for the transfer of life skills in sport-based youth development. Specifically, this article focuses on how research overlooked the cognitive processes that bridge student learning within a sport program to application outside of the program. A description of the cognitive components youth experience during transfer are described and represented in a logic model.","keywords_author":["Transfer","life skills","sport-based youth development","transformative learning"],"keywords_other":["VALUES","PHYSICAL-ACTIVITY","STUDENTS","IMPLEMENTATION","MODEL","CONTEXT","IMPACT","HIGH-SCHOOL COACHES","SOCIAL-RESPONSIBILITY","TRANSFORMATIVE EXPERIENCE"],"max_cite":0.0,"pub_year":2018.0,"sources":"['wos']","rawkeys":["social-responsibility","transfer","sport-based youth development","high-school coaches","students","model","values","transformative experience","physical-activity","transformative learning","implementation","context","life skills","impact"],"tags":["social-responsibility","sport-based youth development","high-school coaches","students","model","recognition","transformative experience","values","physical activity","transform learning","implementation","context","life skills","impact"]},{"p_id":80899,"title":"Making learning meaningful: facilitating interest development and transfer in at-risk college students","abstract":"The Teaching for Transformative Experience in Science (TTES) model has shown to be a useful tool to generate learning and engagement in science. We investigated the effectiveness of TTES for facilitating transformative experience (TE), learning, the development of topic interest and transfer of course concepts to other courses employing a quasi-experimental design. Our goal was to determine the effectiveness of TTES compared to an alternative teaching method in a course designed for academically at-risk undergraduate students. Specifically, we explored the impact of TTES for teaching about motivation in a college success course. The results showed that TTES generated TE outside of the biological sciences, increased learning, developed student interest and facilitated self-reported transfer to other courses. The findings have important implications for facilitating learning and motivation in academically at-risk college students, which may subsequently impact college student retention and academic success.","keywords_author":["Transformative experience","interest","transfer"],"keywords_other":["CONCEPTUAL CHANGE","STRATEGIES","SCIENCE","ANALOGICAL TRANSFER","GOALS","MOTIVATION","TRANSFORMATIVE EXPERIENCE","CLASSROOM"],"max_cite":7.0,"pub_year":2017.0,"sources":"['wos']","rawkeys":["transfer","goals","interest","transformative experience","conceptual change","strategies","science","motivation","classroom","analogical transfer"],"tags":["goals","recognition","interest","transformative experience","conceptual change","strategies","science","motivation","classroom","analogical transfer"]},{"p_id":113683,"title":"The CNN as a Guided Multilayer RECOS Transform","abstract":null,"keywords_author":null,"keywords_other":["MODEL","RECOGNITION"],"max_cite":2.0,"pub_year":2017.0,"sources":"['wos']","rawkeys":["recognition","model"],"tags":["recognition","model"]},{"p_id":39956,"title":"Self-healing data exchange process under evolving schemas: A new mapping adaptation approach based on self-optimization","abstract":"Today, more than ever, enterprises are relying on highly complex IT solutions to respond flexibly and rapidly to the constant changing business environment. Yet, the increasing complexity of IT solutions presents significant challenges. In this paper, we propose a solution to reduce the human intervention needed to maintain data exchange processes after a schema evolution (changes impacting source or target system schemas participating in a data exchange scenario). Our approach, toward reliable self-healed data exchange processes under evolving schemas, is called DEAM (Data Exchange Autonomic Manager). \u00a9 2011 IEEE.","keywords_author":["Autonomic computing","Data exchange","Dependability","Fault tolerance","Machine learning","Mapping adaptation","Schema mapping","Schema matching","Selfmanaged systems","Sufficient correctness"],"keywords_other":["Autonomic Computing","Machine-learning","Schema matching","Sufficient correctness","Schema mappings","Dependability"],"max_cite":1.0,"pub_year":2011.0,"sources":"['scp']","rawkeys":["data exchange","schema matching","sufficient correctness","selfmanaged systems","schema mappings","machine learning","autonomic computing","schema mapping","machine-learning","mapping adaptation","dependability","fault tolerance"],"tags":["data exchange","recognition","schema matching","sufficient correctness","selfmanaged systems","machine learning","autonomic computing","map adaptation","schema mapping","fault tolerance"]},{"p_id":31771,"title":"Cyberbullying detection and prevention: Data mining and psychological perspective","abstract":"\u00a9 2014 IEEE.Bullying is defined as targeting an individual or a group of individuals and exposing them to ridicule and negative actions both physical and mental deliberately. This is a common but serious and demoralizing experience that every individual encounters at least once in his or her lifetime. With the advent of technology, a form of bullying known as cyberbullying has spread very quickly targeting masses of innocent people very easily. Cyberbullying involves the use of computers, mobile phones, etc. for bullying activities. In this paper we focus on the data mining and machine learning techniques which have been proposed to detect and prevent cyberbullying and implement one such machine learning technique to identify the presence or absence of cyberbullying using the dataset from a popular social networking website. We also discuss the psychological factors related to cyberbullying and how the problem can be tackled along those factors. A few proposals for the future algorithms for the detection and prevention of cyberbullying are also put forth.","keywords_author":["Cyberbullying","Data mining","Detect","Machine learning","Perspectives","Prevent","Psychological"],"keywords_other":["Cyber bullying","Detect","Perspectives","Prevent","Psychological"],"max_cite":3.0,"pub_year":2014.0,"sources":"['scp']","rawkeys":["data mining","prevent","detect","cyberbullying","perspectives","machine learning","psychological","cyber bullying"],"tags":["prevention","data mining","recognition","machine learning","perspective","detection","cyber bullying"]},{"p_id":80970,"title":"\"Positive\" Results Increase Down the Hierarchy of the Sciences","abstract":"The hypothesis of a Hierarchy of the Sciences with physical sciences at the top, social sciences at the bottom, and biological sciences in-between is nearly 200 years old. This order is intuitive and reflected in many features of academic life, but whether it reflects the \"hardness\" of scientific research-i.e., the extent to which research questions and results are determined by data and theories as opposed to non-cognitive factors-is controversial. This study analysed 2434 papers published in all disciplines and that declared to have tested a hypothesis. It was determined how many papers reported a \"positive\" (full or partial) or \"negative\" support for the tested hypothesis. If the hierarchy hypothesis is correct, then researchers in \"softer\" sciences should have fewer constraints to their conscious and unconscious biases, and therefore report more positive outcomes. Results confirmed the predictions at all levels considered: discipline, domain and methodology broadly defined. Controlling for observed differences between pure and applied disciplines, and between papers testing one or several hypotheses, the odds of reporting a positive result were around 5 times higher among papers in the disciplines of Psychology and Psychiatry and Economics and Business compared to Space Science, 2.3 times higher in the domain of social sciences compared to the physical sciences, and 3.4 times higher in studies applying behavioural and social methodologies on people compared to physical and chemical studies on non-biological material. In all comparisons, biological studies had intermediate values. These results suggest that the nature of hypotheses tested and the logical and methodological rigour employed to test them vary systematically across disciplines and fields, depending on the complexity of the subject matter and possibly other factors (e. g., a field's level of historical and\/or intellectual development). On the other hand, these results support the scientific status of the social sciences against claims that they are completely subjective, by showing that, when they adopt a scientific approach to discovery, they differ from the natural sciences only by a matter of degree.","keywords_author":null,"keywords_other":["ANIMAL BEHAVIOR","JOURNALS","PUBLICATION BIAS","DISCIPLINE","ARTICLES","SOFT","ECOLOGY","PARTICLE PHYSICS","PSYCHOLOGY","STATISTICAL POWER"],"max_cite":202.0,"pub_year":2010.0,"sources":"['wos']","rawkeys":["soft","articles","publication bias","discipline","particle physics","animal behavior","ecology","statistical power","psychology","journals"],"tags":["recognition","publication bias","discipline","particle physics","softness","animal behavior","ecology","statistical power","article","journals"]},{"p_id":7242,"title":"A review of unsupervised feature learning and deep learning for time-series modeling","abstract":"This paper gives a review of the recent developments in deep learning and unsupervised feature learning for time-series problems. While these techniques have shown promise for modeling static data, such as computer vision, applying them to time-series data is gaining increasing attention. This paper overviews the particular challenges present in time-series data and provides a review of the works that have either applied time-series data to unsupervised feature learning algorithms or alternatively have contributed to modifications of feature learning algorithms to take into account the challenges present in time-series data. \u00a9 2014 Elsevier Ltd.","keywords_author":["Deep learning","Time-series","Unsupervised feature learning","Time-series","Unsupervised feature learning","Deep learning"],"keywords_other":["PREDICTION","BOLTZMANN MACHINES","QUALITY","DELAY NEURAL-NETWORKS","CLASSIFICATION","IDENTIFICATION","ELECTRONIC NOSE SYSTEM","OLFACTION","RECOGNITION","SPEECH"],"max_cite":157.0,"pub_year":2014.0,"sources":"['ieee', 'scp', 'wos']","rawkeys":["quality","identification","recognition","time-series","olfaction","deep learning","unsupervised feature learning","boltzmann machines","prediction","delay neural-networks","electronic nose system","classification","speech"],"tags":["quality","identification","recognition","unsupervised feature learning","olfaction","boltzmann machines","prediction","machine learning","delay neural-networks","classification","speech","electronic nose systems","time series"]},{"p_id":72802,"title":"Age Estimation of Face Images Based on CNN and Divide-and-Rule Strategy","abstract":"In recent years, the research on age estimation based on face images has drawn more and more attention, which includes two processes: feature extraction and estimation function learning. In the aspect of face feature extraction, this paper leverages excellent characteristics of convolution neural network in the field of image application, by using deep learning method to extract face features, and adopts factor analysis model to extract robust features. In terms of age estimation function learning, age-based and sequential study of rank-based age estimation learning methods is utilized and then a divide-and-rule face age estimator is proposed. Experiments in FG-NET, MORPH Album 2, and IMDB-WIKI show that the feature extraction method is more robust than traditional age feature extraction method and the performance of divide-and-rule estimator is superior to classical SVM and SVR.","keywords_author":null,"keywords_other":["REGRESSION","MANIFOLD","RECOGNITION","SCATTERING","PATTERNS"],"max_cite":0.0,"pub_year":2018.0,"sources":"['wos']","rawkeys":["recognition","patterns","manifold","scattering","regression"],"tags":["recognition","patterns","manifolds","scattering","regression"]},{"p_id":40040,"title":"Random artificial incorporation of noise in a learning classifier system environment","abstract":"Effective rule generalization in learning classifier systems (LCSs) has long since been an important consideration. In noisy problem domains, where attributes do not precisely determine class, overemphasis on accuracy without sufficient generalization leads to over-fitting of the training data, and a large discrepancy between training and testing accuracies. This issue is of particular concern within noisy bioinformatic problems such as complex disease, gene association studies. In an effort to promote effective generalization we introduce and explore a simple strategy which seeks to discourage over-fitting via the probabilistic incorporation of random noise within training instances. We evaluate a variety of noise models and magnitudes which either specify an equal probability of noise per attribute, or target higher noise probability to the attributes which tend to be more frequently generalized. Our results suggest that targeted noise incorporation can reduce training accuracy without eroding testing accuracy. In addition, we observe a slight improvement in our power estimates (i.e. ability to detect the true underlying model(s)). \u00a9 2011 ACM.","keywords_author":["gene association study","generalization","genetic algorithm","genetics-based machine learning","learning classifier system","noise","ucs"],"keywords_other":["noise","Genetics based machine learning","generalization","Learning classifier system","ucs"],"max_cite":1.0,"pub_year":2011.0,"sources":"['scp']","rawkeys":["learning classifier system","noise","generalization","genetic algorithm","genetics-based machine learning","gene association study","genetics based machine learning","ucs"],"tags":["learning classifier system","recognition","noise","genetic algorithm","genetics-based machine learning","gene association study","ucs"]},{"p_id":7274,"title":"Human-level concept learning through probabilistic program induction","abstract":"People learning new concepts can often generalize successfully from just a single example, yet machine learning algorithms typically require tens or hundreds of examples to perform with similar accuracy. People can also use learned concepts in richer ways than conventional algorithms-for action, imagination, and explanation. We present a computational model that captures these human learning abilities for a large class of simple visual concepts: handwritten characters from the world's alphabets. The model represents concepts as simple programs that best explain observed examples under a Bayesian criterion. On a challenging one-shot classification task, the model achieves human-level performance while outperforming recent deep learning approaches.We also present several \"visual Turing tests\" probing the model's creative generalization abilities, which in many cases are indistinguishable from human behavior.","keywords_author":null,"keywords_other":["Concept Formation","Generalization (Psychology)","Algorithms","Bayes Theorem","Humans","Machine Learning","NEURAL-NETWORKS","RECOGNITION","EXEMPLAR","Computer Simulation","MODELS","GENERATION","CATEGORIES","BRAIN"],"max_cite":156.0,"pub_year":2015.0,"sources":"['ieee', 'scp', 'wos']","rawkeys":["neural-networks","recognition","generalization (psychology)","computer simulation","concept formation","machine learning","exemplar","brain","categories","humans","models","algorithms","bayes theorem","generation"],"tags":["recognition","model","neural networks","computer simulation","concept formation","machine learning","exemplar","brain","categories","humans","algorithms","bayes theorem","generation"]},{"p_id":72819,"title":"Inferring Emotional Tags From Social Images With User Demographics","abstract":"Social images, which are images uploaded and shared on social networks, are used to express users' emotions. Inferring emotional tags from social images is of great importance; it can benefit many applications, such as image retrieval and recommendation. Whereas previous related research has primarily focused on exploring image visual features, we aim to address this problem by studying whether user demographics make a difference regarding users' emotional tags of social images. We first consider how to model the emotions of social images. Then, we investigate how user demographics, such as gender, marital status, and occupation, are related to the emotional tags of social images. A partially labeled factor graph model named the demographics factor graph model (D-FGM) is proposed to leverage the uncovered patterns. Experiments on a data set collected from the world's largest image sharing website Flickr(1) confirm the accuracy of the proposed model. We also find some interesting phenomena. For example, men and women have different patterns to tag \"anger\" for social images.","keywords_author":["Emotion","image","user demographics"],"keywords_other":["INFORMATION","NETWORKS","FEATURES","STATES","PERCEPTION","EXPRESSION","DATABASE","RECOGNITION","GENDER-DIFFERENCES","SPEECH"],"max_cite":1.0,"pub_year":2017.0,"sources":"['wos']","rawkeys":["states","recognition","features","emotion","database","expression","gender-differences","image","information","networks","perception","speech","user demographics"],"tags":["recognition","images","state","features","emotion","databases","gender differences","expression","networks","perceptions","information","speech","user demographics"]},{"p_id":113793,"title":"View-based 3D model retrieval via supervised multi-view feature learning","abstract":"With the development of the processing technologies of 3D model and the increasing of 3D model in different application flieds, 3D model retrieval is attracting more and more people's attention. In order to handle this problem, most of approaches focus on the feature extraction form different virtual view. It is hard to guarantee the robustness and also ignore the correlation between both views. Thus, we propose an effective view-based 3D model retrieval method via supervised multi-view feature learning (SMFL). First, the subspace dimension of viusal feature is generated through Singular Value Decomposition (SVD) algorithm. This step is used to select main information from multi-view in order to reduce the final amount of calculation; Secondly, we consider the relationship of multi-view from same class and the correlation between two different classes to make the feature mapping in order to reduce the different of views from the same class and increase the different of views from the difference class; Finally, the projection mapping corresponding to the inner product of each 3D model helps to calculate the similarities between two different 3D models. The extensive experiments are conducted on popular ETH, NTU, MV-RED and PSB 3D model datasets with Zernike moments. The comparative results or The experimental results with existing 3D model retrieval methods show the superiority of the proposed method.","keywords_author":["3D model retrieval","Multi-view","Feature learning","Feature dimensionality reduction","SVD","Zernike moments"],"keywords_other":["OBJECT RETRIEVAL","CLASSIFICATION","RECOGNITION","SEARCH ENGINE","SHAPES"],"max_cite":0.0,"pub_year":2018.0,"sources":"['wos']","rawkeys":["object retrieval","recognition","3d model retrieval","search engine","zernike moments","shapes","svd","multi-view","classification","feature learning","feature dimensionality reduction"],"tags":["object retrieval","recognition","3d model retrieval","search engine","zernike moments","singular value decomposition","shape","classification","feature learning","feature dimensionality reduction","multi-views"]},{"p_id":7312,"title":"Convolutional neural networks for P300 detection with application to brain-computer interfaces","abstract":"A Brain-Computer Interface (BCI) is a specific type of human-computer interface that enables the direct communication between human and computers by analyzing brain measurements. Oddball paradigms are used in BCI to generate event-related potentials (ERPs), like the P300 wave, on targets selected by the user. A P300 speller is based on this principle, where the detection of P300 waves allows the user to write characters. The P300 speller is composed of two classification problems. The first classification is to detect the presence of a P300 in the electroencephalogram (EEG). The second one corresponds to the combination of different P300 responses for determining the right character to spell. A new method for the detection of P300 waves is presented. This model is based on a convolutional neural network (CNN). The topology of the network is adapted to the detection of P300 waves in the time domain. Seven classifiers based on the CNN are proposed: four single classifiers with different features set and three multiclassifiers. These models are tested and compared on the Data set II of the third BCI competition. The best result is obtained with a multiclassifier solution with a recognition rate of 95.5 percent, without channel selection before the classification. The proposed approach provides also a new way for analyzing brain activities due to the receptive field of the CNN models. \u00a9 2006 IEEE.","keywords_author":["brain-computer interface (BCI)","convolution","electroencephalogram (EEG)","gradient-based learning","Neural network","P300","spatial filters","Neural network","convolution","gradient-based learning","spatial filters","brain-computer interface (BCI)","electroencephalogram (EEG)","P300"],"keywords_other":["electroencephalogram (EEG)","BCI-COMPETITION-III","P300","CLASSIFICATION","PROGRESS","Computer interfaces","MENTAL PROSTHESIS","COMMUNICATION","RECOGNITION","gradient-based learning","Spatial filters","SPELLER","SINGLE-TRIAL EEG"],"max_cite":154.0,"pub_year":2011.0,"sources":"['ieee', 'scp', 'wos']","rawkeys":["neural network","bci-competition-iii","p300","recognition","spatial filters","brain-computer interface (bci)","convolution","communication","single-trial eeg","classification","gradient-based learning","mental prosthesis","progress","electroencephalogram (eeg)","speller","computer interfaces"],"tags":["brain-computer interfaces","bci-competition-iii","p300","recognition","spatial filters","neural networks","convolution","eeg","communication","single-trial eeg","classification","gradient-based learning","mental prosthesis","progression","speller","computer interfaces"]},{"p_id":31900,"title":"Machine learning approach for emotion recognition in speech","abstract":"This paper presents a machine learning approach to automatic recognition of human emotions from speech. The approach consists of three steps. First, numerical features are extracted from the sound database by using audio feature extractor. Then, feature selection method is used to select the most relevant features. Finally, a machine learning model is trained to recognize seven universal emotions: anger, fear, sadness, happiness, boredom, disgust and neutral. A thorough ML experimental analysis is performed for each step. The results showed that 300 (out of 1582) features, as ranked by the gain ratio, are sufficient for achieving 86% accuracy when evaluated with 10 fold cross-validation. SVM achieved the highest accuracy when compared to KNN and Naive Bayes. We additionally compared the accuracy of the standard SVM (with default parameters) and the one enhanced by Auto-WEKA (optimized algorithm parameters) using the leave-one-speaker-out technique. The results showed that the SVM enhanced with Auto-WEKA achieved significantly better accuracy than the standard SVM, i.e., 73% and 77% respectively. Finally, the results achieved with the 10 fold cross-validation are comparable and similar to the ones achieved by a human, i.e., 86% accuracy in both cases. Even more, low energy emotions (boredom, sadness and disgust) are better recognized by our machine learning approach compared to the human.","keywords_author":["Auto-WEKA","Emotions","Machine learning","Recognition","Speech"],"keywords_other":["Emotions","Feature selection methods","Recognition","10-fold cross-validation","Automatic recognition","Machine learning approaches","Machine learning models","Auto-WEKA"],"max_cite":3.0,"pub_year":2014.0,"sources":"['scp']","rawkeys":["auto-weka","recognition","emotions","feature selection methods","10-fold cross-validation","machine learning models","machine learning","speech","automatic recognition","machine learning approaches"],"tags":["auto-weka","recognition","feature selection methods","10-fold cross-validation","emotion","machine learning models","machine learning","speech","automatic recognition","machine learning approaches"]},{"p_id":113847,"title":"Detecting Flying Objects Using a Single Moving Camera","abstract":"We propose an approach for detecting flying objects such as Unmanned Aerial Vehicles (UAVs) and aircrafts when they occupy a small portion of the field of view, possibly moving against complex backgrounds, and are filmed by a camera that itself moves. We argue that solving such a difficult problem requires combining both appearance and motion cues. To this end we propose a regression-based approach for object-centric motion stabilization of image patches that allows us to achieve effective classification on spatio-temporal image cubes and outperform state-of-the-art techniques. As this problem has not yet been extensively studied, no test datasets are publicly available. We therefore built our own, both for UAVs and aircrafts, and will make them publicly available so they can be used to benchmark future flying object detection and collision avoidance algorithms.","keywords_author":["Motion compensation","object detection"],"keywords_other":["NEURAL-NETWORKS","RECOGNITION","VISION","SYSTEM"],"max_cite":5.0,"pub_year":2017.0,"sources":"['wos']","rawkeys":["neural-networks","recognition","motion compensation","system","vision","object detection"],"tags":["recognition","motion compensation","neural networks","system","vision","object detection"]},{"p_id":113849,"title":"An overview of traffic sign detection and classification methods","abstract":"Over the last few years, different traffic sign recognition systems were proposed. The present paper introduces an overview of some recent and efficient methods in the traffic sign detection and classification. Indeed, the main goal of detection methods is localizing regions of interest containing traffic sign, and we divide detection methods into three main categories: color-based (classified according to the color space), shape-based, and learning-based methods (including deep learning). In addition, we also divide classification methods into two categories: learning methods based on hand-crafted features (HOG, LBP, SIFT, SURF, BRISK) and deep learning methods. For easy reference, the different detection and classification methods are summarized in tables along with the different datasets. Furthermore, future research directions and recommendations are given in order to boost TSR's performance.","keywords_author":["Traffic sign detection","Traffic sign classification","Image processing","Object detection","Vehicle safety"],"keywords_other":["VEHICLE","SYSTEM","MODEL","RECOGNITION","ALGORITHMS","REAL-TIME","ROBUST","CONVOLUTIONAL NEURAL-NETWORKS"],"max_cite":0.0,"pub_year":2017.0,"sources":"['ieee', 'wos']","rawkeys":["traffic sign classification","recognition","model","image processing","real-time","system","object detection","convolutional neural-networks","robust","algorithms","traffic sign detection","vehicle","vehicle safety"],"tags":["traffic sign classification","robustness","recognition","model","image processing","real time","system","vehicles","object detection","convolutional neural network","traffic sign detection","algorithms","vehicle safety"]},{"p_id":97471,"title":"Robot Intelligence for Real World Applications","abstract":"This paper presents a brief review on recent work on machine intelligence for real-world applications of robots. To act in a real world environment, a robot should possess a broad sense of intelligence including speech, perception, reasoning, action, etc. In this paper, we particularly deal with the intelligence involving action or body motion. The intelligence related to robot action\/motion can be classified into two categories: manipulation intelligence and mobility intelligence. The manipulation intelligence means the skill\/intelligence of reliably manipulating objects according to tasks and the mobility intelligence corresponds to the ability of autonomously moving, or flying, and or jumping in a natural environment. Human-robot interaction is another important topic for real-world applications. In addition to reviewing the major approaches, this paper also gives an overview on our efforts in these important topics.","keywords_author":["Machine intelligence","Simultaneous localization and mapping (SLAM)","Medical robots","Human robot interaction"],"keywords_other":["FORM-CLOSURE GRASPS","DEFORMABLE OBJECTS","MOBILE ROBOTS","POSE ESTIMATION","FLEXIBLE OBJECT","MANIPULATORS","RECOGNITION","FEEDBACK","OPTIMIZATION","SIMULTANEOUS LOCALIZATION"],"max_cite":0.0,"pub_year":2018.0,"sources":"['wos']","rawkeys":["form-closure grasps","machine intelligence","pose estimation","simultaneous localization and mapping (slam)","recognition","flexible object","simultaneous localization","human robot interaction","mobile robots","manipulators","feedback","optimization","medical robots","deformable objects"],"tags":["form-closure grasps","machine intelligence","pose estimation","deformable object","recognition","flexible object","human-robot interaction","simultaneous localization","robotics","mobile robots","feedback","optimization","medical robotics","manipulation"]},{"p_id":7360,"title":"The pen is mightier than the keyboard: Advantages of longhand over laptop note taking","abstract":"Taking notes on laptops rather than in longhand is increasingly common. Many researchers have suggested that laptop note taking is less effective than longhand note taking for learning. Prior studies have primarily focused on students' capacity for multitasking and distraction when using laptops. The present research suggests that even when laptops are used solely to take notes, they may still be impairing learning because their use results in shallower processing. In three studies, we found that students who took notes on laptops performed worse on conceptual questions than students who took notes longhand. We show that whereas taking more notes can be beneficial, laptop note takers' tendency to transcribe lectures verbatim rather than processing information and reframing it in their own words is detrimental to learning. \u00a9 The Author(s) 2014.","keywords_author":["Academic achievement","Cognitive processes","Educational psychology","Memory","Open data","Open materials"],"keywords_other":["Efficiency","Microcomputers","Male","Teaching","Learning","Attitude to Computers","Humans","Memory","Education","Psychology, Educational","Cognition","Female","Students"],"max_cite":151.0,"pub_year":2014.0,"sources":"['scp']","rawkeys":["microcomputers","memory","academic achievement","education","students","open data","psychology","educational psychology","attitude to computers","teaching","learning","humans","educational","cognition","male","cognitive processes","efficiency","open materials","female"],"tags":["education","microcomputers","students","recognition","attitude to computers","memory","educational-psychology","male","female","open data","teaching","cognitive process","machine learning","efficiency","humans","open materials","academic-achievement","cognition"]},{"p_id":56530,"title":"Transfer learning approach for classification and noise reduction on noisy web data","abstract":"One of the main ingredients to learn a visual representation of an object using the Convolutional Neural Networks is a large and carefully annotated dataset. Acquiring a dataset in a demanded scale is not a straightforward task; therefore, the community attempts to solve this problem by creating noisy datasets gathered from web sources. In this paper, this issue is tackled by designing a vehicle recognition system using Convolutional Neural Networks and noisy web data. In the proposed system, the transfer learning technique is employed, and behavior of several deep architectures trained on a noisy dataset are studied. In addition, the external noise of the gathered dataset is reduced by exploiting an unsupervised method called Isolation Forest, and the new training results are examined. Based on the experiments, high recognition accuracies were achieved by training two states of the art networks on the noisy dataset, and the obtained results were slightly improved by using the proposed noise reduction framework. Finally, a demonstration application is provided to show the capability and the performance of the proposed approach. (C) 2018 Elsevier Ltd. All rights reserved.","keywords_author":["Convolutional Neural Networks","Transfer learning","Vehicle recognition","Noisy dataset","Isolation forest"],"keywords_other":["RECOGNITION"],"max_cite":0.0,"pub_year":2018.0,"sources":"['wos']","rawkeys":["recognition","convolutional neural networks","noisy dataset","transfer learning","vehicle recognition","isolation forest"],"tags":["recognition","noisy dataset","transfer learning","vehicle recognition","convolutional neural network","isolation forest"]},{"p_id":56570,"title":"Precision Security: Integrating Video Surveillance with Surrounding Environment Changes","abstract":"Video surveillance plays a vital role in maintaining the social security although, until now, large uncertainty still exists in danger understanding and recognition, which can be partly attributed to intractable environment changes in the backgrounds. This article presents a brain-inspired computing of attention value of surrounding environment changes (EC) with a processes-based cognition model by introducing a ratio value lambda of EC-implications within considered periods. Theoreticalmodels for computation of warning level of EC-implications to the universal video recognition efficiency (quantified as time cost of implication-ratio variations from lambda(k) to lambda(k+1), k = 1,2,...) are further established. Imbedding proposed models into the online algorithms is suggested as a future research priority towards precision security for critical applications and, furthermore, schemes for a practical implementation of such integration are also preliminarily discussed.","keywords_author":null,"keywords_other":["LANE DETECTION","UNMANNED AERIAL VEHICLE","FRAMEWORK","ALGORITHM","RECOGNITION","SEGMENTATION","NAVIGATION","SUBTRACTION","OBJECT DETECTION","TRACKING"],"max_cite":0.0,"pub_year":2018.0,"sources":"['ieee', 'wos']","rawkeys":["unmanned aerial vehicle","algorithm","recognition","subtraction","segmentation","framework","tracking","lane detection","object detection","navigation"],"tags":["unmanned aerial vehicle","recognition","subtraction","segmentation","framework","tracking","lane detection","object detection","algorithms","navigation"]},{"p_id":97533,"title":"Learning shape retrieval from different modalities","abstract":"We propose in this paper a new framework for 3D shape retrieval using queries of different modalities, which can include 3D models, images and sketches. The main scientific challenge is that different modalities have different representations and thus lie in different spaces. Moreover, the features that can be extracted from 2D images or 2D sketches are often different from those that can be computed from 3D models. Our solution is a new method based on Convolutional Neural Networks (CNN) that embeds all these entities into a common space. We propose a novel 3D shape descriptor based on local CNN features encoded using vectors of locally aggregated descriptors instead of conventional global CNN. Using a kernel function computed from 3D shape similarity, we build a target space in which wild images and sketches can be projected via two different CNNs. With this construction, matching can be performed in the common target space between same entities (sketch-sketch, image-image and 3D shape-3D shape) and more importantly across different entities (sketch-image, sketch-3D shape andimage-3D shape). We demonstrate the performance of the proposed framework using different benchmarks including large scale SHREC 3D datasets. (C) 2017 Elsevier B.V. All rights reserved.","keywords_author":["Multimodal 3D retrieval","Convolutional Neural Networks","3D shape","Object retrieval","Sketch retrieval"],"keywords_other":["MODEL RETRIEVAL","NYSTROM METHOD","REPRESENTATION","CLASSIFICATION","RECOGNITION","3D"],"max_cite":0.0,"pub_year":2017.0,"sources":"['ieee', 'wos']","rawkeys":["object retrieval","convolutional neural networks","recognition","model retrieval","representation","sketch retrieval","nystrom method","3d shape","3d","classification","multimodal 3d retrieval"],"tags":["object retrieval","recognition","model retrieval","representation","sketch retrieval","3-d shape","nystrom method","three-dimensional","classification","convolutional neural network","multimodal 3d retrieval"]},{"p_id":56574,"title":"Multimodal Deep Embedding via Hierarchical Grounded Compositional Semantics","abstract":"For a number of important problems, isolated semantic representations of individual syntactic words or visual objects do not suffice, but instead a compositional semantic representation is required; for example, a literal phrase or a set of spatially concurrent objects. In this paper, we aim to harness the existing image-sentence databases to exploit the compositional nature of image-sentence data for multimodal deep embedding. In particular, we propose an approach called hierarchical-alike (bottom-up two layers) multimodal grounded compositional semantics (hiMoCS) learning. The proposed hiMoCS systemically captures the compositional semantic connotation of multimodal data in the setting of hierarchical-alike deep learning by modeling the inherent correlations between two modalities of collaboratively grounded semantics, such as the textual entity (with its describing attribute) and visual object, the phrase (e.g., subject-verb-object triplet), and spatially concurrent objects. We argue that hiMoCS is more appropriate to reflect the multimodal compositional semantics of the image and its narrative textual sentence, which are strongly coupled. We evaluate hiMoCS on the several benchmark data sets and show that the utilization of the hiMoCS (textual entities and visual objects, textual phrase, and spatially concurrent objects) achieves a much better performance than only using the flat grounded compositional semantics.","keywords_author":["Compositional semantics","multimodal analysis","multimodal embedding"],"keywords_other":["RETRIEVAL","RECOGNITION","MODELS","LEARNING FRAMEWORK"],"max_cite":0.0,"pub_year":2018.0,"sources":"['ieee', 'wos']","rawkeys":["recognition","multimodal analysis","multimodal embedding","models","retrieval","compositional semantics","learning framework"],"tags":["recognition","model","multimodal analysis","multimodal embedding","learning frameworks","retrieval","compositional semantics"]},{"p_id":64767,"title":"Face spoofing detection based on color texture Markov feature and support vector machine recursive feature elimination","abstract":"Aiming to counterstrike face spoofing attacks such as photo attacks and video attacks, a face spoofing detection scheme based on color texture Markov feature (CTMF) and support vector machine recursive feature elimination (SVM-RFE) is proposed. In this paper, the adjacent facial pixels discrepancy between the real and the fake face is analyzed, and texture information between the color channels is fully considered. Firstly, the directional difference filter is used to capture the facial texture difference between the real and the fake face, which can be regarded as low-level features of CTMF. Then, the facial texture difference is modeled by the Markov process to form a high-level representation of the low-level features. Meanwhile, the mutual information of facial texture between the color channels, which is ignored in the previous literature, is investigated. In addition, SVM-RFE is utilized to reduce the feature dimension and makes it suitable for real-time detection. Experiments on four public benchmark databases indicate that the proposed scheme can effectively resist photo and video spoofing attacks in face recognition.","keywords_author":["Face anti-spoofing","Color texture Markov feature","Adjacent facial pixels discrepancy","SVM-RFE"],"keywords_other":["IMAGE","ATTACKS","SYSTEMS","CLASSIFICATION","RECOGNITION"],"max_cite":0.0,"pub_year":2018.0,"sources":"['ieee', 'wos']","rawkeys":["color texture markov feature","recognition","systems","svm-rfe","image","adjacent facial pixels discrepancy","classification","attacks","face anti-spoofing"],"tags":["color texture markov feature","recognition","images","system","support vector machine recursive feature eliminations","adjacent facial pixels discrepancy","classification","attacks","face anti-spoofing"]},{"p_id":97537,"title":"Shape classification using spectral graph wavelets","abstract":"Spectral shape descriptors have been used extensively in a broad spectrum of geometry processing applications ranging from shape retrieval and segmentation to classification. In this paper, we propose a spectral graph wavelet approach for 3D shape classification using the bag-of-features paradigm. In an effort to capture both the local and global geometry of a 3D shape, we present a three-step feature description framework. First, local descriptors are extracted via the spectral graph wavelet transform having the Mexican hat wavelet as a generating kernel. Second, mid-level features are obtained by embedding local descriptors into the visual vocabulary space using the soft-assignment coding step of the bag-of-features model. Third, a global descriptor is constructed by aggregating mid-level features weighted by a geodesic exponential kernel, resulting in a matrix representation that describes the frequency of appearance of nearby codewords in the vocabulary. Experimental results on two standard 3D shape benchmarks demonstrate the effectiveness of the proposed classification approach in comparison with state-of-the-art methods.","keywords_author":["Spectral graph wavelet","Laplace-Beltrami","Bag-of-features","Support vector machines","Shape descriptors","Classification"],"keywords_other":["DIFFUSION","RETRIEVAL","RECOGNITION","SIGNATURE","DESCRIPTOR"],"max_cite":0.0,"pub_year":2017.0,"sources":"['ieee', 'wos']","rawkeys":["bag-of-features","recognition","spectral graph wavelet","laplace-beltrami","shape descriptors","signature","classification","retrieval","support vector machines","descriptor","diffusion"],"tags":["bag-of-features","recognition","laplace-beltrami","shape descriptors","machine learning","signature","classification","retrieval","spectral graph wavelets","descriptors","diffusion"]},{"p_id":97538,"title":"3D Object retrieval based on viewpoint segmentation","abstract":"In the last decades, extensive efforts have been dedicated to develop better 3D object retrieval methods. View-based methods have attracted a significant amount of attention, not only because of their state-of-the-art performance, but also they merely require some of a 3D object's 2D view images. However, most recent approaches only deal with the images' content difference without the discrepancy of view relative positions. In this paper, we propose a normal method for view segmentation, based on Markov random field (MRF) model, which consider not only the difference between the content of views but also the relative locations. Each view is obtained by projecting at certain viewpoints and angels, therefore, these locations can be applied to depict each view, with content of views. We use the MRF to implement view segmentation and choose the representative views. Finally, we present a framework based on the proposed view segmentation method for 3D object retrieval and the experimental results demonstrate that the proposed method can achieve better retrieval effectiveness than state-of-the-art methods under several standard evaluation measures.","keywords_author":["3D model retrieval","View segmentation","Markov random filed"],"keywords_other":["MODEL RETRIEVAL","SYSTEM","SHAPE RETRIEVAL","RELEVANCE FEEDBACK","RECOGNITION","SEARCH ENGINE","MECHANISM","DESCRIPTOR","DISTRIBUTIONS"],"max_cite":0.0,"pub_year":2017.0,"sources":"['ieee', 'wos']","rawkeys":["markov random filed","recognition","search engine","3d model retrieval","mechanism","model retrieval","shape retrieval","view segmentation","system","distributions","descriptor","relevance feedback"],"tags":["markov random filed","recognition","search engine","3d model retrieval","shape retrieval","model retrieval","mechanisms","view segmentation","system","random forests","distributions","descriptors"]},{"p_id":97539,"title":"Sparse Robust Filters for scene classification of Synthetic Aperture Radar (SAR) images","abstract":"With the increasing resolution of Synthetic Aperture Radar (SAR) images, extracting their discriminative features for scenes classification has become a challenging task, because SAR images are very sensitive to target aspect brought by shadowing effects, interaction of the signature with the environment, and so on. Moreover, SAR images are remarkably polluted by the multiplicative speckle noise, which makes the conventional feature extractors inefficient. In this paper we advance new Sparse Robust Filters (SRFs) for automatic learning of discriminant features of scenes. A Hierarchical Group Sparse Coding (HGSC) model is proposed to learn a set of sparse and robust filters, to capture the multiscale local descriptors that are robust to noises. Some experiments are taken on a TerraSAR-X images dataset (in the middle of the Swabian Jura, the Nordlinger Ries, HH, observed on July, 2007), and a Moving and Stationary Target Acquisition and Recognition (MSTAR) dataset, to evaluate the performance of our proposed method. The experimental results show that our method can achieve higher classification accuracy compared with other related approaches. (C) 2015 Elsevier B.V. All rights reserved.","keywords_author":["Scene classification","Synthetic Aperture Radar","Sparse Robust Filters","Hierarchical group sparse coding"],"keywords_other":["REPRESENTATION","FRAMEWORK","ALGORITHM","POLARIMETRIC SAR","COVER CLASSIFICATION","MODEL","RECOGNITION","HIERARCHICAL FEATURES","DICTIONARIES","HIGH-LEVEL FEATURE"],"max_cite":1.0,"pub_year":2016.0,"sources":"['ieee', 'wos']","rawkeys":["algorithm","recognition","model","synthetic aperture radar","framework","hierarchical features","representation","scene classification","polarimetric sar","high-level feature","hierarchical group sparse coding","sparse robust filters","dictionaries","cover classification"],"tags":["high-level features","recognition","model","synthetic aperture radar","framework","hierarchical features","representation","scene classification","polarimetric sar","hierarchical group sparse coding","algorithms","sparse robust filters","dictionaries","cover classification"]},{"p_id":40196,"title":"A multiobjective learning and ensembling approach to high-performance speech enhancement with compact neural network architectures","abstract":"\u00a9 2014 IEEE. In this study, we propose a novel deep neural network (DNN) architecture for speech enhancement (SE) via a multiobjective learning and ensembling (MOLE) framework to achieve a compact and lowlatency design, while maintaining good performance in quality evaluations. MOLE follows the boosting concept when combining weak models into a strong classifier and consists of two compact DNNs. The first, called the multiobjective learning DNN (MOL-DNN), takes multiple features, such as log-power spectra (LPS), mel-frequency cepstral coefficients (MFCCs) and Gammatone frequency cepstral coefficients (GFCCs) to predict a multiobjective set that includes clean speech feature, dynamic noise feature, and ideal ratio mask (IRM). The second, called the multiobjective ensembling DNN (MOE-DNN), takes the learned features from MOL-DNN as inputs and separately predicts clean LPS and IRM, clean MFCC and IRM, and clean GFCC and IRM using three sets of weak regression functions. Finally, a postprocessing operation can be applied to the estimated clean features by leveraging the multiple targets learned from both the MOL-DNN and the MOE-DNN. On speech corrupted by 15 noise types not seen in model training the SE results show that the MOLE approach, which features a small model size and low run-time latency, can achieve consistent improvements over both DNN- and long short-term memory (LSTM)-based techniques in terms of all the objective metrics evaluated in this study for all three cases (the input contexts contain 1-frame, 4-frame and 7-frame instances). The 1-frame MOLE-based SE system outperforms the DNN-based SE system with a 7-frame input expansion at a 3-frame delay and also achieves better performance than the LSTM-based SE system with 4-frame, no delay expansion by including only 3 previous frames, and with 170 times less processing latency.","keywords_author":["compact and low-latency design","deep neural network (DNN)","multiobjective ensembling","multiobjective learning","Speech enhancement (SE)","Speech enhancement (SE)","deep neural network (DNN)","multiobjective learning","multiobjective ensembling","compact and low-latency design"],"keywords_other":["AUDITORY SCENE ANALYSIS","NOISE-REDUCTION","ROBUST SPEAKER IDENTIFICATION","FEATURES","SEPARATION","SYSTEMS","Multi objective","ENVIRONMENTS","Noise measurements","RECOGNITION","Multi-objective learning","MASK","SPECTRAL AMPLITUDE ESTIMATOR","Mel frequency cepstral co-efficient","Low latency"],"max_cite":1.0,"pub_year":2018.0,"sources":"['ieee', 'scp', 'wos']","rawkeys":["spectral amplitude estimator","multi objective","multi-objective learning","features","mask","multiobjective ensembling","environments","deep neural network (dnn)","recognition","mel frequency cepstral co-efficient","speech enhancement (se)","robust speaker identification","noise-reduction","noise measurements","auditory scene analysis","multiobjective learning","low latency","compact and low-latency design","systems","separation"],"tags":["spectral amplitude estimator","multi objective","multi-objective learning","speech enhancement","environment","convolutional neural network","noise measurement","features","masking","system","multiobjective ensembling","noise reduction","recognition","mel-frequency cepstral coefficients","robust speaker identification","auditory scene analysis","low latency","compact and low-latency design","separation"]},{"p_id":64769,"title":"Face presentation attack detection using guided scale texture","abstract":"Aiming to counter presentation attack (also known as spoofing attack) in face recognition system, a face presentation attack detection (also known as spoofing detection or liveness detection) scheme based on guided scale texture is proposed. In order to minimize the influence of the redundant noise contamination, guided scale space is proposed to reduce the redundancy of the original facial texture and to extract more powerful facial edges. Based on the guided scale space, two guided scale texture descriptors are proposed to extract liveness detection features, and they are guided scale based local binary pattern (GS-LBP) and local guided binary pattern (LGBP). GS-LBP takes advantage of the edge-preserving property of the guided scale space, and joint quantization is used in LGBP to encode the neighboring relationships of the original face and the guided scale face without using additional features. With the guided scale texture features, presentation attack detection is accomplished by the use of a linear support vector machine classifier. Experiments are done with public MSU MFSD, CASIA FASD, Replay-Attack and Replay-Mobile databases, and the results indicate its effectiveness. The proposed method can effectively be applied for countering photo attack and video attack in face recognition systems.","keywords_author":["Face presentation attack detection","Guided scale","Guided scale based local binary pattern","Local guided binary pattern"],"keywords_other":["IRIS","FUSION","SPOOFING DETECTION","BIOMETRICS","SYSTEMS","CLASSIFICATION","RECOGNITION","IMAGE QUALITY"],"max_cite":0.0,"pub_year":2018.0,"sources":"['wos']","rawkeys":["guided scale","spoofing detection","iris","recognition","guided scale based local binary pattern","face presentation attack detection","local guided binary pattern","classification","biometrics","image quality","systems","fusion"],"tags":["guided scale","spoofing detection","iris","recognition","guided scale based local binary pattern","face presentation attack detection","system","local guided binary pattern","classification","biometrics","image quality","fusion"]},{"p_id":40254,"title":"Face Liveness Detection Using a Flash Against 2D Spoofing Attack","abstract":"\u00a9 2005-2012 IEEE.Face recognition technique has been widely applied to personal identification systems due to its satisfying performance. However, its security may be a crucial issue, since many studies have shown that face recognition systems may be vulnerable in an adversarial environment, in which an adversary can camouflage as a legitimate user in order to mislead the system. Although face liveness detection methods have been proposed to distinguish real and fake faces, they are either time-consuming, costly, or sensitive to noise and illumination. This paper proposes a face liveness detection method with flash against 2D spoofing attack. Flash not only can enhance the differentiation between legitimate and illegitimate users, but it also reduces the influence of environmental factors. Two images are taken from a subject, one with flash and another without flash. Four texture and 2D structure descriptors with low computational complexity are used to capture information of the two images in our model. Advantages of our method include low installation cost of flash and no user cooperation required. A data set of 50 subjects collected under different scenarios is used in the experiments to evaluate the proposed method. The experimental results indicate that the proposed model performs better than existing liveness detection methods in different environmental scenarios. This paper confirms that the use of flash successfully improves face liveness detection in terms of accuracy, robustness, and running time.","keywords_author":["2D spoofing attack","adversarial learning","Face liveness detection","flash light","Face liveness detection","2D spoofing attack","flash light","adversarial learning"],"keywords_other":["Face liveness","Spoofing attacks","AUTHENTICATION","CHALLENGES","Two-dimensional displays","SYSTEMS","LOCAL BINARY PATTERNS","Time complexity","Adversarial learning","RECOGNITION","Face","FOURIER SPECTRA","IMAGE QUALITY","MOTION"],"max_cite":1.0,"pub_year":2018.0,"sources":"['ieee', 'scp', 'wos']","rawkeys":["time complexity","adversarial learning","recognition","authentication","face liveness detection","spoofing attacks","image quality","motion","face liveness","challenges","fourier spectra","two-dimensional displays","flash light","local binary patterns","2d spoofing attack","systems","face"],"tags":["time complexity","adversarial learning","recognition","authentication","face liveness detection","spoofing attacks","image quality","motion","face liveness","challenges","fourier spectra","system","flash light","local binary patterns","2d spoofing attack","face","two dimensional displays"]},{"p_id":56653,"title":"Improved Deep Hybrid Networks for Urban Traffic Flow Prediction Using Trajectory Data","abstract":"The urban traffic flow prediction is a significant issue in the intelligent transportation system. In consideration of nonlinear and spatial-temporal features of urban traffic data, we propose a deep hybrid neural network improved by greedy algorithm for urban traffic flow prediction with taxi GPS trace. The proposed deep neural network model first combines the convolutional neural network (CNN), which extracts the spatial features, with the long short term memory (LSTM), which captures the temporal information, to predict urban traffic flow. Then, the proposed model is trained by a greedy policy to short time consumption and improves accuracy when a network goes deeper. Experimental results with real taxis GPS trajectory data from Xi'an city show that the improved deep hybrid CNN-LSTM model can achieve higher prediction accuracy and shorter time consumption compared with existing methods.","keywords_author":["Deep hybrid networks","greedy policy","trajectory data","urban traffic-flow prediction"],"keywords_other":["REGRESSION","GRADIENT","MODEL","RECOGNITION","TIME","PATTERNS","CONTROL DESIGN"],"max_cite":0.0,"pub_year":2018.0,"sources":"['ieee', 'wos']","rawkeys":["deep hybrid networks","recognition","model","greedy policy","urban traffic-flow prediction","patterns","time","trajectory data","gradient","control design","regression"],"tags":["deep hybrid networks","recognition","model","greedy policy","urban traffic-flow prediction","patterns","time","trajectory data","gradient","control design","regression"]},{"p_id":56654,"title":"Multisource Transfer Double DQN Based on Actor Learning","abstract":"Deep reinforcement learning (RL) comprehensively uses the psychological mechanisms of \"trial and error\" and \"reward and punishment\" in RL as well as powerful feature expression and nonlinear mapping in deep learning. Currently, it plays an essential role in the fields of artificial intelligence and machine learning. Since an RL agent needs to constantly interact with its surroundings, the deep Q network (DQN) is inevitably faced with the need to learn numerous network parameters, which results in low learning efficiency. In this paper, a multisource transfer double DQN (MTDDQN) based on actor learning is proposed. The transfer learning technique is integrated with deep RL to make the RL agent collect, summarize, and transfer action knowledge, including policy mimic and feature regression, to the training of related tasks. There exists action overestimation in DQN, i.e., the lower probability limit of action corresponding to the maximum Q value is nonzero. Therefore, the transfer network is trained by using double DQN to eliminate the error accumulation caused by action overestimation. In addition, to avoid negative transfer, i.e., to ensure strong correlations between source and target tasks, a multisource transfer learning mechanism is applied. The Atari2600 game is tested on the arcade learning environment platform to evaluate the feasibility and performance of MTDDQN by comparing it with some mainstream approaches, such as DQN and double DQN. Experiments prove that MTDDQN achieves not only human-like actor learning transfer capability, but also the desired learning efficiency and testing accuracy on target task.","keywords_author":["Actor learning","Atari2600 game","double deep Q network (DQN)","multisource transfer"],"keywords_other":["REINFORCEMENT","SYSTEMS"],"max_cite":0.0,"pub_year":2018.0,"sources":"['ieee', 'wos']","rawkeys":["double deep q network (dqn)","multisource transfer","reinforcement","actor learning","atari2600 game","systems"],"tags":["recognition","double deep q network (dqn)","multisource transfer","system","actor learning","atari2600 game"]},{"p_id":81239,"title":"Minimal ensemble based on subset selection using ECG to diagnose categories of CAN","abstract":"Background and objective: Early diagnosis of cardiac autonomic neuropathy (CAN) is critical for reversing or decreasing its progression and prevent complications. Diagnostic accuracy or precision is one of the core requirements of CAN detection. As the standard Ewing battery tests suffer from a number of shortcomings, research in automating and improving the early detection of CAN has recently received serious attention in identifying additional clinical variables and designing advanced ensembles of classifiers to improve the accuracy or precision of CAN diagnostics. Although large ensembles are commonly proposed for the automated diagnosis of CAN, large ensembles are characterized by slow processing speed and computational complexity. This paper applies ECG features and proposes a new ensemble-based approach for diagnosis of CAN progression.","keywords_author":["Automated diagnosis","Cardiac autonomic neuropathy","Diabetes complications","Ensemble classifiers","Multi-tier system","Cross validation"],"keywords_other":["PAROXYSMAL ATRIAL-FIBRILLATION","FEATURES","VENTRICULAR-FIBRILLATION","ALGORITHM","CLASSIFICATION","HEART-RATE-VARIABILITY","RECOGNITION","SIGNALS","CARDIAC AUTONOMIC NEUROPATHY","IMAGES"],"max_cite":1.0,"pub_year":2018.0,"sources":"['wos']","rawkeys":["signals","algorithm","cardiac autonomic neuropathy","recognition","cross validation","images","features","automated diagnosis","paroxysmal atrial-fibrillation","heart-rate-variability","ventricular-fibrillation","classification","ensemble classifiers","multi-tier system","diabetes complications"],"tags":["signals","cardiac autonomic neuropathy","recognition","images","features","automated diagnosis","paroxysmal atrial-fibrillation","ventricular fibrillation","classification","heart rate variability","algorithms","computer vision","multi-tier system","ensemble classifiers","diabetes complications"]},{"p_id":7522,"title":"Human tracking using convolutional neural networks","abstract":"In this paper, we treat tracking as a learning problem of estimating the location and the scale of an object given its previous location, scale, as well as current and previous image frames. Given a set of examples, we train convolutional neural networks (CNNs) to perform the above estimation task. Different from other learning methods, the CNNs learn both spatial and temporal features jointly from image pairs of two adjacent frames. We introduce multiple path ways in CNN to better fuse local and global information. A creative shift-variant CNN architecture is designed so as to alleviate the drift problem when the distracting objects are similar to the target in cluttered environment. Furthermore, we employ CNNs to estimate the scale through the accurate localization of some key points. These techniques are object-independent so that the proposed method can be applied to track other types of object. The capability of the tracker of handling complex situations is demonstrated in many testing sequences. \u00a9 2010 IEEE.","keywords_author":["Convolutional neural networks","machine learning","visual tracking","Convolutional neural networks","machine learning","visual tracking"],"keywords_other":["Global informations","Visual Tracking","Keypoints","Image pairs","Image frames","Multiple-path","Learning methods","MULTIPLE","Human Tracking","machine learning","Drift problem","Learning problem","RECOGNITION","VISUAL TRACKING","Temporal features","Shift-variant","Convolutional neural network","Testing sequences","Cluttered environments"],"max_cite":140.0,"pub_year":2010.0,"sources":"['wos', 'scp', 'ieee']","rawkeys":["global informations","learning methods","image frames","convolutional neural network","machine learning","cluttered environments","temporal features","drift problem","multiple-path","recognition","convolutional neural networks","testing sequences","shift-variant","keypoints","learning problem","visual tracking","human tracking","multiple","image pairs"],"tags":["global informations","recognition","test sequence","keypoints","learning methods","machine learning","cluttered environments","image frames","learning problem","temporal features","visual tracking","drift problem","convolutional neural network","human tracking","multiple-path","multiple","image pairs","shift-variant"]},{"p_id":7544,"title":"Describing textures in the wild","abstract":"\u00a9 2014 IEEE.Patterns and textures are key characteristics of many natural objects: a shirt can be striped, the wings of a butterfly can be veined, and the skin of an animal can be scaly. Aiming at supporting this dimension in image understanding, we address the problem of describing textures with semantic attributes. We identify a vocabulary of forty-seven texture terms and use them to describe a large dataset of patterns collected 'in the wild'. The resulting Describable Textures Dataset (DTD) is a basis to seek the best representation for recognizing describable texture attributes in images. We port from object recognition to texture recognition the Improved Fisher Vector (IFV) and Deep Convolutional-network Activation Features (DeCAF), and show that surprisingly, they both outperform specialized texture descriptors not only on our problem, but also in established material recognition datasets. We also show that our describable attributes are excellent texture descriptors, transferring between datasets and tasks, in particular, combined with IFV and DeCAF, they significantly outperform the state-of-the-art by more than 10% on both FMD and KTH-TIPS-2b benchmarks. We also demonstrate that they produce intuitive descriptions of materials and Internet images.","keywords_author":["attribute","convolutional neural network","Fisher Vector","material","recognition","texture"],"keywords_other":["Material recognition","recognition","Texture descriptors","Texture recognition","Fisher vectors","Convolutional neural network","attribute","Network activations"],"max_cite":139.0,"pub_year":2014.0,"sources":"['ieee', 'scp', 'wos']","rawkeys":["network activations","recognition","material recognition","texture descriptors","texture recognition","fisher vector","material","texture","fisher vectors","convolutional neural network","attribute"],"tags":["recognition","material recognition","materials","texture descriptors","attributes","texture recognition","network activities","texture","fisher vectors","convolutional neural network"]},{"p_id":81278,"title":"Person re-identification post-rank optimization via hypergraph-based learning","abstract":"In computer vision, person re-identification has recently received significant attention from researchers and is becoming an emerging research domain with various challenges. Specifically, re-ranking or post-rank optimization is a significant challenge. Existing re-identification methods perform well in certain particular scenarios, but their performance at rank-1 remains a major concern. Such methods cannot model the complex and higher-order relationship among the images. To address such issues, we present a hypergraph-based learning scheme that not only improves the rank-1 accuracy but also models the complex and higher-order relationships among the images. After obtaining the rank list using a baseline method, we apply a new refinement algorithm on it to classify ranks accordingly. Furthermore, to discover the relationship among samples, we utilize the hypergraphs for re-rank learning. A soft assignment technique is used to perform weight learning of hyperedges. The proposed method achieves better ranking performance; consequently, the re-identification is improved. An extensive experimental analysis on challenging and publicly available datasets reveals that the proposed re-ranking scheme performs better than the existing methods. (C) 2018 Elsevier B.V. All rights reserved.","keywords_author":["Hypergraph-based learning","Post-rank optimization","Person re-identification","Rank classification"],"keywords_other":["FEATURES","SEARCH","CLASSIFICATION","DISTANCE","RECOGNITION","CAMERA","TRENDS"],"max_cite":0.0,"pub_year":2018.0,"sources":"['ieee', 'wos']","rawkeys":["recognition","rank classification","search","features","person re-identification","distance","post-rank optimization","camera","classification","trends","hypergraph-based learning"],"tags":["recognition","rank classification","search","features","person re-identification","distance","cameras","post-rank optimization","classification","trends","hypergraph-based learning"]},{"p_id":114048,"title":"Deep cross residual network for HEp-2 cell staining pattern classification","abstract":"Many computer-aided systems have been developed for Human epithelial type 2 (HEp-2) cell classification recently, but there is still a big performance gap between them and specialist doctors. Inspired by the recent successes of convolutional neural network, we proposed a deep cross residual network (DCR-Net) for HEp-2 cell classification. A cross connection based residual block was proposed to increase the information flow among different network layers. We used two benchmark datasets to evaluate our system. The state-of-art results, i.e. the average class accuracy of 80.8% in the International Conference on Pattern Recognition (ICPR) 2012 dataset and the mean class accuracy of 85.1% in the Indirect Immunofluorescence Image (I3A) dataset, were achieved. Our result on the ICPR 2012 dataset is so far the best among all works reported in the literature. Our algorithm was winner of the most recent ICPR 2016 contest and the accuracy beat all of the top performers in the previous International Conference on Image Processing (ICIP) 2013 and the ICPR 2014 contests. (C) 2018 Elsevier Ltd. All rights reserved.","keywords_author":["Convolutional neural network","Cross connection","Deep cross residual network","HEp-2 classification"],"keywords_other":["FEATURES","SPACE","IMAGE CLASSIFICATION","RECOGNITION","SCALE","SPARSE REPRESENTATION"],"max_cite":0.0,"pub_year":2018.0,"sources":"['wos']","rawkeys":["recognition","hep-2 classification","features","space","sparse representation","scale","deep cross residual network","cross connection","convolutional neural network","image classification"],"tags":["recognition","hep-2 classification","features","space","sparse representation","scale","deep cross residual network","cross connection","convolutional neural network","image classification"]},{"p_id":7555,"title":"Adaptive Resonance Theory: How a brain learns to consciously attend, learn, and recognize a changing world","abstract":"Adaptive Resonance Theory, or ART, is a cognitive and neural theory of how the brain autonomously learns to categorize, recognize, and predict objects and events in a changing world. This article reviews classical and recent developments of ART, and provides a synthesis of concepts, principles, mechanisms, architectures, and the interdisciplinary data bases that they have helped to explain and predict. The review illustrates that ART is currently the most highly developed cognitive and neural theory available, with the broadest explanatory and predictive range. Central to ART's predictive power is its ability to carry out fast, incremental, and stable unsupervised and supervised learning in response to a changing world. ART specifies mechanistic links between processes of consciousness, learning, expectation, attention, resonance, and synchrony during both unsupervised and supervised learning. ART provides functional and mechanistic explanations of such diverse topics as laminar cortical circuitry; invariant object and scenic gist learning and recognition; prototype, surface, and boundary attention; gamma and beta oscillations; learning of entorhinal grid cells and hippocampal place cells; computation of homologous spatial and temporal mechanisms in the entorhinal-hippocampal system; vigilance breakdowns during autism and medial temporal amnesia; cognitive-emotional interactions that focus attention on valued objects in an adaptively timed way; item-order-rank working memories and learned list chunks for the planning and control of sequences of linguistic, spatial, and motor information; conscious speech percepts that are influenced by future context; auditory streaming in noise during source segregation; and speaker normalization. Brain regions that are functionally described include visual and auditory neocortex; specific and nonspecific thalamic nuclei; inferotemporal, parietal, prefrontal, entorhinal, hippocampal, parahippocampal, perirhinal, and motor cortices; frontal eye fields; supplementary eye fields; amygdala; basal ganglia: cerebellum; and superior colliculus. Due to the complementary organization of the brain, ART does not describe many spatial and motor behaviors whose matching and learning laws differ from those of ART. ART algorithms for engineering and technology are listed, as are comparisons with other types of models. \u00a9 2012 Elsevier Ltd.","keywords_author":["Adaptive Resonance Theory","Adaptive timing","Amygdala","Attention","Basal ganglia","Consciousness","Entorhinal cortex","Expectation","Gamma and beta oscillations","Hippocampal cortex","Inferotemporal cortex","Learning","Parietal cortex","Prefrontal cortex","Recognition","Reinforcement learning","Speech perception","Synchrony","Working memory"],"keywords_other":["Speech perception","Working memories","Parietal cortex","Adaptive resonance theory","Entorhinal cortex","Expectation","Learning","Recognition","Attention","Inferotemporal cortex","Hippocampal cortex","Consciousness","Adaptive timing","Synchrony","Prefrontal cortex","Gamma and beta oscillations","Basal ganglia","Amygdala"],"max_cite":139.0,"pub_year":2013.0,"sources":"['ieee', 'scp', 'wos']","rawkeys":["amygdala","consciousness","parietal cortex","prefrontal cortex","expectation","reinforcement learning","adaptive timing","working memory","entorhinal cortex","basal ganglia","adaptive resonance theory","hippocampal cortex","recognition","learning","attention","synchrony","inferotemporal cortex","speech perception","working memories","gamma and beta oscillations"],"tags":["amygdala","consciousness","parietal cortex","prefrontal cortex","expectation","reinforcement learning","art","adaptive timing","machine learning","entorhinal cortex","basal ganglia","hippocampal cortex","recognition","attention","synchrony","inferotemporal cortex","speech perception","working-memory","gamma and beta oscillations"]},{"p_id":89529,"title":"Incorporation of Multiple-Days Information to Improve the Generalization of EEG-Based Emotion Recognition Over Time","abstract":"Current studies have got a series of satisfying accuracies in EEG-based emotion classification, but most of the classifiers used in previous studies are totally time-limited. To produce generalizable results, the emotion classifier should be stable over days, in which the day-to-day variations of EEG should be appropriately handled. To improve the generalization of EEG-based emotion recognition over time by learning multiple-days information which embraces the day-to-day variations, in this paper, 17 subjects were recruited to view several video clips to experience different emotion states, and each subject was required to perform five sessions in 5 days distributed over 1 month. Support vector machine was built to perform a classification, in which the training samples may come from 1, 2, 3, or 4 days' sessions but have a same number, termed learning 1-days information (L1DI), learning 2-days information (L2DI), learning 3-days information (L3DI), and learning 4-days information (L4DI) conditions, respectively. The results revealed that the EEG variability could impair the performance of emotion classifier dramatically, and learning more days' information to construct a classifier could significantly improve the generalization of EEG-based emotion recognition over time. Mean accuracies were 62.78, 67.92, 70.75, and 72.50% at L1DI, L2DI, L3DI, and L4DI conditions, respectively. Features at L4DI condition were ranked by modified RFE, and features providing better contribution were applied to obtain the performances of all conditions, results showed that the performance of SVMs trained and tested with the feature subset were all improved for L1DI, L2DI (*p < 0.05), L3DI (**p < 0.01), and L4DI (*p < 0.05) conditions. It could be a substantial step forward in the development of emotion recognition from EEG signals because it may enable a classifier trained on one time to handle another.","keywords_author":["emotion","electroencephalogram (EEG)","generalization","emotion recognition","day-to-day variations"],"keywords_other":["PHYSIOLOGICAL SIGNALS","CLASSIFICATION","FEATURES","TEST-RETEST RELIABILITY"],"max_cite":0.0,"pub_year":2018.0,"sources":"['wos']","rawkeys":["emotion","features","physiological signals","generalization","test-retest reliability","emotion recognition","classification","electroencephalogram (eeg)","day-to-day variations"],"tags":["recognition","emotion","features","physiological signals","eeg","test-retest reliability","emotion recognition","classification","day-to-day variations"]},{"p_id":64961,"title":"A Fast SVD-Hidden-nodes based Extreme Learning Machine for Large-Scale Data Analytics","abstract":"Big dimensional data is a growing trend that is emerging in many real world contexts, extending from web mining, gene expression analysis, protein-protein interaction to high-frequency financial data. Nowadays, there is a growing consensus that the increasing dimensionality poses impeding effects on the performances of classifiers, which is termed as the \"peaking phenomenon'' in the field of machine intelligence. To address the issue, dimensionality reduction is commonly employed as a preprocessing step on the Big dimensional data before building the classifiers. In this paper, we propose an Extreme Learning Machine (ELM) approach for large-scale data analytic. In contrast to existing approaches, we embed hidden nodes that are designed using singular value decomposition (SVD) into the classical ELM. These SVD nodes in the hidden layer are shown to capture the underlying characteristics of the Big dimensional data well, exhibiting excellent generalization performances. The drawback of using SVD on the entire dataset, however, is the high computational complexity involved. To address this, a fast divide and conquer approximation scheme is introduced to maintain computational tractability on high volume data. The resultant algorithm proposed is labeled here as Fast Singular Value Decomposition-Hidden-nodes based Extreme Learning Machine or FSVD-H-ELM in short. In FSVD-H-ELM, instead of identifying the SVD hidden nodes directly from the entire dataset, SVD hidden nodes are derived from multiple random subsets of data sampled from the original dataset. Comprehensive experiments and comparisons are conducted to assess the FSVD-H-ELM against other state-of-the-art algorithms. The results obtained demonstrated the superior generalization performance and efficiency of the FSVD-H-ELM. (C) 2016 Elsevier Ltd. All rights reserved.","keywords_author":["Extreme Learning Machine","Singular value decomposition","Big data","Big dimensional data","Fast approximation method"],"keywords_other":["REGRESSION","SETS","SYSTEMS","CLASSIFICATION","NEURAL-NETWORKS","RECOGNITION","STOCHASTIC GRADIENT DESCENT","DIMENSIONAL FEATURE-SELECTION"],"max_cite":9.0,"pub_year":2016.0,"sources":"['ieee', 'wos']","rawkeys":["dimensional feature-selection","fast approximation method","neural-networks","recognition","big data","singular value decomposition","classification","sets","stochastic gradient descent","systems","extreme learning machine","regression","big dimensional data"],"tags":["dimensional feature-selection","fast approximation method","recognition","big data","neural networks","singular value decomposition","system","classification","sets","stochastic gradient descent","extreme learning machine","regression","big dimensional data"]},{"p_id":7650,"title":"Deep feature learning with relative distance comparison for person re-identification","abstract":"\u00a9 2015 Elsevier Ltd. All rights reserved.Identifying the same individual across different scenes is an important yet difficult task in intelligent video surveillance. Its main difficulty lies in how to preserve similarity of the same person against large appearance and structure variation while discriminating different individuals. In this paper, we present a scalable distance driven feature learning framework based on the deep neural network for person re-identification, and demonstrate its effectiveness to handle the existing challenges. Specifically, given the training images with the class labels (person IDs), we first produce a large number of triplet units, each of which contains three images, i.e. one person with a matched reference and a mismatched reference. Treating the units as the input, we build the convolutional neural network to generate the layered representations, and follow with the L2 distance metric. By means of parameter optimization, our framework tends to maximize the relative distance between the matched pair and the mismatched pair for each triplet unit. Moreover, a nontrivial issue arising with the framework is that the triplet organization cubically enlarges the number of training triplets, as one image can be involved into several triplet units. To overcome this problem, we develop an effective triplet generation scheme and an optimized gradient descent algorithm, making the computational load mainly depend on the number of original images instead of the number of triplets. On several challenging databases, our approach achieves very promising results and outperforms other state-of-the-art approaches.","keywords_author":["Deep learning","Distance comparison","Person re-identification","Person re-identification","Deep learning","Distance comparison"],"keywords_other":["Deep learning","Intelligent video surveillance","Person re identifications","Parameter optimization","Distance comparison","State-of-the-art approach","RECOGNITION","Layered representation","Convolutional neural network"],"max_cite":133.0,"pub_year":2015.0,"sources":"['ieee', 'scp', 'wos']","rawkeys":["recognition","distance comparison","person re identifications","deep learning","intelligent video surveillance","person re-identification","layered representation","convolutional neural network","parameter optimization","state-of-the-art approach"],"tags":["recognition","distance comparison","intelligent video surveillance","machine learning","person re-identification","layered representation","convolutional neural network","parameter optimization","state-of-the-art approach"]},{"p_id":32229,"title":"Robust learning intrusion detection for attacks on wireless networks","abstract":"We address the problem of evaluating the robustness of machine learning based detectors for deployment in real life networks. To this end, we employ Genetic Programming for evolving classifiers and Artificial Neural Networks as our machine learning paradigms under three different Denial-of-Service attacks at the Data Link layer (De-authentication, Authentication and Association attacks). We investigate their cross-platform robustness and cross-attack robustness. Cross-platform robustness is the ability to seamlessly port an Intrusion Detector trained on one network to another network with little or no change and without a drop in performance. Cross-attack robustness is the ability of a detector trained on one attack type to detect a different but similar attack on which it has not been trained. Our results show that the potential of a machine learning based detector can be significantly enhanced or limited by the representation of the training data for the learning algorithms. \u00a9 2011-IOS Press and the authors. All rights reserved.","keywords_author":["dependability","Intrusion detection","machine learning","robustness","wireless networks"],"keywords_other":["Training data","Robust learning","Cross-platform","Denial of service attacks","Machine-learning","Real-life networks","Intrusion detectors","Data link layer","dependability"],"max_cite":3.0,"pub_year":2011.0,"sources":"['scp']","rawkeys":["real-life networks","robustness","robust learning","intrusion detection","machine learning","denial of service attacks","intrusion detectors","data link layer","machine-learning","wireless networks","dependability","training data","cross-platform"],"tags":["real-life networks","robustness","recognition","robust learning","intrusion detection systems","machine learning","denial of service attacks","intrusion detectors","data link layer","wireless networks","training data","cross-platform"]},{"p_id":15851,"title":"Can you tell who i am? Neuroticism, extraversion, and online self-presentation among young adults","abstract":"The present study examined the link between neuroticism, extraversion, as well as presentation of the real, the ideal, and the false self on Facebook. Self-reports were collected from 261 young adults (ages 18-30) about personality, online self-presentation, and Facebook use. Level of extraversion was positively associated with Facebook activity level. A series of regression analyses revealed that young adults high in neuroticism reported presenting their ideal and false self on Facebook to a greater extent whereas those low in extraversion reported engaging in greater online self-exploratory behaviors. Findings suggest that young adults who are experiencing emotional instability may be strategic in their online self-presentation perhaps to seek reassurance, and those who have self-doubt further explore their self online. \u00a9 2014 Elsevier Ltd. All rights reserved.","keywords_author":["Extraversion","Facebook","Neuroticism","Personality","Self-presentation","Social networking sites"],"keywords_other":null,"max_cite":41.0,"pub_year":2014.0,"sources":"['scp']","rawkeys":["self-presentation","facebook","neuroticism","personality","extraversion","social networking sites"],"tags":["self-presentation","facebook","recognition","neuroticism","social networking sites","personalizations"]},{"p_id":56811,"title":"Deep Adaptive Log-Demons: Diffeomorphic Image Registration with Very Large Deformations","abstract":"This paper proposes a new framework for capturing large and complex deformation in image registration. Traditionally, this challenging problem relies firstly on a preregistration, usually an affine matrix containing rotation, scale, and translation and afterwards on a nonrigid transformation. According to preregistration, the directly calculated affine matrix, which is obtained by limited pixel information, may misregistrate when large biases exist, thus misleading following registration subversively. To address this problem, for two-dimensional (2D) images, the two-layer deep adaptive registration framework proposed in this paper firstly accurately classifies the rotation parameter through multilayer convolutional neural networks (CNNs) and then identifies scale and translation parameters separately. For three-dimensional (3D) images, affine matrix is located through feature correspondences by a triplanar 2D CNNs. Then deformation removal is done iteratively through preregistration and demons registration. By comparison with the state-of-the-art registration framework, our method gains more accurate registration results on both synthetic and real datasets. Besides, principal component analysis (PCA) is combined with correlation like Pearson and Spearman to form new similarity standards in 2D and 3D registration. Experiment results also show faster convergence speed.","keywords_author":null,"keywords_other":["CONVOLUTIONAL NEURAL-NETWORK","RECOGNITION","ROBUST"],"max_cite":1.0,"pub_year":2015.0,"sources":"['wos']","rawkeys":["convolutional neural-network","recognition","robust"],"tags":["robustness","recognition","convolutional neural network"]},{"p_id":65004,"title":"Identification of Invariant Sensorimotor Structures as a Prerequisite for the Discovery of Objects","abstract":"Perceiving the surrounding environment in terms of objects is useful for any general purpose intelligent agent. In this paper, we investigate a fundamental mechanism making object perception possible, namely the identification of spatio-temporally invariant structures in the sensorimotor experience of an agent. We take inspiration from the Sensorimotor Contingencies Theory to define a computational model of this mechanism through a sensorimotor, unsupervised and predictive approach. Our model is based on processing the unsupervised interaction of an artificial agent with its environment. We show how spatio-temporally invariant structures in the environment induce regularities in the sensorimotor experience of an agent, and how this agent, while building a predictive model of its sensorimotor experience, can capture them as densely connected subgraphs in a graph of sensory states connected by motor commands. Our approach is focused on elementary mechanisms, and is illustrated with a set of simple experiments in which an agent interacts with an environment. We show how the agent can build an internal model of moving but spatio-temporally invariant structures by performing a Spectral Clustering of the graph modeling its overall sensorimotor experiences. We systematically examine properties of the model, shedding light more globally on the specificities of the paradigm with respect to methods based on the supervised processing of collections of static images.","keywords_author":["object perception","sensorimotor contingencies theory","unsupervised learning","predictive coding","grounding problem"],"keywords_other":["DEEP","REPRESENTATION","PRINCIPLE","RECOGNITION","BRAIN"],"max_cite":0.0,"pub_year":2018.0,"sources":"['ieee', 'wos']","rawkeys":["recognition","deep","grounding problem","predictive coding","principle","representation","brain","unsupervised learning","sensorimotor contingencies theory","object perception"],"tags":["recognition","principles","deep","grounding problem","predictive coding","representation","brain","unsupervised learning","sensorimotor contingencies theory","object perception"]},{"p_id":114162,"title":"Nanogenerator-based dual-functional and self-powered thin patch loudspeaker or microphone for flexible electronics","abstract":"Ferroelectret nanogenerators were recently introduced as a promising alternative technology for harvesting kinetic energy. Here we report the device's intrinsic properties that allow for the bidirectional conversion of energy between electrical and mechanical domains; thus extending its potential use in wearable electronics beyond the power generation realm. This electromechanical coupling, combined with their flexibility and thin film-like form, bestows dual-functional transducing capabilities to the device that are used in this work to demonstrate its use as a thin, wearable and self-powered loudspeaker or microphone patch. To determine the device's performance and applicability, sound pressure level is characterized in both space and frequency domains for three different configurations. The confirmed device's high performance is further validated through its integration in three different systems: a music-playing flag, a sound recording film and a flexible microphone for security applications.","keywords_author":null,"keywords_other":["WEARABLE DEVICES","SPEAKER","TRIBOELECTRIC NANOGENERATOR","FILM TRANSISTORS","NETWORKS","DISPLAY","RECOGNITION","PERFORMANCE","MOTION","BIOMECHANICAL ENERGY"],"max_cite":17.0,"pub_year":2017.0,"sources":"['wos']","rawkeys":["wearable devices","recognition","performance","motion","biomechanical energy","networks","speaker","triboelectric nanogenerator","display","film transistors"],"tags":["wearable devices","recognition","displays","performance","motion","biomechanical energy","networks","speaker","triboelectric nanogenerator","film transistors"]},{"p_id":114165,"title":"Increasing the robustness of CNN acoustic models using autoregressive moving average spectrogram features and channel dropout","abstract":"Developing automatic speech recognition systems that are robust to mismatched and noisy channel conditions is a challenging problem, especially when the training and the test conditions are different. Here, we seek to increase the robustness of convolutional neural network (CNN) acoustic models under such circumstances by combining two methods. Firstly, we propose an improved version of input dropout, which exploits the special structure of the input time-frequency representation. Instead of just dropping out random 'pixels' of the spectrogram, the proposed channel dropout approach discards whole spectral channels. We expect that this dropout strategy will force the network to rely less on the whole spectrum, and make it more robust to channel mismatches and narrow-band noise. Secondly, we replaced the standard mel-spectrogram input representation with the autoregressive moving average (ARMA) spectrogram, which was recently shown to outperform the former under mismatched train-test conditions. In our experiments on the Aurora-4 database, the proposed channel dropout method attained relative word error rate reductions of 16% with ARMA features (an absolute improvement of 3%), and 20% with FBANK features (an absolute improvement of 7%) over the baseline CNN, when using the clean training scenario. (C) 2017 Elsevier B.V. All rights reserved.","keywords_author":["Convolutional neural network","Input dropout","ARMA spectrogram","Aurora-4"],"keywords_other":["NEURAL-NETWORKS","RECOGNITION","SPEECH"],"max_cite":0.0,"pub_year":2017.0,"sources":"['wos']","rawkeys":["neural-networks","input dropout","recognition","aurora-4","convolutional neural network","speech","arma spectrogram"],"tags":["input dropout","recognition","neural networks","aurora-4","convolutional neural network","speech","arma spectrogram"]},{"p_id":65014,"title":"Automatic large-scale classification of bird sounds is strongly improved by unsupervised feature learning","abstract":"Automatic species classification of birds from their sound is a computational tool of increasing importance in ecology, conservation monitoring and vocal communication studies. To make classification useful in practice, it is crucial to improve its accuracy while ensuring that it can run at big data scales. Many approaches use acoustic measures based on spectrogram-type data, such as the Mel-frequency cepstral coefficient (MFCC) features which represent a manually-designed summary of spectral information. However, recent work in machine learning has demonstrated that features learnt automatically from data can often outperform manually-designed feature transforms. Feature learning can be performed at large scale and \"unsupervised\", meaning it requires no manual data labelling, yet it can improve performance on \"supervised\" tasks such as classification. In this work we introduce a technique for feature learning from large volumes of bird sound recordings, inspired by techniques that have proven useful in other domains. We experimentally compare twelve different feature representations derived from the Mel spectrum (of which six use this technique), using four large and diverse databases of bird vocalisations, classified using a random forest classifier. We demonstrate that in our classification tasks, MFCCs can often lead to worse performance than the raw Mel spectral data from which they are derived. Conversely, we demonstrate that unsupervised feature learning provides a substantial boost over MFCCs and Mel spectra without adding computational complexity after the model has been trained. The boost is particularly notable for single-label classification tasks at large scale. The spectro-temporal activations learned through our procedure resemble spectro-temporal receptive fields calculated from avian primary auditory forebrain. However, for one of our datasets, which contains substantial audio data but few annotations, increased performance is not discernible. We study the interaction between dataset characteristics and choice of feature representation through further empirical analysis.","keywords_author":["Bioacoustics","Machine learning","Birds","Classification","Vocalisation","Birdsong"],"keywords_other":["REPRESENTATION","RECOGNITION"],"max_cite":55.0,"pub_year":2014.0,"sources":"['ieee', 'wos']","rawkeys":["birds","bioacoustics","birdsong","recognition","machine learning","representation","classification","vocalisation"],"tags":["birds","bioacoustics","birdsong","recognition","machine learning","representation","classification","vocalisation"]},{"p_id":114171,"title":"Image Intelligent Detection Based on the Gabor Wavelet and the Neural Network","abstract":"This paper first analyzes the one-dimensional Gabor function and expands it to a two-dimensional one. The two-dimensional Gabor function generates the two-dimensional Gabor wavelet through measure stretching and rotation. At last, the two-dimensional Gabor wavelet transform is employed to extract the image feature information. Based on the back propagation (BP) neural network model, the image intelligent test model based on the Gabor wavelet and the neural network model is built. The human face image detection is adopted as an example. Results suggest that, although there are complex textures and illumination variations on the images of the face database named AT&T, the detection accuracy rate of the proposed method can reach above 0.93. In addition, extensive simulations based on the Yale and extended Yale B datasets further verify the effectiveness of the proposed method.","keywords_author":["Gabor wavelet","feature information","neural network","face recognition"],"keywords_other":["RECOGNITION"],"max_cite":1.0,"pub_year":2016.0,"sources":"['wos']","rawkeys":["neural network","feature information","recognition","gabor wavelet","face recognition"],"tags":["recognition","feature information","neural networks","face recognition","gabor wavelets"]},{"p_id":114173,"title":"An SVM-AdaBoost-based face detection system","abstract":"Face detection is the first significant step in face recognition and many computer vision applications. The goal of this work was to improve detection accuracy as well as reducing the execution time. Images are pre-processed, scaled and normalised with the discrete cosine transform. Gabor feature extraction techniques were employed to extract thousands of facial vectors. An AdaBoost-based feature selection tool was formulated to select a few hundreds of the Gabor wavelets. These vectors representing significant salient local features are used as input vectors to a support vector machine classifier. The classifier is trained and becomes capable of detecting faces. A detection rate of 97.6% with acceptable false positives was registered with a test set of 507 faces. The execution time of a pixel of size 320x240 is 0.0285s, which is very promising. A comparative evaluation of receiver operating characteristic (ROC) curves of different detectors on FDDB set shows that the proposed method is very effective.","keywords_author":["support vector machine","face detection","discrete cosine transform","Gabor filter","AdaBoost"],"keywords_other":["RECOGNITION","FEATURES"],"max_cite":1.0,"pub_year":2014.0,"sources":"['wos']","rawkeys":["discrete cosine transform","recognition","gabor filter","features","face detection","support vector machine","adaboost"],"tags":["discrete cosine transform","recognition","gabor filter","features","machine learning","face detection","adaboost"]},{"p_id":89599,"title":"Seabird image identification in natural scenes using Grabcut and combined features","abstract":"This paper proposes an automated seabird segmentation and identification method that applies to seabird images taken in natural scenes with a non-uniform and complex background. A variety of different bird postures appeared in natural scenes present different features from different points of view, even for the same posture. At first, the Grabcut method is introduced to segment seabird unit from a complicated background. Then, global features, namely the colour, shape and texture characteristics, and local features are integrated to describe the birds regarding various postures. Later, a combined recognition model, which is built using the k-Nearest Neighbor, Logistic Boost and Random Forest models by a voting mechanism that is designed for seabird identification. Finally, the efficiency and effectiveness of the proposed method in recognising seabirds were experimentally demonstrated. The experimental results on 900 field samples (6 seabird species, and 150 samples of each species) achieved a recognition accuracy of 88.1%, which indicates that the proposed method is effective for automated seabird identification. (C) 2016 Elsevier B.V. All rights reserved.","keywords_author":["Seabird identification","Natural scene","Grabcut segmentation","Combined features","Voting"],"keywords_other":["RANDOM FORESTS","CLASSIFICATION","RECOGNITION","DESCRIPTORS","INDICATORS"],"max_cite":1.0,"pub_year":2016.0,"sources":"['wos']","rawkeys":["grabcut segmentation","recognition","seabird identification","indicators","natural scene","random forests","classification","descriptors","combined features","voting"],"tags":["grabcut segmentation","natural scenes","recognition","seabird identification","indicators","random forests","classification","descriptors","combined features","voting"]},{"p_id":65024,"title":"SkyNet: an efficient and robust neural network training tool for machine learning in astronomy","abstract":"We present the first public release of our generic neural network training algorithm, called SkyNet. This efficient and robust machine learning tool is able to train large and deep feed-forward neural networks, including autoencoders, for use in a wide range of supervised and unsupervised learning applications, such as regression, classification, density estimation, clustering and dimensionality reduction. SkyNet uses a 'pre-training' method to obtain a set of network parameters that has empirically been shown to be close to a good solution, followed by further optimization using a regularized variant of Newton's method, where the level of regularization is determined and adjusted automatically; the latter uses second-order derivative information to improve convergence, but without the need to evaluate or store the full Hessian matrix, by using a fast approximate method to calculate Hessian-vector products. This combination of methods allows for the training of complicated networks that are difficult to optimize using standard backpropagation techniques. SkyNet employs convergence criteria that naturally prevent overfitting, and also includes a fast algorithm for estimating the accuracy of network outputs. The utility and flexibility of SkyNet are demonstrated by application to a number of toy problems, and to astronomical problems focusing on the recovery of structure from blurred and noisy images, the identification of gamma-ray bursters, and the compression and denoising of galaxy images. The SkyNet software, which is implemented in standard ANSI c and fully parallelized using MPI, is available at http:\/\/www.mrao.cam.ac.uk\/software\/skynet\/.","keywords_author":["methods: data analysis","methods: statistical"],"keywords_other":["DEEP","NETS","CLASSIFICATION","COSMOLOGICAL PARAMETER-ESTIMATION","RECOGNITION"],"max_cite":27.0,"pub_year":2014.0,"sources":"['ieee', 'wos']","rawkeys":["methods: statistical","recognition","deep","classification","nets","cosmological parameter-estimation","methods: data analysis"],"tags":["methods: statistical","recognition","deep","classification","nets","cosmological parameter-estimation","methods: data analysis"]},{"p_id":65025,"title":"A method for named entity normalization in biomedical articles: application to diseases and plants","abstract":"Background: In biomedical articles, a named entity recognition (NER) technique that identifies entity names from texts is an important element for extracting biological knowledge from articles. After NER is applied to articles, the next step is to normalize the identified names into standard concepts (i.e., disease names are mapped to the National Library of Medicine's Medical Subject Headings disease terms). In biomedical articles, many entity normalization methods rely on domain-specific dictionaries for resolving synonyms and abbreviations. However, the dictionaries are not comprehensive except for some entities such as genes. In recent years, biomedical articles have accumulated rapidly, and neural network-based algorithms that incorporate a large amount of unlabeled data have shown considerable success in several natural language processing problems.","keywords_author":["Text mining","Named entity recognition","Entity name normalization","Disease names","Plant names","Neural networks"],"keywords_other":["RECOGNITION","TEXT"],"max_cite":0.0,"pub_year":2017.0,"sources":"['wos']","rawkeys":["recognition","text mining","disease names","neural networks","entity name normalization","named entity recognition","text","plant names"],"tags":["recognition","text mining","disease names","neural networks","entity name normalization","named entity recognition","text","plant names"]},{"p_id":65026,"title":"Image Segmentation for Fruit Detection and Yield Estimation in Apple Orchards","abstract":"Ground vehicles equipped with monocular vision systems are a valuable source of high-resolution image data for precision agriculture applications in orchards. This paper presents an image processing framework for fruit detection and counting using orchard image data. A general-purpose image segmentation approach is used, including two feature learning algorithms; multiscale multilayered perceptrons (MLP) and convolutional neural networks (CNN). These networks were extended by including contextual information about how the image data was captured (metadata), which correlates with some of the appearance variations and\/or class distributions observed in the data. The pixel-wise fruit segmentation output is processed using the watershed segmentation (WS) and circular Hough transform (CHT) algorithms to detect and count individual fruits. Experiments were conducted in a commercial apple orchard near Melbourne, Australia. The results show an improvement in fruit segmentation performance with the inclusion of metadata on the previously benchmarked MLP network. We extend this work with CNNs, bringing agrovision closer to the state-of-the-art in computer vision, where although metadata had negligible influence, the best pixel-wise F1-score of 0.791 was achieved. The WS algorithm produced the best apple detection and counting results, with a detection F1-score of 0.861. As a final step, image fruit counts were accumulated over multiple rows at the orchard and compared against the post-harvest fruit counts that were obtained from a grading and counting machine. The count estimates using CNN and WS resulted in the best performance for this data set, with a squared correlation coefficient of r(2) = 0.826. (C) 2017 Wiley Periodicals, Inc.","keywords_author":null,"keywords_other":["MANGO","MACHINE","VISION","COLOR","HARVESTING ROBOT","RECOGNITION","NIGHT","ILLUMINATION"],"max_cite":5.0,"pub_year":2017.0,"sources":"['ieee', 'wos']","rawkeys":["illumination","recognition","machine","night","vision","color","mango","harvesting robot"],"tags":["illumination","recognition","machine","night","vision","color","mango","harvesting robot"]},{"p_id":89602,"title":"A survey on image-based insect classification","abstract":"Entomology has had many applications in many biological domains (i.e insect counting as a biodiversity index). To meet a growing biological demand and to compensate a decreasing workforce amount, automated entomology has been around for decades. This challenge has been tackled by computer scientists as well as by biologists themselves. This survey investigates fourty-four studies on this topic and tries to give a global picture on what are the scientific locks and how the problem was addressed. Views are adopted on image capture, feature extraction, classification methods and the tested datasets. A general discussion is finally given on the questions that might still remain unsolved such as: the image capture conditions mandatory to good recognition performance, the definition of the problem and whether computer scientist should consider it as a problem in its own or just as an instance of a wider image recognition problem.","keywords_author":["Image-based insect recognition","Classification","Automated entomology","Arthropods","Biology","Recognition","Image"],"keywords_other":["ASSEMBLAGES","INDICATORS","REPRESENTATION","MACHINE VISION","BIODIVERSITY","SYSTEM","NEURAL-NETWORKS","ARTHROPODS","AUTOMATED SPECIES IDENTIFICATION","ENVIRONMENT"],"max_cite":4.0,"pub_year":2016.0,"sources":"['ieee', 'wos']","rawkeys":["automated entomology","neural-networks","automated species identification","recognition","image-based insect recognition","arthropods","indicators","machine vision","representation","system","image","classification","environment","biodiversity","biology","assemblages"],"tags":["automated entomology","automated species identification","images","image-based insect recognition","recognition","neural networks","arthropods","indicators","machine vision","representation","system","classification","environment","biological","biodiversity","assemblages"]},{"p_id":65031,"title":"Role of zero synapses in unsupervised feature learning","abstract":"Synapses in real neural circuits can take discrete values including zero (silent or potential) synapses. The computational role of zero synapses in unsupervised feature learning of unlabeled noisy data is still unclear, thus it is important to understand how the sparseness of synaptic activity is shaped during learning and its relationship with receptive field formation. Here, we formulate this kind of sparse feature learning by a statistical mechanics approach. We find that learning decreases the fraction of zero synapses, and when the fraction decreases rapidly around a critical data size, an intrinsically structured receptive field starts to develop. Further increasing the data size refines the receptive field, while a very small fraction of zero synapses remain to act as contour detectors. This phenomenon is discovered not only in learning a handwritten digits dataset, but also in learning retinal neural activity measured in a natural-movie-stimuli experiment.","keywords_author":["unsupervised learning","statistical mechanics","phase transition"],"keywords_other":["CELL","RECOGNITION","POPULATION","CODE"],"max_cite":0.0,"pub_year":2018.0,"sources":"['wos']","rawkeys":["phase transition","recognition","population","cell","unsupervised learning","statistical mechanics","code"],"tags":["cells","recognition","codes","population","phase transitions","unsupervised learning","statistical mechanics"]},{"p_id":32301,"title":"Incremental on-line learning: A review and comparison of state of the art algorithms","abstract":"\u00a9 2017 Elsevier B.V. Recently, incremental and on-line learning gained more attention especially in the context of big data and learning from data streams, conflicting with the traditional assumption of complete data availability. Even though a variety of different methods are available, it often remains unclear which of them is suitable for a specific task and how they perform in comparison to each other. We analyze the key properties of eight popular incremental methods representing different algorithm classes. Thereby, we evaluate them with regards to their on-line classification error as well as to their behavior in the limit. Further, we discuss the often neglected issue of hyperparameter optimization specifically for each method and test how robustly it can be done based on a small set of examples. Our extensive evaluation on data sets with different characteristics gives an overview of the performance with respect to accuracy, convergence speed as well as model complexity, facilitating the choice of the best method for a given application.","keywords_author":["Data streams","Hyperparameter optimization","Incremental learning","Model selection","On-line learning","Incremental learning","On-line learning","Data streams","Hyperparameter optimization","Model selection"],"keywords_other":["Model Selection","FEATURES","NETWORKS","CLASSIFIERS","MACHINE","CLASSIFICATION","Online learning","VIDEO SURVEILLANCE","RECOGNITION","Incremental learning","OBJECT TRACKING","DATA STREAMS","Data stream","Hyper-parameter optimizations"],"max_cite":3.0,"pub_year":2018.0,"sources":"['ieee', 'scp', 'wos']","rawkeys":["hyperparameter optimization","object tracking","recognition","features","hyper-parameter optimizations","data stream","incremental learning","machine","model selection","classifiers","networks","on-line learning","classification","video surveillance","online learning","data streams"],"tags":["recognition","object tracking","hyper-parameter optimizations","features","data stream","incremental learning","machine","multiple sclerosis","networks","classifier","classification","on-line learning","video surveillance","online learning"]},{"p_id":32312,"title":"Features for Masking-Based Monaural Speech Separation in Reverberant Conditions","abstract":"\u00a9 2014 IEEE. Monaural speech separation is a fundamental problem in speech and signal processing. This problem can be approached from a supervised learning perspective by predicting an ideal time-frequency mask from features of noisy speech. In reverberant conditions at low signal-to-noise ratios (SNRs), accurate mask prediction is challenging and can benefit from effective features. In this paper, we investigate an extensive set of acoustic-phonetic features extracted in adverse conditions. Deep neural networks are used as the learning machine, and separation performance is evaluated using standard objective speech intelligibility metrics. Separation performance is systematically evaluated in both nonspeech and speech interference, in a variety of SNRs, reverberation times, and direct-to-reverberant energy ratios. Considerable performance improvement is observed by using contextual information, likely due to temporal effects of room reverberation. In addition, we construct feature combination sets using a sequential floating forward selection algorithm, and combined features outperform individual ones. We also find that optimal feature sets in anechoic conditions are different from those in reverberant conditions.","keywords_author":["Deep neural networks","feature combination","monaural speech separation","room reverberation","speech intelligibility","Deep neural networks","feature combination","monaural speech separation","room reverberation","speech intelligibility"],"keywords_other":["PITCH ESTIMATION","Feature combination","Low signal-to-noise ratio","REGRESSION","Contextual information","Optimal feature sets","Reverberant condition","NOISE","HEARING-IMPAIRED LISTENERS","ALGORITHM","PERCEPTION","Room reverberations","RECOGNITION","Speech separation","Separation performance","DEEP NEURAL-NETWORKS","INTELLIGIBILITY","ROBUST"],"max_cite":4.0,"pub_year":2017.0,"sources":"['ieee', 'scp', 'wos']","rawkeys":["contextual information","deep neural-networks","pitch estimation","hearing-impaired listeners","low signal-to-noise ratio","separation performance","robust","intelligibility","regression","speech separation","algorithm","recognition","noise","optimal feature sets","monaural speech separation","speech intelligibility","room reverberation","feature combination","room reverberations","deep neural networks","reverberant condition","perception"],"tags":["robustness","intelligence","contextual information","convolutional neural network","pitch estimation","hearing-impaired listeners","low signal-to-noise ratio","separation performance","algorithms","regression","speech separation","recognition","noise","optimal feature sets","monaural speech separation","perceptions","speech intelligibility","room reverberation","feature combination","reverberant condition"]},{"p_id":7755,"title":"Incremental learning, clustering and hierarchy formation of whole body motion patterns using adaptive Hidden Markov Chains","abstract":"This paper describes a novel approach for autonomous and incremental learning of motion pattern primitives by observation of human motion. Human motion patterns are abstracted into a dynamic stochastic model, which can be used for both subsequent motion recognition and generation, analogous to the mirror neuron hypothesis in primates. The model size is adaptable based on the discrimination requirements in the associated region of the current knowledge base. A new algorithm for sequentially training the Markov chains is developed, to reduce the computation cost during model adaptation. As new motion patterns are observed, they are incrementally grouped together using hierarchical agglomerative clustering based on their relative distance in the model space. The clustering algorithm forms a tree structure, with specialized motions at the tree leaves, and generalized motions closer to the root. The generated tree structure will depend on the type of training data provided, so that the most specialized motions will be those for which the most training has been received. Tests with motion capture data for a variety of motion primitives demonstrate the efficacy of the algorithm. \u00a9 SAGE Publications 2008.","keywords_author":["Cognitive robotics","Computer vision","Gesture","Human-centred and life-like robotics","Humanoid robots","Learning and adaptive systems","Posture","Recognition","Sensing and perception","Social spaces and facial expressions"],"keywords_other":["New algorithm","Hidden Markov chains","relative distances","tree leaves","Incremental Learning","Whole-body motion","model adaptation","Hierarchical agglomerative clustering","Dynamic stochastic model","Sage Publications (CO)","Model size","Training data","Knowledge base (KB)","nove l approach","Model spaces","motion primitives","Motion recognition","tree structures","Markov chains (MC)","Observation of human motion","Motion capture data","Human motions","motion patterns","Computation costs"],"max_cite":128.0,"pub_year":2008.0,"sources":"['scp']","rawkeys":["human-centred and life-like robotics","tree leaves","relative distances","incremental learning","whole-body motion","dynamic stochastic model","model spaces","motion capture data","training data","model adaptation","observation of human motion","model size","hierarchical agglomerative clustering","hidden markov chains","learning and adaptive systems","markov chains (mc)","recognition","new algorithm","human motions","nove l approach","motion primitives","social spaces and facial expressions","tree structures","cognitive robotics","computer vision","posture","sage publications (co)","motion patterns","motion recognition","computation costs","sensing and perception","gesture","knowledge base (kb)","humanoid robots"],"tags":["human-centred and life-like robotics","tree leaves","relative distances","incremental learning","knowledge base","dynamic stochastic model","model spaces","whole-body motion","computational costs","motion capture data","motion pattern","training data","model adaptation","observation of human motion","markov chains","model size","hierarchical agglomerative clustering","hidden markov chains","learning and adaptive systems","recognition","new algorithm","human motions","nove l approach","motion primitives","social spaces and facial expressions","tree structures","humanoid robot","cognitive robotics","computer vision","posture","sage publications (co)","motion recognition","sensing and perception","gestures"]},{"p_id":48716,"title":"Autonomous exploration of mobile robots through deep neural networks","abstract":"\u00a9 The Author(s) 2017.The exploration problem of mobile robots aims to allow mobile robots to explore an unknown environment. We describe an indoor exploration algorithm for mobile robots using a hierarchical structure that fuses several convolutional neural network layers with decision-making process. The whole system is trained end to end by taking only visual information (RGB-D information) as input and generates a sequence of main moving direction as output so that the robot achieves autonomous exploration ability. The robot is a TurtleBot with a Kinect mounted on it. The model is trained and tested in a real world environment. And the training data set is provided for download. The outputs of the test data are compared with the human decision. We use Gaussian process latent variable model to visualize the feature map of last convolutional layer, which proves the effectiveness of this deep convolution neural network mode. We also present a novel and lightweight deep-learning library libcnn especially for deep-learning processing of robotics tasks.","keywords_author":["CNN","Deep learning","Robot exploration","Robot exploration","deep learning","CNN"],"keywords_other":["Depth cuboid similarity feature","Activity recognition","Space scale","Daily activity","Camera angles","MODEL","Feature representation","Large scale data sets","RECOGNITION","NAVIGATION","Data set"],"max_cite":0.0,"pub_year":2017.0,"sources":"['ieee', 'scp', 'wos']","rawkeys":["recognition","activity recognition","model","deep learning","camera angles","data set","daily activity","cnn","depth cuboid similarity feature","robot exploration","space scale","feature representation","navigation","large scale data sets"],"tags":["recognition","activity recognition","model","camera angles","daily activity","machine learning","large-scale datasets","data sets","depth cuboid similarity feature","convolutional neural network","space scale","feature representation","robotic exploration","navigation"]},{"p_id":32341,"title":"The robustness of face-based CAPTCHAs","abstract":"\u00a9 2015 IEEE. FaceDCAPTCHA and FR-CAPTCHA, proposed in 2014, are both face-based CAPTCHAs relying on human face recognition. The security of FaceDCAPTCHA is based on the difficulty of classifying real human faces and fake faces while the FR-CAPTCHA finds two faces belonging to the same person in a complex background. In this paper, edge detection is employed to obtain the small faces in FaceDCAPTCHA and then an SVM classifier is used to differentiate the images of real human faces and fake faces with color and texture, LBP, SIFT and Laws' Masks features extracted from the faces. The attack success rate of FaceDCAPTCHA is 48%. OpenCV is utilized to detect faces in FR-CAPTCHA and four features are extracted from the faces to find the most probability pair. The final attack success rate is 23%. In the end of the paper, an improved face-based CAPTCHA is proposed, which overcome the disadvantages of the two schemes. The preliminary attack results (less than 7%) demonstrated the security of the new scheme.","keywords_author":["CAPTCHA","Face","Recognition"],"keywords_other":["Human face recognition","Complex background","Color and textures","Recognition","Human faces","SVM classifiers","Face","CAPTCHAs"],"max_cite":3.0,"pub_year":2015.0,"sources":"['ieee', 'scp', 'wos']","rawkeys":["captchas","human faces","recognition","color and textures","svm classifiers","captcha","face","human face recognition","complex background"],"tags":["captchas","human faces","recognition","color and textures","svm classifiers","face","human face recognition","complex background"]},{"p_id":81494,"title":"Fault diagnosis method of rotating machinery based on stacked denoising autoencoder","abstract":"Based on the deficiency in the traditional fault diagnosis method of rotating machinery, i.e. shallow learning is usually used to characterize complex mapping relationship between vibration signals and the rotor system, a deep neural network (DNN) based on stacked denoising autoencoder (SDAE) is proposed. The proposed method has been successfully applied to the fault diagnosis of rotating machinery. In the proposed method, the frequency domain information of vibration signal is used as input signal, and the deep neural network is obtained by layer-by-layer feature extraction from denoising autoencoder (DAE). Then the dropout method is used to adjust the network parameters, and reduces the over-fitting phenomenon. In additional, the principal component analysis is used to extract fault features. The experiment result shows that the proposed method is very effective, and can effectively extract the hidden features in the vibration signal of rotating machinery.","keywords_author":["Stacked denoising autoencoder (SDAE)","deep learning","fault diagnosis","rotating machinery"],"keywords_other":["NEURAL-NETWORKS","RECOGNITION","DEEP","REPRESENTATIONS"],"max_cite":1.0,"pub_year":2018.0,"sources":"['wos']","rawkeys":["neural-networks","recognition","stacked denoising autoencoder (sdae)","deep learning","deep","representations","rotating machinery","fault diagnosis"],"tags":["recognition","neural networks","deep","machine learning","representation","rotating machinery","fault diagnosis","stacked denoising autoencoder"]},{"p_id":73304,"title":"Multi-task learning for dangerous object detection in autonomous driving","abstract":"Recently, autonomous driving has been extensively studied and has shown considerable promise. Vision-based dangerous object detection is a crucial technology of autonomous driving. In previous work, dangerous object detection is generally formulated as a typical object detection problem and a distance-based danger assessment problem, separately. These two problems are usually dealt with using two independent models. In fact, vision based object detection and distance prediction present prominent visual relationship. The objects with different distance to the camera have different attributes (pose, size and definition), which are very worthy to be exploited for dangerous object detection. However, these characteristics are usually ignored in previous work. In this paper, we propose a novel multi-task learning (MTL) method to jointly model object detection and distance prediction with a Cartesian product-based multi-task combination strategy. Furthermore, we mathematically prove that the proposed Cartesian product-based combination strategy is more optimal than the linear multi-task combination strategy that is usually used in MTL models, when the multi-task itself is not independent. Systematic experiments show that the proposed approach consistently achieves better object detection and distance prediction performances compared to both the single-task and multi-task dangerous object detection methods. (C) 2017 Elsevier Inc. All rights reserved.","keywords_author":["Dangerous object detection","Autonomous driving","Multi-task learning","Convolutional neural network"],"keywords_other":["FUSION","VEHICLES","RECOGNITION","SYSTEM"],"max_cite":1.0,"pub_year":2018.0,"sources":"['wos']","rawkeys":["multi-task learning","recognition","system","autonomous driving","vehicles","dangerous object detection","convolutional neural network","fusion"],"tags":["recognition","system","autonomous driving","multitask learning","vehicles","dangerous object detection","convolutional neural network","fusion"]},{"p_id":81515,"title":"Keep cool: Memory is retained during hibernation in Alpine marmots","abstract":"Hibernators display severe changes in brain structure during deep torpor, including alterations in synaptic constitution. To address a possible effect on long-term memory, we examined learning behavior and memory of the hibernator Marmota marmota. In two operant conditioning tasks, the marmots learned to jump on two boxes or to walk through a tube. The animals were trained during their active season. Performance improved during the training phase and remained stable in a last test, four weeks before entrance into hibernation. When retested after six months of hibernation, skills were found to be unimpaired (box: before hibernation: 258.2 +\/- 17.7 s, after hibernation: 275.0 +\/- 19.8 s: tube: before hibernation: 158.4 +\/- 9.0 s, after hibernation: 137.7 +\/- 6.3 s). Contrary to these findings, memory seemed to be less fixed during the active season, since changes in test procedure resulted in impaired test performance. Besides the operant conditioning, we investigated the animals' habituation to a novel environment by repeated open field exposure. In the first run, animals showed exploratory behavior and thus a high locomotor activity was observed (63.6 +\/- 10.7 crossed squares). Upon a second exposure, all animals immediately moved into one corner and locomotion ceased (7.2 +\/- 1.9 crossed squares). This habituation was not altered even after hibernation (6.1 +\/- 1.1 crossed squares). We thus conclude that long-term memory is unaffected by hibernation in Alpine marmots. (C) 2009 Elsevier Inc. All rights reserved.","keywords_author":["Marmota","Behavior","Learning, Operant conditioning","Positive reinforcement","Open held test","Familiarity detection","Locomotor activity"],"keywords_other":["EUROPEAN GROUND-SQUIRRELS","PLASTICITY","IN-VIVO","DENDRITIC MORPHOLOGY","CITELLUS","METABOLISM","DAILY TORPOR","RECOGNITION","TEMPERATURE","HYPOTHERMIA"],"max_cite":13.0,"pub_year":2009.0,"sources":"['wos']","rawkeys":["dendritic morphology","recognition","locomotor activity","in-vivo","citellus","learning","open held test","operant conditioning","plasticity","marmota","metabolism","positive reinforcement","temperature","daily torpor","european ground-squirrels","hypothermia","familiarity detection","behavior"],"tags":["dendritic morphology","recognition","in-vivo","citellus","machine learning","open held test","plasticity","marmota","metabolism","positive reinforcement","operating condition","daily torpor","european ground-squirrels","locomotor-activity","temperature","hypothermia","familiarity detection","behavior"]},{"p_id":7826,"title":"Facial expression recognition using constructive feedforward neural networks","abstract":"A new technique for facial expression recognition is proposed, which uses the two-dimensional (2-D) discrete cosine transform (DCT) over the entire face image as a feature detector and a constructive one-hidden-layer feedforward neural network as a facial expression classifier. An input-side pruning technique, proposed previously by the authors, is also incorporated into the constructive learning process to reduce the network size without sacrificing the performance of the resulting network. The proposed technique is applied to a database consisting of images of 60 men, each having five facial expression images (neutral, smile, anger, sadness, and surprise). Images of 40 men are used for network training, and the remaining images of 20 men are used for generalization and testing. Confusion matrices calculated in both network training and generalization for four facial expressions (smile, anger, sadness, and surprise) are used to evaluate the performance of the trained network. It is demonstrated that the best recognition rates are 100% and 93.75% (without rejection), for the training and generalizing images, respectively. Furthermore, the input-side weights of the constructed network are reduced by approximately 30% using our pruning method. In comparison with the fixed structure back propagation-based recognition methods in the literature, the proposed technique constructs one-hidden-layer feedforward neural network with fewer number of hidden units and weights, while simultaneously provide improved generalization and recognition performance capabilities.","keywords_author":["Constructive neural networks","Facial recognition","Generalization","Pruning strategies","Two-dimensional (2-D) discrete cosine transform"],"keywords_other":["Pruning strategies","Constructive neural networks"],"max_cite":124.0,"pub_year":2004.0,"sources":"['scp']","rawkeys":["constructive neural networks","pruning strategies","two-dimensional (2-d) discrete cosine transform","generalization","facial recognition"],"tags":["recognition","two-dimensional (2-d) discrete cosine transform","constructive neural network","facial recognition","pruning strategy"]},{"p_id":32421,"title":"Anomaly detection system using resource pattern learning","abstract":"In this paper, Anomaly Detection by Resource Monitoring (Ayaka), a novel lightweight anomaly and fault detection infrastructure, is presented for Information Appliances. Ayaka provides a general monitoring method for detecting anomalies using only resource usage information on systems independent of its domain, target application and programming languages. Ayaka modifies the kernel to detect faults and uses a completely application black-box approach based on machine learning methods. It uses the clustering method to quantize the resource usage vector data and learn the normal patterns with Hidden Markov Model. In the running phase, Ayaka finds anomalies by comparing the application resource usage with learned model. The evaluation experiment indicates that our prototype system is able to detect anomalies, such as SQL injection and buffer overrun, without significant overheads. \u00a9 2009 IEEE.","keywords_author":["Anomaly Detection","Dependability","Hidden Markov Model","Machine Learning"],"keywords_other":["Monitoring methods","Anomaly detection","Information appliances","Anomaly detection systems","Resource monitoring","Evaluation experiments","Dependability","Clustering methods"],"max_cite":3.0,"pub_year":2009.0,"sources":"['scp']","rawkeys":["resource monitoring","clustering methods","monitoring methods","anomaly detection","machine learning","anomaly detection systems","information appliances","evaluation experiments","hidden markov model","dependability"],"tags":["hidden markov models","recognition","clustering methods","monitoring methods","anomaly detection","resource monitoring","machine learning","anomaly detection systems","information appliances","evaluation experiments"]},{"p_id":97961,"title":"EEG-Based Detection of Braking Intention Under Different Car Driving Conditions","abstract":"The anticipatory recognition of braking is essential to prevent traffic accidents. For instance, driving assistance systems can be useful to properly respond to emergency braking situations. Moreover, the response time to emergency braking situations can be affected and even increased by different driver's cognitive states caused by stress, fatigue, and extra workload. This work investigates the detection of emergency braking from driver's electroencephalographic (EEG) signals that precede the brake pedal actuation. Bioelectrical signals were recorded while participants were driving in a car simulator while avoiding potential collisions by performing emergency braking. In addition, participants were subjected to stress, workload, and fatigue. EEG signals were classified using support vector machines (SVM) and convolutional neural networks (CNN) in order to discriminate between braking intention and normal driving. Results showed significant recognition of emergency braking intention which was on average 71.1% for SVM and 71.8% CNN. In addition, the classification accuracy for the best participant was 80.1 and 88.1% for SVM and CNN, respectively. These results show the feasibility of incorporating recognizable driver's bioelectrical responses into advanced driver-assistance systems to carry out early detection of emergency braking situations which could be useful to reduce car accidents.","keywords_author":["driving","braking","intention","electroencephalogram","detection","stress","workload","fatigue"],"keywords_other":["VISUAL INFORMATION","FATIGUE","ACCIDENTS","DRIVERS","CLASSIFICATION","SLEEPINESS","RECOGNITION","SIMULATOR","LEVEL","REACTION-TIME"],"max_cite":0.0,"pub_year":2018.0,"sources":"['wos']","rawkeys":["braking","reaction-time","recognition","simulator","visual information","accidents","level","stress","electroencephalogram","sleepiness","detection","classification","fatigue","intention","workload","driving","drivers"],"tags":["braking","recognition","visual information","reaction time","accidents","level","eeg","stress","sleepiness","detection","classification","fatigue","intention","workload","driving","drivers","simulation"]},{"p_id":16047,"title":"Support vector regression for improved real-time, simultaneous myoelectric control","abstract":"\u00a9 2014 IEEE. This study describes the first application of a support vector machine (SVM) based scheme for real-time simultaneous and proportional myoelectric control of multiple degrees of freedom (DOFs). Three DOFs including wrist flexion-extension, abduction-adduction and forearm pronation-supination were investigated with 10 able-bodied subjects and two individuals with transradial limb deficiency (LD). A Fitts' law test involving real-time target acquisition tasks was conducted to compare the usability of the SVM-based control system to that of an artificial neural network (ANN) based method. Performance was assessed using the Fitts' law throughput value as well as additional metrics including completion rate, path efficiency and overshoot. The SVM-based approach outperformed the ANN-based system in every performance measure (p < 0.05) for able-bodied subjects. The SVM outperformed the ANN in path efficiency and throughput with the first LD subject and in throughput with the second LD subject. The superior performance of the SVM-based system appears to be due to its higher estimation accuracy of all DOFs during inactive and low amplitude segments (these periods were frequent during real-time control). Another advantage of the SVM-based method was that it substantially reduced the processing time for both training and real time control.","keywords_author":["Amputee","electromyogram (EMG)","myoelectric control","simultaneous control","support vector machines"],"keywords_other":["Sensitivity and Specificity","Support Vector Machines","Humans","Reproducibility of Results","Movement","Computer Systems","Electromyography","Muscle, Skeletal","Pattern Recognition, Automated","Muscle Contraction","Data Interpretation, Statistical","Female","Myoelectric control","Multiple degrees of freedom","Wrist flexion-extension","Performance measure","Amputee","Male","Young Adult","Middle Aged","Simultaneous control","Support vector regression (SVR)","Adult","Regression Analysis","Electromyogram","Biofeedback, Psychology"],"max_cite":40.0,"pub_year":2014.0,"sources":"['scp', 'wos']","rawkeys":["statistical","automated","wrist flexion-extension","movement","regression analysis","young adult","adult","muscle","biofeedback","amputee","electromyography","performance measure","support vector regression (svr)","middle aged","muscle contraction","computer systems","psychology","sensitivity and specificity","electromyogram","reproducibility of results","humans","multiple degrees of freedom","myoelectric control","male","data interpretation","skeletal","support vector machines","simultaneous control","pattern recognition","female","electromyogram (emg)"],"tags":["automated","wrist flexion-extension","movement","regression analysis","young adult","adult","muscle","electromyography","performance measure","support vector regression (svr)","computational system","middle aged","machine learning","amputees","muscle contraction","sensitivity and specificity","recognition","reproducibility of results","humans","multiple degrees of freedom","myoelectric control","statistics","male","data interpretation","skeletal","simultaneous control","pattern recognition","female","biofeedback"]},{"p_id":40653,"title":"Unsupervised Domain Adaptation for Face Anti-Spoofing","abstract":"\u00a9 2005-2012 IEEE. Face anti-spoofing (a.k.a. presentation attack detection) has recently emerged as an active topic with great significance for both academia and industry due to the rapidly increasing demand in user authentication on mobile phones, PCs, tablets, and so on. Recently, numerous face spoofing detection schemes have been proposed based on the assumption that training and testing samples are in the same domain in terms of the feature space and marginal probability distribution. However, due to unlimited variations of the dominant conditions (illumination, facial appearance, camera quality, and so on) in face acquisition, such single domain methods lack generalization capability, which further prevents them from being applied in practical applications. In light of this, we introduce an unsupervised domain adaptation face anti-spoofing scheme to address the real-world scenario that learns the classifier for the target domain based on training samples in a different source domain. In particular, an embedding function is first imposed based on source and target domain data, which maps the data to a new space where the distribution similarity can be measured. Subsequently, the Maximum Mean Discrepancy between the latent features in source and target domains is minimized such that a more generalized classifier can be learned. State-of-the-art representations including both hand-crafted and deep neural network learned features are further adopted into the framework to quest the capability of them in domain adaptation. Moreover, we introduce a new database for face spoofing detection, which contains more than 4000 face samples with a large variety of spoofing types, capture devices, illuminations, and so on. Extensive experiments on existing benchmark databases and the new database verify that the proposed approach can gain significantly better generalization capability in cross-domain scenarios by providing consistently better anti-spoofing performance.","keywords_author":["domain adaptation","Face anti-spoofing","maximum mean discrepancy","Face anti-spoofing","domain adaptation","maximum mean discrepancy"],"keywords_other":["Generalization capability","Face spoofing detections","LIVENESS DETECTION","Anti-spoofing","REGULARIZATION","Maximum Mean Discrepancy","Real-world scenario","SINGLE IMAGE","Training and testing","DATABASE","RECOGNITION","Face","SCALE","Domain adaptation","IMAGE FEATURES"],"max_cite":1.0,"pub_year":2018.0,"sources":"['ieee', 'scp', 'wos']","rawkeys":["face spoofing detections","single image","image features","liveness detection","generalization capability","real-world scenario","recognition","face anti-spoofing","database","training and testing","anti-spoofing","domain adaptation","face","scale","regularization","maximum mean discrepancy"],"tags":["face spoofing detections","single images","image features","liveness detection","generalization capability","real-world scenario","databases","denoising autoencoder","recognition","training and testing","anti-spoofing","face anti-spoofing","face","scale","regularization","maximum mean discrepancy"]},{"p_id":7900,"title":"Robustness and regularization of support vector machines","abstract":"We consider regularized support vector machines (SVMs) and show that they are precisely equivalent to a new robust optimization formulation. We show that this equivalence of robust optimization and regularization has implications for both algorithms, and analysis. In terms of algorithms, the equivalence suggests more general SVM-like algorithms for classification that explicitly build in protection to noise, and at the same time control overfitting. On the analysis front, the equivalence of robustness and regularization provides a robust optimization interpretation for the success of regularized SVMs. We use this new robustness interpretation of SVMs to give a new proof of consistency of (kernelized) SVMs, thus establishing robustness as the reason regularized SVMs generalize well. \u00a9 2009 Huan Xu, Constantine Caramanis and Shie Mannor.","keywords_author":["Generalization","Kernel","Regularization","Robustness","Support vector machine"],"keywords_other":["Robust optimization","Robustness","Overfitting","Regularization","Kernel","Time control","Generalization"],"max_cite":121.0,"pub_year":2009.0,"sources":"['ieee', 'scp', 'wos']","rawkeys":["robustness","robust optimization","generalization","kernel","support vector machine","overfitting","time control","regularization"],"tags":["robustness","recognition","machine learning","robust optimization","kernel","overfitting","time control","regularization"]},{"p_id":7904,"title":"Deep convolutional neural networks for hyperspectral image classification","abstract":"\u00a9 2015 Wei Hu et al.Recently, convolutional neural networks have demonstrated excellent performance on various visual tasks, including the classification of common two-dimensional images. In this paper, deep convolutional neural networks are employed to classify hyperspectral images directly in spectral domain. More specifically, the architecture of the proposed classifier contains five layers with weights which are the input layer, the convolutional layer, the max pooling layer, the full connection layer, and the output layer. These five layers are implemented on each spectral signature to discriminate against others. Experimental results based on several hyperspectral image data sets demonstrate that the proposed method can achieve better classification performance than some traditional methods, such as support vector machines and the conventional deep learning-based methods.","keywords_author":null,"keywords_other":["REMOTE-SENSING IMAGES","Classification performance","Spectral signature","Two dimensional images","Hyper-spectral images","Hyperspectral image datas","SUPPORT VECTOR MACHINES","RECOGNITION","Convolutional neural network","Spectral domains","Hyperspectral image classification"],"max_cite":121.0,"pub_year":2015.0,"sources":"['ieee', 'scp', 'wos']","rawkeys":["recognition","classification performance","hyper-spectral images","spectral domains","hyperspectral image datas","hyperspectral image classification","remote-sensing images","convolutional neural network","support vector machines","spectral signature","two dimensional images"],"tags":["recognition","classification performance","hyper-spectral images","spectral domains","hyperspectral image datas","machine learning","hyperspectral image classification","convolutional neural network","remote sensing images","spectral signature","two dimensional images"]},{"p_id":73460,"title":"Predicting speech intelligibility with deep neural networks","abstract":"An accurate objective prediction of human speech intelligibility is of interest for many applications such as the evaluation of signal processing algorithms. To predict the speech recognition threshold (SRT) of normal-hearing listeners, an automatic speech recognition (ASR) system is employed that uses a deep neural network (DNN) to convert the acoustic input into phoneme predictions, which are subsequently decoded into word transcripts. ASR results are obtained with and compared to data presented in Schubotz et al. (2016), which comprises eight different additive maskers that range from speech-shaped stationary noise to a single -talker interferer and responses from eight normal-hearing subjects. The task for listeners and ASR is to identify noisy words from a German matrix sentence test in monaural conditions. Two ASR training schemes typically used in applications are considered: (A) matched training, which uses the same noise type for training and testing and (B) multi-condition training, which covers all eight maskers. For both training schemes, ASR-based predictions outperform established measures such as the extended speech intelligibility index (ESII), the multi-resolution speech envelope power spectrum model (mr-sEPSM) and others. This result is obtained with a speaker-independent model that compares the word labels of the utterance with the ASR transcript, which does not require separate noise and speech signals. The best predictions are obtained for multi-condition training with amplitude modulation features, which implies that the noise type has been seen during training. Predictions and measurements are analyzed by comparing speech recognition thresholds and individual psychometric functions to the DNN-based results. (C) 2017 Elsevier Ltd. All rights reserved.","keywords_author":["Speech intelligibility prediction","Deep neural networks","Automatic speech recognition"],"keywords_other":["FEATURES","FLUCTUATING NOISE","FREQUENCY-SELECTIVITY","AMPLITUDE-MODULATION","RECEPTION THRESHOLD","PERCEPTION","MODEL","LISTENERS","RECOGNITION","NORMAL-HEARING"],"max_cite":1.0,"pub_year":2017.0,"sources":"['ieee', 'wos']","rawkeys":["recognition","model","features","automatic speech recognition","deep neural networks","frequency-selectivity","perception","fluctuating noise","reception threshold","listeners","speech intelligibility prediction","normal-hearing","amplitude-modulation"],"tags":["attention model","recognition","model","features","automatic speech recognition","normal hearing","frequency-selective","fluctuating noise","convolutional neural network","listeners","perceptions","reception threshold","speech intelligibility prediction"]},{"p_id":73462,"title":"A unified DNN approach to speaker-dependent simultaneous speech enhancement and speech separation in low SNR environments","abstract":"We propose a unified speech enhancement framework to jointly handle both background noise and interfering speech in a speaker-dependent scenario based on deep neural networks (DNNs). We first explore speaker-dependent speech enhancement that can significantly improve system performance over speaker-independent systems. Next, we consider interfering speech as one noise type, thus a speaker-dependent DNN system can be adopted for both speech enhancement and separation. Experimental results demonstrate that the proposed unified system can achieve comparable performances to specific systems where only noise or speech interference is present. Furthermore, much better results can be obtained over individual enhancement or separation systems in mixed background noise and interfering speech scenarios. The training data for the two specific tasks are also found to be complementary. Finally, an ensemble learning-based framework is employed to further improve the system performance in low signal-to-noise ratio (SNR) environments. A voice activity detection (VAD) DNN and an ideal ratio mask (IRM) DNN are investigated to provide prior information to integrate two sub-modules at frame level and time-frequency level, respectively. The results demonstrate the effectiveness of the ensemble method in low SNR environments.","keywords_author":["Speaker-dependent speech processing","Speech enhancement","Speech separation","Deep neural network","Low SNR"],"keywords_other":["NOISE","ALGORITHM","VOICE ACTIVITY DETECTION","RECOGNITION","DEEP NEURAL-NETWORKS","SPECTRAL AMPLITUDE ESTIMATOR","COCHANNEL SPEECH"],"max_cite":0.0,"pub_year":2017.0,"sources":"['ieee', 'wos']","rawkeys":["cochannel speech","algorithm","speaker-dependent speech processing","recognition","deep neural network","noise","spectral amplitude estimator","voice activity detection","deep neural-networks","low snr","speech enhancement","speech separation"],"tags":["cochannel speech","speaker-dependent speech processing","recognition","noise","spectral amplitude estimator","voice activity detection","low snr","speech enhancement","convolutional neural network","algorithms","speech separation"]},{"p_id":73463,"title":"Two-Stage Single-Channel Audio Source Separation Using Deep Neural Networks","abstract":"Most single channel audio source separation approaches produce separated sources accompanied by interference fromother sources and other distortions. To tackle this problem, we propose to separate the sources in two stages. In the first stage, the sources are separated from the mixed signal. In the second stage, the interference between the separated sources and the distortions are reduced using deep neural networks (DNNs). We propose two methods that use DNNs to improve the quality of the separated sources in the second stage. In the first method, each separated source is improved individually using its own trained DNN, while in the second method all the separated sources are improved together using a single DNN. To further improve the quality of the separated sources, the DNNs in the second stage are trained discriminatively to further decrease the interference and the distortions of the separated sources. Our experimental results show that using two stages of separation improves the quality of the separated signals by decreasing the interference between the separated sources and distortions compared to separating the sources using a single stage of separation.","keywords_author":["Audio enhancement","discriminative learning","deep neural networks","single channel audio source separation"],"keywords_other":["NONNEGATIVE MATRIX FACTORIZATION","QUALITY","NOISE","SUPERVISED SPEECH SEPARATION","VOICE ACTIVITY DETECTION","RECOGNITION"],"max_cite":4.0,"pub_year":2017.0,"sources":"['ieee', 'wos']","rawkeys":["quality","recognition","noise","deep neural networks","supervised speech separation","nonnegative matrix factorization","voice activity detection","single channel audio source separation","audio enhancement","discriminative learning"],"tags":["quality","recognition","noise","supervised speech separation","nonnegative matrix factorization","voice activity detection","single channel audio source separation","audio enhancement","convolutional neural network","discriminative learning"]},{"p_id":73465,"title":"The benefit of combining a deep neural network architecture with ideal ratio mask estimation in computational speech segregation to improve speech intelligibility","abstract":"Computational speech segregation attempts to automatically separate speech from noise. This is challenging in conditions with interfering talkers and low signal-to-noise ratios. Recent approaches have adopted deep neural networks and successfully demonstrated speech intelligibility improvements. A selection of components may be responsible for the success with these state-of-the-art approaches: the system architecture, a time frame concatenation technique and the learning objective. The aim of this study was to explore the roles and the relative contributions of these components by measuring speech intelligibility in normal-hearing listeners. A substantial improvement of 25.4 percentage points in speech intelligibility scores was found going from a subband-based architecture, in which a Gaussian Mixture Model-based classifier predicts the distributions of speech and noise for each frequency channel, to a state-of-the-art deep neural network-based architecture. Another improvement of 13.9 percentage points was obtained by changing the learning objective from the ideal binary mask, in which individual time-frequency units are labeled as either speech- or noise-dominated, to the ideal ratio mask, where the units are assigned a continuous value between zero and one. Therefore, both components play significant roles and by combining them, speech intelligibility improvements were obtained in a six-talker condition at a low signal-to-noise ratio.","keywords_author":null,"keywords_other":["BINARY","MODULATION ANALYSIS","NOISE","ALGORITHM","HEARING-IMPAIRED LISTENERS","SYSTEMS","RECOGNITION","SEPARATION"],"max_cite":0.0,"pub_year":2018.0,"sources":"['wos']","rawkeys":["algorithm","recognition","noise","hearing-impaired listeners","modulation analysis","binary","systems","separation"],"tags":["recognition","noise","hearing-impaired listeners","system","modulation analysis","binary","algorithms","separation"]},{"p_id":7945,"title":"Gait recognition using radon transform and linear discriminant analysis","abstract":"A new feature extraction process is proposed for gait representation and recognition. The new system is based on the Radon transform of binary silhouettes. For each gait sequence, the transformed silhouettes are used for the computation of a template. The set of all templates is subsequently subjected to linear discriminant analysis and subspace projection. In this manner, each gait sequence is described using a low-dimensional feature vector consisting of selected Radon template coefficients. Given a test feature vector, gait recognition and verification is achieved by appropriately comparing it to feature vectors in a reference gait database. By using the new system on the Gait Challenge database, very considerable improvements in recognition performance are seen in comparison to state-of-the-art methods for gait recognition. \u00a9 2007 IEEE.","keywords_author":["Gait","Linear discriminant analysis (LDA)","Radon transform","Recognition"],"keywords_other":["Radon transform","Linear discriminant analysis","Gait recognition"],"max_cite":119.0,"pub_year":2007.0,"sources":"['scp']","rawkeys":["gait recognition","recognition","linear discriminant analysis","radon transform","gait","linear discriminant analysis (lda)"],"tags":["gait recognition","recognition","linear discriminant analysis","radon transform","gait"]},{"p_id":73487,"title":"Protein alignment based on higher order conditional random fields for template-based modeling","abstract":"The query-template alignment of proteins is one of the most critical steps of template-based modeling methods used to predict the 3D structure of a query protein. This alignment can be interpreted as a temporal classification or structured prediction task and first order Conditional Random Fields have been proposed for protein alignment and proven to be rather successful. Some other popular structured prediction problems, such as speech or image classification, have gained from the use of higher order Conditional Random Fields due to the well known higher order correlations that exist between their labels and features. In this paper, we propose and describe the use of higher order Conditional Random Fields for query-template protein alignment. The experiments carried out on different public datasets validate our proposal, especially on distantly-related protein pairs which are the most difficult to align.","keywords_author":null,"keywords_other":["PREDICTION","MATRICES","SEARCH","SERVER","RECOGNITION"],"max_cite":0.0,"pub_year":2018.0,"sources":"['wos']","rawkeys":["recognition","search","prediction","server","matrices"],"tags":["recognition","search","prediction","servers","matrices"]},{"p_id":73488,"title":"Complete fold annotation of the human proteome using a novel structural feature space","abstract":"Recognition of protein structural fold is the starting point for many structure prediction tools and protein function inference. Fold prediction is computationally demanding and recognizing novel folds is difficult such that the majority of proteins have not been annotated for fold classification. Here we describe a new machine learning approach using a novel feature space that can be used for accurate recognition of all 1,221 currently known folds and inference of unknown novel folds. We show that our method achieves better than 94% accuracy even when many folds have only one training example. We demonstrate the utility of this method by predicting the folds of 34,330 human protein domains and showing that these predictions can yield useful insights into potential biological function, such as prediction of RNA-binding ability. Our method can be applied to de novo fold prediction of entire proteomes and identify candidate novel fold families.","keywords_author":null,"keywords_other":["NETWORKS","EVC2","CLASSIFICATION","SERVER","VAN-CREVELD-SYNDROME","PROTEINS","RECOGNITION","SUPERFAMILY","GENE","STRUCTURE PREDICTION"],"max_cite":0.0,"pub_year":2017.0,"sources":"['wos']","rawkeys":["structure prediction","recognition","proteins","van-creveld-syndrome","gene","superfamily","networks","evc2","classification","server"],"tags":["structure prediction","recognition","proteins","van-creveld-syndrome","genes","servers","superfamily","networks","evc2","classification"]},{"p_id":73490,"title":"Computer-aided diagnosis of abnormal breasts in mammogram images by weighted-type fractional Fourier transform","abstract":"Abnormal breast can be diagnosed using the digital mammography. Traditional manual interpretation method cannot yield high accuracy. In this study, we proposed a novel computer-aided diagnosis system for detecting abnormal breasts. Our dataset contains 200 mammogram images with size of 1024 x 1024. First, we segmented the region of interest from mammogram images. Second, the fractional Fourier transform was employed to obtain the unified time-frequency spectrum. Third, spectrum coefficients were reduced by principal component analysis. Finally, both support vector machine and k-nearest neighbors were used and compared. The proposed \"weighted-type fractional Fourier transform + principal component analysis + support vector machine achieved sensitivity of 92.22% +\/- 4.16%, specificity of 92.10% +\/- 2.75%, and accuracy of 92.16% +\/- 3.60%. It is better than both the proposed \"weighted-type fractional Fourier transform+principal component analysis + k-nearest neighbors\" and other five state-of-the-art approaches in terms of sensitivity, specificity, and accuracy. The proposed computer-aided diagnosis system is effective in detecting abnormal breasts.","keywords_author":["Fractional Fourier transform","k-nearest neighbors","abnormal breast","computer-aided diagnosis","mammogram","support vector machine"],"keywords_other":["FEATURES","SUPPORT VECTOR MACHINE","WOMEN","CLASSIFICATION","SCREENING MAMMOGRAPHY","RECOGNITION","SEGMENTATION","DIGITAL MAMMOGRAPHY","CANCER","REGION"],"max_cite":10.0,"pub_year":2016.0,"sources":"['wos']","rawkeys":["k-nearest neighbors","women","recognition","segmentation","cancer","features","computer-aided diagnosis","digital mammography","mammogram","screening mammography","abnormal breast","classification","support vector machine","fractional fourier transform","region"],"tags":["k-nearest neighbors","women","recognition","segmentation","cancer","features","computer-aided diagnosis","digital mammography","mammograms","screening mammography","machine learning","abnormal breast","regions","classification","fractional fourier transform"]},{"p_id":7955,"title":"GENETAG: A tagged corpus for gene\/protein named entity recognition","abstract":"Background: Named entity recognition (NER) is an important first step for text mining the biomedical literature. Evaluating the performance of biomedical NER systems is impossible without a standardized test corpus. The annotation of such a corpus for gene\/protein name NER is a difficult process due to the complexity of gene\/protein names. We describe the construction and annotation of GENETAG, a corpus of 20K MEDLINE\u00ae sentences for gene\/protein NER. 15K GENETAG sentences were used for the BioCreAtIvE Task IA Competition. Results: To ensure heterogeneity of the corpus, MEDLINE sentences were first scored for term similarity to documents with known gene names, and 10K high- and 10K low-scoring sentences were chosen at random. The original 20K sentences were run through a gene\/protein name tagger, and the results were modified manually to reflect a wide definition of gene\/protein names subject to a specificity constraint, a rule that required the tagged entities to refer to specific entities. Each sentence in GENETAG was annotated with acceptable alternatives to the gene\/protein names it contained, allowing for partial matching with semantic constraints. Semantic constraints are rules requiring the tagged entity to contain its true meaning in the sentence context. Application of these constraints results in a more meaningful measure of the performance of an NER system than unrestricted partial matching. Conclusion: The annotation of GENETAG required intricate manual judgments by annotators which hindered tagging consistency. The data were pre-segmented into words, to provide indices supporting comparison of system responses to the \"gold standard\". However, character-based indices would have been more robust than word-based indices. GENETAG Train, Test and Round I data and ancillary programs are freely available at ftp:\/\/ftp.ncbi.nim.nih.Lyov\/pub\/tanabe\/ GENETAG.tar.gz. A newer version of GENETAG-05, will be released later this year.","keywords_author":null,"keywords_other":["MEDLINE","Gold standards","Biomedical literature","Semantic constraints","Named entity recognition","Humans","Animals","System response","Recognition (Psychology)","Proteins","Partial matching","Terminology as Topic","Genes","Term Similarity","Standardized tests"],"max_cite":119.0,"pub_year":2005.0,"sources":"['scp', 'wos']","rawkeys":["medline","semantic constraints","terminology as topic","standardized tests","recognition (psychology)","term similarity","proteins","named entity recognition","genes","humans","gold standards","system response","partial matching","biomedical literature","animals"],"tags":["medline","semantic constraints","terminology as topic","recognition","term similarity","proteins","standard tests","named entity recognition","genes","humans","gold standards","system response","partial matching","biomedical literature","animals"]},{"p_id":7956,"title":"Comparative analysis of quality of service and memory usage for adaptive failure detectors in healthcare systems","abstract":"Failure detection (FD) is an important issue for supporting dependability in distributed healthcare systems to guarantee continuous, safe, secure, and dependable operation, and often is an important performance bottleneck in the event of node failure. FD can be used to manage the health status of communication for delivering telemedicine services, and then to help distributed healthcare system reduce fatal accident rate and increase the reliability and safety of systems. Ensuring acceptable quality of service (QoS) is made difficult by the relative unpredictability of the network environment. In this paper, first, we compare QoS metrics of several adaptive FDs, discuss their properties and their relation, and then propose one optimization over the existing methods, called tuning adaptive margin failure detector (TAM FD), which significantly improves QoS, especially in the aggressive range and when the network is unstable. Second, we address the problem of most adaptive schemes, namely their need for a large window of samples. So we also analyze the impact of memory size on the performance of FDs, and then prove that the presented scheme is designed to use a fixed and very limited amount of memory for the distributed system. Our experimental results over several kinds of networks (Cluster, WiFi, LAN, Intercontinental WAN) show that the properties of the existing adaptive failure detectors, and demonstrate that the optimization is reasonable and acceptable. Furthermore, the extensive experimental results show what is the effect of memory size on the overall QoS of each adaptive failure detector. For our TAM FD, the effect of window size on their QoS is very small and can be negligible. \u00a9 2006 IEEE.","keywords_author":["Adaptation, Comparative analysis, Dependability, Failure detection, Healthcare, Quality-of-service"],"keywords_other":["Memory size","Fatal accidents","Performance bottlenecks","Memory usage","Failure detection","Window Size","Qos metrics","Node failure","Health status","Network environments","Distributed systems","Adaptive scheme","Existing method","Telemedicine services","Comparative analysis","Adaptation, Comparative analysis, Dependability, Failure detection, Healthcare, Quality-of-service","Health-care system","Failure Detectors"],"max_cite":119.0,"pub_year":2009.0,"sources":"['ieee', 'scp', 'wos']","rawkeys":["memory size","fatal accidents","network environments","window size","health-care system","quality-of-service","healthcare","telemedicine services","existing method","node failure","performance bottlenecks","memory usage","comparative analysis","adaptive scheme","dependability","qos metrics","health status","failure detectors","distributed systems","adaptation","failure detection"],"tags":["adaptation scheme","memory size","fatal accidents","network environments","quality of service","window size","health-care system","healthcare","telemedicine services","recognition","existing method","node failure","performance bottlenecks","memory usage","comparative analysis","qos metrics","health status","failure detectors","distributed systems","adaptation","failure detection"]},{"p_id":98114,"title":"Simultaneous Determination of Modulation Types and Signal-to-Noise Ratios Using Feature-Based Approach","abstract":"This paper presents a low-complexity technique for simultaneous determination of modulation types and signal-to-noise ratios (SNRs) in wireless communication systems. The proposed approach exploits the extracted features of patterns observed in signals' asynchronous amplitudes histograms, for the simultaneous determination of these quantities using support vector machine. Features extraction has been performed by a well-known technique called principal component analysis which is used to extract the most significant features before being supplied to the artificial intelligent system. Simulations for three commonlyused modulation types have been conducted under real-world channel conditions. The results conclude that the presented approach can accurately identify the modulation types with 99.83% accuracy despite the existence of real-world channel impairments. Furthermore, the algorithm is capable of SNRs estimation over a broad range of 0-30 dB with average estimation error of 0.79 dB. The proposed paper exploits the simplicity of generating asynchronous amplitudes histograms to enable cost-effective and reduced-complexity implementation in cognitive wireless systems.","keywords_author":["Simultaneous determination","modulation recognition","SNR estimation","support vector machine","feature-based approach","cognitive wireless systems"],"keywords_other":["CLASSIFICATION","IDENTIFICATION","SUPPORT VECTOR MACHINES","RECOGNITION","AIDED SNR ESTIMATION"],"max_cite":0.0,"pub_year":2018.0,"sources":"['wos']","rawkeys":["identification","recognition","feature-based approach","simultaneous determination","snr estimation","cognitive wireless systems","classification","support vector machines","aided snr estimation","support vector machine","modulation recognition"],"tags":["identification","recognition","feature based approaches","machine learning","simultaneous determination","snr estimation","cognitive wireless systems","classification","aided snr estimation","modulation recognition"]},{"p_id":16200,"title":"A survey of machine learning for big data processing","abstract":"\u00a9 2016, Qiu et al.There is no doubt that big data are now rapidly expanding in all science and engineering domains. While the potential of these massive data is undoubtedly significant, fully making sense of them requires new ways of thinking and novel learning techniques to address the various challenges. In this paper, we present a literature survey of the latest advances in researches on machine learning for big data processing. First, we review the machine learning techniques and highlight some promising learning methods in recent studies, such as representation learning, deep learning, distributed and parallel learning, transfer learning, active learning, and kernel-based learning. Next, we focus on the analysis and discussions about the challenges and possible solutions of machine learning for big data. Following that, we investigate the close connections of machine learning with signal processing techniques for big data processing. Finally, we outline several open issues and research trends.","keywords_author":["Big data","Data mining","Machine learning","Signal processing techniques","Machine learning","Big data","Data mining","Signal processing techniques"],"keywords_other":["COGNITIVE RADIO NETWORKS","Transfer learning","DEEP","CHALLENGES","Literature survey","SIGNAL","CLASSIFICATION","Kernel-based learning","Signal processing technique","Learning techniques","RECOGNITION","ALGORITHMS","INTERNET","OPTIMIZATION","Parallel learning","Science and engineering","Machine learning techniques","DATA ANALYTICS"],"max_cite":38.0,"pub_year":2016.0,"sources":"['ieee', 'scp', 'wos']","rawkeys":["big data","data analytics","deep","challenges","classification","science and engineering","internet","transfer learning","machine learning","signal processing techniques","kernel-based learning","learning techniques","optimization","algorithms","literature survey","data mining","recognition","machine learning techniques","signal","cognitive radio networks","signal processing technique","parallel learning"],"tags":["big data","data analytics","deep","challenges","classification","science and engineering","internet","transfer learning","machine learning","kernel-based learning","learning techniques","optimization","algorithms","literature survey","data mining","recognition","machine learning techniques","cognitive radio network","signals","signal processing technique","parallel learning"]},{"p_id":48971,"title":"A novel approach of deep convolutional neural networks for sketch recognition","abstract":"\u00a9 Springer International Publishing AG 2017. Deep Neural Networks (DNNs) have recently achieved impressive performance for many recognition tasks across different disciplines including image recognition task. However, most of existing works on deep learning for image recognition focus on natural image data (photo-based images) and not on sketches. Moreover, most of existing works on sketch classification are based on hand crafted feature representations. In this paper, we propose to train a convolutional neural network for sketch recognition using the TU-Berlin sketch dataset composed of 250 object categories with 80 images each. We find that training a CNN with a proper data-augmentation and a multi-scale multi-angle voting technique can achieve an accuracy of 75.43%, which surpasses human-level performance in the standard sketch classification benchmark and significantly outperforms state-of-the-art sketch recognition methods.","keywords_author":["Convolutional neural network","Deep learning","Learning","Recognition","Sketch"],"keywords_other":["Sketch","Human-level performance","Recognition","Learning","Feature representation","Object categories","Convolutional neural network","Sketch recognition"],"max_cite":0.0,"pub_year":2017.0,"sources":"['scp']","rawkeys":["recognition","deep learning","learning","human-level performance","sketch recognition","object categories","sketch","convolutional neural network","feature representation"],"tags":["recognition","machine learning","human-level performance","sketch recognition","object categories","sketch","convolutional neural network","feature representation"]},{"p_id":73551,"title":"Unit Selection Speech Synthesis Using Frame-Sized Speech Segments and Neural Network Based Acoustic Models","abstract":"This paper proposes to select frame-sized speech segments for waveform concatenation speech synthesis using neural network based acoustic models. First, a deep neural network (DNN) based frame selection method is presented. In this method, three DNNs are adopted to calculate target costs and concatenation costs respectively for selecting candidate frames of 5ms length. One DNN is built in the same way as the DNN-based statistical parametric speech synthesis, which predicts target acoustic features given linguistic context inputs. The distance between the acoustic features of a candidate unit and the predicted ones for a target unit is calculated as the target cost. The other two DNNs are constructed to predict the acoustic features at current frame using its context features and the acoustic features of preceding frames. At synthesis time, these two DNNs are employed to calculate the concatenation cost for each candidate frame given its preceding ones. Furthermore, recurrent neural networks (RNNs) with long short-term memory (LSTM) cells are adopted to replace DNNs for acoustic modeling in order to make better use of the sequential information. A strategy of using multi-frame instead of single frame as the basic unit for selection is also presented to reduce the concatenation points within synthetic speech. Experimental results show that our proposed method can achieve better naturalness than the hidden Markov model (HMM)-based frame selection method and the HMM-based parametric speech synthesis method.","keywords_author":["Speech synthesis","Unit selection","Deep neural network","Recurrent neural network","Long short-term memory"],"keywords_other":["RECOGNITION","ALGORITHM","GENERATION"],"max_cite":0.0,"pub_year":2018.0,"sources":"['ieee', 'wos']","rawkeys":["algorithm","recognition","deep neural network","long short-term memory","recurrent neural network","speech synthesis","unit selection","generation"],"tags":["recognition","neural networks","long short-term memory","speech synthesis","convolutional neural network","algorithms","unit selection","generation"]},{"p_id":24416,"title":"Image location inference by multisaliency enhancement","abstract":"\u00a9 1999-2012 IEEE.Locations of images have been widely used in many application scenarios for large geotagged image corpora. As to images that are not geographically tagged, we estimate their locations with the help of the large geotagged image set by content-based image retrieval. Bag-of-words image representation has been utilized widely. However, the individual visual word-based image retrieval approach is not effective in expressing the salient relationships of image region. In this paper, we present an image location estimation approach by multisaliency enhancement. We first extract region-of-interests (ROIs) by mean-shift clustering on the visual words and salient map of the image based on which we further determine the importance of the ROI. Then, we describe each ROI by the spatial descriptors of visual words. Finally, region-based visual phrases are generated to further enhance the saliency in image location estimation. Experiments show the effectiveness of our proposed approach.","keywords_author":["Location estimation","region of interest (ROI)","salient map","salient region","spatial constraint","visual phrase","Location estimation","region of interest (ROI)","visual phrase","salient map","salient region","spatial constraint"],"keywords_other":["SALIENCY","SUPERRESOLUTION","Spatial constraints","DEEP","Location estimation","VISUAL-SEARCH","Visual phrase","FEATURES","Salient maps","MODEL","RETRIEVAL","RECOGNITION","Region of interest","Salient regions"],"max_cite":9.0,"pub_year":2017.0,"sources":"['ieee', 'scp', 'wos']","rawkeys":["recognition","model","region of interest","features","salient maps","deep","spatial constraints","superresolution","salient map","visual phrase","salient region","region of interest (roi)","location estimation","retrieval","salient regions","saliency","spatial constraint","visual-search"],"tags":["recognition","model","region of interest","features","salient maps","deep","spatial constraints","sparse representation","visual search","visual phrase","location estimation","retrieval","salient regions","saliency"]},{"p_id":16225,"title":"Interpretation of the modality of touch on an artificial arm covered with an EIT-based sensitive skin","abstract":"During social interaction humans extract important information from tactile stimuli that can improve their understanding of the interaction. The development of a similar capability in a robot will contribute to the future success of intuitive human-robot interaction. This paper presents a thin, flexible and stretchable artificial skin for robotics based on the principle of electrical impedance tomography. This skin, which can be used to extract information such as location, duration and intensity of touch, was used to cover the forearm and upper arm of a full-size mannequin. A classifier based on the 'LogitBoost' algorithm was used to classify the modality of eight different types of touch applied by humans to the mannequin arm. Experiments showed that the modality of touch was correctly classified in approximately 71% of the trials. This was shown to be comparable to the accuracy of humans when identifying touch. The classification accuracies obtained represent significant improvements over previous classification algorithms applied to artificial sensitive skins. It is shown that features based on touch duration and intensity are sufficient to provide a good classification of touch modality. Gender and cultural background were examined and found to have no statistically significant effect on the classification results. \u00a9 The Author(s) 2012.","keywords_author":["artificial sensitive skin","electrical impedance tomography","force and tactile sensing","LogitBoost","Physical human-robot interaction","recognition","sensing and perception","supervised machine learning"],"keywords_other":["LogitBoost","recognition","Electrical impedance tomography","Sensitive skin","Tactile sensing","Supervised machine learning","Sensing and perception"],"max_cite":38.0,"pub_year":2012.0,"sources":"['scp']","rawkeys":["logitboost","recognition","electrical impedance tomography","tactile sensing","sensing and perception","artificial sensitive skin","sensitive skin","physical human-robot interaction","supervised machine learning","force and tactile sensing"],"tags":["logitboost","recognition","electrical impedance tomography","tactile sensing","physical human-robot interactions","artificial sensitive skin","sensing and perception","sensitive skin","supervised machine learning","force and tactile sensing"]},{"p_id":73572,"title":"Tag-aware recommender systems based on deep neural networks","abstract":"Many researchers have introduced tag information to recommender systems to improve the performance of traditional recommendation techniques. However, user-defined tags will usually suffer from many problems, such as sparsity, redundancy, and ambiguity. To address these problems, we propose a new recommendation algorithm based on deep neural networks. In the proposed algorithm, users' profiles are initially represented by tags and then a deep neural network model is used to extract the in-depth features from tag space layer by layer. In this way, representations of the raw data will become more abstract and advanced, and therefore the unique structure of tag space will be revealed automatically. Based on those extracted abstract features, users' profiles are updated and used for making recommendations. The experimental results demonstrate the usefulness of the proposed algorithm and show its superior performance over the clustering based recommendation algorithms. In addition, the impact of network depth on the algorithm performance is also investigated. (C) 2016 Elsevier B.V. All rights reserved.","keywords_author":["Recommender systems","Tag information","Redundancy","Ambiguity","Deep neural networks"],"keywords_other":["RECOGNITION"],"max_cite":9.0,"pub_year":2016.0,"sources":"['ieee', 'wos']","rawkeys":["recognition","deep neural networks","recommender systems","redundancy","ambiguity","tag information"],"tags":["recognition","recommender systems","redundancy","convolutional neural network","ambiguity","tag information"]},{"p_id":65407,"title":"Towards enhancing stacked extreme learning machine with sparse autoencoder by correntropy","abstract":"The stacked extreme learning machine (S-ELM) is an advanced framework of deep learning. It passes the 'reduced' outputs of the previous layer to the current layer, instead of directly propagating the previous outputs to the next layer in traditional deep learning. The S-ELM could address some large and complex data problems with a high accuracy and a relatively low requirement for memory. However, there is still room for improvement of the time complexity as well as robustness while using S-ELM. In this article, we propose an enhanced S-ELM by replacing the original principle component analysis (PCA) technique used in this algorithm with the correntropy-optimized temporal PCA (CTPCA), which is robust for outliers rejection and significantly improves the training speed. Then, the CTPCA-based S-ELM performs better than S-ELM in both accuracy and learning speed, when dealing with dataset disturbed by outliers. Furthermore, after integrating the extreme learning machine (ELM) sparse autoencoder (AE) method into the CTPCA-based S-ELM, the learning accuracy is further improved while spending a little more training time. Meanwhile, the sparser and more compact feature information are available by using the ELM sparse AE with more computational efforts. The simulation results on some benchmark datasets verify the effectiveness of our proposed methods. (c) 2017 The Franklin Institute. Published by Elsevier Ltd. All rights reserved.","keywords_author":null,"keywords_other":["NETWORKS","SCHEME","ALGORITHM","CLASSIFICATION","RECOGNITION"],"max_cite":15.0,"pub_year":2017.0,"sources":"['ieee', 'wos']","rawkeys":["algorithm","recognition","networks","classification","scheme"],"tags":["recognition","networks","classification","algorithms","scheme"]},{"p_id":32659,"title":"Active self-paced learning for cost-effective and progressive face identification","abstract":"This paper aims to develop a novel cost-effective framework for face identification, which progressively maintains a batch of classifiers with the increasing face images of different individuals. By naturally combining two recently rising techniques: active learning (AL) and self-paced learning (SPL), our framework is capable of automatically annotating new instances and incorporating them into training under weak expert recertification. We first initialize the classifier using a few annotated samples for each individual, and extract image features using the convolutional neural nets. Then, a number of candidates are selected from the unannotated samples for classifier updating, in which we apply the current classifiers ranking the samples by the prediction confidence. In particular, our approach utilizes the high-confidence and low-confidence samples in the self-paced and the active user-query way, respectively. The neural nets are later fine-tuned based on the updated classifiers. Such heuristic implementation is formulated as solving a concise active SPL optimization problem, which also advances the SPL development by supplementing a rational dynamic curriculum constraint. The new model finely accords with the \"instructor-student-collaborative\" learning mode in human education. The advantages of this proposed framework are two-folds: i) The required number of annotated samples is significantly decreased while the comparable performance is guaranteed. A dramatic reduction of user effort is also achieved over other state-of-the-art active learning techniques. ii) The mixture of SPL and AL effectively improves not only the classifier accuracy compared to existing AL\/SPL methods but also the robustness against noisy data. We evaluate our framework on two challenging datasets, which include hundreds of persons under diverse conditions, and demonstrate very promising results. Please find the code of this project at: http:\/\/hcp.sysu.edu.cn\/projects\/aspl\/","keywords_author":["Cost-effective model","active learning","self-paced learning","incremental processing","face identification"],"keywords_other":["NETWORKS","RECOGNITION","CLASSIFICATION"],"max_cite":3.0,"pub_year":2018.0,"sources":"['ieee', 'scp', 'wos']","rawkeys":["face identification","recognition","active learning","incremental processing","networks","classification","cost-effective model","self-paced learning"],"tags":["face identification","recognition","machine learning","incremental processing","networks","classification","cost-effective model","self-paced learning"]},{"p_id":65428,"title":"Deep adaptive feature embedding with local sample distributions for person re-identification","abstract":"Person re-identification (re-id) aims to match pedestrians observed by disjoint camera views. It attracts increasing attention in computer vision due to its importance to surveillance systems. To combat the major challenge of cross-view visual variations, deep embedding approaches are proposed by learning a compact feature space from images such that the Euclidean distances correspond to their cross-view similarity metric. However, the global Euclidean distance cannot faithfully characterize the ideal similarity in a complex visual feature space because features of pedestrian images exhibit unknown distributions due to large variations in poses, illumination and occlusion. Moreover, intra-personal training samples within a local range which are robust to guide deep embedding against uncontrolled variations cannot be captured by a global Euclidean distance. In this paper, we study the problem of person re-id by proposing a novel sampling to mine suitable positives (i.e., intra-class) within a local range to improve the deep embedding in the context of large intra-class variations. Our method is capable of learning a deep similarity metric adaptive to local sample structure by minimizing each sample's local distances while propagating through the relationship between samples to attain the whole intra-class minimization. To this end, a novel objective function is proposed to jointly optimize similarity metric learning, local positive mining and robust deep feature embedding. This attains local discriminations by selecting local-ranged positive samples, and the learned features are robust to dramatic intra-class variations. Experiments on benchmarks show state-of-the-art results achieved by our method. (C) 2017 Elsevier Ltd. All rights reserved.","keywords_author":["Deep feature embedding","Person re-identification","Local positive mining"],"keywords_other":["SELECTION","RECOGNITION"],"max_cite":2.0,"pub_year":2017.0,"sources":"['ieee', 'wos']","rawkeys":["recognition","selection","person re-identification","local positive mining","deep feature embedding"],"tags":["recognition","selection","person re-identification","local positive mining","deep feature embedding"]},{"p_id":65435,"title":"Multiple-shot person re-identification via fair set-collaboration metric learning","abstract":"As an issue that attracts increasing interests in both academia and industry, multiple-shot person re identification has shown promising results but suffers from real-scenario complexities and feature crafting heuristics. To tackle the problems of set-level data variation and sparseness during re identification, this paper proposes a novel metric learning method, named \"Fair Set-Collaboration Metric Learning\", motivated by utilizing the opportunities whilst overcoming the challenges from the set of multiple instances. This method optimizes a new set-collaboration dissimilarity measure, which introduces the fairness principle into the collaborative representation based set to sets distance, in the set based metric learning framework. Experiments on widely-used benchmark datasets have demonstrated the advantages of this method in terms of effectiveness and robustness. (C) 2017 Elsevier B.V. All rights reserved.","keywords_author":["Multiple-shot person re-identification","Metric learning","Set-collaboration dissimilarity","Fairness principle"],"keywords_other":["REPRESENTATION","RECOGNITION"],"max_cite":0.0,"pub_year":2017.0,"sources":"['ieee', 'wos']","rawkeys":["fairness principle","multiple-shot person re-identification","recognition","set-collaboration dissimilarity","representation","metric learning"],"tags":["fairness principle","multiple-shot person re-identification","recognition","set-collaboration dissimilarity","representation","metric learning"]},{"p_id":65437,"title":"Person re-identification by kernel null space marginal Fisher analysis","abstract":"Distance metric learning has been widely applied for person re-identification. However, the typical Small Sample Size (SSS) problem, which is induced by high dimensional feature and limited training samples in most re-identification datasets, may lead to a sub-optimal metric. In this work, we propose to embed samples into a discriminative null space based on Marginal Fisher Analysis (MFA) to overcome the SSS problem. In such a null space, multiple images of the same pedestrian are collapsed to a single point, resulting the extreme Marginal Fisher Criterion. We theoretically analyze the null space and derive its closed-form solution which can be computed very efficiently. To deal with the heavy storage burden in computation, we further extend the proposed method to kernel version, which is called Kernel Null Space Marginal Fisher Analysis (KNSMFA). Experiments on four challenging person re-identification datasets show that KNSMFA uniformly outperforms state-of-the-art approaches. (C) 2017 Elsevier B.V. All rights reserved.","keywords_author":["Person re-identification","Null space","Marginal Fisher analysis","Kernel method"],"keywords_other":["ASSOCIATION","VERIFICATION","RANKING","CONTEXT","RECOGNITION"],"max_cite":0.0,"pub_year":2017.0,"sources":"['ieee', 'wos']","rawkeys":["association","null space","recognition","person re-identification","verification","ranking","context","kernel method","marginal fisher analysis"],"tags":["association","null space","recognition","standards","person re-identification","context","verification","kernel methods","marginal fisher analysis"]},{"p_id":65442,"title":"Person re-identification by enhanced local maximal occurrence representation and generalized similarity metric learning","abstract":"To solve the challenging person re-identification problem, great efforts have been devoted to feature representation and metric learning. However, existing feature extractors are either stripe-based or dense-block-based, the fine details and coarse appearance are not well integrated. What is more, the metrics are generally learned independently from distance view or bilinear similarity view. Few works have exploited the mutual complementary effects of their combination. To address these issues, we propose a new feature representation termed enhanced Local Maximal Occurrence (eLOMO) which fuses a new overlapping-stripe-based descriptor with the Local Maximal Occurrence (LOMO) extracted from dense blocks. Such integration makes eLOMO resemble the coarse-to-fine recognition mechanism of human vision system, thus it can provide a more discriminative descriptor for re-identification. Besides, we show the advantages of learning generalized similarity by combining the Mahalanobis distance and bilinear similarity together. Specifically, we derive a logistic metric learning method to jointly learn a distance metric and a bilinear similarity metric, which exploits both the distance and angle information from training data. Taking advantage of learning in the intra-class subspace, the proposed method can be solved efficiently by coordinate descent optimization. Experiments on four challenging datasets including VIPeR, PRID450S, QMUL GRID, and CUHK01, show that the proposed method outperforms the state-of-the-art approaches significantly. (C) 2018 Elsevier B.V. All rights reserved.","keywords_author":["Person re-identification","Feature representation","Metric learning","Generalized similarity"],"keywords_other":["VERIFICATION","ALGORITHM","RECOGNITION","FEATURES"],"max_cite":0.0,"pub_year":2018.0,"sources":"['wos']","rawkeys":["generalized similarity","algorithm","recognition","features","person re-identification","verification","feature representation","metric learning"],"tags":["generalized similarity","recognition","features","person re-identification","verification","algorithms","feature representation","metric learning"]},{"p_id":65444,"title":"Maximal granularity structure and generalized multi-view discriminant analysis for person re-identification","abstract":"This paper proposes a novel descriptor called Maximal Granularity Structure Descriptor (MGSD) for feature representation and an effective metric learning method called Generalized Multi-view Discriminant Analysis based on representation consistency (GMDA-RC) for person re-identification (Re-ID). The proposed descriptor of MGSD captures rich local structural information from overlapping macro-pixels in an image, analyzes the horizontal occurrence of multi-granularity and maximizes the occurrence to extract a robust representation for viewpoint changes. As a result, the proposed descriptor of MGSD can obtain rich person appearance whilst being robust against different condition changes. Besides, considering multi-view information, we present a new GMDA-RC for different views, inspired by the observation that different views share similar data structures. The proposed metric learning method of GMDA-RC seeks multiple discriminant common spaces for multiple views by jointly learning multiple view-specific linear transforms. Finally, we evaluate the proposed method of (MGSD+GMDA-RC) on three publicly available person Re-ID datasets: VIPeR, CUHK-01 and Wide Area Re-ID dataset (WARD). For the VIPeR and CUHK-01, the experimental results show that our method significantly outperforms the state-of-the-art methods, achieving the rank-1 matching rates of 67.09%, 70.61%, and the improvements of 17.41%, 5.34%, respectively. For the WARD, we consider different pairwise camera views (camera 1-2, camera 1-3, camera 2-3) and our method can achieve the rank-1 matching rates of 64.33%, 59.42%, 70.32%, increasing of 5.68%, 11.04%, 9.06% compared with the state-of-the-art methods, respectively. (C) 2018 Elsevier Ltd. All rights reserved.","keywords_author":["Person re-identification","Maximal granularity structure descriptor","Generalized multi-view discriminant analysis","Representation consistency"],"keywords_other":["IMAGE RETRIEVAL","RECOGNITION","FEATURES"],"max_cite":0.0,"pub_year":2018.0,"sources":"['ieee', 'wos']","rawkeys":["maximal granularity structure descriptor","generalized multi-view discriminant analysis","recognition","features","image retrieval","representation consistency","person re-identification"],"tags":["maximal granularity structure descriptor","generalized multi-view discriminant analysis","recognition","features","image retrieval","representation consistency","person re-identification"]},{"p_id":98214,"title":"A Convolutional Approach to Automating Histopathologic Detection of Melanocytic Atypia: Bridging the Gap","abstract":null,"keywords_author":null,"keywords_other":["RECOGNITION","NEVI","DIAGNOSIS"],"max_cite":0.0,"pub_year":2018.0,"sources":"['wos']","rawkeys":["diagnosis","recognition","nevi"],"tags":["diagnosis","recognition","nevi"]},{"p_id":57270,"title":"Supervised, Multivariate, Whole-Brain Reduction Did Not Help to Achieve High Classification Performance in Schizophrenia Research","abstract":"We examined how penalized linear discriminant analysis with resampling, which is a supervised, multivariate, whole-brain reduction technique, can help schizophrenia diagnostics and research. In an experiment with magnetic resonance brain images of 52 first-episode schizophrenia patients and 52 healthy controls, this method allowed us to select brain areas relevant to schizophrenia, such as the left prefrontal cortex, the anterior cingulum, the right anterior insula, the thalamus, and the hippocampus. Nevertheless, the classification performance based on such reduced data was not significantly better than the classification of data reduced by mass univariate selection using a t-test or unsupervised multivariate reduction using principal component analysis. Moreover, we found no important influence of the type of imaging features, namely local deformations or gray matter volumes, and the classification method, specifically linear discriminant analysis or linear support vector machines, on the classification results. However, we ascertained significant effect of a cross validation setting on classification performance as classification results were overestimated even though the resampling was performed during the selection of brain imaging features. Therefore, it is critically important to perform cross validation in all steps of the analysis (not only during classification) in case there is no external validation set to avoid optimistically biasing the results of classification studies.","keywords_author":["computational neuroanatomy","pattern recognition","classification","penalized linear discriminant analysis","support vector machines","cross-validation","magnetic resonance imaging","schizophrenia"],"keywords_other":["METAANALYSIS","PATTERN-CLASSIFICATION","REGRESSION","HEALTHY CONTROLS","VOXEL-BASED MORPHOMETRY","RECOGNITION","1ST-EPISODE SCHIZOPHRENIA","VOLUMES","FMRI","STRUCTURAL MRI"],"max_cite":1.0,"pub_year":2016.0,"sources":"['wos']","rawkeys":["recognition","structural mri","computational neuroanatomy","1st-episode schizophrenia","voxel-based morphometry","volumes","metaanalysis","fmri","classification","cross-validation","penalized linear discriminant analysis","pattern recognition","schizophrenia","healthy controls","regression","magnetic resonance imaging","support vector machines","pattern-classification"],"tags":["recognition","regression","structural mri","computational neuroanatomy","1st-episode schizophrenia","pattern recognition","machine learning","voxel-based morphometry","metaanalysis","fmri","classification","penalized linear discriminant analysis","computer vision","schizophrenia","healthy controls","pattern classification","magnetic resonance imaging","volume"]},{"p_id":8122,"title":"Deep Convolutional Neural Networks for Large-scale Speech Tasks","abstract":"\u00a9 2014 Elsevier Ltd.Convolutional Neural Networks (CNNs) are an alternative type of neural network that can be used to reduce spectral variations and model spectral correlations which exist in signals. Since speech signals exhibit both of these properties, we hypothesize that CNNs are a more effective model for speech compared to Deep Neural Networks (DNNs). In this paper, we explore applying CNNs to large vocabulary continuous speech recognition (LVCSR) tasks. First, we determine the appropriate architecture to make CNNs effective compared to DNNs for LVCSR tasks. Specifically, we focus on how many convolutional layers are needed, what is an appropriate number of hidden units, what is the best pooling strategy. Second, investigate how to incorporate speaker-adapted features, which cannot directly be modeled by CNNs as they do not obey locality in frequency, into the CNN framework. Third, given the importance of sequence training for speech tasks, we introduce a strategy to use ReLU+dropout during Hessian-free sequence training of CNNs. Experiments on 3 LVCSR tasks indicate that a CNN with the proposed speaker-adapted and ReLU+dropout ideas allow for a 12%-14% relative improvement in WER over a strong DNN system, achieving state-of-the art results in these 3 tasks.","keywords_author":["Deep learning","Neural networks","Speech recognition","Deep learning","Neural networks","Speech recognition"],"keywords_other":["Deep learning","Large vocabulary continuous speech recognition","Neural Networks (Computer)","Spectral correlation","State of the art","Speech","RECOGNITION","Speech signals","Convolutional neural network","Spectral variation","Deep neural networks","Speech Recognition Software"],"max_cite":111.0,"pub_year":2015.0,"sources":"['ieee', 'scp', 'wos']","rawkeys":["neural networks (computer)","recognition","deep learning","deep neural networks","neural networks","speech recognition software","state of the art","speech recognition","large vocabulary continuous speech recognition","convolutional neural network","speech","speech signals","spectral variation","spectral correlation"],"tags":["recognition","neural networks","speech recognition software","state of the art","machine learning","speech recognition","large vocabulary continuous speech recognition","convolutional neural network","speech","speech signals","spectral variation","spectral correlation"]},{"p_id":65474,"title":"How do you smile? Towards a comprehensive smile analysis system","abstract":"To better understand the expression of human smile, there have been considerable studies about automatic smile detection. Despite all the research, few attention is paid to analyze a smile in a comprehensive Way. In this paper, a smile analysis system is presented to detailedly measure a person's smile, which consists of three main modules: smile detection, smile intensity estimation and spontaneous versus posed (SVP) smile recognition. Firstly, our recent proposed feature, Self-Similarity of Gradients (GSS), is employed to detect smiling facial images in unconstrained scenarios. Secondly, the smile intensity is estimated in terms of different facial regions rather than merely the mouth region, which is also applied in the temporal phase segmentation of a smile. Finally, in SVP smile recognition module, a discriminative learning model (DLM) is proposed based on a local spatial-temporal feature, which devotes to obtaining most robust and discrihainative patterns of interest. The first two modules are the bases of the last, preparing a deeper understanding of a smile. Experiments on benchmark databases are carried out and compared with the state-of-the-art methods respectively, which validate the advantages of our approach of SVP smile recognition. Moreover, a comprehensive analysis of human smile is given for the first time to the best of our knowledge, which Could pave the way for computers that better assess the emotional states of their users and provide useful and important information in helping the research of psychology and behavior science.","keywords_author":["Smile detection","Intensity estimation","Spontaneous versus posed (SVP)","Facial landmark localization"],"keywords_other":["REGRESSION","FACES","RECOGNITION","EIGENFACES","FACIAL EXPRESSIONS"],"max_cite":0.0,"pub_year":2017.0,"sources":"['wos']","rawkeys":["recognition","faces","eigenfaces","smile detection","facial expressions","spontaneous versus posed (svp)","intensity estimation","regression","facial landmark localization"],"tags":["recognition","eigenfaces","smile detection","facial expressions","spontaneous versus posed (svp)","intensity estimation","face","regression","facial landmark localization"]},{"p_id":32708,"title":"Location-driven image retrieval for images collected by a mobile robot","abstract":"Mobile robot teleoperation is a method for a human user to interact with a mobile robot over time and distance. Successful teleoperation depends on how well images taken by the mobile robot are visualized to the user. To enhance the efficiency and flexibility of the visualization, an image retrieval system on such a robot's image database would be very useful. The main difference of the robot's image database from standard image databases is that various relevant images exist due to variety of viewing conditions. The main contribution of this paper is to propose an efficient retrieval approach, named location-driven approach, utilizing correlation between visual features and real world locations of images. Combining the location-driven approach with the conventional feature-driven approach, our goal can be viewed as finding an optimal classifier between relevant and irrelevant feature-location pairs. An active learning technique based on support vector machine is extended for this aim.","keywords_author":["Active learning","Feature selection","Image retrieval","Machine learning","Measurement","Moving robot","Reasoning","Recognition","Support vector machine"],"keywords_other":["Conventional feature-driven approach","Mobile robot teleoperation","Location-driven approach"],"max_cite":3.0,"pub_year":2007.0,"sources":"['scp']","rawkeys":["measurement","recognition","image retrieval","machine learning","active learning","reasoning","feature selection","moving robot","support vector machine","mobile robot teleoperation","conventional feature-driven approach","location-driven approach"],"tags":["measurement","recognition","image retrieval","machine learning","moving robots","reasoning","feature selection","mobile robot teleoperation","conventional feature-driven approach","location-driven approach"]},{"p_id":32721,"title":"Conference resolution using decision trees","abstract":"Corefence resolution is the process of determining whether two expressions in natural language refer to the same entity in the world. We adopt machine learning approach using decision tree to a coreference resolution of general noun phrases in unrestricted text based on well defined features. We also use approximate matching algorithms for a string match feature and databases of American last names and male and female first names for gender agreement and alias feature. For the evaluation we use MUC-6 coreference corpora. We show that pessimisitc error pruning method gives better generalization in a coreference resolution task than that reported in Soon et al. [20], when weights of positive and negative examples are properly chosen. \u00a9 2006 IEEE.","keywords_author":["Approximate string matching","Coreference resolution","Decision tree","Machine learning","Pessimistic error pruning"],"keywords_other":["natural languages","Conference resolution","(e ,3e) process","Coreference","Noun phrases","Corpora (CO)","Negative examples","Neural network applications","machine-learning","General (CO)","Approximate matching","coreference resolution"],"max_cite":3.0,"pub_year":2006.0,"sources":"['scp']","rawkeys":["pessimistic error pruning","conference resolution","noun phrases","negative examples","approximate matching","machine learning","general (co)","natural languages","neural network applications","(e ,3e) process","coreference","decision tree","machine-learning","corpora (co)","approximate string matching","coreference resolution"],"tags":["corpora","conference resolution","recognition","processing","negative examples","approximate matching","machine learning","natural languages","decision trees","neural network applications","coreference","noun phrase","pessimistic error pruning","approximate string matching","coreference resolution"]},{"p_id":73681,"title":"Adaptive multiclass support vector machine for multimodal data analysis (Retracted article. See vol. 76, pg. 762, 2018)","abstract":"Multimodal data commonly exists in human lives. Early analysis usually concentrates on mining information based on single modality. Recent studies show that learning tasks could be greatly enhanced by analyzing data from the aspect of multimodality. This paper deals with classifying multimodal data comprised of visual and acoustic contents. Different data features are fused under a hierarchical structure to achieve a good semantic understanding. Then, to accomplish accurate classification, an adaptive support vector machine method (ASVM) is proposed. The method is support vector machine with hyperparameters controlled by a novel and efficient artificial bee colony algorithm. First, a micro colony is set as the number of hyperparameters is usually less than 5. Second, one position inheritance based on roulette wheel selection is used. Third, discarded solutions are mutated by position shift operation instead of random reinitialization. The ASVM method is first verified on classical data sets demonstrating the goodness of the proposed method. Then the proposed method is applied on a multimodal data set. Each sample includes both image and audio data features. Experimental results show that the ASVM method is more effective and robust than the compared methods. (C) 2017 Elsevier Ltd. All rights reserved.","keywords_author":["Artificial bee colony","Feature selection","Hyperparameter optimization","Multiclass classification","Support vector machine"],"keywords_other":["SENSOR NETWORKS","ARTIFICIAL BEE COLONY","MULTIMEDIA DATA","PARAMETERS","CLASSIFICATION","RECOGNITION","ALGORITHMS","PERFORMANCE","KERNEL","OPTIMIZATION"],"max_cite":3.0,"pub_year":2017.0,"sources":"['ieee', 'wos']","rawkeys":["multiclass classification","multimedia data","hyperparameter optimization","parameters","performance","recognition","artificial bee colony","feature selection","kernel","classification","optimization","algorithms","support vector machine","sensor networks"],"tags":["multimedia data","parameters","performance","recognition","hyper-parameter optimizations","machine learning","artificial bee colonies","feature selection","kernel","classification","optimization","algorithms","sensor networks","multi-class classification"]},{"p_id":90090,"title":"Human-Centered Saliency Detection","abstract":"We introduce a new concept for detecting the saliency of 3-D shapes, that is, human-centered saliency (HCS) detection on the surface of shapes, whereby a given shape is analyzed not based on geometric or topological features directly obtained from the shape itself, but by studying how a human uses the object. Using virtual agents to simulate the ways in which humans interact with objects helps to understand shapes and detect their salient parts in relation to their functions. HCS detection is less affected by inconsistencies between the geometry or topology of the analyzed 3-D shapes. The potential benefit of the proposed method is that it is adaptable to variable shapes with the same semantics, as well as being robust against a geometrical and topological noise. Given a 3-D shape, its salient part is detected by automatically selecting a corresponding agent and making them interact with each other. Their adaption and alignment depend on an optimization framework and a training process. We demonstrate the detected salient parts for different types of objects together with the stability thereof. The salient parts can be used for important vision tasks, such as 3-D shape retrieval.","keywords_author":["3-D shape","functional saliency","human-centered saliency (HCS)","optimization","shape retrieval"],"keywords_other":["MODEL RETRIEVAL","SURFACES","SIMILARITY","3D SHAPE RETRIEVAL","RECOGNITION","SPARSE","DESCRIPTORS","HIGH-LEVEL FEATURE","MESH SALIENCY","OBJECT DETECTION"],"max_cite":7.0,"pub_year":2016.0,"sources":"['ieee', 'wos']","rawkeys":["surfaces","recognition","shape retrieval","functional saliency","model retrieval","similarity","3d shape retrieval","3-d shape","human-centered saliency (hcs)","sparse","high-level feature","object detection","optimization","descriptors","mesh saliency"],"tags":["high-level features","recognition","surfaces","shape retrieval","functional saliency","model retrieval","similarity","3d shape retrieval","3-d shape","human-centered saliency (hcs)","sparse","object detection","optimization","descriptors","mesh saliency"]},{"p_id":98293,"title":"SelexGLM differentiates androgen and glucocorticoid receptor DNA-binding preference over an extended binding site","abstract":"The DNA-binding interfaces of the androgen (AR) and glucocorticoid (GR) receptors are virtually identical, yet these transcription factors share only about a third of their genomic binding sites and regulate similarly distinct sets of target genes. To address this paradox, we determined the intrinsic specificities of the AR and GR DNA-binding domains using a refined version of SELEX-seq. We developed an algorithm, SelexGLM, that quantifies binding specificity over a large (31-bp) binding site by iteratively fitting a feature-based generalized linear model to SELEX probe counts. This analysis revealed that the DNA-binding preferences of AR and GR homodimers differ significantly, both within and outside the 15-bp core binding site. The relative preference between the two factors can be tuned over a wide range by changing the DNA sequence, with AR more sensitive to sequence changes than GR. The specificity of AR extends to the regions flanking the core 15-bp site, where isothermal calorimetry measurements reveal that affinity is augmented by enthalpy-driven readout of poly(A) sequences associated with narrowed minor groove width. We conclude that the increased specificity of AR is correlated with more enthalpy-driven binding than GR. The binding models help explain differences in AR and GR genomic binding and provide a biophysical rationale for how promiscuous binding by GR allows functional substitution for AR in some castration-resistant prostate cancers.","keywords_author":null,"keywords_other":["RESPONSE ELEMENTS","TRANSCRIPTION-FACTOR-BINDING","IN-VIVO","GENOME","DETERMINANTS","PROTEINS","PROSTATE-CANCER","RECOGNITION","SEQUENCE SPECIFICITY","TITRATION CALORIMETRY DATA"],"max_cite":2.0,"pub_year":2018.0,"sources":"['wos']","rawkeys":["recognition","proteins","in-vivo","response elements","transcription-factor-binding","titration calorimetry data","sequence specificity","prostate-cancer","determinants","genome"],"tags":["recognition","proteins","in-vivo","response elements","titration calorimetry data","prostate cancer","determinants","transcription factor-binding","genomics","sequence specificity"]},{"p_id":98294,"title":"Recognizing RNA structural motifs in HT-SELEX data for ribosomal protein S15","abstract":"Background: Proteins recognize many different aspects of RNA ranging from single stranded regions to discrete secondary or tertiary structures. High-throughput sequencing (HTS) of in vitro selected populations offers a large scale method to study RNA-proteins interactions. However, most existing analysis methods require that the binding motifs are enriched in the population relative to earlier rounds, and that motifs are found in a loop or single stranded region of the potential RNA secondary structure. Such methods do not generalize to all RNA-protein interaction as some RNA binding proteins specifically recognize more complex structures such as double stranded RNA.","keywords_author":["SELEX","Ribosomal protein","Motif","S15"],"keywords_other":["ESCHERICHIA-COLI","BINDING PROTEINS","APTAMERS","SEQUENCE","MESSENGER-RNA","RECOGNITION","SECONDARY STRUCTURES","DISCOVERY","SPECIFICITIES","DNA"],"max_cite":4.0,"pub_year":2017.0,"sources":"['wos']","rawkeys":["specificities","s15","messenger-rna","recognition","discovery","ribosomal protein","aptamers","dna","escherichia-coli","sequence","motif","binding proteins","selex","secondary structures"],"tags":["s15","messenger-rna","recognition","specificity","discovery","ribosomal protein","aptamers","dna","motifs","escherichia-coli","sequence","secondary structure","binding proteins","selex"]}]