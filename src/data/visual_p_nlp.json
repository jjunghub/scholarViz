[{"p_id":36864,"title":"Memory-efficient word embedding vectors","abstract":"Word embedding is a technique for identifying the semantic relationships between words by computer. Word embedding vectors enable computers to provide a guess similar to the intuition or common sense of human beings. This article introduces a method for reducing the required memory consumption of this important fundamental operation of word embedding vectors while maintaining the ability to calculate semantic relationships, which is an important property when this technique is applied to real world systems.","keywords_author":["Deep learning","Natural language processing","Word embeddings"],"keywords_other":["Fundamental operations","Common sense","Human being","Memory efficient","Memory consumption","Semantic relationships","Real-world system","Embeddings"],"max_cite":1.0,"pub_year":2017.0,"sources":"['scp']","rawkeys":["embeddings","real-world system","common sense","deep learning","natural language processing","memory efficient","word embeddings","fundamental operations","semantic relationships","memory consumption","human being"],"tags":["embeddings","real-world system","common sense","machine learning","natural language processing","word embedding","fundamental operations","semantic relationships","memory efficiency","memory consumption","human being"]},{"p_id":2050,"title":"Investigation of stochastic Hessian-Free optimization in Deep neural networks for speech recognition","abstract":"Effective training of Deep neural networks (DNNs) has very important significance for the DNNs based speech recognition systems. Stochastic gradient descent (SGD) is the most popular method for training DNNs. SGD often provides the solutions that are well adapt to generalization on held-out data. Recently, Hessian Free (HF) optimization have proved another optional algorithm for training DNNs. HF can be used for solving the pathological tasks. Stochastic Hessian Free (SHF) is a variation of HF, which can combine the generalization advantages of stochastic gradient descent (SGD) with second-order information from Hessian Free. This paper focus on investigating the SHF algorithm for DNN training. We conduct this algorithm on 100 hours Mandarin Chinese recorded speech recognition task. The first experiment shows that choosing proper size of gradient and curvature minibatch results in less training time and good performance. Next, it is observed that the performance of SHF does not depend on the initial parameters. Further more, experimental results shows that SHF performs with comparable results with SGD but better than traditional HF. Finally, we find that additional performance improvement is obtained with a dropout algorithm.","keywords_author":["Deep neural networks","Stochastic Hessian-Free optimization","Dropout","Speech recognition"],"keywords_other":["stochastic programming","stochastic Hessian-Free optimization","speech recognition","Hessian free optimization","Neural networks","stochastic Hessian free","Stochastic processes","generalization advantages","speech recognition system","Optimization","gradient methods","pathological tasks","SHF algorithm","Mandarin Chinese recorded speech recognition task","Hafnium","Training","HF optimization","stochastic gradient descent","Error analysis","neural nets","DNN training","second-order information","deep neural networks","optional algorithm","natural language processing","SGD","Speech recognition"],"max_cite":1.0,"pub_year":2014.0,"sources":"['ieee']","rawkeys":["stochastic programming","stochastic hessian-free optimization","stochastic hessian free","speech recognition","error analysis","generalization advantages","hf optimization","speech recognition system","gradient methods","pathological tasks","optimization","hafnium","neural networks","hessian free optimization","stochastic processes","training","sgd","stochastic gradient descent","neural nets","dnn training","shf algorithm","dropout","second-order information","deep neural networks","optional algorithm","natural language processing","mandarin chinese recorded speech recognition task"],"tags":["stochastic programming","stochastic hessian-free optimization","stochastic hessian free","speech recognition","error analysis","convolutional neural network","generalization advantages","hf optimization","hessian-free optimization","gradient methods","pathological tasks","optimization","hafnium","neural networks","stochastic processes","training","stochastic gradient descent","speech recognition systems","dnn training","shf algorithm","dropout","second-order information","optional algorithm","natural language processing","mandarin chinese recorded speech recognition task"]},{"p_id":34819,"title":"Cross-hospital portability of information extraction of cancer staging information","abstract":"\u00a9 2014 Elsevier B.V.Objective: We address the task of extracting information from free-text pathology reports, focusing on staging information encoded by the TNM (tumour-node-metastases) and ACPS (Australian clinico-pathological stage) systems. Staging information is critical for diagnosing the extent of cancer in a patient and for planning individualised treatment. Extracting such information into more structured form saves time, improves reporting, and underpins the potential for automated decision support. Methods and material: We investigate the portability of a text mining model constructed from records from one health centre, by applying it directly to the extraction task over a set of records from a different health centre, with different reporting narrative characteristics. Other than a simple normalisation step on features associated with target labels, we apply the models from one system directly to the other. Results: The best F-scores for in-hospital experiments are 81%, 85%, and 94% (for staging T, N, and M respectively), while best cross-hospital F-scores reach 84%, 81%, and 91% for the same respective categories. Conclusions: Our performance results compare favourably to the best levels reported in the literature, and-most relevant to our aim here-the cross-corpus results demonstrate the portability of the models we developed.","keywords_author":["Cancer staging detection","Colorectal cancer","Information extraction","Machine learning","Text mining"],"keywords_other":["Methods and materials","Text mining","Medical Records","Algorithms","Cancer staging","Colorectal Neoplasms","Neoplasm Staging","Humans","Target labels","Natural Language Processing","Decision supports","Normalisation","Data Mining","Colorectal cancer","Hospital Information Systems","Extracting information"],"max_cite":2.0,"pub_year":2014.0,"sources":"['scp']","rawkeys":["colorectal neoplasms","hospital information systems","target labels","cancer staging","colorectal cancer","text mining","information extraction","machine learning","algorithms","medical records","normalisation","data mining","humans","extracting information","methods and materials","natural language processing","neoplasm staging","cancer staging detection","decision supports"],"tags":["colorectal neoplasms","hospital information systems","target labels","cancer staging","colorectal cancer","text mining","information extraction","machine learning","medical record","algorithms","normalisation","data mining","humans","extracting information","methods and materials","natural language processing","neoplasm staging","cancer staging detection","decision supports"]},{"p_id":34820,"title":"Retrieval and discovery of cell cycle literature and proteins by means of machine learning, text mining and network analysis","abstract":"\u00a9 Springer International Publishing Switzerland 2014.The cell cycle is one of the most important biological processes, being studied intensely by experimental as well as bioinformatics means. A considerable amount of literature provides relevant descriptions of proteins involved in this complex process. These proteins are often key to understand cellular alterations encountered in pathological conditions such as abnormal cell growth. The authors explored the use of text mining strategies to improve the retrieval of relevant articles and individual sentences for this topic. Moreover information extraction and text mining was used to detect and rank automatically Arabidopsis proteins important for the cell cycle. The obtained results were evaluated using independent data collections and compared to keyword-based strategies. The obtained results indicate that the use of machine learning methods can improve the sensitivity compared to term-co-occurrence, although with considerable differences when using abstracts and full text articles as input. At the level of document triage the recall ranges for abstracts from around 16% for keyword indexing, 37% for a sentence SVM classifier to 57% for SVM abstract classifier. In case of full text data, keyword and cell cycle phrase indexing obtained a recall of 42% and 55% respectively compared to 94% reached by a sentence classifier. In case of the cell cycle protein detection, the cell cycle keyword-protein co-occurrence strategy had a recall of 52% for abstracts and 70% for full text while a protein mentioning sentence classifier obtained a recall of over 83% for abstracts and 79% for full text. The generated cell cycle term co-occurrence statistics and SVM confidence scores for each protein were explored to rank proteins and filter a protein network in order to derive a topic specific subnetwork. All the generated protein cell cycle scores together with a global protein interaction and gene regulation network for Arabidopsis are available at: http:\/\/zope.bioinfo.cnio.es\/cellcyle addmaterial.","keywords_author":["Cell cycle","Machine learning","Natural language processing","Protein ranking","Text mining"],"keywords_other":["Sentence classifiers","Text mining","Gene Regulation Network","Machine learning methods","Cell cycle","Pathological conditions","NAtural language processing","Protein interaction"],"max_cite":2.0,"pub_year":2014.0,"sources":"['scp']","rawkeys":["machine learning methods","sentence classifiers","protein interaction","cell cycle","protein ranking","text mining","gene regulation network","pathological conditions","natural language processing","machine learning"],"tags":["machine learning methods","sentence classifiers","protein interaction","cell cycle","protein ranking","text mining","gene regulation network","pathological conditions","natural language processing","machine learning"]},{"p_id":12290,"title":"Exploring convolutional neural networks and topic models for user profiling from drug reviews","abstract":"Pharmacovigilance, and generally applications of natural language processing models to healthcare, have attracted growing attention over the recent years. In particular, drug reactions can be extracted from user reviews posted on the Web, and automated processing of this information represents a novel and exciting approach to personalized medicine and wide-scale drug tests. In medical applications, demographic information regarding the authors of these reviews such as age and gender is of primary importance; however, existing studies usually either assume that this information is available or overlook the issue entirely. In this work, we propose and compare several approaches to automated mining of demographic information from user-generated texts. We compare modern natural language processing techniques, including extensions of topic models and convolutional neural networks (CNN). We apply single-task and multi-task learning approaches to this problem. Based on a real-world dataset mined from a health-related web site, we conclude that while CNNs perform best in terms of predicting demographic information by jointly learning different user attributes, topic models provide additional information and reflect gender-specific and age-specific symptom profiles that may be of interest for a researcher.","keywords_author":["Convolutional neural networks","Deep learning","Demographic attributes","Demographic prediction","Mental health","Multi-task learning","Natural language processing","Single-task learning","Social media","Text mining","Topic modeling","User reviews","Text mining","Natural language processing","Topic modeling","Deep learning","Convolutional neural networks","Multi-task learning","Single-task learning","User reviews","Demographic prediction","Demographic attributes","Social media","Mental health"],"keywords_other":["Text mining","TOOL","Multitask learning","Mental health","Single task learning","Social media","Demographic attributes","Demographic predictions","Convolutional neural network","ONLINE SOCIAL NETWORKS","AGE","User reviews","Topic Modeling"],"max_cite":0.0,"pub_year":2018.0,"sources":"['scp', 'wos']","rawkeys":["online social networks","demographic prediction","convolutional neural network","demographic attributes","text mining","topic modeling","tool","mental health","multitask learning","age","multi-task learning","convolutional neural networks","user reviews","demographic predictions","deep learning","social media","single-task learning","single task learning","natural language processing"],"tags":["online social networks","single task learning","text mining","user reviews","demographic predictions","topic modeling","social media","machine learning","natural language processing","tool","mental health","multitask learning","aged","convolutional neural network","demographic attributes"]},{"p_id":10247,"title":"Convolutional Recurrent Deep Learning Model for Sentence Classification","abstract":"As the amount of unstructured text data that humanity produces overall and on the Internet grows, so does the need to intelligently to process it and extract different types of knowledge from it. Convolutional neural networks (CNNs) and recurrent neural networks (RNNs) have been applied to natural language processing systems with comparative, remarkable results. The CNN is a noble approach to extract higher level features that are invariant to local translation. However, it requires stacking multiple convolutional layers in order to capture long-term dependencies, due to the locality of the convolutional and pooling layers. In this paper, we describe a joint CNN and RNN framework to overcome this problem. Briefly, we use an unsupervised neural language model to train initial word embeddings that are further tuned by our deep learning network, then, the pre-trained parameters of the network are used to initialize the model. At a final stage, the proposed framework combines former information with a set of feature maps learned by a convolutional layer with long-term dependencies learned via long-short-term memory. Empirically, we show that our approach, with slight hyperparameter tuning and static vectors, achieves outstanding results on multiple sentiment analysis benchmarks. Our approach outperforms several existing approaches in term of accuracy; our results are also competitive with the state-of-the-art results on the Stanford Large Movie Review data set with 93.3% accuracy, and the Stanford Sentiment Treebank data set with 48.8% fine-grained and 89.2% binary accuracy, respectively. Our approach has a significant role in reducing the number of parameters and constructing the convolutional layer followed by the recurrent layer as a substitute for the pooling layer. Our results show that we were able to reduce the loss of detailed, local information and capture long-term dependencies with an efficient framework that has fewer parameters and a high level of performance.","keywords_author":["Convolutional neural network","deep learning","long-term dependencies","natural language processing","recurrent neural network","sentiment analysis","Convolutional neural network","recurrent neural network","natural language processing","deep learning","sentiment analysis","long-term dependencies"],"keywords_other":["LANGUAGE","Unstructured texts","Computational model","Local information","Sentence classifications","NEURAL-NETWORKS","Convolutional neural network","Long-term dependencies","Task analysis","Recurrent neural network (RNNs)"],"max_cite":2.0,"pub_year":2018.0,"sources":"['scp', 'wos']","rawkeys":["neural-networks","recurrent neural network (rnns)","task analysis","local information","deep learning","natural language processing","recurrent neural network","unstructured texts","sentence classifications","convolutional neural network","long-term dependencies","language","computational model","sentiment analysis"],"tags":["computational modeling","task analysis","local information","neural networks","machine learning","natural language processing","unstructured texts","sentence classifications","convolutional neural network","long-term dependencies","language","sentiment analysis"]},{"p_id":2055,"title":"Cross-modal associative memory by MultiSOM","abstract":"This paper proposed a novel Associative Memory model base on Self-organization Map(SOM), called MultiSOM. This model could learn associative relationships between data from different sources, mostly in different modality. However, data and relationships between them will not be entered into the network and trained directly. Instead, they should be trained each with a same semantic data and at last share one topological map. Cross-modally, this paper trains the MultiSOM model to learn associative memory between images and human voice of Chinese characters, with their meanings as sematic data, and the experiment results suggest that this MultiSOM model could learn the bidirectional associative relationship.","keywords_author":["Associative Memory","SOM","cross-modal","Chinese processing"],"keywords_other":["sematic data","Chinese characters","image processing","content-addressable storage","Semantics","Training","natural language processing","Biological neural networks","cross-modal associative memory","self-organising feature maps","Associative memory","Vectors","Organizations","self-organization map","Neurons","MultiSOM"],"max_cite":null,"pub_year":2014.0,"sources":"['ieee']","rawkeys":["content-addressable storage","cross-modal","self-organising feature maps","multisom","vectors","chinese processing","cross-modal associative memory","som","self-organization map","biological neural networks","neurons","chinese characters","semantics","training","organizations","sematic data","associative memory","image processing","natural language processing"],"tags":["sematic data","associative memory","chinese characters","content-addressable storage","image processing","semantics","organization","natural language processing","chinese processing","cross-modal","training","cross-modal associative memory","self-organising feature maps","multisom","self-organizing map","vectors","biological neural networks","neurons"]},{"p_id":51209,"title":"An approach to recognize temporal relations between Chinese events","abstract":"\u00a9 Springer International Publishing Switzerland 2015. The research on temporal relations between events plays an important role in natural language processing tasks, such as information extraction, question answering and text summarization. In this paper, we first annotate a document-level corpus to be used for the recognition of temporal relations between Chinese events. We then introduce several effective features according to the characteristics of Chinese, such as trigger semantics, special words, event arguments, event co-reference relation, etc. to improve system performance over the baseline. The experimental results on our annotated corpus show that our system outperforms the baseline by 3.55% of relative improvement in F1.","keywords_author":["Corpus","Event","Machine learning","Temporal relation"],"keywords_other":["Event","Question Answering","Corpus","Text summarization","Temporal relation","NAtural language processing","Special word"],"max_cite":0.0,"pub_year":2015.0,"sources":"['scp']","rawkeys":["question answering","text summarization","natural language processing","machine learning","special word","temporal relation","corpus","event"],"tags":["events","text summarization","natural language processing","machine learning","information retrieval","special word","temporal relation","corpus"]},{"p_id":15,"title":"Deep visual-semantic alignments for generating image descriptions","abstract":"We present a model that generates natural language descriptions of images and their regions. Our approach leverages datasets of images and their sentence descriptions to learn about the inter-modal correspondences between language and visual data. Our alignment model is based on a novel combination of Convolutional Neural Networks over image regions, bidirectional Recurrent Neural Networks over sentences, and a structured objective that aligns the two modalities through a multimodal embedding. We then describe a Multimodal Recurrent Neural Network architecture that uses the inferred alignments to learn to generate novel descriptions of image regions. We demonstrate that our alignment model produces state of the art results in retrieval experiments on Flickr8K, Flickr30K and MSCOCO datasets. We then show that the generated descriptions significantly outperform retrieval baselines on both full images and on a new dataset of region-level annotations.","keywords_author":null,"keywords_other":["MSCOCO dataset","convolutional neural network","Natural languages","sentence description","Visual data","recurrent neural nets","Image descriptions","learning (artificial intelligence)","deep visual-semantic alignment","visual data","natural language image description","learning","image retrieval","State of the art","Flickr8K dataset","multimodal recurrent neural network architecture","Image regions","Bidirectional recurrent neural networks","image segmentation","Visual semantics","intermodal correspondence","Flickr30K dataset","natural language processing","multimodal embedding","image description generation","image region","bidirectional recurrent neural network","Convolutional neural network","region-level annotation"],"max_cite":602.0,"pub_year":2015.0,"sources":"['wos', 'scp', 'ieee']","rawkeys":["flickr30k dataset","natural languages","state of the art","convolutional neural network","sentence description","flickr8k dataset","recurrent neural nets","mscoco dataset","bidirectional recurrent neural networks","learning (artificial intelligence)","deep visual-semantic alignment","visual data","image retrieval","learning","natural language image description","image descriptions","multimodal recurrent neural network architecture","image segmentation","image regions","intermodal correspondence","visual semantics","natural language processing","multimodal embedding","image description generation","image region","bidirectional recurrent neural network","region-level annotation"],"tags":["flickr30k dataset","natural languages","state of the art","convolutional neural network","sentence description","flickr8k dataset","recurrent neural nets","mscoco dataset","bidirectional recurrent neural networks","machine learning","deep visual-semantic alignment","visual data","image retrieval","natural language image description","image descriptions","multimodal recurrent neural network architecture","image segmentation","image regions","intermodal correspondence","visual semantics","natural language processing","multimodal embedding","image description generation","region-level annotation"]},{"p_id":26639,"title":"MDLText: An efficient and lightweight text classifier","abstract":"\u00a9 2016In many areas, the volume of text information is increasing rapidly, thereby demanding efficient text classification approaches. Several methods are available at present, but most exhibit declining performance as the dimensionality of the problem increases, or they incur high computational costs for training, which limit their application in real scenarios. Thus, it is necessary to develop a method that can process high dimensional data in a rapid manner. In this study, we propose the MDLText, an efficient, lightweight, scalable, and fast multinomial text classifier, which is based on the minimum description length principle. MDLText exhibits fast incremental learning as well as being sufficiently robust to prevent overfitting, which are desirable features in real-world applications, large-scale problems, and online scenarios. Our experiments were carefully designed to ensure that we obtained statistically sound results, which demonstrated that the proposed approach achieves a good balance between predictive power and computational efficiency.","keywords_author":["Classification","Machine learning","Minimum description length","Natural language processing","Text categorization"],"keywords_other":["Minimum description length principle","Text categorization","Text classification","Minimum description length","Large-scale problem","High dimensional data","Incremental learning","NAtural language processing"],"max_cite":6.0,"pub_year":2017.0,"sources":"['scp']","rawkeys":["high dimensional data","incremental learning","machine learning","natural language processing","minimum description length principle","large-scale problem","classification","minimum description length","text categorization","text classification"],"tags":["high dimensional data","incremental learning","machine learning","natural language processing","minimum description length principle","large-scale problem","classification","minimum description length","text categorization","text classification"]},{"p_id":2066,"title":"Study on Distributed Representation of Words with Sparse Neural Network Language Model","abstract":"These days a neural network is paid attention to again since it is improved as deep learning. Deep learning achieves good data representation according to a data distribution and get over state of the art classifiers in computer vision and speech recognition. The representation captures abstracts of many data and is used as general features to solve many types of classification problems. A neural network is applied to natural language processing, too. In natural language processing neural networks achieve the best distributed representation of words and many researchers pay attention to the neural network language model. The distributed representation allocates words in a continuous feature space and there are semantically or syntactically similar words near area in the space. Hence, the distributed representation contributes to a solution of analogical reasoning tasks. In this paper a sparse neural network language model (SNNLM) is used, which achieves sparse active neurons in the hidden layer and a distributed representation of words is obtained. In evaluational experiments SNNLM selects words that do not occur in the same sentence at all as related words and it is confirmed that the word selection is appropriate manually.","keywords_author":["Natural language processing","Neural network"],"keywords_other":["Equations","natural language processing","word processing","sparse neural network language model","Biological neural networks","Mathematical model","Vectors","sparse active neurons","distributed word representation","word selection","neural nets","SNNLM","Neurons","Natural language processing"],"max_cite":null,"pub_year":2014.0,"sources":"['ieee']","rawkeys":["mathematical model","equations","neural network","natural language processing","vectors","sparse neural network language model","word processing","snnlm","sparse active neurons","distributed word representation","word selection","biological neural networks","neurons","neural nets"],"tags":["mathematical model","equations","neural networks","natural language processing","vectors","sparse neural network language model","word processing","sparse active neurons","distributed word representation","word selection","biological neural networks","snnlm","neurons"]},{"p_id":26643,"title":"Neural Network Methods for Natural Language Processing","abstract":"Copyright \u00a9 2017 by Morgan & Claypool. Neural networks are a family of powerful machine learning models. This book focuses on the application of neural network models to natural language data. The first half of the book (Parts I and II) covers the basics of supervised machine learning and feed-forward neural networks, the basics of working with machine learning over language data, and the use of vector-based rather than symbolic representations for words. It also covers the computation-graph abstraction, which allows to easily define and train arbitrary neural networks, and is the basis behind the design of contemporary neural network software libraries. The second part of the book (Parts III and IV) introduces more specialized neural network architectures, including 1D convolutional neural networks, recurrent neural networks, conditioned-generation models, and attention-based models. These architectures and techniques are the driving force behind state-of-the-art algorithms for machine translation, syntactic parsing, and many other applications. Finally, we also discuss tree-shaped networks, structured prediction, and the prospects of multi-task learning.","keywords_author":["deep learning","machine learning","natural language processing","neural networks","recurrent neural networks","sequence to sequence models","supervised learning","word embeddings"],"keywords_other":["Sequence models","Symbolic representation","Neural network software","State-of-the-art algorithms","Machine learning models","Convolutional neural network","Embeddings","Supervised machine learning"],"max_cite":6.0,"pub_year":2017.0,"sources":"['scp', 'ieee']","rawkeys":["embeddings","supervised learning","state-of-the-art algorithms","deep learning","neural network software","machine learning models","neural networks","machine learning","natural language processing","sequence models","sequence to sequence models","recurrent neural networks","symbolic representation","word embeddings","convolutional neural network","supervised machine learning"],"tags":["embeddings","supervised learning","sequence modeling","state-of-the-art algorithms","neural network software","neural networks","machine learning models","symbolic representation","machine learning","natural language processing","word embedding","sequence-to-sequence model","convolutional neural network","supervised machine learning"]},{"p_id":20,"title":"Show and tell: A neural image caption generator","abstract":"Automatically describing the content of an image is a fundamental problem in artificial intelligence that connects computer vision and natural language processing. In this paper, we present a generative model based on a deep recurrent architecture that combines recent advances in computer vision and machine translation and that can be used to generate natural sentences describing an image. The model is trained to maximize the likelihood of the target description sentence given the training image. Experiments on several datasets show the accuracy of the model and the fluency of the language it learns solely from image descriptions. Our model is often quite accurate, which we verify both qualitatively and quantitatively. For instance, while the current state-of-the-art BLEU-1 score (the higher the better) on the Pascal dataset is 25, our approach yields 59, to be compared to human performance around 69. We also show BLEU-1 score improvements on Flickr30k, from 56 to 66, and on SBU, from 19 to 28. Lastly, on the newly released COCO dataset, we achieve a BLEU-4 of 27.7, which is the current state-of-the-art.","keywords_author":null,"keywords_other":["COCO dataset","Measurement","machine translation","Google","Machine translations","Flickr30k","recurrent neural nets","natural sentence generation","Visualization","SBU","Human performance","Image descriptions","language fluency","Target descriptions","BLEU-1 score","generative model","learning (artificial intelligence)","Training","learning","State of the art","Pascal dataset","Training image","computer vision","Logic gates","NAtural language processing","artificial intelligence","Generative model","Recurrent neural networks","natural language processing","image content description","deep recurrent architecture","neural image caption generator"],"max_cite":566.0,"pub_year":2015.0,"sources":"['wos', 'scp', 'ieee']","rawkeys":["bleu-1 score","state of the art","machine translation","recurrent neural networks","sbu","target descriptions","logic gates","visualization","google","recurrent neural nets","natural sentence generation","language fluency","machine translations","measurement","generative model","learning (artificial intelligence)","pascal dataset","training","learning","image descriptions","neural image caption generator","computer vision","training image","artificial intelligence","coco dataset","natural language processing","image content description","flickr30k","human performance","deep recurrent architecture"],"tags":["bleu-1 score","state of the art","sbu","target descriptions","logic gates","visualization","google","recurrent neural nets","natural sentence generation","machine learning","language fluency","machine translations","measurement","generative model","pascal dataset","neural networks","training","image descriptions","neural image caption generator","computer vision","training image","coco dataset","natural language processing","image content description","flickr30k","human performance","deep recurrent architecture"]},{"p_id":36884,"title":"Detecting phishing attacks from URL by using NLP techniques DDI Y\u00f6ntemleri ile Oltalama Saldirilarinin URL'Den Tespit Edilmesi","abstract":"\u00a9 2017 IEEE. Nowadays, cyber attacks affect many institutions and individuals, and they result in a serious financial loss for them. Phishing Attack is one of the most common types of cyber attacks which is aimed at exploiting people's weaknesses to obtain confidential information about them. This type of cyber attack threats almost all internet users and institutions. To reduce the financial loss caused by this type of attacks, there is a need for awareness of the users as well as applications with the ability to detect them. In the last quarter of 2016, Turkey appears to be second behind China with an impact rate of approximately 43% in the Phishing Attack Analysis report between 45 countries. In this study, firstly, the characteristics of this type of attack are explained, and then a machine learning based system is proposed to detect them. In the proposed system, some features were extracted by using Natural Language Processing (NLP) techniques. The system was implemented by examining URLs used in Phishing Attacks before opening them with using some extracted features. Many tests have been applied to the created system, and it is seen that the best algorithm among the tested ones is the Random Forest algorithm with a success rate of89.9%.","keywords_author":["Cyberattackdetection","Cybersecurity","Machine learning","NLP","Phishingattack","Random forestalgorithm"],"keywords_other":["Phishingattack","Cyber security","Random forestalgorithm","Cyberattackdetection","Financial loss","Confidential information","Internet users","Phishing attacks"],"max_cite":1.0,"pub_year":2017.0,"sources":"['scp']","rawkeys":["cyberattackdetection","cybersecurity","nlp","confidential information","machine learning","internet users","phishing attacks","random forestalgorithm","phishingattack","cyber security","financial loss"],"tags":["random forest algorithm","confidential information","machine learning","internet users","natural language processing","phishing attacks","phishingattack","cyber security","cyber-attack detection","financial loss"]},{"p_id":45080,"title":"Personal attributes extraction in Chinese text based on distant-supervision and LSTM","abstract":"\u00a9 Springer Nature Singapore Pte Ltd. 2018. In this paper, we proposed a distant-supervision approach to solve the problem of insufficient training corpus for extracting attribute from the unstructured text, by using the wiki infobox information to tag the Wikipedia text to get the training corpus. We consider the extract attribute as the sequence annotation question and use the wiki personal text as the training corpus. The clp-2014 task4 is used as the test corpus to test. The experiment result show that this method can enhance the quality of the attribute extraction.","keywords_author":["Deep learning","Distant-supervised","Entity attribute extraction","LSTM","NLP","Sequence padding"],"keywords_other":["Personal attributes","Entity attribute extractions","Distant-supervised","Sequence padding","Unstructured texts","Training corpus","LSTM","Attribute extraction"],"max_cite":0.0,"pub_year":2018.0,"sources":"['scp']","rawkeys":["lstm","nlp","personal attributes","deep learning","unstructured texts","entity attribute extractions","training corpus","attribute extraction","sequence padding","entity attribute extraction","distant-supervised"],"tags":["personal attributes","long short-term memory","machine learning","entity attribute extractions","distant supervision","natural language processing","training corpus","attribute extraction","sequence padding","unstructured texts"]},{"p_id":16413,"title":"Automatic detection and rating of dementia of alzheimer type through lexical analysis of spontaneous speech","abstract":"Current methods of assessing dementia of Alzheimer type (DAT) in older adults involve structured interviews that attempt to capture the complex nature of deficits suffered. One of the most significant areas affected by the disease is the capacity for functional communication as linguistic skills break down. These methods often do note capture the true nature of language deficits in spontaneous speech. We address this issue by exploring novel automatic and objective methods for diagnosing patients through analysis of spontaneous speech. We detail several lexical approaches to the problem of detecting and rating DAT. The approaches explored rely on character n-gram-based techniques, shown recently to perform successfully in a different, but related task of automatic authorship attribution. We also explore the correlation of usage frequency of different parts of speech and DAT. We achieve a high 95% accuracy of detecting dementia when compared with a control group, and we achieve 70% accuracy in rating dementia in two classes, and 50% accuracy in rating dementia into four classes. Our results show that purely computational solutions offer a viable alternative to standard approaches to diagnosing the level of impairment in patients. These results are significant step forward toward automatic and objective means to identifying early symptoms of DAT in older adults. \u00a9 2005 IEEE.","keywords_author":["Automatic diagnostics","Machine learning","Natural language processing"],"keywords_other":["Automatic diagnostics","Objective methods"],"max_cite":37.0,"pub_year":2005.0,"sources":"['scp']","rawkeys":["natural language processing","automatic diagnostics","machine learning","objective methods"],"tags":["natural language processing","automatic diagnostics","machine learning","objective methods"]},{"p_id":38942,"title":"Speeding up operations on feature terms using constraint programming and variable symmetry","abstract":"\u00a9 2014 Elsevier B.V. All rights reserved.Feature terms are a generalization of first-order terms which have recently received increased attention for their usefulness in structured machine learning, natural language processing and other artificial intelligence applications. One of the main obstacles for their wide usage is that, when set-valued features are allowed, their basic operations (subsumption, unification, and antiunification) have a very high computational cost. We present a Constraint Programming formulation of these operations, which in some cases provides orders of magnitude speed-ups with respect to the standard approaches. In addition, exploiting several symmetries - that often appear in feature terms databases - causes substantial additional savings. We provide experimental results of the benefits of this approach.","keywords_author":["Constraint programing","Feature terms","Inductive logic programming","Structured machine learning","Symmetries"],"keywords_other":["Constraint programming","Symmetries","Orders of magnitude","Feature terms","Constraint programing","Computational costs","Variable symmetries","NAtural language processing"],"max_cite":1.0,"pub_year":2015.0,"sources":"['scp']","rawkeys":["structured machine learning","orders of magnitude","symmetries","variable symmetries","natural language processing","feature terms","constraint programming","computational costs","constraint programing","inductive logic programming"],"tags":["structured machine learning","orders of magnitude","variable symmetries","natural language processing","symmetry","feature terms","constraint programming","computational costs","inductive logic programming"]},{"p_id":8225,"title":"Using lexical chains for keyword extraction","abstract":"Keywords can be considered as condensed versions of documents and short forms of their summaries. In this paper, the problem of automatic extraction of keywords from documents is treated as a supervised learning task. A lexical chain holds a set of semantically related words of a text and it can be said that a lexical chain represents the semantic content of a portion of the text. Although lexical chains have been extensively used in text summarization, their usage for keyword extraction problem has not been fully investigated. In this paper, a keyword extraction technique that uses lexical chains is described, and encouraging results are obtained. \u00a9 2007 Elsevier Ltd. All rights reserved.","keywords_author":["Keyword extraction","Lexical chains","Machine learning","Natural language processing"],"keywords_other":["Lexical chains","Keyword extraction"],"max_cite":107.0,"pub_year":2007.0,"sources":"['scp']","rawkeys":["natural language processing","keyword extraction","machine learning","lexical chains"],"tags":["natural language processing","keyword extraction","machine learning","lexical chain"]},{"p_id":26659,"title":"A multiclass classification method based on deep learning for named entity recognition in electronic medical records","abstract":"\u00a9 2016 IEEE. Research of named entity recognition (NER) on electrical medical records (EMRs) focuses on verifying whether methods to NER in traditional texts are effective for that in EMRs, and there is no model proposed for enhancing performance of NER via deep learning from the perspective of multiclass classification. In this paper, we annotate a real EMR corpus to accomplish the model training and evaluation. And, then, we present a Convolutional Neural Network (CNN) based multiclass classification method for mining named entities from EMRs. The method consists of two phases. In the phase 1, EMRs are pre-processed for representing samples with word embedding. In the phase 2, the method is built by segmenting training data into many subsets and training a CNN binary classification model on each of subset. Experimental results showed the effectiveness of our method.","keywords_author":["convolutional neural network","electrical medical records","machine learning","named entity recognition","natural language processing"],"keywords_other":["Binary classification","Multi-class classification","Multiclass classification methods","Named entity recognition","Convolutional neural network","Electrical medical records","Electronic medical record","NAtural language processing"],"max_cite":6.0,"pub_year":2016.0,"sources":"['scp', 'ieee']","rawkeys":["multiclass classification methods","named entity recognition","machine learning","electrical medical records","binary classification","natural language processing","electronic medical record","convolutional neural network","multi-class classification"],"tags":["multiclass classification methods","named entity recognition","machine learning","electrical medical records","binary classification","natural language processing","electronic medical record","convolutional neural network","multi-class classification"]},{"p_id":45100,"title":"Extraction of Professional Details from Web-URLs using DeepDive","abstract":"\u00a9 2018 The Authors. Published by Elsevier Ltd. Manual extraction of data from unstructured data sources like websites is labour intensive and becomes almost in-feasible at large scale. Recent state-of-the-art techniques for the task of information extraction show encouraging results. In this work, we make an attempt to extract professional details like name, email, address, contact number, and specialization from home pages of doctors. The work covers two possible scenarios of websites having these details. One scenario is where a website contains details of a single doctor. Another scenario is where a website may contain multiple information of multiple doctors\/professionals at the same time. The problem is attempted to be solved as a relation extraction task for Information Extraction. The proposed solution has been built on top of DeepDive, a tool developed by Stanford. In both scenarios, DeepDive takes pre-processed data sentences as input and constructs entity-relations. For each entity-relation, DeepDive computes a probability that the relationship is a correct match using distance supervision and user-defined heuristic rules. In case of experiment-1, our system achieves 69.14% accuracy for the name, 88.67% accuracy for location and 100% for email, number and specialization. In case of experiment-2, the observed probabilities are not so significant and mostly around 0.5-0.7 but we present some solutions for future work. The techniques presented here can easily be extended to generalize for other types of professionals too and not just doctors.","keywords_author":["DeepDive","Distance Supervision","Information Extraction","Machine Learning","Natural Language Processing"],"keywords_other":["Relation extraction","DeepDive","Labour-intensive","Observed probabilities","Pre-processed data","Heuristic rules","Unstructured data","Distance Supervision"],"max_cite":0.0,"pub_year":2018.0,"sources":"['scp']","rawkeys":["distance supervision","information extraction","machine learning","natural language processing","pre-processed data","observed probabilities","unstructured data","labour-intensive","deepdive","relation extraction","heuristic rules"],"tags":["distance supervision","information extraction","machine learning","natural language processing","pre-processed data","observed probabilities","unstructured data","labour-intensive","deepdive","relation extraction","heuristic rules"]},{"p_id":49197,"title":"General symptom extraction from VA electronic medical notes","abstract":"\u00a9 2017 International Medical Informatics Association (IMIA) and IOS Press.There is need for cataloging signs and symptoms, but not all are documented in structured data. The text from clinical records are an additional source of signs and symptoms. We describe a Natural Language Processing (NLP) technique to identify symptoms from text. Using a human-annotated reference corpus from VA electronic medical notes we trained and tested an NLP pipeline to identify and categorize symptoms. The technique includes a model created from an automatic machine learning model selection tool. Tested on a hold-out set, its precision at the mention level was 0.80, recall 0.74 and an overall f-score of 0.80. The tool was scaled-up to process a large corpus of 964,105 patient records.","keywords_author":["Diagnosis","Machine learning","Natural language processing"],"keywords_other":null,"max_cite":0.0,"pub_year":2017.0,"sources":"['scp']","rawkeys":["diagnosis","natural language processing","machine learning"],"tags":["diagnosis","natural language processing","machine learning"]},{"p_id":49,"title":"Big data deep learning: Challenges and perspectives","abstract":"Deep learning is currently an extremely active research area in machine learning and pattern recognition society. It has gained huge successes in a broad area of applications such as speech recognition, computer vision, and natural language processing. With the sheer size of data available today, big data brings big opportunities and transformative potential for various sectors; on the other hand, it also presents unprecedented challenges to harnessing data and information. As the data keeps getting bigger, deep learning is coming to play a key role in providing big data predictive analytics solutions. In this paper, we provide a brief overview of deep learning, and highlight current research efforts and the challenges to big data, as well as the future trends.","keywords_author":["Classifier design and evaluation","Feature representation","Machine learning","Neural nets models","Parallel processing","Classifier design and evaluation","feature representation","machine learning","neural nets models","parallel processing"],"keywords_other":["parallel processing","Pattern recognition","Information analysis","Feature representation","Parallel processing","Predictive analytics","machine learning","Machine learning","predictive analytics solutions","Future trends","learning (artificial intelligence)","Research efforts","deep learning","Classifier design and evaluation","Big Data","neural nets models","feature representation","Data processing","NAtural language processing","Big data","Data and information","pattern recognition","data analysis","Natural language processing"],"max_cite":153.0,"pub_year":2014.0,"sources":"['ieee', 'wos', 'scp']","rawkeys":["parallel processing","learning (artificial intelligence)","big data","deep learning","predictive analytics","information analysis","future trends","machine learning","natural language processing","classifier design and evaluation","data processing","data and information","neural nets models","predictive analytics solutions","pattern recognition","feature representation","research efforts","data analysis"],"tags":["parallel processing","big data","predictive analytics","machine learning","future trends","information analysis","natural language processing","classifier design and evaluation","data processing","data and information","neural nets models","predictive analytics solutions","pattern recognition","feature representation","research efforts","data analysis"]},{"p_id":2045,"title":"Multiple time-span feature fusion for deep neural network modeling","abstract":"In this paper, we exploit long term information from multiple time-spans for automatic speech recognition. The multiple time-span information is encoded into three different feature streams: speaker-adaptation-transformed features, deep bottleneck features and deep hierarchical bottleneck features. By combining three different time-spans in discriminative acoustic modeling, the character\/syllable error rate improves for Mandarin and Vietnamese conversational telephone speech recognition. We obtain 0.8% and 1.9% absolute over DNN-HMM baselines in character error rate and syllable error rate for Mandarin and Vietnamese, respectively. Further analysis also suggests that our proposed feature fusion approach is able to encode finer-grain temporal information than directly using input features of long time-spans in DNN-HMM baselines.","keywords_author":["Feature representation","deep bottleneck","deep hierarchical bottleneck","deep neural network (DNN)","Hidden Markov model (HMM)"],"keywords_other":["Vietnamese language","automatic speech recognition","syllable error rate","speech recognition","conversational telephone speech recognition","DNN-HMM baselines","Mandarin language","Neural networks","discriminative acoustic modeling","Feature extraction","Speech","error statistics","acoustic signal processing","deep neural network modeling","hidden Markov models","multiple time-span feature fusion","speaker-adaptation-transformed features","Training","character error rate","hidden Markov model","multiple time-span information","Hidden Markov models","deep hierarchical bottleneck features","neural nets","natural language processing","long term information","Speech recognition","feature extraction","feature streams","sensor fusion","Acoustics"],"max_cite":2.0,"pub_year":2014.0,"sources":"['ieee']","rawkeys":["automatic speech recognition","syllable error rate","speech recognition","conversational telephone speech recognition","hidden markov model","acoustics","discriminative acoustic modeling","dnn-hmm baselines","mandarin language","error statistics","acoustic signal processing","deep hierarchical bottleneck","speech","deep neural network (dnn)","deep neural network modeling","neural networks","multiple time-span feature fusion","speaker-adaptation-transformed features","training","character error rate","hidden markov model (hmm)","multiple time-span information","vietnamese language","deep hierarchical bottleneck features","feature representation","neural nets","hidden markov models","deep bottleneck","natural language processing","long term information","feature extraction","feature streams","sensor fusion"],"tags":["automatic speech recognition","long-term information","syllable error rate","speech recognition","conversational telephone speech recognition","convolutional neural network","acoustics","discriminative acoustic modeling","dnn-hmm baselines","mandarin language","error statistics","acoustic signal processing","deep hierarchical bottleneck","speech","neural networks","multiple time-span feature fusion","speaker-adaptation-transformed features","training","character error rate","multiple time-span information","vietnamese language","deep hierarchical bottleneck features","feature representation","deep neural network model","hidden markov models","deep bottleneck","natural language processing","feature extraction","feature streams","sensor fusion"]},{"p_id":53300,"title":"Tamil question classification using morpheme features","abstract":"Question classification plays an important role in question answering systems. This paper presents the Conditional Random field (CRF) model based on Morpheme features for Tamil question classification. It is a process that analyzes a question and labels it based on its question type and expected answer type (EAT). The selected features are the morpheme parts of the question terms and its dependent terms. The main contribution in this work is in the way of selection of features for constructing CRF Model. They discriminates the position of expected answer type information with respect to question term's position. The CRF model to find out the phrase which contains the information about EAT is trained with tagged question corpus. The EAT is semantically derived by analyzing the phrase obtained from CRF engine using WordNet. The performance of this morpheme based CRF model is compared with the generic CRF engine. \u00a9 2008 Springer-Verlag Berlin Heidelberg.","keywords_author":["Classification","Machine learning"],"keywords_other":["Type information","Word net","Question types","Question classification","Machine learning","International conferences","Classification","Model based","Conditional random field","NAtural language processing","Question-answering systems"],"max_cite":0.0,"pub_year":2008.0,"sources":"['scp']","rawkeys":["word net","question-answering systems","question types","conditional random field","international conferences","machine learning","natural language processing","model based","question classification","classification","type information"],"tags":["wordnet","conditional random field","international conferences","machine learning","natural language processing","question answering systems","question classification","question type","classification","type information","model-based"]},{"p_id":26677,"title":"SentiWords: Deriving a High Precision and High Coverage Lexicon for Sentiment Analysis","abstract":"\u00a9 2016 IEEE. Deriving prior polarity lexica for sentiment analysis - where positive or negative scores are associated with words out of context - is a challenging task. Usually, a trade-off between precision and coverage is hard to find, and it depends on the methodology used to build the lexicon. Manually annotated lexica provide a high precision but lack in coverage, whereas automatic derivation from pre-existing knowledge guarantees high coverage at the cost of a lower precision. Since the automatic derivation of prior polarities is less time consuming than manual annotation, there has been a great bloom of these approaches, in particular based on the SentiWordNet resource. In this paper, we compare the most frequently used techniques based on SentiWordNet with newer ones and blend them in a learning framework (a so called 'ensemble method'). By taking advantage of manually built prior polarity lexica, our ensemble method is better able to predict the prior value of unseen words and to outperform all the other SentiWordNet approaches. Using this technique we have built SentiWords, a prior polarity lexicon of approximately 155,000 words, that has both a high precision and a high coverage. We finally show that in sentiment analysis tasks, using our lexicon allows us to outperform both the single metrics derived from SentiWordNet and popular manually annotated sentiment lexica.","keywords_author":["machine learning","Natural language processing","text analysis"],"keywords_other":["Lower precision","Text analysis","Sentiment analysis","Learning frameworks","Manual annotation","Automatic derivation","High-precision","Ensemble methods"],"max_cite":6.0,"pub_year":2016.0,"sources":"['scp', 'wos']","rawkeys":["manual annotation","automatic derivation","ensemble methods","machine learning","natural language processing","learning frameworks","text analysis","lower precision","high-precision","sentiment analysis"],"tags":["manual annotation","automatic derivation","ensemble methods","machine learning","natural language processing","learning frameworks","text analysis","lower precision","high-precision","sentiment analysis"]},{"p_id":49206,"title":"A semi-automatic framework to identify abnormal states in EHR narratives","abstract":"\u00a9 2017 International Medical Informatics Association (IMIA) and IOS Press.Disease ontology, defined as a causal chain of abnormal states, is believed to be a valuable knowledge base in medical information systems. Automatic mapping between electronic health records (EHR) and disease ontology is indispensable for applying disease ontology in real clinical settings. Based on an analysis of ontologies of 148 chronic diseases, approximately 41% of abnormal states require information extraction from clinical narratives. This paper presents a semi-automatic framework to identify abnormal states in clinical narratives. This framework aims to effectively build mapping modules between EHR and disease ontology. We show that the proposed method is effective in data mapping for 18%-33% of the abnormal states in the ontologies of chronic diseases. Moreover, we analyze the abnormal states for which our method is invalid in extracting information from clinical narratives.","keywords_author":["Machine learning","Natural language processing","Ontology"],"keywords_other":["Electronic Health Records","Humans","Information Storage and Retrieval","Narration","Data Mining","Diagnosis"],"max_cite":0.0,"pub_year":2017.0,"sources":"['scp']","rawkeys":["diagnosis","narration","data mining","ontology","machine learning","electronic health records","humans","natural language processing","information storage and retrieval"],"tags":["diagnosis","narration","data mining","machine learning","electronic health records","humans","natural language processing","information storage and retrieval"]},{"p_id":51255,"title":"Extracting information from electronic medical records to identify obesity status of a patient based on comorbidities and bodyweight measures","abstract":"\u00a9 Springer International Publishing Switzerland 2015. Obesity is a chronic disease with an increasing impact on the world\u2019s population. In this work, we present a method to identify obesity using text mining techniques and information related to body weight measures and obesity comorbidities. We used a dataset of 2412 de-identified medical records that contains labels for two classification problems. The first classification problem recognizes between obesity, overweight, normal weight, and underweight. The second problem of classification corresponds to the obesity types under the obesity category to recognize between super obesity, morbid obesity, severe obesity and moderate obesity. We used a Bag of Words approach to represent the records together with unigram and bigram representation of the features. We used Support Vector Machine and Na\u00efve Bayes together with ten-fold cross validation to evaluate and compare performances. In general, our results show that Support Vector Machine obtains better performances than Na\u00efve Bayes for both classification problems. We also observed that bigram representation improves performance compared with unigram representation.","keywords_author":["Comorbidities","Machine learning","Natural language processing","Obesity"],"keywords_other":["Cross validation","NAtural language processing","Obesity","Text mining techniques","Comorbidities","Electronic medical record","Chronic disease","Extracting information"],"max_cite":0.0,"pub_year":2015.0,"sources":"['scp']","rawkeys":["obesity","cross validation","machine learning","natural language processing","extracting information","comorbidities","chronic disease","electronic medical record","text mining techniques"],"tags":["obesity","comorbidity","machine learning","natural language processing","extracting information","chronic disease","electronic medical record","computer vision","text mining techniques"]},{"p_id":2103,"title":"Adapting deep belief nets to Chinese entity detection","abstract":"This paper adapts deep belief networks (DBN) to detect entity mentions in Chinese documents. Our results exhibit how the depth of architecture and quantity of unit in hidden layer influence the performance. Different feature combinations are used to show their advantages and disadvantages in DBN for this task. Moreover, we combined Chinese word segmentation systems to alleviate word segmentation error. Token labels are produced independently by DBN which does not concerned what are the token labels before current word. Viterbi algorithm is a good solution to find the most likely probability label path to make DBN be more effective for entity detection. Furthermore, this paper demonstrates DBN is a proper model for our tasks and its results are better than Support Vector Machine (SVM), Artificial Neural Network (ANN) and Conditional Random Field (CRF).","keywords_author":["deep belief nets","entity detection","viterbi algorithm"],"keywords_other":["Artificial neural networks","Abstracts","deep belief networks","document handling","probability label path","Computer architecture","word segmentation error","Feature extraction","Chinese documents","Learning systems","Chinese entity detection","DBN","belief networks","probability","Viterbi algorithm","entity mention detection","token labels","Chinese word segmentation systems","natural language processing","Support vector machines"],"max_cite":null,"pub_year":2013.0,"sources":"['ieee']","rawkeys":["entity detection","chinese entity detection","document handling","probability label path","learning systems","viterbi algorithm","word segmentation error","dbn","deep belief nets","abstracts","computer architecture","belief networks","probability","entity mention detection","token labels","natural language processing","chinese documents","artificial neural networks","feature extraction","support vector machines","chinese word segmentation systems","deep belief networks"],"tags":["entity detection","chinese entity detection","document handling","probability label path","learning systems","viterbi algorithm","word segmentation error","machine learning","abstracts","computer architecture","neural networks","belief networks","probability","entity mention detection","token labels","natural language processing","chinese documents","feature extraction","chinese word segmentation systems","deep belief networks"]},{"p_id":24633,"title":"Knowledge based word-concept model estimation and refinement for biomedical text mining","abstract":"\u00a9 2014 Elsevier Inc.Text mining of scientific literature has been essential for setting up large public biomedical databases, which are being widely used by the research community. In the biomedical domain, the existence of a large number of terminological resources and knowledge bases (KB) has enabled a myriad of machine learning methods for different text mining related tasks. Unfortunately, KBs have not been devised for text mining tasks but for human interpretation, thus performance of KB-based methods is usually lower when compared to supervised machine learning methods. The disadvantage of supervised methods though is they require labeled training data and therefore not useful for large scale biomedical text mining systems. KB-based methods do not have this limitation.In this paper, we describe a novel method to generate word-concept probabilities from a KB, which can serve as a basis for several text mining tasks. This method not only takes into account the underlying patterns within the descriptions contained in the KB but also those in texts available from large unlabeled corpora such as MEDLINE. The parameters of the model have been estimated without training data. Patterns from MEDLINE have been built using MetaMap for entity recognition and related using co-occurrences.The word-concept probabilities were evaluated on the task of word sense disambiguation (WSD). The results showed that our method obtained a higher degree of accuracy than other state-of-the-art approaches when evaluated on the MSH WSD data set. We also evaluated our method on the task of document ranking using MEDLINE citations. These results also showed an increase in performance over existing baseline retrieval approaches.","keywords_author":["Biomedical literature","Information retrieval","Text mining","Word sense disambiguation","Word-concept probability"],"keywords_other":["Text mining","Biomedical literature","Probability","Unified Medical Language System","MEDLINE","Biomedical text minings","Labeled training data","Artificial Intelligence","Knowledge Bases","Algorithms","Word Sense Disambiguation","Machine learning methods","Models, Statistical","Semantics","Natural Language Processing","State-of-the-art approach","Data Mining","Supervised machine learning","Computational Biology"],"max_cite":9.0,"pub_year":2015.0,"sources":"['scp']","rawkeys":["medline","statistical","unified medical language system","biomedical text minings","word-concept probability","biomedical literature","text mining","models","algorithms","knowledge bases","data mining","semantics","information retrieval","word sense disambiguation","probability","machine learning methods","artificial intelligence","labeled training data","computational biology","natural language processing","state-of-the-art approach","supervised machine learning"],"tags":["medline","knowledge base","biomedical text minings","word-concept probability","biomedical literature","text mining","machine learning","algorithms","unified medical language systems","data mining","semantics","information retrieval","word sense disambiguation","probability","machine learning methods","statistics","labeled training data","model","computational biology","natural language processing","state-of-the-art approach","supervised machine learning"]},{"p_id":34876,"title":"Automating credibility assessment of Arabic news","abstract":"During the past few years internet has witnessed a massive increase of Arabic language users. Accompanied with this increase in the number of users is an increase in e-publishing. However, necessary laws and regulations are not yet available to control the credibility of e-published content. Furthermore, many political conflicts have risen after the Arab Spring. All of this led to an increasing demand for assessing the credibility of news in general and e-news in particular. In this work, we present a system for automating credibility assessment of a news article based on two of the most important and most frequently violated criteria; (i) Does the news article indicate the source of its information? (ii) Does the news article indicate the time of occurrence of the reported event? For each of the chosen criteria, we build a classification model to classify a news article as either violating the criteria or not. News articles previously evaluated by MCE Watch (a manual service for news credibility assessment) are used in building and evaluation of our model. Experimental evaluations show that our model has accuracy that exceeds 82% for both criteria. \u00a9 2013 Springer International Publishing.","keywords_author":["Arabic language","credibility","machine learning","natural language processing","news"],"keywords_other":["Experimental evaluation","Classification models","news","Arabic languages","credibility","Laws and regulations","NAtural language processing","Credibility assessment"],"max_cite":2.0,"pub_year":2013.0,"sources":"['scp']","rawkeys":["laws and regulations","credibility assessment","experimental evaluation","arabic language","machine learning","natural language processing","news","credibility","arabic languages","classification models"],"tags":["laws and regulations","credibility assessment","experimental evaluation","machine learning","natural language processing","news","credibility","arabic languages","classification models"]},{"p_id":2109,"title":"Improving deep neural networks for LVCSR using dropout and shrinking structure","abstract":"Recently, the hybrid deep neural networks and hidden Markov models (DNN\/HMMs) have achieved dramatic gains over the conventional GMM\/HMMs method on various large vocabulary continuous speech recognition (LVCSR) tasks. In this paper, we propose two new methods to further improve the hybrid DNN\/HMMs model: i) use dropout as pre-conditioner (DAP) to initialize DNN prior to back-propagation (BP) for better recognition accuracy; ii) employ a shrinking DNN structure (sDNN) with hidden layers decreasing in size from bottom to top for the purpose of reducing model size and expediting computation time. The proposed DAP method is evaluated in a 70-hour Mandarin transcription (PSC) task and the 309-hour Switchboard (SWB) task. Compared with the traditional greedy layer-wise pre-trained DNN, it can achieve about 10% and 6.8% relative recognition error reduction for PSC and SWB tasks respectively. In addition, we also evaluate sDNN as well as its combination with DAP on the SWB task. Experimental results show that these methods can reduce model size to 45% of original size and accelerate training and test time by 55%, without losing recognition accuracy.","keywords_author":["dropout","dropout as pre-conditioner (DAP)","shrinking hidden layer","deep neural networks","LVCSR","DNN-HMM"],"keywords_other":["DAP","SWB task","speech recognition","Neural networks","Computational modeling","dropout as preconditioner","time 309 hr","hybrid DNN\/HMMs model","model size reduction","Speech","time 70 hour","PSC","hybrid deep neural networks","LVCSR tasks","large vocabulary continuous speech recognition tasks","hidden Markov models","sDNN","Training","Hidden Markov models","neural nets","Mandarin transcription task","shrinking DNN structure","natural language processing","Switches","Speech recognition","switchboard task","backpropagation"],"max_cite":18.0,"pub_year":2014.0,"sources":"['wos', 'ieee']","rawkeys":["sdnn","speech recognition","dnn-hmm","swb task","dropout as preconditioner","dap","time 309 hr","model size reduction","dropout as pre-conditioner (dap)","time 70 hour","hybrid dnn\/hmms model","switches","speech","hybrid deep neural networks","shrinking hidden layer","computational modeling","large vocabulary continuous speech recognition tasks","lvcsr tasks","mandarin transcription task","neural networks","training","lvcsr","neural nets","dropout","hidden markov models","shrinking dnn structure","deep neural networks","natural language processing","psc","switchboard task","backpropagation"],"tags":["sdnn","speech recognition","convolutional neural network","dnn-hmm","swb task","dropout as preconditioner","dap","time 309 hr","model size reduction","hybrid deep neural network","time 70 hour","hybrid dnn\/hmms model","switches","speech","shrinking hidden layer","computational modeling","large vocabulary continuous speech recognition tasks","lvcsr tasks","mandarin transcription task","neural networks","training","dropout","hidden markov models","shrinking dnn structure","natural language processing","large vocabulary continuous speech recognition","psc","switchboard task","backpropagation"]},{"p_id":38975,"title":"Open Problem: The landscape of the loss surfaces of multilayer networks","abstract":"\u00a9 2015 A. Agarwal & S. Agarwal. Deep learning has enjoyed a resurgence of interest in the last few years for such applications as image and speech recognition, or natural language processing. The vast majority of practical applications of deep learning focus on supervised learning, where the supervised loss function is minimized using stochastic gradient descent. The properties of this highly non-convex loss function, such as its landscape and the behavior of critical points (maxima, minima, and saddle points), as well as the reason why large- and small-size networks achieve radically different practical performance, are however very poorly understood. It was only recently shown that new results in spin-glass theory potentially may provide an explanation for these problems by establishing a connection between the loss function of the neural networks and the Hamiltonian of the spherical spin-glass models. The connection between both models relies on a number of possibly unrealistic assumptions, yet the empirical evidence suggests that the connection may exist in real. The question we pose is whether it is possible to drop some of these assumptions to establish a stronger connection between both models.","keywords_author":["Deep learning","Hamiltonian","Multilayer networks","Nonconvex optimization","Spherical spin-glass model"],"keywords_other":["Deep learning","Spin-glass theory","Stochastic gradient descent","Non-convex loss function","Nonconvex optimization","Spherical spin glass","Multi-layer network","NAtural language processing"],"max_cite":2.0,"pub_year":2015.0,"sources":"['scp', 'wos']","rawkeys":["multi-layer network","spin-glass theory","deep learning","natural language processing","non-convex loss function","hamiltonian","nonconvex optimization","spherical spin glass","stochastic gradient descent","spherical spin-glass model","multilayer networks"],"tags":["multi-layer network","spin-glass theory","natural language processing","machine learning","non-convex loss function","hamiltonian","nonconvex optimization","spherical spin glass","stochastic gradient descent","multilayer network","spherical spin-glass model"]},{"p_id":26688,"title":"An alternative technique for populating Thai tourism ontology from texts based on machine learning","abstract":"\u00a9 2016 IEEE. This paper proposes an alternative technique to perform ontology population by using natural language processing and machine learning techniques. This study conceptually considers the population task as classifying terms into ontological subcategories. The proposed technique adopts the recognition method named Conditional Random Fields (CRFs) to identify boundary of instances and define types of subconcepts to generate relationships between instance-of and related concept. Also, the lexico-syntactic pattern is used to identify the relationships between instances. The experiments are conducted on Thai language documents in the tourism domain. The experimental results showed that the instances extraction step provided 77.62% and 70.87% of precision and recall measures, respectively, and relationships extraction step yielded 82.67% and 72.61% of recall measures.","keywords_author":["Conditional Random Fields (CRFs)","Machine Learning","Ontology Population","Tourism Ontology"],"keywords_other":["Lexico-syntactic patterns","Conditional Random Fields(CRFs)","Machine learning techniques","Precision and recall","Recognition methods","Ontology Population","NAtural language processing","Instances extractions"],"max_cite":6.0,"pub_year":2016.0,"sources":"['scp']","rawkeys":["instances extractions","lexico-syntactic patterns","recognition methods","ontology population","machine learning techniques","machine learning","natural language processing","conditional random fields (crfs)","conditional random fields(crfs)","tourism ontology","precision and recall"],"tags":["instances extractions","lexico-syntactic patterns","recognition methods","conditional random field","ontology population","machine learning techniques","machine learning","natural language processing","tourism ontology","precision and recall"]},{"p_id":53318,"title":"Large scale experiments with naive bayes and decision trees for function tagging","abstract":"This paper describes the use of two machine learning techniques, naive Bayes and decision trees, to address the task of assigning function tags to nodes in a syntactic parse tree. Function tags are extra functional information, such as logical subject or predicate, that can be added to certain nodes in syntactic parse trees. We model the function tags assignment problem as a classification problem. Each function tag is regarded as a class and the task is to find what class\/tag a given node in a parse tree belongs to from a set of predefined classes\/tags. The paper offers the first systematic comparison of the two techniques, naive Bayes and decision trees, for the task of function tags assignment. The comparison is based on a standardized data set, the Penn Treebank, a collection of sentences annotated with syntactic information including function tags. We found out that decision trees generally outperform naive Bayes for the task of function tagging. Furthermore, this is the first large scale evaluation of decision trees based solutions to the task of functional tagging. \u00a9 2008 World Scientific Publishing Company.","keywords_author":["Decision trees","Function tags","Machine learning","Naive Bayes","Natural language processing"],"keywords_other":["Functional information","classificatio n problems","Syntactic information","Systematic (CO)","treebank","Large scales","Data sets","Assignment problem (AP)","Naive Bayes (NB)","Large scale experiments","Parse trees","Machine learning techniques"],"max_cite":0.0,"pub_year":2008.0,"sources":"['scp']","rawkeys":["large scales","large scale experiments","classificatio n problems","treebank","systematic (co)","naive bayes (nb)","functional information","machine learning","machine learning techniques","natural language processing","data sets","assignment problem (ap)","function tags","syntactic information","naive bayes","parse trees","decision trees"],"tags":["large scale experiments","classificatio n problems","treebanks","functional information","machine learning","assignment problems","machine learning techniques","large-scale","natural language processing","data sets","function tags","systematics","syntactic information","naive bayes","parse trees","decision trees"]},{"p_id":28745,"title":"Conceptual models of drug-drug interactions: A summary of recent efforts","abstract":"\u00a9 2016 Conceptual modeling elicits and describes general knowledge in a particular domain and is a fundamental step in the development of knowledge-based systems. However, different conceptual models (CMs) could represent the same domain because they result from human intellectual activity with different objectives. Analyzing previous related efforts is crucial when conceptualizing a domain to avoid duplication, increase interoperability and ensure scientific conformity. Our domain of interest is drug-drug interactions (DDIs), and here we review 15 studies that have attempted total or partial representation of the DDI domain. Direct comparison of these different conceptualizations is complex because CMs are usually not provided, differ considerably from each other or are described with diverse formalisms at different abstraction levels. Therefore, to compare these CMs, we represent all of them in a common representation framework. Here, we compare the scope, content, final implementation and applications of CMs of the DDI domain. We aim to identify which aspects of DDIs have been conceptualized, characterize how this information has been modeled by different research groups, describe how each CM has been translated and illustrate the applications generated from the final models.","keywords_author":["Computational inference","Conceptual modeling","Drug-drug interactions","Knowledge representation","Natural language processing","Ontology"],"keywords_other":["Conceptual model","Intellectual activities","Drug-drug interactions","Abstraction level","Research groups","Computational inferences","General knowledge","NAtural language processing"],"max_cite":5.0,"pub_year":2016.0,"sources":"['scp']","rawkeys":["knowledge representation","conceptual model","drug-drug interactions","natural language processing","ontology","computational inference","research groups","abstraction level","intellectual activities","conceptual modeling","general knowledge","computational inferences"],"tags":["conceptual model","drug-drug interactions","natural language processing","machine learning","computational inference","research groups","abstraction level","intellectual activities","knowledge representation","general knowledge"]},{"p_id":2127,"title":"Contextual domain classification in spoken language understanding systems using recurrent neural network","abstract":"In a multi-domain, multi-turn spoken language understanding session, information from the history often greatly reduces the ambiguity of the current turn. In this paper, we apply the recurrent neural network (RNN) to exploit contextual information for query domain classification. The Jordan-type RNN directly sends the vector of output distribution to the next query turn as additional input features to the convolutional neural network (CNN). We evaluate our approach against SVM with and without contextual features. On our contextually labeled dataset, we observe a 1.4% absolute (8.3% relative) improvement in classification error rate over the non-contextual SVM, and 0.9% absolute (5.5% relative) improvement over the contextual SVM.","keywords_author":["Recurrent neural network","contextual domain classification"],"keywords_other":["Support vector machines","Predictive models","convolutional neural network","contextual domain classification","recurrent neural nets","Feature extraction","contextual SVM","CNN","vectors","spoken language understanding systems","multidomain spoken language understanding session","Context","query processing","recurrent neural network","Vectors","Jordan-type RNN","Error analysis","signal classification","multiturn spoken language understanding session","Recurrent neural networks","contextual features","natural language processing","speech processing","output distribution vector","feature extraction","query domain classification"],"max_cite":9.0,"pub_year":2014.0,"sources":"['ieee']","rawkeys":["predictive models","contextual svm","error analysis","recurrent neural networks","convolutional neural network","contextual domain classification","recurrent neural nets","vectors","spoken language understanding systems","multidomain spoken language understanding session","jordan-type rnn","query processing","recurrent neural network","cnn","signal classification","multiturn spoken language understanding session","contextual features","natural language processing","speech processing","output distribution vector","feature extraction","support vector machines","context","query domain classification"],"tags":["predictive models","contextual svm","error analysis","convolutional neural network","contextual domain classification","recurrent neural nets","contextual feature","machine learning","vectors","spoken language understanding systems","multidomain spoken language understanding session","jordan-type rnn","query processing","neural networks","signal classification","multiturn spoken language understanding session","natural language processing","speech processing","output distribution vector","feature extraction","context","query domain classification"]},{"p_id":38991,"title":"DL-WSDM'15: Workshop on Deep Learning for Web Search and Data Mining","abstract":"Copyright \u00a9 2015 ACM.In recent years, deep learning has been a very hot topic in the machine learning community. It has brought break-through results in image classification and speech recognition. Most recently, researchers have also got many promising results in natural language processing using deep learning techniques. As machine learning techniques are widely used in the Web search and data mining applications, many researchers and practitioners are studying the possibility of applying the recently-developed deep learning techniques into these appli- cations. Some of them have made very promising progress, and thus it is a good time to hold a workshop to discuss and share the problems and progress in using deep learning techniques to improve Web search and data mining tasks.","keywords_author":["Data mining","Deep learning","Information retrieval","Machine learning"],"keywords_other":["Deep learning","Web searches","Data mining tasks","Hot topics","Data mining applications","Machine learning communities","NAtural language processing","Machine learning techniques"],"max_cite":1.0,"pub_year":2015.0,"sources":"['scp']","rawkeys":["data mining","deep learning","data mining applications","machine learning techniques","machine learning","natural language processing","information retrieval","web searches","machine learning communities","data mining tasks","hot topics"],"tags":["data mining","data mining applications","machine learning techniques","machine learning","natural language processing","information retrieval","web searches","machine learning communities","data mining tasks","hot topics"]},{"p_id":51279,"title":"An investigation of neural embeddings for coreference resolution","abstract":"\u00a9 Springer International Publishing Switzerland 2015.Coreference Resolution is an important task in Natural Language Processing (NLP) and involves finding all the phrases in a document that refer to the same entity in the real world, with applications in question answering and document summarisation. Work from deep learning has led to the training of neural embeddings of words and sentences from unlabelled text. Word embeddings have been shown to capture syntactic and semantic properties of the words and have been used in POS tagging and NER tagging to achieve state of the art performance. Therefore, the key contribution of this paper is to investigate whether neural embeddings can be leveraged to overcome challenges associated with the scarcity of coreference resolution labelled datasets for benchmarking. We show, as a preliminary result, that neural embeddings improve the performance of a coreference resolver when compared to a baseline.","keywords_author":["Coreference resolution","Deep learning","Neural embeddings"],"keywords_other":["Deep learning","State-of-the-art performance","NAtural language processing","Semantic properties","Co-reference resolutions","Question Answering","Embeddings","PoS tagging"],"max_cite":0.0,"pub_year":2015.0,"sources":"['scp']","rawkeys":["embeddings","question answering","co-reference resolutions","deep learning","natural language processing","semantic properties","neural embeddings","state-of-the-art performance","pos tagging","coreference resolution"],"tags":["embeddings","natural language processing","machine learning","information retrieval","semantic properties","neural embeddings","state-of-the-art performance","pos tagging","coreference resolution"]},{"p_id":47183,"title":"Machine learning: A structuralist discipline?","abstract":"\u00a9 2017 Springer-Verlag London Ltd. Advances in machine learning and natural language processing are revolutionizing the way we live, work, and think. As for any science, they are based on assumptions about what the world is, and how humans interact with it. In this paper, I discuss what is potentially one of these assumptions: structuralism, which states that all cultures share a hidden structure. I illustrate this assumption with political footprints: a machine-learning technique using pre-trained word vectors for political discourse analysis. I introduce some of the benefits and limitations of structuralism when applied to machine learning, and the risks of exploiting a technology before establishing the validity of all its hypotheses. I consider how machine-learning techniques could evolve towards hybrid structuralism or post-structuralism, and how deeply these developments would impact cultural studies.","keywords_author":["Culture","Human mind","Knowledge","Linguistic","Machine learning","Natural language processing","Political discourse","Post-structuralism","Semantics","Structuralism"],"keywords_other":["Post-structuralism","Human mind","Structuralism","Knowledge","Political discourse"],"max_cite":0.0,"pub_year":2017.0,"sources":"['scp']","rawkeys":["knowledge","political discourse","semantics","structuralism","linguistic","machine learning","natural language processing","post-structuralism","culture","human mind"],"tags":["knowledge","political discourse","linguistics","semantics","machine learning","natural language processing","connectivity","post-structuralism","culture","human mind"]},{"p_id":67667,"title":"Prediction of enhancer-promoter interactions via natural language processing","abstract":"Background: Precise identification of three-dimensional genome organization, especially enhancer-promoter interactions (EPIs), is important to deciphering gene regulation, cell differentiation and disease mechanisms. Currently, it is a challenging task to distinguish true interactions from other nearby non-interacting ones since the power of traditional experimental methods is limited due to low resolution or low throughput.","keywords_author":["Enhancer-promoter interactions","Three-dimensinal interactions","Natural language processing","Unsupervised learning"],"keywords_other":["PRINCIPLES","CHROMOSOME CONFORMATION","NONCODING RNAS","DISEASE","CELLS","EXPRESSION","DISCOVERY","CHROMATIN","GENE","HUMAN GENOME"],"max_cite":0.0,"pub_year":2018.0,"sources":"['wos']","rawkeys":["chromosome conformation","cells","chromatin","discovery","noncoding rnas","principles","disease","enhancer-promoter interactions","human genome","natural language processing","three-dimensinal interactions","gene","expression","unsupervised learning"],"tags":["chromosome conformation","cells","chromatin","discovery","noncoding rnas","principles","disease","enhancer-promoter interactions","genes","human genome","natural language processing","three-dimensinal interactions","expression","unsupervised learning"]},{"p_id":2132,"title":"Semi-supervised Dual Recurrent Neural Network for Sentiment Analysis","abstract":"Sentiment analysis is one of the most important challenges to understand opinions online. In this research, inspired by the idea that the structural information among words, phrases and sentences is playing important role in identifying the overall statement's polarity, a novel sentiment analysis model is proposed based on recurrent neural network. The key point of the proposed approach, in order to utilise recurrent character, is to take the partial document as input and then the next parts to predict the sentiment label distribution rather than the next word. The proposed method learns words representation simultaneously the sentiment distribution. Experimental studies have been conducted on commonly used datasets and the results have shown its promising potential.","keywords_author":["Sentiment analysis","Recurrent Neural Network","Segment"],"keywords_other":["recurrent neural nets","sentiment label distribution","semisupervised dual recurrent neural network","sentiment analysis model","partial document","Recurrent neural networks","Entropy","sentiment distribution","natural language processing","word processing","Sentiment analysis","Biological neural networks","Vectors","words representation","Neurons","Computer architecture"],"max_cite":3.0,"pub_year":2013.0,"sources":"['ieee']","rawkeys":["recurrent neural nets","sentiment label distribution","semisupervised dual recurrent neural network","sentiment analysis model","partial document","sentiment distribution","natural language processing","recurrent neural network","segment","vectors","recurrent neural networks","entropy","word processing","words representation","sentiment analysis","biological neural networks","neurons","computer architecture"],"tags":["recurrent neural nets","sentiment label distribution","semisupervised dual recurrent neural network","sentiment analysis model","segmentation","partial document","neural networks","sentiment distribution","natural language processing","vectors","word processing","entropy","sentiment analysis","word representations","biological neural networks","neurons","computer architecture"]},{"p_id":14421,"title":"Robust and efficient multiclass SVM models for phrase pattern recognition","abstract":"Phrase pattern recognition (phrase chunking) refers to automatic approaches for identifying predefined phrase structures in a stream of text. Support vector machines (SVMs)-based methods had shown excellent performance in many sequential text pattern recognition tasks such as protein name finding, and noun phrase (NP)-chunking. Even though they yield very accurate results, they are not efficient for online applications, which need to handle hundreds of thousand words in a limited time. In this paper, we firstly re-examine five typical multiclass SVM methods and the adaptation to phrase chunking. However, most of them were inefficient when the number of phrase types scales. We thus introduce the proposed two new multiclass SVM models that make the system substantially faster in terms of training and testing while keeps the SVM accurate. The two methods can also be applied to similar tasks such as named entity recognition and Chinese word segmentation. Experiments on CoNLL-2000 chunking and Chinese base-chunking tasks showed that our method can achieve very competitive accuracy and at least 100 times faster than the state-of-the-art SVM-based phrase chunking method. Besides, the computational time complexity and the time cost analysis of our methods were also given in this paper. \u00a9 2008 Elsevier Ltd. All rights reserved.","keywords_author":["Machine learning","Multiclass classification","Natural language processing","Support vector machines"],"keywords_other":["Computational time complexity","Multiclass classification"],"max_cite":59.0,"pub_year":2008.0,"sources":"['scp']","rawkeys":["multiclass classification","natural language processing","machine learning","computational time complexity","support vector machines"],"tags":["multi-class classification","natural language processing","machine learning","computational time complexity"]},{"p_id":51286,"title":"A new multilingual stemmer based on the extraction of the root","abstract":"Stemming is a technique used to reduce inflected and derived words to their basic forms (stem or root). It is a very important step of pre-processing in text mining, and generally used in many areas of research such as: Natural language Processing NLP, Text Categorization TC, Text Summarizing TS, Information Retrieval IR, and other tasks in text mining. Stemming is frequently useful in text categorization to reduce the size of terms vocabulary, and in information retrieval to improve the search effectiveness and then gives us relevant results. In this paper, we propose a new multilingual stemmer based on the extraction of word root and in which we use the technique of n-grams. We validated our stemmer on three languages which are: Arabic, French and English.","keywords_author":["Bigrams technique","Information retrieval","Machine learning","Natural language processing","Root extraction","Stemming","Text mining"],"keywords_other":["Text mining","Root extraction","Bigrams","Stemming","NAtural language processing"],"max_cite":0.0,"pub_year":2015.0,"sources":"['scp']","rawkeys":["bigrams","text mining","machine learning","natural language processing","information retrieval","stemming","bigrams technique","root extraction"],"tags":["bigrams","text mining","machine learning","natural language processing","information retrieval","bigrams technique","root extraction","stem"]},{"p_id":45142,"title":"Can staff distinguish falls: Experimental hypothesis verification using Japanese incident reports and natural language processing","abstract":"\u00a9 2018 International Medical Informatics Association (IMIA) and IOS Press. Falls are generally classified into two groups in clinical settings in Japan: falls from the same level and falls from one level to another. We verified whether clinical staff could distinguish between these two types of falls by comparing 3,078 free-text incident reports about falls using a natural language processing technique and a machine learning technique. Common terms were used in reports for both types of falls, but the similarity score between the two types of reports was low, and the performance of identification based on the classification model constructed by support vector machine and deep learning was low. Although it is possible that adjustment of hyper parameters during construction of the classification model was required, we believe that clinical staff cannot distinguish between the two types of falls and do not record the distinction in incident reports.","keywords_author":["Accidental Falls","Machine Learning","Natural Language Processing"],"keywords_other":null,"max_cite":0.0,"pub_year":2018.0,"sources":"['scp']","rawkeys":["natural language processing","machine learning","accidental falls"],"tags":["natural language processing","machine learning","accidental falls"]},{"p_id":45145,"title":"User profile detection in health online fora","abstract":"\u00a9 2018 European Federation for Medical Informatics (EFMI) and IOS Press.Exchanges between diabetic patients on discussion fora permit to study their understanding of their disorder, their behavior and needs when facing health problems. When analyzing these exchanges and behavior, it is necessary to collect information on user profile. We present an approach combining lexicon and supervised classifiers for the identification of age and gender of contributors, their disorders and relation between contributor and patient. According to parameters of the method, precision is between 100% for gender and 53.48% for disorders.","keywords_author":["Demographic information","Machine learning","Natural language processing","Online discussion fora"],"keywords_other":["Internet","Humans","Patients","Social Media","Data Mining","Diabetes Mellitus"],"max_cite":0.0,"pub_year":2018.0,"sources":"['scp']","rawkeys":["patients","data mining","social media","machine learning","natural language processing","humans","diabetes mellitus","online discussion fora","internet","demographic information"],"tags":["data mining","social media","machine learning","natural language processing","humans","patient","diabetes mellitus","online discussion fora","internet","demographic information"]},{"p_id":26717,"title":"Querying database using a universal natural language interface based on machine learning","abstract":"\u00a9 2016 IEEE.Extracting information from a database system becomes a primary obligation. More and more we are forced to recognize the importance of providing easy access to information stored in a database system. However existing tools that allow users to query database using database query languages such as SQL (Structured Query Language) are difficult for non-experts users. Wherefore asking questions to databases in natural language is a very simple method that can provide powerful improvements to the use of data stored in databases. This paper presents the Architecture and the implementation of a generic natural language interface based on machine learning approach for a relational database. The advantage of this interface is that it functions independently of the database domain and automatically improves through experience its knowledge base.","keywords_author":["Extended Context Free Grammar (ECFG)","Intermediate XML Logical Query (IXLQ)","Machine Learning","Natural Language Processing (NLP)","Relational Databases"],"keywords_other":["Extended Context Free Grammar (ECFG)","Relational Database","Structured Query Language","Intermediate XML Logical Query (IXLQ)","Natural language interfaces","Database query language","NAtural language processing","Extracting information"],"max_cite":6.0,"pub_year":2016.0,"sources":"['scp']","rawkeys":["natural language interfaces","extended context free grammar (ecfg)","machine learning","natural language processing","natural language processing (nlp)","extracting information","relational database","relational databases","database query language","structured query language","intermediate xml logical query (ixlq)"],"tags":["natural language interfaces","extended context free grammar (ecfg)","machine learning","natural language processing","relational database","extracting information","database query language","structured query language","intermediate xml logical query (ixlq)"]},{"p_id":2142,"title":"Non-Negative Factor Analysis of Gaussian Mixture Model Weight Adaptation for Language and Dialect Recognition","abstract":"Recent studies show that Gaussian mixture model (GMM) weights carry less, yet complimentary, information to GMM means for language and dialect recognition. However, state-of-the-art language recognition systems usually do not use this information. In this research, a non-negative factor analysis (NFA) approach is developed for GMM weight decomposition and adaptation. This modeling, which is conceptually simple and computationally inexpensive, suggests a new low-dimensional utterance representation method using a factor analysis similar to that of the i-vector framework. The obtained subspace vectors are then applied in conjunction with i-vectors to the language\/dialect recognition problem. The suggested approach is evaluated on the NIST 2011 and RATS language recognition evaluation (LRE) corpora and on the QCRI Arabic dialect recognition evaluation (DRE) corpus. The assessment results show that the proposed adaptation method yields more accurate recognition results compared to three conventional weight adaptation approaches, namely maximum likelihood re-estimation, non-negative matrix factorization, and a subspace multinomial model. Experimental results also show that the intermediate-level fusion of i-vectors and NFA subspace vectors improves the performance of the state-of-the-art i-vector framework especially for the case of short utterances.","keywords_author":["Non-negative factor analysis","model adaptation","Gaussian mixture model weight","dialect recognition","language recognition"],"keywords_other":["nonnegative factor analysis approach","nonnegative matrix factorization","maximum likelihood estimation","maximum likelihood reestimation","NFA subspace vectors","DRE corpus","low-dimensional utterance representation method","weight adaptation approaches","subspace multinomial model","Optimization","language recognition systems","dialect recognition","Speech","LRE corpora","mixture models","NIST 2011 language recognition evaluation corpora","GMM weight decomposition","GMM weight adaptation","NFA approach","intermediate-level fusion","Gaussian processes","GMM weights","Training","Gaussian mixture model weight adaptation","Vectors","RATS language recognition evaluation corpora","Maximum likelihood estimation","i-vector framework","Support vector machine classification","QCRI Arabic dialect recognition evaluation corpus","subspace vectors","natural language processing","matrix decomposition","text analysis","Speech recognition"],"max_cite":12.0,"pub_year":2014.0,"sources":"['ieee']","rawkeys":["support vector machine classification","nfa approach","nonnegative factor analysis approach","nonnegative matrix factorization","qcri arabic dialect recognition evaluation corpus","speech recognition","nist 2011 language recognition evaluation corpora","maximum likelihood estimation","model adaptation","maximum likelihood reestimation","low-dimensional utterance representation method","subspace multinomial model","weight adaptation approaches","language recognition systems","vectors","dialect recognition","dre corpus","mixture models","optimization","speech","intermediate-level fusion","non-negative factor analysis","training","gaussian mixture model weight","nfa subspace vectors","gmm weight adaptation","lre corpora","i-vector framework","subspace vectors","natural language processing","language recognition","gaussian mixture model weight adaptation","matrix decomposition","text analysis","gaussian processes","gmm weights","gmm weight decomposition","rats language recognition evaluation corpora"],"tags":["support vector machine classification","nfa approach","nonnegative factor analysis approach","nonnegative matrix factorization","qcri arabic dialect recognition evaluation corpus","speech recognition","nist 2011 language recognition evaluation corpora","maximum likelihood estimation","model adaptation","maximum likelihood reestimation","low-dimensional utterance representation method","subspace multinomial model","weight adaptation approaches","language recognition systems","vectors","dialect recognition","dre corpus","mixture models","optimization","speech","intermediate-level fusion","non-negative factor analysis","training","gaussian mixture model weight","nfa subspace vectors","gmm weight adaptation","lre corpora","i-vector framework","subspace vectors","natural language processing","language recognition","gaussian mixture model weight adaptation","matrix decomposition","text analysis","gaussian processes","gmm weights","gmm weight decomposition","rats language recognition evaluation corpora"]},{"p_id":95,"title":"Application of deep belief networks for natural language understanding","abstract":"Applications of Deep Belief Nets (DBN) to various problems have been the subject of a number of recent studies ranging from image classification and speech recognition to audio classification. In this study we apply DBNs to a natural language understanding problem. The recent surge of activity in this area was largely spurred by the development of a greedy layer-wise pretraining method that uses an efficient learning algorithm called Contrastive Divergence (CD). CD allows DBNs to learn a multi-layer generative model from unlabeled data and the features discovered by this model are then used to initialize a feed-forward neural network which is fine-tuned with backpropagation. We compare a DBN-initialized neural network to three widely used text classification algorithms: Support Vector Machines (SVM), boosting and Maximum Entropy (MaxEnt). The plain DBN-based model gives a call-routing classification accuracy that is equal to the best of the other models. However, using additional unlabeled data for DBN pre-training and combining DBN-based learned features with the original features provides significant gains over SVMs, which, in turn, performed better than both MaxEnt and Boosting.","keywords_author":["Call-routing","DBN","Deep learning","Deep neural nets","Natural language understanding","RBM","Call-Routing","DBN","Deep Learning","Deep Neural Nets","Natural language Understanding","RBM"],"keywords_other":["maximum entropy","Support vector machines","MaxEnt","RBM","speech recognition","Deep neural nets","Speech processing","image classification","Boosting","Deep learning","deep belief network application","Natural language understanding","SVM","contrastive divergence","Speech","audio classification","feedforward neural network","feedforward neural nets","learning (artificial intelligence)","Training","Vectors","Hidden Markov models","DBN","belief networks","learning algorithm","Call-routing","natural language understanding","natural language processing","CD","support vector machines","Call routing"],"max_cite":120.0,"pub_year":2014.0,"sources":"['ieee', 'scp', 'wos']","rawkeys":["deep neural nets","maximum entropy","speech recognition","image classification","call routing","boosting","deep belief network application","contrastive divergence","rbm","dbn","vectors","svm","audio classification","feedforward neural network","cd","speech","call-routing","feedforward neural nets","maxent","learning (artificial intelligence)","deep learning","training","belief networks","learning algorithm","natural language understanding","hidden markov models","natural language processing","speech processing","support vector machines"],"tags":["deep neural nets","maximum entropy","speech recognition","image classification","boosting","deep belief network application","contrastive divergence","machine learning","vectors","audio classification","speech","call-routing","feedforward neural nets","neural networks","training","belief networks","learning algorithm","restricted boltzmann machine","natural language understanding","hidden markov models","natural language processing","speech processing","deep belief networks"]},{"p_id":32862,"title":"Automatic domain ontology learning based on web mining","abstract":"This paper proposes a web-based learning model to acquire domain ontologies and to quantify the confidence of concept relations. An ontology backbone was captured with an extensible pattern set and a distributional semantic model to find the general relations between concepts using association rules, as well as to prune and merge candidate ontologies. The confidence of concept relations was determined by the confidence of patterns, semantic distances and association features of concepts. Model parameters and thresholds were optimized with a recursive procedure of text analysis-ontology learning-text enrichment. An experiment proved effectiveness of this model. This model solves the problem of lexicon or core ontology dependencies in traditional ontology learning methods.","keywords_author":["Context signatures","Distributional semantic model","Machine learning","Natural language processing","Ontology","Topic signatures","Web mining"],"keywords_other":["Core ontologies","Web based learning models","Concept relations","Patterns confidence","Association rules","Lexicon","Domain ontologies","Web mining","Semantic distances"],"max_cite":3.0,"pub_year":2005.0,"sources":"['scp']","rawkeys":["semantic distances","web based learning models","web mining","core ontologies","lexicon","ontology","machine learning","distributional semantic model","natural language processing","domain ontologies","patterns confidence","topic signatures","context signatures","concept relations","association rules"],"tags":["web based learning models","web mining","core ontologies","lexicon","machine learning","distributional semantic model","natural language processing","domain ontologies","patterns confidence","topic signatures","context signatures","semantic distance","concept relations","association rules"]},{"p_id":36961,"title":"Location detection and disambiguation from twitter messages","abstract":"\u00a9 2017, Springer Science+Business Media New York. A remarkable amount of Twitter messages are generated every second. Detecting the location entities mentioned in these messages is useful in text mining applications. Therefore, techniques for extracting the location entities from the Twitter textual content are needed. In this work, we approach this task in a similar manner to the Named Entity Recognition (NER) task, but we focus only on locations, while NER systems detect names of persons, organizations, locations, and sometimes more (e.g., dates, times). But, unlike NER systems, we address a deeper task: classifying the detected locations into names of cities, provinces\/states, and countries in order to map them into physical locations. We approach the task in a novel way, consisting in two stages. In the first stage, we train Conditional Random Fields (CRF) models that are able to detect the locations mentioned in the messages. We train three classifiers: one for cities, one for provinces\/states, and one for countries, with various sets of features. Since a dataset annotated with this kind of information was not available, we collected and annotated our own dataset to use for training and testing. In the second stage, we resolve the remaining ambiguities, namely, cases when there exists more than one place with the same name. We proposed a set of heuristics able to choose the correct physical location in these cases. Our two-stage model will allow a social media monitoring system to visualize the places mentioned in Twitter messages on a map of the world or to compute statistics about locations. This kind of information can be of interest to business or marketing applications.","keywords_author":["Artificial intelligence","Information extraction","Machine learning","Natural language processing","Social media"],"keywords_other":["Named entity recognition","Social media","Marketing application","Training and testing","Social media monitoring","Location detection","Conditional random field","NAtural language processing"],"max_cite":1.0,"pub_year":2017.0,"sources":"['scp']","rawkeys":["artificial intelligence","information extraction","conditional random field","social media","named entity recognition","machine learning","natural language processing","training and testing","location detection","marketing application","social media monitoring"],"tags":["information extraction","conditional random field","social media","named entity recognition","machine learning","natural language processing","training and testing","marketing application","location detection","social media monitoring"]},{"p_id":12381,"title":"A Sense Embedding of Deep Convolutional Neural Networks for Sentiment Classification","abstract":"Sentiment classification task has attracted considerable interest as sentiment information is crucial for many natural language processing (NLP) applications.","keywords_author":["Natural Language Processing","Text Classification","Sense Sensitive","Word Embedding"],"keywords_other":null,"max_cite":0.0,"pub_year":2016.0,"sources":"['wos']","rawkeys":["word embedding","sense sensitive","natural language processing","text classification"],"tags":["word embedding","sense sensitive","natural language processing","text classification"]},{"p_id":51299,"title":"Vira@FIRE 2015: Entity extraction from social media text Indian languages (ESM-IL)","abstract":"In this paper we have tried to identify and extract \"Named Entities\" from social media text using conditional random field-(CRF) [3]. The paper represents our working methodology and result on Entity Extraction from Social Media Text Indian Languages task of FIRE-2015. We have extracted named entities from two languages Hindi and English. Named Entity Extraction system is implemented based on CRFSuite. CRFSuite [8] is the populer implementation of Conditional Random Fields (CRF). This is a sequential labelling task to achieve the desired tagging output. Conditional random fields (CRF) are a class of statistical modelling method often applied in pattern recognition, machine learning and many natural language processing tasks. We get F1-score of 19.82 and 3.72 for the Hindi and English text respectively.","keywords_author":["Machine learning","Named entity extraction","Named entity recognition"],"keywords_other":["Named entity extraction","Named entities","Named entity recognition","Statistical modelling","Indian languages","Entity extractions","Conditional random field","NAtural language processing"],"max_cite":0.0,"pub_year":2015.0,"sources":"['scp']","rawkeys":["statistical modelling","conditional random field","named entity extraction","named entity recognition","machine learning","natural language processing","indian languages","entity extractions","named entities"],"tags":["conditional random field","named entity extraction","named entity recognition","machine learning","natural language processing","statistical models","indian languages","entity extractions","named entities"]},{"p_id":47199,"title":"Approaches to Automated Detection of Cyberbullying: A Survey","abstract":"IEEE Research into cyberbullying detection has increased in recent years, due in part to the proliferation of cyberbullying across social media and its detrimental effect on young people. A growing body of work is emerging on automated approaches to cyberbullying detection. These approaches utilise machine learning and natural language processing techniques to identify the characteristics of a cyberbullying exchange and automatically detect cyberbullying by matching textual data to the identified traits. In this paper, we present a systematic review of published research (as identified via Scopus, ACM and IEEE Xplore bibliographic databases) on cyberbullying detection approaches. On the basis of our extensive literature review, we categorise existing approaches into 4 main classes, namely; supervised learning, lexicon based, rule based and mixed-initiative approaches. Supervised learning-based approaches typically use classifiers such as SVM and Na&#x00EF;ve Bayes to develop predictive models for cyberbullying detection. Lexicon based systems utilise word lists and use the presence of words within the lists to detect cyberbullying. Rules-based approaches match text to predefined rules to identify bullying and mixed-initiatives approaches combine human-based reasoning with one or more of the aforementioned approaches. We found lack of quality representative labelled datasets and non-holistic consideration of cyberbullying by researchers when developing detection systems are two key challenges facing cyberbullying detection research. This paper essentially maps out the state-of-the-art in cyberbullying detection research and serves as a resource for researchers to determine where to best direct their future research efforts in this field.","keywords_author":["Abuse and crime involving computers","Computers","data mining","Electronic mail","machine learning","natural language processing","Sentiment analysis","sentiment analysis","Social network services","social networking","Supervised learning"],"keywords_other":["Learning-based approach","Automated detection","Sentiment analysis","Social network services","Literature reviews","Bibliographic database","Abuse and crime involving computers","Detection approach"],"max_cite":0.0,"pub_year":2017.0,"sources":"['scp']","rawkeys":["supervised learning","data mining","detection approach","automated detection","electronic mail","abuse and crime involving computers","machine learning","literature reviews","natural language processing","social network services","bibliographic database","computers","learning-based approach","sentiment analysis","social networking"],"tags":["supervised learning","data mining","detection approach","automated detection","neural networks","abuse and crime involving computers","electronic mail","machine learning","literature reviews","natural language processing","social network services","social networks","bibliographic database","learning-based approach","sentiment analysis"]},{"p_id":18533,"title":"Semi-supervised clinical text classification with Laplacian SVMs: An application to cancer case management","abstract":"Objective: To compare linear and Laplacian SVMs on a clinical text classification task; to evaluate the effect of unlabeled training data on Laplacian SVM performance. Background: The development of machine-learning based clinical text classifiers requires the creation of labeled training data, obtained via manual review by clinicians. Due to the effort and expense involved in labeling data, training data sets in the clinical domain are of limited size. In contrast, electronic medical record (EMR) systems contain hundreds of thousands of unlabeled notes that are not used by supervised machine learning approaches. Semi-supervised learning algorithms use both labeled and unlabeled data to train classifiers, and can outperform their supervised counterparts. Methods: We trained support vector machines (SVMs) and Laplacian SVMs on a training reference standard of 820 abdominal CT, MRI, and ultrasound reports labeled for the presence of potentially malignant liver lesions that require follow up (positive class prevalence 77%). The Laplacian SVM used 19,845 randomly sampled unlabeled notes in addition to the training reference standard. We evaluated SVMs and Laplacian SVMs on a test set of 520 labeled reports. Results: The Laplacian SVM trained on labeled and unlabeled radiology reports significantly outperformed supervised SVMs (Macro-F1 0.773 vs. 0.741, Sensitivity 0.943 vs. 0.911, Positive Predictive value 0.877 vs. 0.883). Performance improved with the number of labeled and unlabeled notes used to train the Laplacian SVM (pearson's \u03c1= 0.529 for correlation between number of unlabeled notes and macro-F1 score). These results suggest that practical semi-supervised methods such as the Laplacian SVM can leverage the large, unlabeled corpora that reside within EMRs to improve clinical text classification. \u00a9 2013 Elsevier Inc.","keywords_author":["Graph Laplacian","Natural language processing","Semi-supervised learning","Support vector machine"],"keywords_other":["Clinical text classifications","Labeled and unlabeled data","Graph Laplacian","Support vector machine (SVMs)","Semi-supervised learning","Positive predictive values","NAtural language processing","Supervised machine learning"],"max_cite":24.0,"pub_year":2013.0,"sources":"['scp', 'wos']","rawkeys":["clinical text classifications","labeled and unlabeled data","natural language processing","semi-supervised learning","graph laplacian","support vector machine","supervised machine learning","support vector machine (svms)","positive predictive values"],"tags":["labeled and unlabeled data","natural language processing","machine learning","semi-supervised learning","clinical text classification","graph laplacian","supervised machine learning","positive predictive values"]},{"p_id":39018,"title":"Automatic extraction of reference gene from literature in plants based on texting mining","abstract":"Copyright \u00a9 2015 Inderscience Enterprises Ltd. Real-Time Quantitative Polymerase Chain Reaction (qRT-PCR) is widely used in biological research. It is a key to the availability of qRT-PCR experiment to select a stable reference gene. However, selecting an appropriate reference gene usually requires strict biological experiment for verification with high cost in the process of selection. Scientific literatures have accumulated a lot of achievements on the selection of reference gene. Therefore, mining reference genes under specific experiment environments from literatures can provide quite reliable reference genes for similar qRT-PCR experiments with the advantages of reliability, economic and efficiency. An auxiliary reference gene discovery method from literature is proposed in this paper which integrated machine learning, natural language processing and text mining approaches. The validity tests showed that this new method has a better precision and recall on the extraction of reference genes and their environments.","keywords_author":["Bioinformatics","Biological knowledge discovery","Machine learning","NLP","Real-time quantitative polymerase chain reaction","Reference gene","Text mining"],"keywords_other":["Genes, Plant","Natural Language Processing","Periodicals as Topic","Plants","Artificial Intelligence","Data Mining"],"max_cite":1.0,"pub_year":2015.0,"sources":"['scp']","rawkeys":["artificial intelligence","data mining","nlp","periodicals as topic","plant","text mining","reference gene","plants","machine learning","genes","natural language processing","bioinformatics","real-time quantitative polymerase chain reaction","biological knowledge discovery"],"tags":["data mining","periodicals as topic","text mining","reference gene","machine learning","genes","natural language processing","bioinformatics","real-time quantitative polymerase chain reaction","biological","biological knowledge discovery"]},{"p_id":39024,"title":"Comparing recursive autoencoder and convolutional network for phrase-level sentiment polarity classification","abstract":"\u00a9 Springer International Publishing Switzerland 2015.We present a comparative evaluation of two neural network architectures, which can be used to compute representations of phrases or sentences. The Semi-Supervised Recursive Autoencoder (SRAE) and the Convolutional Neural Network (CNN) are both methods that directly operate on sequences of words represented via word embeddings and jointly model the syntactic and semantic peculiarities of phrases. We compare both models with respect to their classification accuracy on the task of binary sentiment polarity classification. Our evaluation shows that a single-layer CNN produces equally accurate phrase representations and that both methods profit from the initialization with word embeddings trained by a language model. We observe that the initialization with domain specific word embeddings has no significant effect on the accuracy of both phrase models. A pruning experiment revealed that up to 95% of the parameters used to train the CNN could be removed afterwards without affecting the model\u2019s accuracy.","keywords_author":["Artificial neural networks","Convolutional neural network","Deep learning","Natural language processing","Recursive autoencoder"],"keywords_other":["Deep learning","Convolutional networks","NAtural language processing","Polarity classification","Comparative evaluations","Convolutional neural network","Auto encoders","Classification accuracy"],"max_cite":1.0,"pub_year":2015.0,"sources":"['scp']","rawkeys":["deep learning","classification accuracy","auto encoders","convolutional networks","natural language processing","recursive autoencoder","artificial neural networks","convolutional neural network","polarity classification","comparative evaluations"],"tags":["neural networks","classification accuracy","auto encoders","machine learning","natural language processing","recursive auto encoders (raes)","convolutional neural network","polarity classification","comparative evaluations"]},{"p_id":39025,"title":"Detection of fraudulent financial reports with machine learning techniques","abstract":"\u00a9 2015 IEEE. This paper describes our efforts to apply various advanced supervised machine learning and natural language processing techniques, including Binomial Logistic Regression, Support Vector Machines, Neural Networks, Ensemble Techniques, and Latent Dirichlet Allocation (LDA), to the problem of detecting fraud in financial reporting documents available from the United States' Security and Exchange Commission EDGAR database. Specifically, we apply LDA to a collection of type 10-K financial reports and to generate document-topic frequency matrix, and then submit these data to a series of advanced classification algorithms. We then apply evaluation metrics, such as Precision, Receiver Operating Characteristic Curve, and Area Under the Curve to evaluate the performance of each algorithm. We conclude that these methods show promise and suggest applying the approach to a larger set of input documents.","keywords_author":["Ensemble","Financial Fraud Detection","Latent Dirichlet Allocation","Machine Learning","Natural Language Processing","Support Vector Machines"],"keywords_other":["Ensemble","Binomial logistic regressions","Financial fraud detections","Receiver operating characteristic curves","Security and exchange commissions","Latent dirichlet allocations","NAtural language processing","Latent Dirichlet allocation"],"max_cite":1.0,"pub_year":2015.0,"sources":"['scp']","rawkeys":["latent dirichlet allocation","financial fraud detections","receiver operating characteristic curves","security and exchange commissions","machine learning","ensemble","natural language processing","latent dirichlet allocations","binomial logistic regressions","support vector machines","financial fraud detection"],"tags":["receiver operating characteristics","linear discriminant analysis","financial fraud detections","security and exchange commissions","machine learning","ensemble","natural language processing","binomial logistic regressions"]},{"p_id":51318,"title":"A method for building a labeled named entity recognition corpus using ontologies","abstract":"\u00a9 Springer International Publishing Switzerland 2015. Building a labeled corpus which contains sufficient data and good coverage along with solving the problems of cost, effort and time is a popular research topic in natural language processing. The problem of constructing automatic or semi-automatic training data has become a matter of the research community. For this reason, we consider the problem of building a corpus in phenotype entity recognition problem, classspecific feature detectors from unlabeled data based on over 10260 unique terms (more than 15000 synonyms) describing human phenotypic features in the Human Phenotype Ontology (HPO) and about 9000 unique terms (about 24000 synonyms) of mouse abnormal phenotype descriptions in the Mammalian Phenotype Ontology. This corpus evaluated on three corpora: Khordad corpus, Phenominer 2012 and Phenominer 2013 corpora with Maximum Entropy and Beam Search method. The performance is good for three corpora, with F-scores of 31.71% and 35.77% for Phenominer 2012 corpus and Phenominer 2013 corpus; 78.36% for Khordad corpus.","keywords_author":["Biomedical ontology","Machine learning","Named entity recognition","Phenotype"],"keywords_other":["Biomedical ontologies","Phenotype","Named entity recognition","Phenotypic features","Semi-automatic trainings","Research communities","NAtural language processing","Entity recognition"],"max_cite":0.0,"pub_year":2015.0,"sources":"['scp']","rawkeys":["entity recognition","biomedical ontologies","named entity recognition","machine learning","natural language processing","biomedical ontology","phenotypic features","phenotype","research communities","semi-automatic trainings"],"tags":["entity recognition","biomedical ontologies","named entity recognition","natural language processing","machine learning","phenotypic features","phenotype","research communities","semi-automatic trainings"]},{"p_id":24695,"title":"Natural Language Processing for Social Media","abstract":"\u00a9 2011 by Morgan & Claypool. In recent years, online social networking has revolutionized interpersonal communication. The newer research on language analysis in social media has been increasingly focusing on the latter's impact on our daily lives, both on a personal and a professional level. Natural language processing (NLP) is one of the most promising avenues for social media data processing. It is a scientific challenge to develop powerful methods and algorithms which extract relevant information from a large volume of data coming from multiple sources and languages in various formats or in free form. We discuss the challenges in analyzing social media texts in contrast with traditional documents. Research methods in information extraction, automatic categorization and clustering, automatic summarization and indexing, and statistical machine translation need to be adapted to a new kind of data. This book reviews the current research on Natural Language Processing (NLP) tools and methods for processing the non-traditional information from social media data that is available in large amounts (big data), and shows how innovative NLP approaches can integrate appropriate linguistic information in various fields such as social media monitoring, health care, business intelligence, industry, marketing, and security and defense. We review the existing evaluation metrics for NLP and social media applications, and the new efforts in evaluation campaigns or shared tasks on new datasets collected from social media. Such tasks are organized by the Association for Computational Linguistics (such as SemEval tasks) or by the National Institute of Standards and Technology via the Text REtrieval Conference (TREC) and the Text Analysis Conference (TAC). In the concluding chapter, we discuss the importance of this dynamic discipline and its great potential for NLP in the coming decade, in the context of changes in mobile technology, cloud computing, and social networking.","keywords_author":["big data","natural language processing","NLP","semantic analysis","social computing","social media","social networking"],"keywords_other":["Social computing","Statistical machine translation","Social media","Semantic analysis","Online social networkings","National Institute of Standards and Technology","Inter-personal communications","Text retrieval conferences"],"max_cite":9.0,"pub_year":2015.0,"sources":"['scp']","rawkeys":["online social networkings","nlp","big data","national institute of standards and technology","social media","natural language processing","semantic analysis","social computing","inter-personal communications","text retrieval conferences","social networking","statistical machine translation"],"tags":["online social networks","big data","national institute of standards and technology","social media","natural language processing","semantic analysis","social networks","social computing","inter-personal communications","text retrieval conferences","statistical machine translation"]},{"p_id":24698,"title":"TRUPI: Twitter recommendation based on users\u2019 personal interests","abstract":"\u00a9 Springer International Publishing Switzerland 2015. Twitter has emerged as one of the most powerful microbloggingservices for real-time sharing of information on the web. Thelarge volume of posts in several topics is overwhelming to twitter userswho might be interested in only few topics. To this end, we proposeTRUPI, a personalized recommendation system for the timelines of twitterusers where tweets are ranked by the user\u2019s personal interests. Theproposed system combines the user social features and interactions aswell as the history of her tweets content to attain her interests. Thesystem captures the users interests dynamically by modeling them asa time variant in different topics to accommodate the change of theseinterests over time. More specifically, we combine a set of machine learningand natural language processing techniques to analyze the topics ofthe various tweets posted on the user\u2019s timeline and rank them basedon her dynamically detected interests. Our extensive performance evaluationon a publicly available dataset demonstrates the effectiveness ofTRUPI and shows that it outperforms the competitive state of the artby 25% on nDCG@25, and 14% on MAP.","keywords_author":["Dynamic interests","Personalized recommendation","Twitter"],"keywords_other":["Twitter","NAtural language processing","Personalized recommendation","Personalized recommendation systems","Real time","Time variant"],"max_cite":9.0,"pub_year":2015.0,"sources":"['scp']","rawkeys":["personalized recommendation","real time","natural language processing","dynamic interests","twitter","time variant","personalized recommendation systems"],"tags":["personalized recommendation","real time","natural language processing","dynamic interests","twitter","time variant","personalized recommendation systems"]},{"p_id":20603,"title":"Methods to develop an electronic medical record phenotype algorithm to compare the risk of coronary artery disease across 3 chronic disease cohorts","abstract":"\u00a9 2015 Liao et al. Background Typically, algorithms to classify phenotypes using electronic medical record (EMR) data were developed to perform well in a specific patient population. There is increasing interest in analyses which can allow study of a specific outcome across different diseases. Such a study in the EMR would require an algorithm that can be applied across different patient populations. Our objectives were: (1) to develop an algorithm that would enable the study of coronary artery disease (CAD) across diverse patient populations; (2) to study the impact of adding narrative data extracted using natural language processing (NLP) in the algorithm. Additionally, we demonstrate how to implement CAD algorithm to compare risk across 3 chronic diseases in a preliminary study. Methods and Results We studied 3 established EMR based patient cohorts: diabetes mellitus (DM, n = 65,099), inflammatory bowel disease (IBD, n = 10,974), and rheumatoid arthritis (RA, n = 4,453) from two large academic centers. We developed a CAD algorithm using NLP in addition to structured data (e.g. ICD9 codes) in the RA cohort and validated it in the DM and IBD cohorts. The CAD algorithm using NLP in addition to structured data achieved specificity >95% with a positive predictive value (PPV) 90% in the training (RA) and validation sets (IBD and DM). The addition of NLP data improved the sensitivity for all cohorts, classifying an additional 17% of CAD subjects in IBD and 10% in DM while maintaining PPV of 90%. The algorithm classified 16,488 DM (26.1%), 457 IBD (4.2%), and 245 RA (5.0%) with CAD. In a crosssectional analysis, CAD risk was 63% lower in RA and 68% lower in IBD compared to DM (p<0.0001) after adjusting for traditional cardiovascular risk factors. Conclusions We developed and validated a CAD algorithm that performed well across diverse patient populations. The addition of NLP into the CAD algorithm improved the sensitivity of the algorithm, particularly in cohorts where the prevalence of CAD was low. Preliminary data suggest that CAD risk was significantly lower in RA and IBD compared to DM.","keywords_author":null,"keywords_other":["Male","Algorithms","Electronic Health Records","Phenotype","Hyperlipidemias","Humans","Aged","Middle Aged","Adult","Natural Language Processing","Risk Factors","Arthritis, Rheumatoid","Diabetes Mellitus","Inflammatory Bowel Diseases","Coronary Artery Disease","Female"],"max_cite":17.0,"pub_year":2015.0,"sources":"['scp']","rawkeys":["rheumatoid","risk factors","male","female","middle aged","electronic health records","humans","natural language processing","aged","diabetes mellitus","arthritis","coronary artery disease","hyperlipidemias","algorithms","inflammatory bowel diseases","phenotype","adult"],"tags":["rheumatoid","risk factors","male","computer-aided diagnosis","female","inflammatory bowel disease","middle aged","electronic health records","humans","natural language processing","aged","diabetes mellitus","arthritis","algorithms","hyperlipidemia","phenotype","adult"]},{"p_id":34942,"title":"A controlled greedy supervised approach for co-reference resolution on clinical text","abstract":"Identification of co-referent entity mentions inside text has significant importance for other natural language processing (NLP) tasks (e.g. event linking). However, this task, known as co-reference resolution, remains a complex problem, partly because of the confusion over different evaluation metrics and partly because the well-researched existing methodologies do not perform well on new domains such as clinical records. This paper presents a variant of the influential mention-pair model for co-reference resolution. Using a series of linguistically and semantically motivated constraints, the proposed approach controls generation of less-informative\/sub-optimal training and test instances. Additionally, the approach also introduces some aggressive greedy strategies in chain clustering. The proposed approach has been tested on the official test corpus of the recently held i2b2\/VA 2011 challenge. It achieves an unweighted average F1 score of 0.895, calculated from multiple evaluation metrics (MUC, B3 and CEAF scores). These results are comparable to the best systems of the challenge. What makes our proposed system distinct is that it also achieves high average F1 scores for each individual chain type (Test: 0.897, Person: 0.852, Problem: 0.855, Treatment: 0.884). Unlike other works, it obtains good scores for each of the individual metrics rather than being biased towards a particular metric. \u00a9 2013 Elsevier Inc.","keywords_author":["Clinical text","Co-reference resolution","Knowledge engineering","Machine learning","Natural language processing"],"keywords_other":["Greedy strategies","Complex problems","Co-reference resolutions","Clinical text","Evaluation metrics","Approach controls","Clinical records","NAtural language processing"],"max_cite":2.0,"pub_year":2013.0,"sources":"['scp']","rawkeys":["greedy strategies","co-reference resolutions","machine learning","natural language processing","evaluation metrics","clinical records","knowledge engineering","clinical text","co-reference resolution","approach controls","complex problems"],"tags":["greedy strategies","machine learning","natural language processing","evaluation metrics","clinical records","knowledge engineering","clinical text","approach controls","complex problems","coreference resolution"]},{"p_id":49279,"title":"Modern statistical and linguistic approaches to processing texts in natural languages","abstract":"\u00a9 2005 - 2016 JATIT & LLS. All rights reserved.Natural language processing (NLP) is a research area that focuses on studying the methods of computer analysis and synthesis of natural languages. The sources of information can include not only texts, but also audio and video data. In this article, we will focus on text mining. The analysis is divided into the following subtasks: information extraction, tonality analysis, question-answer systems, etc. In turn, information extraction also includes subtasks: named entity recognition (NER), relation extraction, extraction of keywords and word combinations (collocations). The methods of NLP are divided into linguistic (based on rules and grammars) and probabilistic; there are also hybrid methods that combine both approaches. The aim of this paper is to provide an overview of modern approaches to text processing using the example of the tasks of named entities recognition and identifying the relationships between them.","keywords_author":["Information extraction","Linguistic method","Machine learning","Named entity recognition","NER","NLP","Relation extraction","Semi-supervised learning","Statistical method","Supervised learning","Text mining"],"keywords_other":null,"max_cite":0.0,"pub_year":2016.0,"sources":"['scp']","rawkeys":["statistical method","ner","supervised learning","text mining","nlp","information extraction","named entity recognition","machine learning","semi-supervised learning","linguistic method","relation extraction"],"tags":["supervised learning","text mining","linguistic methods","information extraction","named entity recognition","natural language processing","machine learning","semi-supervised learning","relation extraction","statistical methods"]},{"p_id":2177,"title":"Optimizing deep bottleneck feature extraction","abstract":"We investigate several optimizations to a recently published architecture for extracting bottleneck features for large-vocabulary speech recognition with deep neural networks. We are able to improve recognition performance of first-pass systems from a 12% relative word error rate reduction reported previously to 21%, compared to MFCC baselines on a Tagalog conversational telephone speech corpus. This is achieved by using different input features, training the network to predict context-dependent targets, employing an efficient learning rate schedule and varying several architectural details. Evaluations on two larger German and French speech transcription tasks show that the optimizations proposed are universally applicable and yield comparable gains on other corpora (19.9% and 22.8%, respectively).","keywords_author":null,"keywords_other":["input features","speech recognition","German speech transcription tasks","Neural networks","Feature extraction","French speech transcription tasks","Optimization","first-pass systems","Speech","learning rate schedule","recognition performance improvement","Mel frequency cepstral coefficient","learning (artificial intelligence)","Training","relative word error rate reduction","context-dependent target prediction","network training","large-vocabulary speech recognition","Hidden Markov models","deep bottleneck feature extraction optimization","neural nets","optimisation","Tagalog conversational telephone speech corpus","deep neural networks","natural language processing","feature extraction"],"max_cite":4.0,"pub_year":2013.0,"sources":"['ieee']","rawkeys":["input features","speech recognition","first-pass systems","learning rate schedule","optimization","recognition performance improvement","speech","learning (artificial intelligence)","neural networks","training","relative word error rate reduction","context-dependent target prediction","network training","large-vocabulary speech recognition","deep bottleneck feature extraction optimization","neural nets","french speech transcription tasks","tagalog conversational telephone speech corpus","hidden markov models","optimisation","deep neural networks","natural language processing","german speech transcription tasks","mel frequency cepstral coefficient","feature extraction"],"tags":["input features","speech recognition","convolutional neural network","first-pass systems","machine learning","optimization","recognition performance improvement","speech","neural networks","learning rate scheduling","relative word error rate reduction","training","context-dependent target prediction","network training","mel-frequency cepstral coefficients","deep bottleneck feature extraction optimization","french speech transcription tasks","tagalog conversational telephone speech corpus","hidden markov models","optimisation","large vocabulary speech recognition","natural language processing","german speech transcription tasks","feature extraction"]},{"p_id":39041,"title":"A convolutional deep neural network for coreference resolution via modeling hierarchical features","abstract":"\u00a9 Springer International Publishing Switzerland 2015.Coreference resolution is a major task of natural language processing (NLP) identifying which noun phrases (or mentions) refer to the same real-world entity or concept. The state-of-the-art methods applied to coreference resolution are mainly based on statistical machine learning, and their performance strongly depends on the quality of the extracted features. The extracted features are usually shallow features by artificial selection, which leads to the loss of unknown useful deep semantic information and becomes an obstacle for improving system performance. We explored a convolutional deep neural network (CDNN) to extract discourse level features automatically. Our method utilized all of the word tokens as input without complicated pre-processing. To begin with, the word tokens were transformed to vectors by looking up word embeddings. Secondly, mention-pair level features were extracted according to the given mentions. In the meanwhile, distance features were computed easily. Moreover, discourse level features were learned using a convolutional approach. Finally, these features were fed into a softmax classifier to predict the equivalence between two marked mentions. The experimental results demonstrate that our approach obtains a competitive score of average F1 over MUC, B3, and CEAF, which places it above the mean score of other systems on the dataset of CoNLL-2012 Shared Task.","keywords_author":["Convolutional deep neural network","Coreference resolution","Deep learning","Mention-pairs"],"keywords_other":["Deep learning","NAtural language processing","State-of-the-art methods","Co-reference resolutions","Statistical machine learning","Deep neural networks","Mention-pairs","Hierarchical features"],"max_cite":1.0,"pub_year":2015.0,"sources":"['scp']","rawkeys":["convolutional deep neural network","state-of-the-art methods","co-reference resolutions","deep learning","deep neural networks","statistical machine learning","hierarchical features","natural language processing","mention-pairs","coreference resolution"],"tags":["convolutional deep neural network","state-of-the-art methods","statistical machine learning","machine learning","hierarchical features","natural language processing","mention-pairs","convolutional neural network","coreference resolution"]},{"p_id":26756,"title":"AMRITA-CEN@FIRE 2016: Code-mix entity extraction for Hindi-English and Tamil-English tweets","abstract":"Social media text holds information regarding various important aspects. Extraction of such information serves as the basis for the most preliminary task in Natural Language Processing called Entity extraction. The work is submitted as a part of Shared task on Code Mix Entity Extraction for Indian Languages(CMEE-IL) at Forum for Information Retrieval Evaluation (FIRE) 2016. Three different methodology is proposed in this paper for the task of entity extraction for code-mix data. Proposed systems include approaches based on the Embedding models and feature based model. Creation of trigram embedding and BIO tag formatting were done during feature extraction. Evaluation of the system is carried out using machine learning based classifier, SVM-Light. Overall accuracy through cross validation has proven that the proposed system is efficient in classifying unknown tokens too.","keywords_author":["Code-Mix","Entity extraction","Machine learning","Support vector machine (SVM)","Word embedding"],"keywords_other":["Cross validation","Word embedding","Social media","Overall accuracies","Entity extractions","Feature based modeling","Indian languages","NAtural language processing"],"max_cite":6.0,"pub_year":2016.0,"sources":"['scp']","rawkeys":["cross validation","overall accuracies","social media","machine learning","natural language processing","support vector machine (svm)","indian languages","code-mix","word embedding","entity extractions","feature based modeling","entity extraction"],"tags":["social media","machine learning","natural language processing","indian languages","word embedding","entity extractions","code-mixing","feature based modeling","optimization algorithms","computer vision"]},{"p_id":24709,"title":"Measuring the credibility of Arabic text content in twitter","abstract":"Recent research in measuring web content credibility automatically for text and multimedia have addressed many languages. Unfortunately, there is no research for credibility measurements of Arabic web content. Thus, this paper proposes a solution to automatically measure the credibility of Arabic web content. The automatic tool we present in this paper measures Arabic content credibility published in Twitter (a micro blogging service) targeting the news domain. Our tool is based on two approaches. The first approach uses the similarity between Twitter posts and authentic news sources, while the second approach is based on a set of proposed features, among them is the similarity with verified content. Preliminary evaluations of our tool have shown that the first approach for rating tweets credibility has higher precision and recall compared with the second approach. \u00a92010 IEEE.","keywords_author":["Arabic language","Credibility","Natural language processing","Twitter","Web content"],"keywords_other":["Twitter","Credibility","Web content","Arabic languages","Natural language processing"],"max_cite":9.0,"pub_year":2010.0,"sources":"['scp']","rawkeys":["web content","arabic language","natural language processing","credibility","arabic languages","twitter"],"tags":["web content","natural language processing","credibility","arabic languages","twitter"]},{"p_id":16518,"title":"Show and Tell: Lessons Learned from the 2015 MSCOCO Image Captioning Challenge","abstract":"\u00a9 2016 IEEE. Automatically describing the content of an image is a fundamental problem in artificial intelligence that connects computer vision and natural language processing. In this paper, we present a generative model based on a deep recurrent architecture that combines recent advances in computer vision and machine translation and that can be used to generate natural sentences describing an image. The model is trained to maximize the likelihood of the target description sentence given the training image. Experiments on several datasets show the accuracy of the model and the fluency of the language it learns solely from image descriptions. Our model is often quite accurate, which we verify both qualitatively and quantitatively. Finally, given the recent surge of interest in this task, a competition was organized in 2015 using the newly released COCO dataset. We describe and analyze the various improvements we applied to our own baseline and show the resulting performance in the competition, which we won ex-aequo with a team from Microsoft Research.","keywords_author":["Image captioning","language model","recurrent neural network","sequence-to-sequence"],"keywords_other":["Language model","Image captioning","Microsoft researches","Image descriptions","sequence-to-sequence","Target descriptions","Machine translations","NAtural language processing"],"max_cite":36.0,"pub_year":2017.0,"sources":"['scp', 'ieee']","rawkeys":["target descriptions","image captioning","natural language processing","recurrent neural network","language model","image descriptions","machine translations","sequence-to-sequence","microsoft researches"],"tags":["target descriptions","image captioning","neural networks","natural language processing","language model","image descriptions","machine translations","sequence-to-sequence","microsoft researches"]},{"p_id":26757,"title":"ECSTRA-INSERM @ CLEF eHealth2016-Task 2: ICD10 Code Extraction from Death Certificates","abstract":"This paper describes the participation of ECSTRA-INSERM team at CLEF eHealth 2016, task 2.C. The task involves extracting ICD10 codes from death certificates, mainly described with short plain texts. We cast the task as a machine learning problem involving the prediction of the ICD10 codes (categorical variable) from the raw text transformed into a bag-of-words matrix. We rely on probabilistic topic models that we evaluate against classical classifiers such as SVM and Naive Bayes. We demonstrate the effectiveness of topic models for this task in terms of prediction accuracy and result interpretation.","keywords_author":["Cause of death extraction","ICD10 code assignment","Machine learning","Natural language processing","Text mining","Topic models"],"keywords_other":["Text mining","Code assignments","Machine learning problem","Topic model","Categorical variables","Prediction accuracy","Probabilistic topic models","NAtural language processing"],"max_cite":6.0,"pub_year":2016.0,"sources":"['scp']","rawkeys":["code assignments","text mining","cause of death extraction","machine learning","natural language processing","machine learning problem","topic model","categorical variables","prediction accuracy","icd10 code assignment","topic models","probabilistic topic models"],"tags":["code assignments","text mining","cause of death extraction","topic modeling","machine learning","natural language processing","machine learning problem","categorical variables","prediction accuracy","icd10 code assignment","probabilistic topic models"]},{"p_id":39051,"title":"Effect research of aspects extraction for Chinese hotel reviews based on machine learning method","abstract":"\u00a9 2015 SERSC. With the development of Web, there are more and more online reviews. These reviews have become the important information for people. Therefore, automatic analysis of online reviews has become the active research area in natural language processing and management sciences. Apart from the researches about document sentiment classification and sentence sentiment classification, there are an increasing number researches about aspect-based sentiment analysis. This paper tries to use machine learning to extract aspect from Chinese hotel reviews. Through a lot of experiments, we find that machine learning methods are suitable for the aspect extraction of Chinese reviews. This paper adopts different dimensions of features, feature representation methods and classifiers to analyze the integral effect of aspect extraction. The experiment result shows that ME is the best classifiers and presence is most suitable feature representation method for aspect extraction.","keywords_author":["Aspect extraction","Chinese hotel reviews","Machine learning"],"keywords_other":["Online reviews","Automatic analysis","Document sentiment classification","Sentiment analysis","Feature representation","Machine learning methods","Sentiment classification","NAtural language processing"],"max_cite":1.0,"pub_year":2015.0,"sources":"['scp']","rawkeys":["machine learning methods","aspect extraction","sentiment classification","automatic analysis","online reviews","machine learning","natural language processing","document sentiment classification","feature representation","sentiment analysis","chinese hotel reviews"],"tags":["machine learning methods","aspect extraction","sentiment classification","automatic analysis","online reviews","machine learning","natural language processing","document sentiment classification","feature representation","sentiment analysis","chinese hotel reviews"]},{"p_id":53387,"title":"Category based feature weighting for automatic text categorization","abstract":"Vector space model representation of documents is an important aspect in text classification. Feature selection methods in turn use these representations to select important features for a category. Although many feature weighting schemes are available, none truly exploited the category-related information to capture the importance of the term with respect to its presence or absence in the category level rather than only in the document level. In this paper, we propose two new methods of capturing category-related information, which were combined with four feature weighting methods to produce enhanced performance. The effectiveness of these enhancements is demonstrated by using these schemes to calculate the term weight of the documents. We have compared their performances with their original versions when applied in text classification. Encouraged by these results, we introduce a new feature weighting scheme, which takes care of the category-related information and performs impressively in text classification. Copyright \u00a9 2007 IICAI.","keywords_author":["Document categorization","Feature selection","Machine learning","Natural language processing","Text mining"],"keywords_other":["Text mining","Automatic text categorization","Feature weighting","Feature selection methods","Text classification","Document categorization","Term weight","Vector space models","NAtural language processing"],"max_cite":0.0,"pub_year":2007.0,"sources":"['scp']","rawkeys":["text mining","automatic text categorization","feature selection methods","term weight","machine learning","natural language processing","feature selection","vector space models","document categorization","text classification","feature weighting"],"tags":["text mining","automatic text categorization","feature selection methods","machine learning","natural language processing","feature selection","term weighting","vector space models","document categorization","text classification","feature weighting"]},{"p_id":16525,"title":"Extractive summarisation of legal texts","abstract":"We describe research carried out as part of a text summarisation project for the legal domain for which we use a new XML corpus of judgments of the UK House of Lords. These judgments represent a particularly important part of public discourse due to the role that precedents play in English law. We present experimental results using a range of features and machine learning techniques for the task of predicting the rhetorical status of sentences and for the task of selecting the most summary-worthy sentences from a document. Results for these components are encouraging as they achieve state-of-the-art accuracy using robust, automatically generated cue phrase information. Sample output from the system illustrates the potential of summarisation technology for legal information management systems and highlights the utility of our rhetorical annotation scheme as a model of legal discourse, which provides a clear means for structuring summaries and tailoring them to different types of users. \u00a9 Springer Science+Business Media B.V. 2007.","keywords_author":["Automatic text summarisation","Knowledge management","Legal discourse","Machine learning","Natural language processing","XML"],"keywords_other":["Automatic text summarisation","Phrase information"],"max_cite":36.0,"pub_year":2006.0,"sources":"['scp']","rawkeys":["natural language processing","machine learning","legal discourse","automatic text summarisation","phrase information","knowledge management","xml"],"tags":["natural language processing","machine learning","legal discourse","automatic text summarisation","phrase information","knowledge management","xml"]},{"p_id":39056,"title":"Automated Learning of Temporal Expressions","abstract":"\u00a9 2015 IMIA and IOS Press. Clinical notes contain important temporal information that are critical for making clinical diagnosis and treatment as well as for retrospective analyses. Manually created regular expressions are commonly used for the extraction of temporal information; however, this can be a time consuming and brittle approach. We describe a novel algorithm for automatic learning of regular expressions in recognizing temporal expressions. Five classes of temporal expressions are identified. Keywords specific to those classes are used to retrieve snippets of text representing the same keywords in context. Those snippets are used for Regular Expression Discovery Extraction (REDEx). These learned regular expressions are then evaluated using 10-fold cross validation. Precision and recall are very high, above 0.95 for most classes.","keywords_author":["Electronic Medical Record","Machine Learning"],"keywords_other":["Sensitivity and Specificity","Electronic Health Records","Chronology as Topic","Time Factors","Reproducibility of Results","Terminology as Topic","Semantics","Natural Language Processing","Machine Learning","Vocabulary, Controlled","Data Mining"],"max_cite":1.0,"pub_year":2015.0,"sources":"['scp']","rawkeys":["terminology as topic","vocabulary","data mining","time factors","semantics","machine learning","electronic health records","natural language processing","reproducibility of results","chronology as topic","electronic medical record","controlled","sensitivity and specificity"],"tags":["terminology as topic","vocabulary","data mining","time factors","semantics","control","electronic health records","machine learning","natural language processing","reproducibility of results","chronology as topic","electronic medical record","sensitivity and specificity"]},{"p_id":34963,"title":"Integrating statistical and lexical information for recognizing textual entailments in text","abstract":"Recognizing textual entailment is to infer that a given text span follows from the meaning of a given hypothesis. To have better recognition capability, it is necessary to employ deep text processing units such as syntactic parsers and semantic taggers. However, these resources are not usually available in other non-English languages. In this paper, we present a light-weight Chinese textual entailment recognition system using part-of-speech information only. We designed two different feature models from training data and employed the well-known kernel method to learn to predict testing data. One feature set abstracts the generic statistics between the text pairs, while the other set directly models lexical features based on the traditional bag-of-words model. The ability of the proposed feature models not only brings additional statistical information from their datasets but also helps to enhance the prediction capability. To validate this, we conducted the experiments on the novel benchmark corpus-NTCIR-RITE-2011. The empirical results demonstrate that our method achieves the best results in comparison to the other competitors. In terms of accuracy, our method achieves 54.77% for the NTCIR RITE MC task. \u00a9 2012 Elsevier B.V. All rights reserved.","keywords_author":["Kernel methods","Machine learning","Natural language processing","Text mining","Textual entailment"],"keywords_other":["Text mining","Statistical information","Non-English languages","Data sets","Syntactic parsers","Lexical features","Part Of Speech","Semantic tagger","Lexical information","Recognition systems","Testing data","Training data","Bag-of-words models","Kernel methods","Recognizing textual entailments","NAtural language processing","Textual entailment","Feature sets","Prediction capability","Directly model","Light weight","Feature models","Processing units","Deep text"],"max_cite":2.0,"pub_year":2013.0,"sources":"['scp']","rawkeys":["lexical information","syntactic parsers","textual entailment","deep text","data sets","kernel methods","lexical features","training data","processing units","text mining","prediction capability","machine learning","statistical information","feature models","light weight","recognizing textual entailments","natural language processing","testing data","directly model","non-english languages","part of speech","bag-of-words models","recognition systems","semantic tagger","feature sets"],"tags":["lexical information","syntactic parsers","textual entailment","deep text","data sets","predictive capabilities","kernel methods","lexical features","training data","processing units","text mining","machine learning","statistical information","feature models","test data","light weight","recognizing textual entailments","natural language processing","directly model","non-english languages","part of speech","bag-of-words models","recognition systems","semantic tagger","feature sets"]},{"p_id":26774,"title":"Sentiment analysis using convolutional neural network","abstract":"\u00a9 2015 IEEE.Sentiment analysis of text content is important for many natural language processing tasks. Especially, as the development of the social media, there is a big need in dig meaningful information from the big data on Internet through the sentiment analysis. Inspired by the successes of deep learning, we are interested in handling the sentiment analysis task using deep learning models. In this paper, we propose a framework called Word2vec + Convolutional Neural Network (CNN). Firstly, we use the word2vec proposed by Google to compute vector representations of words, which will be the input for the CNN. The purpose of using word2vec is to gain the vector representation of word and reflect the distance of words. That will lead to initialize the parameters at a good point of CNN, which can efficiently improve the performance of the nets in this problem. Secondly, we design a suitable CNN architecture for the sentiment analysis task. We use 3 pairs of convolutional layers and pooling layers in this architecture. To the best of our knowledge, this is the first time that a 7-layers architecture model is applied using word2vec and CNN to analyze sentences' sentiment. We also use the Parametric Rectified Linear Unit (PReLU), Normalization and Dropout technology to improve the accuracy and generalizability of our model. We test our framework in a public dataset which is the corpus of movie review excerpts that includes fives labels: negative, somewhat negative, neural, somewhat positive and positive. Our network achieves test accuracy of 45.4% in this dataset, which is a better performance than some other neural network model like Recursive Neural Network (RNN) and Matrix-Vector Recursive Neural Network (MV-RNN).","keywords_author":["Deep learning","Sentiment analysis","Word2vec"],"keywords_other":["Deep learning","Word2vec","Sentiment analysis","Vector representations","Recursive neural networks","Convolutional neural network","Architecture modeling","NAtural language processing"],"max_cite":6.0,"pub_year":2015.0,"sources":"['ieee', 'scp', 'wos']","rawkeys":["deep learning","word2vec","natural language processing","vector representations","convolutional neural network","architecture modeling","sentiment analysis","recursive neural networks"],"tags":["word2vec","natural language processing","machine learning","vector representations","convolutional neural network","architecture modeling","sentiment analysis","recursive neural networks"]},{"p_id":34968,"title":"Using Machine Learning and Information Retrieval Techniques to Improve Software Maintainability","abstract":"In this paper, we investigate some ideas based on Machine Learning, Natural Language Processing, and Information Retrieval to outline possible research directions in the field of software architecture recovery and clone detection. In particular, after presenting an extensive related work, we illustrate two proposals for addressing these two issues, that represent hot topics in the field of Software Maintenance. Both proposals use Kernel Methods for exploiting structural representation of source code and to automate the detection of clones and the recovery of the actually implemented architecture in a subject software system. \u00a9 Springer-Verlag Berlin Heidelberg 2013.","keywords_author":["Information Retrieval","Machine Learning","Natural Language Processing","Software Maintenance and Evolution"],"keywords_other":["Clone detection","Structural representation","Implemented architectures","Software maintenance and evolution","Software systems","Software architecture recovery","NAtural language processing","Software maintainability"],"max_cite":2.0,"pub_year":2013.0,"sources":"['scp']","rawkeys":["implemented architectures","structural representation","natural language processing","machine learning","information retrieval","software systems","software maintainability","clone detection","software architecture recovery","software maintenance and evolution"],"tags":["implemented architectures","structural representation","natural language processing","machine learning","information retrieval","software systems","software maintainability","clone detection","software architecture recovery","software maintenance and evolution"]},{"p_id":20633,"title":"On the automatic classification of app reviews","abstract":"\u00a9 2016, Springer-Verlag London. App stores like Google Play and Apple AppStore have over 3 million apps covering nearly every kind of software and service. Billions of users regularly download, use, and review these apps. Recent studies have shown that reviews written by the users represent a rich source of information for the app vendors and the developers, as they include information about bugs, ideas for new features, or documentation of released features. The majority of the reviews, however, is rather non-informative just praising the app and repeating to the star ratings in words. This paper introduces several probabilistic techniques to classify app reviews into four types: bug reports, feature requests, user experiences, and text ratings. For this, we use review metadata such as the star rating and the tense, as well as, text classification, natural language processing, and sentiment analysis techniques. We conducted a series of experiments to compare the accuracy of the techniques and compared them with simple string matching. We found that metadata alone results in a poor classification accuracy. When combined with simple text classification and natural language preprocessing of the text\u2014particularly with bigrams and lemmatization\u2014the classification precision for all review types got up to 88\u201392 % and the recall up to 90\u201399 %. Multiple binary classifiers outperformed single multiclass classifiers. Our results inspired the design of a review analytics tool, which should help app vendors and developers deal with the large amount of reviews, filter critical reviews, and assign them to the appropriate stakeholders. We describe the tool main features and summarize nine interviews with practitioners on how review analytics tools including ours could be used in practice.","keywords_author":["Data-driven requirements engineering","Machine learning","Natural language processing","Review analytics","Software analytics","User feedback"],"keywords_other":["Automatic classification","NAtural language processing","Classification precision","Probabilistic technique","Data driven","Multi-class classifier","Classification accuracy","User feedback"],"max_cite":16.0,"pub_year":2016.0,"sources":"['scp']","rawkeys":["classification precision","software analytics","review analytics","automatic classification","classification accuracy","machine learning","natural language processing","data-driven requirements engineering","multi-class classifier","data driven","probabilistic technique","user feedback"],"tags":["classification precision","software analytics","review analytics","automatic classification","classification accuracy","machine learning","natural language processing","data-driven requirements engineering","multi-class classifier","data driven","probabilistic technique","user feedback"]},{"p_id":20642,"title":"Bringing order to chaos in MOOC discussion forums with content-related thread identification","abstract":"\u00a9 2016 ACM.This study addresses the issues of overload and chaos in MOOC discussion forums by developing a model to categorize and identify threads based on whether or not they are substantially related to the course content. Content-related posts were defined as those that give\/seek help for the learning of course material and share\/comment on relevant resources. A linguistic model was built based on manually-coded starting posts in threads from a statistics MOOC (n=837) and tested on thread starting posts from the second offering of the same course (n=304) and a different statistics course (n=298). The number of views and votes threads received were tested to see if they helped classification. Results showed that content-related posts in the statistics MOOC had distinct linguistic features which appeared to be unrelated to the subject-matter domain; the linguistic model demonstrated good cross-course reliability (all recall and precision > .77) and was useful across all time segments of the courses; number of views and votes were not helpful for classification.","keywords_author":["Discussion forum","Machine learning","Massive open online courses","Natural language processing","Social interaction"],"keywords_other":["Social interactions","Number of views","Discussion forum","Linguistic modeling","Recall and precision","Massive open online course","Linguistic features","NAtural language processing"],"max_cite":16.0,"pub_year":2016.0,"sources":"['scp']","rawkeys":["recall and precision","social interactions","massive open online course","machine learning","natural language processing","number of views","linguistic features","linguistic modeling","massive open online courses","discussion forum","social interaction"],"tags":["recall and precision","social interactions","machine learning","natural language processing","number of views","linguistic features","linguistic modeling","mooc","discussion forum"]},{"p_id":39075,"title":"Proper nouns recognition in Arabic crime text using machine learning approach","abstract":"\u00a9 2005 - 2015 JATIT & LLS. All rights reserved. Named Entity Recognition (NER) identifies proper nouns in a text and categorizes it as a distinct kind of named entities. This function enables the extraction of peoples name, locations, organizations, and currencies. Several research abound in this area in Arabic NER is concerned. However, recognizing Arabic named entities is challenging due to the complexity in the Arabic language. These complexities are represented by non-existence of capitalization feature which facilitates the process of NER. Furthermore, there is a lack of lexical corpora that may include all the Arabic NEs. On other hand, most of the approaches that have been proposed for Arabic NER were based on handcrafted rule-based methods which can be laborious and time consuming. Therefore, this paper presents our attempt at recognizing and extracting the most important named entities, such as names of persons, locations, organizations, crime types, dates and times in Arabic crime documents using the Decision Tree classifier and feature extraction for crime dataset. The dataset consists of varying data sizes collected from online resources and undergone multiple pre-processing tasks. Additionally, the feature extraction task which includes POS tagging, keyword trigger, definite articles and affixes has been performed. Furthermore the classifier will utilize these features in order to classify the named entities. The results demonstrate that the use of the Decision Tree (DT) yields good results in small size datasets, but failed when the dataset is large in the case of the crime domain. This is due to the difficulty of relevant keywords and features in such fields. The best result for this experiment is 81.35% F-measure.","keywords_author":["Arabic crime document","Decision tree","Machine learning","Named entity recognition","Natural language processing"],"keywords_other":null,"max_cite":1.0,"pub_year":2015.0,"sources":"['scp']","rawkeys":["named entity recognition","natural language processing","machine learning","arabic crime document","decision tree"],"tags":["named entity recognition","natural language processing","machine learning","arabic crime document","decision trees"]},{"p_id":51363,"title":"Detection of demographics and identity in spontaneous speech and writing","abstract":"\u00a9 Springer International Publishing Switzerland 2015. This chapter focuses on the automatic identification of demographic traits and identity in both speech and writing. We address language use in the virtual world of online games and text entry on mobile devices in the form of chat, email and nicknames, and demonstrate text factors that correlate with demographics, such as age, gender, personality, and interaction style. Also presented here is work on speakers identification in spontaneous language use, where we describe the state of the art in verification, feature extraction, modeling and calibration across multiple environmental conditions. Finally, we bring speech and writing together to explore approaches to user authentication that span language in general. We discuss how speech-specific factors such as intonation, and writing-specific features such as spelling, punctuation, and typing correction correlate and predict one another as a function of users\u2019 sociolinguistic characteristics.","keywords_author":["Active authentication","Biometrics","Calibration","Demographics","Language","Machine learning","Natural language processing","Prosody","Sociolinguistics","Speaker recognition","Speech","Stylistics","Virtual worlds","Writing"],"keywords_other":["Demographics","Stylistics","Language","Sociolinguistics","Prosody","Virtual worlds","Speaker recognition"],"max_cite":0.0,"pub_year":2015.0,"sources":"['scp']","rawkeys":["active authentication","calibration","sociolinguistics","virtual worlds","machine learning","natural language processing","speaker recognition","prosody","biometrics","speech","stylistics","demographics","writing","language"],"tags":["active authentication","calibration","sociolinguistics","virtual worlds","machine learning","natural language processing","speaker recognition","prosody","biometrics","speech","stylistics","demographics","writing","language"]},{"p_id":53411,"title":"Comparison of clinical knowledge bases for summarization of electronic health records","abstract":"Automated summarization tools that create condition-specific displays may improve clinician efficiency. These tools require new kinds of knowledge that is difficult to obtain. We compared five problem-medication pair knowledge bases generated using four previously described knowledge base development approaches. The number of pairs in the resulting mapped knowledge bases varied widely due to differing mapping techniques from the source terminologies, ranging from 2,873 to 63,977,738 pairs. The number of overlapping pairs across knowledge bases was low, with one knowledge base having half of the pairs overlapping with another knowledge base, and most having less than a third overlapping. Further research is necessary to better evaluate the knowledge bases independently in additional settings, and to identify methods to integrate the knowledge bases. \u00a9 2013 IMIA and IOS Press.","keywords_author":["data collection","Electronic health records","knowledge bases","machine learning"],"keywords_other":["Electronic Health Records","Systems Integration","Pattern Recognition, Automated","Natural Language Processing","Medical Record Linkage","Vocabulary, Controlled","Artificial Intelligence","United States"],"max_cite":0.0,"pub_year":2013.0,"sources":"['scp']","rawkeys":["knowledge bases","artificial intelligence","vocabulary","automated","machine learning","electronic health records","natural language processing","united states","data collection","controlled","medical record linkage","pattern recognition","systems integration"],"tags":["vocabulary","automated","machine learning","electronic health records","control","knowledge base","natural language processing","system integration","united-states","data collection","medical record linkage","pattern recognition"]},{"p_id":39078,"title":"A preliminary study of clinical abbreviation disambiguation in real time","abstract":"\u00a9 Schattauer 2015. Objective: To save time, healthcare providers frequently use abbreviations while authoring clinical documents. Nevertheless, abbreviations that authors deem unambiguous often confuse other readers, including clinicians, patients, and natural language processing (NLP) systems. Most current clinical NLP systems \u201cpost-process\u201d notes long after clinicians enter them into electronic health record systems (EHRs). Such post-processing cannot guarantee 100% accuracy in abbreviation identification and disambiguation, since multiple alternative interpretations exist. Methods: Authors describe a prototype system for real-time Clinical Abbreviation Recognition and Disambiguation (rCARD) \u2013 i.e., a system that interacts with authors during note generation to verify correct abbreviation senses. The rCARD system design anticipates future integration with webbased clinical documentation systems to improve quality of healthcare records. When clinicians enter documents, rCARD will automatically recognize each abbreviation. For abbreviations with multiple possible senses, rCARD will show a ranked list of possible meanings with the best predicted sense at the top. The prototype application embodies three word sense disambiguation (WSD) methods to predict the correct senses of abbreviations. We then conducted three experments to evaluate rCARD, including 1) a performance evaluation of different WSD methods; 2) a time evaluation of real-time WSD methods; and 3) a user study of typing clinical sentences with abbreviations using rCARD. Results: Using 4,721 sentences containing 25 commonly observed, highly ambiguous clinical abbreviations, our evaluation showed that the best profile-based method implemented in rCARD achieved a reasonable WSD accuracy of 88.8% (comparable to SVM \u2013 89.5%) and the cost of time for the different WSD methods are also acceptable (ranging from 0.630 to 1.649 milliseconds within the same network). The preliminary user study also showed that the extra time costs by rCARD were about 5% of total document entry time and users did not feel a significant delay when using rCARD for clinical document entry. Conclusion: The study indicates that it is feasible to integrate a real-time, NLP-enabled abbreviation recognition and disambiguation module with clinical documentation systems.","keywords_author":["Clinical abbreviation","Clinical documentation system","Machine learning"],"keywords_other":["Health Personnel","Abbreviations as Topic","User-Computer Interface","Time Factors","Natural Language Processing","Documentation"],"max_cite":1.0,"pub_year":2015.0,"sources":"['scp']","rawkeys":["time factors","documentation","machine learning","natural language processing","health personnel","clinical abbreviation","user-computer interface","abbreviations as topic","clinical documentation system"],"tags":["time factors","documentation","machine learning","natural language processing","health personnel","clinical abbreviation","user-computer interface","abbreviations as topic","clinical documentation system"]},{"p_id":39090,"title":"RENA: A named entity recognition system for Arabic","abstract":"\u00a9 Springer International Publishing Switzerland 2015.The Named Entity Recognition (NER) task aims to identify and categorize proper and important nouns in a text. This Natural Language Processing task proved to be challenging for languages with a rich morphology such as the Arabic language. In this paper, We introduce a new named entity recognizer for Arabic. This recognizer is based on Conditional Random Fields (CRF) and an optimized feature set that combines contextual, lexical, morphological and gazetteers features. Our system outperforms the state-of-the-art Arabic NER systems with a Fmeasure of 93.5% when applied to ANERcorp standard dataset.","keywords_author":["Arabic","Machine learning","Named entity recognition","Natural language processing"],"keywords_other":["Named entities","Named entity recognition","Arabic","State of the art","Arabic languages","Conditional random field","NAtural language processing","Feature sets"],"max_cite":1.0,"pub_year":2015.0,"sources":"['scp']","rawkeys":["conditional random field","named entity recognition","machine learning","natural language processing","state of the art","arabic","arabic languages","named entities","feature sets"],"tags":["conditional random field","named entity recognition","machine learning","natural language processing","state of the art","arabic","arabic languages","named entities","feature sets"]},{"p_id":18616,"title":"NeuroElectro: A window to the world's neuron electrophysiology data","abstract":"The behavior of neural circuits is determined largely by the electrophysiological properties of the neurons they contain. Understanding the relationships of these properties requires the ability to first identify and catalog each property. However, information about such properties is largely locked away in decades of closed-access journal articles with heterogeneous conventions for reporting results, making it difficult to utilize the underlying data. We solve this problem through the NeuroElectro project: a Python library, RESTful API, and web application (at http:\/\/neuroelectro.org) for the extraction, visualization, and summarization of published data on neurons' electrophysiological properties. Information is organized both by neuron type (using neuron definitions provided by NeuroLex) and by electrophysiological property (using a newly developed ontology). We describe the techniques and challenges associated with the automated extraction of tabular electrophysiological data and methodological metadata from journal articles. We further discuss strategies for how to best combine, normalize and organize data across these heterogeneous sources. NeuroElectro is a valuable resource for experimental physiologists attempting to supplement their own data, for computational modelers looking to constrain their model parameters, and for theoreticians searching for undiscovered relationships among neurons and their properties. \u00a9 2014 Tripathy, Savitskaya, Burton, Urban and Gerkin.","keywords_author":["API","Database","Electrophysiology","Machine learning","Metadata","Natural language processing","Neuroinformatics","Text-mining"],"keywords_other":null,"max_cite":23.0,"pub_year":2014.0,"sources":"['scp']","rawkeys":["neuroinformatics","electrophysiology","machine learning","natural language processing","database","api","text-mining","metadata"],"tags":["neuroinformatics","text mining","electrophysiology","databases","machine learning","natural language processing","api","metadata"]},{"p_id":53436,"title":"Computational geometry applied to natural language processing","abstract":"Sometimes we need to distinguish person's identities by language Style by computer, i.e., we need to explore some computational method of Language Style. This needs cross-field research around Language Information Retrieval technique. Analysis on some properties of a natural language model by Abstract Geometry gives out the Machine Learning theory of Language Style, which is a branch of the Natural Language Processing. The theoretical basis of Language Style computation is established and a new Machine Learning method is given at the same time.","keywords_author":["Abstract geometry","Artificial psychology","Machine learning","Natural language processing"],"keywords_other":["Language style","Person identities","Abstract geometry","Artificial psychology"],"max_cite":0.0,"pub_year":2007.0,"sources":"['scp']","rawkeys":["language style","natural language processing","machine learning","artificial psychology","person identities","abstract geometry"],"tags":["language style","natural language processing","machine learning","artificial psychology","personal identity","abstract geometry"]},{"p_id":45246,"title":"Extracting cancer mortality statistics from death certificates: A hybrid machine learning and rule-based approach for common and rare cancers","abstract":"\u00a9 2018 Objective: Death certificates are an invaluable source of cancer mortality statistics. However, this value can only be realised if accurate, quantitative data can be extracted from certificates\u2014an aim hampered by both the volume and variable quality of certificates written in natural language. This paper proposes an automatic classification system for identifying all cancer related causes of death from death certificates. Methods: Detailed features, including terms, n-grams and SNOMED CT concepts were extracted from a collection of 447,336 death certificates. The features were used as input to two different classification sub-systems: a machine learning sub-system using Support Vector Machines (SVMs) and a rule-based sub-system. A fusion sub-system then combines the results from SVMs and rules into a single final classification. A held-out test set was used to evaluate the effectiveness of the classifiers according to precision, recall and F-measure. Results: The system was highly effective at determining the type of cancers for both common cancers (F-measure of 0.85) and rare cancers (F-measure of 0.7). In general, rules performed superior to SVMs; however, the fusion method that combined the two was the most effective. Conclusion: The system proposed in this study provides automatic identification and characterisation of cancers from large collections of free-text death certificates. This allows organisations such as Cancer Registries to monitor and report on cancer mortality in a timely and accurate manner. In addition, the methods and findings are generally applicable beyond cancer classification and to other sources of medical text besides death certificates.","keywords_author":["Cancer classification","Death certificates","Hybrid","Machine learning","Natural language processing","Rules"],"keywords_other":["Automatic identification","Hybrid machine learning","Cancer classification","Rules","Death certificates","Automatic classification systems","Support vector machine (SVMs)","Hybrid"],"max_cite":0.0,"pub_year":2018.0,"sources":"['scp']","rawkeys":["automatic classification systems","hybrid","rules","machine learning","natural language processing","cancer classification","automatic identification","hybrid machine learning","support vector machine (svms)","death certificates"],"tags":["automatic classification systems","rules","machine learning","natural language processing","cancer classification","automatic identification","hybrid machine learning","heterogeneity","death certificates"]},{"p_id":16578,"title":"Toward deep learning software repositories","abstract":"\u00a9 2015 IEEE.Deep learning subsumes algorithms that automatically learn compositional representations. The ability of these models to generalize well has ushered in tremendous advances in many fields such as natural language processing (NLP). Recent research in the software engineering (SE) community has demonstrated the usefulness of applying NLP techniques to software corpora. Hence, we motivate deep learning for software language modeling, highlighting fundamental differences between state-of-the-practice software language models and connectionist models. Our deep learning models are applicable to source code files (since they only require lexically analyzed source code written in any programming language) and other types of artifacts. We show how a particular deep learning model can remember its state to effectively model sequential data, e.g., Streaming software tokens, and the state is shown to be much more expressive than discrete tokens in a prefix. Then we instantiate deep learning models and show that deep learning induces high-quality models compared to n-grams and cache-based n-grams on a corpus of Java projects. We experiment with two of the models' hyper parameters, which govern their capacity and the amount of context they use to inform predictions, before building several committees of software language models to aid generalization. Then we apply the deep learning models to code suggestion and demonstrate their effectiveness at a real SE task compared to state-of-the-practice models. Finally, we propose avenues for future work, where deep learning can be brought to bear to support model-based testing, improve software lexicons, and conceptualize software artifacts. Thus, our work serves as the first step toward deep learning software repositories.","keywords_author":["Deep learning","Machine learning","N-grams","Neural networks","Software language models","Software repositories"],"keywords_other":["Deep learning","Compositional representation","Software repositories","State of the practice","N-grams","Software languages","Connectionist models","NAtural language processing"],"max_cite":35.0,"pub_year":2015.0,"sources":"['ieee', 'scp', 'wos']","rawkeys":["state of the practice","connectionist models","compositional representation","n-grams","deep learning","neural networks","machine learning","natural language processing","software repositories","software language models","software languages"],"tags":["state of the practice","connectionist models","n-grams","neural networks","natural language processing","composite representations","machine learning","software repositories","software language models","software languages"]},{"p_id":20675,"title":"Sentiment analysis: Measuring opinions","abstract":"\u00a9 2015 Published by Elsevier B.V. This is an open access article under the CC BY-NC-ND license.Sentiment Analysis is an area of study within Natural Language Processing that is concerned with identifying the mood or opinion of subjective elements within a text. This paper focuses on the various methods used for classifying a given piece of natural language text according to the opinions expressed in it i.e. whether the general attitude is negative or positive. We also discuss the two-step method (aspect classification followed by polarity classification) that we followed along with the experimental setup.","keywords_author":["Machine learning","Mood detection","NLP","Opinion mining","Sentiment Analysis"],"keywords_other":["Polarity classification","NLP","Sentiment analysis","Natural language text","Mood detection","Two step method","NAtural language processing","Opinion mining"],"max_cite":16.0,"pub_year":2015.0,"sources":"['scp']","rawkeys":["nlp","natural language processing","machine learning","two step method","mood detection","sentiment analysis","opinion mining","polarity classification","natural language text"],"tags":["natural language processing","machine learning","two step method","mood detection","sentiment analysis","opinion mining","polarity classification","natural language text"]},{"p_id":18629,"title":"Text summarization in the biomedical domain: A systematic review of recent research","abstract":"\u00a9 2014 Elsevier Inc.Objective: The amount of information for clinicians and clinical researchers is growing exponentially. Text summarization reduces information as an attempt to enable users to find and understand relevant source texts more quickly and effortlessly. In recent years, substantial research has been conducted to develop and evaluate various summarization techniques in the biomedical domain. The goal of this study was to systematically review recent published research on summarization of textual documents in the biomedical domain. Materials and methods: MEDLINE (2000 to October 2013), IEEE Digital Library, and the ACM digital library were searched. Investigators independently screened and abstracted studies that examined text summarization techniques in the biomedical domain. Information is derived from selected articles on five dimensions: input, purpose, output, method and evaluation. Results: Of 10,786 studies retrieved, 34 (0.3%) met the inclusion criteria. Natural language processing (17; 50%) and a hybrid technique comprising of statistical, Natural language processing and machine learning (15; 44%) were the most common summarization approaches. Most studies (28; 82%) conducted an intrinsic evaluation. Discussion: This is the first systematic review of text summarization in the biomedical domain. The study identified research gaps and provides recommendations for guiding future research on biomedical text summarization. Conclusion: Recent research has focused on a hybrid technique comprising statistical, language processing and machine learning techniques. Further research is needed on the application and evaluation of text summarization in real research or patient care settings.","keywords_author":["Biomedical domain","Intrinsic evaluation","Language processing","Machine learning","Text summarization"],"keywords_other":["MEDLINE","Clinical researchers","Language processing","Abstracting and Indexing as Topic","Humans","Amount of information","Natural Language Processing","Text summarization","Intrinsic evaluation","Artificial Intelligence","Information Storage and Retrieval","Biomedical domain","NAtural language processing","Machine learning techniques"],"max_cite":23.0,"pub_year":2014.0,"sources":"['scp']","rawkeys":["medline","artificial intelligence","amount of information","abstracting and indexing as topic","intrinsic evaluation","machine learning","humans","machine learning techniques","natural language processing","text summarization","biomedical domain","clinical researchers","language processing","information storage and retrieval"],"tags":["medline","clinical research","amount of information","abstracting and indexing as topic","machine learning","intrinsic evaluation","humans","machine learning techniques","natural language processing","text summarization","biomedical domain","language processing","information storage and retrieval"]},{"p_id":24773,"title":"Predicting early psychiatric readmission with natural language processing of narrative discharge summaries","abstract":"The ability to predict psychiatric readmission would facilitate the development of interventions to reduce this risk, a major driver of psychiatric health-care costs. The symptoms or characteristics of illness course necessary to develop reliable predictors are not available in coded billing data, but may be present in narrative electronic health record (EHR) discharge summaries. We identified a cohort of individuals admitted to a psychiatric inpatient unit between 1994 and 2012 with a principal diagnosis of major depressive disorder, and extracted inpatient psychiatric discharge narrative notes. Using these data, we trained a 75-topic Latent Dirichlet Allocation (LDA) model, a form of natural language processing, which identifies groups of words associated with topics discussed in a document collection. The cohort was randomly split to derive a training (70%) and testing (30%) data set, and we trained separate support vector machine models for baseline clinical features alone, baseline features plus common individual words and the above plus topics identified from the 75-topic LDA model. Of 4687 patients with inpatient discharge summaries, 470 were readmitted within 30 days. The 75-topic LDA model included topics linked to psychiatric symptoms (suicide, severe depression, anxiety, trauma, eating\/weight and panic) and major depressive disorder comorbidities (infection, postpartum, brain tumor, diarrhea and pulmonary disease). By including LDA topics, prediction of readmission, as measured by area under receiver-operating characteristic curves in the testing data set, was improved from baseline (area under the curve 0.618) to baseline+1000 words (0.682) to baseline+75 topics (0.784). Inclusion of topics derived from narrative notes allows more accurate discrimination of individuals at high risk for psychiatric readmission in this cohort. Topic modeling and related approaches offer the potential to improve prediction using EHRs, if generalizability can be established in other clinical cohorts.","keywords_author":null,"keywords_other":["Male","Cohort Studies","Risk Assessment","Electronic Health Records","Depressive Disorder, Major","Time Factors","Humans","Aged","Middle Aged","Adult","Natural Language Processing","Patient Readmission","Patient Discharge Summaries","Narration","Massachusetts","Models, Statistical","Female","Kaplan-Meier Estimate"],"max_cite":9.0,"pub_year":2016.0,"sources":"['scp']","rawkeys":["statistical","aged","patient readmission","adult","major","kaplan-meier estimate","middle aged","electronic health records","risk assessment","models","narration","cohort studies","time factors","depressive disorder","humans","massachusetts","male","natural language processing","female","patient discharge summaries"],"tags":["aged","patient readmission","adult","major","kaplan-meier estimate","middle aged","electronic health records","risk assessment","narration","cohort studies","time factors","depressive disorder","humans","massachusetts","statistics","model","male","natural language processing","female","patient discharge summaries"]},{"p_id":2246,"title":"Integration of behaviors and languages with a hierarchal structure self-organized in a neuro-dynamical model","abstract":"This paper proposes an approach for robots to ac-quire language grounding in their robot's sensory-motor flow using neuro-dynamical models. We trained our neuro-dynamical model over a set of sentences represented as sequences of characters. For the integrated recognition, we introduced a cognitive hypothesis for integrated recognition where a human's brain separately processed the \u201cstructure\u201d and \u201ccontents\u201d of a sentence. Our model was trained with the spelling of words and their semantic role emerged in the first model. As a result of binding the model with sensory-motion patterns, we confirmed that it could associate proper word spellings with a sensory-motor flows and a semantic roles, even if an observed flow had not been learned.","keywords_author":["neuro-dynamical systems","behaviors","language grammer","word contents","symbol grounding"],"keywords_other":["sensory motion patterns","cognitive hypothesis","human-robot interaction","neurodynamical models","human brain","Pragmatics","word spellings","natural language interfaces","hierarchal structure","robot sensory motor flow","integrated recognition","Hidden Markov models","Robot sensing systems","neural nets","semantic roles","Neurons","language grounding","natural language processing","speech processing","Brain modeling","Grounding"],"max_cite":1.0,"pub_year":2013.0,"sources":"['ieee']","rawkeys":["sensory motion patterns","cognitive hypothesis","human-robot interaction","neurodynamical models","human brain","neuro-dynamical systems","brain modeling","pragmatics","robot sensing systems","symbol grounding","neurons","grounding","word spellings","natural language interfaces","hierarchal structure","robot sensory motor flow","integrated recognition","neural nets","semantic roles","hidden markov models","language grounding","natural language processing","behaviors","speech processing","language grammer","word contents"],"tags":["sensory motion patterns","cognitive hypothesis","human-robot interaction","human brain","neuro-dynamical systems","brain modeling","pragmatics","robot sensing systems","symbol grounding","neurons","behavior","grounding","neurodynamic models","word spellings","natural language interfaces","hierarchical structures","neural networks","robot sensory motor flow","integrated recognition","semantic roles","hidden markov models","language grounding","natural language processing","speech processing","language grammer","word contents"]},{"p_id":2247,"title":"Document similarity estimation for sentiment analysis using neural network","abstract":"It is important to classify documents according to their contents because of finding necessary documents efficiently. To achieve good classification document similarity estimation is one of key techniques since classification is executed based on the document similarity. In natural language processing bag-of-words model is used to extract features from documents and term occurrence frequency based value is used as a weight of each features. However, the term weight methodologies usually use predefined models and include some limitations. New approaches to construct feature vectors based on data distribution are desired to achieve high performance of natural language processing. These days many researchers pay attention to deep learning. Deep learning is a new approach to transform raw data to feature vectors using many unlabeled data. This characteristics is desirable to satisfy a previous need. In natural language processing a main aim is to construct a language model on a deep architecture neural network. In this paper we use a deep architecture neural network to estimate document similarity. To obtain good article similarity estimation we have to generate good article vectors that can represent all article characteristics. Hence, we use many stock market news to train the deep architecture neural network and generate article vectors with the trained neural network. And we calculate cosine similarity between labeled articles and discuss performance of the deep architecture neural network. In evaluation we do not focus on articles' contents but on their sentiment polarity. Hence, we discuss whether the proposed method classifies articles according to their sentiment polarity or not. We confirmed though the proposed method is an unsupervised learning approach, it achieves good performance in stock market news similarity estimation. The results show a deep architecture neural network can be applied to more natural language processing tasks.","keywords_author":null,"keywords_other":["Biological neural networks","document classification","term occurrence frequency based value","document handling","data distribution","sentiment analysis","Feature extraction","feature vectors","Estimation","deep learning","term weight methodologies","bag-of-words model","Vectors","language model","unsupervised learning","neural nets","document similarity estimation","pattern classification","Neurons","unsupervised learning approach","stock market news similarity estimation","sentiment polarity","natural language processing","feature extraction","deep architecture neural network","Natural language processing"],"max_cite":1.0,"pub_year":2013.0,"sources":"['wos', 'ieee']","rawkeys":["document classification","document handling","term occurrence frequency based value","data distribution","sentiment analysis","vectors","feature vectors","biological neural networks","neurons","estimation","deep learning","term weight methodologies","bag-of-words model","language model","unsupervised learning","neural nets","document similarity estimation","pattern classification","unsupervised learning approach","sentiment polarity","stock market news similarity estimation","natural language processing","feature extraction","deep architecture neural network"],"tags":["document classification","document handling","term occurrence frequency based value","data distribution","sentiment analysis","machine learning","vectors","feature vectors","biological neural networks","neurons","estimation","neural networks","term weight methodologies","language model","unsupervised learning","document similarity estimation","pattern classification","unsupervised learning approach","sentiment polarity","stock market news similarity estimation","natural language processing","feature extraction","deep architecture neural network","bag-of-words models"]},{"p_id":14537,"title":"AutoCog: Measuring the description-to-permission fidelity in Android applications","abstract":"The booming popularity of smartphones is partly a result of application markets where users can easily download wide range of third-party applications. However, due to the open nature of markets, especially on Android, there have been several privacy and security concerns with these applications. On Google Play, as with most other markets, users have direct access to natural-language descriptions of those applications, which give an intuitive idea of the functionality including the security-related information of those applications. Google Play also provides the permissions requested by applications to access security and privacy-sensitive APIs on the devices. Users may use such a list to evaluate the risks of using these applications. To best assist the end users, the descriptions should reflect the need for permissions, which we term description-to-permission fidelity. In this paper, we present a system AutoCog to automatically assess description-to-permission fidelity of applications. AutoCog employs state-of-the-art techniques in natural language processing and our own learning-based algorithm to relate description with permissions. In our evaluation, AutoCog outperforms other related work on both performance of detection and ability of generalization over various permissions by a large extent. On an evaluation of eleven permissions, we achieve an average precision of 92.6% and an average recall of 92.0%. Our large-scale measurements over 45,811 applications demonstrate the severity of the problem of low description-to-permission fidelity. AutoCog helps bridge the long-lasting usability gap between security techniques and average users. Copyright 2014 ACM.","keywords_author":["Android","Google play","Machine learning","Mobile","Natural language processing","Permissions"],"keywords_other":["Permissions","Google plays","Android","Mobile","NAtural language processing"],"max_cite":57.0,"pub_year":2014.0,"sources":"['scp', 'wos']","rawkeys":["machine learning","android","mobile","natural language processing","google plays","google play","permissions"],"tags":["neural networks","natural language processing","machine learning","mobile","google plays","permissions"]},{"p_id":59595,"title":"Computer Vision and Natural Language Processing: Recent Approaches in Multimedia and Robotics","abstract":"Integrating computer vision and natural language processing is a novel interdisciplinary field that has received a lot of attention recently. In this survey, we provide a comprehensive introduction of the integration of computer vision and natural language processing in multimedia and robotics applications with more than 200 key references. The tasks that we survey include visual attributes, image captioning, video captioning, visual question answering, visual retrieval, human-robot interaction, robotic actions, and robot navigation. We also emphasize strategies to integrate computer vision and natural language processing models as a unified theme of distributional semantics. We make an analog of distributional semantics in computer vision and natural language processing as image embedding and word embedding, respectively. We also present a unified view for the field and propose possible future directions.","keywords_author":["Computer Vision","Natural Language Processing","Robotics","Language and vision","survey","multimedia","robotics","symbol grounding","distributional semantics","computer vision","natural language processing","visual attribute","image captioning","imitation learning","word2vec","word embedding","image embedding","semantic parsing","lexical semantics"],"keywords_other":["WEB DATA","DISTRIBUTIONAL SEMANTICS","PERCEPTION","NORMS","RECOGNITION","COMMUNICATION","MODELS","GRAMMAR","ANNOTATION","IMAGES"],"max_cite":0.0,"pub_year":2017.0,"sources":"['wos']","rawkeys":["semantic parsing","distributional semantics","image captioning","language and vision","web data","survey","imitation learning","word embedding","models","multimedia","symbol grounding","annotation","image embedding","visual attribute","recognition","images","word2vec","norms","communication","lexical semantics","computer vision","grammar","natural language processing","perception","robotics"],"tags":["semantic parsing","distributional semantics","image captioning","language and vision","web data","survey","imitation learning","word embedding","multimedia","symbol grounding","annotation","image embedding","recognition","images","word2vec","norms","communication","lexical semantics","perceptions","computer vision","visual attributes","model","grammar","natural language processing","robotics"]},{"p_id":43214,"title":"Multi-Source Learning for Sales Prediction","abstract":"\u00a9 2017 IEEE. With the convenience and popularity of Internet, the sales on e-commerce platforms have grown significantly. This exponential growth generates massive chunks of data, which provides the opportunity to utilize the historical data for predicting the customers' behaviors and can thus offer better services. One of the major tasks is to correctly estimate the coming sales of products since the e-retailers can reserve the products in a smart way. However, even with massive data, it is still challenging to estimate the sale amount of each product due to 1) the complicated language structures, 2) difficulties in integrating the features, and 3) missing values. To address these issues, we propose a framework for predicting the sales, which contains two phases: 1) feature extraction from product attributes and reviews, and 2) tensor decomposition for multi-source learning. The experimental results show that our framework outperforms baselines by 73%.","keywords_author":["Machine learning","natural language processing","tensor decomposition"],"keywords_other":["Historical data","Product attributes","Tensor decomposition","Massive data","Multi-Sources","Language structure","Missing values","Exponential growth"],"max_cite":0.0,"pub_year":2018.0,"sources":"['scp']","rawkeys":["product attributes","massive data","machine learning","language structure","natural language processing","historical data","tensor decomposition","exponential growth","multi-sources","missing values"],"tags":["product attributes","massive data","machine learning","language structure","natural language processing","historical data","tensor decomposition","exponential growth","missing values","multisources"]},{"p_id":59602,"title":"Visually Grounded Meaning Representations","abstract":"In this paper we address the problem of grounding distributional representations of lexical meaning. We introduce a new model which uses stacked autoencoders to learn higher-level representations from textual and visual input. The visual modality is encoded via vectors of attributes obtained automatically from images. We create a new large-scale taxonomy of 600 visual attributes representing more than 500 concepts and 700 K images. We use this dataset to train attribute classifiers and integrate their predictions with text-based distributional models of word meaning. We evaluate our model on its ability to simulate word similarity judgments and concept categorization. On both tasks, our model yields a better fit to behavioral data compared to baselines and related models which either rely on a single modality or do not make use of attribute-based input.","keywords_author":["Cognitive simulation","computer vision","distributed representations","concept learning","connectionism and neural nets","natural language processing"],"keywords_other":["LARGE SET","MODEL","RECOGNITION","FEATURE PRODUCTION NORMS","CATEGORIZATION","LANGUAGE","NETWORK","SEMANTIC MEMORY","SIMILARITY","ATTRIBUTES"],"max_cite":0.0,"pub_year":2017.0,"sources":"['ieee', 'wos']","rawkeys":["cognitive simulation","large set","network","recognition","model","semantic memory","similarity","natural language processing","attributes","categorization","distributed representations","feature production norms","concept learning","computer vision","connectionism and neural nets","language"],"tags":["large set","semantic memory","recognition","model","similarity","natural language processing","attributes","categorization","distributed representation","feature production norms","networks","concept learning","cognitive simulations","computer vision","connectionism and neural nets","language"]},{"p_id":35027,"title":"Adding intelligence to non-corpus based word sense disambiguation","abstract":"Natural language processing applications invariably perform word sense disambiguation as one of its processing steps. The accuracy of sense disambiguation depends upon an efficient algorithm as well as a reliable knowledge-base in the form of annotated corpus and\/or dictionaries in machine readable form. Algorithms working on corpus for sense disambiguation are generally employed as supervised machine learning systems. But such systems need ample training on the corpus before being applied on the actual data set. This paper discusses an unsupervised approach of a graph-based technique that solely works on a machine-readable dictionary as the knowledge source. This approach can improve the bottleneck problem that persists in corpus-based word sense disambiguation. The method described here attempts to make the algorithm more intelligent by considering various WordNet semantic relations and auto-filtration of content words before graph generation. \u00a9 2012 IEEE.","keywords_author":["Natural Language Processing","Semantic networks","Semi-supervised Machine Learning","Similarity Measures","Text Mining","Unsupervised Machine Learning","Word Sense Disambiguation"],"keywords_other":["Text mining","Semantic network","Semi-supervised","Similarity measure","Word Sense Disambiguation","Unsupervised machine learning","NAtural language processing"],"max_cite":2.0,"pub_year":2012.0,"sources":"['scp']","rawkeys":["unsupervised machine learning","semi-supervised machine learning","text mining","semantic network","semi-supervised","natural language processing","similarity measure","word sense disambiguation","similarity measures","semantic networks"],"tags":["unsupervised machine learning","semi-supervised machine learning","text mining","semantic network","semi-supervised","natural language processing","similarity measure","word sense disambiguation"]},{"p_id":51411,"title":"Mixed-script query labelling using supervised learning and ad hoc retrieval using sub word indexing","abstract":"\u00a9 2015 ACM. Much of the user generated content on the internet is written in their transliterated form instead of in their indigenous script. Due to this search engines receive a large number of transliterated search queries. This paper presents our approach to handle labelling of queries and ad hoc retrieval of documents based on these queries, as part of the FIRE2014 shared task on transliterated search. The content of each document is written in either the native Devanagari script or its transliterated form in Roman script or a combination of both. The queries to retrieve these documents can also be in mixed script. The task is challenging primarily due to the spelling variations that occur in the transliterated form of search queries. This particular problem is addressed by using back transliteration to reduce spelling variations, and a set of hand-tailored rules for consonant mapping. Sub-word indexing is done to take care of breaking and joining of transliterated words. Implementation of query labelling of the mixed script content was done using a supervised learning approach where an SVM classifier was trained using character n-grams as features for language identification. A Na\u00efve Bayes classifier was used for classifying transliterated words that can belong to both Hindi and English when looked at individually. The 2 runs submitted by our team (BITS-Lipyantaran) performs best across all metrics for Subtask 2 among all the teams that participated, with a MRR score of 0.8171 and MAP score of 0.6421.","keywords_author":["Language Identification","Language Modelling","Machine Learning","Mixed-script information retrieval","Natural Language Processing","Na\u00efve Bayes","Sub-word Indexing","Supervised learning","Support Vector Machines"],"keywords_other":["Supervised learning approaches","User-generated content","Back-transliteration","Language identification","Bayes Classifier","Language modelling","Sub words","NAtural language processing"],"max_cite":0.0,"pub_year":2014.0,"sources":"['scp']","rawkeys":["supervised learning","sub-word indexing","sub words","bayes classifier","language modelling","na\u00efve bayes","machine learning","natural language processing","supervised learning approaches","language identification","back-transliteration","user-generated content","support vector machines","mixed-script information retrieval"],"tags":["supervised learning","sub-word indexing","sub words","bayes classifier","machine learning","natural language processing","supervised learning approaches","language identification","back-transliteration","user-generated content","language model","naive bayes","mixed-script information retrieval"]},{"p_id":8406,"title":"Ask your neurons: A neural-based approach to answering questions about images","abstract":"\u00a9 2015 IEEE.We address a question answering task on real-world images that is set up as a Visual Turing Test. By combining latest advances in image representation and natural language processing, we propose Neural-Image-QA, an end-to-end formulation to this problem for which all parts are trained jointly. In contrast to previous efforts, we are facing a multi-modal problem where the language output (answer) is conditioned on visual and natural language input (image and question). Our approach Neural-Image-QA doubles the performance of the previous best approach on this problem. We provide additional insights into the problem by analyzing how much information is contained only in the language part for which we provide a new human baseline. To study human consensus, which is related to the ambiguities inherent in this challenging task, we propose two novel metrics and collect additional answers which extends the original DAQUAR dataset to DAQUAR-Consensus.","keywords_author":null,"keywords_other":["Multimodal problems","End to end","Image representations","Question Answering Task","Natural languages","Turing tests","NAtural language processing","Real-world image"],"max_cite":100.0,"pub_year":2015.0,"sources":"['ieee', 'scp', 'wos']","rawkeys":["multimodal problems","question answering task","end to end","real-world image","natural languages","natural language processing","turing tests","image representations"],"tags":["multimodal problems","question answering task","end to end","real-world image","natural languages","natural language processing","turing tests","image representation"]},{"p_id":2262,"title":"Conditional Random Fields in Speech, Audio, and Language Processing","abstract":"Conditional random fields (CRFs) are probabilistic sequence models that have been applied in the last decade to a number of applications in audio, speech, and language processing. In this paper, we provide a tutorial overview of CRF technologies, pointing to other resources for more in-depth discussion; in particular, we describe the common linear-chain model as well as a number of common extensions within the CRF family of models. An overview of the mathematical techniques used in training and evaluating these models is also provided, as well as a discussion of the relationships with other probabilistic models. Finally, we survey recent work in speech, audio, and language processing to show how the same CRF technology can be deployed in different scenarios.","keywords_author":["Automatic speech recognition (ASR)","natural language processing (NLP)","random fields","statistical learning"],"keywords_other":["probabilistic models","probability","audio signal processing","Natural language processing","conditional random fields","Random processes","probabilistic sequence models","Information processing","Statistical learning","speech processing","mathematical techniques","linear-chain model","Speech processing","Automatic speech recognition","random processes","language processing","audio processing","CRF technologies"],"max_cite":7.0,"pub_year":2013.0,"sources":"['ieee']","rawkeys":["crf technologies","audio signal processing","conditional random fields","random fields","automatic speech recognition","audio processing","language processing","statistical learning","random processes","probabilistic models","information processing","probabilistic sequence models","natural language processing (nlp)","mathematical techniques","probability","natural language processing","speech processing","automatic speech recognition (asr)","linear-chain model"],"tags":["crf technologies","probabilistic models","audio signal processing","information processing","probability","automatic speech recognition","probabilistic sequence models","conditional random field","natural language processing","speech processing","random forests","mathematical techniques","statistical learning","linear-chain model","random processes","language processing","audio processing"]},{"p_id":43225,"title":"Expected Likelihood: Not a Good Metric for Gaussian Embeddings","abstract":"\u00a9 2017 IEEE. The success of neural network methods in Natural Language Processing (NLP) tasks is fairly related to the widely use of word embeddings in which a word is represented as a point in a vector space. Previous research has shown that high quality word embeddings can improve the performance of NLP models. Since natural language is intrinsically ambiguous, different senses of a word should be in different positions of the vector space. So that representing a word as a single point in the vector space is problematic. Some recent work proposed to use Gaussian distributions to represent a word which is named Gaussian embeddings. This idea is very natural and proved to be useful in tasks like word similarity and word entailment. However, Gaussian embeddings are very difficult to train, especially for the commonly used loss function based on Expected Likelihood (EL). In this paper, we study the training process of Gaussian embeddings based on EL. Supported by our experimental results, we find that EL is not suitable for training Gaussian embeddings. Besides, we propose an effective algorithm to train Gaussian embeddings and improve the performance on word similarity tasks.","keywords_author":["Expected Likelihood","Gaussian Embedding","Machine Learning","Natural Language Processing","Word Embedding"],"keywords_other":["Expected likelihoods","Effective algorithms","Neural network method","Word embedding","Natural languages","Gaussians","Word similarity","Training process"],"max_cite":0.0,"pub_year":2018.0,"sources":"['scp']","rawkeys":["expected likelihood","expected likelihoods","effective algorithms","machine learning","natural language processing","natural languages","gaussians","gaussian embedding","word embedding","neural network method","word similarity","training process"],"tags":["expected likelihoods","effective algorithms","machine learning","natural language processing","natural languages","gaussians","gaussian embedding","word embedding","neural network method","word similarity","training process"]},{"p_id":45275,"title":"Using tools to assist identification of non-requirements in requirements specifications \u2013 A controlled experiment","abstract":"\u00a9 Springer International Publishing AG, part of Springer Nature 2018. [Context and motivation] In many companies, textual fragments in specification documents are categorized into requirements and non-requirements. This categorization is important for determining liability, deriving test cases, and many more decisions. In practice, this categorization is usually performed manually, which makes it labor-intensive and error-prone. [Question\/problem] We have developed a tool to assist users in this task by providing warnings based on classification using neural networks. However, we currently do not know whether using the tool actually helps increasing the classification quality compared to not using the tool. [Principal idea\/results] Therefore, we performed a controlled experiment with two groups of students. One group used the tool for a given task, whereas the other did not. By comparing the performance of both groups, we can assess in which scenarios the application of our tool is beneficial. [Contribution] The results show that the application of an automated classification approach may provide benefits, given that the accuracy is high enough.","keywords_author":["Convolutional neural networks","Machine learning","Natural language processing","Requirements engineering"],"keywords_other":["Error prones","Classification quality","Requirements specifications","Convolutional neural network","Controlled experiment","Test case","Labor intensive","Automated classification"],"max_cite":0.0,"pub_year":2018.0,"sources":"['scp']","rawkeys":["automated classification","convolutional neural networks","requirements engineering","error prones","machine learning","natural language processing","test case","labor intensive","convolutional neural network","requirements specifications","classification quality","controlled experiment"],"tags":["automated classification","requirements engineering","error prones","machine learning","natural language processing","test case","labor intensive","convolutional neural network","requirements specifications","classification quality","controlled experiment"]},{"p_id":37084,"title":"Prioritized active learning for malicious URL detection using weighted text-based features","abstract":"\u00a9 2017 IEEE.Data analytics is being increasingly used in cyber-security problems, and found to be useful in cases where data volumes and heterogeneity make it cumbersome for manual assessment by security experts.","keywords_author":["active learning","cyber-security","Machine learning","Malicious threat detection","natural language processing","Phishing","security analytics"],"keywords_other":["Threat detection","Phishing","Cyber security","Security Analytics","Active Learning"],"max_cite":1.0,"pub_year":2017.0,"sources":"['scp']","rawkeys":["cyber-security","threat detection","machine learning","active learning","natural language processing","malicious threat detection","cyber security","security analytics","phishing"],"tags":["threat detection","natural language processing","machine learning","malicious threat detection","cyber security","security analytics","phishing"]},{"p_id":39138,"title":"Assessment of the extent of the necessary clinical testing of new biotechnological products based on the analysis of scientific publications and clinical trials reports","abstract":"To estimate patients risks and make clinical decisions, evidence based medicine (EBM) relies upon the results of reproducible trials and experiments supported by accurate mathematical methods. Experimental and clinical evidence is crucial, but laboratory testing and especially clinical trials are expensive and time-consuming. On the other hand, a new medical product to be evaluated may be similar to one or many already tested. Results of the studies hitherto performed with similar products may be a useful tool to determine the extent of further pre-clinical and clinical testing. This paper suggests a workflow design aimed to support such an approach including methods for information collection, assessment of research reliability, extraction of structured information about trials and meta-analysis. Additionally, the paper contains a discussion of the issues emering during development of an integrated software system that implements the proposed workflow.","keywords_author":["Clinical trials","Information retrieval","Machine learning","Meta analysis","Natural language processing"],"keywords_other":["Meta analysis","Clinical trial","Scientific publications","Structured information","Information collections","Integrated software system","NAtural language processing","Evidence-based medicine"],"max_cite":1.0,"pub_year":2015.0,"sources":"['scp']","rawkeys":["clinical trials","evidence-based medicine","machine learning","information collections","information retrieval","integrated software system","natural language processing","clinical trial","meta analysis","scientific publications","structured information"],"tags":["evidence-based medicine","machine learning","information collections","information retrieval","integrated software system","natural language processing","clinical trial","metaanalysis","structural information","scientific publications"]},{"p_id":49380,"title":"Error leakage and wasted time: sensitivity and effort analysis of a requirements consistency checking process","abstract":"Copyright \u00ef\u00bf\u00bd 2016 John Wiley & Sons, Ltd.Several techniques are used by requirements engineering practitioners to address difficult problems such as specifying precise requirements while using inherently ambiguous natural language text and ensuring the consistency of requirements. Often, these problems are addressed by building processes\/tools that combine multiple techniques where the output from 1 technique becomes the input to the next. While powerful, these techniques are not without problems. Inherent errors in each technique may leak into the subsequent step of the process. We model and study 1 such process, for checking the consistency of temporal requirements, and assess error leakage and wasted time. We perform an analysis of the input factors of our model to determine the effect that sources of uncertainty may have on the final accuracy of the consistency checking process. Convinced that error leakage exists and negatively impacts the results of the overall consistency checking process, we perform a second simulation to assess its impact on the analysts' efforts to check requirements consistency. We show that analyst's effort varies depending on the precision and recall of the subprocesses and that the number and capability of analysts affect their effort. We share insights gained and discuss applicability to other processes built of piped techniques.","keywords_author":["classification","consistency checking","error leakage","error propagation","genetic algorithms","information retrieval","machine learning","natural language processing","process model","requirements engineering","search-based software engineering","semantic role labeling","sensitivity analysis","work flow process"],"keywords_other":["Error propagation","Consistency checking","Search-based software engineering","Process Modeling","Semantic role labeling","Work-flows","NAtural language processing"],"max_cite":0.0,"pub_year":2016.0,"sources":"['scp']","rawkeys":["sensitivity analysis","error propagation","consistency checking","requirements engineering","process model","machine learning","natural language processing","information retrieval","search-based software engineering","classification","genetic algorithms","semantic role labeling","work-flows","process modeling","error leakage","work flow process"],"tags":["sensitivity analysis","workflow","error propagation","consistency checking","requirements engineering","process model","machine learning","natural language processing","information retrieval","genetic algorithm","search-based software engineering","classification","semantic role labeling","error leakage","work flow process"]},{"p_id":49382,"title":"A literature survey on entity extraction techniques in bio-medical datasets","abstract":"\u00a9 2016, International Journal of Pharmacy and Technology. All rights reserved.The term \u201cEntity Extraction\u201d is a renowned technique now and broadly employed as a part of opinion mining and sentiment analysis. This technique is one of the information retrieval approaches, where the precise data have been obtained from the huge volume of information resources. Entity extraction is a traditional approach of named entity extraction, which is basically, retrieves the person name, product names, organization names, services, political issues titles, news article titles, locations and local and global time and etc. This entity extraction is very interesting filed and plays a major role in bio-medical field to recognize the drug entities, understand biological data in bioinformatics and categorize bio-medical documents, document summarization and explore the bio-medical terms and etc. Since the volume of online data has been increasing day by day, the entity extraction process needs more efficient techniques to find the entity replication, document replication, semantics between entities and documents and etc. This paper presents a study of various approaches employed in entity extraction on unstructured data.","keywords_author":["Data mining","Latent dirichlet allocation and latent semantic information","Machine learning","Named entity extraction","Natural language processing"],"keywords_other":null,"max_cite":0.0,"pub_year":2016.0,"sources":"['scp']","rawkeys":["data mining","named entity extraction","machine learning","natural language processing","latent dirichlet allocation and latent semantic information"],"tags":["data mining","named entity extraction","machine learning","natural language processing","latent dirichlet allocation and latent semantic information"]},{"p_id":24809,"title":"Extraction of emotions from multilingual text using intelligent text processing and computational linguistics","abstract":"\u00a9 2017 Elsevier B.V.Extraction of Emotions from Multilingual Text posted on social media by different categories of users is one of the crucial tasks in the field of opining mining and sentiment analysis. Every major event in the world has an online presence and social media. Users use social media platforms to express their sentiments and opinions towards it. In this paper, an advanced framework for detection of emotions of users in Multilanguage text data using emotion theories has been presented, which deals with linguistics and psychology. The emotion extraction system is developed based on multiple features groups for the better understanding of emotion lexicons. Empirical studies of three real-time events in domains like a Political election, healthcare, and sports are performed using proposed framework. The technique used for dynamic keywords collection is based on RSS (Rich Site Summary) feeds of headlines of news articles and trending hashtags from Twitter. An intelligent data collection model has been developed using dynamic keywords. Every word of emotion contained in a tweet is important in decision making and hence to retain the importance of multilingual emotional words, effective pre-processing technique has been used. Naive Bayes algorithm and Support Vector Machine (SVM) are used for fine-grained emotions classification of tweets. Experiments conducted on collected data sets, show that the proposed method performs better in comparison to corpus-driven approach which assign affective orientation or scores to words. The proposed emotion extraction framework performs better on the collected dataset by combining feature sets consisting of words from publicly available lexical resources. Furthermore, the presented work for extraction of emotion from tweets performs better in comparisons of other popular sentiment analysis techniques which are dependent of specific existing affect lexicons.","keywords_author":["Classification","Emotion extraction","Machine learning","Natural language processing","Text mining","Twitter"],"keywords_other":["Text mining","Twitter","Naive-Bayes algorithm","Intelligent text processing","Multilingual texts","Social media platforms","Emotion extractions","NAtural language processing"],"max_cite":8.0,"pub_year":2017.0,"sources":"['scp', 'wos']","rawkeys":["intelligent text processing","text mining","multilingual texts","emotion extractions","social media platforms","machine learning","natural language processing","emotion extraction","classification","naive-bayes algorithm","twitter"],"tags":["intelligent text processing","text mining","multilingual texts","social media platforms","machine learning","natural language processing","emotion extraction","classification","naive-bayes algorithm","twitter"]},{"p_id":49387,"title":"Opinion mining using an intuitive scoring approach","abstract":"\u00a9 2016, International Journal of Pharmacy and Technology. All rights reserved.Recommender systems help the customers in choosing the right products\/services by mining the opinion expressed in the user reviews. The volume of review documents available on the web is huge and it is a challenging task to read manually and summarize them. In this paper, an intuitive scoring approach is proposed for analyzing the overall opinion expressed in a review document. For the first time, an intuitive scoring (IS) approach based on the concept of hedges is used in combination with a new type of term-document matrix called as \u201crepresentative term-document matrix\u201d for summarizing the user reviews. The proposed classifier achieved a near 90% precision and accuracy of 80% for the dataset consisting of up to 538 reviews.","keywords_author":["Data mining","Document classification","Fuzzy set qualifiers and Hedges","Machine learning","Natural language processing","Opinion mining","Recommender systems","Sentiment analysis"],"keywords_other":null,"max_cite":0.0,"pub_year":2016.0,"sources":"['scp']","rawkeys":["fuzzy set qualifiers and hedges","data mining","natural language processing","machine learning","document classification","recommender systems","opinion mining","sentiment analysis"],"tags":["fuzzy set qualifiers and hedges","data mining","natural language processing","machine learning","document classification","recommender systems","opinion mining","sentiment analysis"]},{"p_id":45291,"title":"Extracting the population, intervention, comparison and sentiment from randomized controlled trials","abstract":"\u00a9 2018 European Federation for Medical Informatics (EFMI) and IOS Press.In this paper, an identification approach for the Population (e.g. patients with headache), the Intervention (e.g. aspirin) and the Comparison (e.g. vitamin C) in Randomized Controlled Trials (RCTs) is proposed. Contrary to previous approaches, the identification is done on a word level, rather than on a sentence level. Additionally, we classify the sentiment of RCTs to determine whether an Intervention is more effective than its Comparison. Two new corpora were created to evaluate both approaches. In the experiments, an average F1 score of 0.85 for the PIC identification and 0.72 for the sentiment classification was achieved.","keywords_author":["Information extraction","Machine learning","Natural language processing","Sentiment analysis"],"keywords_other":["Humans","Research Design","Randomized Controlled Trials as Topic","Data Mining"],"max_cite":0.0,"pub_year":2018.0,"sources":"['scp']","rawkeys":["data mining","information extraction","machine learning","natural language processing","humans","randomized controlled trials as topic","research design","sentiment analysis"],"tags":["data mining","information extraction","machine learning","natural language processing","humans","randomized controlled trials as topic","research design","sentiment analysis"]},{"p_id":20722,"title":"Multiparticipant chat analysis: A survey","abstract":"We survey research on the analysis of multiparticipant chat. Multiple research and applied communities (e.g., AI, educational, law enforcement, military) have interest in this topic. After introducing some context, we describe relevant problems and how these have been addressed using AI techniques. We also identify recent research trends and unresolved issues that could benefit from more attention. \u00a9 2013 Elsevier B.V.","keywords_author":["Artificial intelligence","Machine learning","Multiparticipant chat","Natural language processing","Text analysis"],"keywords_other":["Multiparticipant chat","Recent researches","Text analysis","Survey research","Multiple research","Chat analysis","AI techniques","NAtural language processing"],"max_cite":16.0,"pub_year":2013.0,"sources":"['scp']","rawkeys":["survey research","artificial intelligence","ai techniques","machine learning","natural language processing","chat analysis","multiparticipant chat","text analysis","multiple research","recent researches"],"tags":["survey research","ai techniques","machine learning","natural language processing","chat analysis","multiparticipant chat","text analysis","multiple research","recent researches"]},{"p_id":30962,"title":"TensorLayer: A versatile library for efficient deep learning development","abstract":"\u00a9 2017 Copyright held by the owner\/author(s).Recently we have observed emerging uses of deep learning techniques in multimedia systems. Developing a practical deep learning system is arduous and complex. It involves labor-intensive tasks for constructing sophisticated neural networks, coordinating multiple network models, and managing a large amount of training-related data. To facilitate such a development process, we propose TensorLayer which is a Python-based versatile deep learning library. TensorLayer provides high-level modules that abstract sophisticated operations towards neuron layers, network models, training data and dependent training jobs. In spite of offering simplicity, it has transparent module interfaces that allows developers to flexibly embed low-level controls within a backend engine, with the aim of supporting fine-grain tuning towards training. Real-world cluster experiment results show that TensorLayer is able to achieve competitive performance and scalability in critical deep learning tasks. TensorLayer was released in September 2016 on GitHub. Since after, it soon become one of the most popular open-sourced deep learning library used by researchers and practitioners.","keywords_author":["Computer vision","Data management","Deep learning","Natural language processing","Parallel computation","Reinforcement learning"],"keywords_other":["Multiple networks","Competitive performance","Low level control","Development process","Learning techniques","Parallel Computation","Module interfaces","Labor intensive"],"max_cite":3.0,"pub_year":2017.0,"sources":"['scp', 'wos']","rawkeys":["competitive performance","module interfaces","low level control","development process","deep learning","natural language processing","learning techniques","labor intensive","multiple networks","reinforcement learning","computer vision","data management","parallel computation"],"tags":["competitive performance","module interfaces","parallel computing","development process","low-level controllers","machine learning","natural language processing","learning techniques","labor intensive","multiple networks","reinforcement learning","computer vision","data management"]},{"p_id":41202,"title":"A target oriented agent to ollect specific information in a chat medium","abstract":"Internet and chat mediums provide important and quite useful information about human life in different societies such as their current interests, habits, social behaviors and criminal tendency. In this study, we have presented an intelligent identification system that is designed to identify the sex of a person in a Turkish chat medium. To do this task, a target oriented chat agent is implemented. A simple discrimination function is proposed for sex identification. The system also uses a semantic analysis method to determine the sex of the chatters. Here, the sex identification is taken as an example in the information extraction in chat mediums. This proposed identification system employs the agent and acquires data from a chat medium, and then automatically detects the chatter's sex from the information exchanged between the agent and chatter. The system has achieved accuracy about 90% in the sex identification in a real chat medium. \u00a9 Springer-Verlag Berlin Heidelberg 2006.","keywords_author":["Chat conversations","Intelligent agent","Machine learning","Natural language processing","Sex identification"],"keywords_other":["Sex identification","Chat conversations","Discrimination function","Intelligent identification system"],"max_cite":1.0,"pub_year":2006.0,"sources":"['scp']","rawkeys":["chat conversations","machine learning","natural language processing","intelligent agent","discrimination function","sex identification","intelligent identification system"],"tags":["chat conversations","machine learning","natural language processing","intelligent agents","discriminant functions","sex identification","intelligent identification system"]},{"p_id":37108,"title":"Automatic prediction of coronary artery disease from clinical narratives","abstract":"\u00a9 2017 Coronary Artery Disease (CAD) is not only the most common form of heart disease, but also the leading cause of death in both men and women (Coronary Artery Disease: MedlinePlus, 2015). We present a system that is able to automatically predict whether patients develop coronary artery disease based on their narrative medical histories, i.e., clinical free text. Although the free text in medical records has been used in several studies for identifying risk factors of coronary artery disease, to the best of our knowledge our work marks the first attempt at automatically predicting development of CAD. We tackle this task on a small corpus of diabetic patients. The size of this corpus makes it important to limit the number of features in order to avoid overfitting. We propose an ontology-guided approach to feature extraction, and compare it with two classic feature selection techniques. Our system achieves state-of-the-art performance of 77.4% F1 score.","keywords_author":["Coronary artery disease","Dimensionality reduction","Machine learning","Natural language processing","Ontology","Prediction"],"keywords_other":["State-of-the-art performance","Prognosis","Male","Diabetic patient","Dimensionality reduction","Coronary artery disease","Selection techniques","Forecasting","Automatic prediction","Humans","Natural Language Processing","Vocabulary, Controlled","Medical history","Narration","Medical record","Coronary Artery Disease","Female"],"max_cite":1.0,"pub_year":2017.0,"sources":"['scp']","rawkeys":["vocabulary","diabetic patient","medical history","dimensionality reduction","forecasting","machine learning","medical record","state-of-the-art performance","narration","humans","prognosis","automatic prediction","controlled","male","selection techniques","ontology","natural language processing","prediction","coronary artery disease","female"],"tags":["vocabulary","diabetic patient","medical history","dimensionality reduction","computer-aided diagnosis","forecasting","control","machine learning","medical record","state-of-the-art performance","narration","humans","prognosis","automatic prediction","male","selection techniques","prediction","natural language processing","female"]},{"p_id":51442,"title":"Two-phase reanalysis model for understanding user intention","abstract":"This paper proposes a two-phase reanalysis model for understanding user intention in utterances, by considering the correlative characteristics between the three attributes relating to user intention. The proposed model comprises two phases. In the first phase, each attribute is analyzed in the optimized sequence. The results of the analysis are then used as features that undergo reanalysis in the second phase, with the assumption that the relationship between the attributes is correlative. The experiments conducted showed that the proposed model improves user intention analysis over the baseline model, with an error reduction rate in Speech Act, Concept Sequence, and Arguments of 0.64%, 14.78%, and 5.84%, respectively. \u00a9 2014 Published by Elsevier B.V.","keywords_author":["Dialogue system","Machine learning","Natural language processing","User intention analysis"],"keywords_other":null,"max_cite":0.0,"pub_year":2014.0,"sources":"['scp']","rawkeys":["natural language processing","user intention analysis","machine learning","dialogue system"],"tags":["dialogue systems","natural language processing","machine learning","user intention analysis"]},{"p_id":35064,"title":"Classifying natural language sentences for policy","abstract":"Organizations derive policies from a wide variety of sources, such business plans, laws, regulations, and contracts. However, an efficient process does not yet exist for quickly finding or automatically deriving policies from uncontrolled natural language sources. The goal of our research is to assure compliance with established policies by ensuring policies in existing natural language texts are discovered, appropriately represented, and implemented. We propose a tool-based process to parse natural language documents, learn which statements signify policy, and then generate appropriate policy representations. To evaluate the initial work on our process, we analyze four data use agreements for a particular project and classify sentences as to whether or not they pertain to policy, requirements, or neither. Our k-nearest neighbor classifier with a unique distance metric had a precision of 0.82 and a recall of 0.81, outperforming weighted random guess, which had a precision of 0.44 and a recall of 0.46. The initial results demonstrate the feasibility of classifying sentences for policy and we plan to continue this work to derive policy elements from the natural language text. \u00a9 2012 IEEE.","keywords_author":["classification","data use agreements","machine learning","natural language processing","policy"],"keywords_other":["Distance metrics","Business plans","Natural language text","K-nearest neighbor classifier","Efficient process","Natural languages","NAtural language processing"],"max_cite":2.0,"pub_year":2012.0,"sources":"['scp']","rawkeys":["k-nearest neighbor classifier","data use agreements","business plans","natural languages","machine learning","natural language processing","distance metrics","classification","efficient process","policy","natural language text"],"tags":["data use agreements","business plans","natural languages","machine learning","natural language processing","distance metrics","classification","efficient process","policy","k-nearest neighbors","natural language text"]},{"p_id":53496,"title":"Research on Olympics-oriented mobile game news ordering system","abstract":"The Olympics-oriented mobile game news ordering system introduced in the paper involved the technology of natural language processing, machine learning and information extraction. We collected text information from the major Internet media websites about the table-tennis and badminton games of the 2004 Athens Olympics; then we transformed these texts information into structured data through information extraction. Based on this source data-base, we can provide game news to the end users in the form that they can read them on their mobile phones easily.","keywords_author":["Information extraction","Machine learning","Short message service (SMS)"],"keywords_other":["Short message services","Text information","Ordering system","End users","Internet media","Information Extraction","Table-tennis","Olympics","Structured data","NAtural language processing","Mobile games"],"max_cite":0.0,"pub_year":2006.0,"sources":"['scp']","rawkeys":["information extraction","short message services","text information","machine learning","natural language processing","short message service (sms)","table-tennis","structured data","end users","mobile games","ordering system","olympics","internet media"],"tags":["information extraction","short message services","text information","machine learning","natural language processing","table-tennis","structured data","end users","mobile games","ordering system","olympics","internet media"]},{"p_id":24828,"title":"Designing reconfigurable large-scale deep learning systems using stochastic computing","abstract":"\u00a9 2016 IEEE. Deep Learning, as an important branch of machine learning and neural network, is playing an increasingly important role in a number of fields like computer vision, natural language processing, etc. However, large-scale deep learning systems mainly operate in high-performance server clusters, thus restricting the application extensions to personal or mobile devices. The solution proposed in this paper is taking advantage of the fantastic features of stochastic computing methods. Stochastic computing is a type of data representation and processing technique, which uses a binary bit stream to represent a probability number (by counting the number of ones in this bit stream). In the stochastic computing area, some key arithmetic operations such as additions or multiplications can be implemented with very simple components like AND gates or multiplexers, respectively. Thus it provides an immense design space for integrating a large amount of neurons and enabling fully parallel and scalable hardware implementations of large-scale deep learning systems. In this paper, we present a reconfigurable large-scale deep learning system based on stochastic computing technologies, including the design of the neuron, the convolution function, the back-propagation function and some other basic operations. And the network-on-chip technique is also proposed in this paper to achieve the goal of implementing a large-scale hardware system. Our experiments validate the functionality of reconfigurable deep learning systems using stochastic computing, and demonstrate that when the bit streams are set to be 8192 bits, classification of MNIST digits by stochastic computing can perform as low error rate as that by normal arithmetic operations.","keywords_author":["deep learning","large-scale","neuron","reconfigurable","Stochastic computing"],"keywords_other":["Deep learning","Stochastic computing","Convolution functions","Reconfigurable","large-scale","Arithmetic operations","Processing technique","NAtural language processing"],"max_cite":8.0,"pub_year":2016.0,"sources":"['ieee', 'scp', 'wos']","rawkeys":["arithmetic operations","deep learning","neuron","natural language processing","large-scale","convolution functions","processing technique","stochastic computing","reconfigurable"],"tags":["arithmetic operations","natural language processing","machine learning","large-scale","convolution functions","processing technique","stochastic computing","neurons","reconfigurable"]},{"p_id":26879,"title":"Comparing machine learning classifiers for movie WOM opinion mining","abstract":"\u00a9 2015 KSII. Nowadays, online word-of-mouth has become a powerful influencer to marketing and sales in business. Opinion mining and sentiment analysis is frequently adopted at market research and business analytics field for analyzing word-of-mouth content. However, there still remain several challengeable areas for 1) sentiment analysis aiming for Korean word-of-mouth content in film market, 2) availability of machine learning models only using linguistic features, 3) effect of the size of the feature set. This study took a sample of 10,000 movie reviews which had posted extremely negative\/positive rating in a movie portal site, and conducted sentiment analysis with four machine learning algorithms: na\u00efve Bayesian, decision tree, neural network, and support vector machines. We found neural network and support vector machine produced better accuracy than na\u00efve Bayesian and decision tree on every size of the feature set. Besides, the performance of them was boosting with increasing of the feature set size.","keywords_author":["Film market","Machine learning","NLP","Opinion mining","WOM"],"keywords_other":["Online word of mouths","NLP","Machine learning models","Sentiment analysis","Business analytics","WOM","Linguistic features","Opinion mining"],"max_cite":6.0,"pub_year":2015.0,"sources":"['scp']","rawkeys":["film market","nlp","machine learning models","machine learning","linguistic features","wom","online word of mouths","business analytics","opinion mining","sentiment analysis"],"tags":["film market","machine learning models","machine learning","natural language processing","linguistic features","word of mouth","online word of mouths","business analytics","opinion mining","sentiment analysis"]},{"p_id":20736,"title":"A machine-learning approach to negation and speculation detection in clinical texts","abstract":"Detecting negative and speculative information is essential in most biomedical text-mining tasks where these language forms are used to express impressions, hypotheses, or explanations of experimental results. Our research is focused on developing a system based on machine-learning techniques that identifies negation and speculation signals and their scope in clinical texts. The proposed system works in two consecutive phases: first, a classifier decides whether each token in a sentence is a negation\/speculation signal or not. Then another classifier determines, at sentence level, the tokens which are affected by the signals previously identified. The system was trained and evaluated on the clinical texts of the BioScope corpus, a freely available resource consisting of medical and biological texts: full-length articles, scientific abstracts, and clinical reports. The results obtained by our system were compared with those of two different systems, one based on regular expressions and the other based on machine learning. Our system's results outperformed the results obtained by these two systems. In the signal detection task, the F-score value was 97.3% in negation and 94.9% in speculation. In the scope-finding task, a token was correctly classified if it had been properly identified as being inside or outside the scope of all the negation signals present in the sentence. Our proposal showed an F score of 93.2% in negation and 80.9% in speculation. Additionally, the percentage of correct scopes (those with all their tokens correctly classified) was evaluated obtaining F scores of 90.9% in negation and 71.9% in speculation. \u00a9 2012 ASIS&T.","keywords_author":["biomedical information","machine learning","natural language processing"],"keywords_other":["Text-mining","Machine-learning","Biomedical information","Regular expressions","Bioscopes","F-score","Sentence level","NAtural language processing","Machine learning techniques"],"max_cite":16.0,"pub_year":2012.0,"sources":"['scp']","rawkeys":["bioscopes","machine learning techniques","machine learning","biomedical information","natural language processing","f-score","text-mining","machine-learning","regular expressions","sentence level"],"tags":["bioscopes","text mining","machine learning techniques","machine learning","biomedical information","natural language processing","f-score","regular expressions","sentence level"]},{"p_id":260,"title":"Multilingual training of deep neural networks","abstract":"We investigate multilingual modeling in the context of a deep neural network (DNN) - hidden Markov model (HMM) hybrid, where the DNN outputs are used as the HMM state likelihoods. By viewing neural networks as a cascade of feature extractors followed by a logistic regression classifier, we hypothesise that the hidden layers, which act as feature extractors, will be transferable between languages. As a corollary, we propose that training the hidden layers on multiple languages makes them more suitable for such cross-lingual transfer. We experimentally confirm these hypotheses on the GlobalPhone corpus using seven languages from three different language families: Germanic, Romance, and Slavic. The experiments demonstrate substantial improvements over a monolingual DNN-HMM hybrid baseline, and hint at avenues of further exploration.","keywords_author":["deep learning","multilingual modeling","neural networks","Speech recognition","Speech recognition","deep learning","neural networks","multilingual modeling"],"keywords_other":["linguistics","speech recognition","regression analysis","Germanic language","multilingual modeling","Neural networks","Feature extractor","Deep learning","Logistic regression classifier","HMM state likelihoods","Feature extraction","hidden layers","GlobalPhone corpus","Speech","Multiple languages","Deep neural networks","hidden Markov models","Training","logistic regression classifier","hybrid DNN-HMM","hidden Markov model","multilingual training","Hidden Markov models","Slavic language","neural nets","pattern classification","deep neural network","natural language processing","cross-lingual transfer","Speech recognition","Cross-lingual","feature extractors","feature extraction","Multilingual trainings","Acoustics","Romance language","Hidden layers"],"max_cite":69.0,"pub_year":2013.0,"sources":"['ieee', 'scp', 'wos']","rawkeys":["linguistics","regression analysis","speech recognition","multilingual modeling","hidden markov model","acoustics","globalphone corpus","cross-lingual","multiple languages","hidden layers","speech","deep learning","neural networks","logistic regression classifier","training","multilingual training","slavic language","feature extractor","multilingual trainings","neural nets","pattern classification","hidden markov models","germanic language","deep neural network","hmm state likelihoods","deep neural networks","hybrid dnn-hmm","natural language processing","romance language","cross-lingual transfer","feature extraction","feature extractors"],"tags":["linguistics","regression analysis","speech recognition","multilingual modeling","convolutional neural network","acoustics","globalphone corpus","cross-lingual","multiple languages","hidden layers","machine learning","romance languages","speech","neural networks","logistic regression classifier","training","multilingual training","feature extractor","pattern classification","german language","hidden markov models","hmm state likelihoods","hybrid dnn-hmm","natural language processing","cross-lingual transfer","feature extraction","slavic languages"]},{"p_id":45317,"title":"Sentiment analysis of movie review using machine learning techniques","abstract":"\u00a9 2018 Authors. Today's online world was fully filled up with blogs, views, comments, posts through various websites and social-surfs. People were habituated with posting every incident into blogs, messed with comments like text and emotions, which are a mixed bag of sad, happy, worry, cry etc. Analysing such data was called as Sentimental Analysis. To analysis, these unordered data we use new emerged technology algorithms. Machine learning a transpire technology which is engaged with almost all the fields, where its algorithms are more powerful that give with better faultless results. In this paper, we are analyzing tweets based on movie reviews using the Multinomial Logistic Regression, Na\u00efve Bayes, and SVM algorithms to compare score value to show the best text analysis algorithm.","keywords_author":["Machine learning","Natural language processing","Opinion mining","Sentiment analysis","Twitter analysis"],"keywords_other":null,"max_cite":0.0,"pub_year":2018.0,"sources":"['scp']","rawkeys":["twitter analysis","natural language processing","machine learning","opinion mining","sentiment analysis"],"tags":["twitter analysis","natural language processing","machine learning","opinion mining","sentiment analysis"]},{"p_id":26890,"title":"Identifying Patients with Depression Using Free-text Clinical Documents","abstract":"\u00a9 2015 IMIA and IOS Press. About 1 in 10 adults are reported to exhibit clinical depression and the associated personal, societal, and economic costs are significant. In this study, we applied the MTERMS NLP system and machine learning classification algorithms to identify patients with depression using discharge summaries. Domain experts reviewed both the training and test cases, and classified these cases as depression with a high, intermediate, and low confidence. For depression cases with high confidence, all of the algorithms we tested performed similarly, with MTERMS' knowledge-based decision tree slightly better than the machine learning classifiers, achieving an F-measure of 89.6%. MTERMS also achieved the highest F-measure (70.6%) on intermediate confidence cases. The RIPPER rule learner was the best performing machine learning method, with an F-measure of 70.0%, and a higher precision but lower recall than MTERMS. The proposed NLP-based approach was able to identify a significant portion of the depression cases (about 20%) that were not on the coded diagnosis list.","keywords_author":["Depression","Machine Learning","Natural Language Processing","Text Classification"],"keywords_other":["Boston","Sensitivity and Specificity","Electronic Health Records","Humans","Diagnosis, Computer-Assisted","Reproducibility of Results","Natural Language Processing","Machine Learning","Data Mining","Depression","Decision Support Systems, Clinical"],"max_cite":6.0,"pub_year":2015.0,"sources":"['scp']","rawkeys":["diagnosis","text classification","data mining","computer-assisted","machine learning","electronic health records","humans","natural language processing","reproducibility of results","clinical","depression","boston","sensitivity and specificity","decision support systems"],"tags":["diagnosis","text classification","data mining","computer-assisted","machine learning","electronic health records","humans","natural language processing","reproducibility of results","clinical","depression","boston","sensitivity and specificity","decision support systems"]},{"p_id":102666,"title":"Behind the scenes: A medical natural language processing project","abstract":"Advancement of Artificial Intelligence (AI) capabilities in medicine can help address many pressing problems in healthcare. However, AI research endeavors in healthcare may not be clinically relevant, may have unrealistic expectations, or may not be explicit enough about their limitations. A diverse and well-functioning multi-disciplinary team (MDT) can help identify appropriate and achievable AI research agendas in healthcare, and advance medical AI technologies by developing AI algorithms as well as addressing the shortage of appropriately labeled datasets for machine learning. In this paper, our team of engineers, clinicians and machine learning experts share their experience and lessons learned from their two-year-long collaboration on a natural language processing (NLP) research project. We highlight specific challenges encountered in cross-disciplinary teamwork, dataset creation for NLP research, and expectation setting for current medical AI technologies.","keywords_author":["Artificial intelligence in medicine","Natural language processing","Machine learning","Text analytics","Multidisciplinary teamwork","Cross-disciplinary research","Translational research"],"keywords_other":["ARTIFICIAL-INTELLIGENCE","WATSON"],"max_cite":0.0,"pub_year":2018.0,"sources":"['wos']","rawkeys":["text analytics","machine learning","cross-disciplinary research","multidisciplinary teamwork","natural language processing","translational research","watson","artificial intelligence in medicine","artificial-intelligence"],"tags":["text analytics","natural language processing","machine learning","cross-disciplinary research","multidisciplinary teamwork","translational research","watson","artificial intelligence in medicine"]},{"p_id":269,"title":"Semi-supervised training of Deep Neural Networks","abstract":"In this paper we search for an optimal strategy for semi-supervised Deep Neural Network (DNN) training. We assume that a small part of the data is transcribed, while the majority of the data is untranscribed. We explore self-training strategies with data selection based on both the utterance-level and frame-level confidences. Further on, we study the interactions between semi-supervised frame-discriminative training and sequence-discriminative sMBR training. We found it beneficial to reduce the disproportion in amounts of transcribed and untranscribed data by including the transcribed data several times, as well as to do a frame-selection based on per-frame confidences derived from confusion in a lattice. For the experiments, we used the Limited language pack condition for the Surprise language task (Vietnamese) from the IARPA Babel program. The absolute Word Error Rate (WER) improvement for frame cross-entropy training is 2.2%, this corresponds to WER recovery of 36% when compared to the identical system, where the DNN is built on the fully transcribed data.","keywords_author":["semi-supervised training","self-training","deep network","DNN","Babel program"],"keywords_other":["WER recovery","Vietnamese language","IARPA Babel program","Surprise language task","Maximum likelihood decoding","frame-level confidence","disproportion reduction","frame cross-entropy training","transcribed data","Lattices","Speech","sequence-discriminative sMBR training","absolute WER improvement","learning (artificial intelligence)","Training data","optimal strategy","utterance-level confidence","Training","data selection","limited language pack condition","neural nets","semisupervised deep-neural network training","DNN training","Data models","untranscribed data","natural language processing","semisupervised frame-discriminative training","absolute word error rate improvement","entropy","Acoustics"],"max_cite":30.0,"pub_year":2013.0,"sources":"['ieee']","rawkeys":["surprise language task","frame-level confidence","disproportion reduction","acoustics","training data","frame cross-entropy training","transcribed data","absolute wer improvement","sequence-discriminative smbr training","dnn","speech","learning (artificial intelligence)","babel program","optimal strategy","utterance-level confidence","training","data selection","deep network","limited language pack condition","vietnamese language","neural nets","semisupervised deep-neural network training","dnn training","self-training","iarpa babel program","untranscribed data","natural language processing","semisupervised frame-discriminative training","wer recovery","maximum likelihood decoding","data models","absolute word error rate improvement","lattices","semi-supervised training","entropy"],"tags":["surprise language task","convolutional neural network","frame-level confidence","disproportion reduction","acoustics","training data","frame cross-entropy training","transcribed data","absolute wer improvement","machine learning","optimization strategy","sequence-discriminative smbr training","speech","babel program","neural networks","utterance-level confidence","training","data selection","limited language pack condition","vietnamese language","semisupervised deep-neural network training","dnn training","self-training","iarpa babel program","untranscribed data","natural language processing","semisupervised frame-discriminative training","wer recovery","maximum likelihood decoding","data models","absolute word error rate improvement","lattices","semi-supervised training","entropy","deep networks"]},{"p_id":45328,"title":"Standardization of Featureless Variables for Machine Learning Models Using Natural Language Processing","abstract":"\u00a9 Springer International Publishing AG, part of Springer Nature 2018. AI and machine learning are mathematical modeling methods for learning from data and producing intelligent models based on this learning. The data these models need to deal with, is normally a mixed of data type where both numerical (continuous) variables and categorical (non-numerical) data types. Most models in AI and machine learning accept only numerical data as their input and thus, standardization of mixed data into numerical data is a critical step when applying machine learning models. Having data in the standard shape and format that models require often a time consuming, nevertheless very significant step of the process.","keywords_author":["Machine learning","Mixed type variables","Natural Language Processing"],"keywords_other":["Data type","Mixed data","Machine learning models","Intelligent models","Mixed type","Numerical data","Critical steps"],"max_cite":0.0,"pub_year":2018.0,"sources":"['scp']","rawkeys":["mixed type","critical steps","data type","machine learning models","machine learning","natural language processing","mixed type variables","intelligent models","mixed data","numerical data"],"tags":["mixed type","critical steps","data type","machine learning models","machine learning","natural language processing","mixed type variables","intelligent models","mixed data","numerical data"]},{"p_id":35091,"title":"Recognizing emotions in short texts","abstract":"Affective Computing is one of the fields used by computer scientists to transfer the knowledge from psychology to the Human-Machine Interaction research field, while offering a better understanding on Human to Human Interaction. Since the classification problem is not typical, the difficulty is increased by the fuzziness of the data sets. Our paper proposes a method that aims at a better recognition rate of human emotions. Our model is based on the Self-Organizing Maps algorithm and it can be applied on short texts with a high degree of affective content. It is designed to be integrated into an Embodied Conversational Agent.","keywords_author":["Affective computing","Emotion detection","Machine learning","Natural language processing","Text mining"],"keywords_other":["Affective Computing","Text mining","Human emotion","NAtural language processing","Human machine interaction","Research fields","Data sets","Recognizing emotions","Recognition rates","Embodied conversational agent","Computer scientists","Emotion detection","Human interactions"],"max_cite":2.0,"pub_year":2012.0,"sources":"['scp']","rawkeys":["text mining","computer scientists","research fields","affective computing","embodied conversational agent","machine learning","natural language processing","human interactions","recognizing emotions","data sets","human emotion","recognition rates","human machine interaction","emotion detection"],"tags":["text mining","computer scientists","research fields","affective computing","embodied conversational agent","machine learning","natural language processing","human interactions","recognizing emotions","data sets","human emotion","human machine interface","recognition rates","emotion detection"]},{"p_id":24853,"title":"Simplifying deep neural networks for neuromorphic architectures","abstract":"\u00a9 2016 ACM.Deep learning using deep neural networks is taking machine intelligence to the next level in computer vision, speech recognition, natural language processing, etc. Brain-like hardware platforms for the brain-inspired computational models are being studied, but none of such platforms deals with the huge size of practical deep neural networks. This paper presents two techniques, factorization and pruning, that not only compress the models but also maintain the form of the models for the execution on neuromorphic architectures. We also propose a novel method to combine the two techniques. The proposed method shows significant improvements in reducing the number of model parameters over standalone use of each method while maintaining the performance. Our experimental results show that the proposed method can achieve 31\u00d7 reduction rate without loss of accuracy for the largest layer of AlexNet.","keywords_author":["Deep learning","Deep neural network","Neuromorphic computing","Sparse network"],"keywords_other":["Deep learning","Neuromorphic Architectures","Computational model","Sparse network","Neuromorphic computing","Machine intelligence","Deep neural networks","NAtural language processing"],"max_cite":8.0,"pub_year":2016.0,"sources":"['scp', 'ieee']","rawkeys":["machine intelligence","deep neural network","sparse network","deep learning","deep neural networks","natural language processing","neuromorphic computing","computational model","neuromorphic architectures"],"tags":["computational modeling","machine intelligence","sparse network","natural language processing","machine learning","convolutional neural network","neuromorphic computing","neuromorphic architectures"]},{"p_id":35100,"title":"Fuzzy ILP Classification of web reports after linguistic text mining","abstract":"In this paper we study the problem of classification of textual web reports. We are specifically focused on situations in which structured information extracted from the reports is used for classification. We present an experimental classification system based on usage of third party linguistic analyzers, our previous work on web information extraction, and fuzzy inductive logic programming (fuzzy ILP). A detailed study of the so-called 'Fuzzy ILP Classifier' is the main contribution of the paper. The study includes formal models, prototype implementation, extensive evaluation experiments and comparison of the classifier with other alternatives like decision trees, support vector machines, neural networks, etc. \u00a9 2011 Elsevier Ltd. All rights reserved.","keywords_author":["Fuzzy","Inductive logic programming","Information extraction","Machine learning","Natural language processing"],"keywords_other":["Inductive logic","Machine-learning","Information Extraction","Fuzzy","NAtural language processing"],"max_cite":2.0,"pub_year":2012.0,"sources":"['scp']","rawkeys":["information extraction","inductive logic","machine learning","natural language processing","fuzzy","machine-learning","inductive logic programming"],"tags":["information extraction","inductive logic","machine learning","natural language processing","fuzzy","inductive logic programming"]},{"p_id":31005,"title":"Using linguistic features to automatically extract web page title","abstract":"\u00a9 2017 Elsevier Ltd Existing methods for extracting titles from HTML web page mostly rely on visual and structural features. However, this approach fails in the case of service-based web pages because advertisements are often given more visual emphasize than the main headlines. To improve the current state-of-the-art, we propose a novel method that combines statistical features, linguistic knowledge, and text segmentation. Using annotated English corpus, we learn the morphosyntactic characteristics of known titles and define a part-of-speech tag patterns that help to extract candidate phrases from the web page. To evaluate the proposed method, we compared two datasets Titler and Mopsi and evaluated the extracted features using four classifiers: Na\u00efve Bayes, k-NN, SVM, and clustering. Experimental results show that the proposed method outperform the solution used by Google from 0.58 to 0.85 on Titler corpus and from 0.43 to 0.55 on Mopsi dataset, and offers a readily available solution for the title extraction problem.","keywords_author":["Information extraction","Machine learning","Natural language processing","Title extraction","Web content mining"],"keywords_other":["Statistical features","Text segmentation","Title extractions","Structural feature","Linguistic knowledge","Web content mining","Linguistic features","NAtural language processing"],"max_cite":3.0,"pub_year":2017.0,"sources":"['scp']","rawkeys":["structural feature","information extraction","statistical features","text segmentation","web content mining","natural language processing","machine learning","title extractions","linguistic features","title extraction","linguistic knowledge"],"tags":["structural feature","information extraction","statistical features","text segmentation","web content mining","natural language processing","machine learning","linguistic features","title extraction","linguistic knowledge"]},{"p_id":51486,"title":"One system to solve them All","abstract":"\u00a9 NLP Consulting 2014.People are daily confronted with hundreds of situations in which they could use the knowledge of stylometry. In this paper, I propose a universal system to solve these situations using stylometry features, machine learning techniques and nature language processing tools. The proposed tool can help translation companies to recognize machine translation falsely submitted as a work of a human expert; identify school essays not written by the underwritten student; or cluster product reviews by authors and merge user reviews written by one author using multiple accounts. All examples above use same techniques and procedures to solve the problem, therefore it is preferred to merge algorithms and implementation of these tasks to a single framework.","keywords_author":["Machine learning","Stylometry"],"keywords_other":["Human expert","Nature language processing","User reviews","Machine translations","Stylometry","Product reviews","Machine learning techniques"],"max_cite":0.0,"pub_year":2014.0,"sources":"['scp']","rawkeys":["user reviews","machine learning techniques","machine learning","nature language processing","machine translations","human expert","stylometry","product reviews"],"tags":["user reviews","machine learning techniques","natural language processing","machine learning","machine translations","human expert","stylometry","product reviews"]},{"p_id":24864,"title":"Opinion mining from student feedback data using supervised learning algorithms","abstract":"\u00a9 2016 IEEE.This paper explores opinion mining using supervised learning algorithms to find the polarity of the student feedback based on pre-defined features of teaching and learning. The study conducted involves the application of a combination of machine learning and natural language processing techniques on student feedback data gathered from module evaluation survey results of Middle East College, Oman. In addition to providing a step by step explanation of the process of implementation of opinion mining from student comments using the open source data analytics tool Rapid Miner, this paper also presents a comparative performance study of the algorithms like SVM, Na\u00efve Bayes, K Nearest Neighbor and Neural Network classifier. The data set extracted from the survey is subjected to data preprocessing which is then used to train the algorithms for binomial classification. The trained models are also capable of predicting the polarity of the student comments based on extracted features like examination, teaching etc. The results are compared to find the better performance with respect to various evaluation criteria for the different algorithms.","keywords_author":["Learning Analytics","Machine Learning","Natural Language Processing","Opinion mining","Rapid Miner","Sentiment analysis","Supervised learning","Text Analytics"],"keywords_other":["Sentiment analysis","Learning Analytics","Text analytics","NAtural language processing","Opinion mining"],"max_cite":8.0,"pub_year":2016.0,"sources":"['scp']","rawkeys":["supervised learning","text analytics","rapid miner","learning analytics","natural language processing","machine learning","opinion mining","sentiment analysis"],"tags":["supervised learning","text analytics","rapid miner","learning analytics","natural language processing","machine learning","opinion mining","sentiment analysis"]},{"p_id":35104,"title":"Implicit group membership detection in online text: analysis and applications","abstract":"Our thesis is that members of the same group have shared tendencies and nuances in communication style and substance, particularly online. In this paper, we dicuss some potential applications of accuarate authorship affiliation technology. We also discuss related work in similar author identification efforts and the research issues that currently exist when trying to perform automated authorship affiliation. We provide quantitative results from our recent Machine Learning experimenation using Support Vector Machines as some initial validation of our theory. In this paper, we applied our work towards the task of classifying website forum posts by the affiliation of their author. We discuss in detail the stylometric features we used to perform the automated classification and split the original features into individual groups to isolate their respective contributions and\/or discriminating capability. Our results show promise towards automating group representation, an important first step in studying group formation. \u00a9 2012 Springer-Verlag.","keywords_author":["Authorship affiliation","Deception detection","Group detection","Group Membership","Machine Learning","Natural Language Processing","Stylometrics","Text classification"],"keywords_other":["Text classification","Machine-learning","Group detection","Deception detection","Authorship affiliation","Group memberships","Stylometrics","NAtural language processing"],"max_cite":2.0,"pub_year":2012.0,"sources":"['scp']","rawkeys":["authorship affiliation","group detection","stylometrics","machine learning","natural language processing","deception detection","machine-learning","group memberships","text classification","group membership"],"tags":["authorship affiliation","group detection","stylometrics","machine learning","natural language processing","deception detection","group memberships","text classification"]},{"p_id":291,"title":"A high-performance Cantonese keyword search system","abstract":"We present a system for keyword search on Cantonese conversational telephony audio, collected for the IARPA Babel program, that achieves good performance by combining postings lists produced by diverse speech recognition systems from three different research groups. We describe the keyword search task, the data on which the work was done, four different speech recognition systems, and our approach to system combination for keyword search. We show that the combination of four systems outperforms the best single system by 7%, achieving an actual term-weighted value of 0.517.","keywords_author":["deep learning","keyword search","spoken term detection","system combination","keyword search","spoken term detection","system combination","deep learning"],"keywords_other":["Speech recognition systems","IARPA Babel program","speech recognition","term-weighted value","telephony","Deep learning","Research groups","diverse speech recognition","Speech","Cantonese","System combination","Decoding","Cantonese conversational telephony audio","Training","Indexes","Keyword search","Cantonese keyword search system","natural language processing","Speech recognition","Acoustics","Spoken term detections"],"max_cite":39.0,"pub_year":2013.0,"sources":"['scp', 'ieee']","rawkeys":["speech recognition","cantonese keyword search system","term-weighted value","telephony","acoustics","spoken term detections","cantonese","diverse speech recognition","research groups","speech","deep learning","indexes","spoken term detection","training","cantonese conversational telephony audio","speech recognition systems","iarpa babel program","natural language processing","system combination","keyword search","decoding"],"tags":["speech recognition","cantonese keyword search system","term-weighted value","telephony","acoustics","cantonese","diverse speech recognition","machine learning","index","research groups","speech","training","spoken term detection","cantonese conversational telephony audio","speech recognition systems","iarpa babel program","natural language processing","system combination","keyword search","decoding"]},{"p_id":26915,"title":"Structural information aware deep semi-supervised recurrent neural network for sentiment analysis","abstract":"\u00a9 2014, Higher Education Press and Springer-Verlag Berlin Heidelberg.With the development of Internet, people are more likely to post and propagate opinions online. Sentiment analysis is then becoming an important challenge to understand the polarity beneath these comments. Currently a lot of approaches from natural language processing\u2019s perspective have been employed to conduct this task. The widely used ones include bag-of-words and semantic oriented analysis methods. In this research, we further investigate the structural information among words, phrases and sentences within the comments to conduct the sentiment analysis. The idea is inspired by the fact that the structural information is playing important role in identifying the overall statement\u2019s polarity. As a result a novel sentiment analysis model is proposed based on recurrent neural network, which takes the partial document as input and then the next parts to predict the sentiment label distribution rather than the next word. The proposed method learns words representation simultaneously the sentiment distribution. Experimental studies have been conducted on commonly used datasets and the results have shown its promising potential.","keywords_author":["deep learning","machine learning","recurrent neural network","sentiment analysis","sentiment analysis","recurrent neural network","deep learning","machine learning"],"keywords_other":["Deep learning","LANGUAGE","Semi-supervised","Structural information","ALGORITHM","Sentiment analysis","CLASSIFICATION","TIME","Label distribution","Bag of words","Analysis method","NAtural language processing"],"max_cite":6.0,"pub_year":2015.0,"sources":"['ieee', 'scp', 'wos']","rawkeys":["algorithm","bag of words","deep learning","semi-supervised","machine learning","natural language processing","label distribution","recurrent neural network","time","structural information","classification","analysis method","language","sentiment analysis"],"tags":["bag of words","neural networks","semi-supervised","machine learning","natural language processing","label distribution","time","structural information","classification","algorithms","analysis method","language","sentiment analysis"]},{"p_id":41255,"title":"Studies on traditional chinese poetry style identification","abstract":"Based on a Machine Learning method, Na\u00efve Bayes, this paper proposes a Traditional Chinese Poetry Style Identification Calculation Model to distinguish Bold-and-Unrestrained or Graceful-and-Restrained styles, which derives from Machine Learning Chinese Classical Ci in Song Dynasty and has achieved satisfactory identification results in application.","keywords_author":["Literature Style Identification","Machine Learning","Natural Language Processing","Text Categorization"],"keywords_other":["Literature style identification","Traditional chinese poetry","Machine learning method","Text categorization"],"max_cite":1.0,"pub_year":2004.0,"sources":"['scp']","rawkeys":["machine learning method","traditional chinese poetry","natural language processing","machine learning","literature style identification","text categorization"],"tags":["machine learning methods","traditional chinese poetry","natural language processing","machine learning","literature style identification","text categorization"]},{"p_id":43303,"title":"Co-occurrence graphs for word sense disambiguation in the biomedical domain","abstract":"\u00a9 2018 Elsevier B.V. Word sense disambiguation is a key step for many natural language processing tasks (e.g. summarization, text classification, relation extraction) and presents a challenge to any system that aims to process documents from the biomedical domain. In this paper, we present a new graph-based unsupervised technique to address this problem. The knowledge base used in this work is a graph built with co-occurrence information from medical concepts found in scientific abstracts, and hence adapted to the specific domain. Unlike other unsupervised approaches based on static graphs such as UMLS, in this work the knowledge base takes the context of the ambiguous terms into account. Abstracts downloaded from PubMed are used for building the graph and disambiguation is performed using the personalized PageRank algorithm. Evaluation is carried out over two test datasets widely explored in the literature. Different parameters of the system are also evaluated to test robustness and scalability. Results show that the system is able to outperform state-of-the-art knowledge-based systems, obtaining more than 10% of accuracy improvement in some cases, while only requiring minimal external resources.","keywords_author":["Graph-based systems","Information extraction","Natural language processing","Unified medical language system","Unsupervised machine learning","Word sense disambiguation"],"keywords_other":["Unified medical language systems","Graph-based","Co-occurrence informations","Personalized PageRank","Unsupervised approaches","Word Sense Disambiguation","Unsupervised machine learning","Unsupervised techniques"],"max_cite":0.0,"pub_year":2018.0,"sources":"['scp']","rawkeys":["unsupervised machine learning","unified medical language systems","unified medical language system","co-occurrence informations","graph-based systems","unsupervised approaches","information extraction","graph-based","natural language processing","personalized pagerank","word sense disambiguation","unsupervised techniques"],"tags":["unsupervised machine learning","unified medical language systems","unsupervised approaches","co-occurrence informations","graph-based systems","information extraction","graph-based","natural language processing","personalized pagerank","word sense disambiguation","unsupervised techniques"]},{"p_id":51497,"title":"Opinion search in spanish written press","abstract":"\u00a9 Springer International Publishing Switzerland 2014. We describe a press reading tool that focuses on sayings or opinions in the news about topics and sources of the reader\u2019s choice. This tool offers a graphical interface through which several sources can be linked given a certain topic and which allows readers to visualize opinions in a timeline and browse through them. This is done over a corpus that includes three Uruguayan written press media and integrates their current and previous editions. The opinion recognizer is mainly rule-based, with rules written in a formalism called Contextual Rules and machine learning methods and regular expressions rules were used during the different stages of the system. The accuracy of the system was evaluated with respect to the information retrieved, showing 76% precision.","keywords_author":["Data mining","Information retrieval","Machine learning","Natural language processing","Online newspapers","Opinion search","Opinions extraction"],"keywords_other":["Contextual rules","Graphical interface","Regular expressions","Online newspaper","Opinion searches","Different stages","Machine learning methods","NAtural language processing"],"max_cite":0.0,"pub_year":2014.0,"sources":"['scp']","rawkeys":["machine learning methods","data mining","opinion searches","different stages","opinion search","opinions extraction","contextual rules","machine learning","natural language processing","information retrieval","online newspaper","graphical interface","online newspapers","regular expressions"],"tags":["machine learning methods","data mining","opinion searches","different stages","contextual rules","machine learning","natural language processing","information retrieval","online newspaper","graphical interface","opinion extraction","regular expressions"]},{"p_id":35120,"title":"A machine learning technique for semantic search engine","abstract":"In this world of information technology revolution we human wants a way of automation in every field, but the intelligent technique behind the way how we visualize an information is not equivalent to how an computer system visualize .In web information content is simply a resource with some specific hyper-link path. In this paper we propose an idea of giving semantic to a web page so a system can understand the semantic behind the web page which automatically increases the efficiency of information search. So we represent a way of converting an ordinary Syntactic page into a Semantic web page with corresponding Ontology which would pave the way of advancement in Semantic Web Learning technology. \u00a9 2012 Published by Elsevier Ltd.","keywords_author":["Enity relationship","Machine learning","NLP","RDFS & OWL","Semantic web"],"keywords_other":null,"max_cite":2.0,"pub_year":2012.0,"sources":"['scp']","rawkeys":["nlp","rdfs & owl","machine learning","enity relationship","semantic web"],"tags":["rdfs & owl","natural language processing","machine learning","enity relationship","semantic web"]},{"p_id":51505,"title":"Translation between English and Mauritian Creole: A statistical machine translation approach","abstract":"Translation between one language and another is extensively carried out by students, tourists and business persons. The importance of translation cannot be underestimated, especially with the number of translation devices in use by travellers. Google Translate has set up a multilingual translation system and it is regularly used by people all over the world. In Mauritius, translation of the Mauritian Creole language is not available from Google Translate as the available size of the parallel corpora is quite small. The objective of this study is, therefore, to develop a web-based system for the translation between English language and the Mauritian Creole language for learning the two languages. This project on machine translation for the Mauritian Creole language is a starting point that will benefit the Mauritian population at large as well as the Tourism and Business sectors. Statistical Machine Translation, which is the state of-the-art technique, has been adopted for translation between English and Mauritian Creole language. In this paper, the translation between English and Mauritian Creole language is explored. \u00a9 2014 IIMC.","keywords_author":["Artificial Intelligence","Machine learning","Mauritian Creole","Natural language processing","Statistical machine translation","Technology-enhanced learning"],"keywords_other":["Statistical machine translation","English languages","Google translate","Technology enhanced learning","Multilingual translations","Parallel corpora","Mauritian Creole","NAtural language processing"],"max_cite":0.0,"pub_year":2014.0,"sources":"['scp']","rawkeys":["artificial intelligence","parallel corpora","technology enhanced learning","mauritian creole","machine learning","natural language processing","google translate","multilingual translations","english languages","technology-enhanced learning","statistical machine translation"],"tags":["parallel corpora","technology enhanced learning","mauritian creole","machine learning","natural language processing","google translate","multilingual translations","english languages","statistical machine translation"]},{"p_id":306,"title":"Deep Sentence embedding using long short-term memory networks: Analysis and application to information retrieval","abstract":"This paper develops a model that addresses sentence embedding, a hot topic in current natural language processing research, using recurrent neural networks (RNN) with Long Short-Term Memory (LSTM) cells. The proposed LSTM-RNN model sequentially takes each word in a sentence, extracts its information, and embeds it into a semantic vector. Due to its ability to capture long term memory, the LSTM-RNN accumulates increasingly richer information as it goes through the sentence, and when it reaches the last word, the hidden layer of the network provides a semantic representation of the whole sentence. In this paper, the LSTM-RNN is trained in a weakly supervised manner on user click-through data logged by a commercial web search engine. Visualization and analysis are performed to understand how the embedding process works. The model is found to automatically attenuate the unimportant words and detect the salient keywords in the sentence. Furthermore, these detected keywords are found to automatically activate different cells of the LSTM-RNN, where words belonging to a similar topic activate the same cell. As a semantic representation of the sentence, the embedding vector can be used in many different applications. These automatic keyword detection and topic allocation abilities enabled by the LSTM-RNN allow the network to perform document retrieval, a difficult language processing task, where the similarity between the query and documents can be measured by the distance between their corresponding sentence embedding vectors computed by the LSTM-RNN. On a web search task, the LSTM-RNN embedding is shown to significantly outperform several existing state of the art methods. We emphasize that the proposed model generates sentence embedding vectors that are specially useful for web document retrieval tasks. A comparison with a well known general sentence embedding method, the Paragraph Vector, is performed. The results show that the proposed method in this paper significantly outperforms Paragraph Vector method for web document retrieval task.","keywords_author":["Deep learning","Long short-term memory","Sentence embedding","Deep learning","long short-term memory","sentence embedding","Deep Learning","Long Short-Term Memory","Sentence Embedding"],"keywords_other":["Semantic representation","Web document retrieval tasks","recurrent neural networks","Web search","document handling","Speech processing","Visualization and analysis","deep sentence embedding","Deep learning","recurrent neural nets","Recurrent neural network (RNN)","semantic representation","Speech","weakly supervised manner","Long short term memory","SPEECH RECOGNITION","LSTM-RNN model","State-of-the-art methods","natural language processing research","long short-term memory","information retrieval","Sentence embedding","commercial Web search engine","NAtural language processing","Recurrent neural networks","Semantics","natural language processing","IEEE transactions","sentence embedding","automatic keyword detection","Data visualization","search engines","long short-term memory networks"],"max_cite":70.0,"pub_year":2016.0,"sources":"['scp', 'wos', 'ieee']","rawkeys":["state-of-the-art methods","speech recognition","recurrent neural networks","document handling","ieee transactions","web search","deep sentence embedding","recurrent neural nets","semantic representation","lstm-rnn model","weakly supervised manner","speech","natural language processing research","deep learning","semantics","long short-term memory","data visualization","information retrieval","web document retrieval tasks","recurrent neural network (rnn)","commercial web search engine","natural language processing","speech processing","sentence embedding","automatic keyword detection","search engines","long short-term memory networks","visualization and analysis","long short term memory"],"tags":["state-of-the-art methods","speech recognition","document handling","ieee transactions","deep sentence embedding","recurrent neural nets","machine learning","semantic representation","lstm-rnn model","weakly supervised manner","speech","natural language processing research","neural networks","semantics","long short-term memory","data visualization","information retrieval","web document retrieval tasks","search engine","commercial web search engine","natural language processing","speech processing","sentence embedding","automatic keyword detection","web searches","visualization and analysis"]},{"p_id":53564,"title":"Machine learning for automatic acquisition of Chinese linguistic ontology knowledge","abstract":"Due to the complexity and flexibility of natural language, automatic linguistic knowledge acquisition and its application research becomes difficult. In this paper, we present a machine learning method to automatically acquire Chinese linguistic ontology knowledge from typical corpus. This study, first, defined the description frame of Chinese linguistic ontology knowledge, and then, automatically acquired the usage of a Chinese word with its co-occurrence of context in using semantic, pragmatics, syntactic, etc from the corpus, final, the above information and their representation will acted as Chinese linguistic ontology knowledge bank. We completed two groups of experiments, i.e. documents similarity computing, text reordering for information retrieval. Compared with previous works, the proposed method solves the inferior Precision of nature language processing. \u00a92005 IEEE.","keywords_author":["Knowledge acquisition","Linguistic ontology knowledge","Machine learning","Natural language processing"],"keywords_other":["Ontology","Linguistic ontology knowledge"],"max_cite":0.0,"pub_year":2005.0,"sources":"['scp']","rawkeys":["knowledge acquisition","ontology","natural language processing","machine learning","linguistic ontology knowledge"],"tags":["natural language processing","linguistic ontology knowledge","knowledge acquisition","machine learning"]},{"p_id":35137,"title":"A comparison of classifiers for detecting hedges","abstract":"A hedge is a linguistic device used to avoid using a categorical sentence. Hedges can be used to determine whether a sentence is factual by merely regarding a sentence containing hedges as non-factual. In this paper, we perform a comparative experiment of various classification methods for hedge detection. Among four different classification methods, we observe that SVM shows the best performance and that the SVM-based method finally outperforms the best system in the CoNLL2010-ST task. \u00a9 2011 Springer-Verlag.","keywords_author":["Hedge Detection","Information Extraction","Machine Learning","Natural Language Processing"],"keywords_other":["Classification methods","Linguistic devices","Information Extraction","Comparative experiments","SVM-based methods","Comparison of classifiers","NAtural language processing"],"max_cite":2.0,"pub_year":2011.0,"sources":"['scp']","rawkeys":["comparative experiments","hedge detection","classification methods","information extraction","machine learning","natural language processing","linguistic devices","svm-based methods","comparison of classifiers"],"tags":["comparative experiments","hedge detection","classification methods","information extraction","machine learning","natural language processing","linguistic devices","svm-based methods","comparison of classifiers"]},{"p_id":45379,"title":"Question categorization and classification using grammar based approach","abstract":"\u00a9 2018 Elsevier Ltd Question-answering has become one of the most popular information retrieval applications. Despite that most question-answering systems try to improve the user experience and the technology used in finding relevant results, many difficulties are still faced because of the continuous increase in the amount of web content. Questions Classification (QC) plays an important role in question-answering systems, with one of the major tasks in the enhancement of the classification process being the identification of questions types. A broad range of QC approaches has been proposed with the aim of helping to find a solution for the classification problems; most of these are approaches based on bag-of-words or dictionaries. In this research, we present an analysis of the different type of questions based on their grammatical structure. We identify different patterns and use machine learning algorithms to classify them. A framework is proposed for question classification using a grammar-based approach (GQCC) which exploits the structure of the questions. Our findings indicate that using syntactic categories related to different domain-specific types of Common Nouns, Numeral Numbers and Proper Nouns enable the machine learning algorithms to better differentiate between different question types. The paper presents a wide range of experiments the results show that the GQCC using J48 classifier has outperformed other classification methods with 90.1% accuracy.","keywords_author":["Machine learning","Natural language processing (NLP)","Question classification","Text classification","Text mining"],"keywords_other":["Text mining","Grammar based approach","Classification methods","Question classification","Text classification","Classification process","Question answering systems","Retrieval applications"],"max_cite":0.0,"pub_year":2018.0,"sources":"['scp']","rawkeys":["grammar based approach","text mining","classification methods","machine learning","natural language processing (nlp)","question answering systems","question classification","retrieval applications","classification process","text classification"],"tags":["grammar based approach","text mining","classification methods","natural language processing","machine learning","question answering systems","question classification","retrieval applications","classification process","text classification"]},{"p_id":49477,"title":"Is sentiment analysis an art or a science? Impact of lexical richness in training corpus on machine learning","abstract":"\u00a9 2016 IEEE. Social Media is exploding with data - that can help you derive an optimal marketing strategy in the internet world, engage with your audience on the fly, and protect your reputation from smearing campaigns if it is processed and analyzed in a timely fashion. Digital marketing analysts and data scientists rely on social media analytics tools to deduce customer sentiment from countless opinions and reviews. While numerous attempts have been made to improve their accuracy in the past, yet we know surprisingly little about how accurate their results are. We present an unbiased study of users' tweets and the methods that leverage the available tools & technologies for opinion mining. Our prime focus is on improving the consistency of text classifiers used for linguistic analysis. We also measure the impact of lexical richness in the sample data on the trained algorithm. This paper attempts to improve the reliability of sentiment classification process by the creation of a custom vote classifier using natural language processing techniques and various machine learning algorithms.","keywords_author":["lexical richness","machine learning","natural language processing","opinion mining","sentiment classification","social media"],"keywords_other":["NAtural language processing","lexical richness","Social media","Sentiment classification","Opinion mining"],"max_cite":0.0,"pub_year":2016.0,"sources":"['scp']","rawkeys":["sentiment classification","lexical richness","social media","natural language processing","machine learning","opinion mining"],"tags":["sentiment classification","lexical richness","social media","natural language processing","machine learning","opinion mining"]},{"p_id":8518,"title":"Pharmacovigilance from social media: Mining adverse drug reaction mentions using sequence labeling with word embedding cluster features","abstract":"\u00a9 The Author 2015.Objective Social media is becoming increasingly popular as a platform for sharing personal health-related information. This information can be utilized for public health monitoring tasks, particularly for pharmacovigilance, via the use of natural language processing (NLP) techniques. However, the language in social media is highly informal, and userexpressed medical concepts are often nontechnical, descriptive, and challenging to extract. There has been limited progress in addressing these challenges, and thus far, advanced machine learning-based NLP techniques have been underutilized. Our objective is to design a machine learning-based approach to extract mentions of adverse drug reactions (ADRs) from highly informal text in social media. Methods: We introduce ADRMine, a machine learning-based concept extraction system that uses conditional random fields (CRFs). ADRMine utilizes a variety of features, including a novel feature for modeling words' semantic similarities. The similarities are modeled by clustering words based on unsupervised, pretrained word representation vectors (embeddings) generated from unlabeled user posts in social media using a deep learning technique. Results: ADRMine outperforms several strong baseline systems in the ADR extraction task by achieving an F-measure of 0.82. Feature analysis demonstrates that the proposed word cluster features significantly improve extraction performance. Conclusion: It is possible to extract complex medical concepts, with relatively high performance, from informal, usergenerated content. Our approach is particularly scalable, suitable for social media mining, as it relies on large volumes of unlabeled data, thus diminishing the need for large, annotated training data sets.","keywords_author":["ADR","Adverse drug reaction","Deep learning word embeddings","Machine learning","Natural language processing","Pharmacovigilance","Social media mining"],"keywords_other":["Pharmacovigilance","Humans","Semantics","Natural Language Processing","Social Media","Artificial Intelligence","Data Mining"],"max_cite":97.0,"pub_year":2015.0,"sources":"['scp', 'wos']","rawkeys":["artificial intelligence","adverse drug reaction","data mining","social media mining","semantics","pharmacovigilance","social media","machine learning","natural language processing","humans","adr","deep learning word embeddings"],"tags":["data mining","social media mining","semantics","pharmacovigilance","social media","machine learning","natural language processing","humans","adverse drug reactions","deep learning word embeddings"]},{"p_id":43336,"title":"Deep Learning Techniques for Polarity Classification in Multimodal Sentiment Analysis","abstract":"\u00a9 2018 World Scientific Publishing Company. Contemporary research in Multimodal Sentiment Analysis (MSA) using deep learning is becoming popular in Natural Language Processing. Enormous amount of data are obtainable from social media such as Facebook, WhatsApp, YouTube, Twitter and microblogs every day. In order to deal with these large multimodal data, it is difficult to identify the relevant information from social media websites. Hence, there is a need to improve an intellectual MSA. Here, Deep Learning is used to improve the understanding and performance of MSA better. Deep Learning delivers automatic feature extraction and supports to achieve the best performance to enhance the combined model that integrates Linguistic, Acoustic and Video information extraction method. This paper focuses on the various techniques used for classifying the given portion of natural language text, audio and video according to the thoughts, feelings or opinions expressed in it, i.e., whether the general attitude is Neutral, Positive or Negative. From the results, it is perceived that Deep Learning classification algorithm gives better results compared to other machine learning classifiers such as KNN, Naive Bayes, Random Forest, Random Tree and Neural Net model. The proposed MSA in deep learning is to identify sentiment in web videos which conduct the poof-of-concept experiments that proved, in preliminary experiments using the ICT-YouTube dataset, our proposed multimodal system achieves an accuracy of 96.07%.","keywords_author":["artificial intelligence","Deep learning","feature extraction","multimodal sentiment analysis","natural language processing"],"keywords_other":null,"max_cite":0.0,"pub_year":2018.0,"sources":"['scp']","rawkeys":["artificial intelligence","deep learning","natural language processing","feature extraction","multimodal sentiment analysis"],"tags":["natural language processing","machine learning","feature extraction","multimodal sentiment analysis"]},{"p_id":29001,"title":"Bi-directional LSTM recurrent neural network for chinese word segmentation","abstract":"\u00ef\u00bf\u00bd Springer International Publishing AG 2016. Recurrent neural network (RNN) has been broadly applied to natural language process (NLP) problems. This kind of neural network is designed for modeling sequential data and has been testified to be quite efficient in sequential tagging tasks. In this paper, we propose to use bi-directional RNN with long short-term memory (LSTM) units for Chinese word segmentation, which is a crucial task for modeling Chinese sentences and articles. Classical methods focus on designing and combining hand-craft features from context, whereas bi-directional LSTM network (BLSTM) does not need any prior knowledge or pre-designing, and is expert in creating hierarchical feature representation of contextual information from both directions. Experiment result shows that our approach gets state-of-the-art performance in word segmentation on both traditional Chinese datasets and simplified Chinese datasets.","keywords_author":["Chinese word segmentation","Long short-term memory","Neural network"],"keywords_other":["State-of-the-art performance","Classical methods","Contextual information","Chinese word segmentation","Natural language process","Recurrent neural network (RNN)","Long short term memory","Hierarchical features"],"max_cite":5.0,"pub_year":2016.0,"sources":"['scp', 'wos']","rawkeys":["classical methods","neural network","natural language process","chinese word segmentation","contextual information","long short-term memory","hierarchical features","recurrent neural network (rnn)","state-of-the-art performance","long short term memory"],"tags":["classical methods","chinese word segmentation","contextual information","neural networks","long short-term memory","hierarchical features","natural language processing","state-of-the-art performance"]},{"p_id":41293,"title":"From computational intelligence to web intelligence: An ensemble from potpourri","abstract":"\u00a9 Springer-Verlag Berlin Heidelberg 2001.The advent of the internet has changed the world in possibly more significant ways than any other event in the history of humanity. Is internet access and use beyond the reach of ordinary people with ordinary intelligence? Ignoring for the moment economic issues of access for all citizenry, what is it about internet access and use that hinders more widespread acceptability? We explore several issues, not exclusive, that attempt to provoke and poke at answers to these simple questions. Largely speculative, as invited talks ought to be, we explore 3 topics, well studied but as yet generally unsolved, in computational intelligence and explore their impact on web intelligence. These topics are machine translation, machine learning, and user interface design. Conclusion will be mine; readers will draw general conclusions.","keywords_author":["Computational Intelligence","Data Mining","Machine Learning","Machine Translation","Natural Language Processing","User Interfaces","Web Intelligence"],"keywords_other":["User interface designs","Internet access","Economic issues","Web intelligence","Access for all","Ordinary people","Machine translations","NAtural language processing"],"max_cite":1.0,"pub_year":2001.0,"sources":"['scp']","rawkeys":["web intelligence","data mining","economic issues","machine learning","natural language processing","machine translation","access for all","ordinary people","user interfaces","machine translations","computational intelligence","internet access","user interface designs"],"tags":["web intelligence","data mining","economic issues","user interface","machine learning","natural language processing","ordinary people","access for all","machine translations","computational intelligence","internet access","user interface designs"]},{"p_id":22867,"title":"Knowledge representation learning: A review","abstract":"\u00a9 2016, Science Press. All right reserved.Knowledge bases are usually represented as networks with entities as nodes and relations as edges. With network representation of knowledge bases, specific algorithms have to be designed to store and utilize knowledge bases, which are usually time consuming and suffer from data sparsity issue. Recently, representation learning, delegated by deep learning, has attracted many attentions in natural language processing, computer vision and speech analysis. Representation learning aims to project the interested objects into a dense, real-valued and low-dimensional semantic space, whereas knowledge representation learning focuses on representation learning of entities and relations in knowledge bases. Representation learning can efficiently measure semantic correlations of entities and relations, alleviate sparsity issues, and significantly improve the performance of knowledge acquisition, fusion and inference. In this paper, we will introduce the recent advances of representation learning, summarize the key challenges and possible solutions, and further give a future outlook on the research and application directions.","keywords_author":["Deep learning","Distributed representation","Knowledge graph","Knowledge representation","Representation learning"],"keywords_other":["Deep learning","Network representation","Representation learning","Research and application","Low dimensional","Knowledge graphs","Distributed representation","NAtural language processing"],"max_cite":11.0,"pub_year":2016.0,"sources":"['scp']","rawkeys":["knowledge graphs","network representation","research and application","deep learning","low dimensional","knowledge graph","natural language processing","representation learning","distributed representation","knowledge representation"],"tags":["knowledge graphs","network representation","research and application","low dimensional","machine learning","natural language processing","representation learning","distributed representation","knowledge representation"]},{"p_id":26969,"title":"Web metadata extraction and semantic indexing for learning objects extraction","abstract":"Secondary-school teachers are in constant need of finding relevant digital resources to support specific didactic goals. Unfortunately, generic search engines do not allow them to identify learning objects among semi-structured candidate educational resources, much less retrieve them by teaching goals. This article describes a multi-strategy approach for semantically guided extraction, indexing and search of educational metadata; it combines machine learning, concept analysis, and corpus-based natural language processing techniques. The overall model was validated by comparing extracted metadata against standard search methods and heuristic-based techniques for Classification Accuracy and Metadata Quality (as evaluated by actual teachers), yielding promising results and showing that this semantically guided metadata extraction can effectively enhance access and use of educational digital material. \u00a9 2014 Springer Science+Business Media New York.","keywords_author":["Learning objects","Machine learning","Metadata extraction","Semantic analysis","Text mining"],"keywords_other":["Educational metadata","Text mining","Meta-data extractions","NAtural language processing","Semantic analysis","Learning objects","Classification accuracy","Educational resource"],"max_cite":6.0,"pub_year":2014.0,"sources":"['scp']","rawkeys":["text mining","meta-data extractions","classification accuracy","machine learning","natural language processing","educational resource","educational metadata","semantic analysis","metadata extraction","learning objects"],"tags":["text mining","classification accuracy","machine learning","natural language processing","educational resource","educational metadata","semantic analysis","metadata extraction","learning objects"]},{"p_id":51545,"title":"Classification and generation of grammatical errors","abstract":"The misuse of grammar is a common and natural nuisance, and a strategy for automatically detecting mistakes in grammatical syntax is warranted. This research defines and implements a unique approach that combines machine-learning and statistical natural language processing techniques. Several important methods are established: (1) the automated and systematic generation of grammatical errors and parallel error corpora; (2) the definition and extraction of over 150 features of a sentence; and (3) the application of various machine-learning classification algorithms on extracted feature data, in order to classify and predict the presence of grammatical errors in a sentence. \u00a9 2014 ACM.","keywords_author":["artificial intelligence","classification","grammar","machine learning","natural language processing"],"keywords_other":["Grammatical errors","Machine-learning","grammar","Feature data","Classification algorithm","Statistical natural language processing","NAtural language processing"],"max_cite":0.0,"pub_year":2014.0,"sources":"['scp']","rawkeys":["artificial intelligence","statistical natural language processing","grammar","machine learning","natural language processing","feature data","classification","machine-learning","classification algorithm","grammatical errors"],"tags":["statistical natural language processing","grammar","machine learning","natural language processing","feature data","classification","classification algorithm","grammatical errors"]},{"p_id":352,"title":"Deep belief nets for natural language call-routing","abstract":"This paper considers application of Deep Belief Nets (DBNs) to natural language call routing. DBNs have been successfully applied to a number of tasks, including image, audio and speech classification, thanks to the recent discovery of an efficient learning technique. DBNs learn a multi-layer generative model from unlabeled data and the features discovered by this model are then used to initialize a feed-forward neural network which is fine-tuned with backpropagation. We compare a DBN-initialized neural network to three widely used text classification algorithms; Support Vector machines (SVM), Boosting and Maximum Entropy (MaxEnt). The DBN-based model gives a call-routing classification accuracy that is equal to the best of the other models even though it currently uses an impoverished representation of the input.","keywords_author":["Call-Routing","DBN","Deep Learning","RBM","Call-Routing","Deep Learning","DBN","RBM"],"keywords_other":["Call-Routing","Maximum entropy","maximum entropy","Artificial neural networks","Efficient learning","RBM","Natural languages","support vector machine","Boosting","Deep learning","SVM","Text classification","Natural language call routing","natural language call routing","Speech classification","deep belief nets","feedforward neural nets","Training data","maximum entropy methods","Training","multilayer generative model","DBN","belief networks","Data models","Generative model","Unlabeled data","feed-forward neural network","natural language processing","Accuracy","learning technique","support vector machines","Support vector machines","Classification accuracy","backpropagation"],"max_cite":38.0,"pub_year":2011.0,"sources":"['scp', 'ieee']","rawkeys":["maximum entropy","natural languages","support vector machine","training data","text classification","boosting","rbm","dbn","efficient learning","natural language call routing","svm","speech classification","deep belief nets","call-routing","feedforward neural nets","generative model","maximum entropy methods","deep learning","training","unlabeled data","multilayer generative model","belief networks","accuracy","feed-forward neural network","classification accuracy","natural language processing","artificial neural networks","data models","learning technique","support vector machines","backpropagation"],"tags":["maximum entropy","natural languages","maximum entropy models","training data","text classification","boosting","efficient learning","machine learning","natural language call routing","learning techniques","speech classification","call-routing","feedforward neural nets","generative model","neural networks","training","unlabeled data","multilayer generative model","belief networks","restricted boltzmann machine","accuracy","classification accuracy","natural language processing","data models","deep belief networks","backpropagation"]},{"p_id":22881,"title":"Adapting existing natural language processing resources for cardiovascular risk factors identification in clinical notes","abstract":"\u00a9 2015 Elsevier Inc. The 2014 i2b2 natural language processing shared task focused on identifying cardiovascular risk factors such as high blood pressure, high cholesterol levels, obesity and smoking status among other factors found in health records of diabetic patients. In addition, the task involved detecting medications, and time information associated with the extracted data. This paper presents the development and evaluation of a natural language processing (NLP) application conceived for this i2b2 shared task. For increased efficiency, the application main components were adapted from two existing NLP tools implemented in the Apache UIMA framework: Textractor (for dictionary-based lookup) and cTAKES (for preprocessing and smoking status detection). The application achieved a final (micro-averaged) F1-measure of 87.5% on the final evaluation test set. Our attempt was mostly based on existing tools adapted with minimal changes and allowed for satisfying performance with limited development efforts.","keywords_author":["Cardiovascular disease","Clinical narrative","Information extraction","Machine learning","Medical records","Natural language processing","Risk factors","Text mining"],"keywords_other":["Text mining","Clinical narrative","Humans","Vocabulary, Controlled","Risk factors","Aged","Pattern Recognition, Automated","Female","Cohort Studies","Cardiovascular Diseases","Comorbidity","Diabetes Complications","Narration","Medical record","Longitudinal Studies","Utah","Incidence","NAtural language processing","Confidentiality","Male","Risk Assessment","Electronic Health Records","Computer Security","Middle Aged","Natural Language Processing","Data Mining","Cardio-vascular disease"],"max_cite":11.0,"pub_year":2015.0,"sources":"['scp']","rawkeys":["vocabulary","automated","computer security","aged","cardio-vascular disease","utah","comorbidity","text mining","information extraction","machine learning","electronic health records","medical record","middle aged","risk assessment","medical records","cardiovascular disease","clinical narrative","diabetes complications","narration","cohort studies","data mining","incidence","humans","confidentiality","cardiovascular diseases","controlled","risk factors","longitudinal studies","male","natural language processing","pattern recognition","female"],"tags":["vocabulary","automated","computer security","aged","utah","comorbidity","text mining","information extraction","clinical narratives","control","electronic health records","machine learning","medical record","middle aged","risk assessment","cardiovascular disease","diabetes complications","narration","cohort studies","data mining","incidence","humans","confidentiality","risk factors","longitudinal studies","male","natural language processing","pattern recognition","female"]},{"p_id":356,"title":"Text-Attentional Convolutional Neural Network for Scene Text Detection","abstract":"Recent deep learning models have demonstrated strong capabilities for classifying text and non-text components in natural images. They extract a high-level feature globally computed from a whole image component (patch), where the cluttered background information may dominate true text features in the deep representation. This leads to less discriminative power and poorer robustness. In this paper, we present a new system for scene text detection by proposing a novel text-attentional convolutional neural network (Text-CNN) that particularly focuses on extracting text-related regions and features from the image components. We develop a new learning mechanism to train the Text-CNN with multi-level and rich supervised information, including text region mask, character label, and binary text\/non-text information. The rich supervision information enables the Text-CNN with a strong capability for discriminating ambiguous texts, and also increases its robustness against complicated background components. The training process is formulated as a multi-task learning problem, where low-level supervised information greatly facilitates the main task of text\/non-text classification. In addition, a powerful low-level detector called contrast-enhancement maximally stable extremal regions (MSERs) is developed, which extends the widely used MSERs by enhancing intensity contrast between text patterns and background. This allows it to detect highly challenging text patterns, resulting in a higher recall. Our approach achieved promising results on the ICDAR 2013 data set, with an F-measure of 0.82, substantially improving the state-of-the-art results.","keywords_author":["convolutional neural networks","Maximally stable extremal regions","multi-level supervised information","multi-task learning","text detector","Maximally stable extremal regions","text detector","convolutional neural networks","multi-level supervised information","multi-task learning","Maximally Stable Extremal Regions","text detector","convolutional neural networks","multi-level supervised information","multi-task learning"],"keywords_other":["ambiguous texts","Humans","Contrast Enhancement","text detection","binary text\/non-text information","scene text detection","text region mask","Text recognition","Neural networks","Discriminative power","READING TEXT","Maximally Stable Extremal Regions","text-attentional convolutional neural network","Computational modeling","contrast-enhancement maximally stable extremal regions","Multilevels","text detector","Feature extraction","NATURAL IMAGES","multi-level supervised information","multi-task learning","Algorithms","convolutional neural networks","Robustness","Training","RECOGNITION","deep learning models","image enhancement","STABLE EXTREMAL REGIONS","neural nets","Background components","Detectors","LOCALIZATION","Neural Networks (Computer)","Multitask learning","ICDAR 2013 data set","Natural Language Processing","Maximally stable extremal regions","character label","Cluttered backgrounds","Convolutional neural network"],"max_cite":51.0,"pub_year":2016.0,"sources":"['ieee', 'scp', 'wos']","rawkeys":["robustness","ambiguous texts","localization","text detection","binary text\/non-text information","contrast enhancement","scene text detection","text region mask","convolutional neural network","icdar 2013 data set","text-attentional convolutional neural network","contrast-enhancement maximally stable extremal regions","text detector","multitask learning","multilevels","algorithms","natural images","multi-level supervised information","stable extremal regions","computational modeling","multi-task learning","neural networks (computer)","convolutional neural networks","maximally stable extremal regions","reading text","neural networks","recognition","training","humans","text recognition","background components","image enhancement","deep learning models","neural nets","cluttered backgrounds","detectors","discriminative power","natural language processing","character label","feature extraction"],"tags":["robustness","ambiguous texts","localization","text detection","binary text\/non-text information","contrast enhancement","scene text detection","text region mask","convolutional neural network","text-attentional convolutional neural network","contrast-enhancement maximally stable extremal regions","text detector","machine learning","multitask learning","algorithms","natural images","multi-level supervised information","stable extremal regions","computational modeling","recognition","maximally stable extremal regions","deep learning model","icdar2013 datasets","neural networks","reading text","training","humans","text recognition","background components","image enhancement","cluttered backgrounds","detectors","discriminative power","natural language processing","character label","feature extraction"]},{"p_id":47460,"title":"A Privacy Guard Service","abstract":"\u00a9 2017 IEEE. Mobile devices are changing the way that people conduct their daily businesses and carry out their works. More and more people are using mobile devices to view personal or work-related information in public places, e.g. caf\u00e9, train, etc. This type of information might contain sensitive data, e.g. personal information, trade secrets, etc. As high-resolution cameras are widely used and public places are always crowded, viewing information in public carries the risk of having the screen filmed by cameras or being peeked by the near-by people. This paper proposes a privacy guard service to help people to safe-guard their sensitive data while viewing information in public. The service uses machine learning and natural language processing techniques to identify sensitive information in documents. The service automatically converts the sensitive information to meaningless strings. Text-to-speech code is embedded behind the symbols. The code enable the users to listen to the content of the original sensitive information by using an earphone when the users tap on the symbols. As a result, users are able to view their sensitive data in public without worrying about privacy leaks due to visual observation attacks.","keywords_author":["Machine learning","Natural language processing","Privacy","Shoulder surfing","Visual observation attack"],"keywords_other":["Personal information","Text to speech","Visual observations","Sensitive informations","High resolution camera","Sensitive datas","Natural languages","Shoulder surfing"],"max_cite":0.0,"pub_year":2017.0,"sources":"['scp']","rawkeys":["visual observation attack","personal information","privacy","visual observations","natural languages","natural language processing","machine learning","sensitive informations","shoulder surfing","sensitive datas","text to speech","high resolution camera"],"tags":["visual observation attack","personal information","privacy","visual observations","natural languages","natural language processing","machine learning","sensitive informations","shoulder surfing","sensitive datas","text to speech","high resolution camera"]},{"p_id":51558,"title":"Image recognition method which measures angular velocity from a back of hand for developing a valve UI","abstract":"The training that uses virtual reality technology is expected to be useful for preventing the accidents caused by human factor. However, there is a problem that a trainee cannot train due to lack of the UI such as realizing the real valve operations in industrial plant. We propose an image recognition method to measure the valve opening manipulation velocity for developing UI to realize the training of valve manipulation in virtual environment. The method to measure the angular velocity of the circular handle uses the optical natural feature points on the handle turning the valve. The advantages of our valve manipulation UI are that one is low cost and another it can be applied to various valves with circular handle.","keywords_author":["Image recognition","Machine learning","Natural language processing","Training and Learning","Virtual reality"],"keywords_other":["Valve opening","Valve operation","Low costs","Recognition methods","Virtual reality technology","NAtural language processing","Natural features"],"max_cite":0.0,"pub_year":2014.0,"sources":"['scp']","rawkeys":["natural features","recognition methods","valve opening","virtual reality","low costs","natural language processing","machine learning","valve operation","virtual reality technology","image recognition","training and learning"],"tags":["natural features","recognition methods","valve opening","virtual reality","low costs","natural language processing","machine learning","valve operation","virtual reality technology","image recognition","training and learning"]},{"p_id":47461,"title":"Improved Candidate Answers Ranking for QA","abstract":"\u00a9 2017 IEEE. Query answering system (QA) has become an important branch of nature language processing (NLP) and information retrieval (IR), it encompasses a wide range of topics related to the storage and retrieval of all kinds of media. This paper presents a new combination of IR grammars, Latent Dirichlet Allocation (LDA) grammars Doc2Vec models and Deep Learning models to improve the value of Mean Reciprocal Rank (MRR) in the factoid question answering task. Empirical study shows the proposed model can effectively position the correct answer and improve the value of MRR when combined with LDA grammars.","keywords_author":["Deep learning","Doc2Vec","IR","LDA","NLP","Query answering system"],"keywords_other":["Factoid questions","Doc2Vec","Mean reciprocal ranks","Storage and retrievals","Nature language processing","Latent dirichlet allocations","Query answering systems","Empirical studies"],"max_cite":0.0,"pub_year":2017.0,"sources":"['scp']","rawkeys":["doc2vec","factoid questions","nlp","query answering systems","deep learning","empirical studies","storage and retrievals","latent dirichlet allocations","mean reciprocal ranks","nature language processing","query answering system","ir","lda"],"tags":["doc2vec","factoid questions","linear discriminant analysis","query answering systems","empirical studies","machine learning","natural language processing","information retrieval","mean reciprocal ranks","storage and retrievals"]},{"p_id":55663,"title":"For a responsible TAL","abstract":"Artificial intelligence (AI) has evolved in recent years along with societal concerns. Various committees were introduced in order to brainstorm on the consequences of these developments. These authorities are also concerned by Natural Language Processing (NLP), not only as a subfield of AI but also as a specific field with which it interacts. In this article we review the links between AI and NLP but also where they differ. We focus on ethical clues for both of them. Finally we argue for not using ethics as a unique solution, but rather as the way to abstract over our researches. In the end, we go back on how to interpret machine learning methods in the context of NLP.","keywords_author":["ethics","artificial intelligence","Natural Language Processing","Epistemology"],"keywords_other":null,"max_cite":0.0,"pub_year":2016.0,"sources":"['wos']","rawkeys":["artificial intelligence","ethics","natural language processing","epistemology"],"tags":["natural language processing","ethics","machine learning","epistemology"]},{"p_id":16753,"title":"Word sense disambiguation across two domains: Biomedical literature and clinical notes","abstract":"The aim of this study is to explore the word sense disambiguation (WSD) problem across two biomedical domains-biomedical literature and clinical notes. A supervised machine learning technique was used for the WSD task. One of the challenges addressed is the creation of a suitable clinical corpus with manual sense annotations. This corpus in conjunction with the WSD set from the National Library of Medicine provided the basis for the evaluation of our method across multiple domains and for the comparison of our results to published ones. Noteworthy is that only 20% of the most relevant ambiguous terms within a domain overlap between the two domains, having more senses associated with them in the clinical space than in the biomedical literature space. Experimentation with 28 different feature sets rendered a system achieving an average F-score of 0.82 on the clinical data and 0.86 on the biomedical literature. \u00a9 2008 Elsevier Inc. All rights reserved.","keywords_author":["Artificial intelligence","Biomedical natural language processing","Information extraction","Machine learning","Natural language processing","Word sense disambiguation"],"keywords_other":["Biomedical natural language processing","Machine learning","Word sense disambiguation","Information extraction","Natural language processing"],"max_cite":34.0,"pub_year":2008.0,"sources":"['scp']","rawkeys":["artificial intelligence","information extraction","natural language processing","machine learning","word sense disambiguation","biomedical natural language processing"],"tags":["information extraction","natural language processing","machine learning","word sense disambiguation","biomedical natural language processing"]},{"p_id":55667,"title":"Machine Translation: Mining Text for Social Theory","abstract":"More of the social world lives within electronic text than ever before, from collective activity on the web, social media, and instant messaging to online transactions, government intelligence, and digitized libraries. This supply of text has elicited demand for natural language processing and machine learning tools to filter, search, and translate text into valuable data. We survey some of the most exciting computational approaches to text analysis, highlighting both supervised methods that extend old theories to new data and unsupervised techniques that discover hidden regularities worth theorizing. Wethen review recent research that uses these tools to develop social insight by exploring (a) collective attention and reasoning through the content of communication; (b) social relationships through the process of communication; and (c) social states, roles, and moves identified through heterogeneous signals within communication. Wehighlight social questions for which these advances could offer powerful new insight.","keywords_author":["content analysis","big data","natural language processing","machine learning","text analysis","computational methods","grounded theory"],"keywords_other":["REGRESSION","CULTURE","NETWORKS","STRATEGIES","SCIENCE","DISCOURSE","PERFORMANCE","MEDIA","LANGUAGE","TOPIC MODELS"],"max_cite":8.0,"pub_year":2016.0,"sources":"['wos']","rawkeys":["computational methods","performance","big data","machine learning","content analysis","natural language processing","strategies","media","networks","science","text analysis","topic models","discourse","culture","language","grounded theory","regression"],"tags":["computational methods","performance","big data","topic modeling","machine learning","content analysis","natural language processing","strategies","media","networks","science","text analysis","discourse","culture","language","grounded theory","regression"]},{"p_id":375,"title":"Spoken language understanding using long short-term memory neural networks","abstract":"Neural network based approaches have recently produced record-setting performances in natural language understanding tasks such as word labeling. In the word labeling task, a tagger is used to assign a label to each word in an input sequence. Specifically, simple recurrent neural networks (RNNs) and convolutional neural networks (CNNs) have shown to significantly outperform the previous state-of-the-art - conditional random fields (CRFs). This paper investigates using long short-term memory (LSTM) neural networks, which contain input, output and forgetting gates and are more advanced than simple RNN, for the word labeling task. To explicitly model output-label dependence, we propose a regression model on top of the LSTM un-normalized scores. We also propose to apply deep LSTM to the task. We investigated the relative importance of each gate in the LSTM by setting other gates to a constant and only learning particular gates. Experiments on the ATIS dataset validated the effectiveness of the proposed models.","keywords_author":["Recurrent neural networks","long short-term memory","language understanding"],"keywords_other":["conditional random fields","recurrent neural networks","output-label dependence","spoken language understanding","recurrent neural nets","LSTM neural networks","convolution","CNN","word processing","Speech","regression model","convolutional neural networks","Training","neural network based approach","Vectors","CRF","ATIS dataset","Logic gates","long short-term memory neural networks","LSTM unnormalized scores","word labeling task","RNN","Recurrent neural networks","Semantics","natural language processing","speech processing","natural language understanding tasks"],"max_cite":22.0,"pub_year":2014.0,"sources":"['ieee']","rawkeys":["lstm unnormalized scores","conditional random fields","recurrent neural networks","output-label dependence","logic gates","spoken language understanding","recurrent neural nets","convolution","vectors","word processing","regression model","speech","convolutional neural networks","semantics","long short-term memory","neural network based approach","training","cnn","lstm neural networks","rnn","atis dataset","crf","long short-term memory neural networks","word labeling task","natural language processing","speech processing","natural language understanding tasks","language understanding"],"tags":["lstm unnormalized scores","convolutional neural network","output-label dependence","logic gates","spoken language understanding","recurrent neural nets","conditional random field","convolution","vectors","word processing","regression model","speech","neural networks","semantics","long short-term memory","neural network based approach","training","lstm neural networks","atis dataset","word labeling task","natural language processing","speech processing","natural language understanding tasks","language understanding"]},{"p_id":376,"title":"Developing speech recognition systems for corpus indexing under the IARPA Babel program","abstract":"Automatic speech recognition is a core component of many applications, including keyword search. In this paper we describe experiments on acoustic modeling, language modeling, and decoding for keyword search on a Cantonese conversational telephony corpus collected as part of the IARPA Babel program. We show that acoustic modeling techniques such as the bootstrapped-and-restructured model and deep neural network acoustic model significantly outperform a state-of-the-art baseline GMM\/HMM model, in terms of both recognition performance and keyword search performance, with improvements of up to 11% relative character error rate reduction and 31% relative maximum term weighted value improvement. We show that while an interpolated Model M and neural network LM improve recognition performance, they do not improve keyword search results; however, the advanced LM does reduce the size of the keyword search index. Finally, we show that a simple form of automatically adapted keyword search performs 16% better than a preindexed search system, indicating that out-of-vocabulary search is still a challenge.","keywords_author":["acoustic modeling","bootstrap","deep learning","keyword search","language modeling","acoustic modeling","language modeling","bootstrap","deep learning","keyword search"],"keywords_other":["bootstrap","automatic speech recognition","GMM model","HMM model","IARPA Babel program","speech recognition","out-of-vocabulary search","interpolation","model M interpolation","telephony","Deep learning","Lattices","Cantonese conversational telephony corpus","acoustic signal processing","hidden Markov models","relative character error rate reduction","Gaussian processes","Training","Keyword search","Hidden Markov models","Acoustic model","neural nets","keyword search performance","Adaptation models","neural network LM","speech coding","search problems","Language model","natural language processing","acoustic modeling","language modeling","decoding","Acoustics","Decoding"],"max_cite":30.0,"pub_year":2013.0,"sources":"['ieee', 'scp']","rawkeys":["bootstrap","automatic speech recognition","adaptation models","speech recognition","out-of-vocabulary search","interpolation","telephony","acoustics","model m interpolation","acoustic signal processing","relative character error rate reduction","deep learning","training","gmm model","neural network lm","language model","cantonese conversational telephony corpus","neural nets","keyword search performance","hidden markov models","acoustic model","speech coding","search problems","iarpa babel program","natural language processing","acoustic modeling","gaussian processes","language modeling","lattices","keyword search","decoding","hmm model"],"tags":["bootstrap","automatic speech recognition","hmm models","adaptation models","speech recognition","out-of-vocabulary search","interpolation","telephony","acoustics","model m interpolation","machine learning","acoustic signal processing","relative character error rate reduction","neural networks","training","gmm model","neural network lm","language model","cantonese conversational telephony corpus","keyword search performance","hidden markov models","acoustic model","speech coding","search problems","iarpa babel program","natural language processing","gaussian processes","lattices","keyword search","decoding"]},{"p_id":45433,"title":"Neural machine translation system for indic languages using deep neural architecture","abstract":"\u00a9 Springer Nature Singapore Pte Ltd. 2018. Translating into an Indic language is a challenging task. Indic languages are based on Sanskrit and have rich and diverse grammar. Due to its vast grammar, it requires very large number of complex rules for creating traditional rule based machine translation system. In this work, we have created an Indic machine translation system that utilize recent advancement in the area of machine translation using deep neural architectures. Results presented in this article shows that using neural machine translation we can achieve more natural translation for indic languages such as Hindi that was previously not efficiently possible with rule based or phrase based translation systems.","keywords_author":["Deep learning","Deep neural network","Hindi neural machine translation","Indic translation","Machine translation","Natural language processing (NLP)"],"keywords_other":["Rule-based machine translations","Translation systems","Machine translation systems","Rule based","Machine translations","Neural architectures"],"max_cite":0.0,"pub_year":2018.0,"sources":"['scp']","rawkeys":["deep neural network","deep learning","rule based","indic translation","machine translation systems","machine translation","natural language processing (nlp)","translation systems","machine translations","neural architectures","rule-based machine translations","hindi neural machine translation"],"tags":["rule based","indic translation","machine learning","machine translation systems","natural language processing","translation systems","convolutional neural network","machine translations","neural architectures","rule-based machine translations","hindi neural machine translation"]},{"p_id":27002,"title":"An integrated approach to spam classification on Twitter using URL analysis, natural language processing and machine learning techniques","abstract":"In the present day world, people are so much habituated to Social Networks. Because of this, it is very easy to spread spam contents through them. One can access the details of any person very easily through these sites. No one is safe inside the social media. In this paper we are proposing an application which uses an integrated approach to the spam classification in Twitter. The integrated approach comprises the use of URL analysis, natural language processing and supervised machine learning techniques. In short, this is a three step process. \u00a9 2014 IEEE.","keywords_author":["machine learning","natural language processing","tweets","URLs"],"keywords_other":["Three-step process","Social media","tweets","Supervised machine learning","Spam classification","NAtural language processing","Machine learning techniques","Integrated approach"],"max_cite":6.0,"pub_year":2014.0,"sources":"['scp']","rawkeys":["spam classification","integrated approach","urls","social media","machine learning techniques","natural language processing","machine learning","supervised machine learning","three-step process","tweets"],"tags":["spam classification","integrated approach","url","social media","machine learning techniques","natural language processing","machine learning","supervised machine learning","three-step process","tweets"]},{"p_id":18813,"title":"Natural language processing in radiology: A systematic review","abstract":"\u00a9 2015 RSNA. Radiological reporting has generated large quantities of digital content within the electronic health record, which is potentially a valuable source of information for improving clinical care and supporting research. Although radiology reports are stored for communication and documentation of diagnostic imaging, harnessing their potential requires efficient and automated information extraction: they exist mainly as free-text clinical narrative, from which it is a major challenge to obtain structured data. Natural language processing (NLP) provides techniques that aid the conversion of text into a structured representation, and thus enables computers to derive meaning from human (ie, natural language) input. Used on radiology reports, NLP techniques enable automatic identification and extraction of information. By exploring the various purposes for their use, this review examines how radiology benefits from NLP. A systematic literature search identified 67 relevant publications describing NLP methods that support practical applications in radiology. This review takes a close look at the individual studies in terms of tasks (ie, the extracted information), the NLP methodology and tools used, and their application purpose and performance results. Additionally, limitations, future challenges, and requirements for advancing NLP in radiology will be discussed.","keywords_author":null,"keywords_other":["Electronic Health Records","Humans","Natural Language Processing","Radiology","Information Storage and Retrieval","Radiology Information Systems"],"max_cite":25.0,"pub_year":2016.0,"sources":"['scp', 'wos']","rawkeys":["natural language processing","electronic health records","humans","radiology","radiology information systems","information storage and retrieval"],"tags":["natural language processing","electronic health records","humans","radiology","radiology information systems","information storage and retrieval"]},{"p_id":24959,"title":"Tweeting traffic: Analyzing twitter for generating real-time city traffic insights and predictions","abstract":"\u00a9 2015 Copyright held by the owner\/author(s). Crowd sourced road traffic management is an open, unexplored problem in data science. With the growth of mobile communications and social media networks, more people are expressing their traffic situations in real-time. We explore how this social media data can be analyzed to generate valuable insights, useful for traffic management and city planning. Our method utilizes background knowledge from structured data repositories for entity extraction from tweets. We proceed to use this spatio-temporal data for traffic incident clustering and prediction. With accuracy and precision measurements providing encouraging results, we build on our methods and present our Continuous Traffic Management Dashboard (CTMD) system: an automated computer system for generating real-time, historic and predictive traffic insights.","keywords_author":["Data science","Machine learning","Natural language processing","Prediction","Road traffic","Twitter"],"keywords_other":["Data science","Twitter","Road traffic","Social media networks","Road traffic management","Accuracy and precision","Back-ground knowledge","NAtural language processing"],"max_cite":8.0,"pub_year":2015.0,"sources":"['scp']","rawkeys":["social media networks","prediction","natural language processing","machine learning","road traffic management","back-ground knowledge","accuracy and precision","road traffic","data science","twitter"],"tags":["social media networks","prediction","natural language processing","machine learning","road traffic management","back-ground knowledge","accuracy and precision","road traffic","data science","twitter"]},{"p_id":45439,"title":"Build Chinese language model with recurrent neural network","abstract":"\u00a9 Springer Nature Singapore Pte Ltd. 2018. In recent years, the introduction of Deep Learning based machine learning methods have greatly enhanced the performance of Natural Language Processing (NLP). However, most Deep Learning based NLP studies in the literature are aimed at the Latin family languages. There is seldom research which takes Chinese language model as the objective. In this paper, we use Deep Learning method to build language model for Chinese. In our model, the Fully-connected Neural Network which is a popular structure used in NLP is replaced by the Recurrent Neural Network to build a better language model. In the experiments, we compare and summarize the differences between the results obtained by using the original Deep Learning method and our model. And the results prove the effectiveness of our proposed model.","keywords_author":["Deep learning","Language model","Natural language processing","Recurrent neural networks"],"keywords_other":["Learning methods","Fully connected neural network","Language model","Machine learning methods","Chinese language modeling"],"max_cite":0.0,"pub_year":2018.0,"sources":"['scp']","rawkeys":["machine learning methods","deep learning","learning methods","natural language processing","fully connected neural network","recurrent neural networks","language model","chinese language modeling"],"tags":["machine learning methods","neural networks","learning methods","machine learning","fully connected neural network","natural language processing","language model","chinese language modeling"]},{"p_id":47490,"title":"Coverage for character based neural machine translation","abstract":"\u00a9 2017 Sociedad Espanola para el Procesamiento del Lenguaje Natural. In recent years, Neural Machine Translation (NMT) has achieved stateof- the-art performance in translating from a language; source language, to another; target language. However, many of the proposed methods use word embedding techniques to represent a sentence in the source or target language. Character embedding techniques for this task has been suggested to represent the words in a sentence better. Moreover, recent NMT models use attention mechanism where the most relevant words in a source sentence are used to generate a target word. The problem with this approach is that while some words are translated multiple times, some other words are not translated. To address this problem, coverage model has been integrated into NMT to keep track of already-translated words and focus on the untranslated ones. In this research, we present a new architecture in which we use character embedding for representing the source and target languages, and also use coverage model to make certain that all words are translated. Experiments were performed to compare our model with coverage and character model and the results show that our model performs better than the other two models.","keywords_author":["Deep learning","Machine learning","Natural language processing","Neural machine translation"],"keywords_other":null,"max_cite":0.0,"pub_year":2017.0,"sources":"['scp']","rawkeys":["deep learning","natural language processing","machine learning","neural machine translation"],"tags":["natural language processing","machine learning","neural machine translation"]},{"p_id":387,"title":"Deep Learning-Based Document Modeling for Personality Detection from Text","abstract":"This article presents a deep learning based method for determining the author's personality type from text: given a text, the presence or absence of the Big Five traits is detected in the author's psychological profile. For each of the five traits, the authors train a separate binary classifier, with identical architecture, based on a novel document modeling technique. Namely, the classifier is implemented as a specially designed deep convolutional neural network, with injection of the document-level Mairesse features, extracted directly from the text, into an inner layer. The first layers of the network treat each sentence of the text separately; then the sentences are aggregated into the document vector. Filtering out emotionally neutral input sentences improved the performance. This method outperformed the state of the art for all five traits, and the implementation is freely available for research purposes.","keywords_author":["artificial intelligence","convolutional neural network","distributional semantics","intelligent systems","natural language processing","neural-based document modeling","personality","personality","natural language processing","distributional semantics","neural-based document modeling","convolutional neural network","intelligent systems","artificial intelligence"],"keywords_other":["Document model","Big Five traits","deep learning based method","document vector","text","personality","deep learning-based document modeling","Neural networks","emotionally neutral input sentence filtering","document-level Mairesse features","Computational modeling","Feature extraction","personality detection","Pragmatics","binary classifier training","author personality type","feedforward neural nets","learning (artificial intelligence)","deep convolutional neural network","pattern classification","NAtural language processing","information filtering","author psychological profile","Semantics","identical architecture","Emotion recognition","Distributional semantics","text analysis","Convolutional neural network","Artificial intelligence"],"max_cite":39.0,"pub_year":2017.0,"sources":"['ieee', 'scp', 'wos']","rawkeys":["deep learning based method","distributional semantics","document vector","text","deep learning-based document modeling","personality","convolutional neural network","emotionally neutral input sentence filtering","pragmatics","personality detection","binary classifier training","big five traits","author personality type","feedforward neural nets","document-level mairesse features","computational modeling","learning (artificial intelligence)","neural networks","semantics","deep convolutional neural network","intelligent systems","pattern classification","information filtering","artificial intelligence","author psychological profile","identical architecture","natural language processing","neural-based document modeling","emotion recognition","text analysis","feature extraction","document model"],"tags":["distributional semantics","text","deep learning-based document modeling","convolutional neural network","deep-learning based methods","personalizations","emotionally neutral input sentence filtering","pragmatics","machine learning","document vectors","binary classifier training","big five traits","author personality type","feedforward neural nets","document-level mairesse features","computational modeling","person detection","neural networks","semantics","intelligent systems","pattern classification","information filtering","author psychological profile","identical architecture","natural language processing","neural-based document modeling","emotion recognition","text analysis","feature extraction","document model"]},{"p_id":51587,"title":"Text message authorship classification using kernel support vector machines","abstract":"We explore the application of Kernel Support Vector Machines (SVM) to the realm of text messages. Our intent is to classify the author of a text message based on usage patterns present in a training set of text messages. We achieve between 57% and 96% accuracy in determining the author of unknown samples. \u00a9 2014 IEEE.","keywords_author":["Classification","Machine Learning Applications","NLP","SVM","Text Messages"],"keywords_other":["SVM","NLP","Training sets","Usage patterns","Machine learning applications"],"max_cite":0.0,"pub_year":2014.0,"sources":"['scp']","rawkeys":["nlp","machine learning applications","training sets","text messages","svm","usage patterns","classification"],"tags":["machine learning applications","training sets","natural language processing","machine learning","text messages","usage patterns","classification"]},{"p_id":37253,"title":"Question part relevance and editing for cooperative and context-aware VQA (C2VQA)","abstract":"\u00a9 2017 Copyright held by the owner\/author(s). Visual Question Answering (VQA), a task that requires the ability to provide an answer to a question given an image, has recently become an important benchmark for computer vision. However, current VQA approaches are unable to adequately handle questions that are \"irrelevant\", such as asking about a cat for an image that has no cat. To date, only one paper has examined the idea of question relevance in VQA, using a binary classification model to assign a relevancy to the entire question \/ image pair. Truly robust VQA models, however, must not only identify potentially irrelevant questions, but also discover the source of irrelevance and seek to correct it. We therefore introduce two novel problems, question part relevance and question editing, and approaches for solving each problem. In question part relevance, our models go beyond binary question relevance by assigning a classification probability to the portion of the question that is irrelevant. The best question part relevance classifier is later used in question editing to rank possible corrections to the irrelevant portion of given questions. Two custom datasets are developed for these problems using the Visual Genome dataset as a source. Our best models show promising results in these novel tasks over baseline approaches and models adapted from whole-question relevance classification. This work contributes directly to the development of more context-aware and cooperative VQA models, dubbed C2VQA.","keywords_author":["Computer vision","Deep learning","Image captioning","Natural language processing","Visual question answering"],"keywords_other":["Binary classification","Question Answering","Image captioning","Context-Aware","Novel task","Best model"],"max_cite":1.0,"pub_year":2017.0,"sources":"['scp']","rawkeys":["question answering","context-aware","deep learning","image captioning","natural language processing","binary classification","novel task","visual question answering","computer vision","best model"],"tags":["context-aware","image captioning","machine learning","natural language processing","binary classification","information retrieval","novel task","computer vision","best model","video quality assessment"]},{"p_id":45445,"title":"Machine learning implementations in Arabic text classification","abstract":"\u00a9 2018, Springer International Publishing AG. Text categorization denotes the process of assigning to a piece of text a label that describes its thematic information. Although this task has been extensively investigated for different languages, it has not been researched thoroughly with respect to the Arabic language. In this chapter, we summarize the major techniques used for addressing different aspects of the text classification problem. These aspects include problem formalization using vector space model, term weighting, feature reduction, and classification algorithms. We pay special attention to the part of research devoted to text categorization in the Arabic language. We conclude that the effect of language is minimized with respect to this task. Moreover, we list the currently unsolved issues in the text classification context and thereby highlight the active research directions.","keywords_author":["Bag of words","Corpus","Machine learning","NLP","Stemming","Supervised classification","Text categorization"],"keywords_other":null,"max_cite":0.0,"pub_year":2018.0,"sources":"['scp']","rawkeys":["supervised classification","bag of words","nlp","machine learning","stemming","corpus","text categorization"],"tags":["supervised classification","bag of words","natural language processing","machine learning","corpus","text categorization","stem"]},{"p_id":393,"title":"Breast Cancer Histopathological Image Classification using Convolutional Neural Networks","abstract":"The performance of most conventional classification systems relies on appropriate data representation and much of the efforts are dedicated to feature engineering, a difficult and time-consuming process that uses prior expert domain knowledge of the data to create useful features. On the other hand, deep learning can extract and organize the discriminative information from the data, not requiring the design of feature extractors by a domain expert. Convolutional Neural Networks (CNNs) are a particular type of deep, feedforward network that have gained attention from research community and industry, achieving empirical successes in tasks such as speech recognition, signal processing, object recognition, natural language processing and transfer learning. In this paper, we conduct some preliminary experiments using the deep learning approach to classify breast cancer histopathological images from BreaKHis, a publicly dataset available at http:\/\/web.inf.ufpr.br\/vri\/breast-cancer-database. We propose a method based on the extraction of image patches for training the CNN and the combination of these patches for final classification. This method aims to allow using the high-resolution histopathological images from BreaKHis as input to existing CNN, avoiding adaptations of the model that can lead to a more complex and computationally costly architecture. The CNN performance is better when compared to previously reported results obtained by other machine learning models trained with hand-crafted textural descriptors. Finally, we also investigate the combination of different CNNs using simple fusion rules, achieving some improvement in recognition rates.","keywords_author":null,"keywords_other":["machine learning models","speech recognition","domain expert","research community","recognition rates","Neural networks","image classification","high-resolution histopathological images","breast cancer histopathological images","transfer learning","BreaKHis","time-consuming process","discriminative information","convolutional neural networks","deep learning","medical image processing","signal processing","breast cancer histopathological image classification","image patches","hand-crafted textural descriptors","neural nets","fusion rules","cancer","data representation","natural language processing","object recognition","feature extractors","CNN performance","feedforward network"],"max_cite":30.0,"pub_year":2016.0,"sources":"['wos', 'ieee']","rawkeys":["machine learning models","speech recognition","domain expert","research community","recognition rates","image classification","breakhis","high-resolution histopathological images","breast cancer histopathological images","cnn performance","transfer learning","time-consuming process","discriminative information","convolutional neural networks","deep learning","neural networks","medical image processing","signal processing","breast cancer histopathological image classification","image patches","hand-crafted textural descriptors","neural nets","fusion rules","cancer","data representation","natural language processing","object recognition","feature extractors","feedforward network"],"tags":["machine learning models","speech recognition","fusion rule","convolutional neural network","recognition rates","image classification","data representations","breakhis","high-resolution histopathological images","breast cancer histopathological images","cnn performance","transfer learning","machine learning","research communities","time-consuming process","discriminative information","neural networks","domain experts","medical image processing","signal processing","breast cancer histopathological image classification","image patches","hand-crafted textural descriptors","feature extractor","cancer","feed-forward network","natural language processing","object recognition"]},{"p_id":24975,"title":"Hybrid classification for tweets related to infection with influenza","abstract":"\u00a9 2015 IEEE.Traditional public health surveillance methods such as those employed by the CDC (United States Centers for Disease Control and Prevention) rely on regular clinical reports, which are almost always manual and labor intensive. Twitter, a popular micro-blogging service, provides the possibility of automated public health surveillance. Tweets, however, are less than 140 characters, and do not provide sufficient word occurrences for conventional classification methods to work reliably. Moreover, natural language is complex. This makes health-related classification more challenging. In this study, we use flu-related classification as a demonstration to propose a hybrid classification method, which combines two classification approaches: manually- defined features and auto-generated features by machine learning approaches. Preprocessing based on Natural Language Processing (NLP) is used to help extract useful information, and to eliminate noise features. Our simulations show an improved accuracy.","keywords_author":["Big data","Classification","Machine learning","Natural Language Processing","Public Health","Social Network","Twitter"],"keywords_other":["Twitter","Conventional classification methods","Classification approach","Machine learning approaches","Centers for disease control and preventions","Public health surveillances","NAtural language processing","Micro-blogging services"],"max_cite":8.0,"pub_year":2015.0,"sources":"['scp']","rawkeys":["conventional classification methods","micro-blogging services","public health surveillances","big data","machine learning","natural language processing","classification approach","social network","classification","public health","centers for disease control and preventions","machine learning approaches","twitter"],"tags":["conventional classification methods","micro-blogging services","public health surveillances","big data","machine learning","natural language processing","classification approach","social networks","classification","public health","centers for disease control and preventions","machine learning approaches","twitter"]},{"p_id":39312,"title":"Multi agent model for skills training of CSCL e-tutors Modelo multi agente para el entrenamiento de habilidades de e-tutores de ACSC","abstract":"Computer Supported Collaborative Learning (CSCL) systems enable not only group learning with independence of the time and space where group members are located, but also they are favorable environments for leadership skills development. However, as interactions that are ideal for learning do not occur spontaneously, participation of e-tutors (teachers) is essential in order to generate interactions that contribute to collaborative building of knowledge. Considering e-tutors of CSCL usually do not know the most effective way to assist students, this article proposes a multi agent model (combining techniques from natural language processing, text mining, and machine learning) that can be used for personalized training of e-tutors. In the proposed model an intelligent agent analyzes group interactions to identify conflicts which resolution needs e-tutors' intervention. In these cases, a training agent suggests to e-tutors necessary actions so as they solve conflicts and simultaneously they develop skills they do not manifest properly. The multi agent model will be implemented in a CSCL environment and its operation will be evaluated through experiments with university students and teachers. \u00a9 2014 ACM.","keywords_author":["Computer Supported Collaborative Learning","E-tutor skills","Machine learning","Personalized training","Text mining"],"keywords_other":["Text mining","Computer Supported Collaborative Learning","E-tutor","University students","Group interaction","Combining techniques","NAtural language processing","Collaborative buildings"],"max_cite":1.0,"pub_year":2014.0,"sources":"['scp']","rawkeys":["text mining","e-tutor","machine learning","natural language processing","collaborative buildings","combining techniques","computer supported collaborative learning","university students","personalized training","e-tutor skills","group interaction"],"tags":["text mining","e-tutor","university-students","machine learning","natural language processing","collaborative buildings","computer supported collaborative learning","combined techniques","personalized training","e-tutor skills","group interaction"]},{"p_id":51599,"title":"Using supervised learning to classify clothing brand styles","abstract":"Machine learning techniques have the potential to alter the highly competitive online fashion retail industry by improving customer service through personalized recommendations. A fashion style classification system can improve the customer search functionality and provide a more personalized experience for the user. Supervised learning techniques with fashion based applications face the problem of developing quantitative measures for describing fashion products which are subjective in nature. To address this issue the authors asked fashion experts to assist in the assembly of a training set of brand-style associations. Quantitative measures were attributed to each brand in the training set by applying natural language processing, text mining, and eBay query results. This data set was used to train a support vector machine which classified the approximately 8000 remaining brands into style categories. The prospective classifier model was assessed based on its positive predictive values which yielded a 56.25% success rate. Given that there are eight different styles to choose from, a baseline for the percentage is only 12.5%. The SVM thus adds significant value to the classification of fashion brands. The final style categorization was integrated as a new filter feature that allows the user to narrow down their searches and access relevant results. \u00a9 2014 IEEE.","keywords_author":["Fashion","Machine learning","Supervised learning","Support vector machines","Text mining"],"keywords_other":["Text mining","Fashion","Classification system","Personalized recommendation","Quantitative measures","Positive predictive values","NAtural language processing","Machine learning techniques"],"max_cite":0.0,"pub_year":2014.0,"sources":"['scp']","rawkeys":["fashion","supervised learning","classification system","quantitative measures","personalized recommendation","text mining","machine learning techniques","machine learning","natural language processing","support vector machines","positive predictive values"],"tags":["fashion","supervised learning","classification system","quantitative measures","personalized recommendation","text mining","machine learning techniques","machine learning","natural language processing","positive predictive values"]},{"p_id":39314,"title":"Association of deep learning algorithm with fuzzy logic for multidocument text summarization","abstract":"Research in text summarization is predominantly targets on measure of the worth of sentences for a summary. The proposed work has associated the Deep learning algorithm with fuzzy logic to improve the efficiency of the generated summary. The proposed work has two phases, they are training phase and testing phases. The training phase utilized to extract the benefits of fuzzy logic and deep learning algorithm for the efficient summary generation. Similar to every training phase, the proposed training phases is also possessed with well known data and attributes. Latter to the training phase, the testing phases is implemented to check the efficiency of the proposed approach. The experimental evaluation of the proposed work provided the predictable results as, the average precision obtained is 0.37, the average recall is 0.86 and the average f-measure is obtained as 0.50%. \u00a9 2005 - 2014 JATIT & LLS. All rights reserved.","keywords_author":["Deep learning","Fuzzy logic","Multi-document summary","Natural language processing","Neural network"],"keywords_other":null,"max_cite":1.0,"pub_year":2014.0,"sources":"['scp', 'ieee']","rawkeys":["neural network","deep learning","fuzzy logic","natural language processing","multi-document summary"],"tags":["neural networks","fuzzy logic","natural language processing","machine learning","multi-document summary"]},{"p_id":45456,"title":"Predicting motion picture box office performance using temporal tweet patterns","abstract":"\u00a9 2018, Emerald Publishing Limited. Purpose: The purpose of this paper is to investigate temporal tweet patterns and their effectiveness in predicting the financial performance of a movie. Specifically, how tweet patterns are formed prior to and after a movie\u2019s release and their usefulness in predicting a movie\u2019s success is explored. Design\/methodology\/approach: Volume was measured and sentiment analysis was performed on a sample of Tweets posted four days before and after the release of 86 movies. The temporal pattern of tweeting for financially successful movies was compared with those that were financial disappointments. Using temporal tweet patterns, a number of machine learning models were developed and their predictive performance was compared. Findings: Results show that the temporal patterns of tweet volume, length and sentiment differ between \u201chits\u201d and \u201cbusts\u201d in the days surrounding their releases. Compared with \u201cbusts\u201d the tweet pattern for \u201chits\u201d reveal higher volume, shorter length, and more favourable sentiment. Discriminant patterns in social media features occur days in advance of a movie\u2019s release and can be used to develop models for predicting a movie\u2019s success. Originality\/value: Analysis of temporal tweet patterns and their usefulness in predicting box office returns is the main contribution of this research. Results of this research could lead to development of analytical tools allowing motion picture studios to accurately predict and possibly influence the opening night box-office receipts prior to the release of the movie. Also, the specific temporal tweet patterns presented by this work may be applied to problems in other areas of research.","keywords_author":["Box office","Forecasting","Machine learning","Natural language processing","Sentiment analysis","Social media"],"keywords_other":["Predictive performance","Financial performance","Machine learning models","Social media","Box-office receipts","Design\/methodology\/approach","Box office","Box-office performance"],"max_cite":0.0,"pub_year":2018.0,"sources":"['scp']","rawkeys":["predictive performance","box-office receipts","financial performance","design\/methodology\/approach","forecasting","machine learning","machine learning models","natural language processing","social media","box office","box-office performance","sentiment analysis"],"tags":["box-office receipts","financial performance","design\/methodology\/approach","forecasting","machine learning","machine learning models","natural language processing","social media","box office","box-office performance","prediction performance","sentiment analysis"]},{"p_id":1301,"title":"Offline handwritten Chinese character recognition based on DBN fusion model","abstract":"For the past several decades, offline handwritten character recognition is widely and deeply studied. However, the requirements of the identification results are constantly improving in practical applications. This paper presents a new classifier cascaded model to improve the accuracy of offline handwritten Chinese character recognition. New model is the fusion of modified quadratic discriminant function (MQDF) and deep belief network (DBN). The main idea behind MQDF-DBN fusion model is that the significant difference on features and classification mechanisms between MQDF and DBN can complete each other. First to recognize and get result using MQDF, calculate the recognition confidence as evaluation criteria. If recognition confidence is high, the recognition result of MQDF will be output directly. Otherwise, using the DBN to make recognition again and getting the final recognition result. Experiment shows that the fusion model of MQDF and DBN proposed in this paper has achieved better accuracy than the single use of MQDF and DBN in the offline handwritten Chinese character recognition task, which is performed on the ETL-9B handwritten Chinese character dataset.","keywords_author":["offline handwritten character","quadratic discriminant function","deep belief network","Chinese character recognition"],"keywords_other":["Character recognition","Mathematical model","feature mechanism","DBN fusion model","quadratic programming","handwritten character recognition","Handwriting recognition","image classification","classification mechanism","Feature extraction","MQDF","modified quadratic discriminant function","deep belief network","handwriting recognition","Training","belief networks","offline handwritten Chinese character recognition","Neurons","Eigenvalues and eigenfunctions","natural language processing","feature extraction"],"max_cite":null,"pub_year":2016.0,"sources":"['ieee']","rawkeys":["mqdf","feature mechanism","quadratic programming","handwritten character recognition","image classification","classification mechanism","dbn fusion model","offline handwritten character","modified quadratic discriminant function","deep belief network","quadratic discriminant function","offline handwritten chinese character recognition","handwriting recognition","neurons","mathematical model","training","character recognition","belief networks","chinese character recognition","natural language processing","eigenvalues and eigenfunctions","feature extraction"],"tags":["mqdf","feature mechanism","quadratic programming","handwritten character recognition","image classification","classification mechanism","dbn fusion model","offline handwritten character","modified quadratic discriminant function","quadratic discriminant function","offline handwritten chinese character recognition","handwriting recognition","neurons","mathematical model","training","character recognition","belief networks","chinese character recognition","natural language processing","eigenvalues and eigenfunctions","feature extraction","deep belief networks"]},{"p_id":45461,"title":"Sentimental analysis on cognitive data using R","abstract":"\u00a9 The Author(s) 2018. Internet is now vested with new form of societal interactive activities like social media, online portals, feeds, reviews, ratings, posts, critics etc., where people are able to post their expression-of-interest as tweets. Sentiment Analysis (SA) is used for better understanding of such linguistics tweets, extracting features, determine subjectivity and polarity of text located in these tweets. SA inherits text mining approach to process, investigate, and analyze idiosyncratic evidences from text. Now a days, SA was screamed as one of a predictor tool for improvement in knowledge management, revenue generation and decision-making in many businesses firms. The purpose of this work is to leverage a constructive tactic for SA towards dispensation of cognitive information, and seed pragmatic alley to researchers in cognitive science community. This study uses machine learning packages of R language over cognitive data to gain knowledge, discover sentiment polarity and better prediction over the data. To carry out a semantic study over cognitive data we thrived text from numerous numbers of social networking sites. This data was articulated in form of unstructured sentences, words and phrases in a document. Suitable linguistic features are captured to engender dissimilar sentiment polarity and analyze expression-of-interest of user. One of the most prevalent text classification method, Na\u00efve bayes is applied over the text corpus to pinpoint the sentiment and assign its polarity. The connotation in this approaches are evaluated in terms of statistical measures precision, recall, f-measure, and accuracy, thereby these substantial outcomes help to arcade user behavior and predict future trends using SA.","keywords_author":["Artificial intelligence","Classification","Classification","Cognitive data","Cognitive science","Data mining","Machine learning","Natural language processing","Sentiment analysis","Text mining"],"keywords_other":["Expression of interests","Text mining","Interactive activities","Cognitive science","Cognitive data","Sentiment analysis","Social networking sites","Text classification methods"],"max_cite":0.0,"pub_year":2018.0,"sources":"['scp']","rawkeys":["expression of interests","artificial intelligence","text classification methods","data mining","text mining","cognitive science","machine learning","natural language processing","classification","interactive activities","social networking sites","cognitive data","sentiment analysis"],"tags":["expression of interests","text classification methods","data mining","text mining","cognitive science","machine learning","natural language processing","classification","interactive activities","social networking sites","cognitive data","sentiment analysis"]},{"p_id":27033,"title":"Towards retrieving relevant information graphics","abstract":"Information retrieval research has made significant progress in the retrieval of text documents and images. However, relatively little attention has been given to the retrieval of information graphics (non-pictorial images such as bar charts and line graphs) despite their proliferation in popular media such as newspapers and magazines. Our goal is to build a system for retrieving bar charts and line graphs that reasons about the content of the graphic itself in deciding its relevance to the user query. This paper presents the first steps toward such a system, with a focus on identifying the category of intended message of potentially relevant bar charts and line graphs. Our learned model achieves accuracy higher than 80% on a corpus of collected user queries. Copyright \u00a9 2013 ACM.","keywords_author":["Graph Retrieval","Machine Learning","Natural Language Processing","Query Processing"],"keywords_other":["Bar chart","Text document","Graph Retrieval","Information retrieval research","Line graph","User query","NAtural language processing","Information graphics"],"max_cite":6.0,"pub_year":2013.0,"sources":"['scp']","rawkeys":["query processing","graph retrieval","bar chart","machine learning","information graphics","natural language processing","user query","line graph","information retrieval research","text document"],"tags":["query processing","graph retrieval","bar chart","machine learning","information graphics","natural language processing","user query","line graph","information retrieval research","text document"]},{"p_id":35227,"title":"Attempts to verify written English","abstract":"The English language offers a complex and ambiguous grammar that is readily understood by its natural users, but at times can be difficult to grasp by beginners\/learners, and no less, by machines. This paper discusses research and implementations of several techniques towards algorithmically analyzing and verifying the grammatical correctness of sentences in written English. \u00a9 2011 ACM.","keywords_author":["genetic algorithms","grammar checking","machine learning","natural language processing","parsing","tagging"],"keywords_other":["Machine-learning","grammar checking","NAtural language processing","parsing","tagging"],"max_cite":2.0,"pub_year":2011.0,"sources":"['scp']","rawkeys":["tagging","natural language processing","machine learning","genetic algorithms","machine-learning","grammar checking","parsing"],"tags":["natural language processing","machine learning","genetic algorithm","tagging","grammar checking","parsing"]},{"p_id":51611,"title":"An automatic coding system with a three-grade confidence level corresponding to the national\/international occupation and industry standard: Open to the public on the web","abstract":"Copyright \u00a9 2014 SCITEPRESS - Science and Technology Publications All rights reserved.We develop a new automatic coding system with a three-grade confidence level corresponding to each of the national\/international standard code sets for answers to open-ended questions regarding to respondent's occupation and industry in social surveys including a national census. The \"occupation and industry coding\" is a necessary task for statistical processing. However, this task requires a great deal of labor and time-consuming. In addition, inconsistent results occur if the coders are not experts of coding. In formal research, various automatic coding systems have been developed, which are incomplete and generally unfriendly to a non-developer user. Our new system assigns three candidate codes to an answer for coders by SVMs (Support Vector Machines), and attaches a three-grade confidence level to the first-ranked predicted code by using classification scores to support a manual check of the results. The system is now open to the public through the Website of the Social Science Japan Data Archive (SSJDA). After the submitted data file which followed the specified format is approved, the users can obtain files of codes for up to four kinds with a three-grade confidence level. In this paper, we describe our system and evaluate it.","keywords_author":["Answers to open-ended question","Automatic coding system","Confidence level","Machine learning","Natural language processing","Occupation and industry coding"],"keywords_other":["Open-ended questions","Coding system","Occupation and industry coding","Confidence levels","NAtural language processing"],"max_cite":0.0,"pub_year":2014.0,"sources":"['scp']","rawkeys":["answers to open-ended question","open-ended questions","confidence level","machine learning","natural language processing","coding system","confidence levels","occupation and industry coding","automatic coding system"],"tags":["answers to open-ended question","open-ended questions","machine learning","natural language processing","coding system","confidence levels","occupation and industry coding","automatic coding system"]},{"p_id":39332,"title":"Hungarian noun phrase extraction using rule-based and hybrid methods","abstract":"We implement and revise Kornai's grammar of Hungarian NPs [11] to create a parser that identifies noun phrases in Hungarian text. After making several practical amendments to our morphological annotation system of choice, we proceed to formulate rules to account for some specific phenomena of the Hungarian language not covered by the original rule system. Although the performance of the final parser is still inferior to state-of-the-art machine learning methods, we use its output successfully to improve the performance of one such system.","keywords_author":["Machine learning","Natural language processing","Parsing"],"keywords_other":["Noun phrase","Rule based","Parsing","Rule systems","Annotation systems","Hybrid method","Hungarians","State-of-the-art machine learning methods"],"max_cite":1.0,"pub_year":2014.0,"sources":"['scp']","rawkeys":["hungarians","rule systems","rule based","natural language processing","machine learning","state-of-the-art machine learning methods","hybrid method","annotation systems","noun phrase","parsing"],"tags":["hungarians","rule systems","rule based","natural language processing","machine learning","state-of-the-art machine learning methods","hybrid method","annotation systems","noun phrase","parsing"]},{"p_id":51623,"title":"A framework for multilingual real-time spoken dialogue agents","abstract":"\u00a9 2014 IEEE.In this paper, we propose a framework for a spoken dialogue agent that is not dependent on any specific language; it takes some dialogues and sentences as training sets and uses them to acquire knowledge about the target language, then it uses this knowledge to generate several possible responses corresponding to the user input and finally it uses a simple score method to select the best one to show to the user. In aim to be language independent the system only uses very basics treatments and combines them to generate the output sentences. Moreover, all the learning and generation processes are realized in independent threads making the system enable to generate the outputs in real-time. Concretely, the user can input a new sentence at any time and influence the current output generation. We carry out experimentation in two grammaticality different languages and got some results proving our system is efficient to generate responses of a simple dialogue.","keywords_author":["machine learning","multi-thread","multilingual","natural language processing","real-time","spoken dialogue agent"],"keywords_other":["multilingual","NAtural language processing","Multi-thread","Spoken dialogue","Real time"],"max_cite":0.0,"pub_year":2014.0,"sources":"['scp']","rawkeys":["multi-thread","spoken dialogue agent","multilingual","spoken dialogue","real time","real-time","machine learning","natural language processing"],"tags":["spoken dialogue agent","multilingual","multi-threading","spoken dialogue","real time","natural language processing","machine learning"]},{"p_id":424,"title":"Joint acoustic modeling of triphones and trigraphemes by multi-task learning deep neural networks for low-resource speech recognition","abstract":"It is well-known in machine learning that multitask learning (MTL) can help improve the generalization performance of singly learning tasks if the tasks being trained in parallel are related, especially when the amount of training data is relatively small. In this paper, we investigate the estimation of triphone acoustic models in parallel with the estimation of trigrapheme acoustic models under the MTL framework using deep neural network (DNN). As triphone modeling and trigrapheme modeling are highly related learning tasks, a better shared internal representation (the hidden layers) can be learned to improve their generalization performance. Experimental evaluation on three low-resource South African languages shows that triphone DNNs trained by the MTL approach perform significantly better than triphone DNNs that are trained by the single-task learning (STL) approach by ~3-13%. The MTL-DNN triphone models also outperform the ROVER result that combines a triphone STL-DNN and a trigrapheme STL-DNN.","keywords_author":["triphone modeling","trigrapheme modeling","multitask learning","deep neural networks"],"keywords_other":["Joints","speech recognition","Speech processing","Neural networks","South African languages","joint acoustic modeling","machine learning","Speech","multitask learning","learning (artificial intelligence)","Training data","trigrapheme STL-DNN","Training","triphone STL-DNN","triphone acoustic models","neural nets","single-task learning","trigrapheme acoustic models","MTL-DNN triphone models","deep neural networks","natural language processing","Acoustics"],"max_cite":20.0,"pub_year":2014.0,"sources":"['ieee', 'wos']","rawkeys":["south african languages","speech recognition","acoustics","training data","triphone stl-dnn","triphone modeling","joint acoustic modeling","machine learning","multitask learning","trigrapheme modeling","speech","joints","mtl-dnn triphone models","learning (artificial intelligence)","neural networks","training","trigrapheme stl-dnn","triphone acoustic models","neural nets","single-task learning","trigrapheme acoustic models","deep neural networks","natural language processing","speech processing"],"tags":["south african languages","speech recognition","convolutional neural network","acoustics","training data","triphone stl-dnn","triphone modeling","joint acoustic modeling","machine learning","multitask learning","trigrapheme modeling","speech","joints","mtl-dnn triphone models","neural networks","training","trigrapheme stl-dnn","triphone acoustic models","trigrapheme acoustic models","single task learning","natural language processing","speech processing"]},{"p_id":39337,"title":"Timely tip selection for foursquare recommendations","abstract":"This poster summarizes the techniques we use to serve Foursquare tips for a given venue and more specifically the strategies employed for choosing timely and seasonal tips.","keywords_author":["Bhattacharyya coefficient","Context-aware recommenders","Foursquare","Machine learning","Natural language processing","Text classification"],"keywords_other":["Foursquare","Text classification","Bhattacharyya coefficient","Context-Aware","NAtural language processing"],"max_cite":1.0,"pub_year":2014.0,"sources":"['scp']","rawkeys":["context-aware","machine learning","natural language processing","foursquare","context-aware recommenders","text classification","bhattacharyya coefficient"],"tags":["context-aware","machine learning","natural language processing","context-aware recommendations","foursquare","text classification","bhattacharyya coefficient"]},{"p_id":18859,"title":"Prior and contextual emotion of words in sentential context","abstract":"A set of words labeled with their prior emotion is an obvious place to start on the automatic discovery of the emotion of a sentence, but it is clear that context must also be considered. It may be that no simple function of the labels on the individual words captures the overall emotion of the sentence; words are interrelated and they mutually influence their affect-related interpretation. It happens quite often that a word which invokes emotion appears in a neutral sentence, or that a sentence with no emotional word carries an emotion. This could also happen among different emotion classes. The goal of this work is to distinguish automatically between prior and contextual emotion, with a focus on exploring features important in this task. We present a set of features which enable us to take the contextual emotion of a word and the syntactic structure of the sentence into account to put sentences into emotion classes. The evaluation includes assessing the performance of different feature sets across multiple classification methods. We show the features and a promising learning method which significantly outperforms two reasonable baselines. We group our features by the similarity of their nature. That is why another facet of our evaluation is to consider each group of the features separately and investigate how well they contribute to the result. The experiments show that all features contribute to the result, but it is the combination of all the features that gives the best performance. \u00a9 2013 Elsevier Ltd.","keywords_author":["Contextual emotion","Emotion","Machine learning","Natural language processing","Polarity","Prior emotion","Sentiment analysis","Syntactic features"],"keywords_other":["Prior emotion","Emotion","Syntactic features","Sentiment analysis","Polarity","Contextual emotion","NAtural language processing"],"max_cite":22.0,"pub_year":2014.0,"sources":"['scp']","rawkeys":["syntactic features","emotion","prior emotion","natural language processing","machine learning","contextual emotion","sentiment analysis","polarity"],"tags":["syntactic features","emotion","prior emotion","natural language processing","machine learning","contextual emotion","sentiment analysis","polarity"]},{"p_id":39339,"title":"Enhancing a rule-based event coder with semantic vectors","abstract":"\u00a9 2014 Published by Elsevier B.V.Rule based systems have achieved success in applications such as information retrieval and Natural Language Processing. However, due to the rigidity of pattern matching, these systems typically require a large number of rules to adequately cover the variations of expression in unstructured text. Consequently, knowledge engineering for a new domain and knowledge maintenance for a fielded system are labor intensive and expensive. In this paper, we present our research on enhancing a rulebased event coding system by relaxing the rigidity of pattern matching with a technique that formulates and matches patterns of the semantics of words instead of literal words. Our technique pairs literal words with semantic vectors that accumulate word meaning from the context of use of the word found in dictionaries, ontologies, and domain corpora. Our method improves the speed, accuracy, and coverage of the event coding algorithm without additional knowledge engineering effort. Operating on semantics instead of syntax, the improved system eases the workload of human analysts who screen input text for critical events. Our algorithms are based on high-dimensional distributed representations, and their effectiveness and versatility derive from the unintuitive properties of such representations-from the mathematical properties of high-dimensional spaces. Our current implementation encodes words, phrases, and rule patterns as semantic vectors using WordNet, We have started experimental evaluation using a large newswire dataset.","keywords_author":["Event coding","Machine learning","NLP","Random indexing","Rule-based system","Semantic vectors"],"keywords_other":["Semantic vectors","Experimental evaluation","Event coding","High dimensional spaces","NLP","Distributed representation","Random indexing","NAtural language processing"],"max_cite":1.0,"pub_year":2014.0,"sources":"['scp']","rawkeys":["nlp","high dimensional spaces","experimental evaluation","event coding","machine learning","natural language processing","random indexing","distributed representation","rule-based system","semantic vectors"],"tags":["high dimensional spaces","experimental evaluation","event coding","rule-based systems","machine learning","natural language processing","random indexing","distributed representation","semantic vectors"]},{"p_id":37292,"title":"Keyword and Keyphrase Extraction using Newton's Law of Universal Gravitation","abstract":"\u00a9 2017 IEEE. In current times, there has been a surge in the amount of collected data from computational systems. The vast amount of data can be useful in many applications and fields, particularly so in Big Data Analytics. However with a large collection of data there is a difficulty discovering important information. Automatic Document Summarization (ADS) systems are suitable for the task of outlining useful data. The ADS system model takes a text document as input, and outputs a semantically-relevant summary of this information. This information can be further separated and outlined as keywords, or keyphrases. This paper proposes a novel unsupervised approach for automatic keyword and keyphrase generation system using Newton's Law of Universal Gravitation. This approach allows for a complete capture of meaningful text, incorporating the physical structure of a document and discovered relationships between highly related words. Our model uses a new weighting method that combines both the character length of a word, and frequency of a word within a document to simulate a mass. Our model then computes the force of attraction and ranks the word-pair-force as a means of keyword and keyphrase extraction. Experimental results on several text documents demonstrated that the proposed approach improves on the state-of-the-art models.","keywords_author":["Big Data","Data and Text Mining","Document Classification","Document Summarization","Gravitational Fields","Keyword Extraction","Machine Learning","Natural Language Processing","Newtonian Gravity","Newtons Law Of Universal Gravitation"],"keywords_other":["Text mining","Gravitational fields","Universal gravitation","Document summarization","Document Classification","Keyword extraction","Newtonian gravity"],"max_cite":1.0,"pub_year":2017.0,"sources":"['scp']","rawkeys":["data and text mining","text mining","document summarization","universal gravitation","big data","keyword extraction","machine learning","natural language processing","newtons law of universal gravitation","document classification","newtonian gravity","gravitational fields"],"tags":["data and text mining","text mining","document summarization","universal gravitation","big data","keyword extraction","machine learning","natural language processing","newtons law of universal gravitation","document classification","newtonian gravity","gravitational fields"]},{"p_id":51627,"title":"Detection of technology opportunities from patents","abstract":"\u00a9 Research India Publications.Technology Opportunity Discovery is a service to detect and provide opportunities for the new technologies. Patent-based information is extracted by natural language processing techniques. All patents published during the past 20 years are target resources and product names and their Part-of relations are target information. A dictionary and similarity-based named entity recognition, a pattern-based relation extraction, and a machine learning-based filtering have been used and showed an encouraging performance.","keywords_author":["Information extraction","Machine learning","Natural language processing","Technology trend analysis"],"keywords_other":null,"max_cite":0.0,"pub_year":2014.0,"sources":"['scp']","rawkeys":["information extraction","technology trend analysis","natural language processing","machine learning"],"tags":["information extraction","technology trend analysis","natural language processing","machine learning"]},{"p_id":37295,"title":"Legal content fusion for legal information retrieval","abstract":"\u00a9 2017 Association for Computing Machinery. With recent increasing attention to legal information processing, legal information retrieval (IR) has become one of the active research elds. However, there are still many hindrances obtaining rigorous results in legal IR applications in comparison with IR applications for general document retrieval. It is mainly due to the characteristics of legal information such as the complicated structure of legal contents and usage of legal jargon. In this paper, we present a legal IR method, which is a structure-wise IR approach. e presented method in this study focuses on analyzing the contents of legal documents and applying the content contributions to the IR processing. We demonstrate the performance of the proposed IR method with the COILEE data set, which are derived from Japanese bar exams.","keywords_author":["Information retrieval","Legal text mining","Machine learning","Natural language processing"],"keywords_other":["Legal information","Content fusion","Legal information retrieval","Legal documents","Complicated structures","Legal texts","Document Retrieval","IR methods"],"max_cite":1.0,"pub_year":2017.0,"sources":"['scp']","rawkeys":["legal information retrieval","document retrieval","content fusion","legal documents","information retrieval","machine learning","natural language processing","legal texts","complicated structures","ir methods","legal information","legal text mining"],"tags":["legal information retrieval","document retrieval","content fusion","legal documents","information retrieval","machine learning","natural language processing","legal texts","complicated structures","ir methods","legal information","legal text mining"]},{"p_id":27063,"title":"Detection of protein catalytic sites in the biomedical literature","abstract":"This paper explores the application of text mining to the problem of detecting protein functional sites in the biomedical literature, and specifically considers the task of identifying catalytic sites in that literature. We provide strong evidence for the need for text mining techniques that address residue-level protein function annotation through an analysis of two corpora in terms of their coverage of curated data sources. We also explore the viability of building a text-based classifier for identifying protein functional sites, identifying the low coverage of curated data sources and the potential ambiguity of information about protein functional sites as challenges that must be addressed. Nevertheless we produce a simple classifier that achieves a reasonable \u223c69% F-score on our full text silver corpus on the first attempt to address this classification task. The work has application in computational prediction of the functional significance of protein sites as well as in curation workflows for databases that capture this information.","keywords_author":["Biomedical literature","Biomedical natural language processing","Catalytic site","Information extraction","Machine learning","Protein functional sites","Text mining"],"keywords_other":["Text mining","Biomedical literature","Catalytic Domain","Catalytic sites","Computational Biology","Natural Language Processing","Protein functional sites","Artificial Intelligence","Binding Sites","Data Mining","Proteins","Databases, Protein","Amino Acids","Ligands","NAtural language processing"],"max_cite":6.0,"pub_year":2013.0,"sources":"['scp']","rawkeys":["databases","protein functional sites","biomedical natural language processing","biomedical literature","catalytic domain","text mining","information extraction","machine learning","catalytic sites","data mining","amino acids","artificial intelligence","proteins","computational biology","ligands","natural language processing","protein","catalytic site","binding sites"],"tags":["data mining","text mining","databases","information extraction","proteins","protein functional sites","computational biology","amino acids","machine learning","ligands","natural language processing","catalytic sites","binding sites","biomedical natural language processing","biomedical literature","catalytic domain"]},{"p_id":18872,"title":"An insight into the Z-number approach to CWW","abstract":"The Z-number is a new fuzzy-theoretic concept, proposed by Zadeh in 2011. It extends the basic philosophy of Computing With Words (CWW) to include the perception of uncertainty of the information conveyed by a natural language statement. The Z-number thus, serves as a model of linguistic summarization of natural language statements, a technique to merge human-affective perspectives with CWW, and consequently can be envisaged to play a radical role in the domain of CWW-based system design and Natural Language Processing (NLP). This article presents a comprehensive investigation of the Z-number approach to CWW. We present here: a) an outline of our understanding of the generic architecture, algorithm and challenges underlying CWW in general; b) a detailed study of the Z-number methodology - where we propose an algorithm for CWW using Z-numbers, define a Z-number based operator for the evaluation of the level of requirement satisfaction, and describe simulation experiments of CWW utilizing Z-numbers; and c) analyse the strengths and the challenges of the Z-numbers, and suggest possible solution strategies. We believe that this article would inspire research on the need for inclusion of human-behavioural aspects into CWW, as well as the integration of CWW and NLP.","keywords_author":["affective computing","Cognition","dialogue-based systems","fuzzy sets","linguistics","machine learning","natural computing","Natural Language Processing (NLP)","perceptions","soft computing","text-summarization"],"keywords_other":["Affective Computing","text-summarization","Natural Computing","Cognition","NAtural language processing"],"max_cite":22.0,"pub_year":2013.0,"sources":"['scp']","rawkeys":["dialogue-based systems","natural computing","fuzzy sets","text-summarization","linguistics","affective computing","machine learning","natural language processing","natural language processing (nlp)","soft computing","perceptions","cognition"],"tags":["dialogue-based systems","natural computing","fuzzy sets","linguistics","affective computing","text summarization","machine learning","natural language processing","soft computing","perceptions","cognition"]},{"p_id":10681,"title":"Deep learning for pharmacovigilance: recurrent neural network architectures for labeling adverse drug reactions in Twitter posts","abstract":"Objective:Social media is an important pharmacovigilance data source for adverse drug reaction (ADR) identification. Human review of social media data is infeasible due to data quantity, thus natural language processing techniques are necessary. Social media includes informal vocabulary and irregular grammar, which challenge natural language processing methods. Our objective is to develop a scalable, deep-learning approach that exceeds state-of-the-art ADR detection performance in social media.","keywords_author":["natural language processing","neural networks (computer)","adverse drug reaction","social media","Twitter messaging"],"keywords_other":["SOCIAL MEDIA","CLASSIFICATION","WEB","SIGNALS","ELECTRONIC HEALTH RECORDS"],"max_cite":5.0,"pub_year":2017.0,"sources":"['wos']","rawkeys":["signals","adverse drug reaction","neural networks (computer)","social media","natural language processing","electronic health records","classification","web","twitter messaging"],"tags":["signals","neural networks","social media","natural language processing","electronic health records","adverse drug reactions","classification","web","twitter messaging"]},{"p_id":47545,"title":"Knowledge Based Artificial Augmentation Intelligence Technology: Next Step in Academic Instructional Tools for Distance Learning","abstract":"\u00a9 2017, Association for Educational Communications & Technology.With augmented intelligence\/knowledge based system (KBS) it is now possible to develop distance learning applications to support both curriculum and administrative tasks. Instructional designers and information technology (IT) professionals are now moving from the programmable systems era that started in the 1950s to the cognitive computing era. In cognitive computing or KBS a machine understands natural language, adapts, learns, and generates and evaluates hypotheses. A KBS system can manage data and assist instructional designers in creating tools and curricula that generate meaningful applications. As a proof of the concept, the authors conducted an exploratory case study with the input of twenty subject-matter experts (programmers, instructional designers, and content experts) for development of a proto-type KBS scholarly writing software (SWS) application that can be used for distance\/online learning. Philosophical differences between the artificial intelligence and augmented intelligence approaches are also discussed. The role of instructional designers in the development and use of augmented intelligence with IBM\u2019s Watson is also a significant part of the discussion.","keywords_author":["Artificial augmentation","Artificial intelligence","Cognitive computing, knowledge based systems","Distance learning and artificial augmentation applications","IBM Watson and distance learning tools","Machine learning","Natural language processing"],"keywords_other":null,"max_cite":0.0,"pub_year":2017.0,"sources":"['scp']","rawkeys":["artificial intelligence","knowledge based systems","artificial augmentation","machine learning","natural language processing","ibm watson and distance learning tools","distance learning and artificial augmentation applications","cognitive computing"],"tags":["cloud computing","knowledge based systems","artificial augmentation","machine learning","natural language processing","ibm watson and distance learning tools","distance learning and artificial augmentation applications"]},{"p_id":444,"title":"Transfer learning for Latin and Chinese characters with deep neural networks","abstract":"We analyze transfer learning with Deep Neural Networks (DNN) on various character recognition tasks. DNN trained on digits are perfectly capable of recognizing uppercase letters with minimal retraining. They are on par with DNN fully trained on uppercase letters, but train much faster. DNN trained on Chinese characters easily recognize uppercase Latin letters. Learning Chinese characters is accelerated by first pretraining a DNN on a small subset of all classes and then continuing to train on all classes. Furthermore, pretrained nets consistently outperform randomly initialized nets on new tasks with few labeled data.","keywords_author":null,"keywords_other":["Artificial neural networks","Pre-training","Transfer learning","Feature extraction","transfer learning","pretrained nets","Labeled data","learning (artificial intelligence)","Training","character recognition","NIST","Error analysis","uppercase letters","Latin characters","neural nets","Neurons","Chinese characters","minimal retraining","deep neural networks","natural language processing","character recognition tasks"],"max_cite":56.0,"pub_year":2012.0,"sources":"['wos', 'scp', 'ieee']","rawkeys":["error analysis","transfer learning","pretrained nets","neurons","chinese characters","learning (artificial intelligence)","pre-training","training","character recognition","neural nets","uppercase letters","latin characters","nist","minimal retraining","deep neural networks","natural language processing","artificial neural networks","feature extraction","labeled data","character recognition tasks"],"tags":["error analysis","convolutional neural network","transfer learning","machine learning","pretrained nets","neurons","chinese characters","neural networks","pre-training","training","character recognition","uppercase letters","latin characters","nist","minimal retraining","natural language processing","feature extraction","labeled data","character recognition tasks"]},{"p_id":45500,"title":"Analysis of social media posts for early detection of mental health conditions","abstract":"\u00a9 Springer International Publishing AG, part of Springer Nature 2018. This paper presents a multipronged approach to predict early risk of mental health issues from user-generated content in social media. Supervised learning and information retrieval methods are used to estimate the risk of depression for a user given the content of its posts in reddit. The approach presented here was evaluated on the CLEF eRisk 2017 pilot task. We describe the details of five systems submitted to the task, and compare their performance. The comparisons show that combining information retrieval and machine learning methods gives the best results.","keywords_author":["Artificial intelligence","Classification","Information retrieval","Machine learning","Mental health","Natural language processing","Social media","Text mining"],"keywords_other":["Text mining","User-generated content","Mental health","Social media","Pilot tasks","Machine learning methods"],"max_cite":0.0,"pub_year":2018.0,"sources":"['scp']","rawkeys":["machine learning methods","pilot tasks","artificial intelligence","text mining","social media","machine learning","natural language processing","information retrieval","mental health","user-generated content","classification"],"tags":["machine learning methods","pilot tasks","text mining","social media","machine learning","natural language processing","information retrieval","mental health","user-generated content","classification"]},{"p_id":18878,"title":"A pipeline arabic named entity recognition using a hybrid approach","abstract":"Most Arabic Named Entity Recognition (NER) systems have been developed using either of two approaches: A rule-based or Machine Learning (ML) based approach, with their strengths and weaknesses. In this paper, the problem of Arabic NER is tackled through integrating the two approaches together in a pipelined process to create a hybrid system with the aim of enhancing the overall performance of NER tasks. The proposed system is capable of recognizing 11 different types of named entities (NEs): Person, Location, Organization, Date, Time, Price, Measurement, Percent, Phone Number, ISBN and File Name. Extensive experiments are conducted using three different ML classifiers to evaluate the overall performance of the hybrid system. The empirical results indicate that the hybrid approach outperforms both the rule-based and the ML-based approaches. Moreover, our system outperforms the state-of-the-art of Arabic NER in terms of accuracy when applied to ANERcorp dataset, with f-measures 94.4% for Person, 90.1% for Location, and 88.2% for Organization. \u00a9 2012 The COLING.","keywords_author":["Machine learning","Named entity recognition","Natural language processing"],"keywords_other":["Phone number","Named entities","Named entity recognition","File names","Hybrid approach","Rule based","Two Approaches","NAtural language processing"],"max_cite":22.0,"pub_year":2012.0,"sources":"['scp']","rawkeys":["file names","rule based","named entity recognition","machine learning","natural language processing","hybrid approach","phone number","named entities","two approaches"],"tags":["file names","rule based","named entity recognition","machine learning","natural language processing","hybrid approach","phone number","named entities","two approaches"]},{"p_id":22975,"title":"Development and evaluation of RapTAT: A machine learning system for concept mapping of phrases from medical narratives","abstract":"Rapid, automated determination of the mapping of free text phrases to pre-defined concepts could assist in the annotation of clinical notes and increase the speed of natural language processing systems. The aim of this study was to design and evaluate a token-order-specific na\u00efve Bayes-based machine learning system (RapTAT) to predict associations between phrases and concepts. Performance was assessed using a reference standard generated from 2860 VA discharge summaries containing 567,520 phrases that had been mapped to 12,056 distinct Systematized Nomenclature of Medicine - Clinical Terms (SNOMED CT) concepts by the MCVS natural language processing system. It was also assessed on the manually annotated, 2010 i2b2 challenge data. Performance was established with regard to precision, recall, and F-measure for each of the concepts within the VA documents using bootstrapping. Within that corpus, concepts identified by MCVS were broadly distributed throughout SNOMED CT, and the token-order-specific language model achieved better performance based on precision, recall, and F-measure (0.95 \u00b1 0.15, 0.96 \u00b1 0.16, and 0.95 \u00b1 0.16, respectively; mean \u00b1 SD) than the bag-of-words based, na\u00efve Bayes model (0.64 \u00b1 0.45, 0.61 \u00b1 0.46, and 0.60 \u00b1 0.45, respectively) that has previously been used for concept mapping. Precision, recall, and F-measure on the i2b2 test set were 92.9%, 85.9%, and 89.2% respectively, using the token-order-specific model. RapTAT required just 7.2. ms to map all phrases within a single discharge summary, and mapping rate did not decrease as the number of processed documents increased. The high performance attained by the tool in terms of both accuracy and speed was encouraging, and the mapping rate should be sufficient to support near-real-time, interactive annotation of medical narratives. These results demonstrate the feasibility of rapidly and accurately mapping phrases to a wide range of medical concepts based on a token-order-specific na\u00efve Bayes model and machine learning. \u00a9 2013.","keywords_author":["Bayesian prediction","Machine learning","Natural language processing","Systematized nomenclature of medicine"],"keywords_other":["Humans","Reproducibility of Results","Unified Medical Language System","Vocabulary, Controlled","Single discharges","Better performance","Specific languages","Medical concepts","Reference standard","Bayesian predictions","Artificial Intelligence","Tennessee","Algorithms","Bayes Theorem","Databases, Factual","Systematized Nomenclature of Medicine","Hospitals, Veterans","Models, Statistical","Concept mapping","Software","NAtural language processing","Electronic Health Records","Terminology as Topic","Natural Language Processing","Automation","Discharge summary"],"max_cite":11.0,"pub_year":2014.0,"sources":"['scp']","rawkeys":["statistical","vocabulary","unified medical language system","databases","bayesian prediction","reference standard","tennessee","terminology as topic","veterans","machine learning","electronic health records","specific languages","models","hospitals","algorithms","better performance","reproducibility of results","humans","controlled","software","factual","bayes theorem","bayesian predictions","concept mapping","artificial intelligence","medical concepts","natural language processing","single discharges","automation","discharge summary","systematized nomenclature of medicine"],"tags":["vocabulary","automated","databases","reference standard","tennessee","terminology as topic","veterans","machine learning","control","electronic health records","specific languages","hospitals","algorithms","unified medical language systems","better performance","reproducibility of results","humans","software","factual","bayes theorem","bayesian predictions","statistics","model","medical concepts","natural language processing","single discharges","discharge summary","systematized nomenclature of medicine","concept maps"]},{"p_id":4545,"title":"A unified architecture for natural language processing: Deep neural networks with multitask learning","abstract":"We describe a single convolutional neural network architecture that, given a sentence, outputs a host of language processing predictions: part-of-speech tags, chunks, named entity tags, semantic roles, sernantically similar words and the likelihood that the sentence makes sense (grammatically and semantically) using a language model. The entire network is trained jointly on all these tasks using weight-sharing, an instance of multitask learning. All the tasks use labeled data except the language model which is learnt from unlabeled text and represents a novel form of semi-supervised learning for the shared tasks. We show how both multitask learning and semi-supervised learning improve the generalization of the shared tasks, resulting in state-of-the-art performance. Copyright 2008 by the author(s)\/owner(s).","keywords_author":null,"keywords_other":["Labeled datums","State-of-the-art performance","Named entities","Multi-task learning","Part-of-speech tags","Language processing","Multi-task learnings","Semantic roles","Language models","Natural Language Processing","Unified architectures","Speech tags","Semi-supervised learnings","Convolutional neural networks","Semi-supervised learning","Labeled datum","NAtural language processing"],"max_cite":1217.0,"pub_year":2008.0,"sources":"['ieee', 'scp', 'wos']","rawkeys":["named entities","multi-task learning","convolutional neural networks","semi-supervised learnings","state-of-the-art performance","labeled datum","natural language processing","unified architectures","semi-supervised learning","speech tags","language processing","semantic roles","part-of-speech tags","language models","labeled datums","multi-task learnings"],"tags":["named entities","state-of-the-art performance","natural language processing","part of speech tagging","semi-supervised learning","multitask learning","language model","speech tags","unified architecture","convolutional neural network","language processing","semantic roles","labeled datums"]},{"p_id":4546,"title":"Natural language processing (almost) from scratch","abstract":"We propose a unified neural network architecture and learning algorithm that can be applied to various natural language processing tasks including part-of-speech tagging, chunking, named entity recognition, and semantic role labeling. This versatility is achieved by trying to avoid task-specific engineering and therefore disregarding a lot of prior knowledge. Instead of exploiting man-made input features carefully optimized for each task, our system learns internal representations on the basis of vast amounts of mostly unlabeled training data. This work is then used as a basis for building a freely available tagging system with good performance and minimal computational requirements. \u00a9 2011 Ronan Collobert, Jason Weston, L\u00e9on Bottou, Michael Karlen, Koray Kavukcuoglu and Pavel Kuksa.","keywords_author":["Natural language processing","Neural networks"],"keywords_other":["Part of speech tagging","Tagging systems","Training data","Named entity recognition","Prior knowledge","Semantic role labeling","Computational requirements","Internal representation","Input features","NAtural language processing","Unified neural networks"],"max_cite":1571.0,"pub_year":2011.0,"sources":"['ieee', 'scp', 'wos']","rawkeys":["internal representation","computational requirements","neural networks","unified neural networks","named entity recognition","natural language processing","input features","part of speech tagging","tagging systems","semantic role labeling","training data","prior knowledge"],"tags":["internal representation","computational requirements","neural networks","unified neural networks","named entity recognition","natural language processing","input features","part of speech tagging","tagging systems","semantic role labeling","training data","prior knowledge"]},{"p_id":31171,"title":"Extractive text summarization system to aid data extraction from full text in systematic review development","abstract":"\u00a9 2016 Elsevier Inc. Objectives Extracting data from publication reports is a standard process in systematic review (SR) development. However, the data extraction process still relies too much on manual effort which is slow, costly, and subject to human error. In this study, we developed a text summarization system aimed at enhancing productivity and reducing errors in the traditional data extraction process. Methods We developed a computer system that used machine learning and natural language processing approaches to automatically generate summaries of full-text scientific publications. The summaries at the sentence and fragment levels were evaluated in finding common clinical SR data elements such as sample size, group size, and PICO values. We compared the computer-generated summaries with human written summaries (title and abstract) in terms of the presence of necessary information for the data extraction as presented in the Cochrane review's study characteristics tables. Results At the sentence level, the computer-generated summaries covered more information than humans do for systematic reviews (recall 91.2% vs. 83.8%, p < 0.001). They also had a better density of relevant sentences (precision 59% vs. 39%, p < 0.001). At the fragment level, the ensemble approach combining rule-based, concept mapping, and dictionary-based methods performed better than individual methods alone, achieving an 84.7% F-measure. Conclusion Computer-generated summaries are potential alternative information sources for data extraction in systematic review development. Machine learning and natural language processing are promising approaches to the development of such an extractive summarization system.","keywords_author":["Data collection","Machine learning","Review literature as topic","Text classification","Text summarization"],"keywords_other":["Publications","Text classification","Humans","Data collection","Natural Language Processing","Extractive summarizations","Text summarization","Machine Learning","Alternative information","Review Literature as Topic","Scientific publications","Data Mining","Language","Ensemble approaches","NAtural language processing"],"max_cite":3.0,"pub_year":2016.0,"sources":"['scp']","rawkeys":["data mining","publications","language","alternative information","scientific publications","text summarization","machine learning","natural language processing","humans","review literature as topic","ensemble approaches","data collection","extractive summarizations","text classification"],"tags":["data mining","publications","language","alternative information","scientific publications","text summarization","machine learning","natural language processing","humans","review literature as topic","ensemble approaches","data collection","extractive summarizations","text classification"]},{"p_id":51649,"title":"The aid of machine learning to overcome the classification of real health discharge reports written in Spanish Aportaciones de las t\u00e9cnicas de aprendizaje autom\u00e1tico a la clasificaci\u00f3n de partes de alta hospitalarios reales en castellano","abstract":"Hospitals attached to the Spanish Ministry of Health are currently using the International Classification of Diseases 9 Clinical Modification (ICD9-CM) to classify health discharge records. Nowadays, this work is manually done by experts. This paper tackles the automatic classification of real Discharge Records in Spanish following the ICD9-CM standard. The challenge is that the Discharge Records are written in spontaneous language. We explore several machine learning techniques to deal with the classification problem. Random Forest resulted in the most competitive one, achieving an F-measure of 0.876. \u00a9 2014 Sociedad Espa\u00f1ola para el Procesamiento del Lenguaje Natural.","keywords_author":["Biomedicine","Machine learning","Natural language processing"],"keywords_other":null,"max_cite":0.0,"pub_year":2014.0,"sources":"['scp']","rawkeys":["biomedicine","natural language processing","machine learning"],"tags":["biomedicine","natural language processing","machine learning"]},{"p_id":37321,"title":"Generative classification model for categorical data based on latent Gaussian process","abstract":"\u00a9 2017 Elsevier B.V. In many machine learning applications such as computer-aided diagnosis, gene sequence analysis or natural language processing, categorical data appears. For small-scale data set with high dimensions, since relatively small proportion of possible categorical configurations are covered by training samples, conventional methods based on frequency information such as Dirichlet Compound Multinomial distribution usually runs into problems of over-fitting. Latent gaussian process is an effective bayesian non-parametric technique for categorical data modeling, which was proposed as an unsupervised method to embed unlabelled categorical data into a continuous and low-dimensional space through gaussian process. As a probabilistic generative model, latent gaussian process owns the ability of density estimation. In this paper, we propose a generative classification model as a supervised method for labelled categorical data, in which we use latent gaussian process to estimate the class-conditional densities. Since the complexity of gaussian process model can adapt to the size of training data, our method is able to effectively model small-sale categorical data. Experimental results show that our proposal can achieve better classification performance compared with other classification models for categorical data.","keywords_author":["Categorical data","Data mining","Gaussian process","Generative classification model","Machine learning"],"keywords_other":["Categorical data","Classification performance","Gaussian Processes","Multinomial distributions","Generative classifications","Non-parametric techniques","Machine learning applications","NAtural language processing"],"max_cite":1.0,"pub_year":2017.0,"sources":"['scp']","rawkeys":["generative classifications","multinomial distributions","data mining","classification performance","machine learning applications","machine learning","natural language processing","gaussian process","non-parametric techniques","gaussian processes","generative classification model","categorical data"],"tags":["generative classifications","multinomial distributions","data mining","classification performance","machine learning applications","machine learning","natural language processing","non-parametric techniques","gaussian processes","generative classification model","categorical data"]},{"p_id":47563,"title":"Medical concept extraction: A comparison of statistical and semantic methods","abstract":"\u00a9 2017 IEEE.The goal of medical concept extraction is to identify phrases that refer to medical concepts of interest such as problems, treatments and tests from medical documents. In this study, three types of medical concept extraction models are developed and then compared them. The first concept extraction task is mainly based upon semantic features obtained from a domain-knowledge based method using MetaMap, and the other two are machine-learning methods with using sequential classifier Conditional Random Fields (CRF) for both with and without MetaMap outputs as features. Among the three concept extraction models, the combined approach of CRF with MetaMap features obtained the best results.","keywords_author":["CRF","Machine Learning","Medical Concept Extraction","MetaMap","NLP"],"keywords_other":["Sequential classifier","Concept extraction","Medical concepts","Machine learning methods","Semantic features","MetaMap","Conditional random field","Medical documents"],"max_cite":0.0,"pub_year":2017.0,"sources":"['scp']","rawkeys":["machine learning methods","medical concept extraction","sequential classifier","nlp","medical concepts","conditional random field","metamap","machine learning","semantic features","crf","concept extraction","medical documents"],"tags":["machine learning methods","medical concept extraction","sequential classifier","medical concepts","conditional random field","metamap","machine learning","natural language processing","semantic features","concept extraction","medical documents"]},{"p_id":22988,"title":"A generic unsupervised method for decomposing multi-author documents","abstract":"Given an unsegmented multi-author text, we wish to automatically separate out distinct authorial threads. We present a novel, entirely unsupervised, method that achieves strong results on multiple testbeds, including those for which authorial threads are topically identical. Unlike previous work, our method requires no specialized linguistic tools and can be easily applied to any text. \u00a9 2013 ASIS&T.","keywords_author":["machine learning","natural language processing","text mining"],"keywords_other":["Text mining","Unsupervised method","NAtural language processing"],"max_cite":11.0,"pub_year":2013.0,"sources":"['scp']","rawkeys":["natural language processing","unsupervised method","machine learning","text mining"],"tags":["natural language processing","unsupervised method","machine learning","text mining"]},{"p_id":49612,"title":"Block2Vec: A Deep Learning Strategy on Mining Block Correlations in Storage Systems","abstract":"\u00a9 2016 IEEE.Block correlations represent the semantic patterns in storage systems. These correlations can be exploited for data caching, pre-fetching, layout optimization, I\/O scheduling, etc. In this paper, we introduce Block2Vec, a deep learning based strategy to mine the block correlations in storage systems. The core idea of Block2Vec is twofold. First, it proposes a new way to abstract blocks, which are considered as multi-dimensional vectors instead of traditional block Ids. In this way, we are able to capture similarity between blocks through the distances of their vectors. Second, based on vector representation of blocks, it further trains a deep neural network to learn the best vector assignment for each block. We leverage the recently advanced word embedding technique in natural language processing to efficiently train the neural network. To demonstrate the effectiveness of Block2Vec, we design a demonstrative block prediction algorithm based on mined correlations. Empirical comparison based on the simulation of real system traces shows that Block2Vec is capable of mining block-level correlations efficiently and accurately. This research and trial show that the deep learning strategy is a promising direction in optimizing storage system performance.","keywords_author":["block correlation","deep learning","embedding","IO"],"keywords_other":["Deep learning","Multi-dimensional vectors","embedding","Vector representations","Empirical - comparisons","Prediction algorithms","Deep neural networks","NAtural language processing"],"max_cite":0.0,"pub_year":2016.0,"sources":"['scp']","rawkeys":["io","deep learning","deep neural networks","empirical - comparisons","multi-dimensional vectors","natural language processing","prediction algorithms","vector representations","block correlation","embedding"],"tags":["embeddings","empirical - comparisons","neural networks","machine learning","multi-dimensional vectors","natural language processing","prediction algorithms","vector representations","block correlation","convolutional neural network"]},{"p_id":37326,"title":"Computational analysis of religious and ideological linguistic behavior","abstract":"\u00a9 2017 IEEE.In today's global environment, effective communication between groups of diverse ideological beliefs can mean the difference between peaceful negotiations and violent conflict. At the root of communication is language, and researchers at the University of Virginia Center for Religion, Politics, and Conflict (RPC) hypothesize that the analysis of the performative character of a group's discourse (how words are used) provides valuable guidance for how to negotiate with groups of differing ideological beliefs. However, high pressure situations leave little time for an exhaustive analysis of this nature. To address this challenge, this paper expands on the signal processing approach of previous work in the literature, which evaluated the efficacy of a computational approach to applying performative analysis to predict linguistic rigidity. Significantly, this paper evaluates the generalizability of computational performative analysis by considering text from non-religious groups. These include groups focused on political and social agendas rather than religion. The key computational and analytical improvements described in this paper include an enriched judgment selection process and the extraction and analysis of pronoun usage. By examining the raw text of various religious and ideological groups, results show an improved accuracy of 97% for predicting linguistic rigidity, compared to the best predictive accuracy of 83% reported in previous work. These results strengthen the evidence for the hypothesis of the effectiveness of computationally implemented performative analysis as predictive of linguistic rigidity. The results also provide evidence that this approach is applicable to non-religious groups since the predictive accuracy is as consistent with these groups as it is for religious groups.","keywords_author":["Machine learning","Natural language processing","Religious conflict"],"keywords_other":["Religious conflict","Computational analysis","Predictive accuracy","University of Virginia","Global environment","Effective communication","Religious groups","Computational approach"],"max_cite":1.0,"pub_year":2017.0,"sources":"['scp']","rawkeys":["religious groups","predictive accuracy","computational analysis","machine learning","effective communication","global environment","natural language processing","university of virginia","religious conflict","computational approach"],"tags":["religious groups","computational analysis","machine learning","effective communication","global environment","natural language processing","university of virginia","prediction accuracy","religious conflict","computational approach"]},{"p_id":31184,"title":"A review on deep learning approaches in speaker identification","abstract":"\u00a9 2016 ACM.Deep learning (DL) is becoming an increasingly interesting and powerful machine learning method with successful applications in many domains, such as natural language processing, image recognition, hand-written character recognition, and computer vision. Despite of its eminent success, limitations of traditional learning approach may still prevent deep learning from achieving a wide range of realistic learning tasks. DL approaches has shown success in speech recognition and speaker identification over traditional approaches such as those that use Mel Frequency Cepstrum Coefficients for feature extraction with Gaussian Mixture Models. However, speaker identification research community are not fully aware of the DL process and its application with respect to speaker identification. This paper is motivated to reduce this knowledge gap and to promote the research of implementing deep learning techniques for speaker identification. In this paper, we present a review of the DL methodologies used for speaker identification and surveys important DL algorithms that can potentially be explored for future works. We categorised the applications of DL for speaker identification according to the process of speaker identification and presented a review of these implementations.","keywords_author":["Deep learning","Feature extraction","Speaker identification"],"keywords_other":["Gaussian Mixture Model","Speaker identification","Traditional approaches","Mel frequency cepstrum coefficients","Hand written character recognition","Machine learning methods","Traditional learning","NAtural language processing"],"max_cite":3.0,"pub_year":2016.0,"sources":"['scp']","rawkeys":["machine learning methods","mel frequency cepstrum coefficients","deep learning","natural language processing","speaker identification","traditional learning","hand written character recognition","feature extraction","traditional approaches","gaussian mixture model"],"tags":["machine learning methods","machine learning","natural language processing","speaker identification","traditional learning","mel-frequency cepstral coefficients","feature extraction","handwritten character recognition","traditional approaches","gaussian mixture model"]},{"p_id":47568,"title":"Sentiment analysis of social network posts in Slovak language","abstract":"\u00a9 2017 IEEE. In this paper we tackle the issue of sentiment analysis of social network posts in a not well targeted language - Slovak. There is a significant lack of research in this area for minor languages, as they often introduce additional language-specific issues for text processing. In case of Slovak, common issues are high flection, complex morphology and syntax. User-generated content of social networks introduces additional challenges (variability of diacritics, inconsistent style, high error rate) that make the task even harder. In this paper, we propose a method for sentiment analysis of social network posts on Facebook. The proposed method is based on machine learning and incorporates multilevel text pre-processing aiming to deal with specifics of user-generated social content. The evaluation in a real-word setting employing data from Facebook pages of multiple well-known companies shows accuracy of our method comparable with approaches for major world languages.","keywords_author":["machine learning","natural language processing","sentiment analysis","social networks","text classification"],"keywords_other":["User-generated content","Multiple wells","Text classification","Complex morphology","Sentiment analysis","Social contents","Slovak languages","User-generated"],"max_cite":0.0,"pub_year":2017.0,"sources":"['scp']","rawkeys":["complex morphology","text classification","user-generated","multiple wells","machine learning","natural language processing","user-generated content","social networks","slovak languages","social contents","sentiment analysis"],"tags":["complex morphology","text classification","user-generated","multiple wells","machine learning","natural language processing","user-generated content","social networks","slovak languages","social contents","sentiment analysis"]},{"p_id":39379,"title":"Distant supervision for relation extraction using ontology class hierarchy-based features","abstract":"\u00a9 Springer International Publishing Switzerland 2014.Relation extraction is a key step in the problem of structuring natural language text. This paper demonstrates a multi-class classifier for relation extraction, constructed using the distant supervision approach, along with resources of the Semantic Web. In particular, the classifier uses a feature based on the class hierarchy of an ontology that, in conjunction with basic lexical features, improves accuracy and recall. The paper contains extensive experiments, using a corpus extracted from the Wikipedia and the DBpedia ontology, to demonstrate the usefulness of the new feature.","keywords_author":["Distant supervision","Machine learning","Natural language processing","Relation extraction","Semantic web"],"keywords_other":null,"max_cite":1.0,"pub_year":2014.0,"sources":"['scp']","rawkeys":["natural language processing","machine learning","distant supervision","semantic web","relation extraction"],"tags":["natural language processing","machine learning","distant supervision","semantic web","relation extraction"]},{"p_id":25044,"title":"Identifying thesis and conclusion statements in student essays to scaffold peer review","abstract":"Peer-reviewing is a recommended instructional technique to encourage good writing. Peer reviewers, however, may fail to identify key elements of an essay, such as thesis and conclusion statements, especially in high school writing. Our system identifies thesis and conclusion statements, or their absence, in students' essays in order to scaffold reviewer reflection. We showed that computational linguistics and interactive machine learning have the potential to facilitate peer-review processes. \u00a9 2014 Springer International Publishing Switzerland.","keywords_author":["discourse analysis","high school writing instruction","interactive machine learning","natural language processing","Peer-review"],"keywords_other":["Writing instruction","Peer review","NAtural language processing","Interactive machine learning","Discourse analysis"],"max_cite":8.0,"pub_year":2014.0,"sources":"['scp']","rawkeys":["high school writing instruction","interactive machine learning","peer-review","natural language processing","discourse analysis","peer review","writing instruction"],"tags":["high school writing instruction","interactive machine learning","natural language processing","discourse analysis","peer review","writing instruction"]},{"p_id":18901,"title":"Machine transliteration survey","abstract":"Machine transliteration is the process of automatically transforming the script of a word from a source language to a target language, while preserving pronunciation. The development of algorithms specifically for machine transliteration began over a decade ago based on the phonetics of source and target languages, followed by approaches using statistical and language-specific methods. In this survey, we review the key methodologies introduced in the transliteration literature. The approaches are categorized based on the resources and algorithms used, and the effectiveness is compared. \u00a9 2011 ACM.","keywords_author":["Automatic translation","Machine learning","Machine transliteration","Natural language processing","Transliteration evaluation"],"keywords_other":["Transliteration evaluation","Automatic translation","Machine transliteration","Machine learning","Natural language processing"],"max_cite":22.0,"pub_year":2011.0,"sources":"['scp']","rawkeys":["automatic translation","natural language processing","machine learning","machine transliteration","transliteration evaluation"],"tags":["automatic translation","natural language processing","machine learning","machine transliteration","transliteration evaluation"]},{"p_id":45523,"title":"Supervised topic models for diagnosis code assignment to discharge summaries","abstract":"\u00a9 Springer International Publishing AG, part of Springer Nature 2018. Mining medical data has significantly gained interest in the recent years thanks to the advances in data mining and machine learning fields. In this work, we focus on a challenging issue in medical data mining: automatic diagnosis code assignment to discharge summaries, i.e., characterizing patient\u2019s hospital stay (diseases, symptoms, treatments, etc.) with a set of codes usually derived from the International Classification of Diseases (ICD). We cast the problem as a machine learning task and we experiment some recent approaches based on the probabilistic topic models. We demonstrate the efficiency of these models in terms of high predictive scores and ease of result interpretation. As such, we show how topic models enable gaining insights into this field and provide new research opportunities for possible improvements.","keywords_author":["ICD code assignment","Machine learning","Natural language processing","Text categorization","Text mining","Topic models"],"keywords_other":["Text mining","Medical data mining","Code assignments","Text categorization","International classification of disease","Research opportunities","Topic model","Probabilistic topic models"],"max_cite":0.0,"pub_year":2018.0,"sources":"['scp']","rawkeys":["code assignments","international classification of disease","text mining","machine learning","natural language processing","icd code assignment","topic model","research opportunities","medical data mining","text categorization","topic models","probabilistic topic models"],"tags":["code assignments","text mining","topic modeling","machine learning","natural language processing","icd code assignment","research opportunities","international classification of diseases","medical data mining","text categorization","probabilistic topic models"]},{"p_id":39386,"title":"Multi-entity polarity analysis in financial documents","abstract":"Copyright \u00a9 2014 ACM. amount of information available in the Internet does not allowing manual content analysis to identify information of interest. Thus automated analyses are used to identify information of interest, and one increasingly important approach is the polarity analysis. Polarity analysis is the classification of a text document in positive, negative, and neutral, according to a certain topic. This classification of information is particularly useful in the finance domain, where news about a company can affect the performance of its stocks. Although most of the methods in financial domain consider that the whole document is associated with a particular entity, this is not always the case. In fact, it is common that authors cite several entities in a single document and these entities are cited with different polarity. Accordingly, the objective of this paper was to study strategies for polarity detection in financial documents with multiple entities. Specifically, we studied methods based on learning of multiple models, one for each observed entity, using SVM classifiers. We evaluated models based on the partition of documents into fragments according to the entities they cite. We used several heuristics to segment documents based on shallow and deep natural language processing (NLP). We found that entity-specific models created by partitioning the document collection into segments outperformed the strategy based on the use of entire documents. We also observed that more complex segmentation using anaphora resolution was not able to outperform a low-cost approach, based on simple string matching.","keywords_author":["Anaphora Resolution","Document Engineering","Machine Learning","Sentiment Analysis","Web Data Annotation"],"keywords_other":["Document collection","Web data","Amount of information","Sentiment analysis","Automated analysis","Document engineering","Anaphora resolution","NAtural language processing"],"max_cite":1.0,"pub_year":2014.0,"sources":"['scp']","rawkeys":["automated analysis","amount of information","machine learning","natural language processing","document collection","web data annotation","anaphora resolution","sentiment analysis","web data","document engineering"],"tags":["automated analysis","amount of information","machine learning","natural language processing","document collection","web data annotation","anaphora resolution","sentiment analysis","web data","document engineering"]},{"p_id":37339,"title":"Hybrid text-based deception models for native and non-native English cybercriminal networks","abstract":"\u00a9 2017 Association for Computing Machinery. Cybercriminals are increasingly using Internet messaging to exploit their victims. We develop and apply a text-based deception detection approach to build hybrid models for detecting cybercrime in the text Internet communications from native and non-native English speaking cybercriminal networks, where our models use both computational linguistics (CL) and psycholinguistic (PL) features. We study four types of deception-based cybercrime: fraud, scam, favorable fake reviews, and unfavorable fake reviews. We build two types of generalized hybrid models for both native and non-native English speaking cybercriminal networks: 2-dataset and 3-dataset hybrid models using Na\u00efve Bayes, Support Vector Machines, and kth Nearest Neighbor algorithms. All 2-dataset models are trained on any two forms of cybercrime in different web genres, which are then used to detect and analyze other types of cybercrime in web genres that were not part of the training set to establish model generalizability. Similarly, the 3-dataset models are trained on any three forms of cybercrime in different web genres, that are also used to detect and analyze cybercrime in a web genre that was not part of the training set. Model performance on the test datasets ranges from 60% to 80% accuracy, with the best performance on detection of unfavorable reviews and fraud, and notable differences emerged between detection in messages from native and non-native English speaking groups. Our work may be applied as provider-or user-based filtering tools to identify cybercriminal actors and block or label undesirable messages before they reach their intended targets.","keywords_author":["Computational linguistics","Cybercrime","Deception","Machine learning","Natural language processing","Psycholinguistics"],"keywords_other":["Cybercrime","Internet communication","Deception detection","Psycholinguistics","Model performance","Deception","K-th nearest neighbors","Filtering tools"],"max_cite":1.0,"pub_year":2017.0,"sources":"['scp']","rawkeys":["deception","cybercrime","psycholinguistics","filtering tools","machine learning","natural language processing","deception detection","computational linguistics","k-th nearest neighbors","internet communication","model performance"],"tags":["deception","cybercrime","psycholinguistics","filtering tools","machine learning","natural language processing","deception detection","computational linguistics","k-th nearest neighbors","internet communication","model performance"]},{"p_id":23006,"title":"The Z-number enigma: A study through an experiment","abstract":"The Z-number, proposed by Zadeh in the year 2011, is a new fuzzy-theoretic approach to the Computing With Words (CWW) paradigm. It aspires to capture the uncertainty of information conveyed by a sentence, and serve as a model for the precisiation and linguistic summarization of a natural language statement. The Z-number thereby, lends a new dimension to CWW - uniting CWW with Natural Language Processing (NLP). This article is an illumination upon our exploration of the Z-number approach to CWW. Here, we enlist the probable contributions of the Z-number to CWW, present our algorithm for CWW using the Z-number, and describe a simulation of the technique with respect to a real-life example of CWW. In the course of the simulation, we extend the interpretation of the set-theoretic intersection operator to evaluate the intersection of perceptions and discover some of the challenges underlying the implementation of the Z-number in the area of CWW. \u00a9 2013 Springer-Verlag Berlin Heidelberg.","keywords_author":["cognition","Computing With Words (CWW)","fuzzy sets","linguistics","machine learning","natural computing","natural language processing","perceptions","soft computing","text summarization"],"keywords_other":null,"max_cite":11.0,"pub_year":2013.0,"sources":"['scp']","rawkeys":["natural computing","fuzzy sets","linguistics","text summarization","machine learning","computing with words (cww)","natural language processing","soft computing","perceptions","cognition"],"tags":["natural computing","fuzzy sets","linguistics","text summarization","machine learning","computing with words (cww)","natural language processing","soft computing","perceptions","cognition"]},{"p_id":16865,"title":"Recent automatic text summarization techniques: a survey","abstract":"\u00a9 2016, Springer Science+Business Media Dordrecht.As information is available in abundance for every topic on internet, condensing the important information in the form of summary would benefit a number of users. Hence, there is growing interest among the research community for developing new approaches to automatically summarize the text. Automatic text summarization system generates a summary, i.e. short length text that includes all the important information of the document. Since the advent of text summarization in 1950s, researchers have been trying to improve techniques for generating summaries so that machine generated summary matches with the human made summary. Summary can be generated through extractive as well as abstractive methods. Abstractive methods are highly complex as they need extensive natural language processing. Therefore, research community is focusing more on extractive summaries, trying to achieve more coherent and meaningful summaries. During a decade, several extractive approaches have been developed for automatic summary generation that implements a number of machine learning and optimization techniques. This paper presents a comprehensive survey of recent text summarization extractive approaches developed in the last decade. Their needs are identified and their advantages and disadvantages are listed in a comparative manner. A few abstractive and multilingual text summarization approaches are also covered. Summary evaluation is another challenging issue in this research field. Therefore, intrinsic as well as extrinsic both the methods of summary evaluation are described in detail along with text summarization evaluation conferences and workshops. Furthermore, evaluation results of extractive summarization approaches are presented on some shared DUC datasets. Finally this paper concludes with the discussion of useful future directions that can help researchers to identify areas where further research is needed.","keywords_author":["Artificial intelligence","Information retrieval","Natural language processing","Summarization survey","Text mining","Text summarization"],"keywords_other":["Text mining","Extractive approach","Research communities","Extractive summarizations","Text summarization","Automatic text summarization","Optimization techniques","NAtural language processing"],"max_cite":33.0,"pub_year":2017.0,"sources":"['scp']","rawkeys":["extractive approach","artificial intelligence","text mining","optimization techniques","summarization survey","natural language processing","text summarization","information retrieval","research communities","extractive summarizations","automatic text summarization"],"tags":["extractive approach","text mining","optimization techniques","summarization survey","machine learning","natural language processing","information retrieval","text summarization","research communities","extractive summarizations","automatic text summarization"]},{"p_id":53729,"title":"Automatic prediction of trauma registry procedure codes from emergency room dictations","abstract":"Current natural language processing techniques for recognition of concepts in the electronic medical record have been insufficient to allow their broad use for coding information automatically. We have undertaken a preliminary investigation into the use of machine learning methods to recognize procedure codes from emergency room dictations for a trauma registry. Our preliminary results indicate moderate success, and we believe future enhancements with additional learning techniques and selected natural language processing approaches will be fruitful. \u00a9 1998 IMIA. All rights reserved.","keywords_author":["Coding","Machine Learning","Natural Language Processing"],"keywords_other":null,"max_cite":0.0,"pub_year":1998.0,"sources":"['scp']","rawkeys":["natural language processing","coding","machine learning"],"tags":["natural language processing","codes","machine learning"]},{"p_id":53733,"title":"Sentence analysis using a concept lattice","abstract":"\u00a9 Springer-Verlag Berlin Heidelberg 1998. Grammatically incorrect sentences result either from an unknown (possibly misspelled) word, an incorrect word order or even an omitted\/redundant word. Sentences with these errors are a bottle-neck to NLP systems because they cannot be parsed correctly. Human beings are able to overcome this problem (either occurring in spoken or written language) since they are capable of doing a semantic similarity search to find out if a similar utterance has been heard before or a syntactic similarity search for a stored utterance that shares structural similarities with the input. If the syntactic and semantic analysis of the rest of the input can be done correctly, then a `gap' that exists in the utterance, can be uniquely identified. In this paper, a system named SAUCOLA which is based on a concept lattice, that mimics human skills in resolving knowledge gaps that exist in written language is presented. The preliminary results show that correct stored sentences can be retrieved based on the words contained in the incorrect input sentence.","keywords_author":["Concept lattice","Example-based machine translation","Machine learning","Machine translation","Natural language processing"],"keywords_other":["Concept Lattices","Syntactic similarities","Example based machine translations","Semantic similarity","Semantic analysis","Structural similarity","Machine translations","NAtural language processing"],"max_cite":0.0,"pub_year":1998.0,"sources":"['scp']","rawkeys":["example-based machine translation","syntactic similarities","machine learning","natural language processing","machine translation","semantic analysis","semantic similarity","concept lattice","concept lattices","machine translations","example based machine translations","structural similarity"],"tags":["example-based machine translation","syntactic similarities","natural language processing","machine learning","semantic analysis","semantic similarity","concept lattices","machine translations","structural similarity"]},{"p_id":51687,"title":"Portuguese part-of-speech tagging with large margin structure learning","abstract":"\u00a9 2014 IEEE.Part-of-Speech Tagging is a fundamental task on many Natural Language Processing systems. This task consists in identifying the syntactic category, i.e. the part of speech, of each word in a sentence. Despite the fact that the current state-of-the-art accuracy for this task is around 97%, any improvement has an immediate impact on more complex tasks, like Parsing, Semantic Role Labeling and Information Extraction. Thus, it is still relevant to explore this task. In this paper, we introduce a part-of-speech tagger based on the Structure Learning framework that reduces the smallest known error on the Portuguese Mac-Morpho corpus by 7.8%. We also apply our tagger to a recently revised version of Mac-Morpho. Our system accuracy on this latter version is competitive with a semi-supervised Neural Network trained on Mac-Morpho plus a very large non-annotated corpus. Additionally, our system is simpler than previous systems and uses a very limited feature set. Our system employs a Large Margin training criteria to derive a structure predictor that is more robust on unseen data.","keywords_author":["Machine Learning","Natural Language Processing","POS Tagging","Structure Learning"],"keywords_other":["Part of speech tagging","PoS tagging","Large margin trainings","Structure-learning","Semantic role labeling","Part-of-speech tagger","NAtural language processing","Non-annotated corpus"],"max_cite":0.0,"pub_year":2014.0,"sources":"['scp']","rawkeys":["structure learning","large margin trainings","non-annotated corpus","natural language processing","machine learning","part of speech tagging","part-of-speech tagger","semantic role labeling","pos tagging","structure-learning"],"tags":["large margin trainings","non-annotated corpus","natural language processing","machine learning","part of speech tagging","part-of-speech tagger","semantic role labeling","pos tagging","structure-learning"]},{"p_id":43496,"title":"Visual Dialog","abstract":"IEEE We introduce the task of Visual Dialog, which requires an AI agent to hold a meaningful dialog with humans in natural, conversational language about visual content. Specifically, given an image, a dialog history, and a question about the image, the agent has to ground the question in image, infer context from history, and answer the question accurately. Visual Dialog is disentangled enough from a specific downstream task so as to serve as a general test of machine intelligence, while being sufficiently grounded in vision to allow objective evaluation of individual responses and benchmark progress. We develop a novel two-person real-time chat data-collection protocol to curate a large-scale Visual Dialog dataset (VisDial). VisDial v0.9 has been released and consists of <formula><tex>$\\sim 1.2M$<\/tex><\/formula> dialog question-answer pairs from 10-round, human-human dialogs grounded in <formula><tex>$\\sim 120k$<\/tex><\/formula> images from the COCO dataset.","keywords_author":["Artificial intelligence","computer vision","History","machine learning","natural language processing","Natural languages","Protocols","Task analysis","visual dialog","Visualization","Wheelchairs"],"keywords_other":["visual dialog","Data collection protocols","Objective evaluation","Question-answer pairs","Machine intelligence","Natural languages","Task analysis","Visual content"],"max_cite":0.0,"pub_year":2018.0,"sources":"['scp']","rawkeys":["machine intelligence","task analysis","artificial intelligence","visual dialog","visual content","question-answer pairs","natural languages","machine learning","data collection protocols","natural language processing","objective evaluation","wheelchairs","protocols","history","computer vision","visualization"],"tags":["machine intelligence","task analysis","visual dialog","visual content","question-answer pairs","natural languages","machine learning","data collection protocols","natural language processing","objective evaluation","wheelchairs","protocols","history","computer vision","visualization"]},{"p_id":18921,"title":"A comparison of tools for detecting fake websites","abstract":"As fake website developers become more innovative, so too must the tools used to protect Internet users. A proposed system combines a support vector machine classifier and a rich feature set derived from website text, linkage, and images to better detect fraudulent sites. \u00a9 2009 IEEE.","keywords_author":["Artificial Intelligence","Computer systems organization","Computing methodologies","Machine learning","Natural language processing","Web text analysis"],"keywords_other":null,"max_cite":22.0,"pub_year":2009.0,"sources":"['scp']","rawkeys":["web text analysis","artificial intelligence","natural language processing","machine learning","computer systems organization","computing methodologies"],"tags":["web text analysis","natural language processing","machine learning","computer systems organization","computing methodologies"]},{"p_id":33257,"title":"Polish texts topic classification evaluation","abstract":"Copyright \u00a9 2018 by SCITEPRESS \u2013 Science and Technology Publications, Lda. All rights reserved. The paper presents preparation, lead and results of evaluation of efficiency of text classification (TC) methods for Polish. The subject language is of complex morphology, it belongs to flexional languages. Thus there is a strong need of making proper text preprocessing in order to guarantee reliable TC. Basing on authors\u2019 practical experience from former TC, IR and general NLP experiments set of preprocessing rules was applied. Also feature-documents matrix was designed with respect to the most promising feature selected. About 216 experiments on exemplar corpus in subject (topic) classification task, with different preprocessing, weighting, filtering (for dimensions reduction) schemes and classifiers was conducted. Results shows there is not substantial increase of accuracy when using most of classical pre-processing steps in case of corpus of large size (at least 1000 exemplars per class). The highest impact authors were able to obtain concerned the system costs of TC processes, not the TC accuracy.","keywords_author":["Feature Selection","NLP","Polish","Supervised Machine Learning","Text Classification","Weighting Schema"],"keywords_other":["Practical experience","Weighting Schema","Topic Classification","Text classification","Classification tasks","Complex morphology","Pre-processing step","Supervised machine learning"],"max_cite":2.0,"pub_year":2018.0,"sources":"['scp']","rawkeys":["complex morphology","nlp","weighting schema","practical experience","classification tasks","feature selection","polish","topic classification","pre-processing step","supervised machine learning","text classification"],"tags":["complex morphology","weighting schema","natural language processing","practical experience","classification tasks","feature selection","polish","topic classification","pre-processing step","supervised machine learning","text classification"]},{"p_id":49650,"title":"Sequence Prediction of Driving Behavior Using Double Articulation Analyzer","abstract":"\u00a9 2013 IEEE. A sequence prediction method for driving behavior data is proposed in this paper. The proposed method can predict a longer latent state sequence of driving behavior data than conventional sequence prediction methods. The proposed method is derived by focusing on the double articulation structure latently embedded in driving behavior data. The double articulation structure is a two-layer hierarchical structure originally found in spoken language, i.e., a sentence is a sequence of words and a word is a sequence of letters. Analogously, we assume that driving behavior data comprise a sequence of driving words and a driving word is a sequence of driving letters. The sequence prediction method is obtained by extending a nonparametric Bayesian unsupervised morphological analyzer using a nested Pitman-Yor language model (NPYLM), which was originally proposed in the natural language processing field. This extension allows the proposed method to analyze incomplete sequences of latent states of driving behavior and to predict subsequent latent states on the basis of a maximum a posteriori criterion. The extension requires a marginalization technique over an infinite number of possible driving words. We derived such a technique on the basis of several characteristics of the NPYLM. We evaluated this proposed sequence prediction method using three types of data: 1) synthetic data; 2) data from test drives around a driving course at a factory; and 3) data from drives on a public thoroughfare. The results showed that the proposed method made better long-term predictions than did the previous methods.","keywords_author":["Bayesian nonparametrics","driving behavior data","machine learning","prediction"],"keywords_other":["Articulation structures","Hierarchical structures","Maximum a posteriori criteria","Non-parametric Bayesian","Morphological analyzer","Driving behavior","NAtural language processing","Bayesian nonparametrics"],"max_cite":0.0,"pub_year":2016.0,"sources":"['scp']","rawkeys":["driving behavior data","hierarchical structures","maximum a posteriori criteria","morphological analyzer","non-parametric bayesian","prediction","machine learning","natural language processing","driving behavior","articulation structures","bayesian nonparametrics"],"tags":["driving behavior data","hierarchical structures","maximum a posteriori criteria","morphological analyzer","non-parametric bayesian","prediction","machine learning","natural language processing","driving behavior","articulation structures","bayesian nonparametrics"]},{"p_id":25075,"title":"What you want is not what you get: Predicting sharing policies for text-based content on facebook","abstract":"As the amount of content users publish on social networking sites rises, so do the danger and costs of inadvertently sharing content with an unintended audience. Studies repeatedly show that users frequently misconfigure their policies or misunderstand the privacy features offered by social networks. A way to mitigate these problems is to develop automated tools to assist users in correctly setting their policy. This paper explores the viability of one such approach: we examine the extent to which machine learning can be used to deduce users' sharing preferences for content posted on Facebook. To generate data on which to evaluate our approach, we conduct an online survey of Facebook users, gathering their Facebook posts and associated policies, as well as their intended privacy policy for a subset of the posts. We use this data to test the efficacy of several algorithms at predicting policies, and the effects on prediction accuracy of varying the features on which they base their predictions. We find that Facebook's default behavior of assigning to a new post the privacy settings of the preceding one correctly assigns policies for only 67% of posts. The best of the prediction algorithms we tested outperforms this baseline for 80% of participants, with an average accuracy of 81%; this equates to a 45% reduction in the number of posts with misconfigured policies. Further, for those participants (66%) whose implemented policy usually matched their intended policy, our approach predicts the correct privacy settings for 94% of posts. \u00a9 2013 ACM.","keywords_author":["facebook","machine learning","natural language processing","privacy","social network"],"keywords_other":["Default behavior","Social networking sites","Facebook","Prediction accuracy","Prediction algorithms","Privacy policies","NAtural language processing","Privacy Settings"],"max_cite":8.0,"pub_year":2013.0,"sources":"['scp']","rawkeys":["facebook","privacy","machine learning","natural language processing","prediction algorithms","social network","prediction accuracy","privacy settings","social networking sites","default behavior","privacy policies"],"tags":["facebook","privacy","machine learning","natural language processing","prediction algorithms","social networks","prediction accuracy","privacy settings","social networking sites","default behavior","privacy policies"]},{"p_id":49651,"title":"The data you have\u22ef Tomorrow's information business","abstract":"\u00a9 2016 IOS Press and the authors. How do you curate your data today to ensure you can capitalize on it to build a successful information business of tomorrow? Can artificial intelligence support and enhance human intelligence? What activities might help us build information services that are essentially the foundation of artificial intelligence in this information community? Where is the best source of user behavioral data? This paper will attempt to answer these questions and more regarding the status of artificial intelligence today.","keywords_author":["Artificial intelligence","automated language processing","automatic indexing","automatic translation","Bayesian analysis","computational linguistics","inference engines","machine learning","natural language processing","neural nets","text analytics","vector spaces","word co-occurrence"],"keywords_other":["Automatic translation","Bayesian Analysis","Language processing","Word co-occurrence","Text analytics","NAtural language processing"],"max_cite":0.0,"pub_year":2016.0,"sources":"['scp']","rawkeys":["artificial intelligence","automatic translation","automatic indexing","text analytics","vector spaces","word co-occurrence","language processing","machine learning","natural language processing","inference engines","computational linguistics","automated language processing","bayesian analysis","neural nets"],"tags":["automatic translation","vector spaces","automatic indexing","text analytics","inference engine","language processing","neural networks","word co-occurrence","machine learning","natural language processing","computational linguistics","automated language processing","bayesian analysis"]},{"p_id":27125,"title":"Emotion recognition of weblog sentences based on an ensemble algorithm of multi-label classification and word emotions","abstract":"Weblogs have greatly changed the communication ways of mankind. Affective analysis of blog posts is found valuable for many applications such as text-to-speech synthesis or computer-assisted recommendation. Traditional emotion recognition in text based on single-label classification can not satisfy higher requirements of affective computing. In this paper, the automatic identification of sentence emotion in weblogs is modeled as a multi-label text categorization task. Experiments are carried out on 12273 blog sentences from the Chinese emotion corpus Ren-CECps with 8-dimension emotion annotation. An ensemble algorithm RAKEL is used to recognize dominant emotions from the writer's perspective. Our emotion feature using detailed intensity representation for word emotions outperforms the other main features such as the word frequency feature and the traditional lexicon-based feature. In order to deal with relatively complex sentences, we integrate grammatical characteristics of punctuations, disjunctive connectives, modification relations and negation into features. It achieves 13.51% and 12.49% increases for Micro-averaged F1 and Macro-averaged F1 respectively compared to the traditional lexicon-based feature. Result shows that multiple-dimension emotion representation with grammatical features can efficiently classify sentence emotion in a multi-label problem. \u00a9 2012 The Institute of Electrical Engineers of Japan.","keywords_author":["Emotion Recognition","Machine Learning","Multi-label Classification","Natural Language Processing","Weblog"],"keywords_other":["Affective Computing","Automatic identification","Emotion representation","Ensemble algorithms","Computer assisted","Multi-label","Text categorization","Weblog","Complex sentences","Emotion recognition","Weblogs","Word frequencies","Emotion feature","NAtural language processing"],"max_cite":6.0,"pub_year":2012.0,"sources":"['scp']","rawkeys":["computer assisted","multi-label classification","emotion representation","multi-label","affective computing","complex sentences","word frequencies","machine learning","emotion feature","natural language processing","ensemble algorithms","weblogs","emotion recognition","automatic identification","weblog","text categorization"],"tags":["emotion representation","computer-assisted","multi-label","affective computing","complex sentences","word frequencies","machine learning","emotion feature","natural language processing","ensemble algorithms","weblogs","emotion recognition","automatic identification","multi label classification","text categorization"]},{"p_id":10747,"title":"Ask Your Neurons: A Deep Learning Approach to Visual Question Answering","abstract":"We propose a Deep Learning approach to the visual question answering task, where machines answer to questions about real-world images. By combining latest advances in image representation and natural language processing, we propose Ask Your Neurons, a scalable, jointly trained, end-to-end formulation to this problem. In contrast to previous efforts, we are facing a multi-modal problem where the language output (answer) is conditioned on visual and natural language inputs (image and question). We evaluate our approaches on the DAQUAR as well as the VQA dataset where we also report various baselines, including an analysis how much information is contained in the language part only. To study human consensus, we propose two novel metrics and collect additional answers which extend the original DAQUAR dataset to DAQUAR-Consensus. Finally, we evaluate a rich set of design choices how to encode, combine and decode information in our proposed Deep Learning formulation.","keywords_author":["Computer vision","Deep learning","Natural language processing","Scene understanding","Visual question answering","Visual turing test","Computer vision","Scene understanding","Deep learning","Natural language processing","Visual turing test","Visual question answering"],"keywords_other":["Multimodal problems","Question Answering","Scene understanding","COEFFICIENT","Learning formulation","Image representations","Question Answering Task","Natural languages","Turing tests"],"max_cite":1.0,"pub_year":2017.0,"sources":"['ieee', 'scp', 'wos']","rawkeys":["question answering","multimodal problems","learning formulation","coefficient","deep learning","question answering task","visual turing test","natural languages","natural language processing","visual question answering","scene understanding","turing tests","computer vision","image representations"],"tags":["multimodal problems","learning formulation","question answering task","visual turing test","natural languages","machine learning","natural language processing","information retrieval","scene understanding","turing tests","computer vision","coefficients","video quality assessment","image representation"]},{"p_id":39422,"title":"Parameterized spatial SQL translation for geographic question answering","abstract":"Spatial SQL (structured query language) is a powerful tool for systematically solving geographic problems, however, it has not been widely applied to the problem of geographic question answering. This paper introduces a parameterized approach to translate natural language geographic questions into spatial SQLs. In particular, three types of complexity are introduced and initial solutions are proposed to deal with these complexities. The entire parameterization process is implemented to generate spatial SQL templates for five types of geographic questions. It is suggested that our approach is useful for solving natural geographic problems using spatial functions such as those in a GIS. \u00a9 2014 IEEE.","keywords_author":["GIS","machine learning","natural language processing","spatial artificial intelligence","spatial ontology","spatial query"],"keywords_other":["Initial solution","Question Answering","Structured Query Language","Spatial functions","Natural languages","Spatial queries","NAtural language processing","Spatial ontologies"],"max_cite":1.0,"pub_year":2014.0,"sources":"['scp']","rawkeys":["gis","question answering","spatial queries","spatial functions","natural languages","natural language processing","machine learning","initial solution","spatial ontologies","spatial artificial intelligence","structured query language","spatial query","spatial ontology"],"tags":["gis","spatial function","natural languages","natural language processing","machine learning","initial solution","information retrieval","spatial artificial intelligence","structured query language","spatial query","spatial ontology"]},{"p_id":29184,"title":"Challenges of Sentiment Analysis for Dynamic Events","abstract":"\u00a9 2001-2011 IEEE. Efforts to assess people's sentiments on Twitter have suggested that Twitter could be a valuable resource for studying political sentiment and that it reflects the offline political landscape. Many opinion mining systems and tools provide users with people's attitudes toward products, people, or topics and their attributes\/aspects. However, although it may appear simple, using sentiment analysis to predict election results is difficult, since it is empirically challenging to train a successful model to conduct sentiment analysis on tweet streams for a dynamic event such as an election. This article highlights some of the challenges related to sentiment analysis encountered during monitoring of the presidential election using Kno.e.sis's Twitris system.","keywords_author":["artificial intelligence","deep learning","intelligent systems","machine learning","natural language processing","sentiment analysis","sentiment for dynamic events","sentiment for volatile content","social media analysis"],"keywords_other":["Social media analysis","Volatile contents","Dynamic events","Offline","Sentiment analysis","Presidential election","Opinion mining"],"max_cite":4.0,"pub_year":2017.0,"sources":"['scp']","rawkeys":["volatile contents","artificial intelligence","offline","deep learning","dynamic events","sentiment for dynamic events","machine learning","natural language processing","intelligent systems","presidential election","social media analysis","sentiment for volatile content","opinion mining","sentiment analysis"],"tags":["volatile contents","offline","dynamic events","sentiment for dynamic events","natural language processing","machine learning","intelligent systems","presidential election","social media analysis","sentiment for volatile content","opinion mining","sentiment analysis"]},{"p_id":43523,"title":"Automatic infection detection based on electronic medical records","abstract":"\u00a9 2018 The Author(s). Background: Making accurate patient care decision, as early as possible, is a constant challenge, especially for physicians in the emergency department. The increasing volumes of electronic medical records (EMRs) open new horizons for automatic diagnosis. In this paper, we propose to use machine learning approaches for automatic infection detection based on EMRs. Five categories of information are utilized for prediction, including personal information, admission note, vital signs, diagnose test results and medical image diagnose. Results: Experimental results on a newly constructed EMRs dataset from emergency department show that machine learning models can achieve a decent performance for infection detection with area under the receiver operator characteristic curve (AUC) of 0.88. Out of all the five types of information, admission note in text form makes the most contribution with the AUC of 0.87. Conclusions: This study provides a state-of-the-art EMRs processing system to automatically make medical decisions. It extracts five types of features associated with infection and achieves a decent performance on automatic infection detection based on machine learning models.","keywords_author":["Automatic disease detection","Electronic medical records","Infection detection","Machine learning","Natural language processing"],"keywords_other":["Medical decision making","Machine learning approaches","Machine learning models","Emergency departments","Receiver operator characteristic curves","Electronic medical records (EMRs)","Disease detection","Electronic medical record"],"max_cite":0.0,"pub_year":2018.0,"sources":"['scp']","rawkeys":["electronic medical records (emrs)","machine learning models","disease detection","machine learning","infection detection","natural language processing","automatic disease detection","electronic medical records","electronic medical record","receiver operator characteristic curves","medical decision making","machine learning approaches","emergency departments"],"tags":["receiver operating characteristics","machine learning models","disease detection","machine learning","infection detection","natural language processing","automatic disease detection","electronic medical record","medical decision making","machine learning approaches","emergency departments"]},{"p_id":47620,"title":"Identifying harm events in clinical care through medical narratives","abstract":"\u00a9 2017 ACM. Preventable medical errors are estimated to be among the leading causes of injury and death in the United States. To prevent such errors, healthcare systems have implemented patient safety and incident reporting systems. These systems enable clinicians to report unsafe conditions and cases where patients have been harmed due to errors in medical care. These reports are narratives in natural language and while they provide detailed information about the situation, it is non-trivial to perform large scale analysis for identifying common causes of errors and harm to the patients. In this work, we present a method for identifying harm events in patient care and categorize the harm event types based on their severity level. We show that our method which is based on convolutional and recurrent networks with an attention mechanism is able to significantly improve over the existing methods on two large scale datasets of patient reports.","keywords_author":["Deep learning","Medical text","Natural language processing"],"keywords_other":["Large-scale analysis","Medical text","Incident reporting systems","Large-scale datasets","Natural languages","Recurrent networks","Health-care system","Attention mechanisms"],"max_cite":0.0,"pub_year":2017.0,"sources":"['scp']","rawkeys":["deep learning","large-scale analysis","attention mechanisms","large-scale datasets","natural language processing","natural languages","medical text","recurrent networks","health-care system","incident reporting systems"],"tags":["large-scale analysis","attention mechanisms","machine learning","large-scale datasets","natural language processing","medical text","natural languages","recurrent networks","health-care system","incident reporting systems"]},{"p_id":27142,"title":"Exploring a corpus-based approach for detecting language impairment in monolingual English-speaking children","abstract":"Objectives: This paper explores the use of an automated method for analyzing narratives of monolingual English speaking children to accurately predict the presence or absence of a language impairment. The goal is to exploit corpus-based approaches inspired by the fields of natural language processing and machine learning. Methods and materials: We extract a large variety of features from language samples and use them to train language models and well known machine learning algorithms as the underlying predictors. The methods are evaluated on two different datasets and three language tasks. One dataset contains samples of two spontaneous narrative tasks performed by 118 children with an average age of 13 years and a second dataset contains play sessions from over 600 younger children with an average age of 6 years. Results: We compare results against a cut off baseline method and show that our results are far superior, reaching F-measures of over 85% in two of the three language tasks, and 48% in the third one. Conclusions: The different experiments we present here show that corpus based approaches can yield good prediction results in the problem of language impairment detection. These findings warrant further exploration of natural language processing techniques in the field of communication disorders. Moreover, the proposed framework can be easily adapted to analyze samples in languages other than English since most of the features are language independent or can be customized with little effort. \u00a9 2011 Elsevier B.V.","keywords_author":["Analysis of orthographic transcriptions","Language impairment","Machine learning","Monolingual English-speaking children","Natural language processing"],"keywords_other":["Monolingual English-speaking children","Orthographic transcription","Machine-learning","Language impairment","NAtural language processing"],"max_cite":6.0,"pub_year":2011.0,"sources":"['scp']","rawkeys":["orthographic transcription","monolingual english-speaking children","machine learning","natural language processing","machine-learning","analysis of orthographic transcriptions","language impairment"],"tags":["orthographic transcription","monolingual english-speaking children","machine learning","natural language processing","analysis of orthographic transcriptions","language impairment"]},{"p_id":27143,"title":"Proximity-based sentiment analysis","abstract":"Sentiment analysis seeks to characterize opinionated or evaluative aspects of natural language text thus helping people to discover valuable information from large amounts of unstructured data [1]. In this paper we explore a new methodology for sentiment analysis called proximity-based sentiment analysis. We take a different approach, by considering a new set of features based on word proximities in a written text. We propose three proximity-based features, namely, proximity distribution, mutual information between proximity types, and proximity patterns. We applied this approach to the analysis of movie reviews. Our experimental results show that proximity-based sentiment analysis is able to extract sentiments from a specific domain, with performance comparable to the state-of-the-art. To the best of our knowledge, this is the first attempt at focusing on only proximity based features as the primary features in sentiment analysis. \u00a9 2011 IEEE.","keywords_author":["machine learning","movie reviews","natural language processing","sentiment analysis","text mining"],"keywords_other":["Text mining","movie reviews","Written texts","Sentiment analysis","Natural language text","Unstructured data","Mutual informations","NAtural language processing"],"max_cite":6.0,"pub_year":2011.0,"sources":"['scp']","rawkeys":["text mining","movie reviews","natural language processing","machine learning","written texts","unstructured data","mutual informations","sentiment analysis","natural language text"],"tags":["text mining","movie reviews","mutual information","natural language processing","machine learning","written texts","unstructured data","sentiment analysis","natural language text"]},{"p_id":33290,"title":"Different Approaches to Assessing the Quality of Explanations Following a Multiple-Document Inquiry Activity in Science","abstract":"\u00a9 2017, International Artificial Intelligence in Education Society.This article describes several approaches to assessing student understanding using written explanations that students generate as part of a multiple-document inquiry activity on a scientific topic (global warming). The current work attempts to capture the causal structure of student explanations as a way to detect the quality of the students\u2019 mental models and understanding of the topic by combining approaches from Cognitive Science and Artificial Intelligence, and applying them to Education. First, several attributes of the explanations are explored by hand coding and leveraging existing technologies (LSA and Coh-Metrix). Then, we describe an approach for inferring the quality of the explanations using a novel, two-phase machine-learning approach for detecting causal relations and the causal chains that are present within student essays. The results demonstrate the benefits of using a machine-learning approach for detecting content, but also highlight the promise of hybrid methods that combine ML, LSA and Coh-Metrix approaches for detecting student understanding. Opportunities to use automated approaches as part of Intelligent Tutoring Systems that provide feedback toward improving student explanations and understanding are discussed.","keywords_author":["Automatic assessment","Causal relations","Causal structure","Explanations","Machine learning","Mental models","Natural language processing"],"keywords_other":["Automated approach","Machine learning approaches","Automatic assessment","Causal relations","Mental model","Explanations","Intelligent tutoring system","Multiple documents"],"max_cite":2.0,"pub_year":2017.0,"sources":"['scp']","rawkeys":["automated approach","explanations","mental models","machine learning","multiple documents","natural language processing","causal relations","machine learning approaches","intelligent tutoring system","causal structure","mental model","automatic assessment"],"tags":["automated approach","explanations","mental models","machine learning","multiple documents","natural language processing","causal relations","machine learning approaches","intelligent transportation systems","causal structure","automatic assessment"]},{"p_id":8715,"title":"Guiding semi-supervision with constraint-driven learning","abstract":"Over the last few years, two of the main research directions in machine learning of natural language processing have been the study of semi-supervised learning algorithms as a way to train classifiers when the labeled data is scarce, and the study of ways to exploit knowledge and global information in structured learning tasks. In this paper, we suggest a method for incorporating domain knowledge in semi-supervised learning algorithms. Our novel framework unifies and can exploit several kinds of task specific constraints. The experimental results presented in the information extraction domain demonstrate that applying constraints helps the model to generate better feedback during learning, and hence the framework allows for high performance learning with significantly less training data than was possible before on these tasks. \u00a9 2007 Association for Computational Linguistics.","keywords_author":null,"keywords_other":["Labeled data","Training data","Global informations","Domain knowledge","Research directions","Information Extraction","Structured learning","Semi-supervised learning","NAtural language processing"],"max_cite":91.0,"pub_year":2007.0,"sources":"['scp']","rawkeys":["structured learning","global informations","information extraction","research directions","natural language processing","semi-supervised learning","domain knowledge","labeled data","training data"],"tags":["training data","global informations","information extraction","research directions","natural language processing","semi-supervised learning","domain knowledge","labeled data","structure-learning"]},{"p_id":522,"title":"Parsing Clinical Text: How Good are the state-of-the-art Deep Learning Based Parsers?","abstract":"A dependency parser generates both a syntactic structure and a shallow semantic structure of a sentence. It is a fundamental component of natural language processing (NLP) based pipelines, which are critical to facilitate research using the Electronic Health Records (EHR). However, current works mainly apply parsers developed in the general English domain to clinical text. There are no formal evaluations and comparisons of deep learning based dependency parsers in the medical domain. No state-of-the-art dependency parsing performance has been established on clinical text, either. In this study, we investigated the performance of four state-ofthe-art deep learning based dependency parsers, Stanford parser, Bist-parser, dependency_tf parser and jPTDP parser, respectively. Experiments for evaluation are conducted on two datasets: (1) The MiPACQ Treebank and (2) A Treebank of progress notes. Our results showed that the original parsers achieved lower performance in clinical text compared to general English text. After retraining on the clinical Treebank, all parsers obtained better performance. Besides, using word embeddings from Gigaword and MIMICIII yielded comparable performance. Interestingly, the transition-based parsers demonstrated stronger generalizability on different treebanks than the graph-based parsers. Overall, Bist-parser achieved the best performance on MiPACQ (88.95% UAS, 92.69% LS, 86.10% LAS). Stanford parser achieved the best performance on progress notes (84.01% UAS, 89\/97% LS, 80.72% LAS).","keywords_author":["Dependency parser, clinical text processing, deep learning, clinical TreeBanks"],"keywords_other":["MIMICs","Medical services","Conferences","Pipelines","Machine learning","Syntactics","Natural language processing"],"max_cite":null,"pub_year":2018.0,"sources":"['ieee']","rawkeys":["pipelines","conferences","clinical text processing","deep learning","machine learning","mimics","natural language processing","dependency parser","syntactics","medical services","clinical treebanks"],"tags":["pipelines","conferences","clinical t ext processing","machine learning","mimics","natural language processing","dependency parser","syntactics","medical services","clinical treebanks"]},{"p_id":37390,"title":"Semantics-Enhanced Online Intellectual Capital Mining Service for Enterprise Customer Centers","abstract":"\u00a9 2016 IEEE.One of the greatest challenges of an enterprise's service center is to ensure that their engineers and customers are provided with the right information in a timely fashion. For this purpose, modern organizations operate a wide range of information support systems to assist customers with critical service requests and to provide proactive monitoring, where possible, to prevent service requests from occurring in the first place. It is often the case that relevant information is scattered over the Internet and\/or maintained on disparate systems, buried in large amount of noisy data, and in heterogeneous formats, thereby complicating the access to reusable knowledge and extending the response time to reach a resolution. To address these challenges, in this paper we propose an effective knowledge mining solution to improve the quality of service request resolution. We model the service resolution problem as an online search and classification problem, and use domain knowledge in the form of ontology to guide effective machine learning. Our proposed solution has been extensively evaluated with experiments and has been used in a real enterprise customer center.","keywords_author":["business rules","data mining","event processing","Knowledge management service","machine learning","natural language processing","ontology","production rule system","semantic web"],"keywords_other":["Service resolutions","Production rules","Intellectual capital","Proactive Monitoring","Business rules","Enterprise customers","Event Processing","Information support systems"],"max_cite":1.0,"pub_year":2017.0,"sources":"['scp']","rawkeys":["production rules","data mining","service resolutions","information support systems","business rules","ontology","machine learning","natural language processing","production rule system","semantic web","enterprise customers","intellectual capital","knowledge management service","event processing","proactive monitoring"],"tags":["production rules","data mining","service resolutions","information support systems","business rules","machine learning","natural language processing","production rule system","semantic web","enterprise customers","intellectual capital","knowledge management service","event processing","proactive monitoring"]},{"p_id":37391,"title":"ZK DrugResist 2.0: A TextMiner to extract semantic relations of drug resistance from PubMed","abstract":"\u00a9 2017 Elsevier Inc. Extracting useful knowledge from an unstructured textual data is a challenging task for biologists, since biomedical literature is growing exponentially on a daily basis. Building an automated method for such tasks is gaining much attention of researchers. ZK DrugResist is an online tool that automatically extracts mutations and expression changes associated with drug resistance from PubMed. In this study we have extended our tool to include semantic relations extracted from biomedical text covering drug resistance and established a server including both of these features. Our system was tested for three relations, Resistance (R), Intermediate (I) and Susceptible (S) by applying hybrid feature set. From the last few decades the focus has changed to hybrid approaches as it provides better results. In our case this approach combines rule-based methods with machine learning techniques. The results showed 97.67% accuracy with 96% precision, recall and F-measure. The results have outperformed the previously existing relation extraction systems thus can facilitate computational analysis of drug resistance against complex diseases and further can be implemented on other areas of biomedicine.","keywords_author":["Drug resistance","Hybrid approach","Machine learning","NLP","Relation extraction","Rule based methods"],"keywords_other":["Relation extraction","Biomedical literature","Computational analysis","Drug resistance","Humans","Semantics","Rule-based method","PubMed","Machine Learning","Drug Resistance","Hybrid approach","Semantic relations","Machine learning techniques"],"max_cite":1.0,"pub_year":2017.0,"sources":"['scp']","rawkeys":["drug resistance","semantic relations","nlp","computational analysis","semantics","machine learning techniques","machine learning","humans","hybrid approach","rule based methods","rule-based method","relation extraction","biomedical literature","pubmed"],"tags":["drug resistance","semantic relations","computational analysis","semantics","machine learning techniques","machine learning","natural language processing","humans","hybrid approach","rule-based method","relation extraction","biomedical literature","pubmed"]},{"p_id":37392,"title":"A Sentiment Analysis System to Improve Teaching and Learning","abstract":"\u00a9 1970-2012 IEEE. Natural language processing and machine learning can be applied to student feedback to help university administrators and teachers address problematic areas in teaching and learning. The proposed system analyzes student comments from both course surveys and online sources to identify sentiment polarity, the emotions expressed, and satisfaction versus dissatisfaction. A comparison with direct-Assessment results demonstrates the system's reliability.","keywords_author":["Advances in Learning Technologies","computing in education","data analysis","education","machine learning","natural language processing","project management","sentiment analysis","SRS","student response system"],"keywords_other":["Sentiment analysis","Computing in education","Learning technology","Student-response system","NAtural language processing"],"max_cite":1.0,"pub_year":2017.0,"sources":"['scp']","rawkeys":["education","computing in education","learning technology","student-response system","machine learning","natural language processing","advances in learning technologies","student response system","srs","data analysis","sentiment analysis","project management"],"tags":["education","stimulated raman-scattering","learning technology","machine learning","natural language processing","advances in learning technologies","student response system","sentiment analysis","data analysis","computers in education","project management"]},{"p_id":39441,"title":"Extracting semantic information from patent claims using phrasal structure annotations","abstract":"\u00a9 2014 IEEE.The rapid change of trading values from tangible assets to Intelectual Property has put both businesses and academia in a race to acquire and protect the rights to exploit such property. This is mainly accomplished in the form of patent issuing by the governments, being time consuming and complicated due to the vast amount of documents that need to be analyzed in order to assert the novelty or validity of a patent application. Patent information retrieval research is thus growing quickly to support document analysis across multiple domains and information systems. One of the big challenges in patent analysis is the identification of the elements of innovation (concepts, processes, materials) and the relations between them, in the patent text. This paper presents a method for extracting semantic information from patent claims by using semantic annotations on phrasal structures, abstracting domain ontology information and outputting ontology-friendly structures to achieve generalization. An extraction system built upon the method is briefly evaluated on a document sample from INPI, the Brazilian patent office, a challenging information source.","keywords_author":["Machine Learning","Natural Language Processing","Patent Information Extraction","Semantic Relation Extraction","Semantic Segmentation"],"keywords_other":["Information sources","Semantic relation extractions","Semantic annotations","Patent applications","Semantic segmentation","NAtural language processing","Semantic information","Extraction systems"],"max_cite":1.0,"pub_year":2014.0,"sources":"['scp']","rawkeys":["semantic relation extraction","semantic annotations","natural language processing","machine learning","information sources","patent applications","semantic segmentation","semantic information","patent information extraction","extraction systems","semantic relation extractions"],"tags":["semantic annotations","natural language processing","machine learning","information sources","patent applications","semantic segmentation","semantic information","patent information extraction","extraction systems","semantic relation extractions"]},{"p_id":31250,"title":"Analyzing and evaluating security features in software requirements","abstract":"\u00a9 2016 IEEE. Software requirements, for complex projects, often contain specifications of non-functional attributes (e.g., security-related features). The process of analyzing such requirements is laborious and error prone. Due to the inherent free-flowing nature of software requirements, it is tempting to apply Natural Language Processing (NLP) based Machine Learning (ML) techniques for analyzing these documents from the point of view of comprehensiveness and consistency. In this paper, we propose novel semi-Automatic methodology that can assess the security requirements of the software system from the perspective of completeness, contradiction, and inconsistency. Security standards introduced by the ISO are used to construct a model for classifying security-based requirements using NLP-based ML techniques. Hence, this approach aims to identify the appropriate structures that underlie software requirement documents. Once such structures are formalized and empirically validated, they will provide guidelines to software organizations for generating comprehensive and unambiguous requirement specification documents as related to security-oriented features. The proposed solution will assist organizations during the early phases of developing secure software and reduce overall development effort and costs.","keywords_author":["Concept Graphs","Machine Learning","Natural Language Processing","Quality of Service","Security","Software Requirements"],"keywords_other":["Software organization","Software requirements","Concept graph","Software requirement documents","Security","Security requirements","Requirement specification","NAtural language processing"],"max_cite":3.0,"pub_year":2016.0,"sources":"['scp']","rawkeys":["security requirements","software organization","concept graph","software requirement documents","software requirements","natural language processing","machine learning","requirement specification","security","quality of service","concept graphs"],"tags":["security requirements","software organization","concept graph","software requirement documents","software requirements","natural language processing","machine learning","security","quality of service","requirements specifications"]},{"p_id":27155,"title":"Text categorization for assessing multiple documents integration, or John Henry visits a data mine","abstract":"A critical need for students in the digital age is to learn how to gather, analyze, evaluate, and synthesize complex and sometimes contradictory information across multiple sources and contexts. Yet reading is most often taught with single sources. In this paper, we explore techniques for analyzing student essays to give feedback to teachers on how well their students deal with multiple texts. We compare the performance of a simple regular expression matcher to Latent Semantic Analysis and to Support Vector Machines, a machine learning approach. \u00a9 2011 Springer-Verlag Berlin Heidelberg.","keywords_author":["Corpus Analysis","Machine Learning","Natural Language Processing"],"keywords_other":["Text categorization","Machine-learning","Regular expressions","Latent Semantic Analysis","Multiple source","Corpus analysis","Single source","Student essays","Multiple documents","NAtural language processing","Digital age"],"max_cite":6.0,"pub_year":2011.0,"sources":"['scp']","rawkeys":["corpus analysis","student essays","digital age","multiple documents","machine learning","multiple source","natural language processing","text categorization","single source","machine-learning","regular expressions","latent semantic analysis"],"tags":["corpus analysis","student essays","digital age","multiple documents","machine learning","multiple source","natural language processing","text categorization","single source","regular expressions","latent semantic analysis"]},{"p_id":53776,"title":"Multilingual Question Classification based on surface text features","abstract":"This paper presents a multilingual approach to Question Classification based on machine learning, using language independent features. This way we obtain a system flexible and easily adaptable to new languages. Using a parallel corpus in English and Spanish, we test the performance of the system with three different techniques: Support Vector Machines, Memory-based Learning and Maximum Entropy. \u00a9 2005 The authors. All rights reserved.","keywords_author":["Machine learning","Natural language processing","Question classification"],"keywords_other":null,"max_cite":0.0,"pub_year":2005.0,"sources":"['scp']","rawkeys":["natural language processing","question classification","machine learning"],"tags":["natural language processing","question classification","machine learning"]},{"p_id":51730,"title":"Enabling 'question answering' in the MBAT vector symbolic architecture by exploiting orthogonal random matrices","abstract":"Vector Symbolic Architectures (VSA) are methods designed to enable distributed representation and manipulation of semantically-structured information, such as natural languages. Recently, a new VSA based on multiplication of distributed vectors by random matrices was proposed, this is known as Matrix-Binding-of-Additive-Terms (MBAT). We propose an enhancement that introduces an important additional feature to MBAT: the ability to 'unbind' symbols. We show that our method, which exploits the inherent properties of orthogonal matrices, imparts MBAT with the 'question answering' ability found in other VSAs. We compare our results with another popular VSA that was recently demonstrated to have high utility in brain-inspired machine learning applications. \u00a9 2014 IEEE.","keywords_author":["brain-inspired machine learning","complex structure methodology","distributed representation","natural language processing","vector symbolic architecture"],"keywords_other":["Complex structure","Orthogonal matrix","Question Answering","Brain-inspired","Natural languages","Distributed representation","Machine learning applications","NAtural language processing"],"max_cite":0.0,"pub_year":2014.0,"sources":"['scp']","rawkeys":["question answering","vector symbolic architecture","complex structure methodology","complex structure","brain-inspired machine learning","machine learning applications","natural languages","natural language processing","orthogonal matrix","brain-inspired","distributed representation"],"tags":["vector symbolic architecture","complex structure methodology","complex structure","brain-inspired machine learning","machine learning applications","natural languages","natural language processing","orthogonal matrix","brain-inspired","information retrieval","distributed representation"]},{"p_id":37398,"title":"Deep learning algorithms based text classifier","abstract":"\u00a9 2016 IEEE. There exists a base classification system for classification of problem tickets in the Enterprise domain. Different deep learning algorithms (Gated Recursive Unit and Long Short Term Memory) were investigated for solving the classification problem. Experiments were conducted for different parameters and layers for these algorithms. Paper brings out the architectures tried, results obtained, our conclusions and way forward.","keywords_author":["Accuracy","AdaDelta","Adagrad","Adam","Classification","Deep Learning","Embedding Layer","Gated Recursive Units","Long Short Term Memory","Machine Learning","Natural Language Processing","Optimizer","Ridge classifier"],"keywords_other":["Embedding Layer","Adagrad","Adam","Accuracy","Optimizers","AdaDelta","Gated Recursive Units"],"max_cite":1.0,"pub_year":2017.0,"sources":"['scp']","rawkeys":["accuracy","optimizer","optimizers","deep learning","adagrad","machine learning","natural language processing","adam","ridge classifier","adadelta","classification","embedding layer","gated recursive units","long short term memory"],"tags":["accuracy","adagrad","long short-term memory","machine learning","natural language processing","adam","adadelta","classification","optimization","embedding layer","gated recursive units","ridge classifier"]},{"p_id":14871,"title":"Semantic Similarity and Relatedness between Clinical Terms: An Experimental Study","abstract":"Automated approaches to measuring semantic similarity and relatedness can provide necessary semantic context information for information retrieval applications and a number of fundamental natural language processing tasks including word sense disambiguation. Challenges for the development of these approaches include the limited availability of validated reference standards and the need for better understanding of the notions of semantic relatedness and similarity in medical vocabulary. We present results of a study in which eight medical residents were asked to judge 724 pairs of medical terms for semantic similarity and relatedness. The results of the study confirm the existence of a measurable mental representation of semantic relatedness between medical terms that is distinct from similarity and independent of the context in which the terms occur. This study produced a validated publicly available dataset for developing automated approaches to measuring semantic relatedness and similarity.","keywords_author":null,"keywords_other":["Humans","Semantics","Natural Language Processing","Information Storage and Retrieval","Vocabulary"],"max_cite":53.0,"pub_year":2010.0,"sources":"['scp', 'wos']","rawkeys":["vocabulary","semantics","natural language processing","humans","information storage and retrieval"],"tags":["vocabulary","semantics","natural language processing","humans","information storage and retrieval"]},{"p_id":21016,"title":"Seventy years beyond neural networks: retrospect and prospect","abstract":"\u00a9 2016, Science Press. All right reserved.As a typical realization of connectionism intelligence, neural network, which tries to mimic the information processing patterns in the human brain by adopting broadly interconnected structures and effective learning mechanisms, is an important branch of artificial intelligence and also a useful tool in the research on brain-like intelligence at present. During the course of seventy years' development, it once received doubts, criticisms and ignorance, but also enjoyed prosperity and gained a lot of outstanding achievements. From the M-P neuron and Hebb learning rule developed in 1940s, to the Hodykin-Huxley equation, perceptron model and adaptive filter developed in 1950s, to the self-organizing mapping neural network, Neocognitron, adaptive resonance network in 1960s, many neural computation models have become the classical methods in the field of signal processing, computer vision, natural language processing and optimization calculation. Currently, as a way to imitate the complex hierarchical cognition characteristic of human brain, deep learning brings an important trend for brain-like intelligence. With the increasing number of layers, deep neural network entitles machines the capability to capture \"abstract concepts\" and it has achieved great success in various fields, leading a new and advanced trend in neural network research. This paper recalls the development of neural network, summarizes the latest progress and existing problems considering neural network and points out its possible future directions.","keywords_author":["Artificial intelligence","Big data","Deep learning","Machine learning","Neural network","Parallel computing"],"keywords_other":["Deep learning","Adaptive resonance networks","Neural computation models","Self-organizing mapping","Interconnected structures","Optimization calculation","Deep neural networks","NAtural language processing"],"max_cite":15.0,"pub_year":2016.0,"sources":"['scp']","rawkeys":["artificial intelligence","neural network","big data","deep learning","deep neural networks","adaptive resonance networks","parallel computing","machine learning","natural language processing","self-organizing mapping","optimization calculation","interconnected structures","neural computation models"],"tags":["big data","neural networks","parallel computing","adaptive resonance networks","machine learning","natural language processing","self-organizing map","convolutional neural network","optimization calculation","interconnected structures","neural computation models"]},{"p_id":45589,"title":"Stylometry analysis of literary texts in Polish","abstract":"\u00a9 Springer International Publishing AG, part of Springer Nature 2018. In this work we compare different methods for deriving features for text representation in two stylometric tasks of gender and author recognition. The first group of methods uses the Bag-of-Words (BoW) approach, which represents the documents with vectors of frequencies of selected features occurring in the documents. We analyze features such as the most frequent 1000 lemmas, word forms, all lemmas, selected (content insensitive) lemmas, bigrams of grammatical classes and mixture of bigrams of grammatical classes, selected lemmas and punctuations. Moreover, the approach based on the recently proposed fastText algorithm (for vector based representation of text) is also applied. We evaluate these different approaches on two publicly available collections of Polish literary texts from late 19th- and early 20th-century: one consisting of 99 novels from 33 authors and the second one 888 novels from 58 authors. Our study suggests that depending on the corpora the best are the style features (grammatical bigrams) or semantic features (1000 lemmas extracted from the training set). We also noticed the importance of proper division of corpora into training and testing sets.","keywords_author":["Bag of words","Machine learning","Natural language processing","Polish","Stylometric","Text analysis"],"keywords_other":["Vector-based representations","Literary texts","Stylometric","Text analysis","Training and testing","Text representation","Semantic features","Bag of words"],"max_cite":0.0,"pub_year":2018.0,"sources":"['scp']","rawkeys":["literary texts","bag of words","stylometric","text representation","vector-based representations","natural language processing","machine learning","semantic features","training and testing","polish","text analysis"],"tags":["literary texts","bag of words","stylometrics","text representation","vector-based representations","natural language processing","machine learning","semantic features","training and testing","polish","text analysis"]},{"p_id":43539,"title":"Identifying Trends in Technologies and Programming Languages Using Topic Modeling","abstract":"\u00a9 2018 IEEE. Technology question and answer websites are a great source of technical knowledge. Users of these websites raise various types of technical questions, and answer them. These questions cover a wide range of domains in Computer Science like Networks, Data Mining, Multimedia, Multi-threading, Web Development, Mobile App Development, etc. Analyzing the actual textual content of these websites can help computer science and software engineering community better understand the needs of developers and learn about the current trends in technology. In this project, textual data from famous question and answer website called StackOverflow, is analyzed using Latent Dirichlet Allocation (LDA) topic modeling algorithm. The results show that this techniques help discover dominant topics in developer discussions. These topics are analyzed to find a number of interesting observations such as popular technology\/language, impact of a technology, technology trends over time, relationship of a technology\/language with other technologies and comparison of technologies addressing an area of computer science or software engineering.","keywords_author":["Latent Dirichlet Allocation (LDA)","Machine Learning","Natural Language Processing","Topic modeling"],"keywords_other":["Textual content","Multi-threading","Topic modeling algorithms","Latent dirichlet allocations","Mobile app development","Technology trends","Computer science and software engineerings","Topic Modeling"],"max_cite":0.0,"pub_year":2018.0,"sources":"['scp']","rawkeys":["computer science and software engineerings","multi-threading","topic modeling","topic modeling algorithms","machine learning","latent dirichlet allocation (lda)","natural language processing","latent dirichlet allocations","technology trends","textual content","mobile app development"],"tags":["linear discriminant analysis","multi-threading","computer science and software engineerings","topic modeling","topic modeling algorithms","technological trends","machine learning","natural language processing","textual content","mobile app development"]},{"p_id":39452,"title":"Opinion mining for predicting peer affective feedback helpfulness","abstract":"Peer feedback has become increasingly popular since the advent of social networks, which has significantly changed the process of learning. Some of today's e-learning systems enable students to communicate with peers (or co-learners) and ask or provide feedback. However, the highly variable nature of peer feedback makes it difficult for a learner who asked for help to notice and benefit from helpful feedback provided by his peers, especially if he is in emotional distress. Helpful feedback in affective context means positive, motivating and encouraging feedback while an unhelpful feedback is negative, bullying and demeaning feedback. In this paper, we propose an approach to predict the helpfulness of a given affective feedback for a learner based on the feedback content and the learner's affective state. The proposed approach uses natural language processing techniques and machine learning algorithms to classify and predict the helpfulness of peers' feedback in the context of an English learning forum. In order to seek the best accuracy possible, we have used several machine learning algorithms. Our results show that Na\u00efve-Bayes provides the best performance with a prediction accuracy of 87.19%.","keywords_author":["Classification","E-learning","Machine learning","Natural language processing","Opinion mining","Peer affective feedback","Peers' interaction","Sentiment analysis"],"keywords_other":["Affective contexts","Peers' interaction","Sentiment analysis","Prediction accuracy","Process of learning","NAtural language processing","English Learning","Opinion mining"],"max_cite":1.0,"pub_year":2014.0,"sources":"['scp']","rawkeys":["opinion mining","machine learning","natural language processing","affective contexts","peer affective feedback","process of learning","classification","prediction accuracy","peers' interaction","english learning","sentiment analysis","e-learning"],"tags":["opinion mining","machine learning","natural language processing","affective contexts","peer affective feedback","process of learning","classification","prediction accuracy","peers' interaction","english learning","sentiment analysis","e-learning"]},{"p_id":53791,"title":"Extracting key sentences with latent argumentative structuring","abstract":"PROBLEM: Key word assignment has been largely used in MEDLINE to provide an indicative \"gist\" of the content of articles. Abstracts are also used for this purpose. However with usually more than 300 words, abstracts can still be regarded as long documents; therefore we design a system to select a unique key sentence. This key sentence must be indicative of the article's content and we assume that abstract's conclusions are good candidates. We design and assess the performance of an automatic key sentence selector, which classifies sentences into 4 argumentative moves: PURPOSE, METHODS, RESULTS and CONCLUSION. METHODS: We rely on Bayesian classifiers trained on automatically acquired data. Features representation, selection and weighting are reported and classification effectiveness is evaluated on the four classes using confusion matrices. We also explore the use of simple heuristics to take the position of sentences into account. Recall, precision and F-scores are computed for the CONCLUSION class. For the CONCLUSION class, the F-score reaches 84%. Automatic argumentative classification is feasible on MEDLINE abstracts and should help user navigation in such repositories.","keywords_author":["Abstracting and indexing","Digital libraries","Information storage and retrieval","Machine learning","Natural language processing"],"keywords_other":["Humans","MEDLINE","Natural Language Processing","Bayes Theorem"],"max_cite":0.0,"pub_year":2005.0,"sources":"['scp']","rawkeys":["medline","machine learning","natural language processing","humans","abstracting and indexing","digital libraries","bayes theorem","information storage and retrieval"],"tags":["medline","machine learning","natural language processing","humans","abstracting and indexing","digital libraries","bayes theorem","information storage and retrieval"]},{"p_id":39460,"title":"Towards personalized offers by means of life event detection on social media and entity matching","abstract":"In this paper we present a system for personalized offers based on two main components: a) a hybrid method, combining rules and machine learning, to find users that post life events on social media networks; and b) an entity matching algorithm to find out possible relation between the detected social media users and current clients. The main assumption is that, if one can detect the life events of these users, a personalized offer can be made to them even before they look for a product or service. This proposed solution was implemented on the IBM InfoSphere BigInsights platform to take advantage of the MapReduce programming framework for large scale capability, and was tested on a dataset containing 9 million posts from Twitter. In this set, 42K life event posts sent by 19K different users were detected, with an overall accuracy of 89% e precision of about 65% to detect life events. The entity matching of these 19K social media users against an internal database of 1.6M users returned 983 users, with accuracy of about 90%.","keywords_author":["Entity matching","Life event detection","Machine learning","Natural language processing","Social media networks"],"keywords_other":["Life events","Combining rules","Social media networks","Overall accuracies","Entity matching","Map-reduce programming","Hybrid method","NAtural language processing"],"max_cite":1.0,"pub_year":2014.0,"sources":"['scp']","rawkeys":["combining rules","social media networks","overall accuracies","machine learning","natural language processing","map-reduce programming","life event detection","hybrid method","entity matching","life events"],"tags":["social media networks","machine learning","natural language processing","map-reduce programming","life event detection","hybrid method","optimization algorithms","combination rules","entity matching","life events"]},{"p_id":49700,"title":"Integrating performance of web search engine with Machine Learning approach","abstract":"\u00a9 2016 IEEE. Todays diversified user query over web search engine for information retrieval; semantic information for relevant web document on web has been plethora of web search research. A lot many web search engine developed based on semantic meaning like ontolook, swoogle etc., for finding relevant information, which helps to find user based semantic meaning related documents. The concept of semantic similarity or semantic information widely focused in many important fields such as Machine Learning, Artificial Intelligence, Cognitive Science, Natural Language Processing and Web Information Retrieval etc., Traditional web search engines and semantic web search engines relates user keyword with terms, entities, texts, documents which have semantic correlation with user query. Both search engines does not use images within web pages to find more relevant information. Now in this paper we have formulated a web document integrated ranking method based on text semantic information and image based object matching information. This integrated approach presented in this paper does not depend upon semantic information of user query but also consider image appearing within web pages to find more relevant information. Approach proposed in this paper includes finding semantic information using ontology based meaning of user query and feature based object matching over image to find image matching score. In proposed approach combined use of ontology based semantic information and image based object matching score will improve web document ranking.","keywords_author":["Machine Learning","Object Matching","Ranking","Semantic Similarity","Semantic Web","Web Document"],"keywords_other":["Web document","Web information retrieval","Object matching","Ranking","Machine learning approaches","Semantic similarity","NAtural language processing","Semantic information"],"max_cite":0.0,"pub_year":2016.0,"sources":"['scp']","rawkeys":["natural language processing","machine learning","semantic similarity","web document","object matching","ranking","semantic information","semantic web","web information retrieval","machine learning approaches"],"tags":["standards","natural language processing","machine learning","semantic similarity","web document","object matching","web information retrieval","semantic information","semantic web","machine learning approaches"]},{"p_id":43559,"title":"Text Labeling Applied in Shopping Assistant Robot Using Long Short-Term Memory","abstract":"\u00a9 2018 IEEE. Shopping assistant robot plays a role as shopping guide in the supermarket. In this paper, we focus on early text labeling tasks which are crucial to design of robot conversation system and instruction selecting system. Part-of-speech tagging and named entity recognition are two fundamental tasks in nature language processing. In this paper, we need to implement both tasks in a robot by employing deep learning technics. The aim of this paper include: 1. Apply deep learning framework TensorFlow presented by Google to integrated robot development platform named Turtlebot2 which is provided by YUJIN ROBOT CO. 2. Construct text labeling model suitable for shopping assistant robot using a special recurrent neural network named long short-Term memory.","keywords_author":["deep learning","long short-Term memory","TensorFlow","text labeling"],"keywords_other":["Part of speech tagging","Named entity recognition","Conversation systems","TensorFlow","Nature language processing","Learning frameworks","Shopping guides","Robot development"],"max_cite":0.0,"pub_year":2018.0,"sources":"['scp']","rawkeys":["shopping guides","deep learning","tensorflow","long short-term memory","named entity recognition","nature language processing","part of speech tagging","learning frameworks","robot development","text labeling","conversation systems"],"tags":["shopping guides","tensorflow","long short-term memory","named entity recognition","machine learning","natural language processing","part of speech tagging","conversational systems","learning frameworks","robot development","text labeling"]},{"p_id":88616,"title":"Using structural topic modeling to identify latent topics and trends in aviation incident reports","abstract":"The Aviation Safety Reporting System includes over a million confidential reports describing aviation safety incidents. Natural language processing techniques allow for relatively rapid and largely automated analysis of large collections of text data. Interpretation of the results and further investigations by subject matter experts can produce meaningful results. This explains the many commercial and academic applications of natural language processing to aviation safety reports. Relatively few published articles have, however, employed topic modeling, an approach that can identify latent structure within a corpus of documents. Topic modeling is more flexible and relies less on subject matter experts than alternative document categorization and clustering methods. It can, for example, uncover any number of topics hidden in a set of incident reports that have been, or would be, assigned to the same category when using labels and methods applied in earlier research. This article describes the application of structural topic modeling to Aviation Safety Reporting System data. The application identifies known issues. The method also reveals previously unreported connections. Sample results reported here highlight fuel pump, tank, and landing gear issues and the relative insignificance of smoke and fire issues for private aircraft. The results also reveal the prominence of the Quiet Bridge Visual and Tip Toe Visual approach paths at San Francisco International Airport in safety incident reports. These results would, ideally, be verified by subject matter experts before being used to set priorities when planning future safety studies.","keywords_author":["Aviation","Aviation safety","Natural language processing","Topic modeling","Text mining","Aviation safety reporting system"],"keywords_other":["SAFETY REPORTS","NETWORK","TRANSPORTATION RESEARCH"],"max_cite":0.0,"pub_year":2018.0,"sources":"['wos']","rawkeys":["network","text mining","transportation research","topic modeling","aviation safety reporting system","natural language processing","aviation","aviation safety","safety reports"],"tags":["text mining","transportation research","topic modeling","aviation safety reporting system","natural language processing","aviation","networks","aviation safety","safety reports"]},{"p_id":31274,"title":"Sentiment analysis of top colleges in India using Twitter data","abstract":"\u00a9 2016 IEEE. In today's world, opinions and reviews accessible to us are one of the most critical factors in formulating our views and influencing the success of a brand, product or service. With the advent and growth of social media in the world, stakeholders often take to expressing their opinions on popular social media, namely Twitter. While Twitter data is extremely informative, it presents a challenge for analysis because of its humongous and disorganized nature. This paper is a thorough effort to dive into the novel domain of performing sentiment analysis of people's opinions regarding top colleges in India. Besides taking additional preprocessing measures like the expansion of net lingo and removal of duplicate tweets, a probabilistic model based on Bayes' theorem was used for spelling correction, which is overlooked in other research studies. This paper also highlights a comparison between the results obtained by exploiting the following machine learning algorithms: Na\u00efve Bayes and Support Vector Machine and an Artificial Neural Network model: Multilayer Perceptron. Furthermore, a contrast has been presented between four different kernels of SVM: RBF, linear, polynomial and sigmoid.","keywords_author":["Machine Learning","Natural Language Processing","Neural Network","Opinion Mining","Sentiment Analysis","Twitter"],"keywords_other":["Twitter","Spelling correction","Sentiment analysis","Critical factors","Probabilistic modeling","Artificial neural network modeling","NAtural language processing","Opinion mining"],"max_cite":3.0,"pub_year":2016.0,"sources":"['scp']","rawkeys":["neural network","critical factors","probabilistic modeling","spelling correction","natural language processing","machine learning","twitter","opinion mining","sentiment analysis","artificial neural network modeling"],"tags":["probabilistic models","critical factors","neural networks","artificial neural network models","spelling correction","natural language processing","machine learning","opinion mining","sentiment analysis","twitter"]},{"p_id":51758,"title":"Financial data extraction - An application","abstract":"Copyright \u00a9 2014 IADIS Press All rights reserved. In this study we have developed a financial data extraction system to tag and extract financial concepts along with corresponding numeric values - monetary and temporal. We employ machine learning and natural language processing methods to identify financial concepts and link them to numerical entities based on the tagged financial data. While large number of US companies is required to tag (US-GAAP<sup>1<\/sup> tags) part of their financial statements filed to the US Security and Exchange Commission quarterly and yearly, only part of the worldwide financial information is tagged. In preliminary evaluations based on publicly available 10-Q<sup>2<\/sup> and 10-K<sup>3<\/sup> documents, the system records an average modulewise accuracy of 87% toward financial concept identification and semantic tag prediction.","keywords_author":["Financial data extraction","Machine learning application","Na\u00efve bayesian classifier"],"keywords_other":["Financial information","Financial data","Bayesian classifier","Machine learning applications","Security and exchange commissions","Financial statements","Concept identification","NAtural language processing"],"max_cite":0.0,"pub_year":2014.0,"sources":"['scp']","rawkeys":["financial data","concept identification","machine learning application","machine learning applications","security and exchange commissions","natural language processing","bayesian classifier","financial data extraction","na\u00efve bayesian classifier","financial information","financial statements"],"tags":["financial data","concept identification","machine learning applications","security and exchange commissions","natural language processing","bayesian classifier","financial data extraction","na\u00efve bayesian classifier","financial information","financial statements"]},{"p_id":51759,"title":"A way to predict and evaluate of software maintainability based on machine learning","abstract":"The accurate maintainability prediction and evaluation of software applications can improve the designing management for these applications, thus benefiting designing organizations. Therefore, there is considerable research interest in development and application of sophisticated techniques which can be used to build models for both predicting and evaluating software maintainability. In this paper, we investigate some ideas based on Machine Learning, Natural Language Processing, Fuzzy Logic, and Systematic Model of Software Maintenance. The idea to compute Interactive Index and the maintainability of software system is useful to study the relation between maintainability prediction and maintainability evaluation in the whole software process. An model basing on fuzzy matrix and BP neural network is built up. It's approved that there are application value of using this model based on BP neural network to predict and evaluate the software maintainability. \u00a9 (2014) Trans Tech Publications, Switzerland.","keywords_author":["Fuzzy matrix","Machine learning","Neural network model","Software maintainability"],"keywords_other":["Evaluating software","Software applications","Fuzzy matrix","Neural network model","Systematic modeling","Development and applications","NAtural language processing","Software maintainability"],"max_cite":0.0,"pub_year":2014.0,"sources":"['scp']","rawkeys":["development and applications","machine learning","natural language processing","neural network model","software maintainability","software applications","fuzzy matrix","systematic modeling","evaluating software"],"tags":["development and applications","machine learning","natural language processing","evaluation software","neural network model","software maintainability","software applications","fuzzy matrix","systematic modeling"]},{"p_id":51760,"title":"Development of machine learning based natural language processing system","abstract":"For disease diagnostic knowledge base system (including Q&A, consistency checker, and informativity checker) requiring higher degree of strictness on the results of a query to the system, at the development stage, great efforts are necessary to improve machine learning based statistical performance tests, measured by precision and recall rates, on the training error and prediction. Performance of the test runs is generally dependent on the two basic factors as the following: first of all, the underlying technique of in-depth context analysis on corpora with inference capability, secondly, that of user's query sentence analysis with inference capability. More importantly, a disease diagnostic knowledge base system should be able to update effectively the latest research achievements in timely manner. To meet the requirements, we propose an automatic system for construction of knowledge base from the academic archive of medical literatures. For the purpose of presentation in this paper, a prototype of knowledge base construction using natural language processing system for early diagnosis of Alzheimer disease has been designed and implemented. Since there are plenty of knowledge base systems available for Alzheimer diagnosis in English language, to differentiate our works with the existing data, we performed our research with the literatures written in Korean. The natural language processing system proposed in this paper consists of 8 modules most of which are machine learning trainer\/prediction model based on maximum entropy algorithm. Tests showed that, for all the modules, iterative training have been succeeded with precision over 90%.","keywords_author":["Disease diagnosis system","Knowledge base system","Machine learning","Natural language processing"],"keywords_other":null,"max_cite":0.0,"pub_year":2014.0,"sources":"['scp']","rawkeys":["natural language processing","disease diagnosis system","knowledge base system","machine learning"],"tags":["knowledge based systems","natural language processing","disease diagnosis system","machine learning"]},{"p_id":33329,"title":"Sentiment analysis on Twitter data with semi-supervised Doc2Vec","abstract":"\u00a9 2017 IEEE. Twitter is one of the most popular microblog sites developed in recent years. Feelings are analysed on the messages shared on Twitter so that users ideas on the products and companies can be determined. Sentiment analysis helps companies to improve their products and services based on the feedback obtained from the users through Twitter. In this study, it was aimed to perform sentiment analysis on Turkish and English Twitter messages using Doc2Vec. The Doc2Vec algorithm was run on Positive, Negative and Neutral tagged data using the Semi-Supervised learning method and the results were recorded.","keywords_author":["Doc2Vec","Machine Learning","Natural Language Processing","Semi-Supervised Learning","Sentiment Analysis"],"keywords_other":["Micro-blog","Semi-supervised","Doc2Vec","Tagged data","Semi- supervised learning","Semi-supervised learning methods","Sentiment analysis","Products and services"],"max_cite":2.0,"pub_year":2017.0,"sources":"['scp']","rawkeys":["doc2vec","semi- supervised learning","semi-supervised","natural language processing","machine learning","products and services","micro-blog","semi-supervised learning","tagged data","sentiment analysis","semi-supervised learning methods"],"tags":["doc2vec","semi-supervised","natural language processing","machine learning","products and services","semi-supervised learning","microblogging","tagged data","sentiment analysis","semi-supervised learning methods"]},{"p_id":45619,"title":"Proposing contextually relevant quotes for images","abstract":"\u00a9 Springer International Publishing AG, part of Springer Nature 2018. Due to the rise in deep learning techniques used for the task of automatic image captioning, it is now possible to generate natural language descriptions of images and their regions. However, these captions are often too plain and simple. Most users on social media and other micro blogging websites use flowery language and quote like captions to describe the pictures they post online. We propose an algorithm that uses a combination of deep learning and natural language processing techniques to provide contextually relevant quotes for any given input image. We also present a new dataset, QUOTES500K, with the goal of advancing research requiring large dataset of quotes. Our dataset contains five hundred thousand (500K) quotes along with the author name and their category tags.","keywords_author":["Automatic image captioning","Computer vision","Deep learning","Natural language processing"],"keywords_other":["Automatic image captioning","Micro blogging","Social media","Learning techniques","Large dataset","Natural languages","Input image"],"max_cite":0.0,"pub_year":2018.0,"sources":"['scp']","rawkeys":["micro blogging","deep learning","social media","natural languages","natural language processing","input image","automatic image captioning","learning techniques","large dataset","computer vision"],"tags":["social media","natural languages","machine learning","natural language processing","input image","large datasets","automatic image captioning","learning techniques","microblogging","computer vision"]},{"p_id":29237,"title":"From social media to public health surveillance: Word embedding based clustering method for twitter classification","abstract":"\u00a9 2017 IEEE. Social media provide a low-cost alternative source for public health surveillance and health-related classification plays an important role to identify useful information. In this paper, we summarized the recent classification methods using social media in public health. These methods rely on bag-of-words (BOW) model and have difficulty grasping the semantic meaning of texts. Unlike these methods, we present a word embedding based clustering method. Word embedding is one of the strongest trends in Natural Language Processing (NLP) at this moment. It learns the optimal vectors from surrounding words and the vectors can represent the semantic information of words. A tweet can be represented as a few vectors and divided into clusters of similar words. According to similarity measures of all the clusters, the tweet can then be classified as related or unrelated to a topic (e.g., influenza). Our simulations show a good performance and the best accuracy achieved was 87.1%. Moreover, the proposed method is unsupervised. It does not require labor to label training data and can be readily extended to other classification problems or other diseases.","keywords_author":["Big data","Clustering Process","Machine learning","Natural Language Processing","Public Health","Similarity Measure","Social Network","Surveillance","Twitter","Unsupervised Classification","Word Embeddings","Word2Vec"],"keywords_other":["Twitter","Word2Vec","Unsupervised classification","Similarity measure","Clustering process","Embeddings"],"max_cite":4.0,"pub_year":2017.0,"sources":"['scp']","rawkeys":["embeddings","unsupervised classification","big data","word2vec","machine learning","natural language processing","twitter","word embeddings","similarity measure","social network","public health","surveillance","clustering process"],"tags":["embeddings","unsupervised classification","big data","word2vec","machine learning","natural language processing","social networks","twitter","word embedding","similarity measure","public health","surveillance","clustering process"]},{"p_id":39477,"title":"A framework for building adaptive intelligent virtual assistants","abstract":"This paper describes a framework to support the construction of intelligent virtual assistants (IVAs). An IVA agent is a software assistant capable of interacting with a user to support sense-making tasks, to determine information needs, to provide relevant information and to improve its performance based on user feedbacks. Currently, there is no integrated software environment available to develop such agents. We are exploring how we can integrate machine learning and natural language processing technologies, available as open source software, to support the construction of intelligent virtual assistants. The framework relies on a combination of question answering (Q\/A), information extraction (IE) and user modeling components. In this paper, we present an overview of the work that is being conducted to build a prototype of the framework.","keywords_author":["Information extraction","Intelligent virtual assistant","Machine learning","Natural language processing","Question answering systems","Topic modeling"],"keywords_other":["Question Answering","Integrated software environments","Question answering systems","Open Source Software","Virtual assistants","Software assistants","NAtural language processing","Topic Modeling"],"max_cite":1.0,"pub_year":2014.0,"sources":"['scp']","rawkeys":["question answering","open source software","information extraction","software assistants","topic modeling","natural language processing","machine learning","question answering systems","virtual assistants","integrated software environments","intelligent virtual assistant"],"tags":["open source software","information extraction","software assistants","topic modeling","natural language processing","machine learning","information retrieval","question answering systems","virtual assistants","integrated software environments","intelligent virtual assistant"]},{"p_id":39479,"title":"Mining semantic representation from medical text: A Bayesian approach","abstract":"\u00a9 2014 IEEE.Machine learning is a subfield of artificial intelligence that deals with the exploration and construction of systems that can learn from data. Machine learning trains the computers to manage the critical situations via examining, self-training, inference by observation and previous experience. This paper provides an overview of the development of an efficient classifier that represents the semantics in medical data (Medline) using a Machine Learning (ML) perspective. In recent days people are more concerned about their health and explore ways to identify health related information. But the process of identifying the semantic representation for the medical terms is a difficult task. The main goal of our work was to identify the semantic representation for the medical abstracts in the Medline repository using Machine Learning and Natural Language Processing (NLP).","keywords_author":["Classification","Machine Learning (ML)","Medline and Healthcare","Natural Language Processing (NLP)"],"keywords_other":["Medical terms","Health related informations","Medical data","Self training","Semantic representation","Medline","Bayesian approaches","NAtural language processing"],"max_cite":1.0,"pub_year":2014.0,"sources":"['scp']","rawkeys":["medline","health related informations","medical data","machine learning (ml)","medical terms","self training","natural language processing","natural language processing (nlp)","semantic representation","medline and healthcare","classification","bayesian approaches"],"tags":["medline","health related informations","medical data","self-training","medical terms","machine learning","natural language processing","semantic representation","medline and healthcare","classification","bayesian approaches"]},{"p_id":31289,"title":"Latte: A language, compiler, and runtime for elegant and efficient deep neural networks","abstract":"\u00a9 2016 ACM. Deep neural networks (DNNs) have undergone a surge in popularity with consistent advances in the state of the art for tasks including image recognition, natural language processing, and speech recognition. The computationally expensive nature of these networks has led to the proliferation of implementations that sacrifice abstraction for high performance. In this paper, we present Latte, a domain-specific language for DNNs that provides a natural abstraction for specifying new layers without sacrificing performance. Users of Latte express DNNs as ensembles of neurons with connections between them. The Latte compiler synthesizes a program based on the user specification, applies a suite of domainspecific and general optimizations, and emits efficient machine code for heterogeneous architectures. Latte also includes a communication runtime for distributed memory data-parallelism. Using networks described using Latte, we demonstrate 3-6 \u00d7 speedup over Caffe (C++\/MKL) on the three state-of-the-art ImageNet models executing on an Intel Xeon E5-2699 v3 x86 CPU.","keywords_author":["Compiler","Deep learning","Domain specific language","Neural networks","Optimization"],"keywords_other":["Deep learning","Heterogeneous architectures","Compiler","Distributed Memory","Domain specific languages","General optimizations","Deep neural networks","NAtural language processing"],"max_cite":3.0,"pub_year":2016.0,"sources":"['scp']","rawkeys":["heterogeneous architectures","distributed memory","domain specific language","deep learning","deep neural networks","neural networks","natural language processing","general optimizations","domain specific languages","compiler","optimization"],"tags":["heterogeneous architectures","distributed memory","neural networks","machine learning","natural language processing","general optimizations","convolutional neural network","domain specific languages","compilers","optimization"]},{"p_id":39482,"title":"Evaluating polarity for verbal phraseological units","abstract":"\u00a9 Springer International Publishing Switzerland 2014. Fixation in linguistic expressions is an inherent property of natural language that plays a central role in their description. Verbal phraseological units are phrases made up of two or more words characterized for presenting certain degree of fixation or idiomaticity (at least one of these words is a verb that plays the role of the predicate). Phraseological units do not appear so frequently in manually constructed lexical resources as they do in real-word text, and this problem of coverage may impact the performance of many natural language processing tasks. Therefore, the construction of automatic understanding systems for these types of linguistic structures is very important, since they are a standard way of expressing a concept or idea. In this paper we present a set of experiments towards the automatic identification of the polarity of verbal phraseological units. We obtained a maximum performance of 80% for this particular task when the contextual information of a phraseological unit is considered, in comparison with a 62% when the VPU alone is only used. These results highlight the importance of analyzing automatically this type of linguistic structures. It should be stressed at the outset that these experiments are intended as a preliminary study rather than as a comprehensive analysis or solution of the aforementioned problem.","keywords_author":["Machine learning","Text polarity","Verbal phraseological units"],"keywords_other":["Linguistic expressions","Automatic identification","Contextual information","Comprehensive analysis","Verbal phraseological units","Automatic understanding","Text polarities","NAtural language processing"],"max_cite":1.0,"pub_year":2014.0,"sources":"['scp']","rawkeys":["contextual information","machine learning","automatic understanding","natural language processing","linguistic expressions","verbal phraseological units","automatic identification","comprehensive analysis","text polarities","text polarity"],"tags":["contextual information","machine learning","automatic understanding","natural language processing","linguistic expressions","verbal phraseological units","automatic identification","comprehensive analysis","text polarity"]},{"p_id":37434,"title":"A Survey on Deep Text Matching","abstract":"\u00a9 2017, Science Press. All right reserved.Many problems in natural language processing, such as information retrieval, question answering, machine translation, dialog system, paraphrase identification and so on, can be treated as a problem of text matching. The past researches on text matching focused on defining artificial features and learning relation between two text features, thus the performance of the text matching model heavily relies on the features designing. Recently, affected by the idea of automatically feature extraction in deep learning, many text matching models based on deep learning, namely Deep Text Matching model, have been proposed. Comparing to the traditional methods, Deep Text Matching models can automatically learn relations among words from big data and make use of the information from phrase patterns and text hierarchical structures. Considering the different structures of Deep Text Matching models, we divide them into three categories: Single semantic document representation based deep matching model, Multiple semantic document representation based deep matching model and Matching pattern based deep matching model. We can see the progressive relationship among three kinds of models in modelling the interaction of texts, while which have their own merits and defects based on a specific task. Experiments were carried out on the typical datasets of paraphrase identification, question answering and information retrieval. We compare and explain the different performance of three kinds of deep text matching models. Finally, we give the key challenges and the future outlooks of the deep text matching models.","keywords_author":["Convolutional neural network","Deep learning","Natural language processing","Recurrent neural network","Social media","Text matching"],"keywords_other":["Document Representation","Paraphrase identifications","Hierarchical structures","Social media","Text-matching","Convolutional neural network","Machine translations","Different structure"],"max_cite":1.0,"pub_year":2017.0,"sources":"['scp']","rawkeys":["hierarchical structures","text-matching","deep learning","different structure","social media","natural language processing","recurrent neural network","paraphrase identifications","convolutional neural network","machine translations","document representation","text matching"],"tags":["hierarchical structures","different structure","neural networks","social media","machine learning","natural language processing","paraphrase identifications","convolutional neural network","machine translations","document representation","text matching"]},{"p_id":49721,"title":"GATE: Classification and clustering of text for semi-vowel\/j\/-morphophonemic approach","abstract":"\u00a9 2016 IEEE.In recent years, many successful machine learning applications have been developed. Classification & Clustering is one such. This application is cross-disciplinary, now that it is based on data mining algorithms on the technical side and on graphemes and morphophonemic on the linguistic side. It will thus map the correspondence between grapheme <y> and related phonemes via morphemes in a given context. The grapheme <y> is often realized as an approximant \/j\/ (a consonant phoneme), as in <yacht> or as vowel \/ i \/ as in <racy>, or a diphthong \/ ai \/ as in <sky>, etc. The objective, that is to say, of this text analysis is to map the various articulators\/\/phonemic realizations of the grapheme <y>. This experiment will thus help the study the occurrence of <y> in different positions in words, word initially, finally or elsewhere, as in these examples. The training data (or corpus) chosen for this experimentation is a set of English literary texts [Harry Potter part 1, part 2, The complete works of William Shakespeare By William Shakespeare, The Adventures of Sherlock Holmes By Arthur Conan Doyle, (A Christmas carol) By Charles Dickens's. As for the tools, the alphabet recognizer is used for retrieving information and categorizing the data as noun, adjective, etc. Then, GATE Developer, the Text engineering tool is used to analyze the dataset and to derive the output with statistical data of global distribution <y> as in all the input documents. This project is particularly relevant because it offers, as will be demonstrated, a solution to problems due to lack of one-to-one correspondence between spelling and pronunciation in English, as for instance, in the context of language pedagogy. Glossary of technical terms (of both linguistics & computing) is appended, now that the paper is inter-disciplinary.","keywords_author":["Clustering and Classification","GATE","grapheme","machine learning","mapping","morpheme","morphophonemic","NLP","phoneme","Text mining"],"keywords_other":["Text mining","morpheme","phoneme","grapheme","morphophonemic","GATE"],"max_cite":0.0,"pub_year":2016.0,"sources":"['scp']","rawkeys":["clustering and classification","morpheme","text mining","nlp","mapping","machine learning","phoneme","grapheme","morphophonemic","gate"],"tags":["clustering and classification","morpheme","text mining","map","machine learning","natural language processing","phoneme","morphophonemic","graphemes","gate"]},{"p_id":49722,"title":"Fusion with sentiment scores for market research","abstract":"\u00a9 2016 ISIF. The recent surge in electronic and social media has led to an explosion of sentiment data embedded in public and private documents, fueling interest in sentiment analysis, especially as individuals, brands and corporations look to manage their reputational risk which is directly correlated to company performance. In this paper, we describe two approaches to score sentiments from a large unstructured text corpus1 to fuse with other relevant structured relational data: 1) a simple but effective and fast lexicon-based approach where the score of a document is based on the occurrences of stemmed words representing positive and negative sentiments; and 2) a supervised machine learning approach where the score is derived by making use of a kernel-based classification model created from the training documents. Example applications of these techniques can be found in our text analytics tool called aText which can compute sentiment scores of product reviews from Amazon and TripAdvisor to gain market insight to products and services. Another example is the computation of sentiment scores using aText for public and private companies from credible financial sources which is further fused with market data (stock price) to create a composite index for financial analysts and traders.","keywords_author":["Machine Learning","Natural Language Processing","Sentiment Analysis","Text Analytics"],"keywords_other":["Kernel based classification","Sentiment analysis","Negative sentiments","Text analytics","Company performance","Products and services","Supervised machine learning","NAtural language processing"],"max_cite":0.0,"pub_year":2016.0,"sources":"['scp']","rawkeys":["text analytics","kernel based classification","company performance","natural language processing","machine learning","products and services","negative sentiments","supervised machine learning","sentiment analysis"],"tags":["text analytics","kernel based classification","company performance","natural language processing","machine learning","products and services","negative sentiments","supervised machine learning","sentiment analysis"]},{"p_id":37438,"title":"A Deep Learning Model Enhanced with Emotion Semantics for Microblog Sentiment Analysis","abstract":"\u00a9 2017, Science Press. All right reserved. Word embedding based on neural language model can automatically learn effective word representation from massive unlabeled text dataset, and has made essential progress in many natural language processing tasks. Emoticons in microblog are important emotion signals for microblog sentiment analysis. There have been a lot of research works exploiting emoticons to improve sentiment classification performance for microblog effectively. Commonly used emoticons are adopted to construct an emotion space as feature representation matrix RE from their word embedding. On the basis of vector based semantic composition, the projection to emotion space is performed as matrix-vector multiplication between RE and other embedding. Then, the results are forward to MCNN to learn a sentiment classifier for microblog. This new model is named as EMCNN, short for Emotion-semantic enhanced MCNN, which seamlessly integrates emotion space projection based on emoticon into deep learning model MCNN to enhance its ability of capturing emotion semantic. On the datasets of NLPCC microblog sentiment analysis task, EMCNN achieves the best performance in several sentiment classification experiments and surpass the state-of-the-art results on all the performance metrics. Comparing to MCNN, EMCNN not only improve the classification performance, but also reduce the training time, i.e. 36.15% for subject classification and 33.82% for 7-class sentiment classification.","keywords_author":["Convolution neural network","Deep learning","Microblog","Natural language processing","Sentiment analysis","Social networks"],"keywords_other":["Micro-blog","Classification performance","Sentiment analysis","Feature representation","Convolution neural network","Matrix vector multiplication","Sentiment classification","Subject classification"],"max_cite":1.0,"pub_year":2017.0,"sources":"['scp']","rawkeys":["matrix vector multiplication","sentiment classification","classification performance","convolution neural network","deep learning","subject classification","natural language processing","social networks","micro-blog","microblog","feature representation","sentiment analysis"],"tags":["matrix vector multiplication","sentiment classification","classification performance","machine learning","natural language processing","social networks","convolutional neural network","microblogging","feature representation","subjectivity classification","sentiment analysis"]},{"p_id":51771,"title":"Conceptual scheme for text classification system","abstract":"The paper describes an application of classification algorithms to the text categorization problem. Author proposes a conceptual scheme for an automatic text categorization system. This system must operate with various text representation models and data mining methods. The novelty of this system consists in advanced implementation of JSM method for automatic hypothesis generation - an original logical-combinatorial technology of data mining, which is developed in Russia by several research groups.","keywords_author":["Data mining","Machine learning","Natural language processing","Text classification system"],"keywords_other":["Automatic text categorization","Text representation models","Data mining methods","Combinatorial technology","Classification algorithm","Hypothesis generation","NAtural language processing","Text classification systems"],"max_cite":0.0,"pub_year":2014.0,"sources":"['scp']","rawkeys":["data mining methods","data mining","automatic text categorization","machine learning","natural language processing","text classification system","combinatorial technology","text classification systems","text representation models","classification algorithm","hypothesis generation"],"tags":["data mining methods","data mining","automatic text categorization","machine learning","natural language processing","combinatorial technology","text classification systems","text representation models","classification algorithm","hypothesis generation"]},{"p_id":575,"title":"Authoritative Prediction of Website Based on Deep Learning","abstract":"Website authoritativeness is generally measured by external links, the more high-quality external links, the higher the authority of the site. Algorithms such as PageRank, etc. are com-monly used in the evaluation of website authoritative, However, this kind of algorithm is selective to evaluate the authoritativeness of websites, which makes this method have some deficiencies. This paper uses deep learning method to evaluate the authority of different websites under a certain search query by mapping the search query and the corresponding title of the website as a vector and calculating the similarity between two vectors. Websites of high results are referred to as authoritative sites under the search query, thus providing a new perspective to measure website authoritativeness. By comparing the three model experiments with Word2vec, CNN and LSTM, the experimental results on open datasets show that it is effective to use these three models, of which the LSTM model works best.","keywords_author":["Website authority","Word2vec","LSTM","CNN","NLP"],"keywords_other":["high-quality external links","authorisation","learning (artificial intelligence)","query processing","deep learning method","authoritative sites","websites","Conferences","vectors","Big Data","website authoritative","Web sites","search query","website authoritativeness","authoritative prediction"],"max_cite":null,"pub_year":2018.0,"sources":"['ieee']","rawkeys":["web sites","big data","websites","website authority","website authoritativeness","authoritative prediction","authorisation","conferences","high-quality external links","deep learning method","vectors","search query","learning (artificial intelligence)","lstm","query processing","authoritative sites","word2vec","cnn","nlp","website authoritative"],"tags":["web sites","big data","website authority","convolutional neural network","authoritative prediction","authorisation","conferences","high-quality external links","machine learning","vectors","website","query processing","authoritative sites","word2vec","long short-term memory","deep learning methods","search queries","natural language processing","website authoritative"]},{"p_id":10817,"title":"Enhancing deep learning sentiment analysis with ensemble techniques in social applications","abstract":"Deep learning techniques for Sentiment Analysis have become very popular. They provide automatic feature extraction and both richer representation capabilities and better performance than traditional feature based techniques (i.e., surface methods). Traditional surface approaches are based on complex manually extracted features, and this extraction process is a fundamental question in feature driven methods. These long-established approaches can yield strong baselines, and their predictive capabilities can be used in conjunction with the arising deep learning methods. In this paper we seek to improve the performance of deep learning techniques integrating them with traditional surface approaches based on manually extracted features. The contributions of this paper are sixfold. First, we develop a deep learning based sentiment classifier using a word embeddings model and a linear machine learning algorithm. This classifier serves as a baseline to compare to subsequent results. Second, we propose two ensemble techniques which aggregate our baseline classifier with other surface classifiers widely used in Sentiment Analysis. Third, we also propose two models for combining both surface and deep features to merge information from several sources. Fourth, we introduce a taxonomy for classifying the different models found in the literature, as well as the ones we propose. Fifth, we conduct several experiments to compare the performance of these models with the deep learning baseline. For this, we use seven public datasets that were extracted from the microblogging and movie reviews domain. Finally, as a result, a statistical study confirms that the performance of these proposed models surpasses that of our original baseline on Fl-Score. (C) 2017 The Authors. Published by Elsevier Ltd.","keywords_author":["Deep learning","Ensemble","Machine learning","Natural language processing","Sentiment analysis","Ensemble","Deep learning","Sentiment analysis","Machine learning","Natural language processing"],"keywords_other":["Ensemble","Social applications","Feature-based techniques","Sentiment analysis","CLASSIFICATION","Predictive capabilities","Learning techniques","POLARITY","Automatic feature extraction","NAtural language processing"],"max_cite":15.0,"pub_year":2017.0,"sources":"['ieee', 'scp', 'wos']","rawkeys":["deep learning","machine learning","ensemble","natural language processing","learning techniques","classification","predictive capabilities","social applications","automatic feature extraction","feature-based techniques","sentiment analysis","polarity"],"tags":["machine learning","ensemble","natural language processing","learning techniques","classification","predictive capabilities","social applications","automatic feature extraction","feature-based techniques","sentiment analysis","polarity"]},{"p_id":43587,"title":"On the design of web crawlers for constructing an efficient Chinese-Portuguese bilingual corpus system","abstract":"\u00a9 2018 Institute of Electronics and Information Engineers. Machine Translation is a very popular and important topic in Natural Language Processing (NLP) during the last few decades. This paper focuses on the design of the Web Crawlers for Chinese-Portuguese bilingual corpus construction, and this corpus would be used in corresponding Machine Translation systems. It accomplished a bilingual corpus construction process from bilingual corpus collection with web crawlers based on different sources. By this mean, this system can be considered as an innovative and reasonable attempt in setting up the bilingual corpora with Chinese and Portuguese, and it has solved some practical problems at the initial stage of the corpus construction.","keywords_author":["Bilingual Corpus","Machine Learning","Machine Translation","NLP","Web Crawler"],"keywords_other":["Practical problems","Machine translation systems","Bilingual corpora","Corpus construction","Machine translations"],"max_cite":0.0,"pub_year":2018.0,"sources":"['scp']","rawkeys":["bilingual corpus","bilingual corpora","nlp","corpus construction","machine learning","machine translation systems","machine translation","machine translations","practical problems","web crawler"],"tags":["bilingual corpus","bilingual corpora","web crawlers","corpus construction","machine learning","machine translation systems","natural language processing","machine translations","practical problems"]},{"p_id":6724,"title":"An introduction to conditional random fields","abstract":"Many tasks involve predicting a large number of variables that depend on each other as well as on other observed variables. Structured prediction methods are essentially a combination of classification and graphical modeling. They combine the ability of graphical models to compactly model multivariate data with the ability of classification methods to perform prediction using large sets of input features. This survey describes conditional random fields, a popular probabilistic method for structured prediction. CRFs have seen wide application in many areas, including natural language processing, computer vision, and bioinformatics. We describe methods for inference and parameter estimation for CRFs, including practical issues for implementing large-scale CRFs. We do not assume previous knowledge of graphical modeling, so this survey is intended to be useful to practitioners in a wide variety of fields. \u00a9 2012 C. Sutton and A. McCallum.","keywords_author":null,"keywords_other":["Conditional random field","Structured prediction","NAtural language processing","Classification methods","Probabilistic methods","Multivariate data","Practical issues","GraphicaL model","Input features","Graphical modeling"],"max_cite":201.0,"pub_year":2011.0,"sources":"['scp']","rawkeys":["practical issues","probabilistic methods","classification methods","conditional random field","natural language processing","input features","graphical model","multivariate data","structured prediction","graphical modeling"],"tags":["practical issues","structure prediction","probabilistic methods","classification methods","conditional random field","natural language processing","input features","graphical model","multivariate data"]},{"p_id":25156,"title":"Effect of adaptive spell checking in Persian","abstract":"In computers era, the flow of producing digital documents simply overwhelmed the traditional manual spell checking, the worst new type of misspelling called typographical errors have been created by machinery text production and management. Therefore, referring to human intolerable load of digital text's spell checking also the irrecusable ability of computers, including accuracy and speed, automatic spell checking using computer systems would be an important application of computer systems. Different users may have their own misspelling patterns or habits so we believe that using a traditional automatic spell checker using a fix set of rules may not be well performable for all kind of misspelling patterns. Therefore, in this paper, we investigate the effect of adaptive spell checking on Persian language comparing a non-adaptive traditional spell checking. Evaluation results show using adaptive spell checking is superior and more efficient than traditional spell checking with a fix set of rules after a short time of usage. \u00a9 2011 IEEE.","keywords_author":["Adaptive Approach","Machine Learning","Natural Language Processing","Persian","Spell Checking"],"keywords_other":["Machine-learning","Persians","Spell-checking","Adaptive approach","NAtural language processing"],"max_cite":8.0,"pub_year":2011.0,"sources":"['scp']","rawkeys":["persians","natural language processing","machine learning","spell checking","spell-checking","persian","machine-learning","adaptive approach"],"tags":["persians","natural language processing","machine learning","spell-checking","adaptive approach"]},{"p_id":37446,"title":"A Review on Sarcasm Detection from Machine-Learning Perspective","abstract":"\u00a9 2017 IEEE. In this paper, we want to review one of the challengingproblems for the opinion mining task, which is sarcasm detection. To be able to do that, many researchers tried to explore suchproperties in sarcasm like theories of sarcasm, syntacticalproperties, psycholinguistic of sarcasm, lexical feature, semanticproperties, etc. Studies done in the last 15 years not only madeprogress in semantic features, but also show increasing amount ofmethod of analysis using a machine-learning approach to processdata. Because of this reason, this paper will try to explain currentmostly used method to detect sarcasm. Lastly, we will present aresult of our finding, which might help other researchers to gaina better result in the future.","keywords_author":["machine learning","natural language processing","opinion mining","review","Sarcasm","semantic analysis"],"keywords_other":["Sarcasm","Machine learning approaches","Semantic analysis","Lexical features","Semantic features","NAtural language processing","Opinion mining"],"max_cite":1.0,"pub_year":2017.0,"sources":"['scp']","rawkeys":["natural language processing","machine learning","semantic analysis","semantic features","sarcasm","review","lexical features","opinion mining","machine learning approaches"],"tags":["natural language processing","machine learning","semantic analysis","semantic features","sarcasm","review","lexical features","opinion mining","machine learning approaches"]},{"p_id":51782,"title":"Identifying the correct root of an ambiguous Hebrew word","abstract":"\u00a9 Springer-Verlag Berlin Heidelberg 2014.Stemming is useful for various natural language processing tasks, such as document indexing and text classification. Therefore, identification of the correct root of any given word is important. For Hebrew this is not a trivial task, due to the complex nature of Hebrew morphology and its orthography. Many Hebrew words are ambiguous in the sense that each one of them can be created from a few possible roots. However, for a given word in a specific context, each word has only one correct root or no root at all. We have developed a variety of features in order to find the correct root for a Hebrew ambiguous word. These features are classified into 3 distinct groups: root-based features, conjugation-based features and statistical features. Several common machine learning methods have been tested in order to find a successful integration of the features. The best result has been achieved by Na\u00efve Bayes, with about 87% accuracy.","keywords_author":["Disambiguation","Hebrew-Aramaic documents","Machine learning methods","Natural language processing","Stemming"],"keywords_other":["Stemming","Disambiguation","Hebrew-Aramaic documents","Machine learning methods","NAtural language processing"],"max_cite":0.0,"pub_year":2014.0,"sources":"['scp']","rawkeys":["machine learning methods","hebrew-aramaic documents","natural language processing","stemming","disambiguation"],"tags":["machine learning methods","hebrew-aramaic documents","natural language processing","disambiguation","stem"]},{"p_id":584,"title":"A dataset for Turkish dialect recognition and classification with deep learning","abstract":"Dialect Recognition Systems (DRS) are systems that group dialects, according to similar acoustic features found in dialect regions. The speaker's age, gender, and dialect characteristics negatively affect the performance of speech recognition systems. To handle dialect differences, dialect recognition systems can be integrated into speech recognition systems. By determining the spoken dialect, the system can be switched to the corresponding speech recognition model. There is no dataset that can be used for Turkish automatic dialect recognition systems. In this study, it is thought that this deficiency should be eliminated in some way. In addition, an experimental study has been carried out to classify the generated data set by convolutional neural networks. The resulting 83.3% accuracy is satisfactory.","keywords_author":["turkisk dialect recognition","turkisk dialect dataset","convolutional neural networks"],"keywords_other":["Python","DRS","speech recognition","Linguistics","spoken dialect","Turkish automatic dialect recognition systems","Speech processing","Turkish dialect classification","convolution","acoustic features","Machine learning","Convolutional neural networks","feedforward neural nets","learning (artificial intelligence)","convolutional neural networks","deep learning","speech recognition systems","natural language processing","Speech recognition","dialect regions","Acoustics"],"max_cite":null,"pub_year":2018.0,"sources":"['ieee']","rawkeys":["linguistics","speech recognition","spoken dialect","acoustics","turkisk dialect recognition","convolution","machine learning","acoustic features","python","turkish automatic dialect recognition systems","feedforward neural nets","turkish dialect classification","learning (artificial intelligence)","convolutional neural networks","deep learning","speech recognition systems","drs","natural language processing","speech processing","dialect regions","turkisk dialect dataset"],"tags":["linguistics","speech recognition","spoken dialect","convolutional neural network","acoustics","turkisk dialect recognition","convolution","machine learning","acoustic features","python","turkish automatic dialect recognition systems","feedforward neural nets","turkish dialect classification","speech recognition systems","drs","natural language processing","speech processing","dialect regions","turkisk dialect dataset"]},{"p_id":47690,"title":"Deep character-level click-Through rate prediction for sponsored search","abstract":"\u00a9 2017 Copyright held by the owner\/author(s). Predicting the click-Through rate of an advertisement is a critical component of online advertising platforms. In sponsored search, the click-Through rate estimates the probability that a displayed advertisement is clicked by a user a.er she submits a query to the search engine. Commercial search engines typically rely on machine learning models trained with a large number of features to make such predictions. .is inevitably requires a lot of engineering efforts to define, compute, and select the appropriate features. In this paper, we propose two novel approaches (one working at character level and the other working at word level) that use deep convolutional neural networks to predict the click-Through rate of a queryadvertisement pair. Speci.cally, the proposed architectures only consider the textual content appearing in a query-Advertisement pair as input, and produce as output a click-Through rate prediction. By comparing the character-level model with the word-level model, we show that language representation can be learnt from scratch at character level when trained on enough data. .rough extensive experiments using billions of query-Advertisement pairs of a popular commercial search engine, we demonstrate that both approaches significantly outperform a baseline model built on well-selected text features and a state-of-The-Art word2vec-based approach. Finally, by combining the predictions of the deep models introduced in this study with the prediction of the model in production of the same commercial search engine, we significantly improve the accuracy and the calibration of the click-Through rate prediction of the production system.","keywords_author":["CTR Prediction","Deep Learning","NLP","Online Advertising","Sponsored Search"],"keywords_other":["Sponsored searches","Online advertising","Production system","Click-through rate","Proposed architectures","State of the art","Convolutional neural network","Critical component"],"max_cite":0.0,"pub_year":2017.0,"sources":"['scp']","rawkeys":["critical component","nlp","deep learning","online advertising","click-through rate","proposed architectures","state of the art","sponsored searches","convolutional neural network","sponsored search","ctr prediction","production system"],"tags":["critical component","proposed architectures","online advertising","click-through rate","state of the art","machine learning","natural language processing","convolutional neural network","sponsored search","ctr prediction","production system"]},{"p_id":23115,"title":"Hybrid text affect sensing system for emotional language analysis","abstract":"It is argued that for a computer to be able to interact with humans it needs to have the communication skills of humans. One of the main skills of human computer intelligent interaction is the affective aspect of communication, and language is one of the main ways for humans to express emotions. In order to analyze the emotions in language, it is necessary to study the general tone of conversation and the semantic content. This paper explores the influence of affective information about words in sentiment analysis and presents a hybrid statistical-semantic system for opinion detection in Spanish language texts. Affect sensing analysis in language content is very important for achieving realistic interaction with intelligent virtual agents, however it is still an unexplored field nowadays. Copyright \u00a9 2009 ACM.","keywords_author":["Affective computing","Machine-learning algorithms","Natural language processing","Sentiment analysis"],"keywords_other":["Affective Computing","Spanish language","Language content","Language analysis","Sentiment analysis","Semantic systems","Sensing systems","Human computer intelligent interactions","Semantic content","NAtural language processing","Communication skills"],"max_cite":11.0,"pub_year":2009.0,"sources":"['scp']","rawkeys":["semantic content","spanish language","affective computing","communication skills","machine-learning algorithms","natural language processing","semantic systems","sensing systems","human computer intelligent interactions","language analysis","sentiment analysis","language content"],"tags":["semantic content","spanish language","affective computing","communication skills","machine learning algorithms","natural language processing","semantic systems","sensing systems","human computer intelligent interactions","language analysis","sentiment analysis","language content"]},{"p_id":31309,"title":"Identifying duplicate functionality in textual use cases by aligning semantic actions","abstract":"\u00a9 2014, Springer-Verlag Berlin Heidelberg. Developing high-quality requirements specifications often demands a thoughtful analysis and an adequate level of expertise from analysts. Although requirements modeling techniques provide mechanisms for abstraction and clarity, fostering the reuse of shared functionality (e.g., via UML relationships for use cases), they are seldom employed in practice. A particular quality problem of textual requirements, such as use cases, is that of having duplicate pieces of functionality scattered across the specifications. Duplicate functionality can sometimes improve readability for end users, but hinders development-related tasks such as effort estimation, feature prioritization, and maintenance, among others. Unfortunately, inspecting textual requirements by hand in order to deal with redundant functionality can be an arduous, time-consuming, and error-prone activity for analysts. In this context, we introduce a novel approach called ReqAligner that aids analysts to spot signs of duplication in use cases in an automated fashion. To do so, ReqAligner combines several text processing techniques, such as a use case-aware classifier and a customized algorithm for sequence alignment. Essentially, the classifier converts the use cases into an abstract representation that consists of sequences of semantic actions, and then these sequences are compared pairwise in order to identify action matches, which become possible duplications. We have applied our technique to five real-world specifications, achieving promising results and identifying many sources of duplication in the use cases.","keywords_author":["Machine learning","Natural language processing","Requirements engineering","Sequence alignment","Use case modeling","Use case refactoring"],"keywords_other":["Abstract representation","Use case model","Requirements Models","Sequence alignments","Quality problems","Use case refactoring","Effort Estimation","NAtural language processing"],"max_cite":3.0,"pub_year":2016.0,"sources":"['scp']","rawkeys":["quality problems","requirements engineering","natural language processing","machine learning","sequence alignments","use case model","requirements models","sequence alignment","effort estimation","use case refactoring","abstract representation","use case modeling"],"tags":["quality problems","requirements engineering","natural language processing","machine learning","use case model","requirements models","sequence alignment","effort estimation","use case refactoring","abstract representation"]},{"p_id":43598,"title":"Quantitative Analysis of Uncertainty in Medical Reporting: Creating a Standardized and Objective Methodology","abstract":"\u00a9 2017, Society for Imaging Informatics in Medicine.Uncertainty in text-based medical reports has long been recognized as problematic, frequently resulting in misunderstanding and miscommunication. One strategy for addressing the negative clinical ramifications of report uncertainty would be the creation of a standardized methodology for characterizing and quantifying uncertainty language, which could provide both the report author and reader with context related to the perceived level of diagnostic confidence and accuracy. A number of computerized strategies could be employed in the creation of this analysis including string search, natural language processing and understanding, histogram analysis, topic modeling, and machine learning. The derived uncertainty data offers the potential to objectively analyze report uncertainty in real time and correlate with outcomes analysis for the purpose of context and user-specific decision support at the point of care, where intervention would have the greatest clinical impact.","keywords_author":["Data mining","Machine learning","Natural language processing","Report uncertainty"],"keywords_other":["String search","Histogram analysis","Report uncertainty","Decision supports","Point of care","Real time","Topic Modeling"],"max_cite":0.0,"pub_year":2018.0,"sources":"['scp']","rawkeys":["data mining","point of care","real time","topic modeling","machine learning","natural language processing","report uncertainty","decision supports","string search","histogram analysis"],"tags":["data mining","point of care","real time","topic modeling","machine learning","natural language processing","report uncertainty","decision supports","string search","histogram analysis"]},{"p_id":21071,"title":"A study on LIWC categories for opinion mining in Spanish reviews","abstract":"\u00a9 The Author(s) 2014. With the exponential growth of social media, that is, blogs and social networks, organizations and individual persons are increasingly using the number of reviews of these media for decision-making about a product or service. Opinion mining detects whether the emotions of an opinion expressed by a user on Web platforms in natural language are positive or negative. This paper presents extensive experiments to study the effectiveness of the classification of Spanish opinions in five categories: highly positive, highly negative, positive, negative and neutral, using the combination of the psychological and linguistic features of LIWC (Linguistic Inquiry and Word Count). LIWC is a text analysis software that enables the extraction of different psychological and linguistic features from natural language text. For this study, two corpora have been used, one about movies and one about technological products. Furthermore, we conducted a comparative assessment of the performance of various classification techniques, J48, SMO and BayesNet, using precision, recall and F-measure metrics. The findings revealed that the positive and negative categories provide better results than the other categories. Finally, experiments on both corpora indicated that SMO produces better results than BayesNet and J48 algorithms, obtaining an F-measure of 90.4 and 87.2% in each domain.","keywords_author":["Machine learning","Natural language processing with LIWC","Opinion mining","Sentiment analysis"],"keywords_other":["Classification technique","Comparative assessment","Sentiment analysis","Natural language text","Linguistic features","NAtural language processing","Exponential growth","Opinion mining"],"max_cite":15.0,"pub_year":2014.0,"sources":"['scp', 'wos']","rawkeys":["machine learning","natural language processing","natural language processing with liwc","linguistic features","classification technique","comparative assessment","exponential growth","opinion mining","sentiment analysis","natural language text"],"tags":["machine learning","natural language processing","natural language processing with liwc","linguistic features","classification technique","comparative assessment","exponential growth","opinion mining","sentiment analysis","natural language text"]},{"p_id":37455,"title":"Survey of scaling platforms for Deep Neural Networks","abstract":"\u00a9 2016 IEEE. Deep Neural Networks have become a state of the art approach in perception processing like speech recognition, image processing and natural language processing. Many state of the art benchmarks for these algorithms are using deep learning techniques. The deep neural networks in today's applications need to process very large amount of data. Different approaches have been proposed to solve scaling these algorithms. Few approach look for providing a solution over existing big data processing platform which usually runs over a large scale commodity cpu cluster. As training deep learning workload require many small computations to be done and large communication to pass the data between layers, General Purpose GPUs seems to the best platforms to train these networks. Different approaches have been proposed to scale processing on cluster of GPU servers. We have summarized various approaches used in this regard.","keywords_author":["big data","deep learning","deep neural network","neuromorphic machines","quantum computing"],"keywords_other":["General-purpose GPUs","Neuromorphic","State of the art","Learning techniques","State-of-the-art approach","Quantum Computing","NAtural language processing","Large amounts"],"max_cite":1.0,"pub_year":2017.0,"sources":"['scp']","rawkeys":["deep neural network","big data","deep learning","neuromorphic machines","state of the art","natural language processing","quantum computing","learning techniques","neuromorphic","general-purpose gpus","large amounts","state-of-the-art approach"],"tags":["big data","neuromorphic machines","state of the art","machine learning","natural language processing","quantum computing","learning techniques","neuromorphic","convolutional neural network","general-purpose gpus","large amounts","state-of-the-art approach"]},{"p_id":590,"title":"Turkish named entity recognition with deep learning","abstract":"Named Entity Recognition (NER) is an important task in Natural Language Processing, Data Mining and Information Extraction areas since 1990's. While NER is a succesfully solved problem in English, it is still a hot topic in agglutinative languages like Turkish, Czech, Finnish languages. With the scope of this study we focus on Bidirectional Long Short-Term Memory (BLSTM) neural network models to solve NER problem. We suggest a succesful implementation of Deep Bidirectional Long Short Term Memory (DBLSTM) which reaches %93.69 F1 score, which is state-of-the-art result for Named Entity Recognition in Turkish.","keywords_author":["Bidirectional Long Short-Term Memory (BLSTM)","Deep Bidirectional Long Short-Term Memory (DBLSTM)","Natural Language Processing","Named Entity Recognition"],"keywords_other":["bidirectional long short-term memory neural network models","named entity recognition","Conferences","deep bidirectional long short term memory","Data mining","recurrent neural nets","information extraction","NER problem","Finnish languages","data mining","learning (artificial intelligence)","deep learning","Hidden Markov models","Task analysis","neural nets","Czech languages","Information retrieval","natural language processing","Turkish languages","text analysis","Natural language processing"],"max_cite":null,"pub_year":2018.0,"sources":"['ieee']","rawkeys":["task analysis","bidirectional long short-term memory neural network models","named entity recognition","finnish languages","deep bidirectional long short term memory","recurrent neural nets","conferences","information extraction","deep bidirectional long short-term memory (dblstm)","turkish languages","data mining","bidirectional long short-term memory (blstm)","learning (artificial intelligence)","deep learning","ner problem","information retrieval","neural nets","hidden markov models","czech languages","natural language processing","text analysis"],"tags":["task analysis","bidirectional long short-term memory neural network models","named entity recognition","finnish languages","deep bidirectional long short term memory","recurrent neural nets","conferences","information extraction","machine learning","blstm","czech language","data mining","neural networks","ner problem","information retrieval","hidden markov models","natural language processing","text analysis","turkish language"]},{"p_id":31314,"title":"GLAsT: Learning formal grammars to translate natural language specifications into hardware assertions","abstract":"\u00a9 2016 EDAA. The purpose of functional verification is to ensure that a design conforms to its specification. However, large written specifications can contain hundreds of statements describing correct operation which an engineer must use to create sets of correctness properties. This laborious manual process increases both verification time and cost. In this work we present GLAsT, a new learning algorithm which accepts a small set of sentences describing correctness properties and corresponding SystemVerilog Assertions (SVAs). GLAsT creates a custom formal grammar which captures the writing style and sentence structure of a specification and facilitates the automatic translation of English specification sentences into formal SystemVerilog Assertions. We evaluate GLAsT on English sentences from two ARM AMBA bus protocols. Results show that a translation system using the formal grammar generated by GLAsT automatically generates correctly formed SVAs from the targeted AMBA specification as well as from a second, different AMBA bus specification.","keywords_author":["hardware design","machine learning","natural language processing","verification"],"keywords_other":["Automatic translation","Functional verification","Hardware design","SystemVerilog assertions","Correctness properties","Sentence structures","NAtural language processing","Natural language specifications"],"max_cite":3.0,"pub_year":2016.0,"sources":"['scp']","rawkeys":["automatic translation","hardware design","systemverilog assertions","correctness properties","machine learning","natural language processing","verification","functional verification","natural language specifications","sentence structures"],"tags":["automatic translation","hardware design","systemverilog assertions","correctness properties","machine learning","natural language processing","verification","functional verification","natural language specifications","sentence structures"]},{"p_id":39510,"title":"Approaches to samples selection for machine learning based classification of textual data","abstract":"The paper focuses on the process of selecting representative sample documents written in a natural language that can be used as the basis for automatic selection or classification of textual documents. A method of selecting the examples from a larger set of candidate examples, called automatic biased sample selection, is compared to random and manual selection. The methods are evaluated by experiments carried out with real world data consisting of customer reviews, with different document representations and similarity measures. The presented approach, that provided satisfactory results, faces problems related to processing user created content and huge computational complexity and can be used as an alternative to manual selection and evaluation of textual samples.","keywords_author":["Information retrieval","Machine learning","Natural language processing","Text classification","Text similarity","Textual patterns"],"keywords_other":["Text similarity","Document Representation","User created content","Text classification","Selection and evaluations","Textual patterns","Representative sample","NAtural language processing"],"max_cite":1.0,"pub_year":2013.0,"sources":"['scp']","rawkeys":["user created content","textual patterns","natural language processing","machine learning","information retrieval","text similarity","selection and evaluations","document representation","text classification","representative sample"],"tags":["user created content","textual patterns","natural language processing","machine learning","information retrieval","text similarity","selection and evaluations","document representation","text classification","representative sample"]},{"p_id":47702,"title":"Deep learning for sentence classification","abstract":"\u00a9 2017 IEEE. Most of the machine learning algorithms requires the input to be denoted as a fixed-length feature vector. In text classifications (bag-of-words) is a popular fixed-length features. Despite their simplicity, they are limited in many tasks; they ignore semantics of words and loss ordering of words. In this paper, we propose a simple and efficient neural language model for sentence-level classification task. Our model employs Recurrent Neural Network Language Model (RNN-LM). Particularly, Long Short-Term Memory (LSTM) over pre-trained word vectors obtained from unsupervised neural language model to capture semantics and syntactic information in a short sentence. We achieved outstanding empirical results on multiple benchmark datasets, IMDB Sentiment analysis dataset, and Stanford Sentiment Treebank (SSTb) dataset. The empirical results show that our model is comparable with neural methods and outperforms traditional methods in sentiment analysis task.","keywords_author":["Deep Learning","LSTM","Natural Language Processing","Recurrent neural network","RNN","Sentiment analysis"],"keywords_other":["Syntactic information","Text classification","Classification tasks","Benchmark datasets","Sentence classifications","Sentiment analysis","LSTM","Feature vectors"],"max_cite":0.0,"pub_year":2017.0,"sources":"['scp']","rawkeys":["benchmark datasets","text classification","lstm","deep learning","natural language processing","recurrent neural network","feature vectors","classification tasks","rnn","sentence classifications","syntactic information","sentiment analysis"],"tags":["benchmark datasets","text classification","neural networks","long short-term memory","machine learning","natural language processing","feature vectors","classification tasks","sentence classifications","syntactic information","sentiment analysis"]},{"p_id":43611,"title":"Deep neural models for ICD-10 coding of death certificates and autopsy reports in free-text","abstract":"\u00a9 2018 Elsevier Inc. We address the assignment of ICD-10 codes for causes of death by analyzing free-text descriptions in death certificates, together with the associated autopsy reports and clinical bulletins, from the Portuguese Ministry of Health. We leverage a deep neural network that combines word embeddings, recurrent units, and neural attention, for the generation of intermediate representations of the textual contents. The neural network also explores the hierarchical nature of the input data, by building representations from the sequences of words within individual fields, which are then combined according to the sequences of fields that compose the inputs. Moreover, we explore innovative mechanisms for initializing the weights of the final nodes of the network, leveraging co-occurrences between classes together with the hierarchical structure of ICD-10. Experimental results attest to the contribution of the different neural network components. Our best model achieves accuracy scores over 89%, 81%, and 76%, respectively for ICD-10 chapters, blocks, and full-codes. Through examples, we also show that our method can produce interpretable results, useful for public health surveillance.","keywords_author":["Artificial intelligence in medicine","Automated ICD coding","Clinical text mining","Deep learning","Natural language processing"],"keywords_other":["Text mining","Intermediate representations","Textual content","Hierarchical structures","Artificial intelligence in medicine","Causes of death","Automated ICD coding","Public health surveillances"],"max_cite":0.0,"pub_year":2018.0,"sources":"['scp']","rawkeys":["hierarchical structures","text mining","public health surveillances","deep learning","natural language processing","intermediate representations","clinical text mining","causes of death","textual content","artificial intelligence in medicine","automated icd coding"],"tags":["hierarchical structures","text mining","public health surveillances","machine learning","intermediate representations","natural language processing","clinical text mining","causes of death","textual content","artificial intelligence in medicine","automated icd coding"]},{"p_id":23137,"title":"Combining automatic acquisition of knowledge with machine learning approaches for multilingual temporal recognition and normalization","abstract":"This paper presents an improvement in the temporal expression (TE) recognition phase of a knowledge based system at a multilingual level. For this purpose, the combination of different approaches applied to the recognition of temporal expressions are studied. In this work, for the recognition task, a knowledge based system that recognizes temporal expressions and had been automatically extended to other languages (TERSEO system) was combined with a system that recognizes temporal expressions using machine learning techniques. In particular, two different techniques were applied: maximum entropy model (ME) and hidden Markov model (HMM), using two different types of tagging of the training corpus: (1) BIO model tagging of literal temporal expressions and (2) BIO model tagging of simple patterns of temporal expressions. Each system was first evaluated independently and then combined in order to: (a) analyze if the combination gives better results without increasing the number of erroneous expressions in the same percentage and (b) decide which machine learning approach performs this task better. When the TERSEO system is combined with the maximum entropy approach the best results for F-measure (89%) are obtained, improving TERSEO recognition by 4.5 points and ME recognition by 7. \u00a9 2008 Elsevier Inc. All rights reserved.","keywords_author":["Machine learning","Natural language processing","Temporal expression normalization","Temporal expression recognition","Temporal expressions","Temporal information","Temporal reasoning"],"keywords_other":["Maximum Entropy Model (MEM)","Knowledge base (KB)","Maximum entropy (MAXENT)","F measure","In order","Different types","Automatic acquisition","Hidden markov model (HMM)","training corpus","Applied (CO)","Elsevier (CO)","Temporal expression (TE)","machine-learning","Languages (traditional)","Machine learning techniques"],"max_cite":11.0,"pub_year":2008.0,"sources":"['scp']","rawkeys":["languages (traditional)","temporal reasoning","maximum entropy (maxent)","maximum entropy model (mem)","in order","temporal expression normalization","different types","machine learning","training corpus","temporal information","temporal expression (te)","machine learning techniques","hidden markov model (hmm)","machine-learning","temporal expression recognition","f measure","temporal expressions","natural language processing","applied (co)","elsevier (co)","knowledge base (kb)","automatic acquisition"],"tags":["maximum entropy","temporal reasoning","knowledge base","maximum entropy models","in order","language","temporal expression normalization","different types","machine learning","training corpus","f-measure","temporal information","machine learning techniques","temporal expression recognition","hidden markov models","temporal expressions","natural language processing","applied (co)","elsevier (co)","automatic acquisition"]},{"p_id":33378,"title":"Why was this asked? Automatically recognizing multiple motivations behind community question-answering questions","abstract":"\u00a9 2017 Elsevier LtdCommunity Question Answering (cQA) services encourage their members to ask any kind of question, which later on can get multiple answers from other community fellows. The research objective of this paper is understanding, and particularly automatically detecting, what motivates community members to ask questions to their unknown peers. In so doing, we first crawled a set of cQA questions from Yahoo! Answers, each of which was manually labelled according to their multiple motivations afterwards. Thus, one of the innovative aspects of our work is exploring a wide variety of multi-label classification strategies for the automatic recognition of concurrent motivations behind cQA questions. In order to build effective models, high-dimensional feature spaces were constructed on top of assorted linguistic features, this way discovering some linguistic traits that characterize some of these combinations. Overall, our experiments reveal that multi-label classification frameworks hold a real promise for this task. More precisely, our best configuration finished with a Hamming Score of 0.71. In terms of features, our outcomes unveil that the concurrence of motivations is likely to be signalled by the complexity in writing and the distribution of entity mentions across the entire question, ergo both question titles and bodies are required to be able to recognize their confluence.","keywords_author":["Community question answering","Multi-label deep learning","Multi-label learning","Natural language processing","Question analysis","User analysis"],"keywords_other":["Community question answering","Multi-label","Multi-label learning","User analysis","Question analysis","NAtural language processing"],"max_cite":2.0,"pub_year":2017.0,"sources":"['scp']","rawkeys":["multi-label learning","question analysis","multi-label","user analysis","natural language processing","multi-label deep learning","community question answering"],"tags":["multi-label learning","question analysis","multi-label","user analysis","natural language processing","multi-label deep learning","community question answering"]},{"p_id":45670,"title":"An Arabic natural language interface for querying relational databases based on natural language processing and graph theory methods","abstract":"\u00a9 2018 Inderscience Enterprises Ltd. Nowadays, databases represent a great source of information. To extract information from these databases, the user needs to write queries using database query languages, such as structured query language (SQL). Generally, for using this language, this user must know the database structure. However, this task can be difficult for non-expert users. In that, the use of natural language to extract data from the database can be an important method. The problems in using natural language query are that it does not give any specification about the path access corresponding to the required data. In this paper, a model of a natural language interface for databases is presented. This interface allows the user to extract data from a database by using Arabic language and it obviates the need for users to know the database structure. Also, it can function independently of the database domain and it can to improve its knowledge base through experience.","keywords_author":["Arabic language","Database","Database structure","Dijkstra algorithm","Extended context free grammar","Graph theory","Knowledge base","Machine learning approach","Natural language interface","Natural language processing"],"keywords_other":["Database structures","Machine learning approaches","Dijkstra algorithms","Arabic languages","Natural language interfaces","Knowledge base"],"max_cite":0.0,"pub_year":2018.0,"sources":"['scp']","rawkeys":["machine learning approach","natural language interfaces","graph theory","arabic language","dijkstra algorithms","database structures","natural language processing","knowledge base","extended context free grammar","database","database structure","arabic languages","dijkstra algorithm","machine learning approaches","natural language interface"],"tags":["natural language interfaces","extended context free grammar (ecfg)","graph theory","databases","database structures","natural language processing","knowledge base","arabic languages","dijkstra algorithm","machine learning approaches"]},{"p_id":29287,"title":"A simple and efficient algorithm for authorship verification","abstract":"\u00a9 2016 ASIS&TThis paper describes and evaluates an unsupervised and effective authorship verification model called Spatium-L1. As features, we suggest using the 200 most frequent terms of the disputed text (isolated words and punctuation symbols). Applying a simple distance measure and a set of impostors, we can determine whether or not the disputed text was written by the proposed author. Moreover, based on a simple rule we can define when there is enough evidence to propose an answer or when the attribution scheme is unable to make a decision with a high degree of certainty. Evaluations based on 6 test collections (PAN CLEF 2014 evaluation campaign) indicate that Spatium-L1 usually appears in the top 3 best verification systems, and on an aggregate measure, presents the best performance. The suggested strategy can be adapted without any problem to different Indo-European languages (such as English, Dutch, Spanish, and Greek) or genres (essay, novel, review, and newspaper article).","keywords_author":["machine learning","natural language processing","text mining"],"keywords_other":["Text mining","European languages","Simple and efficient algorithms","Authorship verification","Degree of certainty","Aggregate measures","Verification systems","NAtural language processing"],"max_cite":4.0,"pub_year":2017.0,"sources":"['scp']","rawkeys":["text mining","aggregate measures","machine learning","natural language processing","degree of certainty","verification systems","european languages","authorship verification","simple and efficient algorithms"],"tags":["text mining","aggregate measures","machine learning","natural language processing","degree of certainty","verification systems","european languages","authorship verification","simple and efficient algorithms"]},{"p_id":47721,"title":"Construction of the Malay language psychometric properties using LIWC from facebook statuses","abstract":"\u00a9 2017 American Scientific Publishers. All rights reserved.This work aims at constructing a Malay language psychometric dictionary from Facebook statuses for LIWC by using the direct translation approach. The linguistic were obtained from 160 Malaysian Facebook users with an extracted textual data derived from 12573 wall-posted statuses. The jury method is used to assign each word to its correct category by comparing the direct translation of the word in Malay versus the English word in the English psychometric dictionary. LIWC classifies words by categories thus assigning the correct ones to the most precise translations and usage are essential. The work successfully categorized 2820 Malay words from the Malaysian Facebook statuses. This methodological note declares the creation of the Malay language version of the LIWC and the exemplary application of the dictionary in classifying personality of extraversion of the Malaysian Facebook users. The constructed Malay LIWC dictionary can be applied to any natural language processing framework or machine learning area that focuses on mining the Malay text language.","keywords_author":["Facebook","Machine learning","Malay language","Malay LIWC","Natural language processing","Psychometric properties","Sentiment analysis","Text classification"],"keywords_other":null,"max_cite":0.0,"pub_year":2017.0,"sources":"['scp']","rawkeys":["facebook","text classification","natural language processing","machine learning","malay liwc","psychometric properties","malay language","sentiment analysis"],"tags":["facebook","text classification","natural language processing","machine learning","malay liwc","psychometric properties","malay language","sentiment analysis"]},{"p_id":27242,"title":"From NLP (Natural Language Processing) to MLP (Machine Language Processing)","abstract":"Natural Language Processing (NLP) in combination with Machine Learning techniques plays an important role in the field of automatic text analysis. Motivated by the successful use of NLP in solving text classification problems in the area of e-Participation and inspired by our prior work in the field of polymorphic shellcode detection we gave classical NLP-processes a trial in the special case of malicious code analysis. Any malicious program is based on some kind of machine language, ranging from manually crafted assembler code that exploits a buffer overflow to high level languages such as Javascript used in web-based attacks. We argue that well known NLP analysis processes can be modified and applied to the malware analysis domain. Similar to the NLP process we call this process Machine Language Processing (MLP). In this paper, we use our e-Participation analysis architecture, extract the various NLP techniques and adopt them for the malware analysis process. As proof-of-concept we apply the adopted framework to malicious code examples from Metasploit. \u00a9 Springer-Verlag 2010.","keywords_author":["Knowledge mining","Machine language processing","Machine learning","Malware analysis","Natural language processing","Semantic networks"],"keywords_other":["NAtural language processing","Machine languages","Machine-learning","Semantic networks","Knowledge mining","Malware analysis"],"max_cite":6.0,"pub_year":2010.0,"sources":"['scp']","rawkeys":["machine languages","knowledge mining","machine learning","natural language processing","machine language processing","malware analysis","machine-learning","semantic networks"],"tags":["machine languages","semantic network","knowledge mining","machine learning","natural language processing","machine language processing","malware analysis"]},{"p_id":47722,"title":"Aspect-Based Sentiment Analysis Based on Multi-Attention CNN","abstract":"\u00a9 2017, Science Press. All right reserved. Unlike general sentiment analysis, aspect-based sentiment classification aims to infer the sentiment polarity of a sentence depending not only on the context but also on the aspect. For example, in sentence \u201cThe food was very good, but the service at that restaurant was dreadful\u201d, for aspect \u201cfood\u201d, the sentiment polarity is positive while the sentiment polarity of aspect \u201cservice\u201d is negative. Even in the same sentence, sentiment polarity could be absolutely opposite when focusing on different aspects, so we need to infer the sentiment polarities of different aspects correctly. The attention mechanism is a good way for aspect-based sentiment classification. In current research, however, the attention mechanism is more combined with RNN or LSTM networks. Such neural network-based architectures generally rely on complex structures and cannot parallelize over the words of a sentence. To address the above problems, this paper proposes a multi-attention convolutional neural networks (MATT-CNN) for aspect-based sentiment classification. This approach can capture deeper level sentiment information and distinguish sentiment polarity of different aspects explicitly through a multi-attention mechanism without using any external parsing results. Experiments on the SemEval2014 and Automotive-domain datasets show that, our approach achieves better performance than traditional CNN, attention-based CNN and attention-based LSTM.","keywords_author":["Aspect-based sentiment analysis","Attention mechanism","Convolutional neural networks","Deep learning","Natural language processing"],"keywords_other":["Complex structure","Network-based architectures","Automotive domains","Sentiment analysis","Convolutional neural network","Sentiment classification","Attention mechanisms"],"max_cite":0.0,"pub_year":2017.0,"sources":"['scp']","rawkeys":["sentiment classification","automotive domains","convolutional neural networks","aspect-based sentiment analysis","complex structure","deep learning","attention mechanisms","natural language processing","attention mechanism","convolutional neural network","network-based architectures","sentiment analysis"],"tags":["sentiment classification","automotive domains","aspect-based sentiment analysis","complex structure","attention mechanisms","machine learning","natural language processing","convolutional neural network","network-based architectures","sentiment analysis"]},{"p_id":47725,"title":"Box office revenue prediction using dual sentiment analysis","abstract":"Twitter is amongst the most widely used social networking website and it is also a reliable source of mass opinion. Success of a movie can be predicted by analyzing tweets and examining the impact of movie on the mob. Pre-release buzz can also be captured through tweets. This knowledge helps in predicting the success of a movie and its approximate revenue. In this paper, Dual Sentiment Analysis (DSA) is used for sentiment analysis of tweets that avoids sentiment classification problems and improves performance. Along with sentiment analysis of tweets, contribution of other factors such as star cast, holiday effect, sequel and genre are also considered. Finally, multivariate linear regression is performed on all above-mentioned factors to predict the Box Office revenue of a movie. The results show that this proposed system performs better while providing better accuracy.","keywords_author":["Machine learning","Natural language processing","Opinion mining","Sentiment analysis","Social media"],"keywords_other":null,"max_cite":0.0,"pub_year":2017.0,"sources":"['scp']","rawkeys":["social media","natural language processing","machine learning","opinion mining","sentiment analysis"],"tags":["social media","natural language processing","machine learning","opinion mining","sentiment analysis"]},{"p_id":37486,"title":"Automatic detection of online recruitment frauds: Characteristics, methods, and a public datase","abstract":"\u00a9 2017 by the authors. The critical process of hiring has relatively recently been ported to the cloud. Specifically, the automated systems responsible for completing the recruitment of new employees in an online fashion, aim to make the hiring process more immediate, accurate and cost-efficient. However, the online exposure of such traditional business procedures has introduced new points of failure that may lead to privacy loss for applicants and harm the reputation of organizations. So far, the most common case of Online Recruitment Frauds (ORF), is employment scam. Unlike relevant online fraud problems, the tackling of ORF has not yet received the proper attention, remaining largely unexplored until now. Responding to this need, the work at hand defines and describes the characteristics of this severe and timely novel cyber security research topic. At the same time, it contributes and evaluates the first to our knowledge publicly available dataset of 17,880 annotated job ads, retrieved from the use of a real-life system.","keywords_author":["Data mining","Dataset","Employment scam","Fraud detection","Job scam","Machine learning","Natural language processing","Online recruitment"],"keywords_other":["Dataset","Online recruitment","Fraud detection","NAtural language processing","Job scam"],"max_cite":1.0,"pub_year":2017.0,"sources":"['scp']","rawkeys":["data mining","dataset","machine learning","natural language processing","fraud detection","job scam","employment scam","online recruitment"],"tags":["data mining","machine learning","natural language processing","fraud detection","data sets","job scam","employment scam","online recruitment"]},{"p_id":43633,"title":"Automatic Determination of the Need for Intravenous Contrast in Musculoskeletal MRI Examinations Using IBM Watson\u2019s Natural Language Processing Algorithm","abstract":"\u00a9 2017, Society for Imaging Informatics in Medicine. Magnetic resonance imaging (MRI) protocoling can be time- and resource-intensive, and protocols can often be suboptimal dependent upon the expertise or preferences of the protocoling radiologist. Providing a best-practice recommendation for an MRI protocol has the potential to improve efficiency and decrease the likelihood of a suboptimal or erroneous study. The goal of this study was to develop and validate a machine learning-based natural language classifier that can automatically assign the use of intravenous contrast for musculoskeletal MRI protocols based upon the free-text clinical indication of the study, thereby improving efficiency of the protocoling radiologist and potentially decreasing errors. We utilized a deep learning-based natural language classification system from IBM Watson, a question-answering supercomputer that gained fame after challenging the best human players on Jeopardy! in 2011. We compared this solution to a series of traditional machine learning-based natural language processing techniques that utilize a term-document frequency matrix. Each classifier was trained with 1240 MRI protocols plus their respective clinical indications and validated with a test set of 280. Ground truth of contrast assignment was obtained from the clinical record. For evaluation of inter-reader agreement, a blinded second reader radiologist analyzed all cases and determined contrast assignment based on only the free-text clinical indication. In the test set, Watson demonstrated overall accuracy of 83.2% when compared to the original protocol. This was similar to the overall accuracy of 80.2% achieved by an ensemble of eight traditional machine learning algorithms based on a term-document matrix. When compared to the second reader\u2019s contrast assignment, Watson achieved 88.6% agreement. When evaluating only the subset of cases where the original protocol and second reader were concordant (n = 251), agreement climbed further to 90.0%. The classifier was relatively robust to spelling and grammatical errors, which were frequent. Implementation of this automated MR contrast determination system as a clinical decision support tool may save considerable time and effort of the radiologist while potentially decreasing error rates, and require no change in order entry or workflow.","keywords_author":["Artificial intelligence","Deep learning","IBM Watson","Imaging protocol","Machine learning","Natural language processing (NLP)","Quality improvement","Workflow efficiency"],"keywords_other":["Classification system","Quality improvement","Imaging protocol","Intravenous contrast","Improving efficiency","Clinical decision support","IBM Watson","Automatic determination"],"max_cite":0.0,"pub_year":2018.0,"sources":"['scp']","rawkeys":["artificial intelligence","classification system","intravenous contrast","workflow efficiency","deep learning","machine learning","imaging protocol","natural language processing (nlp)","quality improvement","clinical decision support","ibm watson","improving efficiency","automatic determination"],"tags":["intravenous contrast","classification system","workflow efficiency","machine learning","imaging protocol","natural language processing","quality improvement","clinical decision support","ibm watson","improving efficiency","automatic determination"]},{"p_id":626,"title":"Study of text emotion analysis based on deep learning","abstract":"In traditional sentiment classification methods, sentiment-based methods rely heavily on the quality and coverage of sentiment lexicons, whereas machine-based approaches rely on features of manual construction and decimation. In recent years, deep learning technology has made great progress in the field of natural language processing, depth model has more powerful skills. This article focuses on several commonly used deep learning models for textual affective classification and compares their strengths and weaknesses.","keywords_author":["deep learning","convolutional neural networks","recurrent neural networks","short and long-term memory recurrent neural networks Introduction"],"keywords_other":["text emotion analysis","sentiment lexicons","learning (artificial intelligence)","textual affective classification","Recurrent neural networks","natural language processing","Biological neural networks","Dictionaries","decimation","Machine learning","sentiment classification methods","emotion recognition","Convolution","Convolutional neural networks","deep learning technology","Logic gates","pattern classification","sentiment analysis"],"max_cite":null,"pub_year":2018.0,"sources":"['ieee']","rawkeys":["decimation","recurrent neural networks","logic gates","sentiment analysis","text emotion analysis","textual affective classification","convolution","machine learning","biological neural networks","sentiment lexicons","learning (artificial intelligence)","convolutional neural networks","deep learning","short and long-term memory recurrent neural networks introduction","pattern classification","dictionaries","natural language processing","emotion recognition","sentiment classification methods","deep learning technology"],"tags":["sentiment lexicons","text emotion analysis","neural networks","textual affective classification","convolution","machine learning","natural language processing","short and long-term memory recurrent neural networks introduction","decimation","emotion recognition","sentiment classification methods","convolutional neural network","deep learning technology","biological neural networks","pattern classification","dictionaries","sentiment analysis","logic gates"]},{"p_id":51835,"title":"NEBEL: Never-ending bilingual equivalent learner","abstract":"\u00a9 Springer International Publishing Switzerland 2014. In this paper, we present NEBEL: an automatic system able to learn bilingual equivalents (translations) using the never-ending machine learning (NEML) strategy. Motivated by the way humans learn, the NEML is a continuous learning strategy which uses the knowledge already acquired to learn new information and, therefore, to improve its performance. The NEML was chosen to be applied in our context because it has two desirable features to deal with our intended problem: (i) it uses the Internet as knowledge source and (ii) it combines different extractions methods to improve the final result. In the experiments presented in this paper, NEBEL reached 65% accuracy in the English-Portuguese pair of languages.","keywords_author":["Bilingual lexicon","Natural language processing","Never-ending machine learning"],"keywords_other":["Automatic systems","Bilingual lexicons","Knowledge sources","Desirable features","Continuous learning","NAtural language processing"],"max_cite":0.0,"pub_year":2014.0,"sources":"['scp']","rawkeys":["never-ending machine learning","continuous learning","natural language processing","automatic systems","bilingual lexicon","desirable features","bilingual lexicons","knowledge sources"],"tags":["never-ending machine learning","continuous learning","natural language processing","automatic systems","desirable features","bilingual lexicons","knowledge sources"]},{"p_id":39548,"title":"Automatic extraction of personal experiences from patients' blogs: A case study in chronic obstructive pulmonary disease","abstract":"People with long-term illness such as chronic obstructive pulmonary disease (COPD) often use social media to document and share information, opinions and their experiences with others. Analysing the self-reported experiences of patients shared online has the potential to help medical researchers gain insight into some of the key issues affecting patients. However, the scale of health conversation taking place online poses considerable challenges to traditional content analysis. In this paper, we present a system which automates extraction of patient statements which refer to a personal experience. We applied a crowdsourcing methodology to create a set of 1770 annotated sentences from blog posts written by COPD patients. Our machine learning approach trained on lexical features successfully extracted sentences about patient experience with 93% precision and 80% recall (F-measure: 86%). Automatic annotation of sentences about patient experience can facilitate subsequent content analysis by highlighting the most relevant sentences to this particular problem. \u00a9 2013 IEEE.","keywords_author":["Blog mining","Blogs","Health informatics","Machine learning","Natural language processing","Social m edia","Text processing"],"keywords_other":["Chronic obstructive pulmonary disease","Health informatics","Machine learning approaches","Automatic annotation","Social m edia","Automatic extraction","Blog mining","NAtural language processing"],"max_cite":1.0,"pub_year":2013.0,"sources":"['scp']","rawkeys":["blog mining","chronic obstructive pulmonary disease","automatic annotation","machine learning","natural language processing","automatic extraction","blogs","health informatics","social m edia","machine learning approaches","text processing"],"tags":["blog mining","chronic obstructive pulmonary disease","social media","automatic annotation","machine learning","natural language processing","automatic extraction","blogs","health informatics","machine learning approaches","text processing"]},{"p_id":29309,"title":"Efficient adverse drug event extraction using Twitter sentiment analysis","abstract":"\u00a9 2016 IEEE. Extensive clinical trials are required before a drug is placed on the market; yet it is difficult to discover all the side effects for any approved drugs. The United States Food and Drug Administration actively monitors approved medications to identify adverse events. The FDA Adverse Event Reporting System contains a database of adverse drug events (ADE) reported by the healthcare providers and consumers. The pervasive online social networks, such as Twitter, can provide additional information ADE. Concurrently, advancements in social media technology have resulted in the booming of massive public data; the availability of these huge datasets offers numerous research opportunities for extracting ADEs. Towards this purpose, in this paper a simple, effective computation pipeline is proposed, which uses simple drug-related classification and sentiment analysis to extract ADEs on Twitter. The pipeline is described in detail, and is implemented into an automatic process. Experiments are carried out based on 4-months of Twitter data collected. Comparing with an existing pipeline, the new design is able to successfully capture 5 times more valid ADEs, among them 20% are new ADEs. The proposed method may be applied to other areas such as food, beverages, and other daily consumer products for identifying side effects and user opinions.","keywords_author":["ADEs","Data Mining","Machine Learning","Nature Language Processing","Sentiment Analysis"],"keywords_other":["ADEs","Event extraction","On-line social networks","Sentiment analysis","Research opportunities","Nature language processing","Health care providers","United states food and drug administrations"],"max_cite":4.0,"pub_year":2016.0,"sources":"['scp']","rawkeys":["ades","on-line social networks","data mining","united states food and drug administrations","machine learning","nature language processing","health care providers","research opportunities","event extraction","sentiment analysis"],"tags":["ade","on-line social networks","data mining","united states food and drug administrations","machine learning","natural language processing","health care providers","research opportunities","event extraction","sentiment analysis"]},{"p_id":39549,"title":"Automatic identification of comparative effectiveness research from medline citations to support clinicians' treatment information needs","abstract":"Online knowledge resources such as Medline can address most clinicians' patient care information needs. Yet, significant barriers, notably lack of time, limit the use of these sources at the point of care. The most common information needs raised by clinicians are treatment-related. Comparative effectiveness studies allow clinicians to consider multiple treatment alternatives for a particular problem. Still, solutions are needed to enable efficient and effective consumption of comparative effectiveness research at the point of care. Objective: Design and assess an algorithm for automatically identifying comparative effectiveness studies and extracting the interventions investigated in these studies. Methods: The algorithm combines semantic natural language processing, Medline citation metadata, and machine learning techniques. We assessed the algorithm in a case study of treatment alternatives for depression. Results: Both precision and recall for identifying comparative studies was 0.83. A total of 86% of the interventions extracted perfectly or partially matched the gold standard. Conclusion: Overall, the algorithm achieved reasonable performance. The method provides building blocks for the automatic summarization of comparative effectiveness research to inform point of care decision-making. \u00a9 2013 IMIA and IOS Press.","keywords_author":["Comparative effectiveness research","computer assisted decision making","information needs","machine learning"],"keywords_other":["MEDLINE","Algorithms","Humans","Pattern Recognition, Automated","Natural Language Processing","Periodicals as Topic","Artificial Intelligence","Information Storage and Retrieval","Outcome Assessment (Health Care)","Depression","Bibliometrics","Decision Support Systems, Clinical"],"max_cite":1.0,"pub_year":2013.0,"sources":"['scp']","rawkeys":["medline","artificial intelligence","outcome assessment (health care)","periodicals as topic","automated","information needs","computer assisted decision making","machine learning","natural language processing","humans","comparative effectiveness research","clinical","depression","algorithms","pattern recognition","information storage and retrieval","decision support systems","bibliometrics"],"tags":["medline","health care","periodicals as topic","automated","computer assisted decision making","machine learning","natural language processing","humans","comparative effectiveness researches","clinical","depression","algorithms","pattern recognition","information storage and retrieval","information need","decision support systems","bibliometrics"]},{"p_id":47741,"title":"Prescription extraction using CRFs and word embeddings","abstract":"\u00a9 2017 In medical practices, doctors detail patients\u2019 care plan via discharge summaries written in the form of unstructured free texts, which among the others contain medication names and prescription information. Extracting prescriptions from discharge summaries is challenging due to the way these documents are written. Handwritten rules and medical gazetteers have proven to be useful for this purpose but come with limitations on performance, scalability, and generalizability. We instead present a machine learning approach to extract and organize medication names and prescription information into individual entries. Our approach utilizes word embeddings and tackles the task in two extraction steps, both of which are treated as sequence labeling problems. When evaluated on the 2009 i2b2 Challenge official benchmark set, the proposed approach achieves a horizontal phrase-level F1-measure of 0.864, which to the best of our knowledge represents an improvement over the current state-of-the-art.","keywords_author":["CRFs","Machine learning","NLP","Prescription extraction","Word embeddings"],"keywords_other":["Discharge summary","Humans","Machine learning approaches","Medical practice","Natural Language Processing","CRFs","State of the art","Machine Learning","Patient Discharge Summaries","Sequence Labeling","Information Storage and Retrieval","Prescriptions","Embeddings","Free texts"],"max_cite":0.0,"pub_year":2017.0,"sources":"['scp']","rawkeys":["embeddings","nlp","medical practice","prescriptions","prescription extraction","state of the art","machine learning","natural language processing","humans","word embeddings","sequence labeling","discharge summary","free texts","crfs","information storage and retrieval","patient discharge summaries","machine learning approaches"],"tags":["embeddings","medical practice","conditional random field","prescription extraction","state of the art","machine learning","natural language processing","humans","sequence labeling","discharge summary","word embedding","free texts","prescription","information storage and retrieval","patient discharge summaries","machine learning approaches"]},{"p_id":27265,"title":"A study on cross-language text summarization using supervised methods","abstract":"In this work, we use Hidden Markov Models (HMM), Conditional Random Field (CRF), Gaussian Mixture Models (GMM) and Mathematical Methods of Statistics (MMS) for Chinese and Japanese text summarization. The purpose of this work is to study the applicability of mentioned three trainable models for cross-language text summarization. For model training, we use several training features such as sentence position, sentence centrality, number of Name Entity and so on. For model testing, Chinese on-line news and Japanese news are used as test data which are extracted from web pages. We evaluate each model by measuring the precision at the compression rate 10%, 20% and 30%. MMS is a baseline method. The results show that HMM, CRF and GMM have remarkable increases than MMS on both Chinese and Japanese text summarization by using the same training features. Especially, GMM model make a best performance in all tests.","keywords_author":["Machine learning","NLP","Text summarization"],"keywords_other":["Japanese text","Model training","Test data","Machine-learning","Text summarization","Training features","Mathematical method","Model testing","Compression rates","Web page","Conditional random field","Gaussian Mixture Model","Baseline methods"],"max_cite":6.0,"pub_year":2009.0,"sources":"['scp']","rawkeys":["model training","nlp","test data","conditional random field","text summarization","machine learning","japanese text","mathematical method","web page","machine-learning","model testing","compression rates","training features","baseline methods","gaussian mixture model"],"tags":["model training","test data","conditional random field","text summarization","machine learning","natural language processing","japanese text","mathematical method","web page","model testing","compression rates","training features","baseline methods","gaussian mixture model"]},{"p_id":35460,"title":"IXIR: A statistical information distillation system","abstract":"The task of information distillation is to extract snippets from massive multilingual audio and textual document sources that are relevant for a given templated query. We present an approach that focuses on the sentence extraction phase of the distillation process. It selects document sentences with respect to their relevance to a query via statistical classification with support vector machines. The distinguishing contribution of the approach is a novel method to generate classification features. The features are extracted from charts, compilations of elements from various annotation layers, such as word transcriptions, syntactic and semantic parses, and information extraction (IE) annotations. We describe a procedure for creating charts from documents and queries, while paying special attention to query slots (free-text descriptions of names, organizations, topic, events and so on, around which templates are centered), and suggest various types of classification features that can be extracted from these charts. While observing a 30% relative improvement due to non-lexical annotation layers, we perform a detailed analysis of the contributions of each of these layers to classification performance. \u00a9 2009 Elsevier Ltd.","keywords_author":["Information distillation","Information extraction","Machine learning","Natural language processing","Question answering"],"keywords_other":["Question answering","Information distillation","Machine learning","Information extraction","Natural language processing"],"max_cite":2.0,"pub_year":2009.0,"sources":"['scp']","rawkeys":["question answering","information distillation","information extraction","natural language processing","machine learning"],"tags":["information distillation","information extraction","natural language processing","machine learning","information retrieval"]},{"p_id":51844,"title":"Machine learning-based detection of chemical risk","abstract":"\u00a9 2014 European Federation for Medical Informatics and IOS Press. Chemical risk appears with chemical substances that are dangerous for human or animal health, or for environment, such as with Bisphenol A and phtalates. Chemical risk causes several severe health disorders and is particularly dangerous for human health. Specific agencies are involved in the verification of the suitability of products and goods to be marketed. For this, a large amount of scientific and institutional literature is manually analyzed to study the current knowledge on the associated chemical risk. We propose to use machine learning and dedicated classification for the automatic detection of chemical risk statements. We test several algorithms and features and obtain between 0.60 and 0.95 F-measure.","keywords_author":["Chemical Risk","Evaluation","Machine Learning","Natural Language Processing","Public Health","Uncertainty"],"keywords_other":["Endocrine Disruptors","Risk Assessment","Phenols","Natural Language Processing","Benzhydryl Compounds","Periodicals as Topic","Artificial Intelligence"],"max_cite":0.0,"pub_year":2014.0,"sources":"['scp']","rawkeys":["artificial intelligence","benzhydryl compounds","periodicals as topic","phenols","chemical risk","uncertainty","machine learning","natural language processing","evaluation","risk assessment","endocrine disruptors","public health"],"tags":["benzhydryl compounds","endocrine disruptor","periodicals as topic","phenols","chemical risk","uncertainty","machine learning","natural language processing","evaluation","risk assessment","public health"]},{"p_id":51845,"title":"An opinion mining approach for Romanian language","abstract":"\u00a9 2014 IEEE. The paper proposes a solution for document and aspect levels sentiment analysis for unstructured documents written in the Romanian language. The opinion extraction relies on two different approaches for polarity identification. At the aspect level we propose a rule-based approach. For the document level we consider supervised learning techniques, based on features extracted and filtered in different layers, based on their polarity discriminative power.","keywords_author":["implementation","machine learning","NLP","opinion mining","Romanian"],"keywords_other":["Unstructured documents","Romanians","NLP","Rule-based approach","Opinion extraction","implementation","Discriminative power","Opinion mining"],"max_cite":0.0,"pub_year":2014.0,"sources":"['scp']","rawkeys":["romanians","nlp","romanian","discriminative power","machine learning","opinion extraction","implementation","rule-based approach","opinion mining","unstructured documents"],"tags":["romanians","discriminative power","natural language processing","machine learning","opinion extraction","implementation","rule-based approach","opinion mining","unstructured documents"]},{"p_id":8840,"title":"Empirical distributional semantics: Methods and biomedical applications","abstract":"Over the past 15 years, a range of methods have been developed that are able to learn human-like estimates of the semantic relatedness between terms from the way in which these terms are distributed in a corpus of unannotated natural language text. These methods have also been evaluated in a number of applications in the cognitive science, computational linguistics and the information retrieval literatures. In this paper, we review the available methodologies for derivation of semantic relatedness from free text, as well as their evaluation in a variety of biomedical and other applications. Recent methodological developments, and their applicability to several existing applications are also discussed. \u00a9 2009 Elsevier Inc. All rights reserved.","keywords_author":["Context vectors","Distributional semantics","Latent semantic analysis","Methodological review","Natural language processing","Random indexing","Semantic similarity"],"keywords_other":["Context vectors","Latent semantic analysis","Semantic similarity","Methodological review","Distributional semantics","Random indexing","Natural language processing"],"max_cite":88.0,"pub_year":2009.0,"sources":"['scp', 'wos']","rawkeys":["distributional semantics","natural language processing","random indexing","semantic similarity","methodological review","context vectors","latent semantic analysis"],"tags":["distributional semantics","natural language processing","random indexing","semantic similarity","methodological review","context vectors","latent semantic analysis"]},{"p_id":33418,"title":"Recurrent neural networks for classifying relations in clinical notes","abstract":"\u00a9 2017 Elsevier Inc. We proposed the first models based on recurrent neural networks (more specifically Long Short-Term Memory - LSTM) for classifying relations from clinical notes. We tested our models on the i2b2\/VA relation classification challenge dataset. We showed that our segment LSTM model, with only word embedding feature and no manual feature engineering, achieved a micro-averaged f-measure of 0.661 for classifying medical problem-treatment relations, 0.800 for medical problem-test relations, and 0.683 for medical problem-medical problem relations. These results are comparable to those of the state-of-the-art systems on the i2b2\/VA relation classification challenge. We compared the segment LSTM model with the sentence LSTM model, and demonstrated the benefits of exploring the difference between concept text and context text, and between different contextual parts in the sentence. We also evaluated the impact of word embedding on the performance of LSTM models and showed that medical domain word embedding help improve the relation classification. These results support the use of LSTM models for classifying relations between medical concepts, as they show comparable performance to previously published systems while requiring no manual feature engineering.","keywords_author":["Long Short-Term Memory","Machine learning","Medical relation classification","Natural language processing","Recurrent neural network","Natural language processing","Medical relation classification","Recurrent neural network","Long Short-Term Memory","Machine learning"],"keywords_other":["State-of-the-art system","ADVERSE DRUG-REACTIONS","Electronic Health Records","Neural Networks (Computer)","Clinical notes","F measure","Humans","TEXT","EXTRACTION","Natural Language Processing","Narration","Feature engineerings","Test relations","Data Mining","Language","Medical domains","Relation classifications","Medical concepts"],"max_cite":2.0,"pub_year":2017.0,"sources":"['ieee', 'scp', 'wos']","rawkeys":["text","feature engineerings","language","extraction","machine learning","electronic health records","relation classifications","clinical notes","narration","data mining","neural networks (computer)","test relations","long short-term memory","recurrent neural network","humans","state-of-the-art system","f measure","medical concepts","natural language processing","medical relation classification","medical domains","adverse drug-reactions"],"tags":["text","feature engineerings","adverse drug reactions","language","extraction","machine learning","electronic health records","f-measure","relation classifications","clinical notes","narration","data mining","test relations","neural networks","long short-term memory","humans","state-of-the-art system","medical concepts","natural language processing","medical relation classification","medical domains"]},{"p_id":49803,"title":"Text2arff: A text representation library Text2arff: Metin Temsil K\u00fct\u00fcphanesi","abstract":"\u00a9 2016 IEEE.Which features are the most important for the text classification tasks? In the automatic text categorization area, several studies seek answers to this question. In this paper, new version of Text2arff (a library for text representation) and its new features (word2vec, Word trajectories, etc.) are presented. Also, the software is now a Java library which can be used in the user's own projects. In the experiments, the library is run on two sample datasets. The results show that the effect of text representation method is bigger than the classification method. This result also emphasizes the importance of developing new test representation methods.","keywords_author":["machine learning","natural language processing","text classification"],"keywords_other":["Automatic text categorization","Java library","Classification methods","Text classification","Representation method","Text representation","NAtural language processing"],"max_cite":0.0,"pub_year":2016.0,"sources":"['scp']","rawkeys":["java library","classification methods","automatic text categorization","text representation","natural language processing","machine learning","text classification","representation method"],"tags":["java library","classification methods","automatic text categorization","text representation","natural language processing","machine learning","text classification","representation method"]},{"p_id":49804,"title":"Text classification with coupled matrix factorization Baglasimli Matris Ayrisimi ile Metin Siniflandirmasi","abstract":"\u00a9 2016 IEEE.We introduce a coupled matrix factorization and logistic regression model for simultaneous feature extraction and binary classification in high dimensional data sets. We write the model starting from probabilistic notation of nonnegative matrix factorization and logistic regression. We derive the gradient descent multiplicative update rules from the log likelihood function. We show that the model yields better results than logistic regression on a recent Turkish text corpus of news articles, while maintaining comparable accuracy in the topic clustering problem.","keywords_author":["Machine learning","matrix factorization","natural language processing"],"keywords_other":["Binary classification","Logistic Regression modeling","Multiplicative updates","Log-likelihood functions","High dimensional data","Nonnegative matrix factorization","Matrix factorizations","NAtural language processing"],"max_cite":0.0,"pub_year":2016.0,"sources":"['scp']","rawkeys":["high dimensional data","multiplicative updates","nonnegative matrix factorization","machine learning","matrix factorization","binary classification","natural language processing","matrix factorizations","log-likelihood functions","logistic regression modeling"],"tags":["high dimensional data","multiplicative updates","nonnegative matrix factorization","machine learning","natural language processing","binary classification","log-likelihood functions","logistic regression modeling"]},{"p_id":27277,"title":"Social network - An autonomous system designed for radio recommendation","abstract":"This paper describes the functions of a system proposed for the music tube recommendation from social network data base. Such a system enables the automatic collection, evaluation and rating of music critics, the possibility to rate music tube by auditors and the recommendation of tubes depended from auditor's profiles in form of regional internet radio. First, the system searches and retrieves probable music reviews from the Internet. Subsequently, the system carries out an evaluation and rating of those reviews. From this list of music tubes the system directly allows notation from our application. Finally the system automatically create the record list diffused each day depended form the region, the year season, day hours and age of listeners. Our system uses linguistics and statistic methods for classifying music opinions and data mining techniques for recommendation part needed for recorded list creation. The principal task is the creation of popular intelligent radio adaptive on auditor's age and region - IA-Regional-Radio. \u00a9 2009 IEEE.","keywords_author":["Machine learning","Natural language processing","Opinion mining","Social network"],"keywords_other":["Internet radio","Statistic method","Data mining techniques","Autonomous systems","Machine learning","System use","Social network","Social Networks","Natural language processing","Opinion mining"],"max_cite":6.0,"pub_year":2009.0,"sources":"['scp']","rawkeys":["autonomous systems","internet radio","natural language processing","machine learning","social networks","social network","data mining techniques","statistic method","opinion mining","system use"],"tags":["autonomous systems","internet radio","natural language processing","machine learning","social networks","data mining techniques","opinion mining","statistical methods","system use"]},{"p_id":35469,"title":"Identifying firm-specific risk statements in news articles","abstract":"Textual data are an important information source for risk management for business organizations. To effectively identify, extract, and analyze risk-related statements in textual data, these processes need to be automated. We developed an annotation framework for firm-specific risk statements guided by previous economic, managerial, linguistic, and natural language processing research. A manual annotation study using news articles from the Wall Street Journal was conducted to verify the framework. We designed and constructed an automated risk identification system based on the annotation framework. The evaluation using manually annotated risk statements in news articles showed promising results for automated risk identification.","keywords_author":["Epistemic modality","Evidentiality","Machine learning","Risk management"],"keywords_other":["Business organizations","Information sources","Textual data","Epistemic modality","Wall Street Journal","News articles","Evidentiality","Machine learning","Manual annotation","Risk Identification","NAtural language processing"],"max_cite":2.0,"pub_year":2009.0,"sources":"['scp']","rawkeys":["manual annotation","epistemic modality","textual data","evidentiality","machine learning","natural language processing","information sources","news articles","business organizations","risk identification","risk management","wall street journal"],"tags":["manual annotation","epistemic modality","textual data","evidentiality","machine learning","natural language processing","information sources","news articles","business organizations","risk identification","risk management","wall street journal"]},{"p_id":35470,"title":"Syntactic and semantic english-korean machine translation using ontology","abstract":"This paper presents the syntactic and semantic method for English-Korean Machine Translation (M1) using ontology for Web-based MT system. We first build word class ontology from the English corpus and calculate the weight of relation between words in the same or different ontologies by counting the frequency of co-occurrence. With our constructed ontologies, we introduce the MT system model including the syntactic and semantic translation module. Each module translates the source language in different way. The syntactic translation module transforms the structure of English into Korean structure. The semantic translation module extracts an exact meaning of a word using ontologies. Through the both translation modules the source language is naturally translated into a target language.","keywords_author":["Machine learning","Machine translation","Natural language process","Ontology"],"keywords_other":["Machine translation","Target language","Natural language process","Source language","Machine learning","Semantic translations","Machine translations","System models","Co-occurrence"],"max_cite":2.0,"pub_year":2009.0,"sources":"['scp']","rawkeys":["natural language process","co-occurrence","ontology","machine learning","source language","machine translation","system models","target language","machine translations","semantic translations"],"tags":["co-occurrence","target language","natural language processing","machine learning","source language","machine translations","system modeling","semantic translations"]},{"p_id":37520,"title":"Augmenting media literacy with automatic characterization of news along pragmatic dimensions","abstract":"Media literacy allows individuals to better interpret the information they need to absorb to contribute to our democratic, knowledge-based society. I propose that by automatically notifying readers of an article's problematic pragmatic characteristics, an application could augment their media literacy. I describe two characteristics for which I have been building automatic classifiers - factiness, and tropes as narrative frames - and discuss the status of each project.","keywords_author":["Computational journalism","Information retrieval","Machine learning","Natural language processing"],"keywords_other":["Automatic classifiers","Computational journalism","Knowledge-based society","NAtural language processing"],"max_cite":1.0,"pub_year":2017.0,"sources":"['scp']","rawkeys":["knowledge-based society","machine learning","natural language processing","information retrieval","automatic classifiers","computational journalism"],"tags":["knowledge-based society","machine learning","natural language processing","information retrieval","automatic classifiers","computational journalism"]},{"p_id":49807,"title":"Turkish TV rating prediction with Twitter Twitter ile T\u00fcrk Televizyonlari Rating Tahmini","abstract":"\u00a9 2016 IEEE. Social Web Mining is a field that has attracted a lot of attention over the past few years. The dynamic human data, which grows in an incredible speed, has been used to predict a variety of things lie flu trends and political election results. In this project a system has been developed to predict the ratings of Turkish TV programs, using Twitter, a rich source of social data. This system has abilities like gathering and cleaning data, natural language processing, machine learning and prediction.","keywords_author":["machine learning","natural language processing","rating prediction","Twitter"],"keywords_other":["Social web minings","Twitter","Turkishs","Social datum","TV programs","Human data","NAtural language processing"],"max_cite":0.0,"pub_year":2016.0,"sources":"['scp']","rawkeys":["turkishs","natural language processing","machine learning","rating prediction","tv programs","twitter","social web minings","social datum","human data"],"tags":["turkishs","natural language processing","machine learning","rating prediction","tv programs","twitter","social web minings","social datum","human data"]},{"p_id":47761,"title":"Semi-Supervised approach to monitoring clinical depressive symptoms in social media","abstract":"\u00a9 2017 Association for Computing Machinery. With the rise of social media, millions of people are routinely expressing their moods, feelings, and daily struggles with mental health issues on social media platforms like Twitter. Unlike traditional observational cohort studies conducted through questionnaires and self-reported surveys, we explore the reliable detection of clinical depression from tweets obtained unobtrusively. Based on the analysis of tweets crawled from users with self-reported depressive symptoms in their Twitter profiles, we demonstrate the potential for detecting clinical depression symptoms which emulate the PHQ-9 questionnaire clinicians use today. Our study uses a semi-supervised statistical model to evaluate how the duration of these symptoms and their expression on Twitter (in terms of word usage patterns and topical preferences) align with the medical findings reported via the PHQ-9. Our proactive and automatic screening tool is able to identify clinical depressive symptoms with an accuracy of 68% and precision of 72%.","keywords_author":["Mental health","Natural language processing","Semi-supervised machine learning","Social media"],"keywords_other":["Automatic screening","Statistical modeling","Semi-supervised","Reliable detection","Mental health","Clinical depression","Social media","Social media platforms"],"max_cite":0.0,"pub_year":2017.0,"sources":"['scp']","rawkeys":["semi-supervised machine learning","automatic screening","statistical modeling","social media","semi-supervised","reliable detection","natural language processing","social media platforms","mental health","clinical depression"],"tags":["semi-supervised machine learning","automatic screening","social media","semi-supervised","reliable detection","natural language processing","social media platforms","mental health","clinical depression","statistical models"]},{"p_id":650,"title":"Deep Learning for Classification and as Tapped-Feature Generator in Medieval Word-Image Recognition","abstract":"Historical manuscripts are the main source of information about past. In recent years, digitization of large quantities of historical handwritten documents is in vogue. This trend gives access to a plethora of information about our medieval past. Such digital archives can be more useful if automatic indexing and retrieval of document images can be provided to the end users of a digital library. An automatic transcription of the full digital archive using traditional Optical Character Recognition (OCR) is still not possible with sufficient accuracy. If full transcription is not available, the end users are interested in indexing and retrieving of particular document pages of their interest. Hence recognition of certain keywords from within the corpus will be sufficient to meet the end users needs. Recently, deep-learning based methods have shown competence in image classification problems. However, one bottleneck with deep-learning based techniques is that it requires a huge amount of training samples per class. Since the number of samples per word class is scarce for collections that are freshly scanned, this is a serious hindrance for direct usage of the deep-learning technique for the purpose of word image recognition in historical document images. This paper aims to investigate the problem of recognizing words from historical document images using a deep-learning based framework for feature extraction and classification while countering the problem of the low amount of image samples using off-line data augmentation techniques. Encouraging results (highest accuracy of 90.03%) were obtained while dealing with 365 different word classes.","keywords_author":null,"keywords_other":["Image recognition","digital archive","historical manuscripts","automatic indexing","historical handwritten documents","Optical Character Recognition","deep-learning based methods","handwritten character recognition","historical document images","image classification","Feature extraction","Machine learning","optical character recognition","tapped-feature generator","digital library","learning (artificial intelligence)","Training","Hidden Markov models","image classification problems","medieval word-image Recognition","document image processing","Task analysis","automatic transcription","natural language processing","Image segmentation","feature extraction","digital libraries"],"max_cite":null,"pub_year":2018.0,"sources":"['ieee']","rawkeys":["task analysis","digital archive","automatic indexing","historical manuscripts","historical handwritten documents","deep-learning based methods","handwritten character recognition","historical document images","image classification","machine learning","optical character recognition","tapped-feature generator","medieval word-image recognition","digital library","learning (artificial intelligence)","training","image classification problems","document image processing","image segmentation","hidden markov models","automatic transcription","natural language processing","feature extraction","image recognition","digital libraries"],"tags":["task analysis","automatic indexing","historical manuscripts","historical handwritten documents","deep-learning based methods","handwritten character recognition","historical document images","image classification","machine learning","optical character recognition","digital archives","tapped-feature generator","medieval word-image recognition","training","image classification problems","document image processing","image segmentation","hidden markov models","automatic transcription","natural language processing","feature extraction","image recognition","digital libraries"]},{"p_id":654,"title":"MedKiosk: An embodied conversational intelligence via deep learning","abstract":"Today, rapid advancement and innovation of technology has changed the perspective and the way information precedes. The utilization of self-service interactive kiosks through the concept of touch screens have been becoming a massive trend used intensively, especially in e-commerce and medical healthcare. Thus, this research will investigate and innovate the current interactive kiosk to provide immediate responses and reliable information incorporating an intelligent conversational agent (CA). A CA also widely known as chatbots is a computer program to simulate the conversations between human and machine. Our goal is to design and develop a framework for revolutionizing medical kiosk via the incorporation of an intelligent chatbots. This research intends to escalate the productivity and efficiency in medical institutions by offering the capabilities to provide immediate reply as well as initiating a conversation, similar to conversing with the experienced customer service assistant. Latest innovations in natural language processing (NLP), machine learning and deep learning in the field of artificial intelligence (AI) ensure smarter decision making in providing accurate, reliable and up-to-date information. The future MedKiosks is reliant on the AI, specifically the machine learning and deep learning to unveil the correlations, algorithms, patterns and anomalies to improve chatbots knowledge base. Moreover the experiment would be hosted by positioning the MedKiosk in the hospital for real-time data collections.","keywords_author":["medical kiosk","artificial intelligence","intelligent conversational agent (CA)","medical"],"keywords_other":["health care","immediate responses","massive trend","medical institutions","interactive systems","up-to-date information","decision making","experienced customer service assistant","e-commerce","Internet","touch screens","machine learning","Customer services","Machine learning","computer program","Hospitals","latest innovations","human machine","medical kiosk","future MedKiosks","learning (artificial intelligence)","intelligent conversational agent","deep learning","reliable information","immediate reply","medical healthcare","intelligent chatbots","customer services","medical information systems","self-service interactive kiosks","artificial intelligence","rapid advancement","embodied conversational intelligence","chatbots knowledge base","natural language processing","Speech recognition","Natural language processing","current interactive kiosk"],"max_cite":null,"pub_year":2017.0,"sources":"['ieee']","rawkeys":["health care","immediate responses","massive trend","medical institutions","speech recognition","interactive systems","internet","up-to-date information","decision making","experienced customer service assistant","e-commerce","touch screens","machine learning","computer program","hospitals","medical","latest innovations","human machine","medical kiosk","learning (artificial intelligence)","intelligent conversational agent","deep learning","reliable information","immediate reply","future medkiosks","intelligent chatbots","medical healthcare","customer services","medical information systems","self-service interactive kiosks","artificial intelligence","intelligent conversational agent (ca)","rapid advancement","embodied conversational intelligence","chatbots knowledge base","natural language processing","current interactive kiosk"],"tags":["health care","human-machine","immediate responses","massive trend","medical institutions","speech recognition","internet","up-to-date information","decision making","experienced customer service assistant","e-commerce","machine learning","computer program","hospitals","medical","touch screen","latest innovations","medical kiosk","reliable information","immediate reply","future medkiosks","intelligent chatbots","medical healthcare","customer services","medical information systems","self-service interactive kiosks","rapid advancement","embodied conversational intelligence","chatbots knowledge base","natural language processing","cellular automata","interactive system","current interactive kiosk"]},{"p_id":47765,"title":"Mining frequency of drug side effects over a large twitter dataset using apache spark","abstract":"\u00a9 2017 Association for Computing Machinery. Despite clinical trials by pharmaceutical companies as well as current FDA reporting systems, there are still drug side effects that have not been caught. To find a larger sample of reports, a possible way is to mine online social media. With its current widespread use, social media such as Twitter has given rise to massive amounts of data, which can be used as reports for drug side effects. To process these large datasets, Apache Spark has become popular for fast, distributed batch processing. In this work, we have improved on previous pipelines in sentimental analysis-based mining, processing, and extracting tweets with drug-caused side effects. We have also added a new ensemble classifier using a combination of sentiment analysis features to increase the accuracy of identifying drug-caused side effects. In addition, the frequency count for the side effects is also provided. Furthermore, we have also implemented the same pipeline in Apache Spark to improve the speed of processing of tweets by 2.5 times, as well as to support the process of large tweet datasets. As the frequency count of drug side effects opens a wide door for further analysis, we present a preliminary study on this issue, including the side effects of simultaneously using two drugs, and the potential danger of using less-common combination of drugs. We believe the pipeline design and the results present in this work would have great implication on studying drug side effects and on big data analysis in general.","keywords_author":["Adverse drug event","Apache spark","classification","Machine learning","Natural language processing","Opinion mining","Sentiment analysis","Supervised learning","Twitter"],"keywords_other":["Adverse drug event","Ensemble classifiers","Twitter","Online social medias","Opinion mining","Sentiment analysis","Reporting systems","Pharmaceutical company"],"max_cite":0.0,"pub_year":2017.0,"sources":"['scp']","rawkeys":["supervised learning","apache spark","pharmaceutical company","machine learning","natural language processing","online social medias","reporting systems","classification","adverse drug event","ensemble classifiers","opinion mining","sentiment analysis","twitter"],"tags":["supervised learning","apache spark","pharmaceutical company","machine learning","natural language processing","online social medias","reporting systems","classification","ensemble classifiers","adverse drug events","opinion mining","sentiment analysis","twitter"]},{"p_id":49814,"title":"Deep neural network language model research and application overview","abstract":"\u00a9 2015 IEEE. Research of the neural network language model in NLP is reviewed. In this paper, the neural network language models are classified into early shallow language models and deep neural network models based on deep learning. This paper emphatically introduces progress of the deep neural network language models, and summarizes the status of deep neural network research's development. Finally, the existing problems and deficiencies are put forward.","keywords_author":["Deep Learning","Deep Neural Network Language Model","Natural Language Processing","Sentiment Analysis","Word Embedding"],"keywords_other":["Deep learning","Sentiment analysis","Word Embedding","Deep neural networks","NAtural language processing"],"max_cite":0.0,"pub_year":2016.0,"sources":"['scp']","rawkeys":["deep learning","deep neural networks","natural language processing","word embedding","deep neural network language model","sentiment analysis"],"tags":["natural language processing","machine learning","word embedding","convolutional neural network","deep neural network language model","sentiment analysis"]},{"p_id":25242,"title":"RDRCE: Combining machine learning and knowledge acquisition","abstract":"We present a new interactive workbench RDRCE (RDR Case Explorer) to facilitate the combination of Machine Learning and manual Knowledge Acquisition for Natural Language Processing problems. We show how to use Brill's well regarded transformational learning approach and convert its results into an RDR tree. RDRCE then strongly guides the systematic inspection of the generated RDR tree in order to further refine and improve it by manually adding more rules. Furthermore, RDRCE also helps in quickly recognising potential noise in the training data and allows to deal with noise effectively. Finally, we present a first study using RDRCE to build a high-quality Part-of-Speech tagger for English. After some 60 hours of manual knowledge acquisition, we already exceed slightly the state-of-the art performance on unseen benchmark test data and the fruits of some 15 years of further research in learning methods for Part-of-Speech taggers. \u00a9 2010 Springer-Verlag Berlin Heidelberg.","keywords_author":["Knowledge Acquisition","Machine Learning","Part-of-Speech tagger","Ripple Down Rules","TBL"],"keywords_other":["High quality","TBL","Ripple Down Rules","Interactive workbench","Training data","Learning methods","Machine-learning","State of the art","Benchmark tests","Learning approach","Part-of-speech tagger","NAtural language processing"],"max_cite":8.0,"pub_year":2010.0,"sources":"['scp']","rawkeys":["knowledge acquisition","learning approach","tbl","ripple down rules","learning methods","state of the art","machine learning","natural language processing","interactive workbench","part-of-speech tagger","high quality","machine-learning","benchmark tests","training data"],"tags":["training data","knowledge acquisition","tbl","ripple down rules","learning methods","state of the art","machine learning","natural language processing","interactive workbench","part-of-speech tagger","high quality","learning approach","benchmark testing"]},{"p_id":21149,"title":"Naive Bayes classifier based arabic document categorization","abstract":"Text Categorization aims to assign an electronic document to one or more categories based on its contents. Due to the rapid growth of the number of online Arabic documents, the information libraries and Arabic document corpus, automatic Arabic document classification becomes an important task. This paper suggests the use of rooting algorithm with Na\u00efve Bayes Classifier to the problem of document categorization of Arabic language and reports the algorithm performance in terms of error rate, accuracy, and micro-average recall measures. Our experimental study shows that using rooting algorithm with Na\u00efve Bayes (NB) Classifier gives \u223c62.23% average accuracy and decreases the dimensionality of the training documents.","keywords_author":["Document categorization","Machine learning","Na\u00efve Bayes classifier","Natural language processing for Arabic language"],"keywords_other":["Machine-learning","Document categorization","Bayes Classifier","Arabic languages","NAtural language processing"],"max_cite":15.0,"pub_year":2010.0,"sources":"['scp']","rawkeys":["bayes classifier","machine learning","natural language processing","na\u00efve bayes classifier","machine-learning","arabic languages","natural language processing for arabic language","document categorization"],"tags":["bayes classifier","machine learning","natural language processing","na\u00efve bayes classifier","arabic languages","natural language processing for arabic language","document categorization"]},{"p_id":37534,"title":"Text mining for security threat detection discovering hidden information in unstructured log messages","abstract":"\u00a9 2016 IEEE.The exponential growth of unstructured messages generated by the computer systems and applications in modern computing environment poses a significant challenge in managing and using the information contained in the messages. Although these data contain a wealth of information that is useful for advanced threat detection, the sheer volume, variety, and complexity of data make it difficult to analyze them even by well-trained security analysts. While conventional Security Information and Event Management (SIEM) systems provide some capability to collect, correlate, and detect certain events from structured messages, their rule-based correlation and detection algorithms fall short in utilizing the information within the unstructured messages. Our study explores the possibility of utilizing the techniques for data mining, text classification, natural language processing, and machine learning to detect security threats by extracting relevant information from various unstructured log messages collected from distributed non-homogeneous systems. The extracted features are used to run a number of experiments on the Packet Clearing House SKAION 2006 IARPA Dataset, and their prediction capability is evaluated. In comparison with the base case without feature extraction, an average of 16.73% performance gain and 84% time reduction was achieved using extracted features only, and a 23.48% performance gain was attained using both unstructured free-text messages and extracted features. The results also show a strong potential for further increase in performance by increasing size of training datasets and extracting more features from the unstructured log messages.","keywords_author":["data mining","machine learning","named entity extraction","natural language processing","Security Information and Event Management","SIEM","text classification","unstructured log messages"],"keywords_other":["Named entity extraction","SIEM","unstructured log messages","Text classification","Security information and event managements","NAtural language processing"],"max_cite":1.0,"pub_year":2017.0,"sources":"['scp']","rawkeys":["data mining","unstructured log messages","security information and event management","named entity extraction","siem","natural language processing","machine learning","security information and event managements","text classification"],"tags":["data mining","unstructured log messages","named entity extraction","natural language processing","machine learning","security information and event managements","text classification"]},{"p_id":49822,"title":"Feature oriented sentiment analysis in social networking sites to track malicious campaigners","abstract":"\u00a9 2015 IEEE. Social networking websites are considered as major sources of opinions and views of the public on the prevalent social issues at a given point in time. Websites like the Twitter1 reflect the public views through its millions of messages posted by its users world wide, whenever a controversial issue arises in the society. It is during this time that we observe significant amount of malicious, violent contents going viral over the internet. In this paper we propose a technique that applies sentiment analysis on data from Twitter and measures the sentiments of posts in order to identify the origin of malicious contents. This is achieved by taking into account the influence of the posts on the public as well. The prominent feature of our work is the technique that is used for feature-oriented sentiment analysis. This involves an algorithm that parses a given tweet and builds a Dependency tree of each sentence in order to effectively identify the sentiment of the tweet. The working and the scope of our techniques are illustrated with a case study and associated results.","keywords_author":["Dependency grammar","Lexical database","Machine learning","NLP","Sentiment analysis","Twitter API"],"keywords_other":["Prominent features","Sentiment analysis","Social networking sites","Dependency trees","Feature-oriented","Twitter API","Lexical database","Dependency grammar"],"max_cite":0.0,"pub_year":2016.0,"sources":"['scp']","rawkeys":["lexical database","prominent features","nlp","twitter api","feature-oriented","machine learning","dependency trees","social networking sites","dependency grammar","sentiment analysis"],"tags":["lexical database","feature-orientation","prominent features","twitter api","machine learning","natural language processing","dependency trees","social networking sites","dependency grammar","sentiment analysis"]},{"p_id":51869,"title":"Natural language processing and semantic technologies. The application on Brand Rain and Anpro21 Aplicaci\u00f3n de tecnolog\u00edas de Procesamiento de lenguaje natural y tecnolog\u00eda sem\u00e1ntica en Brand Rain y Anpro21","abstract":"This paper presents the application and results on research about natural language processing and semantic technologies in Brand Rain and Anpro21. The related projects are explained and the obtained benefits from the research on this new technologies developed are presented. All this research have been applied on the monitoring and reputation system of Brand Rain. \u00a9 2014 Sociedad Espa\u00f1ola para el Procesamiento del Lenguaje Natural.","keywords_author":["Data mining","Machine learning","Natural language processing","Ontologies","Reputation analysis","Semantic web","Sentiment analysis","Text mining"],"keywords_other":null,"max_cite":0.0,"pub_year":2014.0,"sources":"['scp']","rawkeys":["data mining","text mining","natural language processing","machine learning","ontologies","sentiment analysis","semantic web","reputation analysis"],"tags":["data mining","text mining","natural language processing","machine learning","sentiment analysis","semantic web","reputation analysis"]},{"p_id":47773,"title":"Stance classification of twitter debates: The encryption debate as a use case","abstract":"\u00a9 2017 Copyright is held by the owner\/author(s). Publication rights licensed to ACM. Social media have enabled a revolution in user-generated content. They allow users to connect, build community, produce and share content, and publish opinions. To better understand online users' attitudes and opinions, we use stance classification. Stance classification is a relatively new and challenging approach to deepen opinion mining by classifying a user's stance in a debate. Our stance classification use case is tweets that were related to the spring 2016 debate over the FBI's request that Apple decrypt a user's iPhone. In this \"encryption debate,\" public opinion was polarized between advocates for individual privacy and advocates for national security. We propose a machine learning approach to classify stance in the debate, and a topic classification that uses lexical, syntactic, Twitter-specific, and argumentative features as a predictor for classifications. Models trained on these feature sets showed significant increases in accuracy relative to the unigram baseline.","keywords_author":["Argumentative Features.","Natural Language Processing","Stance Classification","Supervised Machine Learning"],"keywords_other":["Individual privacy","User-generated content","Topic Classification","Public opinions","Machine learning approaches","Argumentative Features","Supervised machine learning","Opinion mining"],"max_cite":0.0,"pub_year":2017.0,"sources":"['scp']","rawkeys":["public opinions","natural language processing","argumentative features","user-generated content","individual privacy","topic classification","supervised machine learning","opinion mining","machine learning approaches","stance classification"],"tags":["public opinions","natural language processing","argumentative features","user-generated content","individual privacy","topic classification","supervised machine learning","opinion mining","machine learning approaches","stance classification"]},{"p_id":27298,"title":"Computer-aided TRIZ ideality and level of invention estimation using natural language processing and machine learning","abstract":"\u00a9 IFIP International Federation for Information Processing 2009.Patent textual descriptions provide a wealth of information that can be used to understand the underlying design approaches that result in the generation of novel and innovative technology. This article will discuss a new approach for estimating Degree of Ideality and Level of Invention metrics from the theory of inventive problem solving (TRIZ) using patent textual information. Patent text includes information that can be used to model both the functions performed by a design and the associated costs and problems that affect a design\u2019s value. The motivation of this research is to use patent data with calculation of TRIZ metrics to help designers understand which combinations of system components and functions result in creative and innovative design solutions. This article will discuss in detail methods to estimate these TRIZ metrics using natural language processing and machine learning with the use of neural networks.","keywords_author":["Innovation and Creativity","Machine Learning","Neural Networks","Patent Analysis","TRIZ Natural Language Processing"],"keywords_other":["Textual description","Patent analysis","Design approaches","Textual information","Theory of inventive problem solving","Innovative technology","Wealth of information","NAtural language processing"],"max_cite":6.0,"pub_year":2009.0,"sources":"['scp']","rawkeys":["design approaches","neural networks","textual description","natural language processing","machine learning","innovation and creativity","wealth of information","innovative technology","triz natural language processing","textual information","theory of inventive problem solving","patent analysis"],"tags":["design approaches","neural networks","textual description","natural language processing","machine learning","innovation and creativity","wealth of information","innovative technology","triz natural language processing","textual information","theory of inventive problem solving","patent analysis"]},{"p_id":37538,"title":"Sentiment analysis of Twitter data: Case study on digital India","abstract":"\u00ef\u00bf\u00bd 2016 IEEE. Nowadays Opinion Mining has become an emerging topic of research due to lot of opinionated data available on Blogs & social networking sites. Tracking different types of opinions & summarizing them can provide valuable insight to different types of opinions to users who use Social networking sites to get reviews about any product, service or any topic. Analysis of opinions & its classification on the basis of polarity (positive, negative, neutral) is a challenging task. Lot of work has been done on sentiment analysis of Twitter data and lot needs to be done. In our work we are trying to perform sentiment analysis of the Twitter data set that expresses opinion about Modi ji's Digital India Campaign. In my work, I have collected these sentiments and classified polarity of sentiments in these opinions w.r.t. Positive, Negative or Neutral. Twitter data is collected for analysis using Twitter API. Out of the two widely used approaches used for sentiment analysis, Machine Learning & Dictionary Based approach, we are using Dictionary Based approach to analyze data posted by different users. Then polarity classification of this data is done. In this paper we discuss sentiment analysis of Twitter data, existing tools available for sentiment analysis, related work, framework used, case study to demonstrate the work followed by the results section. Results clearly demonstrate that the 50% of the collected opinions are positive, 20% are Negative and rests 30% are neutral.","keywords_author":["Dictionary Based approach","Digital India","Governance","Machine Learning","Natural Language Processing","Opinion Mining","Sentiment Analysis"],"keywords_other":["Governance","Sentiment analysis","Digital India","NAtural language processing","Opinion mining"],"max_cite":1.0,"pub_year":2017.0,"sources":"['scp']","rawkeys":["governance","natural language processing","machine learning","dictionary based approach","digital india","opinion mining","sentiment analysis"],"tags":["governance","natural language processing","machine learning","sentiment analysis","digital india","opinion mining","dictionary-based approach"]},{"p_id":49828,"title":"Big Data Analytics for Social Media","abstract":"\u00a9 2016 Elsevier Inc. All rights reserved. A large volume of text data is being generated at a high velocity on a routine basis. Natural language processing (NLP) methods are used to parse text data and extract the most impactful entities. Understanding text is governed by grammatical rules of the language. Traditional methods of text analysis have been centered on syntactic methods, but there has been a systematic shift towards the use of statistical methods for text and language processing in recent years. These NLP techniques are central to building everyday applications such as search, recommendation systems, spell checkers, machine translation, question answering machines, etc. Several applications have been developed on top of text data streams. For example, detecting trending terms, potentially across languages, has proven useful in determining early signs of a flu outbreak. In a similar vein, detecting patterns\/anomalies in chat messages in multiplayer games can potentially guide development of new features and can potentially help players to develop better strategies. Detecting change\/anomalies in sentiment (derived via mining of text data streams) has direct applications in, for example, financial markets. Detection of anomalies in time series of terms, obtained from mining of text data streams, is nontrivial owing to, for example, but not limited to, presence of an underlying trend, seasonality and other data characteristics (which are mostly not accounted for by the existing techniques). Further, there is a tradeoff between accuracy and time to detect.","keywords_author":["Anomaly detection","Language detection","Machine learning on text","NLP","Recommendation systems","Search","Text mining"],"keywords_other":["Text mining","Language processing","Question Answering","Anomaly detection","Search","Data characteristics","Language detection","Machine translations"],"max_cite":0.0,"pub_year":2016.0,"sources":"['scp']","rawkeys":["question answering","text mining","nlp","search","anomaly detection","data characteristics","recommendation systems","language detection","machine translations","machine learning on text","language processing"],"tags":["text mining","search","anomaly detection","data characteristics","natural language processing","information retrieval","recommender systems","language detection","machine translations","machine learning on text","language processing"]},{"p_id":29351,"title":"A text feature-based approach for literature mining of lncRNA\u2013protein interactions","abstract":"\u00a9 2016 Elsevier B.V. Long non-coding RNAs (lncRNAs) play important roles in regulating transcriptional and post-transcriptional levels. Currently, Knowledge of lncRNA and protein interactions (LPIs) is crucial for biomedical researches that are related to lncRNA. Many freshly discovered LPIs are stored in biomedical literature. With over one million new biomedical journal articles published every year, just keeping up with the novel finding requires automatically extracting information by text mining. To address this issue, we apply a text feature-based text mining approach to efficiently extract LPIs from biomedical literatures. Our approach consists of four steps. By employ natural language processing (NLP) technologies, this approach extracts text features from sentences that can precisely reflect the real LPIs. Our approach involves four steps including data collection, text pre-processing, structured representation, features extraction and training model and classification. The F-score performance of our approach achieves 79.5%, and the results indicate that the proposed approach can efficiently extract LPIs from biomedical literature.","keywords_author":["LncRNA\u2013protein interaction","Machine learning","Text features","Text mining"],"keywords_other":["Text mining","Biomedical literature","Text feature","Features extraction","Protein interaction","Post-transcriptional","NAtural language processing","Extracting information"],"max_cite":4.0,"pub_year":2016.0,"sources":"['scp']","rawkeys":["protein interaction","text mining","features extraction","text features","machine learning","natural language processing","post-transcriptional","extracting information","text feature","lncrna\u2013protein interaction","biomedical literature"],"tags":["protein interaction","text mining","machine learning","natural language processing","post-transcriptional","extracting information","text feature","lncrna\u2013protein interaction","feature extraction","biomedical literature"]},{"p_id":21160,"title":"Minimum tag error for discriminative training of conditional random fields","abstract":"This paper proposes a new criterion called minimum tag error (MTE) for discriminative training of conditional random fields (CRFs). The new criterion, which is a smoothed approximation to the sentence labeling error, aims to maximize an average of transcription tagging accuracies of all possible sentences, weighted by their probabilities. Corpora from the second international Chinese word segmentation bakeoff (Bakeoff 2005) are used to test the effectiveness of this new training criterion. The experimental results have demonstrated that the proposed minimum tag error criterion can reliably improve the initial performance of supervised conditional random fields. In particular, the recall rate of out-of-vocabulary words (Roov) is significantly improved compared with that obtained using standard conditional random fields. Furthermore, the new training method has the advantage of robustness to segmentation across all datasets. \u00a9 2008 Elsevier Inc. All rights reserved.","keywords_author":["Chinese word segmentation","Conditional random fields","Discriminative training","Machine learning","Natural language processing"],"keywords_other":null,"max_cite":15.0,"pub_year":2009.0,"sources":"['scp', 'ieee']","rawkeys":["conditional random fields","chinese word segmentation","discriminative training","natural language processing","machine learning"],"tags":["chinese word segmentation","discriminative training","conditional random field","natural language processing","machine learning"]},{"p_id":45737,"title":"Artificial intelligence and predictive justice: Limitations and perspectives","abstract":"\u00a9 2018, Springer International Publishing AG, part of Springer Nature. One of the main barriers to effective prediction systems in the legal domain is the very limited availability of relevant data. This paper discusses the particular case of the Federal Court of Canada, and describes some perspectives on how best to overcome these problems. Part of the process involves an automatic annotation system, supervised by a manual annotation process. Several state-of-the-art methods on related tasks are presented, as well as promising approaches leveraging recent advances in natural language processing, such as vector word representations or recurrent neural networks. The insights outlined in the paper will be further explored in a near future, as this work is still an ongoing research.","keywords_author":["Legal artificial intelligence","Machine learning","Natural language processing","Predictive justice"],"keywords_other":["Federal Court of Canada","Predictive justice","Prediction systems","State-of-the-art methods","Word representations","Legal domains","Manual annotation","Automatic annotation"],"max_cite":0.0,"pub_year":2018.0,"sources":"['scp']","rawkeys":["state-of-the-art methods","manual annotation","prediction systems","automatic annotation","machine learning","natural language processing","legal artificial intelligence","predictive justice","word representations","legal domains","federal court of canada"],"tags":["state-of-the-art methods","manual annotation","prediction systems","automatic annotation","machine learning","natural language processing","legal artificial intelligence","predictive justice","word representations","legal domains","federal court of canada"]},{"p_id":31402,"title":"Distributed classification of text documents on Apache Spark platform","abstract":"\u00a9 Springer International Publishing Switzerland 2016. This paper presents implementation of the system for subject classification of text documents based on the Apache Spark distributed computing framework. Classification of text documents starts with generation of high-dimensional feature vectors from documents; the task realized with methods and tools for natural language processing. The next steps involve reduction of dimensionality of feature vectors and training classifiers. In the paper we show how these consecutive steps can be realized on the Apache Spark platform dedicated to distributed processing of big data. We illustrate the proposed method by a sample classifier aimed to predict subject category of a document in English language Wikipedia.","keywords_author":["Apache Spark","Machine learning","Natural Language Processing (NLP)","Text subject classification"],"keywords_other":["English languages","Distributed processing","Distributed computing frameworks","High dimensional feature","Distributed classification","Reduction of dimensionality","NAtural language processing","Subject classification"],"max_cite":3.0,"pub_year":2016.0,"sources":"['scp']","rawkeys":["apache spark","subject classification","machine learning","natural language processing","distributed processing","high dimensional feature","natural language processing (nlp)","distributed computing frameworks","english languages","distributed classification","text subject classification","reduction of dimensionality"],"tags":["apache spark","machine learning","natural language processing","distributed processing","high dimensional feature","distributed computing frameworks","english languages","distributed classification","text subject classification","subjectivity classification","reduction of dimensionality"]},{"p_id":39595,"title":"Classifying case relations using syntactic, semantic and contextual features Clasificaci\u00f3n de roles sem\u00e1nticos usando caracter\u00ed sticas sint\u00e1cticas, sem\u00e1nticas y contextuales","abstract":"This paper presents a classification of semantic roles using syntactic, semantic and contextual features. The aim of our work is to identify types of semantic roles involving events and their actors; therefore, we fulfill a feature analysis in order to select the best feature subset which improves the fulfillment of the task. In addition, we compare four classification algorithms: Support Vector Machine (SVM), k-nearest neighbor (k-NN), Bayes classifier and decision tree classifier C4.5. This comparison was made in order to analyze the performance of these algorithms with all features against relevant features for each semantic role category. In our experimentation, we obtain that feature selection improved the performance of algorithms in our classification task, since with relevant features we obtained the best performance of 84.6% with decision tree classifier C4.5. The results for the labeling task can be used for knowledge representation or ontology learning.","keywords_author":["Knowledge acquisition","Machine learning","Natural language processing","Semantic roles classification"],"keywords_other":null,"max_cite":1.0,"pub_year":2013.0,"sources":"['scp']","rawkeys":["natural language processing","knowledge acquisition","machine learning","semantic roles classification"],"tags":["natural language processing","knowledge acquisition","machine learning","semantic roles classification"]},{"p_id":39596,"title":"Rewriting turkish texts written in english alphabet using turkish alphabet Ingiliz alfabesi kullanilarak yazilmis t\u00fcrk\u00e7e metinlerin t\u00fcrk alfabesine g\u00f6re yeniden olusturulmas","abstract":"Turkish texts written by English characters are easily comprehended by people, although performing this process by machines is still one of the unsolved Word Sense Disambiguation problems. Rewriting texts in English characters using Turkish characters is a natural language processing problem special to Turkish. Choosing the right Turkish word among different alternatives requires consideration of the text semantically. In this study, the effect of examination of the text either sentence or whole text based, on the right word determination is investigated. Performance of machine learning methods and statistical methods in right word determination is examined. The study is tested on randomly selected news texts. It is shown that examination of the text as a whole provides more information compared to sentence based methods and machine learning methods provides better results compared to statistical studies. \u00a9 2013 IEEE.","keywords_author":["Machine learning","Natural language processing","Text mining","Word sense disambiguation"],"keywords_other":["Text mining","Turkishs","Statistical study","Sentence-based","Word Sense Disambiguation","Machine learning methods","Turkish texts","NAtural language processing"],"max_cite":1.0,"pub_year":2013.0,"sources":"['scp']","rawkeys":["machine learning methods","turkishs","sentence-based","text mining","natural language processing","machine learning","turkish texts","word sense disambiguation","statistical study"],"tags":["machine learning methods","turkishs","sentence-based","text mining","natural language processing","machine learning","turkish texts","word sense disambiguation","statistical study"]},{"p_id":47789,"title":"Sentiment analysis for agglutinative languages","abstract":"\u00a9 2016 IEEE. Other people judgment, sentiment, opinion and think about actions or product or speech are important for person or company or institutions for decision making processes. Today people declare their opinions, feeling and judgment about a product or a service or a film or a speech on social media platforms. Social media are easily accessible platforms and provide remarkable sources for analysis and evaluation. Sentiment analysis find out the feeling of people on an object. The feeling covers attitudes, emotion and opinions. Sentiment is subjective impression; not facts. Sentiment analysis is basically a text classifying problem. Therefore, use Natural Language Processing (NLP), statistics, or machine learning methods to extract, identify, or otherwise characterize the sentiment content of a text.","keywords_author":["Machine Learning","NLP","Parity","Semantic Orientation","Sentient Lexical","Sentiment Analysis"],"keywords_other":["Agglutinative language","Analysis and evaluation","Semantic orientation","Sentiment analysis","Machine learning methods","Decision making process","Sentient Lexical","Parity"],"max_cite":0.0,"pub_year":2017.0,"sources":"['scp']","rawkeys":["machine learning methods","analysis and evaluation","nlp","semantic orientation","machine learning","agglutinative language","parity","sentient lexical","sentiment analysis","decision making process"],"tags":["machine learning methods","analysis and evaluation","semantic orientation","machine learning","agglutinative language","natural language processing","parity","sentient lexical","sentiment analysis","decision making process"]},{"p_id":31407,"title":"Hybrid recognition technology for isolated voice commands","abstract":"\u00a9 Springer International Publishing Switzerland 2016.The paper deals with two elements of the artificial intelligence methods\u2014 the natural language processing and machine learning. Hybrid recognition technology for isolated Lithuanian voice commands is described. By the hybrid approach we assume the combination of two different recognition methods to achieve higher recognition accuracy. The method which is based on the machine learning algorithm to combine the recognition results provided by two different recognizers is described. The first recognizer was HTK-based Lithuanian recognizer, the second one\u2014the Spanish language recognizer adapted to the Lithuanian language. The experimental results show that a hybrid decision-making rule learned by \u201crandom forest\u201d classifier works with 99.46% accuracy and exceeds the accuracy of the \u201cblind\u201d decision-making rule (96.12%). The average hybrid operation accuracy reaches 99.24%, when the recognizer recognizes voice commands out of 12 known speakers, and is equal to 99.18%, when it is applied to the unknown speaker.","keywords_author":["Classification algorithms","Hybrid recognition technology","Machine learning","Speech recognition"],"keywords_other":["Decision-making rules","Spanish language","Recognition accuracy","Recognition methods","Hybrid operations","Artificial intelligence methods","Classification algorithm","NAtural language processing"],"max_cite":3.0,"pub_year":2016.0,"sources":"['scp']","rawkeys":["hybrid operations","recognition methods","spanish language","machine learning","classification algorithms","natural language processing","speech recognition","recognition accuracy","decision-making rules","artificial intelligence methods","classification algorithm","hybrid recognition technology"],"tags":["hybrid operations","recognition methods","spanish language","machine learning","natural language processing","speech recognition","recognition accuracy","decision-making rules","artificial intelligence methods","classification algorithm","hybrid recognition technology"]},{"p_id":47793,"title":"ARAACOM: Arabic Algerian corpus for opinion mining","abstract":"\u00a9 2017 Association for Computing Machinery. Nowadays, it is no more needed to do an enormous effort to distribute a lot of forms to thousands of people and collect them, then convert this from into electronic format to track people opinion about some subjects. A lot of web sites can today reach a large spectrum with less effort. The majority of web sites suggest to their visitors to leave backups about their feeling of the site or events. So, this makes for us a lot of data which need powerful mean to exploit. Opinion mining in the web becomes more and more an attracting task, due the increasing need for individuals and societies to track the mood of people against several subjects of daily life (sports, politics, television,...). A lot of works in opinion mining was developed in western languages especially English, such works in Arabic language still very scarce. In this paper, we propose our approach, for opinion mining in Arabic Algerian news paper.","keywords_author":["Arabic comments","Machine learning","Natural language processing","Newspaper","Opinion mining","Sentiment analysis"],"keywords_other":["Electronic formats","Newspaper","Sentiment analysis","Arabic languages","Arabic comments","Daily lives","Opinion mining"],"max_cite":0.0,"pub_year":2017.0,"sources":"['scp']","rawkeys":["electronic formats","newspaper","arabic comments","machine learning","natural language processing","arabic languages","opinion mining","sentiment analysis","daily lives"],"tags":["electronic formats","newspaper","arabic comments","machine learning","natural language processing","arabic languages","opinion mining","sentiment analysis","daily lives"]},{"p_id":51893,"title":"Coreference Resolution in Latvian","abstract":"\u00a9 2014 The Authors and IOS Press. Coreference resolution (CR) is a current problem in natural language processing (NLP) research and it is a key task in applications such as question answering, text summarization and information extraction for which text understanding is of crucial importance. This paper describes a work in progress for improving Latvian coreference resolution that includes further experiments with the rule based LVCoref system, enlarging existing coreference corpus and the first efforts to adapt machine learning methods. LVCoref system now reaches 58.0% F-score using predicted mentions and 76.5% F-score if gold entity mentions are used.","keywords_author":["Coreference resolution","corpus","machine learning","rule based"],"keywords_other":["NAtural language processing","Co-reference resolutions","Question Answering","Text summarization","Machine learning methods","corpus","Rule based","Current problems"],"max_cite":0.0,"pub_year":2014.0,"sources":"['scp']","rawkeys":["machine learning methods","question answering","current problems","co-reference resolutions","rule based","text summarization","machine learning","natural language processing","corpus","coreference resolution"],"tags":["machine learning methods","current problems","rule based","text summarization","machine learning","natural language processing","information retrieval","corpus","coreference resolution"]},{"p_id":51895,"title":"Sentiment polarity identification using machine learning techniques","abstract":"The paper proposes an improved approach to the problem of sentiment polarity identification. Its main focus is on identifying and extracting the relevant information from natural language texts in order to obtain a set of best predictive features to be used for the classification task. Our approach of determining the polarity of a text consists of a combination of several processing techniques that obtains an efficient set of appropriate information for the underlying text. Among techniques, we have considered pruning the feature set to discard features without polarity or with less discriminative power, since their presence tend to mislead the learning process. Moreover, using word co-occurrence techniques, new composed bi-grams with high discriminative power are added which enhances the classification process. The best results are obtained using different combinations of techniques, depending on the dataset's homogeneity. On a homogeneous dataset, the performance in terms of precision is approximately 88% and, in terms of recall, a value of 93% is reached. In the case of a diverse dataset, the performance attained is 100%. \u00a9 2013 IEEE.","keywords_author":["Feature Selection","Machine Learning Techniques","Natural Language Processing","Sentiment Analysis"],"keywords_other":["Classification process","Classification tasks","Sentiment analysis","Natural language text","Discriminative power","Processing technique","NAtural language processing","Machine learning techniques"],"max_cite":0.0,"pub_year":2013.0,"sources":"['scp']","rawkeys":["discriminative power","machine learning techniques","natural language processing","classification tasks","processing technique","feature selection","classification process","sentiment analysis","natural language text"],"tags":["discriminative power","machine learning techniques","natural language processing","classification tasks","processing technique","feature selection","classification process","sentiment analysis","natural language text"]},{"p_id":27322,"title":"Standards alignment for metadata assignment","abstract":"The research in this paper describes a Machine Learning technique called hierarchical text categorization which is used to solve the problem of finding equivalents from among different state and national education standards. The approach is based on a set of manually aligned standards and utilizes the hierarchical structure present in the standards to achieve a more accurate result. Details of this approach and its evaluation are presented. Copyright 2007 ACM.","keywords_author":["Automatic metadata assignment","Educational standards","Hierarchical text classification","Machine learning","Natural language processing","NSDL"],"keywords_other":["Educational standards","Automatic metadata assignment","Hierarchical text classification"],"max_cite":6.0,"pub_year":2007.0,"sources":"['scp']","rawkeys":["hierarchical text classification","natural language processing","automatic metadata assignment","machine learning","educational standards","nsdl"],"tags":["hierarchical text classification","natural language processing","automatic metadata assignment","machine learning","educational standards","nsdl"]},{"p_id":45754,"title":"Social emotion mining techniques for facebook posts reaction prediction","abstract":"Copyright \u00a9 2018 by SCITEPRESS \u2013 Science and Technology Publications, Lda. All rights reserved. As of February 2016 Facebook allows users to express their experienced emotions about a post by using five so-called \u2018reactions\u2019. This research paper proposes and evaluates alternative methods for predicting these reactions to user posts on public pages of firms\/companies (like supermarket chains). For this purpose, we collected posts (and their reactions) from Facebook pages of large supermarket chains and constructed a dataset which is available for other researches. In order to predict the distribution of reactions of a new post, neural network architectures (convolutional and recurrent neural networks) were tested using pretrained word embeddings. Results of the neural networks were improved by introducing a bootstrapping approach for sentiment and emotion mining on the comments for each post. The final model (a combination of neural network and a baseline emotion miner) is able to predict the reaction distribution on Facebook posts with a mean squared error (or misclassification rate) of 0.135.","keywords_author":["Deep Learning","Emotion Mining","Natural Language Processing","Social Media"],"keywords_other":["Supermarket chains","Misclassification rates","Combination of neural-network","Reaction distribution","Social media","Mining techniques","Mean squared error","Reaction prediction"],"max_cite":0.0,"pub_year":2018.0,"sources":"['scp']","rawkeys":["emotion mining","misclassification rates","deep learning","social media","natural language processing","mean squared error","combination of neural-network","supermarket chains","mining techniques","reaction distribution","reaction prediction"],"tags":["emotion mining","misclassification rates","social media","machine learning","natural language processing","mean squared error","combination of neural-network","supermarket chains","mining techniques","reaction distribution","reaction prediction"]},{"p_id":35517,"title":"A cluster-based classification approach to semantic role labeling","abstract":"In this paper, a new approach for multi-class classification problems is applied to the Semantic Role Labeling (SRL) problem, which is an important task for natural language processing systems to achieve better semantic understanding of text. The new approach applies to any classification problem with large feature sets. Data is partitioned using clusters on a subset of the features. A multi-label classifier is then trained individually on each cluster, using automatic feature selection to customize the larger feature set for the cluster. This algorithm is applied to the Semantic Role Labeling problem and achieves improvements in accuracy for both the argument identification classifier and the argument labeling classifier. \u00a9 2008 Springer-Verlag Berlin Heidelberg.","keywords_author":["Classification","Clustering","Machine Learning","Natural Language Processing"],"keywords_other":["Classification approach","Cluster-based","Clustering","Natural Language Processing","Machine Learning","New approaches","Semantic role labeling","Classification","Automatic feature selection","International conferences","Semantic understanding","Multi-class classification problems","Feature sets"],"max_cite":2.0,"pub_year":2008.0,"sources":"['scp']","rawkeys":["new approaches","cluster-based","semantic understanding","international conferences","machine learning","natural language processing","clustering","classification approach","multi-class classification problems","classification","semantic role labeling","automatic feature selection","feature sets"],"tags":["new approaches","multiclass classification problems","cluster-based","semantic understanding","international conferences","machine learning","natural language processing","clustering","classification approach","classification","semantic role labeling","automatic feature selection","feature sets"]},{"p_id":39614,"title":"Efficient online feature selection based on \u21131-regularized logistic regression","abstract":"Finding features for classifiers is one of the most important concerns in various fields, such as information retrieval, speech recognition, bio-informatics and natural language processing, for improving classifier prediction performance. Online grafting is one solution for finding useful features from an extremely large feature set. Given a sequence of features, online grafting selects or discards each feature in the sequence of features one at a time. Online grafting is preferable in that it incrementally selects features, and it is defined as an optimization problem based on \u21131-regularized logistic regression. However, its learning is inefficient due to frequent parameter optimization. We propose two improved methods, in terms of efficiency, for online grafting that approximate original online grafting by testing multiple features simultaneously. The experiments have shown that our methods significantly improved efficiency of online grafting. Though our methods are approximation techniques, deterioration of prediction performance was negligibly small.","keywords_author":["\u21131-regularized logistic regression","Feature selection","Grafting","Machine learning"],"keywords_other":["Multiple features","Optimization problems","Approximation techniques","Parameter optimization","Prediction performance","Logistic regressions","NAtural language processing","Online feature selection"],"max_cite":1.0,"pub_year":2013.0,"sources":"['scp']","rawkeys":["approximation techniques","multiple features","\u21131-regularized logistic regression","optimization problems","machine learning","natural language processing","feature selection","logistic regressions","grafting","online feature selection","parameter optimization","prediction performance"],"tags":["approximation techniques","multiple features","\u21131-regularized logistic regression","optimization problems","machine learning","natural language processing","feature selection","logistic regressions","grafting","online feature selection","parameter optimization","prediction performance"]},{"p_id":21186,"title":"Using semantic predications to uncover drug-drug interactions in clinical data","abstract":"In this study we report on potential drug-drug interactions between drugs occurring in patient clinical data. Results are based on relationships in SemMedDB, a database of structured knowledge extracted from all MEDLINE citations (titles and abstracts) using SemRep. The core of our methodology is to construct two potential drug-drug interaction schemas, based on relationships extracted from SemMedDB. In the first schema, Drug1 and Drug2 interact through Drug1's effect on some gene, which in turn affects Drug2. In the second, Drug1 affects Gene1, while Drug2 affects Gene2. Gene1 and Gene2, together, then have an effect on some biological function. After checking each drug pair from the medication lists of each of 22 patients, we found 19 known and 62 unknown drug-drug interactions using both schemas. For example, our results suggest that the interaction of Lisinopril, an ACE inhibitor commonly prescribed for hypertension, and the antidepressant sertraline can potentially increase the likelihood and possibly the severity of psoriasis. We also assessed the relationships extracted by SemRep from a linguistic perspective and found that the precision of SemRep was 0.58 for 300 randomly selected sentences from MEDLINE. Our study demonstrates that the use of structured knowledge in the form of relationships from the biomedical literature can support the discovery of potential drug-drug interactions occurring in patient clinical data. Moreover, SemMedDB provides a good knowledge resource for expanding the range of drugs, genes, and biological functions considered as elements in various drug-drug interaction pathways. \u00a9 2014 Elsevier Inc.","keywords_author":["Drug-drug interactions","MEDLINE","Natural language processing","Semantic predication","SemMedDB","SemRep"],"keywords_other":["Semantic predications","Sertraline","SemMedDB","Drug-drug interactions","Humans","Semantics","Lisinopril","SemRep","Serotonin Uptake Inhibitors","Medline","Drug Interactions","Angiotensin-Converting Enzyme Inhibitors","NAtural language processing"],"max_cite":15.0,"pub_year":2014.0,"sources":"['scp']","rawkeys":["medline","semrep","semmeddb","lisinopril","semantics","sertraline","drug-drug interactions","natural language processing","drug interactions","humans","semantic predications","serotonin uptake inhibitors","angiotensin-converting enzyme inhibitors","semantic predication"],"tags":["medline","semrep","semmeddb","lisinopril","semantics","sertraline","drug-drug interactions","natural language processing","drug interactions","humans","semantic predications","serotonin uptake inhibitors","angiotensin-converting enzyme inhibitors"]},{"p_id":31427,"title":"Deriving the geographic footprint of cognitive regions","abstract":"\u00a9 2016, Springer International Publishing Switzerland. The characterization of place and its representation in current Geographic Information System (GIS) has become a prominent research topic. This paper concentrates on places that are cognitive regions, and presents a computational framework to derive the geographic footprint of these regions. The main idea is to use Natural Language Processing (NLP) tools to identify unique geographic features from User Generated Content (UGC) sources consisting of textual descriptions of places. These features are used to detect on a map an initial area that the descriptions refer to. A semantic representation of this area is extracted from a GIS and passed over to a Machine Learning (ML) algorithm that locates other areas according to semantic similarity. As a case study, we employ the proposed framework to derive the geographic footprint of the historic center of Vienna and validate the results by comparing the derived region against a historical map of the city.","keywords_author":["Cognitive regions","Geographic information retrieval","Machine learning","Natural language processing","Semantic similarity","User generated content"],"keywords_other":["User-generated content","Semantic representation","Geographic footprints","Semantic similarity","Geographic information retrieval","User generated content (UGC)","Computational framework","Cognitive regions"],"max_cite":3.0,"pub_year":2016.0,"sources":"['scp']","rawkeys":["user generated content (ugc)","cognitive regions","geographic information retrieval","geographic footprints","machine learning","natural language processing","semantic representation","semantic similarity","user-generated content","user generated content","computational framework"],"tags":["cognitive regions","geographic information retrieval","geographic footprints","machine learning","natural language processing","semantic representation","semantic similarity","user-generated content","computational framework"]},{"p_id":25284,"title":"A framework for automatic causality extraction using semantic similarity","abstract":"Textual documents are the most common way of storing and distributing information within organizations. Extracting useful information from large text collections is therefore the goal of every organization that would like to take advantage of the experience encapsulated in those texts. Entering data using a free text style is easy, as it does not require any special training. However, unstructured texts pose a major challenge for automatic extraction and retrieval systems. Generally, deep levels of text analysis using advanced and complex linguistic processing are necessary that involve computational linguistic experts and domain experts. Linguistic experts are rare in engineering organizations, which thus find it difficult to apply and exploit such advanced extraction techniques. It is therefore desirable to minimize the extensive involvement of linguist experts by learning extraction patterns automatically from example texts. In doing so, the analysis of given texts is necessary in order to identify the scope and suitable automatic methods. Focusing on causality reasoning in the field of fault diagnosis, the results of experimenting with an automatic causality extraction method using shallow linguistic processing are presented. Copyright \u00a9 2007 by ASME.","keywords_author":["Automatic causality extraction","Natural language processing","Semantic similarity","Supervised machine learning"],"keywords_other":["deep levels","retrieval systems","Extraction patterns","Extraction techniques","Domain Experts (DEs)","In order","Text analysis","Semantic similarity","Textual documents","Automatic methods","Linguistic processing","Technical conferences","International designs","Fault diagnosis (FD)","Extraction methods","Automatic extraction","Engineering organizations","Free texts"],"max_cite":8.0,"pub_year":2008.0,"sources":"['scp']","rawkeys":["international designs","retrieval systems","extraction techniques","automatic extraction","semantic similarity","automatic methods","in order","extraction methods","textual documents","engineering organizations","linguistic processing","fault diagnosis (fd)","deep levels","extraction patterns","natural language processing","domain experts (des)","automatic causality extraction","text analysis","free texts","technical conferences","supervised machine learning"],"tags":["retrieval systems","extraction techniques","automatic extraction","semantic similarity","extraction method","in order","internal design","deep level","textual documents","engineering organizations","domain experts","linguistic processing","automatic method","extraction patterns","natural language processing","automatic causality extraction","text analysis","free texts","technical conferences","supervised machine learning","fault diagnosis"]},{"p_id":43719,"title":"Review of State-of-the-Art in Deep Learning Artificial Intelligence","abstract":"\u00a9 2018, Allerton Press, Inc. The current state-of-the-art in Deep Learning (DL) based artificial intelligence (AI) is reviewed. A special emphasis is made to compare the level of a concrete AI system with human abilities to show what remains to be done to achieve human level AI. Several estimates are proposed for comparison of the current \u201cintellectual level\u201d of AI systems with the human level. Among them is relation of Shannon\u2019s estimate for lower bound on human word perplexity to recent progress in natural language AI modeling. Relations between the operation of DL constructions and principles of live neural information processing are discussed. The problem of AI risks and benefits is also reviewed based on arguments from both sides.","keywords_author":["artificial intelligence","deep learning","generative adversarial networks","natural language processing","Pavlov Principle","residual networks"],"keywords_other":["Neural information processing","Human abilities","Recent progress","State of the art","Word perplexity","Adversarial networks","Natural languages","Pavlov Principle"],"max_cite":0.0,"pub_year":2018.0,"sources":"['scp']","rawkeys":["artificial intelligence","human abilities","deep learning","generative adversarial networks","natural languages","natural language processing","recent progress","residual networks","state of the art","word perplexity","adversarial networks","neural information processing","pavlov principle"],"tags":["human abilities","generative adversarial networks","natural languages","machine learning","natural language processing","recent progress","state of the art","word perplexity","adversarial networks","residual network","neural information processing","pavlov principle"]},{"p_id":51915,"title":"Automatically identifying health- and clinical-related content in Wikipedia","abstract":"Physicians are increasingly using the Internet for finding medical information related to patient care. Wikipedia is a valuable online medical resource to be integrated into existing clinical question answering (QA) systems. On the other hand, Wikipedia contains a full spectrum of world's knowledge and therefore comprises a large partition of non-health-related content, which makes disambiguation more challenging and consequently leads to large overhead for existing systems to effectively filter irrelevant information. To overcome this, we have developed both unsupervised and supervised approaches to identify health-related articles as well as clinically relevant articles. Furthermore, we explored novel features by extracting health related hierarchy from the Wikipedia category network, from which a variety of features were derived and evaluated. Our experiments show promising results and also demonstrate that employing the category hierarchy can effectively improve the system performance. \u00a9 2013 IMIA and IOS Press.","keywords_author":["Machine Learning","Natural Language Processing"],"keywords_other":["Pattern Recognition, Automated","Natural Language Processing","Social Media","Health Information Management","Vocabulary, Controlled","Artificial Intelligence","Data Mining","Encyclopedias as Topic"],"max_cite":0.0,"pub_year":2013.0,"sources":"['scp']","rawkeys":["artificial intelligence","vocabulary","encyclopedias as topic","data mining","automated","social media","machine learning","natural language processing","controlled","pattern recognition","health information management"],"tags":["vocabulary","encyclopedias as topic","data mining","automated","social media","machine learning","control","natural language processing","pattern recognition","health information management"]},{"p_id":29391,"title":"A personalized recommender system using Machine Learning based Sentiment Analysis over social data","abstract":"\u00a9 2016 IEEE. Social Media platforms are already an indispensable part of our daily lives. With its constant growth, it has contributed to superfluous, heterogeneous data which can be overwhelming due to its volume and velocity, thus limiting the availability of relevant and required information when a particular query is to be served. Hence, a need for personalized, fine-grained user preference-oriented framework for resolving this problem and also, to enhance user experience is increasingly felt. In this paper, we propose a such a social framework, which extracts user's reviews, comments of restaurants and points of interest such as events and locations, to personalize and rank suggestions based on user preferences. Machine Learning and Sentiment Analysis based techniques are used for further optimizing search query results. This provides the user with quicker and more relevant data, thus avoiding irrelevant data and providing much needed personalization.","keywords_author":["Machine Learning","Natural Language Processing","Personalized Search Engine","Sentiment Analysis"],"keywords_other":["Personalized recommender systems","Sentiment analysis","Heterogeneous data","Social media platforms","Personalizations","NAtural language processing","Points of interest","Personalized search"],"max_cite":4.0,"pub_year":2016.0,"sources":"['scp']","rawkeys":["personalized recommender systems","social media platforms","natural language processing","machine learning","personalized search engine","points of interest","heterogeneous data","personalized search","personalizations","sentiment analysis"],"tags":["point of interest","social media platforms","natural language processing","machine learning","personalized search engine","heterogeneous data","personalized search","personalizations","sentiment analysis","personalized recommendation systems"]},{"p_id":13007,"title":"A Review of Best Practice Recommendations for Text Analysis in R (and a User-Friendly App)","abstract":"In recent decades, the amount of text available for organizational science research has grown tremendously. Despite the availability of text and advances in text analysis methods, many of these techniques remain largely segmented by discipline. Moreover, there is an increasing number of open-source tools (R, Python) for text analysis, yet these tools are not easily taken advantage of by social science researchers who likely have limited programming knowledge and exposure to computational methods. In this article, we compare quantitative and qualitative text analysis methods used across social sciences. We describe basic terminology and the overlooked, but critically important, steps in pre-processing raw text (e.g., selection of stop words; stemming). Next, we provide an exploratory analysis of open-ended responses from a prototypical survey dataset using topic modeling with R. We provide a list of best practice recommendations for text analysis focused on (1) hypothesis and question formation, (2) design and data collection, (3) data pre-processing, and (4) topic modeling. We also discuss the creation of scale scores for more traditional correlation and regression analyses. All the data are available in an online repository for the interested reader to practice with, along with a reference list for additional reading, an R markdown file, and an open source interactive topic model tool (topicApp; see https:\/\/github.com\/wesslen\/topicApp, https:\/\/github.com\/wesslen\/text-analysis-org-science, https:\/\/dataverse.unc.edu\/dataset.xhtml?persistentId=doi:10.15139\/S3\/R4W7ZS).","keywords_author":["Text analysis","Topic modeling","Structural topic modeling","Thematic analysis","Content-analysis","Dictionary analysis","Natural language processing"],"keywords_other":["GROUNDED THEORY","VALIDATION","VARIABLES","INFERENCE","TOPIC MODELS"],"max_cite":0.0,"pub_year":2018.0,"sources":"['wos']","rawkeys":["structural topic modeling","variables","thematic analysis","dictionary analysis","topic modeling","natural language processing","validation","text analysis","inference","grounded theory","topic models","content-analysis"],"tags":["thematic analysis","dictionary analysis","structural topic model","natural language processing","content analysis","topic modeling","validation","variability","text analysis","inference","grounded theory"]},{"p_id":6865,"title":"Jumping NLP curves: A review of natural language processing research","abstract":"Natural language processing (NLP) is a theory-motivated range of computational techniques for the automatic analysis and representation of human language. NLP research has evolved from the era of punch cards and batch processing (in which the analysis of a sentence could take up to 7 minutes) to the era of Google and the likes of it (in which millions of webpages can be processed in less than a second). This review paper draws on recent developments in NLP research to look at the past, present, and future of NLP technology in a new light. Borrowing the paradigm of 'jumping curves' from the field of business management and marketing prediction, this survey article reinterprets the evolution of NLP research as the intersection of three overlapping curves-namely Syntactics, Semantics, and Pragmatics Curves- which will eventually lead NLP research to evolve into natural language understanding.","keywords_author":null,"keywords_other":["DOCUMENTS","Natural language understanding","Punch cards","Human language","NETWORKS","Business management","Automatic analysis","CLASSIFICATION","Review papers","Computational technique","TEXT CATEGORIZATION","WEB","MODELS","EVOLUTIONARY","NAtural language processing","LOGIC"],"max_cite":187.0,"pub_year":2014.0,"sources":"['scp', 'wos']","rawkeys":["review papers","computational technique","human language","natural language processing","web","networks","punch cards","classification","models","documents","business management","automatic analysis","text categorization","evolutionary","logic","natural language understanding"],"tags":["review papers","computational technique","model","documentation","business managers","natural language processing","web","networks","punch cards","classification","human language","automatic analysis","text categorization","evolutionary","logic","natural language understanding"]},{"p_id":33495,"title":"Extracting contract elements","abstract":"\u00a9 2017 Association for Computing Machinery. We study how contract element extraction can be automated. We provide a labeled dataset with gold contract element annotations, along with an unlabeled dataset of contracts that can be used to pre-train word embeddings. Both datasets are provided in an encoded form to bypass privacy issues. We describe and experimentally compare several contract element extraction methods that use manually written rules and linear classifiers (logistic regression, SVMs) with hand-crafted features, word embeddings, and part-of-speech tag embeddings. The best results are obtained by a hybrid method that combines machine learning (with hand-crafted features and embeddings) and manually written post-processing rules.","keywords_author":["Contracts","Datasets","Evaluation","Information extraction","Legal text analytics","Machine learning","Natural language processing"],"keywords_other":["Element extraction","Datasets","Linear classifiers","Post processing","Logistic regressions","Labeled dataset","Evaluation","Legal texts"],"max_cite":2.0,"pub_year":2017.0,"sources":"['scp']","rawkeys":["linear classifiers","datasets","element extraction","information extraction","labeled dataset","machine learning","legal text analytics","evaluation","natural language processing","contracts","post processing","logistic regressions","legal texts"],"tags":["linear classifiers","element extraction","information extraction","labeled dataset","machine learning","legal text analytics","evaluation","natural language processing","contracts","data sets","post processing","logistic regressions","legal texts"]},{"p_id":39641,"title":"iTagger: Part-of-speech tagging based on SBCB learning algorithm","abstract":"The problem of part-of-speech (POS) tagging or disambiguation is a practical issue in natural language processing (NLP) community, especially in the development of a machine translation system. The performance of POS tagging system may interference the subsequent analytical tasks in the translation process, and thereafter affects the overall translation quality. This paper presents a novel POS tagging system, iTagger, which is developed based on Selecting Base Classifiers on Bagging (SBCB) learning algorithm. In this work, the POS tagging task is regarded as a classification problem. Features such as the surrounding context of ambiguous candidates, n-gram information, lexical items and linguistic clues are used and automatically extracted from the annotated corpus. The proposed system has been compared against two state-of-the-art tagging methods, Hidden Markov Model (HMM) and Maximum Entropy. The empirical results conducted on the corpora of (English) Brown corpus, (Portuguese) Tycho Brahe corpus and the Chinese Tree Bank corpus reveal the competitiveness of iTagger. Moreover, the iTagger has been developed and released to the public as library and tool for various development and application purposes. \u00a9 (2013) Trans Tech Publications, Switzerland.","keywords_author":["Machine learning","Part-of-speech tagging","POS tagging","SBCB"],"keywords_other":["Base classifiers","Part of speech tagging","PoS tagging","Hidden Markov model(HMM)","Maximum entropy","NAtural language processing","SBCB","Translation process","Practical issues","Machine translation systems","Translation quality","Development and applications","Lexical items","POS tagging systems"],"max_cite":1.0,"pub_year":2013.0,"sources":"['scp']","rawkeys":["development and applications","part-of-speech tagging","pos tagging systems","maximum entropy","hidden markov model(hmm)","practical issues","sbcb","lexical items","machine learning","machine translation systems","natural language processing","part of speech tagging","translation process","translation quality","pos tagging","base classifiers"],"tags":["development and applications","pos tagging systems","hidden markov models","maximum entropy","practical issues","sbcb","lexical items","machine learning","machine translation systems","natural language processing","part of speech tagging","translation process","translation quality","pos tagging","base classifiers"]},{"p_id":23258,"title":"A memory-based approach to learning shallow natural language patterns","abstract":"Recognizing shallow linguistic patterns, such as basic syntactic relationships between words, is a common task in applied natural language and text processing. The common practice for approaching this task is by tedious manual definition of possible pattern structures, often in the form of regular expressions or finite automata. This paper presents a novel memory-based learning method that recognizes shallow patterns in new text based on a bracketed training corpus. The examples are stored as-is, in efficient data structures. Generalization is performed on-line at recognition time by comparing subsequences of the new text to positive and negative evidence in the corpus. This way, no information in the training is lost, as can happen in other learning systems that construct a single generalized model at the time of training. The paper presents experimental results for recognizing noun phrase, subject-verb and verb-object patterns in English. \u00a9 1999 Taylor & Francis Ltd.","keywords_author":["Chunking","Machine learning","Memory based learning","Natural language processing","Noun-phrases","Sequential patterns","Shallow parsing","Statistical language processing"],"keywords_other":null,"max_cite":11.0,"pub_year":1999.0,"sources":"['scp']","rawkeys":["statistical language processing","noun-phrases","natural language processing","machine learning","memory based learning","chunking","sequential patterns","shallow parsing"],"tags":["statistical language processing","noun phrase","natural language processing","machine learning","memory-based learning","chunking","sequential patterns","shallow parsing"]},{"p_id":39644,"title":"Learning multivariate distributions by competitive assembly of marginals","abstract":"We present a new framework for learning high-dimensional multivariate probability distributions from estimated marginals. The approach is motivated by compositional models and Bayesian networks, and designed to adapt to small sample sizes. We start with a large, overlapping set of elementary statistical building blocks, or \"primitives,\" which are low-dimensional marginal distributions learned from data. Each variable may appear in many primitives. Subsets of primitives are combined in a Lego-like fashion to construct a probabilistic graphical model; only a small fraction of the primitives will participate in any valid construction. Since primitives can be precomputed, parameter estimation and structure search are separated. Model complexity is controlled by strong biases; we adapt the primitives to the amount of training data and impose rules which restrict the merging of them into allowable compositions. The likelihood of the data decomposes into a sum of local gains, one for each primitive in the final structure. We focus on a specific subclass of networks which are binary forests. Structure optimization corresponds to an integer linear program and the maximizing composition can be computed for reasonably large numbers of variables. Performance is evaluated using both synthetic data and real datasets from natural language processing and computational biology. \u00a9 2012 IEEE.","keywords_author":["Graphs and networks","linear programming","machine learning","statistical models"],"keywords_other":["High-dimensional","Graphs and networks","Marginal distribution","Real data sets","Structure optimization","Training data","Building blockes","Marginals","Model complexity","Statistical models","Multivariate distributions","Compositional models","Small Sample Size","Probabilistic graphical models","Synthetic data","Integer linear programs","Computational biology","NAtural language processing"],"max_cite":1.0,"pub_year":2013.0,"sources":"['scp']","rawkeys":["structure optimization","training data","building blockes","compositional models","graphs and networks","machine learning","marginal distribution","real data sets","synthetic data","small sample size","model complexity","multivariate distributions","statistical models","probabilistic graphical models","high-dimensional","computational biology","natural language processing","integer linear programs","linear programming","marginals"],"tags":["training data","building blockes","compositional models","structural optimization","graphs and networks","margin","machine learning","marginal distribution","real data sets","synthetic data","small sample size","model complexity","multivariate distributions","statistical models","probabilistic graphical models","high-dimensional","integer linear programming","computational biology","natural language processing","linear programming"]},{"p_id":43741,"title":"Automatically classifying user requests in crowdsourcing requirements engineering","abstract":"\u00a9 2017 In order to make a software project succeed, it is necessary to determine the requirements for systems and to document them in a suitable manner. Many ways for requirements elicitation have been discussed. One way is to gather requirements with crowdsourcing methods, which has been discussed for years and is called crowdsourcing requirements engineering. User requests forums in open source communities, where users can propose their expected features of a software product, are common examples of platforms for gathering requirements from the crowd. Requirements collected from these platforms are often informal text descriptions and we name them user requests. In order to transform user requests into structured software requirements, it is better to know the class of requirements that each request belongs to so that each request can be rewritten according to corresponding requirement templates. In this paper, we propose an effective classification methodology by employing both project-specific and non-project-specific keywords and machine learning algorithms. The proposed strategy does well in achieving high classification accuracy by using keywords as features, reducing considerable manual efforts in building machine learning based classifiers, and having stable performance in finding minority classes no matter how few instances they have.","keywords_author":["Crowdsourcing requirements engineering","Machine learning","Natural language processing","Software requirements classification"],"keywords_other":["Software project","Software requirements","Requirements elicitation","Open source communities","Classification methodologies","Software products","Stable performance","Classification accuracy"],"max_cite":0.0,"pub_year":2018.0,"sources":"['scp']","rawkeys":["open source communities","crowdsourcing requirements engineering","classification accuracy","software requirements","natural language processing","machine learning","software requirements classification","classification methodologies","stable performance","software project","requirements elicitation","software products"],"tags":["open source communities","crowdsourcing requirements engineering","classification accuracy","software requirements","natural language processing","machine learning","software requirements classification","classification methodologies","stable performance","software project","requirements elicitation","software products"]},{"p_id":117472,"title":"Universal, unsupervised (rule-based), uncovered sentiment analysis","abstract":"We present a novel unsupervised approach for multilingual sentiment analysis driven by compositional syntax-based rules. On the one hand, we exploit some of the main advantages of unsupervised algorithms: (1) the interpretability of their output, in contrast with most supervised models, Which behave as a black box and (2) their robustness across different corpora and domains. On the other hand, by introducing the concept of compositional operations and exploiting syntactic information in the form of universal dependencies, we tackle one of their main drawbacks: their rigidity on data that are structured differently depending on the language concerned. Experiments show an improvement both over existing unsupervised methods, and over state-of-the-art supervised models when evaluating outside their corpus of origin. Experiments also show how the same compositional operations can be shared across languages. The system is available at http:\/\/www.grupolys.org\/software\/UUUSA\/. (C) 2016 Elsevier B.V. All rights reserved.","keywords_author":["Sentiment analysis","Multilingual","Dependency parsing","Natural language processing"],"keywords_other":["REVIEWS","SOCIAL MEDIA","LEXICON","TWEETS","CLASSIFICATION","SUBJECTIVITY","WEB","TRANSLATION"],"max_cite":4.0,"pub_year":2017.0,"sources":"['wos']","rawkeys":["multilingual","lexicon","translation","social media","natural language processing","reviews","dependency parsing","classification","web","subjectivity","sentiment analysis","tweets"],"tags":["multilingual","lexicon","translation","social media","natural language processing","dependency parsing","classification","web","review","subjectivity","sentiment analysis","tweets"]},{"p_id":19173,"title":"Emotion recognition in text for 3-D facial expression rendering","abstract":"Emotions are a key semantic component of human communication. This study focuses on automatic emotion detection in descriptive sentences and how this can be used to tune facial expression parameters for 3-D character generation. A comparison of manual and automatic word feature selection approaches is performed to determine the influence of word features on classification accuracy using support vector machines (SVM). The automatic emotion feature selection algorithm presented here builds on the framework used by mutual information for feature selection. Results of the study indicate that the set of automatically selected features was as good as the set of manually selected features. The proposed automatic feature selection algorithm implemented in this study helped to detect new words from the training corpus which were relevant to the classification task but were not considered by the researchers. An example of potential outcomes from facial expression tuning is also presented. The analysis includes initial results for dealing with the class imbalance challenge present in the data. \u00a9 2010 IEEE.","keywords_author":["Machine learning","natural language processing","semantic analysis","text-to-scene processing"],"keywords_other":["Classification tasks","semantic analysis","Facial Expressions","Mutual informations","Emotion detection","Human communications","Training corpus","Semantic components","Facial expression parameters","Emotion feature","Class imbalance","Potential outcomes","Machine-learning","Emotion recognition","natural language processing","Scene processing","Automatic feature selection","Character generation","Classification accuracy","Feature selection"],"max_cite":21.0,"pub_year":2010.0,"sources":"['scp']","rawkeys":["semantic analysis","classification tasks","mutual informations","facial expression parameters","machine learning","training corpus","feature selection","character generation","automatic feature selection","facial expressions","machine-learning","scene processing","class imbalance","human communications","classification accuracy","text-to-scene processing","natural language processing","emotion feature","emotion recognition","emotion detection","potential outcomes","semantic components"],"tags":["semantic analysis","classification tasks","facial expression parameters","machine learning","training corpus","feature selection","character generation","automatic feature selection","facial expressions","scene processing","class imbalance","human communications","mutual information","classification accuracy","text-to-scene processing","natural language processing","emotion feature","emotion recognition","emotion detection","potential outcomes","semantic components"]},{"p_id":33512,"title":"Lexicon based semantic detection of sentiments using expected likelihood estimate smoothed odds ratio","abstract":"\u00a9 2016, Springer Science+Business Media Dordrecht.Sentiment analysis is an active research area in today\u2019s era due to the abundance of opinionated data present on online social networks. Semantic detection is a sub-category of sentiment analysis which deals with the identification of sentiment orientation in any text. Many sentiment applications rely on lexicons to supply features to a model. Various machine learning algorithms and sentiment lexicons have been proposed in research in order to improve sentiment categorization. Supervised machine learning algorithms and domain specific sentiment lexicons generally perform better as compared to the unsupervised or semi-supervised domain independent lexicon based approaches. The core hindrance in the application of supervised algorithms or domain specific sentiment lexicons is the unavailability of sentiment labeled training datasets for every domain. On the other hand, the performance of algorithms based on general purpose sentiment lexicons needs improvement. This research is focused on building a general purpose sentiment lexicon in a semi-supervised manner. The proposed lexicon defines word semantics based on Expected Likelihood Estimate Smoothed Odds Ratio that are then incorporated with supervised machine learning based model selection approach. A comprehensive performance comparison verifies the superiority of our proposed approach.","keywords_author":["Machine learning","Natural language processing","Opinion mining","Sentiment analysis","Support vector machine"],"keywords_other":["Expected likelihoods","Comprehensive performance","Performance of algorithm","On-line social networks","Sentiment analysis","Supervised machine learning","NAtural language processing","Opinion mining"],"max_cite":2.0,"pub_year":2017.0,"sources":"['scp']","rawkeys":["on-line social networks","expected likelihoods","natural language processing","machine learning","sentiment analysis","support vector machine","supervised machine learning","opinion mining","comprehensive performance","performance of algorithm"],"tags":["on-line social networks","expected likelihoods","natural language processing","machine learning","sentiment analysis","supervised machine learning","opinion mining","comprehensive performance","performance of algorithm"]},{"p_id":39658,"title":"Supporting Agile Software Development by Natural Language Processing","abstract":"Agile software development puts more emphasis on working programs than on documentation. However, this may cause complications from the management perspective when an overview of the progress achieved within a project needs to be provided. In this paper, we outline the potential for applying natural language processing (NLP) in order to support agile development. We point out that using NLP, the artifacts created during agile software development activities can be traced back to the requirements expressed in user stories. This allows determining how far the project has progressed in terms of realized requirements. \u00a9 Springer-Verlag Berlin Heidelberg 2013.","keywords_author":["Agile Software Development","Machine Learning","Natural Language Processing","Project Management"],"keywords_other":["User stories","Agile development","NAtural language processing","Agile software development"],"max_cite":1.0,"pub_year":2013.0,"sources":"['scp']","rawkeys":["machine learning","natural language processing","agile development","user stories","agile software development","project management"],"tags":["machine learning","natural language processing","agile development","user stories","agile software development","project management"]},{"p_id":43755,"title":"Recognizing textual entailment: Challenges in the Portuguese language","abstract":"\u00a9 2018 by the authors. Recognizing textual entailment comprises the task of determining semantic entailment relations between text fragments. A text fragment entails another text fragment if, from the meaning of the former, one can infer the meaning of the latter. If such relation is bidirectional, then we are in the presence of a paraphrase. Automatically recognizing textual entailment relations captures major semantic inference needs in several natural language processing (NLP) applications. As in many NLP tasks, textual entailment corpora for English abound, while the same is not true for more resource-scarce languages such as Portuguese. Exploiting what seems to be the only Portuguese corpus for textual entailment and paraphrases (the ASSIN corpus), in this paper, we address the task of automatically recognizing textual entailment (RTE) and paraphrases from text written in the Portuguese language, by employing supervised machine learning techniques. We employ lexical, syntactic and semantic features, and analyze the impact of using semantic-based approaches in the performance of the system. We then try to take advantage of the bi-dialect nature of ASSIN to compensate its limited size. With the same aim, we explore modeling the task of recognizing textual entailment and paraphrases as a binary classification problem by considering the bidirectional nature of paraphrases as entailment relationships. Addressing the task as a multi-class classification problem, we achieve results in line with the winner of the ASSIN Challenge. In addition, we conclude that semantic-based approaches are promising in this task, and that combining data from European and Brazilian Portuguese is less straightforward than it may initially seem. The binary classification modeling of the problem does not seem to bring advantages to the original multi-class model, despite the outstanding results obtained by the binary classifier for recognizing textual entailments.","keywords_author":["Artificial intelligence","Machine learning","Natural language processing","Paraphrase detection","Recognizing textual entailment"],"keywords_other":["Binary classification problems","Binary classification","Bidirectional nature","Portuguese languages","Multiclass classification problems","Semantic entailment","Recognizing textual entailments","Supervised machine learning"],"max_cite":0.0,"pub_year":2018.0,"sources":"['scp']","rawkeys":["bidirectional nature","artificial intelligence","multiclass classification problems","semantic entailment","recognizing textual entailments","machine learning","natural language processing","binary classification","recognizing textual entailment","binary classification problems","supervised machine learning","portuguese languages","paraphrase detection"],"tags":["bidirectional nature","multiclass classification problems","semantic entailment","recognizing textual entailments","machine learning","natural language processing","binary classification","binary classification problems","supervised machine learning","portuguese languages","paraphrase detection"]},{"p_id":117484,"title":"ABSA Toolkit: An Open Source Tool for Aspect Based Sentiment Analysis","abstract":"With a rapid increase in e-commerce websites, people are often interested in analyzing customer reviews expressing customer sentiments on different features of a product before making purchase decisions. In this paper, we present ABSA (Aspect-Based Sentiment Analysis) Toolkit developed for performing aspect-level sentiment analysis on customer reviews. The system has two main phases: (a) development phase and (b) production phase. The development phase allows a user to train models for performing aspect level sentiment analysis tasks on the target domain. In the production phase, a web application is provided through which an end user can submit reviews to analyze aspect level sentiments. The system is built using state-of-the-art approaches of aspect term extraction, aspect category detection, and aspect polarity identification. To the best of our knowledge, there is no framework publicly available to build aspect-level sentiment analysis application. All the source code of the ABSA toolkit is available on GitHub.(a)","keywords_author":["Aspect level opinion mining","sentiment analysis","natural language processing"],"keywords_other":null,"max_cite":0.0,"pub_year":2017.0,"sources":"['wos']","rawkeys":["aspect level opinion mining","natural language processing","sentiment analysis"],"tags":["aspect level opinion mining","natural language processing","sentiment analysis"]},{"p_id":49906,"title":"Performance analysis of recent Word Sense Disambiguation techniques","abstract":"\u00a9 2015 IEEE. This paper presents recent advances in the in the area of Word Sense Disambiguation (WSD). While the supervised machine learning techniques have proven to be most efficient with the problem of availability of sense tagged data. While describing a few important techniques the paper then represents a comparative analysis among them. There is very less commonality among the data sets which have been used but it has been found out that the Genetic Algorithm based approach has the capability to beat other milestone techniques in the literature.","keywords_author":["Conceptual Density","Machine Learning","Natural Language Processing","WSD"],"keywords_other":["Conceptual density","Tagged data","Performance analysis","Word sense disambiguation techniques","Word-sense disambiguation","Supervised machine learning","Comparative analysis","NAtural language processing"],"max_cite":0.0,"pub_year":2016.0,"sources":"['scp']","rawkeys":["performance analysis","natural language processing","machine learning","word sense disambiguation techniques","comparative analysis","conceptual density","word-sense disambiguation","tagged data","supervised machine learning","wsd"],"tags":["performance analysis","natural language processing","machine learning","word sense disambiguation techniques","comparative analysis","conceptual density","word sense disambiguation","tagged data","supervised machine learning"]},{"p_id":19187,"title":"A mixed method lemmatization algorithm using a Hierarchy of Linguistic Identities (HOLI)","abstract":"We present a new mixed method lemmatizer for Icelandic, Lemmald, which achieves good performance by relying on IceTagger [1] for tagging and The Icelandic Frequency Dictionary [2] corpus for training. We combine the advantages of data-driven machine learning with linguistic insights to maximize performance. To achieve this, we make use of a novel approach: Hierarchy of Linguistic Identities (HOLI), which involves organizing features and feature structures for the machine learning based on linguistic knowledge. Accuracy of the lemmatization is further improved using an add-on which connects to the Database of Modern Icelandic Inflections [3]. Given correct tagging, our system lemmatizes Icelandic text with an accuracy of 99.55%. We believe our method can be fruitfully adapted to other morphologically rich languages. \u00a9 2008 Springer-Verlag Berlin Heidelberg.","keywords_author":["BLARK","Icelandic","IceTagger","Lemma","Lemmald","Lemmatization","Machine learning","Normalization"],"keywords_other":["IceTagger","BLARK","Data-driven","Lemma","Machine learning","International conferences","Linguistic knowledge","Icelandic","Mixed Methods","Lemmald","Lemmatization","NAtural language processing","Normalization"],"max_cite":21.0,"pub_year":2008.0,"sources":"['scp']","rawkeys":["lemmatization","mixed methods","icetagger","international conferences","lemmald","machine learning","data-driven","natural language processing","normalization","lemma","blark","icelandic","linguistic knowledge"],"tags":["lemmatization","mixed methods","icetagger","international conferences","data driven","lemmald","machine learning","natural language processing","normalization","lemma","blark","icelandic","linguistic knowledge"]},{"p_id":33523,"title":"Fine-grained financial news sentiment analysis","abstract":"\u00a9 2017 IEEE.The 24-hour news cycle and barrage of online media is a constant drum beat. The flow of positive and negative news is always in flux, influencing our current perspective and reassessing our future outlook. Nowhere is this more true than in the capital markets where assets are priced and risk assessed based on future expectations. While many factors influence a trader's decision to buy or sell an asset it can be argued that the sentiment from the 24-hour news cycle greatly impacts their outlook on the future value of an asset. In this paper we propose new methods to predict the positive or negative sentiment of financial news. Our analysis has found that contemporary document level sentiment analysis methods break down at fine-grained levels. Fine-grained analysis methods are vitally important as the velocity and impact of small texts, such as tweets and news flashes, increase their influence over the decision process. Using Natural Language Processing methods we extract syntactic sentence patterns from financial news headlines. From these patterns we conduct experiments using both lexicon and machine learning sentiment analysis approaches to predict sentiment. We find that our sentiment prediction methods are able to consistently out perform lexicon methods. Our robust techniques give the financial practitioner a method to fold a fine-grained news sentiment factor into their pricing or risk prediction models.","keywords_author":["Financial analytics","Machine learning","Natural language processing","Sentiment analysis","Text analysis"],"keywords_other":["Risk prediction models","Text analysis","Sentiment analysis","Fine-grained analysis","Financial practitioners","Negative sentiments","Financial analytics","Contemporary documents"],"max_cite":2.0,"pub_year":2017.0,"sources":"['scp']","rawkeys":["financial practitioners","fine-grained analysis","machine learning","contemporary documents","natural language processing","negative sentiments","text analysis","risk prediction models","financial analytics","sentiment analysis"],"tags":["financial practitioners","fine-grained analysis","machine learning","contemporary documents","natural language processing","negative sentiments","text analysis","risk prediction models","financial analytics","sentiment analysis"]},{"p_id":117491,"title":"A Study on Text-Score Disagreement in Online Reviews","abstract":"In this paper, we focus on online reviews and employ artificial intelligence tools, taken from the cognitive computing field, to help understand the relationships between the textual part of the review and the assigned numerical score. We move from the intuitions that (1) a set of textual reviews expressing different sentiments may feature the same score (and vice-versa), and (2) detecting and analyzing the mismatches between the review content and the actual score may benefit both service providers and consumers, by highlighting specific factors of satisfaction (and dissatisfaction) in texts. To prove the intuitions, we adopt sentiment analysis techniques and we concentrate on hotel reviews, to find polarity mismatches therein. In particular, we first train a text classifier with a set of annotated hotel reviews, taken from the Booking website. Then, we analyze a large dataset, with around 160k hotel reviews collected from TripAdvisor, with the aim of detecting a polarity mismatch, indicating if the textual content of the review is in line, or not, with the associated score. Using well-established artificial intelligence techniques and analyzing in depth the reviews featuring a mismatch between the text polarity and the score, we find that-on a scale of five stars-those reviews ranked with middle scores include a mixture of positive and negative aspects. The approach proposed here, beside acting as a polarity detector, provides an effective selection of reviews-on an initial very large dataset-that may allow both consumers and providers to focus directly on the review subset featuring a text\/score disagreement,which conveniently convey to the user a summary of positive and negative features of the review target.","keywords_author":["Online reviews","Natural language processing","Artificial intelligence","Data mining","Social science methods or tools","Polarity detection"],"keywords_other":["POLARITY","NETWORK","SENTIMENT ANALYSIS"],"max_cite":0.0,"pub_year":2017.0,"sources":"['wos']","rawkeys":["polarity detection","artificial intelligence","network","data mining","online reviews","natural language processing","social science methods or tools","sentiment analysis","polarity"],"tags":["polarity detection","data mining","online reviews","natural language processing","machine learning","networks","social science methods or tools","sentiment analysis","polarity"]},{"p_id":21238,"title":"Knowledge acquisition of predicate argument structures from technical texts using machine learning: The system ASIUM","abstract":"\u00a9 Springer-Verlag Berlin Heidelberg 1999. In this paper, we describe the Machine Learning system, ASIUM1, which learns Subcaterorization Frames of verbs and ontologies from the syntactic parsing of technical texts in natural language. The restrictions of selection in the subcategorization frames are filled by the ontology\u2019s concepts. Applications requiring such knowledge are crucial and numerous. The most direct applications are semantic control of texts and syntactic parsing disambiguation. This knowledge acquisition task cannot be fully automatically performed. Instead, we propose a cooperative ML method which provides the user with a global view of the acquisition task and also with acquisition tools like automatic concepts splitting, example generation, and an ontology view with attachments to the verbs. Validation steps using these features are intertwined with learning steps so that the user validates the concepts as they are learned. Experiments performed on two different corpora (cooking domain and patents) give very promising results.","keywords_author":["Clustering","Corpus-based learning","Machine learning","Natural language processing","Ontology","Predicate argument structure"],"keywords_other":["Subcategorization","Corpus-based","Clustering","Syntactic parsing","Argument structures","Natural languages","NAtural language processing","Acquisition tools"],"max_cite":15.0,"pub_year":1999.0,"sources":"['scp']","rawkeys":["predicate argument structure","corpus-based learning","argument structures","subcategorization","syntactic parsing","natural languages","machine learning","natural language processing","ontology","clustering","acquisition tools","corpus-based"],"tags":["corpus-based learning","argument structures","subcategorization","syntactic parsing","natural languages","machine learning","natural language processing","clustering","acquisition tools","predicate-argument structure","corpus-based"]},{"p_id":47860,"title":"A review: Information extraction techniques from research papers","abstract":"\u00a9 2017 IEEE. Text extraction is a crucial stage of analyzing Journal papers. Journal papers generally are in PDF format which is semi structured data. Journal papers are presented into different sections like Introduction, Methodology, Experimental setup, Result and analysis etc. so that it is easy to access information from any section as per the reader's interest. The main importance on section extraction is to find a representative subset of the data, which contains the information of the entire set. Various approaches to extract sections from research papers include stastical methods, NLP, Machine Learning etc. In this paper we present review of various extraction techniques from a PDF document.","keywords_author":["Information extraction","Machine Learning","NLP","PDF"],"keywords_other":["Extraction techniques","Text extraction","PDF document","PDF format","Research papers","Semi structured data","Journal paper","Information extraction techniques"],"max_cite":0.0,"pub_year":2017.0,"sources":"['scp']","rawkeys":["nlp","information extraction","pdf document","extraction techniques","pdf format","machine learning","pdf","information extraction techniques","research papers","journal paper","text extraction","semi structured data"],"tags":["information extraction","pdf document","extraction techniques","pdf format","machine learning","natural language processing","information extraction techniques","research papers","probability density function","journal paper","text extraction","semi structured data"]},{"p_id":37625,"title":"Gender identification in Russian texts","abstract":"Gender Identification is a task where we have to identify the gender of the author for written texts. An hybrid approach has been designed by combining deep neural network and a rule-based classifier for Russian texts. LSTM and Bi-LSTM have been used as a part of Neural Network due to their capability to learn long-term dependencies.","keywords_author":["Author profillation","Deep learning","Gender identification","NLP","Rule-based classification"],"keywords_other":["Gender identification","Written texts","Rule-based classification","Hybrid approach","Long-term dependencies","Rule-based classifier","Author profillation"],"max_cite":1.0,"pub_year":2017.0,"sources":"['scp']","rawkeys":["nlp","deep learning","gender identification","author profillation","hybrid approach","rule-based classification","rule-based classifier","written texts","long-term dependencies"],"tags":["machine learning","gender identification","natural language processing","hybrid approach","rule-based classification","author profiling","rule-based classifier","written texts","long-term dependencies"]},{"p_id":51962,"title":"Identifying temporal relations between main events in new articles","abstract":"With the expansion of the Web 2.0, daily huge amount of data is produced everywhere, namely new articles. These contents need to be exploited in order to extract relevant information and to build knowledge databases. In this concern, processing the temporal dimension of language and extracting temporal information from electronic news articles is becoming a prominent task. In this concern, we propose an approach for identifying inter-sentential temporal relations between main events from news articles. Our approach is based on a complete linguistic analysis of texts and supervised learning models. \u00a9 2013 IEEE.","keywords_author":["Classification","Linguistic Analysis","Machine Learning","Natural Language Processing","Temporal Information Extraction","Temporal Relation Identification","Web 2.0"],"keywords_other":["Linguistic analysis","Web 2.0","Temporal information extraction","Temporal relation","NAtural language processing"],"max_cite":0.0,"pub_year":2013.0,"sources":"['scp']","rawkeys":["web 2.0","machine learning","natural language processing","temporal relation identification","temporal relation","classification","linguistic analysis","temporal information extraction"],"tags":["web 2.0","machine learning","natural language processing","temporal relation identification","temporal relation","classification","linguistic analysis","temporal information extraction"]},{"p_id":39680,"title":"Representing text documents in training document spaces: A novel model for document representation","abstract":"In this paper, we propose a novel model for Document Representation in an attempt to address the problem of huge dimensionality and vector sparseness that are commonly faced in Text Classification tasks. The proposed model consists of representing text documents in the space of training documents at a first stage. Afterward, the generated vectors are projected in a new space where the number of dimensions corresponds to the number of categories. To evaluate the effectiveness of our model, we focus on a problem of binary classification. We conduct our experiments on Arabic and English data sets of Opinion Mining. We use as classifiers Support Vector Machines (SVM) and k-Nearest Neighbors (k-NN) which are known by their effectiveness in classical Text Classification tasks. We compare the performance of our model with that of the classical Vector Space Model (VSM) by the consideration of three evaluative criteria, namely dimensionality of the generated vectors, time (of learning and testing) taken by the classifiers, and classification results in terms of accuracy. Our experiments show that the effectiveness of our model (in comparison with the classical VSM) depends on the used classifier. Results yielded by k-NN when applying our model are better or as those obtained when applying the classical VSM. For SVM, results yielded when applying our model are in general, slightly lower than those obtained when using VSM. However, the gain in terms of time and dimensionality reduction is so promising since they are dramatically decreased by the application of our model. \u00a9 2005 - 2013 JATIT & LLS. All rights reserved.","keywords_author":["Document representation","Machine learning","Natural language processing","Opinion mining","Text classification"],"keywords_other":null,"max_cite":1.0,"pub_year":2013.0,"sources":"['scp']","rawkeys":["natural language processing","machine learning","document representation","opinion mining","text classification"],"tags":["natural language processing","machine learning","document representation","opinion mining","text classification"]},{"p_id":768,"title":"Predicting Emotion in Movie Scripts Using Deep Learning","abstract":"In this paper we present a machine learning approach that aims to predict an emotion that follows given text segments. For this, we pre-processed a collection of movie scripts to generate a sequence of sentiment values. The sequence is given to a deep learning network to learn the sentiment value to follow. The evaluation result was promising, showing high cosine similarities between the target and the predicted emotion values.","keywords_author":["deep learning","machine learning","movie ratings","text mining","machine learning","deep learning","movie ratings","text mining"],"keywords_other":["Text mining","Machine learning approaches","sentiment value","text segments","Data mining","Testing","word processing","Machine learning","learning (artificial intelligence)","Training","Evaluation results","Learning network","deep learning network","Cosine similarity","Text segments","movie scripts","Movie ratings","Recurrent neural networks","natural language processing","predicted emotion values","text analysis","Motion pictures"],"max_cite":0.0,"pub_year":2018.0,"sources":"['scp', 'ieee']","rawkeys":["learning network","movie ratings","recurrent neural networks","sentiment value","evaluation results","text segments","text mining","machine learning","word processing","motion pictures","data mining","learning (artificial intelligence)","deep learning","deep learning network","training","movie scripts","machine learning approaches","testing","cosine similarity","natural language processing","predicted emotion values","text analysis"],"tags":["learning network","movie ratings","sentiment value","evaluation results","text mining","machine learning","word processing","motion pictures","data mining","neural networks","text segmentation","deep learning network","training","movie scripts","machine learning approaches","testing","cosine similarity","natural language processing","predicted emotion values","text analysis"]},{"p_id":51978,"title":"The applications of support vector machine in Natural Language Processing","abstract":"This article provides a brief introduction to Natural Language Processing and basic knowledge of Machine Learning and Support Vector Machine at first, and then, gives a more detailed introduction about how to use SVM models in several major directions about NLP, and at the end, a brief summary about the application of SVM in Natural Language Processing is given. \u00a9 (2013) Trans Tech Publications, Switzerland.","keywords_author":["Machine Learning(ML)","Natural Language Processing (NLP)","Support Vector Machine (SVM)"],"keywords_other":["SVM model","NAtural language processing"],"max_cite":0.0,"pub_year":2013.0,"sources":"['scp']","rawkeys":["svm model","natural language processing","natural language processing (nlp)","support vector machine (svm)","machine learning(ml)"],"tags":["natural language processing","svm model","machine learning"]},{"p_id":33547,"title":"Application of machine learning techniques to sentiment analysis","abstract":"\u00a9 2016 IEEE. Today, we live in a 'data age'. Due to rapid increase in the amount of user-generated data on social media platforms like Twitter, several opportunities and new open doors have been prompted for organizations that endeavour hard to keep a track on customer reviews and opinions about their products. Twitter is a huge fast emergent micro-blogging social networking platform for users to express their views about politics, products sports etc. These views are useful for businesses, government and individuals. Hence, tweets can be used as a valuable source for mining public's opinion. Sentiment analysis is a process of automatically identifying whether a user-generated text expresses positive, negative or neutral opinion about an entity (i.e. product, people, topic, event etc). The objective of this paper is to give step-by-step detail about the process of sentiment analysis on twitter data using machine learning. This paper also provides details of proposed approach for sentiment analysis. This work proposes a Text analysis framework for twitter data using Apache spark and hence is more flexible, fast and scalable. Na\u00efve Bayes and Decision trees machine learning algorithms are used for sentiment analysis in the proposed framework.","keywords_author":["machine learning","Natural Language Processing","Sentiment analysis","twitter"],"keywords_other":["Micro blogging","Text analysis","Sentiment analysis","Social media platforms","Customer review","User-generated","Machine learning techniques","twitter"],"max_cite":2.0,"pub_year":2017.0,"sources":"['scp']","rawkeys":["micro blogging","user-generated","social media platforms","machine learning techniques","natural language processing","machine learning","customer review","text analysis","sentiment analysis","twitter"],"tags":["user-generated","social media platforms","machine learning techniques","natural language processing","machine learning","customer review","text analysis","microblogging","sentiment analysis","twitter"]},{"p_id":29453,"title":"Ensembling Classifiers for Detecting User Intentions behind Web Queries","abstract":"\u00a9 1997-2012 IEEE. Discovering user intentions behind Web search queries is key to improving user experience. Usually, this task is seen as a classification problem, in which a sample of annotated user query intentions are provided to a supervised machine learning algorithm or classifier that learns from these examples and then can classify unseen user queries. This article proposes a new approach based on an ensemble of classifiers. The method combines syntactic and semantic features so as to effectively detect user intentions. Different setting experiments show the promise of this linguistically motivated ensembling approach, by reducing the ranking variance of single classifiers across user intentions.","keywords_author":["ensemble learning","Internet\/Web technologies","machine learning","natural language processing","Web search"],"keywords_other":["Web searches","Web search queries","Ensemble learning","User experience","Ensemble of classifiers","Semantic features","Supervised machine learning","NAtural language processing"],"max_cite":4.0,"pub_year":2016.0,"sources":"['scp']","rawkeys":["internet\/web technologies","natural language processing","machine learning","web search queries","semantic features","web searches","user experience","ensemble learning","supervised machine learning","web search","ensemble of classifiers"],"tags":["internet\/web technologies","natural language processing","machine learning","web search queries","semantic features","web searches","user experience","ensemble learning","supervised machine learning","ensemble of classifiers"]},{"p_id":783,"title":"Deep Learning for Human Activity Recognition in Mobile Computing","abstract":"By leveraging advances in deep learning, challenging pattern recognition problems have been solved in computer vision, speech recognition, natural language processing, and more. Mobile computing has also adopted these powerful modeling approaches, delivering astonishing success in the field's core application domains, including the ongoing transformation of human activity recognition technology through machine learning.","keywords_author":["artificial intelligence","complexity","deep learning","embedded systems","HAR","human activity recognition","intelligent systems","machine learning","mobile","mobile and embedded deep learning","modeling","pattern recognition","deep learning","machine learning","pattern recognition","intelligent systems","modeling","complexity","human activity recognition","HAR","artificial intelligence","mobile","embedded systems","mobile and embedded deep learning"],"keywords_other":["Mobile computing","complexity","speech recognition","Human activity recognition","Computational modeling","Data mining","Feature extraction","human activity recognition technology","mobile","machine learning","mobile computing","Machine learning","learning (artificial intelligence)","deep learning","pattern recognition problems","computer vision","Analytical models","Data models","natural language processing","Pattern recognition problems","Model approach","image recognition"],"max_cite":0.0,"pub_year":2018.0,"sources":"['scp', 'wos', 'ieee']","rawkeys":["embedded systems","complexity","speech recognition","mobile and embedded deep learning","human activity recognition technology","machine learning","mobile","mobile computing","har","computational modeling","data mining","learning (artificial intelligence)","deep learning","model approach","human activity recognition","modeling","intelligent systems","pattern recognition problems","computer vision","artificial intelligence","natural language processing","pattern recognition","data models","feature extraction","image recognition","analytical models"],"tags":["embedded systems","complexity","speech recognition","mobile and embedded deep learning","human activity recognition technology","machine learning","mobile","mobile computing","computational modeling","data mining","human activity recognition","intelligent systems","pattern recognition problems","computer vision","model","modeling approach","natural language processing","pattern recognition","data models","feature extraction","image recognition","analytical models"]},{"p_id":43791,"title":"Web pages classification: An effective approach based on text mining techniques","abstract":"\u00a9 2017 IEEE. Some web pages on Internet contain important content that are useful in a long time period or even forever. On the other hand, there are some web pages that are valuable only in a short time period. It is difficult to classify these types of web pages automatically due to their contents. This is an important task for improving the performance of search engines and web page recommender engines. In this project, webpages were classified into two categories with machine learning algorithms. For this purpose, natural language processing and text mining techniques were used for text pre-processing. Then appropriate information was extracted from texts and eventually web pages were classified by using machine learning algorithms. Compared to other approaches, most of the focus in this project is on text pre-processing stage and new strategies were presented to fill the gap. The results indicate that the proposed approach had better performance than other approaches.","keywords_author":["classification","data mining","machine learning","natural language processing","text mining"],"keywords_other":["Text mining","Pre-processing","Time-periods","Pre-processing stages","Effective approaches","Web pages classifications","Text mining techniques"],"max_cite":0.0,"pub_year":2018.0,"sources":"['scp']","rawkeys":["pre-processing","data mining","text mining","effective approaches","natural language processing","machine learning","web pages classifications","pre-processing stages","classification","time-periods","text mining techniques"],"tags":["pre-processing","data mining","text mining","effective approaches","natural language processing","machine learning","pre-processing stages","classification","time-periods","text mining techniques","web page classification"]},{"p_id":29458,"title":"Sentiment analysis of a document using deep learning approach and decision trees","abstract":"\u00a9 2015 IEEE. The given paper describes modern approach to the task of sentiment analysis of movie reviews by using deep learning recurrent neural networks and decision trees. These methods are based on statistical models, which are in a nutshell of machine learning algorithms. The fertile area of research is the application of Google's algorithm Word2Vec presented by Tomas Mikolov, Kai Chen, Greg Corrado and Jeffrey Dean in 2013. The main idea of Word2Vec is the representations of words with the help of vectors in such manner that semantic relationships between words preserved as basic linear algebra operations. The extra advantage of the mentioned algorithm above the alternatives is computational efficiency. This paper focuses on using Word2Vec model for text classification by their sentiment type.","keywords_author":["deep learning","machine learning","NLP","sentiment analysis","text classification"],"keywords_other":["Deep learning","Text classification","Semantic relationships","Linear algebra operations","Sentiment analysis","Movie reviews"],"max_cite":4.0,"pub_year":2016.0,"sources":"['scp']","rawkeys":["text classification","nlp","deep learning","linear algebra operations","movie reviews","machine learning","semantic relationships","sentiment analysis"],"tags":["text classification","movie reviews","linear algebra operations","natural language processing","machine learning","semantic relationships","sentiment analysis"]},{"p_id":43794,"title":"Vector representation of words for sentiment analysis using GloVe","abstract":"\u00a9 2017 IEEE. Sentiment Analysis is one of the application of Natural Language Processing(NLP) methodology. The NLP enables to understand the common day to day language of the people. This understanding is extended to decipher the sentiments of the users and hence interpret the liking and disliking of the people. Vector representation of the words using unsupervised technique like Glove proves to be very effective in interpreting the meaning and hence the sentiments.","keywords_author":["Glove","Machine learning","Natural Language Processing","Sentiment Analysis","vector representation"],"keywords_other":["Vector representations","Unsupervised techniques","Glove"],"max_cite":0.0,"pub_year":2018.0,"sources":"['scp']","rawkeys":["glove","natural language processing","machine learning","vector representation","vector representations","sentiment analysis","unsupervised techniques"],"tags":["glove","natural language processing","machine learning","vector representations","sentiment analysis","unsupervised techniques"]},{"p_id":6934,"title":"Relation classification via convolutional deep neural network","abstract":"The state-of-the-art methods used for relation classification are primarily based on statistical machine learning, and their performance strongly depends on the quality of the extracted features. The extracted features are often derived from the output of pre-existing natural language processing (NLP) systems, which leads to the propagation of the errors in the existing tools and hinders the performance of these systems. In this paper, we exploit a convolutional deep neural network (DNN) to extract lexical and sentence level features. Our method takes all of the word tokens as input without complicated pre-processing. First, the word tokens are transformed to vectors by looking up word embeddings1. Then, lexical level features are extracted according to the given nouns. Meanwhile, sentence level features are learned using a convolutional approach. These two level features are concatenated to form the final extracted feature vector. Finally, the features are fed into a softmax classifier to predict the relationship between two marked nouns. The experimental results demonstrate that our approach significantly outperforms the state-of-the-art methods.","keywords_author":null,"keywords_other":["Pre-processing","State-of-the-art methods","Sentence level","Statistical machine learning","Feature vectors","Relation classifications","Deep neural networks","NAtural language processing"],"max_cite":182.0,"pub_year":2014.0,"sources":"['ieee', 'scp', 'wos']","rawkeys":["state-of-the-art methods","pre-processing","deep neural networks","statistical machine learning","natural language processing","feature vectors","relation classifications","sentence level"],"tags":["state-of-the-art methods","pre-processing","statistical machine learning","natural language processing","feature vectors","convolutional neural network","relation classifications","sentence level"]},{"p_id":39702,"title":"Creating an artificially intelligent director (AID) for theatre and virtual environments","abstract":null,"keywords_author":["Agent Planning","Agent Reasoning","Artificial Intelligence","BML Realizer","Director","Force-Directed Graphs","Hamlet","Machine Learning","Natural Language Processing","Rules Engine","Shakespeare","Spatial Reasoning","Theatre"],"keywords_other":null,"max_cite":1.0,"pub_year":2013.0,"sources":"['scp']","rawkeys":["artificial intelligence","agent reasoning","hamlet","agent planning","rules engine","shakespeare","machine learning","natural language processing","theatre","director","force-directed graphs","spatial reasoning","bml realizer"],"tags":["agent reasoning","hamlet","agent planning","rules engine","shakespeare","machine learning","natural language processing","theatre","director","force-directed graphs","spatial reasoning","bml realizer"]},{"p_id":45846,"title":"An overview of a distributional word representation for an arabic named entity recognition system","abstract":"\u00a9 Springer International Publishing AG, part of Springer Nature 2018. This study attempts to describe and discuss the different approaches and methods dedicated to Named Entity Recognition (NER) systems in various languages, in order to justify the choice of a distributional approach for an Arabic NER system using deep learning methods and a Neural Network word representation (Embeddings) as an add-in feature in the unsupervised learning process.","keywords_author":["Arabic NER","CharWNN model","Deep learning network","Embeddings","Machine learning","NLP"],"keywords_other":["Named entity recognition","Learning methods","Learning network","Word representations","NER system","Arabic NER","Embeddings"],"max_cite":0.0,"pub_year":2018.0,"sources":"['scp']","rawkeys":["embeddings","charwnn model","nlp","learning network","arabic ner","learning methods","deep learning network","machine learning","named entity recognition","ner system","word representations"],"tags":["embeddings","charwnn model","learning network","arabic ner","learning methods","deep learning network","machine learning","named entity recognition","ner system","natural language processing","word representations"]},{"p_id":45849,"title":"Feature based opinion mining and sentiment analysis using fuzzy logic","abstract":"\u00a9 The Author(s) 2018. This paper discusses a new model towards opinion mining and sentiment analysis of the text reviews posted in social media sites which are mostly in unstructured format. In recent years, web forums and social media has become an excellent platform to express or share opinions in the form of text about any product or any interested topic. These opinions are used for making decisions to choose a product or any entity. Opinion mining and sentiment analysis are related in a sense that opining mining deals with analyzing and summarizing expressed opinions whereas sentiment analysis classifies opinionated text into positive and negative. Feature extraction is a crucial problem in sentiment analysis. Model proposed in the paper utilizes machine learning techniques and fuzzy approach for opinion mining and classification of sentiment on textual reviews. The goal is to automate the process of mining attitudes, opinions and hidden emotions from text.","keywords_author":["Machine learning","Natural language processing","Opinion mining","Sentiment analysis"],"keywords_other":["Fuzzy approach","Feature-based","Sentiment analysis","Social media","Web Forums","Making decision","Machine learning techniques","Opinion mining"],"max_cite":0.0,"pub_year":2018.0,"sources":"['scp']","rawkeys":["making decision","social media","machine learning techniques","machine learning","natural language processing","web forums","feature-based","fuzzy approach","opinion mining","sentiment analysis"],"tags":["making decision","social media","machine learning techniques","machine learning","natural language processing","web forums","feature-based","fuzzy approach","opinion mining","sentiment analysis"]},{"p_id":49950,"title":"Text chunker for Malayalam using Memory-Based Learning","abstract":"\u00a9 2015 IEEE.Text chunking consists of dividing a text into syntactically correlated parts of words. Given the words and their morphosyntactic class, a chunker will decide which words can be grouped as chunks. Malayalam is a free word order language and has relatively unrestricted phrase structures that make the problem of chunking quite challenging. This paper aims to develop a text chunker for Malayalam using Memory-Based Learning (MBL) approach. Memory-Based Learning is a machine learning methodology based on the idea that the direct reuse of examples using analogical reasoning is more suited for solving language processing problems than the application of rules extracted from those examples. The chunker was trained using the tool Memory-Based Tagger (MBT) with words and their POS tags as features. The chunker demonstrated an accuracy of 97.14%.","keywords_author":["Machine Learning","Malayalam Chunking","Memory Based Learning","Memory Based Natural Language Processing","Natural Language Processing","POS Tagging","Shallow parsing"],"keywords_other":["NAtural language processing","Memory-based learning","Malayalams","PoS tagging","Shallow parsing"],"max_cite":0.0,"pub_year":2016.0,"sources":"['scp']","rawkeys":["memory based natural language processing","malayalams","machine learning","natural language processing","memory-based learning","memory based learning","shallow parsing","pos tagging","malayalam chunking"],"tags":["memory based natural language processing","malayalams","machine learning","natural language processing","memory-based learning","shallow parsing","pos tagging","malayalam chunking"]},{"p_id":39712,"title":"Processing medical reports to automatically populate ontologies","abstract":"Medical reports are, quite often, written and stored in computer systems in a non-structured free text form. As a consequence, the information contained in these reports is not easily available and it is not possible to take it into account by medical decision support systems. We propose a methodology to automatically process and analyze medical reports, identifying concepts and their instances, and populating a new ontology. This methodology is based in natural language processing techniques using linguistic and statistical information. The proposed system was applied successfully to a set of medical reports from the Veterinary Hospital of the University of \u00c9vora. \u00a9 2013 ITCH 2013 Steering Committee and IOS Press. All rights reserved.","keywords_author":["Information Extraction","Machine Learning","Natural Language Processing","Ontologies","Semantic Web"],"keywords_other":null,"max_cite":1.0,"pub_year":2013.0,"sources":"['scp']","rawkeys":["information extraction","natural language processing","machine learning","ontologies","semantic web"],"tags":["information extraction","semantic web","natural language processing","machine learning"]},{"p_id":21293,"title":"An experimental system for measuring the credibility of news content in Twitter","abstract":"Purpose: Owing to the large amount of information available on Twitter (a micro-blogging service) that is not necessarily true or believable, credibility of news published in such an electronic channel has becomean important area for investigation in the field of web credibility. This paper aimstoaddress this issue. Design\/methodology\/approach: A system was developed to measure the credibility of news content published in Twitter. The system uses two approaches toassign credibility levels (low, high and average) to each tweet. The first approach is based on the similarity between Twitter posts (tweets) and authentic (i.e. verified) news sources. The second approach is based on the similarity with verified news sources in addition to a set of proposed features. Findings: The evaluations of the two approaches showed that assigning credibility levels to Twitter tweets for the first approach has a higher precision and recall. Additional experiments showed that the linking feature has its impact on the second approach results. Research limitations\/implications: The proposed system is experimental; thus further experiments are needed to prove these findings. Originality\/value: This paper contributes to the research on web credibility. It is believed to be the first which provides a proposed system to evaluate the credibility of Twitter news content automatically. \u00a9 Emerald Group Publishing Limited.","keywords_author":["Arabic language","Blogs","Communication","Credibility","Information media","Natural language processing","Trust","Twitter","Web content"],"keywords_other":["Credibility","Twitter","Trust","Web content","Arabic languages","Information media","NAtural language processing"],"max_cite":15.0,"pub_year":2011.0,"sources":"['scp']","rawkeys":["web content","arabic language","natural language processing","information media","blogs","trust","communication","credibility","arabic languages","twitter"],"tags":["web content","credibility","natural language processing","trust","blogs","communication","information media","arabic languages","twitter"]},{"p_id":49966,"title":"A framework for detecting external plagiarism from monolingual documents: Use of shallow NLP and n-gram frequency comparison","abstract":"\u00a9 2016 ACM. The internet has increased the copy-paste scenarios amongst students as well as amongst researchers leading to different levels of plagiarized documents. For this reason, much of research is focused on for detecting plagiarism automatically. In this paper, an initiative is discussed where Natural Language Processing (NLP) techniques, as well as supervised machine learning algorithms have been combined to detect plagiarized texts. Here, the major emphasis is on to construct a framework which detects external plagiarism from monolingual texts successfully. For successfully detecting the plagiarism, n-gram frequency comparison approach has been implemented to construct the model framework. The framework is based on 120 characteristics which have been extracted during pre-processing the documents using NLP approach. Afterwards, filter metrics has been applied to select most relevant characteristics and then supervised classification learning algorithm has been used to classify the documents in four levels of plagiarism. Confusion matrix was built to estimate the false positives and false negatives. Our plagiarism framework achieved a very high accuracy score of 89% with low false positive and false negative rate.","keywords_author":["Lexical matching","Shallow NLP","Supervised Machine Learning Algorithm","Word n-gram"],"keywords_other":["Word n-grams","Supervised classification","Shallow NLP","Lexical matching","Frequency comparison","False positive and false negatives","Supervised machine learning","NAtural language processing"],"max_cite":0.0,"pub_year":2016.0,"sources":"['scp']","rawkeys":["supervised classification","natural language processing","supervised machine learning algorithm","word n-grams","lexical matching","word n-gram","shallow nlp","false positive and false negatives","frequency comparison","supervised machine learning"],"tags":["supervised classification","natural language processing","supervised machine learning algorithm","word n-grams","lexical matching","shallow nlp","false positive and false negatives","frequency comparison","supervised machine learning"]},{"p_id":41776,"title":"A social-event based approach to sentiment analysis of identities and behaviors in text","abstract":"\u00a9 2016 Taylor & Francis. We describe a new methodology to infer sentiments held toward identities and behaviors from social events that we extract from a large corpus of newspaper text. Our approach draws on affect control theory, a mathematical model of how sentiment is encoded in social events and culturally shared views toward identities and behaviors. While most sentiment analysis approaches evaluate concepts on a single, evaluative dimension, our work extracts a three-dimensional sentiment \u201cprofile\u201d for each concept. We can also infer when multiple sentiment profiles for a concept are likely to exist. We provide a case study of a large newspaper corpus on the Arab Spring, which helps to validate our approach.","keywords_author":["Affect control theory","Arab Spring","Bayesian inference","Machine learning","Natural language processing","Sentiment analysis"],"keywords_other":null,"max_cite":1.0,"pub_year":2016.0,"sources":"['scp']","rawkeys":["bayesian inference","natural language processing","machine learning","affect control theory","sentiment analysis","arab spring"],"tags":["bayesian inference","natural language processing","machine learning","affect control theory","sentiment analysis","arab spring"]},{"p_id":37681,"title":"Syntactic parsing and supervised analysis of Sindhi text","abstract":"\u00a9 2017 The Authors. This research study addresses the morphological and syntactic problems of Sindhi language text by proposing an Algorithm for tokenization and syntactic parsing. A Sindhi parser is developed on basis of proposed algorithm to perform syntactic parsing on Sindhi text using Sindhi WordNet (SWN) and corpus. Results of Sindhi syntactic parsing are accumulated to develop multi-class and multi-feature based Sindhi dataset in CSV format. Three attributes of Sindhi dataset are labelled as class. All three classes are comprised with different number of categories. SVM, Random forest and K-NN supervised machine learning methods are used and trained to analyze and evaluate the Sindhi dataset. 80% of dataset is used as training set and 20% of dataset is used as test set. In this research study, 10-fold cross validation technique is applied to evaluate and validate the supervised machine learning process. The SVM classifier gives better results on class phrase and UPOS whereas Random forest gives better result on class TagStatus. Precision, recall, f-measure and confusion matrix approve the performance of all supervised classifiers. The better performance of supervised machine learning methods, support the Sindhi dataset and Sindhi online parser for future research. This study opens new doors for research on right hand written languages especially Sindhi language to solve its computational linguistics problems.","keywords_author":["Machine learning","NLP","Sindhi parser","Sindhi WordNet","Supervised model","Tokenization"],"keywords_other":null,"max_cite":1.0,"pub_year":2017.0,"sources":"['scp']","rawkeys":["nlp","sindhi parser","tokenization","sindhi wordnet","machine learning","supervised model"],"tags":["sindhi parser","tokenization","sindhi wordnet","natural language processing","machine learning","supervised model"]},{"p_id":43824,"title":"Data summarization: a survey","abstract":"\u00a9 2018 Springer-Verlag London Ltd., part of Springer Nature Summarization has been proven to be a useful and effective technique supporting data analysis of large amounts of data. Knowledge discovery from data (KDD) is time consuming, and summarization is an important step to expedite KDD tasks by intelligently reducing the size of processed data. In this paper, different summarization techniques for structured and unstructured data are discussed. The key finding of this survey is that not all summarization techniques create a summary suitable for further analysis. It is highlighted that sampling techniques are a viable way of creating a summary for further knowledge discovery such as anomaly detection from summary. Also different summary evaluation metrics are discussed.","keywords_author":["Cyber security","Machine learning","Natural language processing","Semantics","Statistics","Structured data","Summarization","Unstructured data"],"keywords_other":null,"max_cite":0.0,"pub_year":2018.0,"sources":"['scp']","rawkeys":["statistics","semantics","natural language processing","machine learning","structured data","summarization","unstructured data","cyber security"],"tags":["statistics","semantics","natural language processing","machine learning","structured data","summarization","unstructured data","cyber security"]},{"p_id":33587,"title":"The chatbot feels you - A counseling service using emotional response generation","abstract":"\u00a9 2017 IEEE.Early study tries to use chatbot for counseling services. They changed drinking habit of who being consulted by leading them via intervene chatbot. However, the application did not concerned about psychiatric status through continuous conversation with user monitoring. Furthermore, they had no ethical judgment method that about the intervention of the chatbot. We argue that more reasonable and continuous emotion recognition will make better mental healthcare experiment. It will be more proper clinical psychiatric consolation in ethical view as well. This paper suggests a introduce a novel chatbot system for psychiatric counseling service. Our system understands content of conversation based on recent natural language processing (NLP) methods with emotion recognition. It senses emotional flow through the continuous observation of conversation. Also, we generate personalized counseling response from user input, to do this, we use additional constrains to generation model for the proper response generation which can detect conversational context, user emotion and expected reaction.","keywords_author":["conversational service","deep learning","response generation"],"keywords_other":["Judgment method","Emotional response","Continuous observation","Response generation","User emotions","Emotion recognition","Conversational services","NAtural language processing"],"max_cite":2.0,"pub_year":2017.0,"sources":"['scp']","rawkeys":["continuous observation","response generation","user emotions","deep learning","conversational services","natural language processing","emotional response","emotion recognition","conversational service","judgment method"],"tags":["continuous observation","response generation","conversational services","machine learning","natural language processing","emotional response","emotion recognition","user emotions","judgment method"]},{"p_id":47921,"title":"Challenges with sentiment analysis of on-line micro-texts","abstract":"With the evolution of World Wide Web (WWW) 2.0 and the emergence of many micro-blogging and social networking sites like Twitter, the internet has become a massive source of short textual messages called on-line micro-texts, which are limited to a few number of characters (e.g. 140 characters on Twitter). These on-line micro-texts are considered as real-time text streams. Online micro-texts are extremely subjective; they contain opinions about various events, social issues, personalities, and products. However, despite being so voluminous in quantity, the qualitative nature of these micro-texts is very inconsistent. These qualitative inconsistencies of raw on-line micro-texts impose many challenges in sentiment analysis of on-line micro-texts by using the established methods of sentiment analysis of unstructured reviews. This paper presents many challenges and issues observed during sentiment analysis of On-line Microtexts.","keywords_author":["Machine learning","Natural language processing","On-line micro-texts","Sentiment analysis","Text mining"],"keywords_other":null,"max_cite":0.0,"pub_year":2017.0,"sources":"['scp']","rawkeys":["text mining","natural language processing","machine learning","on-line micro-texts","sentiment analysis"],"tags":["text mining","natural language processing","machine learning","on-line micro-texts","sentiment analysis"]},{"p_id":49973,"title":"A generic platform to automate legal knowledge work process using machine learning","abstract":"\u00a9 2015 IEEE. Management of legal contracts in various business domains such as Real Estate are examples of typical business process outsourcing activity. One of such process is Lease Abstraction, where largely manual inspection and validation of large commercial lease documents made for real estate deals is done by offshore experts and relevant information from the documents is extracted into a structured form. This structured information is further used for aggregate analytics and decision making by large real estate firms. We propose a system based on machine learning techniques to semi automate this process, essentially leading to 50% human effort savings. Our approach weaves together state-of-the-art machine learning techniques like supervised classifier models, sequence modeling techniques and various semi-supervised approaches. We articulate the effectiveness of our solution using the results from the experiments. Our platform is being used in production environment by Accenture Operations and the initial results and user feedback are encouraging.","keywords_author":["Information extraction","Knowledge work automation","Machine learning","Natural language processing","Process automation","Supervised and semi supervised learning"],"keywords_other":["Semi- supervised learning","Process automation","Business process outsourcing","Production environments","Supervised classifiers","Knowledge work","NAtural language processing","Machine learning techniques"],"max_cite":0.0,"pub_year":2016.0,"sources":"['scp']","rawkeys":["knowledge work","semi- supervised learning","production environments","information extraction","business process outsourcing","machine learning techniques","machine learning","natural language processing","process automation","supervised classifiers","supervised and semi supervised learning","knowledge work automation"],"tags":["knowledge work","production environments","information extraction","business process outsourcing","machine learning techniques","machine learning","natural language processing","process automation","semi-supervised learning","supervised and semi supervised learning","supervised classifiers","knowledge work automation"]},{"p_id":39734,"title":"Extending sparse classification knowledge via NLP analysis of classification descriptions","abstract":"Supervised machine learning algorithms, particularly those operating on free text, depend upon the quality of their training datasets to correctly classify unlabeled text instances. In many cases where the classification task is nontrivial, it is difficult to obtain a large enough set of training data to achieve good classification accuracy. In this work we examine one such case in the context of a system designed to ground free text to an organizational hierarchy which is ontologically modeled. We explore the impact of utilizing information garnered from a highly customized Natural Language Processing (NLP) analysis of this ontology to augment a very sparse initial training dataset and compare this to a more labor intensive extraction of a small set of key words and phrases associated with each concept. We demonstrate an approach with significant improvement in classifier performance for concepts having little or no initial training data coverage.","keywords_author":["Automatic document classification","Hierarchical document classification","Machine learning","NLP","Ontology"],"keywords_other":["Classifier performance","NAtural language processing","Document Classification","NLP","Classification description","Hierarchical document","Supervised machine learning","Classification accuracy"],"max_cite":1.0,"pub_year":2012.0,"sources":"['scp']","rawkeys":["hierarchical document classification","nlp","classification accuracy","ontology","machine learning","natural language processing","classification description","document classification","classifier performance","hierarchical document","automatic document classification","supervised machine learning"],"tags":["hierarchical document classification","classification accuracy","machine learning","natural language processing","classification description","document classification","classifier performance","hierarchical document","automatic document classification","supervised machine learning"]},{"p_id":43829,"title":"Comparing detection and disclosure of traffic incidents in social networks: An intelligent approach based on Twitter vs. Waze Comparando la detecci\u00f3n y la divulgaci\u00f3n de incidentes de tr\u00e1nsito en redes sociales: Un enfoque inteligente basado en Twitter vs. Waze","abstract":"\u00a9 IBERAMIA and the authors. Nowadays, social networks have become in a communication medium widely used to disseminate any type of information. In particular, the shared information in social networks usually includes a considerable number of traffic incidents reports of specific cities. In light of this, specialized social networks have emerged for detecting and disseminating traffic incidents, differentiating from generic social networks in which a wide variety of topics are communicated. In this context, Twitter is a case in point of a generic social network in which its users often share information about traffic incidents, while Waze is a social network specialized in traffic. In this paper we present a comparative study between Waze and an intelligent approach that detects traffic incidents by analyzing publications shared in Twitter. The comparative study was carried out considering Ciudad Aut\u00f3noma de Buenos Aires (CABA), Argentina, as the region of interest. The results of this work suggest that both social networks should be considered as complementary sources of information. This conclusion is based on the fact that the proportion of mutual detections, i.e. traffic incidents detected by both approaches, was considerably low since it did not exceed 6% of the cases. Moreover, the results do not show that any of the approaches tend to anticipate in time to the other one in the detection of traffic incidents.","keywords_author":["Machine learning","Natural language processing","Traffic incidents","Twitter","Waze"],"keywords_other":["Twitter","Sources of informations","Shared information","Waze","Comparative studies","Region of interest","Communication medium","Traffic incidents"],"max_cite":0.0,"pub_year":2018.0,"sources":"['scp']","rawkeys":["sources of informations","region of interest","natural language processing","comparative studies","machine learning","traffic incidents","waze","communication medium","shared information","twitter"],"tags":["sources of informations","region of interest","sharing information","natural language processing","comparative studies","machine learning","traffic incidents","waze","communication medium","twitter"]},{"p_id":37692,"title":"How consumers perceive trustworthiness of providers in sharing economy: Effects of photos and comments on demand at airbnb","abstract":"\u00a9 2017 AIS\/ICIS Administrative Office. All Rights Reserved. There is limited systematic research that examines how the perceived trustworthiness of providers from photos and consumer comments influences consumer purchasing behavior in a sharing economy platform. In this paper, we develop a theoretical model that explores the mechanism through which consumers\u2019 visual-based trust and text-based trust in providers influence their purchasing behavior. We test our model using data from Airbnb.com\u2014a leading sharing economy platform for short-term rental. Using computer vision, machine learning and natural language processing algorithms we extract and code the variables in our model from photos of and consumer comments about hosts in Airbnb.com.","keywords_author":["Computer Vision","Machine Learning","Natural Language Processing","Sharing Economy","Trustworthiness"],"keywords_other":["Consumer purchasing behaviors","Trustworthiness","Purchasing behaviors","Sharing Economy","Systematic research","Natural languages","Theoretical modeling","On demands"],"max_cite":1.0,"pub_year":2017.0,"sources":"['scp']","rawkeys":["theoretical modeling","trustworthiness","sharing economy","systematic research","natural languages","natural language processing","machine learning","purchasing behaviors","consumer purchasing behaviors","on demands","computer vision"],"tags":["theoretical modeling","trustworthiness","sharing economy","systematic research","natural languages","natural language processing","machine learning","purchasing behaviors","consumer purchasing behaviors","on demands","computer vision"]},{"p_id":49980,"title":"Summary sentence classification using stylometry","abstract":"\u00a9 2015 IEEE.Summary sentence classification is an important step to generate document surrogates known as summary extracts. The quality of an extract depends much on the correctness of this step. We aim to classify potential summary sentences using a statistical learning method that models sentences according to a linguistic technique which examines writing styles, known as Stylometry. The sentences in documents are represented using a novel set of stylometric attributes. For learning, an innovative two-stage classification is set up that comprises two learners in subsequent steps: k-Nearest Neighbour and Naive Bayes. We train and test the learners with the newswire documents collected from two benchmark datasets, viz., the CAST and the DUC2002 datasets. Extensive experimentation strongly suggests that our method has outstanding performance for the single document summarization task. However, its performance is mixed for classifying summary sentences from multiple documents. Finally, comparisons show that our method performs significantly better than most of the popular extractive summarization methods.","keywords_author":["Classification","Data mining","Machine learning","Natural language processing","Stylometry","Summarization","Text mining"],"keywords_other":["Text mining","Single document summarization","Summarization","Sentence classifications","Extractive summarizations","Statistical learning methods","Stylometry","NAtural language processing"],"max_cite":0.0,"pub_year":2016.0,"sources":"['scp']","rawkeys":["data mining","statistical learning methods","text mining","natural language processing","machine learning","sentence classifications","summarization","classification","extractive summarizations","stylometry","single document summarization"],"tags":["data mining","statistical learning methods","text mining","natural language processing","machine learning","sentence classifications","summarization","classification","extractive summarizations","stylometry","single document summarization"]},{"p_id":11071,"title":"Deep Learning for Automated Extraction of Primary Sites From Cancer Pathology Reports","abstract":"Pathology reports are a primary source of information for cancer registries which process high volumes of free-text reports annually. Information extraction and coding is a manual, labor-intensive process. In this study, we investigated deep learning and a convolutional neural network (CNN), for extracting ICD-O-3 topographic codes from a corpus of breast and lung cancer pathology reports. We performed two experiments, using a CNN and a more conventional term frequency vector approach, to assess the effects of class prevalence and inter-class transfer learning. The experiments were based on a set of 942 pathology reports with human expert annotations as the gold standard. CNN performance was compared against a more conventional term frequency vector space approach. We observed that the deep learning models consistently outperformed the conventional approaches in the class prevalence experiment, resulting in micro-and macro-F score increases of up to 0.132 and 0.226, respectively, when class labels were well populated. Specifically, the best performing CNN achieved a micro-F score of 0.722 over 12 ICD-O-3 topography codes. Transfer learning provided a consistent but modest performance boost for the deep learning methods but trends were contingent on the CNN method and cancer site. These encouraging results demonstrate the potential of deep learning for automated abstraction of pathology reports.","keywords_author":["Convolutional neural network","deep learning","information extraction","natural language processing","pathology reports","primary cancer site"],"keywords_other":null,"max_cite":0.0,"pub_year":2018.0,"sources":"['wos']","rawkeys":["deep learning","information extraction","natural language processing","convolutional neural network","primary cancer site","pathology reports"],"tags":["information extraction","natural language processing","machine learning","convolutional neural network","primary cancer site","pathology reports"]},{"p_id":832,"title":"Text encoding for deep learning neural networks: A reversible base 64 (Tetrasexagesimal) Integer Transformation (RIT64) alternative to one hot encoding with applications to Arabic morphology","abstract":"One Hot Encoding (OHE) is currently the norm in text encoding for deep learning neural models. The main problem with OHE is that the size of the input vector, and hence the number of neurons in the input layer, depends on the size of the vocabulary. Experience has shown that the training time for text classification neural models grows exponentially with the size of the vocabulary when OHE is used. For example, if the size of the vocabulary is 10,000, then the size of the input vector will be model 10,000 implying 10,000 neurons in the input layer. This paper proposes and illustrates the use of an alternative Reversible Integer Transformation (RIT) whereby each word in the training\/testing set is transformed into base-64 integer format. The transformation is reversible, and the output of the network can easily be converted back to string format (without the need for an index). Another important feature is that each character in the word is represented using only six bits at the appropriate position in the resulting base-64 integer. The maximum number of neurons needed in the input layer is 64, but the actual number of neurons depends on the maximum word length in the vocabulary, and is usually below 64.","keywords_author":["Arabic morphology","deep learning","neura networks","reversible transformation","text encoding","neura networks","deep learning","text encoding","reversible transformation","Arabic morphology"],"keywords_other":["one hot encoding","Text categorization","Learning neural networks","Training time","text encoding","OHE","Encoding","Biological neural networks","Text encoding","text classification neural models","Text classification","reversible base 64 integer transformation alternative","Machine learning","deep learning neural models","input vector size","Vocabulary","training time","RIT","learning (artificial intelligence)","Training","Integer transformation","Reversible transformation","vocabulary size","Neural models","Important features","neural nets","pattern classification","resulting base-64 integer","Neurons","RIT64","Arabic morphology","deep learning neural networks","natural language processing","text analysis"],"max_cite":0.0,"pub_year":2018.0,"sources":"['scp', 'ieee']","rawkeys":["one hot encoding","vocabulary","text encoding","learning neural networks","encoding","integer transformation","text classification","neura networks","rit64","text classification neural models","machine learning","reversible base 64 integer transformation alternative","deep learning neural models","input vector size","training time","biological neural networks","neurons","learning (artificial intelligence)","deep learning","training","vocabulary size","neural models","important features","neural nets","pattern classification","resulting base-64 integer","ohe","arabic morphology","reversible transformation","deep learning neural networks","natural language processing","text analysis","rit","text categorization"],"tags":["one hot encoding","vocabulary","text encoding","learning neural networks","encoding","integer transformation","text classification","neura networks","rit64","text classification neural models","machine learning","reversible base 64 integer transformation alternative","deep learning neural models","input vector size","training time","biological neural networks","neurons","neural networks","training","vocabulary size","deep learning neural network","neural models","important features","pattern classification","resulting base-64 integer","ohe","arabic morphology","reversible transformation","natural language processing","text analysis","rit","text categorization"]},{"p_id":39745,"title":"Addressing the problem of unbalanced data sets in sentiment analysis","abstract":"Sentiment Analysis is a research area where the studies focus on processing and analysing the opinions available on the web. This paper deals with the problem of unbalanced data sets in supervised sentiment classification. We propose three different methods to under-sample the majority class documents, namely Remove Similar, Remove Farthest and Remove by Clustering. Our goal is to compare the effectiveness of the proposed methods with the common random under-sampling. We use for classification three standard classifiers: Na\u00efve Bayes, Support Vector Machines and k-Nearest Neighbours. The experiments are carried out on two different Arabic data sets that we have built and labelled manually. We show that results obtained on the first data set, which is slightly skewed, are better than those obtained on the second one which is highly skewed. The results show also that we can rely on the proposed techniques and that they are typically competitive with random under-sampling. Copyright \u00a9 2012 SciTePress - Science and Technology Publications.","keywords_author":["Arabic language","Machine learning","Natural language processing","Opinion mining","Sentiment analysis","Text classification","Unbalanced data sets"],"keywords_other":["Text classification","Sentiment analysis","Arabic languages","Unbalanced data","NAtural language processing","Opinion mining"],"max_cite":1.0,"pub_year":2012.0,"sources":"['scp']","rawkeys":["unbalanced data","text classification","arabic language","natural language processing","machine learning","unbalanced data sets","arabic languages","opinion mining","sentiment analysis"],"tags":["unbalanced data","text classification","unbalanced datasets","natural language processing","machine learning","arabic languages","opinion mining","sentiment analysis"]},{"p_id":37698,"title":"A semi-supervised approach for temporal information extraction from clinical text","abstract":"\u00a9 2016 IEEE.The implementation of electronic medical records (EMRs) produces a huge amount of unstructured clinical text. This domain-specific clinical text has opened a stage for temporal information extraction (TIE) due to its significance of exploitation in medical care and richness of temporality. Processing temporal information in clinical text is much more difficult in comparison to newswire text due to implicit expression of temporal information, domain-specific nature, lack of structure and writing quality. Despite of these limitations, the existing works established various methods to extract temporal information with the help of annotated corpora. But it is costly and time consuming to prepare the annotated corpora and thus their small size inevitably affect the processing quality. Motivated by this fact, in this work we propose a novel two-stage semi-supervised framework to exploit the abundant unannotated clinical text to automatically detect the temporal information and gradually increase the size of annotated corpora and then subsequently improve the temporal information extraction accuracy. In our pilot study of the proposed framework stage-one, we developed a conditional random fields (CRFs) model for the temporal event and expression extractions on the annotated data with the various features sets at phrase level. At first, we generated the possible features from the annotated corpora and significant features are selected. Finally we trained and evaluated our model with the selected features. Our model achieved F-measure of 81.34% for event recognition, and F-measure of 79.95% for temporal expression extraction.","keywords_author":["Clinical text","Electronic medical records","Information extraction","Machine learning","Natural Language Processing","Temporal events","Temporal expressions and Text mining"],"keywords_other":["Text mining","NAtural language processing","Clinical text","Electronic medical record","Temporal events"],"max_cite":1.0,"pub_year":2016.0,"sources":"['scp']","rawkeys":["temporal expressions and text mining","text mining","temporal events","information extraction","machine learning","natural language processing","electronic medical records","electronic medical record","clinical text"],"tags":["temporal expressions and text mining","text mining","temporal events","information extraction","natural language processing","machine learning","electronic medical record","clinical text"]},{"p_id":45893,"title":"Improving layman readability of clinical narratives with unsupervised synonym replacement","abstract":"\u00a9 2018 European Federation for Medical Informatics (EFMI) and IOS Press.We report on the development and evaluation of a prototype tool aimed to assist laymen\/patients in understanding the content of clinical narratives. The tool relies largely on unsupervised machine learning applied to two large corpora of unlabeled text - a clinical corpus and a general domain corpus. A joint semantic word-space model is created for the purpose of extracting easier to understand alternatives for words considered difficult to understand by laymen. Two domain experts evaluate the tool and inter-rater agreement is calculated. When having the tool suggest ten alternatives to each difficult word, it suggests acceptable lay words for 55.51% of them. This and future manual evaluation will serve to further improve performance, where also supervised machine learning will be used.","keywords_author":["Distributional semantics","Electronic health records","Natural language processing","Text simplification","Unsupervised machine learning","Word2vec"],"keywords_other":["Supervised Machine Learning","Humans","Semantics","Unsupervised Machine Learning","Natural Language Processing","Narration","Comprehension"],"max_cite":0.0,"pub_year":2018.0,"sources":"['scp']","rawkeys":["narration","comprehension","text simplification","unsupervised machine learning","distributional semantics","word2vec","semantics","natural language processing","electronic health records","humans","supervised machine learning"],"tags":["narration","comprehension","text simplification","unsupervised machine learning","distributional semantics","word2vec","semantics","natural language processing","electronic health records","humans","supervised machine learning"]},{"p_id":33608,"title":"A personalized time-bound activity Recommendation System","abstract":"\u00a9 2017 IEEE. Activity-listing services like TripAdvisor, Foursquare and Facebook events make use of community opinions\/reviews to help users identify 'points of interest' from a large search space and typically make use of collaborative filtering algorithm. These algorithms analyze reviews from a group of people to find correlations between users and\/or items in order to suggest top items to a querying user. The main issues with collaborative filtering are scalability and sparsity. Data sparsity creates a problem in scenarios where, a new location or activity that has not been reviewed by enough users is less likely to turn up as a recommendation. Also, users with unique interests do not benefit from such a model since the recommendations are irrespective of how relevant they may be to the user. There is also no system known to the authors, which recommends activities taking into consideration the time available with the user. Hence, a content based approach to generate a time-bound, relevant, personalized feed of 'activities' in the nearby area is proposed in this paper. If the user is a die-hard trekker, into indie bands, or dislikes pub hopping, the system will always take that into consideration. Similarly, if someone only wants to spend a couple of hours and is interested in outdoor activities, he\/she would be recommended ice-skating instead of hiking.","keywords_author":["Automatic Travel Guide","Clustering","Feature Extraction","Information Retrieval","Machine Learning","Natural Language Processing","Personalization","Recommendation System","Sentiment Analysis","Time Estimation"],"keywords_other":["Automatic Travel Guide","Clustering","Sentiment analysis","Time estimation","Personalizations","NAtural language processing"],"max_cite":2.0,"pub_year":2017.0,"sources":"['scp']","rawkeys":["automatic travel guide","personalization","machine learning","natural language processing","information retrieval","clustering","recommendation system","time estimation","feature extraction","personalizations","sentiment analysis"],"tags":["automatic travel guide","machine learning","natural language processing","information retrieval","clustering","recommender systems","time estimation","feature extraction","personalizations","sentiment analysis"]},{"p_id":37704,"title":"Predicting Sentiment toward Transportation in Social Media using Visual and Textual Features","abstract":"\u00a9 2016 IEEE.Social media platforms can be used by transportation agencies to receive feedback from their customers, thus creating two-way communication between the service provider and its consumers. Sentiment analysis is one method of aggregating overall polarity (positive or negative) towards a topic. However, most sentiment analysis methods rely on text processing, thus ignoring the large amount of image data present in popular social networks. The primary aim of this study is to exploit image data in conjunction with text and to evaluate this integrated approach for sentiment analysis for transportation. This study used image, captions, and comments data from the Instagram social network that were marked as being relevant to California Department of Transportation (Caltrans) and attempted to predict the expressed sentiment towards this agency. A set of high-level features were extracted from images using the web-based Microsoft Cognitive Services APIs. These features included the detection of faces and 86 categories which describe the images. Text features included the set of individual words and structural features. The experiment results of different machine learning techniques show a gain in precision when images and texts are combined compared to text-only approaches, thus confirming the relevance of visual content usage. The precision reaches a performance close to human classification agreement (typically approximately 80%). However, the results do not indicate that visual features are more informative than text features.","keywords_author":["Classification","Machine learning","Natural language processing","Public transportation","Sentiment analysis"],"keywords_other":["Transportation agencies","Sentiment analysis","Public transportation","California department of transportations","Two way communications","Social media platforms","NAtural language processing","Machine learning techniques"],"max_cite":1.0,"pub_year":2016.0,"sources":"['scp']","rawkeys":["social media platforms","machine learning techniques","natural language processing","machine learning","transportation agencies","two way communications","classification","public transportation","california department of transportations","sentiment analysis"],"tags":["social media platforms","machine learning techniques","natural language processing","machine learning","transportation agencies","two way communications","classification","public transportation","california department of transportations","sentiment analysis"]},{"p_id":37705,"title":"Deep learning for network analysis: Problems, approaches and challenges","abstract":"\u00a9 2016 IEEE. The analysis of social, communication and information networks for identifying patterns, evolutionary characteristics and anomalies is a key problem for the military, for instance in the Intelligence community. Current techniques do not have the ability to discern unusual features or patterns that are not a priori known. We investigate the use of deep learning for network analysis. Over the last few years, deep learning has had unprecedented success in areas such as image classification, speech recognition, etc. However, research on the use of deep learning to network or graph analysis is limited. We present three preliminary techniques that we have developed as part of the ARL Network Science CTA program: (a) unsupervised classification using a very highly trained image recognizer, namely Caffe; (b) supervised classification using a variant of convolutional neural networks on node features such as degree and assortativity; and (c) a framework called node2vec for learning representations of nodes in a network using a mapping to natural language processing.","keywords_author":["Biological neural networks","Convolution","Erbium","Feature extraction","Machine learning","Measurement"],"keywords_other":["Network science","Supervised classification","Unsupervised classification","Biological neural networks","Information networks","Convolutional neural network","Intelligence communities","NAtural language processing"],"max_cite":1.0,"pub_year":2016.0,"sources":"['scp', 'ieee']","rawkeys":["measurement","supervised classification","unsupervised classification","erbium","convolution","machine learning","natural language processing","network science","feature extraction","convolutional neural network","biological neural networks","information networks","intelligence communities"],"tags":["measurement","supervised classification","unsupervised classification","erbium","convolution","machine learning","natural language processing","network science","feature extraction","convolutional neural network","biological neural networks","information networks","intelligence communities"]},{"p_id":843,"title":"A comparative study of open source deep learning frameworks","abstract":"Deep Learning (DL) is one of the hottest trends in machine learning as DL approaches produced results superior to the state-of-the-art in problematic areas such as image processing and natural language processing (NLP). To foster the growth of the DL community, several open source frameworks appeared providing implementations of the most common DL algorithms. These frameworks vary in the algorithms they support and in the quality of their implementations. The purpose of this work is to provide a qualitative and quantitative comparison among three of the most popular and most comprehensive DL frameworks (namely Google's TensorFlow, University of Montreal's Theano, and Microsoft's CNTK). The ultimate goal of this work is to help end users make an informed decision about the best DL framework that suits their needs and resources. To ensure that our study is as comprehensive as possible, we conduct several experiments using multiple benchmark datasets and measure the performance of the frameworks' implementation of different DL algorithms. For most of our experiments, we find out that CNTK's implementations are superior to the other ones under consideration.","keywords_author":["CIFAR-10","CNN","CNTK","Deep Learning","MNIST","TensorFlow","Theano","Deep Learning","CNN","TensorFlow","Theano","CNTK","MNIST","CIFAR-10"],"keywords_other":["Python","Portable computers","quantitative comparison","open source deep learning frameworks","qualitative comparison","MNIST","machine learning","Machine learning","open source frameworks","CIFAR-10","Theano","learning (artificial intelligence)","DL framework","CNTK implementations","Libraries","Graphics processing units","C++ languages","image processing","TensorFlow","natural language processing","Servers","CNTK"],"max_cite":0.0,"pub_year":2018.0,"sources":"['scp', 'ieee']","rawkeys":["portable computers","mnist","cntk","quantitative comparison","open source deep learning frameworks","qualitative comparison","machine learning","python","dl framework","open source frameworks","theano","libraries","learning (artificial intelligence)","deep learning","tensorflow","cntk implementations","cnn","servers","c++ languages","cifar-10","graphics processing units","image processing","natural language processing"],"tags":["portable computers","mnist","cntk","c++ language","convolutional neural network","quantitative comparison","open source deep learning frameworks","qualitative comparison","machine learning","python","dl framework","open source frameworks","theano","libraries","tensorflow","cntk implementations","servers","cifar-10","graphics processing units","image processing","natural language processing"]},{"p_id":39758,"title":"Semi-autonomous hierarchical document classification using an interactive grounding framework","abstract":"Organizations must evolve internally over time to better fulfill their missions. The structural complexity inherent in a large organization can impede an agile evolutionary process, however. In this work we describe a tool created to ease the process of structural evolution within a large, complex and hierarchically organized organization. This tool analyzes request for change documents provided by low-level personnel unfamiliar with the overall organizational structure and determines which portions of the organization will be affected. This task is accomplished using natural language processing to extract relevant textual features from request for change documents and machine learning techniques to ground these features to relevant concepts corresponding to affected portions of the hierarchy. The initial groundings are refined through an interactive question-answer session with the user providing the request for change document. The tool is shown to have desirable convergence traits and a high degree of accuracy.","keywords_author":["Automatic document classification","Hierarchical document classification","Machine learning","Neural networks","NLP"],"keywords_other":["Document Classification","Structural complexity","NLP","Hierarchical document","Organizational structures","High degree of accuracy","NAtural language processing","Machine learning techniques"],"max_cite":1.0,"pub_year":2012.0,"sources":"['scp']","rawkeys":["hierarchical document classification","structural complexity","nlp","organizational structures","neural networks","machine learning techniques","machine learning","natural language processing","high degree of accuracy","document classification","hierarchical document","automatic document classification"],"tags":["hierarchical document classification","structural complexity","organizational structures","neural networks","machine learning techniques","machine learning","natural language processing","high degree of accuracy","document classification","hierarchical document","automatic document classification"]},{"p_id":31567,"title":"Information retrieval: A new multilingual stemmer based on a statistical approach","abstract":"\u00a9 2015 IEEE. Stemming is a technique used to reduce inflected and derived words to their basic forms (stem or root). It is a very important step of pre-processing in text mining, and generally used in many areas of research such as: Natural language Processing NLP, Text Categorization TC, Text Summarizing TS, Information Retrieval IR, and other tasks in text mining. Stemming is frequently useful in text categorization to reduce the size of terms vocabulary, and in information retrieval to improve the search effectiveness and then gives us relevant results. In this paper, we propose a new multilingual stemmer based on the extraction of word root and in which we use the technique of n-grams. We validated our stemmer on three languages which are: Arabic, French and English.","keywords_author":["Bigrams technique","Information retrieval","Machine learning","Natural language processing","Root extraction","Stemming","Text mining"],"keywords_other":["Text mining","Root extraction","Bigrams","Stemming","NAtural language processing"],"max_cite":3.0,"pub_year":2015.0,"sources":"['scp']","rawkeys":["bigrams","text mining","machine learning","natural language processing","information retrieval","stemming","bigrams technique","root extraction"],"tags":["bigrams","text mining","machine learning","natural language processing","information retrieval","bigrams technique","root extraction","stem"]},{"p_id":31572,"title":"Learning to grade short answers using machine learning techniques","abstract":"\u00a9 2015 ACM. In this work, we are attempting to grade short answer automatically which can be efficient and helpful to both students and teachers. It uses a combination of many semantic and graph alignment features and is implemented in the Microsoft Azure Machine Learning using Two-class Averaged Perceptron, Linear and Isotonic Regression. We also provide first attempt to use graph alignment features at sentence level. We compare the results of two machine learning algorithms like Two-class Averaged Perceptron and Twoclass Support Vector Machine in the results of grading short answers. We have devised novel techniques to apply the concept of Random Projection for grading 150 algorithmic answers on a coding question using our own domain specific corpus which gives precise classification of right and wrong answers.","keywords_author":["Computational intelligence","Machine learning","Natural language processing","Short answer grading"],"keywords_other":["Novel techniques","Random projections","Averaged perceptron","Alignment features","Isotonic regression","Two-class support vector machines","NAtural language processing","Machine learning techniques"],"max_cite":3.0,"pub_year":2015.0,"sources":"['scp']","rawkeys":["averaged perceptron","alignment features","machine learning techniques","machine learning","natural language processing","isotonic regression","random projections","novel techniques","two-class support vector machines","computational intelligence","short answer grading"],"tags":["averaged perceptron","alignment features","machine learning techniques","machine learning","natural language processing","isotonic regression","random projections","novel techniques","two-class support vector machines","computational intelligence","short answer grading"]},{"p_id":23382,"title":"Training deep neural networks on imbalanced data sets","abstract":"\u00a9 2016 IEEE.Deep learning has become increasingly popular in both academic and industrial areas in the past years. Various domains including pattern recognition, computer vision, and natural language processing have witnessed the great power of deep networks. However, current studies on deep learning mainly focus on data sets with balanced class labels, while its performance on imbalanced data is not well examined. Imbalanced data sets exist widely in real world and they have been providing great challenges for classification tasks. In this paper, we focus on the problem of classification using deep network on imbalanced data sets. Specifically, a novel loss function called mean false error together with its improved version mean squared false error are proposed for the training of deep networks on imbalanced data sets. The proposed method can effectively capture classification errors from both majority class and minority class equally. Experiments and comparisons demonstrate the superiority of the proposed approach compared with conventional methods in classifying imbalanced data sets on deep neural networks.","keywords_author":["Data imbalance","Deep neural network","Loss function"],"keywords_other":["Data imbalance","Classification tasks","Conventional methods","Imbalanced Data-sets","Loss functions","Classification errors","Deep neural networks","NAtural language processing"],"max_cite":11.0,"pub_year":2016.0,"sources":"['scp', 'wos']","rawkeys":["loss functions","deep neural network","data imbalance","deep neural networks","loss function","natural language processing","conventional methods","classification tasks","classification errors","imbalanced data-sets"],"tags":["loss functions","data imbalance","natural language processing","conventional methods","classification tasks","classification errors","convolutional neural network","imbalanced data-sets"]},{"p_id":39767,"title":"Feature weighting strategies in sentiment analysis","abstract":"In this paper we propose an adaptation of the Kullback- Leibler divergence score for the task of sentiment and opinion classification on a sentence level. We propose to use the obtained score with the SVM model using different thresholds for pruning the feature set. We argue that the pruning of the feature set for the task of sentiment analysis (SA) may be detrimental to classifiers performance on short text. As an alternative approach, we consider a simple additive scheme that takes into account all of the features. Accuracy rates over 10 fold cross-validation indicate that the latter approach outperforms the SVM classification scheme.","keywords_author":["Kullback-leibler divergence","Machine learning","Natural language processing","Opinion detection","Sentiment analysis"],"keywords_other":["Feature weighting","Cross validation","SVM classification","Sentiment analysis","Additive schemes","Kullback Leibler divergence","Opinion detections","NAtural language processing"],"max_cite":1.0,"pub_year":2012.0,"sources":"['scp']","rawkeys":["kullback leibler divergence","cross validation","opinion detection","svm classification","machine learning","natural language processing","kullback-leibler divergence","sentiment analysis","additive schemes","opinion detections","feature weighting"],"tags":["svm classification","machine learning","natural language processing","kullback-leibler divergence","sentiment analysis","computer vision","additive schemes","opinion detections","feature weighting"]},{"p_id":27480,"title":"Preventing failures by mining maintenance logs with case-based reasoning","abstract":"The project integrates work in natural language processing, machine learning, and the semantic web, bringing together these diverse disciplines in a novel way to address a real problem. The objective is to extract and categorize machine components and subsystems and their associated failures using a novel approach that combines text analysis, unsupervised text clustering, and domain models. Through industrial partnerships, this project will demonstrate effectiveness of the proposed approach with actual industry data.","keywords_author":["Case-based reasoning","Data mining","Machine learning","Maintenance log analysis","Natural language processing","Semantic web"],"keywords_other":["Mining maintenance","Text analysis","Domain model","Industrial partnerships","Text Clustering","Log analysis","Real problems","NAtural language processing"],"max_cite":6.0,"pub_year":2005.0,"sources":"['scp']","rawkeys":["data mining","domain model","real problems","mining maintenance","log analysis","maintenance log analysis","machine learning","natural language processing","industrial partnerships","case-based reasoning","text analysis","text clustering","semantic web"],"tags":["data mining","domain model","real problems","mining maintenance","log analysis","maintenance log analysis","machine learning","natural language processing","industrial partnerships","case-based reasoning","text analysis","text clustering","semantic web"]},{"p_id":33626,"title":"Classification and detection of micro-level impact of issue-focused documentary films based on reviews","abstract":"\u00a9 2017 ACM. We present novel research at the intersection of review mining and impact assessment of issue-focused information products, namely documentary films. We develop and evaluate a theoretically grounded classification schema, related codebook, corpus annotation, and prediction model for detecting multiple types of impact that documentaries can have on individuals, such as change versus reaffirmation of behavior, cognition, and emotions, based on user-generated content, i.e., reviews. This work broadens the scope of review mining tasks, which typically comprise the prediction of ratings, helpfulness, and opinions. Our results suggest that documentaries can change or reinforce peoples' conception of an issue. We perform supervised learning to predict impact on the sentence level by using data driven as well as predefined linguistic, lexical, and psychological features; achieving an accuracy rate of 81% (F1) when using a Random Forest classifier, and 73% with a Support Vector Machine.","keywords_author":["Micro-level impact","Natural language processing","Review mining","Social impact assessment","Supervised machine learning"],"keywords_other":["Social impact assessments","User-generated content","Information products","Micro level","Random forest classifier","Psychological features","NAtural language processing","Supervised machine learning"],"max_cite":2.0,"pub_year":2017.0,"sources":"['scp']","rawkeys":["social impact assessment","social impact assessments","review mining","micro level","psychological features","natural language processing","random forest classifier","user-generated content","information products","supervised machine learning","micro-level impact"],"tags":["social impact assessment","review mining","micro level","psychological features","natural language processing","random forest classifier","user-generated content","information products","supervised machine learning","micro-level impact"]},{"p_id":45916,"title":"CNN-encoded radical-level representation for Japanese processing","abstract":"\u00a9 2018, Japanese Society for Artificial Intelligence. All rights reserved. Although word embeddings are powerful, weakness on rare words, unknown words and issues of large vocabulary motivated people to explore alternative representations. While the character embeddings have been successful for alphabetical languages, Japanese is difficult to be processed at the character level as well because of the large vocabulary of kanji, written in the Chinese characters. In order to achieve fewer parameters and better generalization on infrequent words and characters, we proposed a model that encodes Japanese texts from the radical-level representation, inspired by the experimental findings in the field of psycholinguistics. The proposed model is comprised of a convolutional local encoder and a recurrent global encoder. For the convolutional encoder, we propose a novel combination of two kinds of convolutional filters of different strides in one layer to extract information from the different levels. We compare the proposed radical-level model with the state-of-the-art word and character embedding-based models in the sentiment classification task. The proposed model outperformed the state-of-the-art models for the randomly sampled texts and the texts that contain unknown characters, with 91% and 12% fewer parameters than the word embedding-based and character embedding-based models, respectively. Especially for the test sets of unknown characters, the results by the proposed model were 4.01% and 2.38% above the word embedding-based and character embedding-based baselines, respectively. The proposed model is powerful with cheaper computational and storage cost, can be used for devices with limited storage and to process texts of rare characters.","keywords_author":["Convolutional neural networks","Deep learning","Natural language processing","Sub-character language modeling","Text classification"],"keywords_other":["Word and characters","Chinese characters","Text classification","Language model","Extract informations","Convolutional neural network","Sentiment classification","Convolutional encoders"],"max_cite":0.0,"pub_year":2018.0,"sources":"['scp']","rawkeys":["chinese characters","sentiment classification","convolutional neural networks","word and characters","deep learning","convolutional encoders","extract informations","natural language processing","language model","convolutional neural network","text classification","sub-character language modeling"],"tags":["chinese characters","sentiment classification","word and characters","convolutional encoders","machine learning","natural language processing","extracting information","language model","convolutional neural network","text classification","sub-character language modeling"]},{"p_id":862,"title":"Caching for Mobile Social Networks with Deep Learning: Twitter Analysis for 2016 U.S. Election","abstract":"As the rise of the portable devices, people usually access the social media such as Twitter and Facebook through wireless networks. Therefore, data transmission rates significant important to the end users. In this work, we discuss the problem of context-aware data caching in the heterogeneous small cell networks to reduce the service delay and how the device-to-device (D2D) and device-to-infrastructure (D2I) improve the system social welfare. In the data-caching model, we explore three types of cache entities, macro cell base stations, small cell base stations, and end user devices. We propose a long short-term memory (LSTM) deep learning model to perform data analysis and extract information content from the data. By knowing the interest of the data to the cache entities, we can cache the data that will most likely to be requested by the end users to reduce service latency. In simulation, we show our proposed algorithm can efficiently reduce the service latency during 2016 U.S. presidential election where mobile user were urgent to request the election information through wireless networks. Comparing with other mechanisms such as using one-to-many matching algorithm or without D2D communication technology, our proposed algorithm improves significantly on the devices performance and system social welfare.","keywords_author":["caching","D2D","D2I","deep learning","Device-to-device communication","HetNet","Machine learning","matching","Mobile handsets","mobile social media network","natural language processing","Performance evaluation","Twitter","Voting","caching","D2D","D2I","deep learning","HetNet","matching","mobile social media network","natural language processing"],"keywords_other":["Twitter","Device-to-device communication","Voting","Device-to-Device communications","Mobile social medias","HetNet","Machine learning","Performance evaluations","caching","matching","Performance evaluation","Mobile handsets"],"max_cite":0.0,"pub_year":2018.0,"sources":"['scp', 'ieee']","rawkeys":["deep learning","d2d","machine learning","natural language processing","device-to-device communications","mobile handsets","twitter","performance evaluations","caching","matching","mobile social medias","performance evaluation","device-to-device communication","d2i","hetnet","voting","mobile social media network"],"tags":["performance evaluation","d2d","machine learning","natural language processing","device-to-device communications","mobile handsets","twitter","caching","matching","mobile social media network","mobile social medias","d2i","voting","heterogeneous network"]},{"p_id":50015,"title":"Extraction of Definitional Contexts through Machine Learning","abstract":"\u00a9 2015 IEEE. Automatic extraction of definitional contexts has been a problem that deserved to be addressed to in different studies by applications demands in the Natural Language Processing. The first approach to the automatic extraction of these resources has been through specific linguistic patterns, but this approach requires previous extensive linguistic knowledge and a thorough previous work. A model machine learning, on the other hand, reduces the work and, as we believe, can improve the results obtained with only one approach based on linguistic rules. Here experiments for extraction\/classification of definitional contexts with naive bayes classifier and SVM are presented. We show that through machine learning approaches we can improve the results of this specific task. The highest result was obtained by the naive bayes classifier with back-off as smoothing.","keywords_author":["clasification","Definitional contexts","machine learning","naive Bayes","Support Vector Machine","terminology extraction"],"keywords_other":["Naive Bayes classifiers","Naive bayes","Terminology extraction","clasification","Machine learning approaches","Definitional contexts","Automatic extraction","NAtural language processing"],"max_cite":0.0,"pub_year":2016.0,"sources":"['scp']","rawkeys":["clasification","definitional contexts","machine learning","natural language processing","automatic extraction","naive bayes classifiers","terminology extraction","support vector machine","naive bayes","machine learning approaches"],"tags":["clasification","definitional contexts","machine learning","natural language processing","automatic extraction","naive bayes classifiers","terminology extraction","naive bayes","machine learning approaches"]},{"p_id":45918,"title":"From case law to ratio decidendi","abstract":"\u00a9 2018, Springer International Publishing AG, part of Springer Nature. This paper is concerned with the task of automatically identifying legally binding principles, known as ratio decidendi or just ratio, from transcripts of court judgements, also called case law or just cases. After briefly reviewing the relevant definitions and previous work in the area, we present a novel system for automatically extracting ratio from cases using a combination of natural language processing and machine learning. Our approach is based on the hypothesis that the ratio of a given case can be reliably obtained by identifying statements of legal principles in paragraphs that are cited by subsequent cases. Our method differs from related recent work by extracting principles from the text of the cited paragraphs (in the given case) as opposed to the text of the citing paragraphs (in a subsequent case). We conduct our own independent small-scale annotation study which reveals that this seemingly subtle shift of focus substantially increases reliability of finding the ratio. Then, by building on previous work in the automatic detection of legal principles and cross citations, we present a fully automated system that successfully identifies the ratio (in our study) with an accuracy of 72%.","keywords_author":["Case law","Cross reference resolution","Machine learning","Natural language processing","Principle detection","Ratio decidendi"],"keywords_other":["Court judgement","Fully automated","Ratio decidendi","Legal principles","Reference resolution","Case law","Small scale","Automatic Detection"],"max_cite":0.0,"pub_year":2018.0,"sources":"['scp']","rawkeys":["cross reference resolution","fully automated","ratio decidendi","legal principles","reference resolution","machine learning","natural language processing","case law","small scale","principle detection","automatic detection","court judgement"],"tags":["cross reference resolution","fully automated","ratio decidendi","legal principles","reference resolution","machine learning","natural language processing","case law","small scale","principle detection","automatic detection","court judgement"]},{"p_id":45922,"title":"Dual Long Short-Term Memory Networks for Sub-Character Representation Learning","abstract":"\u00a9 2018, Springer International Publishing AG, part of Springer Nature. Characters have commonly been regarded as the minimal processing unit in Natural Language Processing (NLP). But many non-latin languages have hieroglyphic writing systems, involving a big alphabet with thousands or millions of characters. Each character is composed of even smaller parts, which are often ignored by the previous work. In this paper, we propose a novel architecture employing two stacked Long Short-Term Memory Networks (LSTMs) to learn sub-character level representation and capture deeper level of semantic meanings. To build a concrete study and substantiate the efficiency of our neural architecture, we take Chinese Word Segmentation as a research case example. Among those languages, Chinese is a typical case, for which every character contains several components called radicals. Our networks employ a shared radical level embedding to solve both Simplified and Traditional Chinese Word Segmentation, without extra Traditional to Simplified Chinese conversion, in such a highly end-to-end way the word segmentation can be significantly simplified compared to the previous work. Radical level embeddings can also capture deeper semantic meaning below character level and improve the system performance of learning. By tying radical and character embeddings together, the parameter count is reduced whereas semantic knowledge is shared and transferred between two levels, boosting the performance largely. On 3 out of 4 Bakeoff 2005 datasets, our method surpassed state-of-the-art results by up to 0.4%. Our results are reproducible; source codes and corpora are available on GitHub (https:\/\/github.com\/hankcs\/sub-character-cws).","keywords_author":["AI algorithms and applications","Deep learning","Machine learning algorithms","Natural language processing","Neural networks","Pattern recognition"],"keywords_other":["Character representations","Minimal processing","Word segmentation","Novel architecture","Chinese word segmentation","Semantic knowledge","AI algorithms","Neural architectures"],"max_cite":0.0,"pub_year":2018.0,"sources":"['scp']","rawkeys":["novel architecture","word segmentation","chinese word segmentation","deep learning","neural networks","character representations","machine learning algorithms","ai algorithms and applications","natural language processing","ai algorithms","neural architectures","pattern recognition","minimal processing","semantic knowledge"],"tags":["novel architecture","word segmentation","chinese word segmentation","neural networks","character representations","machine learning algorithms","machine learning","ai algorithms and applications","natural language processing","ai algorithms","neural architectures","pattern recognition","minimal processing","semantic knowledge"]},{"p_id":21348,"title":"Multimodal sentiment intensity analysis in videos: Facial gestures and verbal messages","abstract":"\u00a9 2001-2011 IEEE.People share their opinions, stories, and reviews through online video sharing websites every day. The automatic analysis of these online opinion videos is bringing new or understudied research challenges to the field of computational linguistics and multimodal analysis. Among these challenges is the fundamental question of exploiting the dynamics between visual gestures and verbal messages to be able to better model sentiment. This article addresses this question in four ways: introducing the first multimodal dataset with opinion-level sentiment intensity annotations; studying the prototypical interaction patterns between facial gestures and spoken words when inferring sentiment intensity; proposing a new computational representation, called multimodal dictionary, based on a language-gesture study; and evaluating the authors' proposed approach in a speaker-independent paradigm for sentiment intensity prediction. The authors' study identifies four interaction types between facial gestures and verbal content: neutral, emphasizer, positive, and negative interactions. Experiments show statistically significant improvement when using multimodal dictionary representation over the conventional early fusion representation (that is, feature concatenation).","keywords_author":["affective computing","computational linguistics","intelligent systems","machine learning","natural language processing","sentiment analysis"],"keywords_other":["Affective Computing","Speaker independents","Sentiment analysis","Negative interaction","Intensity prediction","Online video sharing","Multimodal analysis","NAtural language processing"],"max_cite":14.0,"pub_year":2016.0,"sources":"['ieee', 'scp', 'wos']","rawkeys":["multimodal analysis","speaker independents","affective computing","machine learning","natural language processing","intelligent systems","negative interaction","computational linguistics","online video sharing","intensity prediction","sentiment analysis"],"tags":["multimodal analysis","speaker independents","affective computing","machine learning","natural language processing","intelligent systems","negative interaction","computational linguistics","online video sharing","intensity prediction","sentiment analysis"]},{"p_id":33637,"title":"Overview of deep learning","abstract":"\u00a9 2016 IEEE.In recent years, deep learning has achieved great success in many fields, such as computer vision and natural language processing. Compared to traditional machine learning methods, deep learning has a strong learning ability and can make better use of datasets for feature extraction. Because of its practicability, deep learning becomes more and more popular for many researchers to do research works. In this paper, we mainly introduce some advanced neural networks of deep learning and their applications. Besides, we also discuss the limitations and prospects of deep learning.","keywords_author":["deep learning","machine learning","neural network"],"keywords_other":["NAtural language processing","Machine learning methods","Learning abilities"],"max_cite":2.0,"pub_year":2017.0,"sources":"['scp']","rawkeys":["machine learning methods","neural network","deep learning","machine learning","natural language processing","learning abilities"],"tags":["machine learning methods","neural networks","natural language processing","machine learning","learning abilities"]},{"p_id":7014,"title":"Profiling, what-if analysis, and costbased optimization of mapreduce programs","abstract":"MapReduce has emerged as a viable competitor to database systems in big data analytics. MapReduce programs are being written for a wide variety of application domains including business data processing, text analysis, natural language processing, Web graph and social network analysis, and computational science. However, MapReduce systems lack a feature that has been key to the historical success of database systems, namely, cost-based optimization. A major challenge here is that, to the MapReduce system, a program consists of black-box map and reduce functions written in some programming language like C++, Java, Python, or Ruby. We introduce, to our knowledge, the first Cost-based Optimizer for simple to arbitrarily complex MapReduce programs. We focus on the optimization opportunities presented by the large space of configuration parameters for these programs. We also introduce a Profiler to collect detailed statistical information from unmodified MapReduce programs, and a What-if Engine for fine-grained cost estimation. All components have been prototyped for the popular Hadoop MapReduce system. The effectiveness of each component is demonstrated through a comprehensive evaluation using representative MapReduce programs from various application domains. \u00a9 2011 VLDB Endowment.","keywords_author":null,"keywords_other":["Comprehensive evaluation","Map-reduce","Large spaces","Statistical information","Text analysis","Computational science","Optimizers","Cost-based optimization","Application domains","Social Network Analysis","Web graphs","Configuration parameters","Big datum","NAtural language processing","Black boxes","Cost estimations"],"max_cite":174.0,"pub_year":2011.0,"sources":"['scp']","rawkeys":["optimizers","web graphs","large spaces","computational science","statistical information","black boxes","cost estimations","comprehensive evaluation","configuration parameters","natural language processing","cost-based optimization","map-reduce","text analysis","social network analysis","big datum","application domains"],"tags":["web graphs","large spaces","statistical information","black boxes","cost estimations","comprehensive evaluation","configuration parameters","natural language processing","cost-based optimization","map-reduce","text analysis","social network analysis","computer science","optimization","big datum","application domains"]},{"p_id":872,"title":"Exploiting approximate computing for deep learning acceleration","abstract":"Deep Neural Networks (DNNs) have emerged as a powerful and versatile set of techniques to address challenging artificial intelligence (AI) problems. Applications in domains such as image\/video processing, natural language processing, speech synthesis and recognition, genomics and many others have embraced deep learning as the foundational technique. DNNs achieve superior accuracy for these applications using very large models which require 100s of MBs of data storage, ExaOps of computation and high bandwidth for data movement. Despite advances in computing systems, training state-of-the-art DNNs on large datasets takes several days\/weeks, directly limiting the pace of innovation and adoption. In this paper, we discuss how these challenges can be addressed via approximate computing. Based on our earlier studies demonstrating that DNNs are resilient to numerical errors from approximate computing, we present techniques to reduce communication overhead of distributed deep learning training via adaptive residual gradient compression (AdaComp), and computation cost for deep learning inference via Prameterized clipping ACTivation (PACT) based network quantization. Experimental evaluation demonstrates order of magnitude savings in communication overhead for training and computational cost for inference while not compromising application accuracy.","keywords_author":null,"keywords_other":["video processing","DNNs","Approximate computing","speech recognition","prameterized clipping ACTivation","Computational modeling","artificial intelligence problems","data movement","gradient methods","Machine learning","PACT","Convolution","data storage","approximate computing","genomics","distributed deep learning training","computational cost","learning (artificial intelligence)","Training","network quantization","Quantization (signal)","adaptive residual gradient compression","neural nets","Data models","foundational technique","image processing","deep neural networks","computing systems","natural language processing","speech synthesis","deep learning inference","deep learning acceleration"],"max_cite":null,"pub_year":2018.0,"sources":"['ieee']","rawkeys":["video processing","speech recognition","prameterized clipping activation","artificial intelligence problems","convolution","data movement","machine learning","gradient methods","data storage","approximate computing","genomics","computational modeling","distributed deep learning training","computational cost","learning (artificial intelligence)","pact","dnns","training","network quantization","adaptive residual gradient compression","neural nets","foundational technique","image processing","deep neural networks","computing systems","natural language processing","quantization (signal)","speech synthesis","deep learning inference","data models","deep learning acceleration"],"tags":["video processing","speech recognition","data movements","computational costs","convolutional neural network","prameterized clipping activation","artificial intelligence problems","convolution","computational system","machine learning","gradient methods","data storage","approximate computing","genomics","computational modeling","distributed deep learning training","deep learning accelerator","pact","neural networks","training","network quantization","adaptive residual gradient compression","signals","foundational technique","image processing","natural language processing","speech synthesis","deep learning inference","data models"]},{"p_id":37737,"title":"CoLUA: Automatically Predicting Configuration Bug Reports and Extracting Configuration Options","abstract":"\u00a9 2016 IEEE.Configuration bugs are among the dominant causes of software failures. Software organizations often use bug tracking systems to manage bug reports collected from developers and users. In order for software developers to understand and reproduce configuration bugs, it is vital for them to know whether a bug in the bug report is related to configuration issues, this is not often easily discerned due to a lack of easy to spot terminology in the bug reports. In addition, to locate and fix a configuration bug, a developer needs to know which configuration options are associated with the bug. To address these two problems, we introduce CoLUA, a two-step automated approach that combines natural language processing, information retrieval, and machine learning. In the first step, CoLUA selects features from the textual information in the bug reports, and uses various machine learning techniques to build classification models, developers can use these models to label a bug report as either a configuration bug report or a non-configuration bug report. In the second step, CoLUA identifies which configuration options are involved in the labeled configuration bug reports. We evaluate CoLUA on 900 bug reports from three large open source software systems. The results show that CoLUA predicts configuration bug reports with high accuracy and that it effectively identifies the root causes of configuration options.","keywords_author":["bug reports","configuration","machine learning"],"keywords_other":["Software organization","Open source software systems","Configuration options","Classification models","Bug reports","configuration","NAtural language processing","Machine learning techniques"],"max_cite":1.0,"pub_year":2016.0,"sources":"['scp']","rawkeys":["software organization","configuration options","bug reports","machine learning","machine learning techniques","natural language processing","open source software systems","configuration","classification models"],"tags":["software organization","configuration options","bug reports","machine learning","machine learning techniques","natural language processing","open source software systems","configuration","classification models"]},{"p_id":15210,"title":"RC-NET: A general framework for incorporating knowledge into word representations","abstract":"Copyright 2014 ACM.Representing words into vectors in continuous space can form up a potentially powerful basis to generate high-quality textual features for many text mining and natural language processing tasks. Some recent efforts, such as the skip-gram model, have attempted to learn word representations that can capture both syntactic and semantic information among text corpus. However, they still lack the capability of encoding the properties of words and the complex relationships among words very well, since text itself often contains incomplete and ambiguous information. Fortunately, knowledge graphs provide a golden mine for enhancing the quality of learned word representations. In particular, a knowledge graph, usually composed by entities (words, phrases, etc.), relations between entities, and some corresponding meta information, can supply invaluable relational knowledge that encodes the relationship between entities as well as categorical knowledge that encodes the attributes or properties of entities. Hence, in this paper, we introduce a novel framework called RC-NET to leverage both the relational and categorical knowledge to produce word representations of higher quality. Specifically, we build the relational knowledge and the categorical knowledge into two separate reg-ularization functions, and combine both of them with the original objective function of the skip-gram model. By solving this combined optimization problem using back propagation neural networks, we can obtain word representations enhanced by the knowledge graph. Experiments on popular text mining and natural language processing tasks, including analogical reasoning, word similarity, and topic prediction, have all demonstrated that our model can significantly improve the quality of word representations.","keywords_author":["Deep learning","Distributed word representations","Knowledge graph"],"keywords_other":["Deep learning","Word representations","Knowledge graphs","Back propagation neural networks","Combined optimization problem","Analogical reasoning","Complex relationships","NAtural language processing"],"max_cite":48.0,"pub_year":2014.0,"sources":"['scp']","rawkeys":["knowledge graphs","back propagation neural networks","deep learning","knowledge graph","natural language processing","combined optimization problem","distributed word representations","word representations","analogical reasoning","complex relationships"],"tags":["knowledge graphs","back propagation neural networks","machine learning","natural language processing","distributed word representation","combined optimization problem","word representations","analogical reasoning","complex relationships"]},{"p_id":876,"title":"Adaptive runtime exploiting sparsity in tensor of deep learning neural network on heterogeneous systems","abstract":"Deep neural networks have been widely applied in many areas, such as computer vision, natural language processing and information retrieval. However, due to the high computation and memory demands, deep learning applications have not been adopted in edge learning. In this paper, we exploit the sparsity in tensors to reduce the computation overheads and memory demands. Unlike other approaches which rely on hardware accelerator designs or sacrifice model accuracy for the performance by pruning parameters, we adaptively partition and deploy the workload to heterogeneous devices to reduce computation and memory requirements and increase computing efficiency. We had implemented our partitioning algorithms in Google's TensorFlow and evaluated on an AMD Kaveri system, which is an HSA-based heterogeneous computing system. Our method has effectively reduced the computation time, cache accesses, and cache miss rates, without impacting the accuracy of the learning models. Our approach achieves 66% and 88% speedup for the lenet-5 model and the lenet-1024-1024 model, respectively. For reducing memory traffic, our approach reduces 71% instruction cache references, 32% data cache references. Our system has also improved cache miss rate from 1.6% to 0.5% during the training of the lenet-1024-1024 model.","keywords_author":null,"keywords_other":["Google tensorflow","adaptive runtime exploiting sparsity","heterogeneous systems","computation overheads","cache storage","memory traffic","edge learning","Computational modeling","instruction cache references","memory requirements","Machine learning","sparse matrices","partitioning algorithms","learning (artificial intelligence)","memory demands","data cache references","Heterogeneous networks","Training","information retrieval","hardware accelerator designs","deep learning neural network","tensors","computer vision","neural nets","Tensile stress","Neurons","AMD Kaveri system","heterogeneous devices","natural language processing","cache accesses","lenet-5 model","HSA-based heterogeneous computing system","Sparse matrices","computation time","cache miss rates"],"max_cite":null,"pub_year":2017.0,"sources":"['ieee']","rawkeys":["adaptive runtime exploiting sparsity","heterogeneous systems","computation overheads","cache storage","memory traffic","edge learning","instruction cache references","memory requirements","tensile stress","machine learning","hsa-based heterogeneous computing system","sparse matrices","neurons","partitioning algorithms","computational modeling","heterogeneous networks","learning (artificial intelligence)","data cache references","memory demands","training","information retrieval","hardware accelerator designs","deep learning neural network","tensors","computer vision","neural nets","heterogeneous devices","natural language processing","cache accesses","google tensorflow","lenet-5 model","computation time","cache miss rates","amd kaveri system"],"tags":["adaptive runtime exploiting sparsity","heterogeneous systems","cache storage","memory traffic","edge learning","computational time","instruction cache references","memory requirements","tensile stress","machine learning","hsa-based heterogeneous computing system","computational overheads","sparse matrices","neurons","partitioning algorithms","cache access","computational modeling","data cache references","memory demands","neural networks","training","information retrieval","hardware accelerator designs","deep learning neural network","computer vision","heterogeneous devices","natural language processing","google tensorflow","lenet-5 model","tensor","cache miss rates","amd kaveri system","heterogeneous network"]},{"p_id":27501,"title":"PubMiner: Machine learning-based text mining system for biomedical information mining","abstract":"PubMiner, an intelligent machine learning based text mining system for mining biological information from the literature is introduced. PubMiner utilize natural language processing and machine learning based data mining techniques for mining useful biological information such as protein-protein interaction from the massive literature data. The system recognizes biological terms such as gene, protein, and enzymes and extracts their interactions described in the document through natural language analysis. The extracted interactions are further analyzed with a set of features of each entity which were constructed from the related public databases to infer more interactions from the original interactions. An inferred interaction from the interaction analysis and native interaction are provided to the user with the link of literature sources. The evaluation of system performance proceeded with the protein interaction data of S.cerevisiae (bakers yeast) from MIPS and SGD. \u00a9 Springer-Verlag Berlin Heidelberg 2004.","keywords_author":["Bioinformatics","Data Mining","Machine Learning","Natural Language Processing","Software Application"],"keywords_other":["Intelligent machine","Software applications","Biological information","Software application","Natural language analysis","Biomedical information mining","Interaction analysis","Bioinformatics","Protein-protein interactions","NAtural language processing","Natural language processing"],"max_cite":6.0,"pub_year":2004.0,"sources":"['scp']","rawkeys":["biological information","data mining","intelligent machine","biomedical information mining","machine learning","natural language processing","interaction analysis","natural language analysis","bioinformatics","software applications","protein-protein interactions","software application"],"tags":["biological information","data mining","intelligent machine","biomedical information mining","machine learning","natural language processing","interaction analysis","natural language analysis","bioinformatics","software applications","protein-protein interactions"]},{"p_id":31598,"title":"Legal retrieval as support to eMediation: matching disputant\u2019s case and court decisions","abstract":"\u00a9 2015, Springer Science+Business Media Dordrecht.The perspective of online dispute resolution (ODR) is to develop an online electronic system aimed at solving out-of-court disputes. Among ODR schemes, eMediation is becoming an important tool for encouraging the positive settlement of an agreement among litigants. The main motivation underlying the adoption of eMediation is the time\/cost reduction for the resolution of disputes compared to the ordinary justice system. In the context of eMediation, a fundamental requirement that an ODR system should meet relates to both litigants and mediators, i.e. to enable an informed negotiation by informing the parties about the rights and duties related to the case. In order to match this requirement, we propose an information retrieval system able to retrieve relevant court decisions with respect to the disputant case description. The proposed system combines machine learning and natural language processing techniques to better match disputant case descriptions (informal and concise) with court decisions (formal and verbose). Experimental results confirm the ability of the proposed solution to empower court decision retrieval, enabling therefore a well-informed eMediation process.","keywords_author":["eMediation","Information retrieval","Machine learning","Natural language processing"],"keywords_other":["Electronic systems","Court decisions","Case description","eMediation","Online dispute resolution","NAtural language processing"],"max_cite":3.0,"pub_year":2015.0,"sources":"['scp']","rawkeys":["electronic systems","case description","machine learning","natural language processing","information retrieval","emediation","court decisions","online dispute resolution"],"tags":["electronic systems","case description","machine learning","natural language processing","information retrieval","emediation","court decisions","online dispute resolution"]},{"p_id":50032,"title":"Quantifying mood, content and dynamics of health forums","abstract":"Copyright 2016 ACM.In this paper we examined content, mood and general dynamics of health forum discussions concerning vaccinations, genetically modified organisms (GMO) and a gluten-free diet and explored the ability to extract sentiment from social media. Using data from the social media website Reddit.com, we applied text mining techniques together with machine learning algorithms to derive insights. We used metadata from the source, text features, Latent Dirichlet Allocation (LDA) topic model outputs and manually annotated disposition labels that separate comments into affirmative or negative groups together with Gradient Boosted Models (GBM) to devise a set of disposition models inferring commentators' sentiment towards each topic and expand our understanding of relevant arguments. Manual annotation resulted in moderate interrater agreement of an average 0.48 Fleiss-Kappa. Despite that, the disposition models for each topic were able to achieve a balanced successful prediction rates of between 68% and 74% providing a considerably better than chance assessment of a commentator's disposition towards each topic. We observed changes in disposition over time and found areas of disagreement between the supporters and opponents of each topic. Despite the limitations associated with manual annotations, we obtained a wider view on the issues concerning the topics of interest than those offered by previous research.","keywords_author":["Classification","Gluten","GMO","Machine learning","Natural language processing","Opinion mining","Reddit","Text mining","Vaccinations"],"keywords_other":["Reddit","Text mining","Gluten","Vaccinations","NAtural language processing","Opinion mining"],"max_cite":0.0,"pub_year":2016.0,"sources":"['scp']","rawkeys":["text mining","machine learning","natural language processing","reddit","gmo","classification","vaccinations","opinion mining","gluten"],"tags":["text mining","machine learning","natural language processing","reddit","vaccination","gmo","classification","opinion mining","gluten"]},{"p_id":43889,"title":"Semantic classification of tweets: A contextual knowledge based approach for tweet classification","abstract":"\u00a9 2017 IEEE. In this paper we propose a novel approach and technique for tweet classification based on the Contextual Knowledge Structures (CKS). We first discover the popular, trending topics, then tap the web for the related, relevant content for the topics and harness the same to build CKS. CKS are built using text mining techniques and Computational Linguistics; they are relevant Subject-Predicate-Object triples that depict a specific topic or event. Since the tweets are sparse and most of them do not contain a hashtag, it is difficult to map them to a specific topic. We leverage the CKS to train the Na\u00efve Bayes (NB) classifier and achieve a semantic classification of user tweets. We evaluate the performance of our CKS based classifier by comparing it with the baseline Bag-of-Words (BOW) learning model. The CKS based NB classifier exhibits a consistent performance with an accuracy of approximately 94%. This approach has a two-fold advantage: a) A small training set of knowledge structures is effectively used for machine learning, b) The model adapts to the topic. Trending topics are automatically discovered, the associated CKS are built and the classifier is trained using these dynamic CKS. This model is dynamic, topic-adaptive and efficient.","keywords_author":["Contextual Knowledge Structures (CKS)","Machine Learning","Natural Language Processing (NLP)","Na\u00efve Bayes","Text Mining","Tweet Classification"],"keywords_other":["Text mining","Contextual knowledge","Learning models","Consistent performance","Trending topics","Text mining techniques","Knowledge structures","Semantic classification"],"max_cite":0.0,"pub_year":2018.0,"sources":"['scp']","rawkeys":["learning models","text mining","trending topics","na\u00efve bayes","semantic classification","knowledge structures","machine learning","natural language processing (nlp)","contextual knowledge structures (cks)","consistent performance","text mining techniques","contextual knowledge","tweet classification"],"tags":["learning models","text mining","trending topics","semantic classification","knowledge structures","machine learning","natural language processing","contextual knowledge structures (cks)","consistent performance","text mining techniques","contextual knowledge","naive bayes","tweet classification"]},{"p_id":50036,"title":"ArtViz: A web platform for artist data visualization and exploration","abstract":"Browsing through the available data nowadays can be a difficult task for the unexperienced users. Every day, the Internet is flooded by unstructured information that sometimes needs time to be extracted and analyzed. When it comes to art data, there aren't any concrete sources that offer consistent information about artists in an interactive way. In this paper we describe the prototype of ArtViz, a web platform that displays art linked data to unexperienced users and helps them discover new information through interactive visualization. Furthermore we will present additional useful features like natural language querying or recommending similar entities (artists).","keywords_author":["Data Visualization","DBPedia","Europeana","Linked Open Data","Machine Learning","Natural Language Processing","ULAN"],"keywords_other":["Linked open datum","ULAN","Europeana","NAtural language processing","Dbpedia"],"max_cite":0.0,"pub_year":2016.0,"sources":"['scp']","rawkeys":["europeana","linked open data","linked open datum","machine learning","data visualization","dbpedia","natural language processing","ulan"],"tags":["europeana","linked open data","linked open datum","machine learning","data visualization","dbpedia","natural language processing","ulan"]},{"p_id":29558,"title":"Rapidly scaling dialog systems with interactive learning","abstract":"\u00a9 Springer International Publishing Switzerland 2015. All rights are reserved.In personal assistant dialog systems, intent models are classifiers that identify the intent of a user utterance, such as to add a meeting to a calendar or get the director of a stated movie. Rapidly adding intents is one of the main bottlenecks to scaling-adding functionality to-personal assistants. In this paper we show how interactive learning can be applied to the creation of statistical intent models. Interactive learning (Simard, ICE: enabling non-experts to build models interactively for large-scale lopsided problems, 2014) combines model definition, labeling, model building, active learning, model evaluation, and feature engineering in a way that allows a domain expert-who need not be a machine learning expert- to build classifiers.We apply interactive learning to build a handful of intent models in three different domains. In controlled lab experiments, we show that intent detectors can be built using interactive learning and then improved in a novel end-toend visualization tool. We then applied this method to a publicly deployed personal assistant-Microsoft Cortana-where a non-machine learning expert built an intent model in just over 2 h, yielding excellent performance in the commercial service.","keywords_author":["Active learning","Interactive learning","Language understanding","Machine learning","Machine teaching","Natural language processing","Spoken dialog systems","Spoken language understanding"],"keywords_other":["Spoken language understanding","Language understanding","Spoken dialog systems","Interactive learning","Active Learning"],"max_cite":4.0,"pub_year":2015.0,"sources":"['scp', 'ieee']","rawkeys":["machine teaching","spoken dialog systems","machine learning","active learning","natural language processing","interactive learning","spoken language understanding","language understanding"],"tags":["machine teaching","spoken dialog systems","natural language processing","machine learning","interactive learning","spoken language understanding","language understanding"]},{"p_id":31610,"title":"A hybrid approach to pronominal anaphora resolution in Arabic","abstract":"\u00a9 2015 Abdullatif Abolohom and Nazlia Omar.One of the challenges in natural language processing is to determine which pronouns to be referred to their intended referents in the discourse. Performing anaphora resolution is considered as an important task for a number of natural language processing applications such as information extraction, question answering and text summarization. Most of the earlier works of anaphora resolution have been applied to English and other languages. However, the work done in Arabic is not sufficiently studied. In this study, a hybrid approach that combines different architectures for resolving pronominal anaphora in Arabic language is presented. The hybrid model adopted the strategy based on the combination of a rule-based and machine learning approach. The collection of anaphora and respective possible antecedents was identified in a rule-based manner with morphological information taken into account. In addition, the selection of the most probable candidate as the antecedent of the anaphor was done by machine learning based on a k-Nearest Neighbor (k-NN) approach. In this study, the appropriate features to be used in this task were determined and their effect on the performance of anaphora resolution was investigated. Experiments of the proposed method were performed using the corpus of the Quran annotated with pronominal anaphora. The experimental results indicate that the proposed hybrid approach is completely reasonable and feasible for Arabic pronominal anaphora resolution.","keywords_author":["Anaphora resolution","Machine learning approach","Natural language processing","Rule-based approach"],"keywords_other":null,"max_cite":3.0,"pub_year":2015.0,"sources":"['scp']","rawkeys":["natural language processing","rule-based approach","machine learning approach","anaphora resolution"],"tags":["natural language processing","rule-based approach","anaphora resolution","machine learning approaches"]},{"p_id":31613,"title":"A random forest approach for authorship profiling","abstract":"In this paper we present our approach to extract profile information from anonymized tweets for the author profiling task at PAN 2015 [10]. Particularly we explore the versatility of random forest classifiers for the genre and age groups information and random forest regressions to score important aspects of the personality of a user. Furthermore we propose a set of features tailored for this task based on characteristics of the twitters. In particular, our approach relies on previous proposed features for sentiment analysis tasks.","keywords_author":["Author profiling","Machine learning","NLP","Random forest","Random forest regression"],"keywords_other":["Random forests","Task-based","Sentiment analysis","Age groups","Random forest classifier","Author profiling"],"max_cite":3.0,"pub_year":2015.0,"sources":"['scp']","rawkeys":["task-based","nlp","age groups","random forest regression","machine learning","random forest classifier","random forests","author profiling","sentiment analysis","random forest"],"tags":["task-based","random forest regression","natural language processing","machine learning","random forests","random forest classifier","author profiling","sentiment analysis","age groups"]},{"p_id":50045,"title":"The utility of including pathology reports in improving the computational identification of patients","abstract":"\u00a9 2016 Journal of Pathology Informatics | Published by Wolters Kluwer -Medknow. Background: Celiac disease (CD) is a common autoimmune disorder. Efficient identification of patients may improve chronic management of the disease. Prior studies have shown searching International Classification of Diseases-9 (ICD-9) codes alone is inaccurate for identifying patients with CD. In this study, we developed automated classification algorithms leveraging pathology reports and other clinical data in Electronic Health Records (EHRs) to refine the subset population preselected using ICD-9 code (579.0). Materials and Methods: EHRs were searched for established ICD-9 code (579.0) suggesting CD, based on which an initial identification of cases was obtained. In addition, laboratory results for tissue transglutaminse were extracted. Using natural language processing we analyzed pathology reports from upper endoscopy. Twelve machine learning classifiers using different combinations of variables related to ICD-9 CD status, laboratory result status, and pathology reports were experimented to find the best possible CD classifier. Ten-fold cross-validation was used to assess the results. Results: A total of 1498 patient records were used including 363 confirmed cases and 1135 false positive cases that served as controls. Logistic model based on both clinical and pathology report features produced the best results: Kappa of 0.78, F1 of 0.92, and area under the curve (AUC) of 0.94, whereas in contrast using ICD-9 only generated poor results: Kappa of 0.28, F1 of 0.75, and AUC of 0.63. Conclusion: Our automated classification system presented an efficient and reliable way to improve the performance of CD patient identification.","keywords_author":["Celiac","classification","machine learning","natural language processing","patient registry"],"keywords_other":null,"max_cite":0.0,"pub_year":2016.0,"sources":"['scp']","rawkeys":["natural language processing","machine learning","celiac","classification","patient registry"],"tags":["natural language processing","machine learning","celiac","classification","patient registry"]},{"p_id":52100,"title":"Extracting domain-specific opinion words for sentiment analysis","abstract":"In this paper, we consider opinion word extraction, one of the key problems in sentiment analysis. Sentiment analysis (or opinion mining) is an important research area within computational linguistics. Opinion words, which form an opinion lexicon, describe the attitude of the author towards certain opinion targets, i.e., entities and their attributes on which opinions have been expressed. Hence, the availability of a representative opinion lexicon can facilitate the extraction of opinions from texts. For this reason, opinion word mining is one of the key issues in sentiment analysis. We designed and implemented several methods for extracting opinion words. We evaluated these approaches by testing how well the resulting opinion lexicons help improve the accuracy of methods for determining the polarity of the reviews if the extracted opinion words are used as features. We used several machine learning methods: SVM, Logistic Regression, Na\u00efve Bayes, and KNN. By using the extracted opinion words as features we were able to improve over the baselines in some cases. Our experiments showed that, although opinion words are useful for polarity detection, they are not sufficient on their own and should be used only in combination with other features. \u00a9 2013 Springer-Verlag.","keywords_author":["Machine Learning","Natural Language Processing","Sentiment Analysis"],"keywords_other":["Opinion targets","Key Issues","Sentiment analysis","Machine learning methods","Logistic regressions","Domain specific","NAtural language processing","Opinion mining"],"max_cite":0.0,"pub_year":2013.0,"sources":"['scp']","rawkeys":["machine learning methods","machine learning","natural language processing","logistic regressions","domain specific","key issues","opinion targets","opinion mining","sentiment analysis"],"tags":["machine learning methods","machine learning","natural language processing","logistic regressions","domain specific","key issues","opinion targets","opinion mining","sentiment analysis"]},{"p_id":37767,"title":"Deep learning based parts of speech tagger for Bengali","abstract":"\u00a9 2016 IEEE. This paper describes the Part of Speech (POS) tagger for Bengali Language. Here, POS tagging is the process of assigning the part of speech tag or other lexical class marker to each and every word in a sentence. In many Natural Language Processing (NLP) applications, POS tagging is considered as the one of the basic necessary tools. Identifying the ambiguities in language lexical items is the challenging objective in the process of developing an efficient and accurate POS Tagger. Different methods of automating the process have been developed and employed for Bengali. In this paper, we report about our work on building POS tagger for Bengali using the Deep Learning. Bengali is a morphologically rich language and our taggers make use of morphological and contextual information of the words. It is observed from the experiments based on Linguistic Data Consortium (LDC) catalog number LDC2010T16 and ISBN 1-58563-561-8 corpus that 93.33% accuracy is obtained for Bengali POS tagger using the Deep Learning.","keywords_author":["Deep Belief Network","Deep Learning","Linear Activation Function","Part of Speech Tagging"],"keywords_other":["Deep learning","Part of speech tagging","Bengali language","NAtural language processing","Contextual information","Deep belief networks","Linguistic data consortiums","Linear activation function"],"max_cite":1.0,"pub_year":2016.0,"sources":"['scp']","rawkeys":["linguistic data consortiums","contextual information","deep learning","natural language processing","bengali language","part of speech tagging","deep belief network","linear activation function","deep belief networks"],"tags":["linguistic data consortiums","contextual information","machine learning","natural language processing","bengali language","part of speech tagging","linear activation function","deep belief networks"]},{"p_id":39816,"title":"On the role of poetic versus nonpoetic features in \"kindred\" and diachronic poetry attribution","abstract":"Author attribution studies have demonstrated remarkable success in applying orthographic and lexicographic features of text in a variety of discrimination problems. What might poetic features, such as syllabic stress and mood, contribute? We address this question in the context of two different attribution problems: (a) kindred: differentiate Langston Hughes' early poems from those of kindred poets and (b) diachronic: differentiate Hughes' early from his later poems. Using a diverse set of 535 generic text features, each categorized as poetic or nonpoetic, correlation-based greedy forward search ranked the features and a support vector machine classified the poems. A small subset of features (\u223c10) achieved cross-validated precision and recall as high as 87%. Poetic features (rhyme patterns particularly) were nearly as effective as nonpoetic in kindred discrimination, but less effective diachronically. In other words, Hughes used both poetic and nonpoetic features in distinctive ways and his use of nonpoetic features evolved systematically while he continued to experiment with poetic features. These findings affirm qualitative studies attesting to structural elements from Black oral tradition and Black folk music (blues) and to the internal consistency of Hughes' early poetry. \u00a9 2012 ASIS&T.","keywords_author":["computational linguistics","machine learning","natural language processing"],"keywords_other":["Oral tradition","Qualitative study","Internal consistency","Discrimination problem","Greedy forward","Structural elements","Precision and recall","Text feature","NAtural language processing"],"max_cite":1.0,"pub_year":2012.0,"sources":"['scp']","rawkeys":["qualitative study","oral tradition","greedy forward","machine learning","natural language processing","internal consistency","precision and recall","text feature","computational linguistics","structural elements","discrimination problem"],"tags":["qualitative study","oral tradition","greedy forward","machine learning","natural language processing","internal consistency","precision and recall","text feature","computational linguistics","structural elements","discrimination problem"]},{"p_id":31628,"title":"DistDL: A distributed deep learning service schema with GPU accelerating","abstract":"\u00a9 Springer International Publishing Switzerland 2015.Deep Learning is a hot topic developed by the industry and academia which integrates the broad field of artificial intelligence with the deployment of deep neural networks in the big data era. Recently, the capability to train neural networks has resulted in state-of-the-art performance in many domains such as computer vision, speech recognition, recommended system, natural language processing, drug discovery and behavioural analysis etc. However, existing deep learning systems are inadequate for scaling, especially in current cloud infrastructures where the nodes are distributed across multiple geographical location. The tendency is evolving that deep learning must be collaborative optimized by the fields both machine learning and systems. In this paper, we have presented DistDL, a novel distributed deep learning service schema in order to reduce the training time consumption along with communication overhead and achieve extremely parallelism with data and model. Additionally, we also took into consideration GPUs inside DistDL by leveraging the remarkable competency of heterogeneous computation. The results of our experiments in the benchmarks suggest that DistDL is adaptive to various scaling patterns with the same accuracy while minimising training time by adopting the GPU, up to 80% speed up.","keywords_author":["Data partition","Deep learning","GPU","Model parallelism","Spark"],"keywords_other":["Deep learning","State-of-the-art performance","Communication overheads","Data partition","Heterogeneous computation","Geographical locations","GPU","NAtural language processing"],"max_cite":3.0,"pub_year":2015.0,"sources":"['scp']","rawkeys":["gpu","deep learning","data partition","geographical locations","heterogeneous computation","natural language processing","model parallelism","communication overheads","spark","state-of-the-art performance"],"tags":["graphics processing units","geographic location","data partition","machine learning","heterogeneous computing","natural language processing","model parallelism","communication overheads","spark","state-of-the-art performance"]},{"p_id":37773,"title":"Learning Semantic Representations for Rating Vietnamese Comments","abstract":"\u00a9 2016 IEEE. Opinion mining and sentiment analysis has recently become a hot topic in the field of natural language processing and text mining. This paper addresses the problem of overall rating for comments in Vietnamese language. The traditional approach of using bag-of-words for feature representation would cause a very high dimensional feature space and doesn't reflect relationship between words. To capture more linguistic information, this paper provides a new neural network model containing three layers: (1) word embedding; (2) comment representation (i.e. comment feature vector); and (3) comment rating prediction. In which, the word embedding layer is designed to learn word embeddings which can capture semantic and syntactic relations between words, the second layer uses a semantic composition model for comment representation, and the third layer is designed as a perceptron and it stands for predicting overall rating of a comment. In experiment, we use a Vietnamese data set which contains comments on the domain of mobile phone products. Experimental results show that our proposed model outperforms traditional neural network models with comment representations based on bag of word model or word vector averaging.","keywords_author":["Comment rating prediction","Deep learning","Neural network","Semantic composition model","Sentiment analysis"],"keywords_other":["Deep learning","Linguistic information","Traditional approaches","Semantic composition","Sentiment analysis","Feature representation","High-dimensional feature space","NAtural language processing"],"max_cite":1.0,"pub_year":2016.0,"sources":"['scp']","rawkeys":["semantic composition","neural network","traditional approaches","deep learning","comment rating prediction","natural language processing","high-dimensional feature space","feature representation","linguistic information","sentiment analysis","semantic composition model"],"tags":["semantic composition","traditional approaches","neural networks","comment rating prediction","machine learning","natural language processing","high-dimensional feature space","feature representation","linguistic information","sentiment analysis","semantic composition model"]},{"p_id":31630,"title":"Information extraction from clinical documents: Towards disease\/disorder template filling","abstract":"\u00a9 Springer International Publishing Switzerland 2015.In recent years there has been an increase in the generation of electronic health records (EHRs), which lead to an increased scope for research on biomedical literature. Many research works have been using various NLP, information retrieval and machine learning techniques to extract information from these records. In this paper, we provide a methodology to extract information for understanding the status of the disease\/disorder. The status of disease\/disorder is based on different attributes like temporal information, severity and progression of the disease. Here, we consider ten attributes that allow us to understand the majority details regarding the status of the disease\/disorder. They are Negation Indicator, Subject Class, Uncertainty Indicator, Course Class, Severity Class, Conditional Class, Generic Class, Body Location, DocTime Class, and Temporal Expression. In this paper, we present rule-based and machine learning approaches to identify each of these attributes and evaluate our system on attribute level and system level accuracies. This project was done as a part of the ShARe\/CLEF eHealth Evaluation Lab 2014. We were able to achieve state-of-art accuracy (0.868) in identifying normalized values of the attributes.","keywords_author":["Apache cTAKES","Information extraction","Machine learning","NLP","Relation extraction","Unified Medical Langugae System (UMLS)"],"keywords_other":["Relation extraction","Biomedical literature","Machine learning approaches","NLP","Unified Medical Langugae System (UMLS)","Apache cTAKES","Electronic health record (EHRs)","Machine learning techniques"],"max_cite":3.0,"pub_year":2015.0,"sources":"['scp']","rawkeys":["nlp","apache ctakes","information extraction","machine learning techniques","machine learning","unified medical langugae system (umls)","machine learning approaches","electronic health record (ehrs)","relation extraction","biomedical literature"],"tags":["apache ctakes","information extraction","machine learning techniques","machine learning","electronic health records","natural language processing","unified medical langugae system (umls)","machine learning approaches","relation extraction","biomedical literature"]},{"p_id":37778,"title":"A comprehensive study of classification techniques for sarcasm detection on textual data","abstract":"\u00a9 2016 IEEE.During the last decade majority of research has been carried out in the area of sentiment Analysis of textual data available on the web. Sentiment Analysis has its challenges, and one of them is Sarcasm. Classification of sarcastic sentences is a difficult task due to representation variations in the textual form sentences. This can affect many Natural Language Processing based applications. Sarcasm is the kind of representation to convey the different sentiment than presented. In our study we have tried to identify different supervised classification techniques mainly used for sarcasm detection and their features. Also we have analyzed results of the classification techniques, on textual data available in various languages on review related sites, social media sites and micro-blogging sites. Furthermore, for each method studied, our paper presents the analysis of data set generation and feature selection process used thereof. We also carried out preliminary experiment to detect sarcastic sentences in 'Hindi' language. We trained SVM classifier with 10X validation with simple Bag-Of-Words as features and TF-IDF as frequency measure of the feature. We found that this simple model based on 'bag-of-words' feature accurately classified 50% of sarcastic sentences. Thus, primary experiment has revealed the fact that simple Bag-of-Words are not sufficient for sarcasm detection.","keywords_author":["Machine Learning","Natural Language Processing","Opinion Mining","Sarcasm Detection","Sentiment Analysis","Text Processing"],"keywords_other":["Classification technique","Supervised classification","Frequency measures","Analysis of data","Sentiment analysis","NAtural language processing","Simple modeling","Opinion mining"],"max_cite":1.0,"pub_year":2016.0,"sources":"['scp']","rawkeys":["supervised classification","simple modeling","frequency measures","sarcasm detection","natural language processing","machine learning","classification technique","analysis of data","opinion mining","sentiment analysis","text processing"],"tags":["supervised classification","simple modeling","sarcasm detection","natural language processing","machine learning","classification technique","analysis of data","frequency measurement","opinion mining","sentiment analysis","text processing"]},{"p_id":52115,"title":"Automatic text analysis by artificial intelligence","abstract":"Text is one of the traditional ways of communication between people. With the growing availability of text data in electronic form, handling and analysis of text by means of computers gained popularity. Handling text data with machine learning methods brought interesting challenges to the area that got further extended by incorporation of some natural language specifics. As the methods were capable of addressing more complex problems related to text data, the expectations become bigger calling for more sophisticated methods, in particular a combination of methods from different research areas including information retrieval, machine learning, statistical data analysis, data mining, natural language processing, semantic technologies. Automatic text analysis become an integral part of many systems, pushing boundaries of research capabilities towards what one can refer to as an artificial intelligence dream - never ending learning from text aiming at mimicking ways of human learning. The paper presents development of text analysis research in Slovenian that we have been personally involved in, pointing out interesting research problems that have been and are still addressed by the research, example tasks that have been addressed and some challenges on the way.","keywords_author":["Big data","Data visualization","Machine learning","Natural language processing","Text mining"],"keywords_other":["Research capabilities","Semantic technologies","Text mining","Statistical data analysis","Machine learning methods","Big datum","NAtural language processing","Learning from texts"],"max_cite":0.0,"pub_year":2013.0,"sources":"['scp']","rawkeys":["machine learning methods","text mining","learning from texts","big data","statistical data analysis","machine learning","data visualization","natural language processing","semantic technologies","big datum","research capabilities"],"tags":["machine learning methods","text mining","learning from texts","big data","statistical data analysis","machine learning","data visualization","natural language processing","semantic technologies","big datum","research capabilities"]},{"p_id":21399,"title":"Computational irony: A survey and new perspectives","abstract":"\u00a9 Springer Science+Business Media Dordrecht 2013.Irony is a fundamental rhetorical device. It is a uniquely human mode of communication, curious in that the speaker says something other than what he or she intends. Recently, computationally detecting irony has attracted attention from the natural language processing (NLP) and machine learning (ML) communities. While some progress has been made toward this end, I argue that current machine learning methods rely too heavily on shallow, unstructured, syntactic modeling of text to consistently discern ironic intent. Irony detection is an interesting machine learning problem because, in contrast to most text classification tasks, it requires a semantics that cannot be inferred directly from word counts over documents alone. To support this position, I survey the large body of existing philosophical\/literary work investigating ironic communication. I then survey more recent computational efforts to operationalize irony detection in the fields of NLP and ML. I identify the disparities of the latter with respect to the former. Specifically, I highlight a major conceptual problem in all existing computational models of irony: none maintain an explicit model of the speaker\/environment. I argue that without such an internal model of the speaker, irony detection is hopeless, as this model is necessary to represent expectations, which play a key role in ironic communication. I sketch possible means of embedding such models into computational approaches to irony detection. In particular, I introduce the pragmatic context model, which looks to operationalize computationally existing theories of irony. This work is a step toward unifying work on irony from literary, empirical and philosophical perspectives with modern computational models.","keywords_author":["Irony","Machine learning","Representation"],"keywords_other":["Machine learning problem","Computational effort","Conceptual problems","Irony","Machine learning methods","Representation","NAtural language processing","Computational approach"],"max_cite":14.0,"pub_year":2015.0,"sources":"['scp']","rawkeys":["machine learning methods","machine learning","natural language processing","representation","computational effort","conceptual problems","machine learning problem","computational approach","irony"],"tags":["machine learning methods","machine learning","natural language processing","representation","computational effort","conceptual problems","machine learning problem","computational approach","irony"]},{"p_id":33693,"title":"Microbial phenomics information extractor (MicroPIE): A natural language processing tool for the automated acquisition of prokaryotic phenotypic characters from text sources","abstract":"\u00ef\u00bf\u00bd 2016 The Author(s).Background: The large-scale analysis of phenomic data (i.e., full phenotypic traits of an organism, such as shape, metabolic substrates, and growth conditions) in microbial bioinformatics has been hampered by the lack of tools to rapidly and accurately extract phenotypic data from existing legacy text in the field of microbiology. To quickly obtain knowledge on the distribution and evolution of microbial traits, an information extraction system needed to be developed to extract phenotypic characters from large numbers of taxonomic descriptions so they can be used as input to existing phylogenetic analysis software packages. Results: We report the development and evaluation of Microbial Phenomics Information Extractor (MicroPIE, version 0.1.0). MicroPIE is a natural language processing application that uses a robust supervised classification algorithm (Support Vector Machine) to identify characters from sentences in prokaryotic taxonomic descriptions, followed by a combination of algorithms applying linguistic rules with groups of known terms to extract characters as well as character states. The input to MicroPIE is a set of taxonomic descriptions (clean text). The output is a taxon-by-character matrix-with taxa in the rows and a set of 42 pre-defined characters (e.g., optimum growth temperature) in the columns. The performance of MicroPIE was evaluated against a gold standard matrix and another student-made matrix. Results show that, compared to the gold standard, MicroPIE extracted 21 characters (50%) with a Relaxed F1 score > 0.80 and 16 characters (38%) with Relaxed F1 scores ranging between 0.50 and 0.80. Inclusion of a character prediction component (SVM) improved the overall performance of MicroPIE, notably the precision. Evaluated against the same gold standard, MicroPIE performed significantly better than the undergraduate students. Conclusion: MicroPIE is a promising new tool for the rapid and efficient extraction of phenotypic character information from prokaryotic taxonomic descriptions. However, further development, including incorporation of ontologies, will be necessary to improve the performance of the extraction for some character types.","keywords_author":["Algorithm evaluation","Character matrices","Information extraction","Machine learning","Microbial phenotypes","Natural language processing","Phenotypic data extraction","Prokaryotic taxonomic descriptions","Support vector machine","Text mining"],"keywords_other":["Text mining","Algorithms","Computational Biology","Phenotype","Algorithm evaluation","Prokaryotic Cells","Biological Evolution","Information Storage and Retrieval","Phenotypic data","Microbial phenotypes","Data Mining","Automation","Bacteria","NAtural language processing","Prokaryotic taxonomic descriptions"],"max_cite":2.0,"pub_year":2016.0,"sources":"['scp']","rawkeys":["prokaryotic cells","phenotype","support vector machine","phenotypic data extraction","text mining","information extraction","character matrices","machine learning","algorithms","information storage and retrieval","microbial phenotypes","phenotypic data","data mining","bacteria","algorithm evaluation","computational biology","biological evolution","natural language processing","automation","prokaryotic taxonomic descriptions"],"tags":["automated","prokaryotic cells","phenotype","phenotypic data extraction","text mining","information extraction","character matrices","machine learning","algorithms","information storage and retrieval","microbial phenotypes","phenotypic data","data mining","bacteria","algorithm evaluation","computational biology","biological evolution","natural language processing","prokaryotic taxonomic descriptions"]},{"p_id":9118,"title":"Selecting attributes for sentiment classification using feature relation networks","abstract":"A major concern when incorporating large sets of diverse n-gram features for sentiment classification is the presence of noisy, irrelevant, and redundant attributes. These concerns can often make it difficult to harness the augmented discriminatory potential of extended feature sets. We propose a rule-based multivariate text feature selection method called Feature Relation Network (FRN) that considers semantic information and also leverages the syntactic relationships between n-gram features. FRN is intended to efficiently enable the inclusion of extended sets of heterogeneous n-gram features for enhanced sentiment classification. Experiments were conducted on three online review testbeds in comparison with methods used in prior sentiment classification research. FRN outperformed the comparison univariate, multivariate, and hybrid feature selection methods; it was able to select attributes resulting in significantly better classification accuracy irrespective of the feature subset sizes. Furthermore, by incorporating syntactic information about n-gram relations, FRN is able to select features in a more computationally efficient manner than many multivariate and hybrid techniques. \u00a9 2006 IEEE.","keywords_author":["affective computing","machine learning","Natural language processing","subspace selection","text mining"],"keywords_other":["Affective Computing","Text mining","Machine-learning","Subspace selection","NAtural language processing"],"max_cite":80.0,"pub_year":2011.0,"sources":"['scp', 'wos']","rawkeys":["subspace selection","text mining","affective computing","natural language processing","machine learning","machine-learning"],"tags":["subspace selection","text mining","affective computing","natural language processing","machine learning"]},{"p_id":50078,"title":"Automatic Evaluation Methods of Trainee's Answers to Develop a 4R Risk Prediction Training System","abstract":"\u00a9 2015 IEEE. 4 Rounds (4R) training method is practiced in industrial office site for reducing accidents caused by human factors. The 4R method enables to raise hazard-prediction capability of worker such as coping, decision-making to avoid danger situation. The workers as trainees train on their own by finding hazards which lurked in the hazard prediction training (KYT in Japanese) sheet. However, there is a large problem that a single trainee cannot train oneself using 4R method because the training of 4R method needs instruction of expert as human instructor. To solve that problem, we aim to develop hazard prediction training system. The advantage of this system enables trainee to train oneself anytime\/anywhere using 4R method. In this research paper, we reports about our proposal of training system, the development of subsystem which based on machine learning to evaluate trainee's answer correct or not, and reports the result of evaluation experimental that showed the average accuracy was 63.0\u00b122.9 [%].","keywords_author":["e-learning","machine learning","natural language processing","training"],"keywords_other":["Training Systems","Training methods","Large problems","Research papers","Hazard prediction","Risk predictions","Automatic evaluation","NAtural language processing"],"max_cite":0.0,"pub_year":2016.0,"sources":"['scp']","rawkeys":["large problems","training","automatic evaluation","machine learning","natural language processing","risk predictions","hazard prediction","research papers","training methods","training systems","e-learning"],"tags":["large problems","training","automatic evaluation","machine learning","natural language processing","risk predictions","hazard prediction","research papers","training methods","training systems","e-learning"]},{"p_id":23456,"title":"The role of fine-grained annotations in supervised recognition of risk factors for heart disease from EHRs","abstract":"\u00a9 2015 Elsevier Inc.This paper describes a supervised machine learning approach for identifying heart disease risk factors in clinical text, and assessing the impact of annotation granularity and quality on the system's ability to recognize these risk factors. We utilize a series of support vector machine models in conjunction with manually built lexicons to classify triggers specific to each risk factor. The features used for classification were quite simple, utilizing only lexical information and ignoring higher-level linguistic information such as syntax and semantics. Instead, we incorporated high-quality data to train the models by annotating additional information on top of a standard corpus. Despite the relative simplicity of the system, it achieves the highest scores (micro- and macro-F1, and micro- and macro-recall) out of the 20 participants in the 2014 i2b2\/UTHealth Shared Task. This system obtains a micro- (macro-) precision of 0.8951 (0.8965), recall of 0.9625 (0.9611), and F1-measure of 0.9276 (0.9277). Additionally, we perform a series of experiments to assess the value of the annotated data we created. These experiments show how manually-labeled negative annotations can improve information extraction performance, demonstrating the importance of high-quality, fine-grained natural language annotations.","keywords_author":["Machine learning","Natural language annotation","Natural language processing"],"keywords_other":["Humans","Vocabulary, Controlled","Natural languages","High quality data","Support vector machine models","Aged","Pattern Recognition, Automated","Lexical information","Negative annotations","Coronary Artery Disease","Female","Cohort Studies","Comorbidity","Diabetes Complications","Narration","Longitudinal Studies","Maryland","Incidence","NAtural language processing","Confidentiality","Linguistic information","Male","Risk Assessment","Electronic Health Records","Supervised Machine Learning","Computer Security","Middle Aged","Natural Language Processing","Data Mining","Supervised machine learning"],"max_cite":10.0,"pub_year":2015.0,"sources":"['scp']","rawkeys":["lexical information","negative annotations","vocabulary","automated","computer security","natural languages","aged","maryland","high quality data","comorbidity","machine learning","electronic health records","middle aged","risk assessment","natural language annotation","linguistic information","diabetes complications","narration","cohort studies","data mining","incidence","humans","confidentiality","controlled","longitudinal studies","male","natural language processing","support vector machine models","coronary artery disease","pattern recognition","supervised machine learning","female"],"tags":["lexical information","negative annotations","vocabulary","automated","computer security","natural languages","aged","maryland","high quality data","comorbidity","computer-aided diagnosis","control","electronic health records","machine learning","middle aged","risk assessment","natural language annotation","linguistic information","diabetes complications","narration","cohort studies","data mining","incidence","humans","confidentiality","longitudinal studies","male","natural language processing","support vector machine models","pattern recognition","supervised machine learning","female"]},{"p_id":23458,"title":"A hybrid model for automatic identification of risk factors for heart disease","abstract":"\u00a9 2015 Elsevier Inc.Coronary artery disease (CAD) is the leading cause of death in both the UK and worldwide. The detection of related risk factors and tracking their progress over time is of great importance for early prevention and treatment of CAD. This paper describes an information extraction system that was developed to automatically identify risk factors for heart disease in medical records while the authors participated in the 2014 i2b2\/UTHealth NLP Challenge. Our approaches rely on several nature language processing (NLP) techniques such as machine learning, rule-based methods, and dictionary-based keyword spotting to cope with complicated clinical contexts inherent in a wide variety of risk factors. Our system achieved encouraging performance on the challenge test data with an overall micro-averaged F-measure of 0.915, which was competitive to the best system (F-measure of 0.927) of this challenge task.","keywords_author":["Clinical text mining","Heart disease","Hybrid model","Machine learning","Natural language processing","Risk factors","Rule-based approach"],"keywords_other":["Text mining","Humans","Heart disease","Vocabulary, Controlled","United Kingdom","Risk factors","Aged","Pattern Recognition, Automated","Hybrid model","Rule-based approach","Coronary Artery Disease","Female","Cohort Studies","Comorbidity","Narration","Longitudinal Studies","Incidence","NAtural language processing","Confidentiality","Male","Risk Assessment","Electronic Health Records","Computer Security","Middle Aged","Natural Language Processing","Data Mining"],"max_cite":10.0,"pub_year":2015.0,"sources":"['scp', 'wos']","rawkeys":["vocabulary","heart disease","automated","computer security","aged","rule-based approach","female","comorbidity","text mining","machine learning","electronic health records","middle aged","clinical text mining","risk assessment","narration","cohort studies","data mining","humans","confidentiality","controlled","hybrid model","united kingdom","risk factors","longitudinal studies","male","natural language processing","coronary artery disease","pattern recognition","incidence"],"tags":["vocabulary","heart disease","automated","computer security","aged","rule-based approach","female","comorbidity","text mining","computer-aided diagnosis","control","electronic health records","machine learning","clinical text mining","middle aged","risk assessment","narration","cohort studies","data mining","humans","confidentiality","hybrid model","united kingdom","risk factors","longitudinal studies","male","natural language processing","pattern recognition","incidence"]},{"p_id":37794,"title":"Stock price forecasting via sentiment analysis on Twitter","abstract":"\u00a9 2016. Stock price forecasting is an important and thriving topic in financial engineering especially since new techniques and approaches on this matter are gaining ground constantly. In the contemporary era, the ceaseless use of social media has reached unprecedented levels, which has led to the belief that the expressed public sentiment could be correlated with the behavior of stock prices. The idea is to recognize patterns which confirm this correlation and use them to predict the future behavior of the various stock prices. With no doubt, though uninteresting individually, tweets can provide a satisfactory reection of public sentiment when taken in aggregate. In this paper, we develop a system which collects past tweets, processes them further, and examines the effectiveness of various machine learning techniques such as Naive Bayes Bernoulli classification and Support Vector Machine (SVM), for providing a positive or negative sentiment on the tweet corpus. Subsequently, we employ the same machine learning algorithms to analyze how tweets correlate with stock market price behavior. Finally, we examine our prediction's error by comparing our algorithm's outcome with next day's actual close price. Overall, the ultimate goal of this project is to forecast how the market will behave in the future via sentiment analysis on a set of tweets over the past few days, as well as to examine if the theory of contrarian investing is applicable. The final results seem to be promising as we found correlation between sentiment of tweets and stock prices.","keywords_author":["Machine learning","NLP","Sentiment analysis","Stock market prediction","Twitter"],"keywords_other":["Twitter","Stock price forecasting","Stock market prediction","Sentiment analysis","Negative sentiments","Financial engineering","Stock market prices","Machine learning techniques"],"max_cite":1.0,"pub_year":2016.0,"sources":"['scp']","rawkeys":["nlp","stock market prediction","machine learning techniques","machine learning","financial engineering","negative sentiments","stock market prices","stock price forecasting","sentiment analysis","twitter"],"tags":["stock market prediction","machine learning techniques","natural language processing","machine learning","financial engineering","negative sentiments","stock market prices","stock price forecasting","sentiment analysis","twitter"]},{"p_id":932,"title":"Finding expert authors in financial forum using deep learning methods","abstract":"The modern stock market is a popular place to increase wealth and generate income, but the fundamental problem of when to buy or sell shares, or which stocks to buy has not been solved. It is very common among investors to have professional financial advisers, but what is the best resource to support the decisions these people make? Investment banks, such as Goldman Sachs, Lehman Brothers, and Salomon Brothers have dominated the world of financial advice for decades. However, due to the popularity of the Internet and financial social networks, such as StockTwits and Seeking Alpha, investors around the world have a new opportunity to gather and share their experiences. This raises new questions: is the information these users provide trustworthy? How can we find the experts? In this paper, we seek to determine if neural network models can help us find the experts in a set of StockTwits tweets. We applied two neural network models - doc2vec and convolutional neural networks - to find top authors in StockTwits based on their messages. Our results showed that a convolutional neural network is the best model to predict such top authors in this data set.","keywords_author":["Deep Learning","information retrieval","natural language processing","Deep Learning","information retrieval","natural language processing"],"keywords_other":["income","financial advice","doc2vec","professional financial advisers","Lehman brothers","Goldman Sachs","social networking (online)","neural network models","investors","Biological neural networks","convolutional neural network","Lehman Brothers","financial data processing","Internet","Learning methods","Salomon Brothers","Financial forums","Stock markets","stocks","wealth","Financial advisers","investment banks","Convolutional neural networks","feedforward neural nets","learning (artificial intelligence)","stock markets","Investment bank","deep learning methods","Social network services","Memory architecture","financial forum","fundamental problem","belief networks","shares","modern stock market","StockTwits tweets","Logistics","popular place","Convolutional neural network","expert authors","financial social networks","Neural network model","data analysis"],"max_cite":0.0,"pub_year":2018.0,"sources":"['scp', 'ieee']","rawkeys":["doc2vec","financial advice","income","professional financial advisers","social networking (online)","learning methods","investors","neural network models","salomon brothers","investment bank","memory architecture","convolutional neural network","financial data processing","goldman sachs","internet","financial advisers","logistics","social network services","stocks","wealth","biological neural networks","investment banks","feedforward neural nets","learning (artificial intelligence)","convolutional neural networks","deep learning","stock markets","information retrieval","lehman brothers","deep learning methods","financial forum","fundamental problem","neural network model","belief networks","shares","modern stock market","stocktwits tweets","financial forums","natural language processing","popular place","expert authors","financial social networks","data analysis"],"tags":["doc2vec","financial advice","income","professional financial advisers","stock","learning methods","investors","salomon brothers","social networks","investment bank","memory architecture","convolutional neural network","financial data processing","goldman sachs","internet","financial advisers","logistics","machine learning","social network services","wealth","biological neural networks","feedforward neural nets","information retrieval","lehman brothers","deep learning methods","financial forum","fundamental problem","neural network model","belief networks","shares","modern stock market","stocktwits tweets","stock market","natural language processing","popular place","expert authors","financial social networks","data analysis"]},{"p_id":52131,"title":"Intelligent classroom system for qualitative analysis of students' conceptual understanding","abstract":"With the increase of ubiquitous data all over the internet, intelligent classroom systems that integrate traditional learning techniques with modern e-learning tools have become quite popular and necessary today. Although a substantial amount of work has been done in the field of e-learning, specifically in automation of objective question and answer evaluation, personalized learning, adaptive evaluation systems, the field of qualitative analysis of a student's subjective paragraph answers remains unexplored to a large extent. The traditional board, chalk, talk based classroom scenario involves a teacher setting question papers based on the concepts taught, checks the answers written by students manually and thus evaluates the students' performance. However, setting question papers remains a time consuming process with the teacher having to bother about question quality, level of difficulty and redundancy. In addition the process of manually correcting students' answers is a cumbersome and tedious task. In this paper, we put forth the design, analysis and implementation details along with some preliminary results to build a system that integrates all the above mentioned tasks with minimal teacher involvement that not only automates the traditional classroom scenario but also overcomes its inherent shortcomings and fallacies. \u00a9 2013 IEEE.","keywords_author":["Document Object Model Parser","E-learning techniques","Information Retrieval","Machine learning","Natural language Processing","Web data Mining"],"keywords_other":["Conceptual understanding","Document object model","Level of difficulties","Qualitative analysis","Web data mining","Personalized learning","Traditional learning","NAtural language processing"],"max_cite":0.0,"pub_year":2013.0,"sources":"['scp']","rawkeys":["web data mining","conceptual understanding","document object model parser","document object model","machine learning","information retrieval","natural language processing","traditional learning","level of difficulties","personalized learning","e-learning techniques","qualitative analysis"],"tags":["web data mining","conceptual understanding","document object model parser","document object model","machine learning","information retrieval","natural language processing","traditional learning","level of difficulties","personalized learning","e-learning techniques","qualitative analysis"]},{"p_id":17318,"title":"Dual Sentiment Analysis: Considering Two Sides of One Review","abstract":"\u00a9 2015 IEEE. Bag-of-words (BOW) is now the most popular way to model text in statistical machine learning approaches in sentiment analysis. However, the performance of BOW sometimes remains limited due to some fundamental deficiencies in handling the polarity shift problem. We propose a model called dual sentiment analysis (DSA), to address this problem for sentiment classification. We first propose a novel data expansion technique by creating a sentiment-reversed review for each training and test review. On this basis, we propose a dual training algorithm to make use of original and reversed training reviews in pairs for learning a sentiment classifier, and a dual prediction algorithm to classify the test reviews by considering two sides of one review. We also extend the DSA framework from polarity (positive-negative) classification to 3-class (positive-negative-neutral) classification, by taking the neutral reviews into consideration. Finally, we develop a corpus-based method to construct a pseudo-antonym dictionary, which removes DSA's dependency on an external antonym dictionary for review reversion. We conduct a wide range of experiments including two tasks, nine datasets, two antonym dictionaries, three classification algorithms, and two types of features. The results demonstrate the effectiveness of DSA in supervised sentiment classification.","keywords_author":["machine learning","natural language processing","opinion mining","sentiment analysis"],"keywords_other":["Corpus-based methods","Sentiment analysis","Statistical machine learning","Classification algorithm","Prediction algorithms","Sentiment classification","NAtural language processing","Opinion mining"],"max_cite":30.0,"pub_year":2015.0,"sources":"['scp']","rawkeys":["sentiment classification","statistical machine learning","natural language processing","machine learning","prediction algorithms","corpus-based methods","classification algorithm","opinion mining","sentiment analysis"],"tags":["sentiment classification","statistical machine learning","natural language processing","machine learning","prediction algorithms","corpus-based methods","classification algorithm","opinion mining","sentiment analysis"]},{"p_id":45991,"title":"Distributed Representation for Neighborhood-Based Collaborative Filtering","abstract":"\u00a9 2017 IEEE. Collaborative filtering is widely used in recommender systems. When training data are extremely sparse, neighbor selection methods work ineffectively. To address this issue, this paper proposes a distributed representation model that represents users as low-dimensional vectors for neighbor selection by considering the chronological order of users' ratings. Experiments show that the proposed method outperforms the state-of-the-art methods solving the sparsity problem with regard to precision and ranking quality.","keywords_author":["collaborative filtering","deep learning","neighbor selection","NLP","Recommender system","word embedding","word2vec"],"keywords_other":["State-of-the-art methods","Chronological order","word2vec","Word embedding","Low dimensional","Distributed representation","Neighbor selection","Sparsity problems"],"max_cite":0.0,"pub_year":2017.0,"sources":"['scp']","rawkeys":["sparsity problems","state-of-the-art methods","nlp","neighbor selection","deep learning","word2vec","low dimensional","chronological order","word embedding","distributed representation","recommender system","collaborative filtering"],"tags":["sparsity problems","state-of-the-art methods","neighbor selection","word2vec","low dimensional","chronological order","machine learning","natural language processing","word embedding","distributed representation","recommender systems","collaborative filtering"]},{"p_id":29610,"title":"Analyzing Delhi Assembly Election 2015 using textual content of social network","abstract":"\u00a9 2015 ACM. With the emergence of web 2.0, a large number of social networking sites (SNSs) have been evolved. Now-a-days, these social networking sites are attracting millions of users for sharing their views on various issues (e.g. politics, sports, products). We believe that, due to active participation of millions of users on social networking sites the SNSs forms a virtual world that has very close correspondence with the real world communities that possess immense possibilities to mirror the real world events and activities. Motivated with this, in this work we performed analysis of the textual content of Twitter's data related to Delhi Assembly Election 2015 in a manner to predict election results. The main contributions of this work includes (i) Preparation and use of events and time specific training dataset to train the classifier for better accuracy (ii) Design of mapping functions that maps the Twitter's sentiment share to the seat counts of top three contesting parties, with minimum root mean squared error (RSME) regardless of having lots of demographic diversities. The overall results are very close to ground reality, which strengthen our beliefs.","keywords_author":["Election prediction","Machine learning","Natural language processing","Sentiment analysis","Twitter"],"keywords_other":["Twitter","Social networking sites (SNSs)","Sentiment analysis","Social networking sites","Root mean squared errors","Mapping functions","Training dataset","NAtural language processing"],"max_cite":4.0,"pub_year":2015.0,"sources":"['scp']","rawkeys":["social networking sites (snss)","election prediction","training dataset","mapping functions","natural language processing","machine learning","social networking sites","root mean squared errors","sentiment analysis","twitter"],"tags":["election prediction","mapping functions","root mean square errors","natural language processing","machine learning","social networking sites","training data sets","sentiment analysis","twitter"]},{"p_id":48043,"title":"Turkish tweet sentiment analysis with word embedding and machine learning Makine \u00d6\u01e7renmesi ve Kelime Vekt\u00f6r Temsili ile T\u00fcrke Tweet Sentiment Analizi","abstract":"\u00a9 2017 IEEE. This work includes processing and classification of tweets which are written in Turkish language. Four different sector tweet datasets are vectorized with Word Embedding model and classified with Support Vector Machine and Random Forests classifiers and results have been compared. We have showed that sector based tweet classification is more successful compared to general tweets. Accuracy rates for Banking sector is 89.97%, for Football 84.02%, for Telecom 73.86%, for Retail 63.68% and for overall 74.60% have been achieved.","keywords_author":["machine learning","natural language processing","Random Forests","sentiment analysis","Support Vector Machine","text classification","word embedding"],"keywords_other":["Turkish language","Random forests","Turkishs","Text classification","Sentiment analysis","Accuracy rate","word embedding","Banking sectors"],"max_cite":0.0,"pub_year":2017.0,"sources":"['scp']","rawkeys":["turkishs","text classification","banking sectors","natural language processing","machine learning","accuracy rate","random forests","word embedding","support vector machine","turkish language","sentiment analysis"],"tags":["turkishs","text classification","banking sectors","natural language processing","machine learning","accuracy rate","random forests","word embedding","turkish language","sentiment analysis"]},{"p_id":39852,"title":"Large-scale learning with structural kernels for class-imbalanced datasets","abstract":"Much of the success in machine learning can be attributed to the ability of learning methods to adequately represent, extract, and exploit inherent structure present in the data under interest. Kernel methods represent a rich family of techniques that harvest on this principle. Domain-specific kernels are able to exploit rich structural information present in the input data to deliver state of the art results in many application areas, e.g. natural language processing (NLP), bio-informatics, computer vision and many others. The use of kernels to capture relationships in the input data has made Support Vector Machine (SVM) algorithm the state of the art tool in many application areas. Nevertheless, kernel learning remains a computationally expensive process. The contribution of this paper is to make learning with structural kernels, e.g. tree kernels, more applicable to real-world large-scale tasks. More specifically, we propose two important enhancements of the approximate cutting plane algorithm to train Support Vector Machines with structural kernels: (i) a new sampling strategy to handle class-imbalanced problem; and (ii) a parallel implementation, which makes the training scale almost linearly with the number of CPUs. We also show that theoretical convergence bounds are preserved for the improved algorithm. The experimental evaluations demonstrate the soundness of our approach and the possibility to carry out large-scale learning with structural kernels. \u00a9 2012 Springer-Verlag.","keywords_author":["Kernel Methods","Machine Learning","Natural Language Processing","Structural Kernels","Support Vector Machine"],"keywords_other":["Experimental evaluation","Learning methods","Structural information","Tree kernels","Convergence bounds","Kernel learning","Application area","State of the art","Data sets","Parallel implementations","Kernel methods","Input datas","Cutting plane algorithms","Domain specific","Structural Kernels","NAtural language processing","Sampling strategies","Support vector machine algorithm"],"max_cite":1.0,"pub_year":2012.0,"sources":"['scp']","rawkeys":["support vector machine algorithm","input datas","learning methods","state of the art","data sets","sampling strategies","domain specific","kernel methods","support vector machine","cutting plane algorithms","machine learning","parallel implementations","tree kernels","structural kernels","structural information","application area","experimental evaluation","natural language processing","kernel learning","convergence bounds"],"tags":["support vector machine algorithm","input datas","learning methods","state of the art","data sets","sampling strategies","domain specific","kernel methods","cutting plane algorithms","machine learning","parallel implementations","tree kernels","structural information","application area","experimental evaluation","natural language processing","structure kernel","kernel learning","convergence bounds"]},{"p_id":37809,"title":"Supervised classification of spam emails with natural language stylometry","abstract":"\u00a9 2015, The Natural Computing Applications Forum. Email spam is one of the biggest threats to today\u2019s Internet. To deal with this threat, there are long-established measures like supervised anti-spam filters. In this paper, we report the development and evaluation of sentinel\u2014an anti-spam filter based on natural language and stylometry attributes. The performance of the filter is evaluated not only on non-personalized emails (i.e., emails collected randomly) but also on personalized emails (i.e., emails collected from particular individuals). Among the non-personalized datasets are CSDMC2010, SpamAssassin, and LingSpam, while the Enron-Spam collection comprises personalized emails. The proposed filter extracts natural language attributes from email text that are closely related to writer stylometry and generate classifiers using multiple learning algorithms. Experimental outcomes show that classifiers generated by meta-learning algorithms such as adaboostm1 and bagging are the best, performing equally well and surpassing the performance of a number of filters proposed in previous studies, while a random forest generated classifier is a close second. On the other hand, the performance of classifiers using support vector machine and Na\u00efve Bayes is not satisfactory. In addition, we find much improved results on personalized emails and mixed results on non-personalized emails.","keywords_author":["Computational linguistics","Natural language processing","Performance evaluation","Spam classification","Stylometry","Supervised machine learning","Text classification","Text mining"],"keywords_other":["Text mining","Text classification","Spam classification","Performance evaluation","Supervised machine learning","Stylometry","NAtural language processing"],"max_cite":1.0,"pub_year":2016.0,"sources":"['scp']","rawkeys":["spam classification","text mining","natural language processing","computational linguistics","performance evaluation","supervised machine learning","stylometry","text classification"],"tags":["spam classification","text mining","natural language processing","computational linguistics","performance evaluation","supervised machine learning","stylometry","text classification"]},{"p_id":39861,"title":"Simple and efficient classification scheme based on specific vocabulary","abstract":"Assuming a binomial distribution for word occurrence, we propose computing a standardized Z score to define the specific vocabulary of a subset compared to that of the entire corpus. This approach is applied to weight terms (character n-gram, word, stem, lemma or sequence of them) which characterize a document. We then show how these Z score values can be used to derive a simple and efficient categorization scheme. To evaluate this proposition and demonstrate its effectiveness, we develop two experiments. First, the system must categorize speeches given by B. Obama as being either electoral or presidential speech. In a second experiment, sentences are extracted from these speeches and then categorized under the headings electoral or presidential. Based on these evaluations, the proposed classification scheme tends to perform better than a support vector machine model for both experiments, on the one hand, and on the other, shows a better performance level than a Na\u00efve Bayes classifier on the first test and a slightly lower performance on the second (10-fold cross validation). \u00a9 2012 Springer-Verlag.","keywords_author":["Corpus linguistics","Machine learning","Natural language processing (NLP)","Statistics in lexical analysis","Text categorization"],"keywords_other":null,"max_cite":1.0,"pub_year":2012.0,"sources":"['scp']","rawkeys":["machine learning","natural language processing (nlp)","statistics in lexical analysis","text categorization","corpus linguistics"],"tags":["natural language processing","machine learning","statistics in lexical analysis","text categorization","corpus linguistics"]},{"p_id":31671,"title":"Query labelling for Indic languages using a hybrid approach","abstract":"With a boom in the internet, social media text has been increasing day by day. Much of the user generated content on internet is written in a very informal way. Usually people tend to write text on social media using indigenous script. To understand a script different from ours is a difficult task. Moreover, nowadays queries received by the search engines are large number of transliterated text. Hence providing a common platform to deal with the problem of transliterated text becomes really important. This paper presents our approach to handle labeling of queries as part of the FIRE2015 shared task on Mixed-Script Information Retrieval. Tokens in the query are labeled on basis of a hybrid approach which involves rule based and machine learning techniques. Each annotation has been dealt separately but sequentially.","keywords_author":["Information retrieval","Language identification","Logistic regression","Machine learning","Natural language processing","Transliteration"],"keywords_other":["User-generated content","Language identification","Hybrid approach","Common platform","Logistic regressions","Transliteration","NAtural language processing","Machine learning techniques"],"max_cite":3.0,"pub_year":2015.0,"sources":"['scp']","rawkeys":["common platform","logistic regression","transliteration","machine learning techniques","machine learning","natural language processing","information retrieval","language identification","hybrid approach","user-generated content","logistic regressions"],"tags":["common platform","transliteration","machine learning techniques","machine learning","natural language processing","information retrieval","language identification","hybrid approach","user-generated content","logistic regressions"]},{"p_id":21433,"title":"PARNT: A statistic based approach to extract non-taxonomic relationships of ontologies from text","abstract":"Learning Non-Taxonomic Relationships is a sub-field of Ontology learning that aims at automating the extraction of these relationships from text. This article proposes PARNT, a novel approach that supports ontology engineers in extracting these elements from corpora of plain English. PARNT is parametrized, extensible and uses original solutions that help to achieve better results when compared to other techniques for extracting non-taxonomic relationships from ontology concepts and English text. To evaluate the PARNT effectiveness, a comparative experiment with another state of the art technique was conducted. \u00a9 2013 IEEE.","keywords_author":["Learning non-taxonomic relationships","Machine learning","Natural language processing","Ontology","Ontology learning"],"keywords_other":["Ontology learning","Statistic-based","Sub fields","Ontology concepts","Comparative experiments","Learning non-taxonomic relationships","State-of-the-art techniques","NAtural language processing"],"max_cite":14.0,"pub_year":2013.0,"sources":"['scp']","rawkeys":["comparative experiments","statistic-based","ontology","natural language processing","machine learning","ontology concepts","learning non-taxonomic relationships","state-of-the-art techniques","sub fields","ontology learning"],"tags":["comparative experiments","statistic-based","natural language processing","machine learning","ontology concepts","learning non-taxonomic relationships","state-of-the-art techniques","subfields","ontology learning"]},{"p_id":50110,"title":"Challenges in detecting privacy revealing information in unstructured text","abstract":"\u00a9 2016, CEUR-WS. All rights reserved.This paper discusses the challenges in detecting privacy revealing information using ontologies, natural language processing and machine learning techniques. It reviews current definitions, and sketches problem levels towards identifying the main open challenges. Furthermore, it elicits that the current notion of personally identifiable information lacks robustness to be used in varying contexts and user perceptions, and shows the need to additionally consider privacy sensitive information.","keywords_author":["Machine learning","Ontology","Personally identifiable information","Privacy","Privacy revealing information","Privacy sensitive information"],"keywords_other":["Current definition","Unstructured texts","User perceptions","Sensitive informations","Personally identifiable information","NAtural language processing","Machine learning techniques"],"max_cite":0.0,"pub_year":2016.0,"sources":"['scp']","rawkeys":["current definition","privacy revealing information","personally identifiable information","privacy","machine learning techniques","natural language processing","machine learning","ontology","privacy sensitive information","sensitive informations","unstructured texts","user perceptions"],"tags":["current definition","privacy revealing information","personally identifiable information","privacy","machine learning techniques","natural language processing","machine learning","sensitive informations","privacy sensitive information","unstructured texts","user perceptions"]},{"p_id":19391,"title":"A hybrid approach to Arabic named entity recognition","abstract":"In this paper, we propose a hybrid named entity recognition (NER) approach that takes the advantages of rule-based and machine learning-based approaches in order to improve the overall system performance and overcome the knowledge elicitation bottleneck and the lack of resources for underdeveloped languages that require deep language processing, such as Arabic. The complexity of Arabic poses special challenges to researchers of Arabic NER, which is essential for both monolingual and multilingual applications. We used the hybrid approach to develop an Arabic NER system that is capable of recognizing 11 types of Arabic named entities: Person, Location, Organization, Date, Time, Price, Measurement, Percent, Phone Number, ISBN and File Name. Extensive experiments were conducted using decision trees, Support Vector Machines and logistic regression classifiers to evaluate the system performance. The empirical results indicate that the hybrid approach outperforms both the rule-based and the ML-based approaches when they are processed independently. More importantly, our system outperforms the state-of-the-art of Arabic NER in terms of accuracy when applied to ANERcorp standard dataset, with F-measures 0.94 for Person, 0.90 for Location and 0.88 for Organization. \u00a9 The Author(s) 2013.","keywords_author":["hybrid approach","information extraction","information retrieval","machine learning approach","named entity recognition","natural language processing","rule-based approach"],"keywords_other":["Named entity recognition","Machine learning approaches","Hybrid approach","Rule-based approach","NAtural language processing"],"max_cite":20.0,"pub_year":2014.0,"sources":"['scp']","rawkeys":["machine learning approach","information extraction","named entity recognition","natural language processing","information retrieval","hybrid approach","rule-based approach","machine learning approaches"],"tags":["information extraction","named entity recognition","natural language processing","information retrieval","hybrid approach","rule-based approach","machine learning approaches"]},{"p_id":7111,"title":"CIDEr: Consensus-based image description evaluation","abstract":"\u00a9 2015 IEEE.Automatically describing an image with a sentence is a long-standing challenge in computer vision and natural language processing. Due to recent progress in object detection, attribute classification, action recognition, etc., there is renewed interest in this area. However, evaluating the quality of descriptions has proven to be challenging. We propose a novel paradigm for evaluating image descriptions that uses human consensus. This paradigm consists of three main parts: a new triplet-based method of collecting human annotations to measure consensus, a new automated metric that captures consensus, and two new datasets: PASCAL-50S and ABSTRACT-50S that contain 50 sentences describing each image. Our simple metric captures human judgment of consensus better than existing metrics across sentences generated by various sources. We also evaluate five state-of-the-art image description approaches using this new protocol and provide a benchmark for future comparisons. A version of CIDEr named CIDEr-D is available as a part of MS COCO evaluation server to enable systematic evaluation and benchmarking.","keywords_author":null,"keywords_other":["Systematic evaluation","Recent progress","Automated metric","Human annotations","Image descriptions","Action recognition","Human judgments","NAtural language processing"],"max_cite":167.0,"pub_year":2015.0,"sources":"['ieee', 'scp', 'wos']","rawkeys":["human annotations","systematic evaluation","automated metric","action recognition","natural language processing","recent progress","image descriptions","human judgments"],"tags":["human annotations","systematic evaluation","automated metric","action recognition","natural language processing","recent progress","image descriptions","human judgments"]},{"p_id":19399,"title":"SWIMS: Semi-supervised subjective feature weighting and intelligent model selection for sentiment analysis","abstract":"\u00a9 2016 Elsevier B.V. All rights reserved. Sentiment Analysis, also called Opinion Mining, is currently one of the most studied research fields. Its aim is to analyze publics' sentiments, opinions, attitudes etc., towards different elements such as topics, products, individuals, organizations, or services. Sentiment classification can be achieved by machine learning or lexical based methodologies or a combination of both. In an effort to improve the performance of domain independent lexicons, this research incorporates machine learning with a lexical based approach introducing a new framework called SWIMS to determine the feature weight based on a well-known general-purpose sentiment lexicon, SentiWordNet. Support vector machine is used to learn the feature weights and an intelligent model selection approach is employed in order to enhance the classification performance. The features are selected based on their subjectivity and the effects of feature selection with respect to their part of speech information are studied extensively. Seven benchmark datasets have been used in this research including large movie review dataset, multi-domain sentiment dataset and Cornell movie review dataset, all of which are available online. In-depth performance comparison is conducted with the state of art machine learning approaches and lexical based methodologies. The evaluation of performance measures proves that the proposed framework outperforms other techniques for sentiment analysis.","keywords_author":["Cornell","Feature selection","Movie reviews","Natural Language Processing (NLP)","Sentiment analysis","Support Vector Machine"],"keywords_other":["Classification performance","Machine learning approaches","Performance comparison","Sentiment analysis","Cornell","Movie reviews","Sentiment classification","NAtural language processing"],"max_cite":20.0,"pub_year":2016.0,"sources":"['scp']","rawkeys":["sentiment classification","classification performance","movie reviews","natural language processing","natural language processing (nlp)","feature selection","cornell","sentiment analysis","support vector machine","performance comparison","machine learning approaches"],"tags":["sentiment classification","classification performance","movie reviews","natural language processing","machine learning","feature selection","cornell","sentiment analysis","performance comparison","machine learning approaches"]},{"p_id":33739,"title":"Summarization of customer reviews for a product on a website using natural language processing","abstract":"\u00a9 2016 IEEE. In the recent past, e-commerce sites have made rapid growth. There are thousands of products and various websites sell these products. Massive growth in the number of reviews and their availability along with the advent of opinion-rich review forums for the products sold online, choosing the right one from a large number of products has become difficult for the users. HELP-ME-BUY APP is an android application that assists buyers in online shopping. It is imminent for buyers to verify for genuineness and quality of products. What better way is there than to ask people who have already bought the product? This is when customer reviews come into picture. The major hitch here is popular products have thousands of reviews-we do not have the time or patience to read all thousands of them. Hence, our application eases this task by analyzing and summarizing all reviews which will help the user decide what other buyers have experienced on buying this product. We carry out this process by a number of modules that include feature extraction and opinion extraction which improves the process of analysis and helps in the formation of an efficient summary.","keywords_author":["Android","Machine Learning","Natural Language processing","POS tagging","raw text analysis","summarization"],"keywords_other":["NAtural language processing","Text analysis","Android","summarization","PoS tagging"],"max_cite":2.0,"pub_year":2016.0,"sources":"['scp']","rawkeys":["raw text analysis","natural language processing","machine learning","android","summarization","text analysis","pos tagging"],"tags":["neural networks","raw text analysis","natural language processing","machine learning","summarization","text analysis","pos tagging"]},{"p_id":52172,"title":"A supervised abbreviation resolution system for medical text","abstract":"We present our participation in Task 2 of the 2013 CLEFeHEALTH Challenge, whose goal was to determine the UMLS concept unique identifier (CUI), if available, of an abbreviation or acronym. We hypothesize that considering only the abbreviations of the training corpus could be sufficient to provide a strong baseline for this task. We therefore test how a fully supervised approach, which predicts the CUI of an abbreviation based only on the abbreviations and CUIs seen in the training corpus, can fare on this task. We adapt to this task the processing pipeline we developed for CLEF-eHEALTH Task 1, entity detection: a supervised MaxEnt model based on a set of features including UMLS Concept Unique Identifiers, complemented here with a rule-based component for document headers. This system confirmed our hypothesis, and was evaluated at 0.664 accuracy (strict) and 0.672 accuracy (relaxed), ranking second out of five teams.","keywords_author":["Abbreviation normalization","Machine learning","Medical records","Natural Language Processing"],"keywords_other":["Unique identifiers","Abbreviation normalization","Training corpus","Rule-based components","Medical record","Entity detection","Resolution systems","NAtural language processing"],"max_cite":0.0,"pub_year":2013.0,"sources":"['scp']","rawkeys":["entity detection","rule-based components","machine learning","medical record","abbreviation normalization","natural language processing","training corpus","unique identifiers","medical records","resolution systems"],"tags":["entity detection","rule-based components","natural language processing","medical record","machine learning","abbreviation normalization","training corpus","unique identifiers","resolution systems"]},{"p_id":31698,"title":"Advances in question classification for open-domain question answering","abstract":"\u00a9, 2015, Chinese Institute of Electronics. All right reserved. Open-domain question answering is becoming a hot topic in the fields of natural language processing and information retrieval. Question classification, as an important component of question answering, has shown its significant influence on the overall performance of question answering systems. It can help reduce the search space and choose the exact search strategy to find answers. In this paper, we present a through overview of the state-of-the-art approaches to question classification, in terms of category\/dataset, feature extraction, classification methods and performance metrics. Firstly, we give a detailed analysis of the supervised learning based question classification approaches. Then, we introduce some related work on question classification, such as kernel methods, semi-supervised learning methods, active learning and transfer learning methods, and so on. Finally, we give some possible research directions on question classification.","keywords_author":["Classifier design","Feature extraction","Machine learning","Open-domain question answering","Question classification"],"keywords_other":["Question classification","Classifier design","Semi-supervised learning methods","Open domain question answering","Question answering systems","State-of-the-art approach","Transfer learning methods","NAtural language processing"],"max_cite":3.0,"pub_year":2015.0,"sources":"['scp']","rawkeys":["natural language processing","machine learning","question answering systems","question classification","open-domain question answering","feature extraction","state-of-the-art approach","open domain question answering","transfer learning methods","classifier design","semi-supervised learning methods"],"tags":["natural language processing","machine learning","question answering systems","question classification","feature extraction","state-of-the-art approach","open domain question answering","transfer learning methods","classifier design","semi-supervised learning methods"]},{"p_id":35799,"title":"Sortal anaphora resolution in medline abstracts","abstract":"This paper reports our investigation of machine learning methods applied to anaphora resolution for biology texts, particularly paper abstracts. Our primary concern is the investigation of features and their combinations for effective anaphora resolution. In this paper, we focus on the resolution of demonstrative phrases and definite determiner phrases, the two most prevalent forms of anaphoric expressions that we find in biology research articles. Different resolution models are developed for demonstrative and definite determiner phrases. Our work shows that models may be optimized differently for each of the phrase types. Also, because a significant number of definite determiner phrases are not anaphoric, we induce a model to detect anaphoricity, i.e., a model that classifies phrases as either anaphoric or nonanaphoric. We propose several novel features that we call highlighting features, and consider their utility particularly for processing paper abstracts. The system using the highlighting features achieved accuracies of 78% and 71% for demonstrative phrases and definite determiner phrases, respectively. The use of the highlighting features reduced the error rate by about 10%. \u00a9 2007 Blackwell Publishing, Inc.","keywords_author":["Anaphora resolution","Bioinformatics","Machine learning","Natural language processing"],"keywords_other":["Anaphora resolution","Resolution models","Natural language processing","Biology texts"],"max_cite":2.0,"pub_year":2007.0,"sources":"['scp']","rawkeys":["resolution models","machine learning","natural language processing","bioinformatics","anaphora resolution","biology texts"],"tags":["machine learning","natural language processing","biological text","bioinformatics","anaphora resolution","resolution modeling"]},{"p_id":23512,"title":"Identifying content-related threads in MOOC discussion forums","abstract":"Copyright \u00a9 2015 ACM. This study investigated the extent to which students asked and instructors answered content-related questions in MOOC discussion forums; subsequently a classification model was built to identify such questions based on extracted linguistic features. Results showed content-related threads were a minority and underaddressed by instructors. However, linguistic modeling was promising in identifying them with high reliability.","keywords_author":["Machine learning","Massive open online courses","Natural language processing","Social interaction"],"keywords_other":["Social interactions","Classification models","High reliability","Discussion forum","Linguistic modeling","Massive open online course","Linguistic features","NAtural language processing"],"max_cite":10.0,"pub_year":2015.0,"sources":"['scp']","rawkeys":["social interaction","social interactions","massive open online course","machine learning","classification models","natural language processing","linguistic features","linguistic modeling","massive open online courses","discussion forum","high reliability"],"tags":["social interactions","machine learning","classification models","natural language processing","linguistic features","linguistic modeling","mooc","discussion forum","high reliability"]},{"p_id":23516,"title":"Using social connection information to improve opinion mining: Identifying negative sentiment about HPV vaccines on Twitter","abstract":"\u00a9 2015 IMIA and IOS Press. The manner in which people preferentially interact with others like themselves suggests that information about social connections may be useful in the surveillance of opinions for public health purposes. We examined if social connection information from tweets about human papillomavirus (HPV) vaccines could be used to train classifiers that identify anti-vaccine opinions. From 42,533 tweets posted between October 2013 and March 2014, 2,098 were sampled at random and two investigators independently identified anti-vaccine opinions. Machine learning methods were used to train classifiers using the first three months of data, including content (8,261 text fragments) and social connections (10,758 relationships). Connection-based classifiers performed similarly to content-based classifiers on the first three months of training data, and performed more consistently than content-based classifiers on test data from the subsequent three months. The most accurate classifier achieved an accuracy of 88.6% on the test data set, and used only social connection features. Information about how people are connected, rather than what they write, may be useful for improving public health surveillance methods on Twitter.","keywords_author":["HPV vaccines","Machine learning","Public health surveillance","Social media","Text mining","Twitter messaging"],"keywords_other":["Papillomavirus Vaccines","Natural Language Processing","Attitude to Health","Social Media","Public Opinion","Data Mining","Vaccination","Social Support"],"max_cite":10.0,"pub_year":2015.0,"sources":"['scp']","rawkeys":["attitude to health","data mining","text mining","social support","social media","natural language processing","machine learning","vaccination","public opinion","twitter messaging","hpv vaccines","public health surveillance","papillomavirus vaccines"],"tags":["attitude to health","data mining","text mining","public health surveillances","public opinions","social support","social media","natural language processing","machine learning","vaccination","twitter messaging","hpv vaccines","papillomavirus vaccines"]},{"p_id":35804,"title":"Learning translation rules for a bidirectional english-filipino machine translator","abstract":"Filipino is a changing language that poses several challenges. Our goal is to develop a bidirectional English-Filipino Machine Translation (MT) system using a hybrid approach to learn rules from examples. The first phase was an English to Filipino MT system that required several language resources. The problem lies on its dependency over the annotated grammar which is currently unavailable for Filipino, which makes reverse translation impossible. Phase 2 addresses this limitation by using information taken from English and Filipino POS Taggers. The seed rules are generated by aligning the POS tags from English and Filipino examples, including their constraints. To perform compositionality, the system deduces the constituent labels by using the longest adjacent POS tags found in both the English and Filipino rule. The system groups together similar rules and generalizes it to encompass a wider range of unseen examples.","keywords_author":["Bilingual corpus","Compositionality","Examplebased","Machine learning","Machine translation","Natural language processing","Rule generalization","Rule generation","Rule-based"],"keywords_other":["Rule generation","Bilingual corpora","Rule generalization","Compositionality","Rule based","Examplebased","Machine translations","NAtural language processing"],"max_cite":2.0,"pub_year":2006.0,"sources":"['scp']","rawkeys":["bilingual corpus","bilingual corpora","examplebased","rule based","rule generation","machine learning","natural language processing","machine translation","rule-based","machine translations","rule generalization","compositionality"],"tags":["bilingual corpus","bilingual corpora","examplebased","rule based","rule generation","machine learning","natural language processing","composition","machine translations","rule generalization"]},{"p_id":29662,"title":"A learning based emotion classifier with semantic text processing","abstract":"\u00a9 Springer International Publishing Switzerland 2015. In this modern era, we depend more and more on machines for day to day activities. However, there is a huge gap between computer and human in emotional thinking, which is the central factor in human communication. This gap can be bridged by implementing several computational approaches, which induce emotional intelligence into a machine. Emotion detection from text is one such method to make the computers emotionally intelligent because text is one of the major media for communication among humans and with the computers. In this paper, we propose an approach which adds natural language processing techniques to improve the performance of learning based emotion classifier by considering the syntactic and semantic features of text. We also present a comprehensive overview of emerging field of emotion detection from text.","keywords_author":["Affective computing","Emotion recognition","Machine learning","Natural language processing"],"keywords_other":["Affective Computing","Human communications","Emotional intelligence","Emotion recognition","Semantic features","Emotion detection","NAtural language processing","Computational approach"],"max_cite":4.0,"pub_year":2015.0,"sources":"['scp']","rawkeys":["human communications","affective computing","machine learning","natural language processing","semantic features","emotion recognition","computational approach","emotion detection","emotional intelligence"],"tags":["human communications","affective computing","machine learning","natural language processing","semantic features","emotion recognition","computational approach","emotion detection","emotional intelligence"]},{"p_id":29663,"title":"Convolutional neural networks for correcting english article errors","abstract":"\u00a9 Springer International Publishing Switzerland 2015. In this paper, convolutional neural networks are employed for English article error correction. Instead of employing features relying on human ingenuity and prior natural language processing knowledge, the words surrounding the context of the article are taken as features. Our approach could be trained both on an error annotated corpus and an error non-annotated corpus. Experiments are conducted on CoNLL-2013 data set. Our approach achieves 38.10% in F1, and outperforms the best system (33.40 %) that participates in the task. Experimental results demonstrate the effectiveness of our proposed approach.","keywords_author":["Article error correction","Convolutional neural networks","Deep learning"],"keywords_other":["Deep learning","Article errors","Convolutional neural network","NAtural language processing","Data set","Non-annotated corpus"],"max_cite":4.0,"pub_year":2015.0,"sources":"['scp']","rawkeys":["convolutional neural networks","deep learning","data set","non-annotated corpus","natural language processing","article errors","convolutional neural network","article error correction"],"tags":["non-annotated corpus","machine learning","natural language processing","data sets","article errors","convolutional neural network","article error correction"]},{"p_id":35808,"title":"A comparison study of biomedical short form definition detection algorithms","abstract":"With more and more research dedicated to literature mining in the biomedical domain, more and more systems are available for people to choose from to build literature mining applications. In this study, we focus on one specific kind of task, i.e., detecting definitions of acronyms\/abbreviations\/ symbols in biomedical text. The study was designed to answer the following questions; i) how well a system performs in detecting definitions when provided with a large set of documents recently published in the biomedical domain, ii) what the coverage is for various knowledge bases in including acronyms\/abbreviations\/symbols as synonyms of their definitions, and iii) how to combine results from various systems. We evaluated three publicly available systems, namely, ALICE (a handcrafted pattern\/rule based system), a system by Chang et al. (a machine-learning system), and an algorithm by Schwartz and Hearst (a simple alignment-based program), in detecting definitions for acronyms\/abbreviations\/symbols as well as the conceptual coverage of existing thesauri, namely, the UMLS (the Unified Medical Language System) and the BioThesaurus (a thesaurus of names for all UniProt protein records). We found that all three systems agreed on a large portion of the results (over 94% of all definitions detected) mainly due to the fact that most acronyms\/abbreviations\/ symbols were formed through various initializations from their definitions. The precisions and recalls of the three systems are comparable. However, based on manual investigation of the results, we found that most systems have some difficulty in detecting definitions for chemical\/gene\/protein symbols where ALICE has relatively better performance of chemical\/gene\/protein symbols comparing to the other two possibly due to fine tuning of the system for those symbols. We also found existing knowledge bases have a good coverage of definitions for those frequently defined acronyms\/abbreviations\/symbols. Potential combinations of the three systems were also discussed and implemented. Copyright 2006 ACM.","keywords_author":["Acronyms\/abbreviations\/symbols","Algorithm evaluation","Biomedical literature mining","Information extraction","Machine learning","Natural language processing","Rule-based systems"],"keywords_other":["Algorithm evaluation","Biomedical literature mining","Information extraction","Rule-based systems","Natural language processing"],"max_cite":2.0,"pub_year":2006.0,"sources":"['scp']","rawkeys":["biomedical literature mining","algorithm evaluation","information extraction","rule-based systems","machine learning","natural language processing","acronyms\/abbreviations\/symbols"],"tags":["biomedical literature mining","algorithm evaluation","information extraction","rule-based systems","machine learning","natural language processing","acronyms\/abbreviations\/symbols"]},{"p_id":25569,"title":"Evaluating electronic health record data sources and algorithmic approaches to identify hypertensive individuals","abstract":"Objective: Phenotyping algorithms applied to electronic health record (EHR) data enable investigators to identify large cohorts for clinical and genomic research. Algorithm development is often iterative, depends on fallible investigator intuition, and is time- and labor-intensive. We developed and evaluated 4 types of phenotyping algorithms and categories of EHR information to identify hypertensive individuals and controls and provide a portable module for implementation at other sites. Materials and Methods: We reviewed the EHRs of 631 individuals followed at Vanderbilt for hypertension status. We developed features and phenotyping algorithms of increasing complexity. Input categories included International Classification of Diseases, Ninth Revision (ICD9) codes, medications, vital signs, narrative-text search results, and UnifiedMedical Language System (UMLS) concepts extracted using natural language processing (NLP). We developed a module and tested portability by replicating 10 of the best-performing algorithms at the Marshfield Clinic. Results: Random forests using billing codes, medications, vitals, and concepts had the best performance with a median area under the receiver operator characteristic curve (AUC) of 0.976. Normalized sums of all 4 categories also performed well (0.959 AUC). The best non-NLP algorithm combined normalized ICD9 codes, medications, and blood pressure readings with a median AUC of 0.948. Blood pressure cutoffs or ICD9 code counts alone had AUCs of 0.854 and 0.908, respectively. Marshfield Clinic results were similar. Conclusion: This work shows that billing codes or blood pressure readings alone yield good hypertension classification performance. However, even simple combinations of input categories improve performance. The most complex algorithms classified hypertension with excellent recall and precision. \u00a9 The Author 2016.","keywords_author":["Electronic health records","Hypertension","Machine learning","Natural language processing","Phenotyping algorithms","Random forests"],"keywords_other":["Clinical Coding","Male","Algorithms","Electronic Health Records","Hypertension","Phenotype","Humans","Aged","Middle Aged","Natural Language Processing","Machine Learning","Information Storage and Retrieval","ROC Curve","Female","Blood Pressure Determination"],"max_cite":8.0,"pub_year":2017.0,"sources":"['scp']","rawkeys":["hypertension","male","female","machine learning","electronic health records","humans","middle aged","natural language processing","phenotyping algorithms","aged","random forests","roc curve","algorithms","phenotype","information storage and retrieval","blood pressure determination","clinical coding"],"tags":["hypertension","male","female","machine learning","electronic health records","humans","middle aged","natural language processing","phenotyping algorithms","aged","random forests","roc curve","algorithms","phenotype","information storage and retrieval","blood pressure determination","clinical coding"]},{"p_id":31715,"title":"Comparing competences on academia and occupational contexts based on similarity measures","abstract":"This paper is a first contribution of a schema to match professional and work competences through similarity measures. In this contribution we focus on the determination of connections between university profiles based in standards (body of knowledge and thesauri) similarity measures and Natural Language Processing (NLP) techniques. Our first experiments proved that this hybrid schema got a promise results in the recognition of competency patterns in order to apply in the laboral context.","keywords_author":["Body of knowledge","DISCO II","Machine learning","NLP","Professional profile","Similarity measures","VSM"],"keywords_other":["DISCO II","NLP","Similarity measure","Professional profile","VSM","Body of knowledge"],"max_cite":3.0,"pub_year":2015.0,"sources":"['scp']","rawkeys":["disco ii","body of knowledge","nlp","professional profile","machine learning","vsm","similarity measure","similarity measures"],"tags":["disco ii","body of knowledge","professional profile","natural language processing","machine learning","similarity measure","vector space models"]},{"p_id":33766,"title":"Using a Distributed Representation of Words in Localizing Relevant Files for Bug Reports","abstract":"\u00a9 2016 IEEE. Once a bug in software is reported, developers have to determine which source files are related to the bug. This process is referred to as bug localization, and an automatic way of bug localization is important to improve developers' productivity. This paper proposes an approach called DrewBL to efficiently localize faulty files for a given bug report using a natural language processing tool, word2vec. In DrewBL, we first build a vector space model named semantic-VSM which represents a distributed representation of words in the bug report and source code files and next compute the relevance between them by feeding the constructed model to word2vec. We also present an approach called CombBL to further improve the accuracy of bug localization which employs not only the proposed DrewBL but also existing bug localization techniques, such as BugLocator based on textual similarity and Bugspots based on bug-fixing history, in a combinational manner. This paper gives our early experimental results to show the effectiveness and efficiency of the proposed approaches using two open source projects.","keywords_author":["Bug localization","Bug report","Information Retrieval","Machine learning","Natural language processing","Vector space model"],"keywords_other":["Natural Language Processing Tools","Textual similarities","Vector space models","Bug localizations","Bug reports","Distributed representation","Effectiveness and efficiencies","NAtural language processing"],"max_cite":2.0,"pub_year":2016.0,"sources":"['scp']","rawkeys":["natural language processing tools","bug localizations","effectiveness and efficiencies","bug reports","machine learning","natural language processing","bug localization","information retrieval","vector space model","distributed representation","bug report","vector space models","textual similarities"],"tags":["natural language processing tools","effectiveness and efficiencies","bug reports","machine learning","natural language processing","information retrieval","bug localization","distributed representation","vector space models","textual similarities"]},{"p_id":19431,"title":"NOAM: News outlets analysis and monitoring system","abstract":"We present NOAM, an integrated platform for the monitoring and analysis of news media content. NOAM is the data management system behind various applications and scientific studies aiming at modelling the mediasphere. The system is also intended to address the need in the AI community for platforms where various AI technologies are integrated and deployed in the real world. It combines a relational database (DB) with state of the art AI technologies, including data mining, machine learning and natural language processing. These technologies are organised in a robust, distributed architecture of collaborating modules, that are used to populate and annotate the DB. NOAM manages tens of millions of news items in multiple languages, automatically annotating them in order to enable queries based on their semantic properties. The system also includes a unified user interface for interacting with its various modules. \u00a9 2011 Authors.","keywords_author":["data mining","large scale","machine learning","media content analysis","news analysis","text analysis"],"keywords_other":["large scale","Relational Database","news analysis","Semantic properties","Data management system","Monitoring and analysis","News media","Scientific studies","State of the art","Multiple languages","Monitoring system","text analysis","Distributed architecture","Media content","AI Technologies","Integrated platform","NAtural language processing"],"max_cite":20.0,"pub_year":2011.0,"sources":"['scp']","rawkeys":["monitoring system","state of the art","relational database","semantic properties","multiple languages","news analysis","machine learning","media content","integrated platform","large scale","ai technologies","data mining","scientific studies","data management system","monitoring and analysis","news media","media content analysis","natural language processing","text analysis","distributed architecture"],"tags":["monitoring system","state of the art","relational database","semantic properties","multiple languages","news analysis","machine learning","large-scale","media content","integrated platform","ai technologies","data mining","scientific studies","data management system","monitoring and analysis","news media","media content analysis","natural language processing","text analysis","distributed architecture"]},{"p_id":23528,"title":"Evaluating techniques for learning non-taxonomic relationships of ontologies from text","abstract":"Learning non-taxonomic relationships is a sub-field of Ontology Learning that aims at automating the extraction of these relationships from text. Several techniques have been proposed based on Natural Language Processing and Machine Learning. However just like for other techniques for Ontology Learning, evaluating techniques for learning non-taxonomic relationships is an open problem. Three general proposals suggest that the learned ontologies can be evaluated in an executable application or by domain experts or even by a comparison with a predefined reference ontology. This article proposes two procedures to evaluate techniques for learning non-taxonomic relationships based on the comparison of the relationships obtained with those of a reference ontology. Also, these procedures are used in the evaluation of two state of the art techniques performing the extraction of relationships from two corpora in the domains of biology and Family Law. \u00a9 2014 Elsevier Ltd. All rights reserved.","keywords_author":["Learning non-taxonomic relationships","Machine learning","Natural language processing","Ontology","Ontology learning"],"keywords_other":["Ontology learning","Family law","Sub fields","Two-state","Learning non-taxonomic relationships","NAtural language processing","Domain experts"],"max_cite":10.0,"pub_year":2014.0,"sources":"['scp']","rawkeys":["two-state","domain experts","natural language processing","machine learning","family law","ontology","learning non-taxonomic relationships","sub fields","ontology learning"],"tags":["two-state","domain experts","natural language processing","machine learning","family law","learning non-taxonomic relationships","subfields","ontology learning"]},{"p_id":39912,"title":"Automatic classification of cancer notifiable death certificates","abstract":"The timely notification of cancer cases is crucial for can- cer monitoring and prevention. However, the abstraction and classification of cancer from the free-text of pathology reports and other relevant documents, such as death certificates, are complex and time-consuming activities. In this paper we investigate approaches for the automatic de- tection of cases where the cause of death is a notifiable cancer from free-text death certificates supplied to Cancer Registries. A number of machine learning classifiers were investigated. A large set of features were also extracted using natural language techniques and the Medtex toolkit; features include stemmed words, bi-grams, and concepts from the SNOMED CT medical terminology. The investigated approaches were found to be very effective in identifying death certificates where the cause of death was a notifiable cancer. Best performance was achieved by a Support Vector Machine (SVM) classifier with an overall F-measure of 0.9647 when evaluated on a set of 5,000 free-text death certificates. This classifier considers as features stemmed token bigrams and information from SNOMED CT concepts filtered by morphological abnormalities and disorders. However, our analysis shows that it is the selection of features that most inuences the performance of the classifiers rather than the type of classifier or the feature weighting schema. Specifically, we found that stemmed token bigrams with or without SNOMED CT concepts are the most effective feature. In addition, the combination of token bi- grams and SNOMED CT information was found to yield the best overall performance.","keywords_author":["Cancer monitoring and reporting","Cancer Registry","Death certificates","Machine learning","Natural language processing","SNOMED CT"],"keywords_other":["Cancer registries","Cancer monitoring","Death certificates","SNOMED-CT","NAtural language processing"],"max_cite":1.0,"pub_year":2012.0,"sources":"['scp']","rawkeys":["snomed ct","cancer monitoring and reporting","snomed-ct","death certificates","machine learning","natural language processing","cancer monitoring","cancer registry","cancer registries"],"tags":["cancer monitoring and reporting","death certificates","machine learning","natural language processing","cancer monitoring","snomed-ct","cancer registries"]},{"p_id":39914,"title":"Named entity recognition","abstract":"\u00a9 The Author(s) 2012. This chapter presents the application of ETL to language independent named entity recognition (NER). The NER task consists of finding all proper nouns in a text and classifying them among several given categories of interest. We apply ETL and ETL Committee to three different corpora in three different languages: Portuguese, Spanish and Dutch. ETL system achieves state-of-the-art competitive results for the three corpora. Moreover, ETL Committee significantly improves the ETL results for the three corpora. This chapter is organized as follows. In Sect. 7.1, we describe the NER task and the selected corpora. In Sect. 7.2, we detail some modeling configurations used in our NER system. In Sect. 7.3, we show some configurations used in the machine learning algorithms. Section 7.4 presents the application of ETL for the HAREM Corpus. In Sect. 7.5, we present the application of ETL for the SPA CoNLL-2002. In Sect. 7.6, we detail the application of ETL for the DUT CoNLL-2002. Finally, Sect. 7.7 presents some concluding remarks.","keywords_author":["Adaboost","Entropy guided transformation learning","ETL committee","Machine learning","Named entity recognition","Natural language processing","Transformation based learning"],"keywords_other":null,"max_cite":1.0,"pub_year":2012.0,"sources":"['scp']","rawkeys":["named entity recognition","machine learning","natural language processing","transformation based learning","entropy guided transformation learning","etl committee","adaboost"],"tags":["named entity recognition","machine learning","natural language processing","transformation based learning","entropy guided transformation learning","etl committee","adaboost"]},{"p_id":39915,"title":"Cognitive category learning","abstract":"Categories play a fundamental role in our daily lives and are the basis for decision making in most professions (e.g., intelligence analysis, healthcare, and engineering). Categories are a set of objects or events that have similar features and are grouped together because of their similarity. Many categories are acquired as a child (e.g., tools, fruit) but we continue to learn and apply new categories throughout life. Categories range from concrete (e.g., a set of physical objects like houses, dogs.) to abstract (e.g., political ideologies, movie genres.) and narrow (e.g., rifles) to broad (e.g., weapons). People in a variety of knowledge intensive fields (e.g., analysts, commanders, medical doctors) recognize categories in streams of data and make a decision about how to act (or pass the information to a decision maker). Such a categorization system is important now, especially because of the large amount of information that people need to sift through to do their jobs on a regular basis. When decision support systems (DSSs) are applied to support human decision making by automatically recognizing categories, these systems are often not practical in complex dynamic real world environments. Rule based DSSs are limited because it is difficult to develop and maintain large complex rule sets. Current machine learning based DSSs are sometimes limited because they require data that encompasses all of the possible variations as examples to learn from and necessitate significant effort to develop and maintain this training data. This paper describes a cognitive category learning system that uses machine learning and natural language processing (NLP) techniques to categorize unstructured documents or semi-structured objects, such as emails, which we used in this experiment. Our system uses several methods to do this categorization of emails and then arbitrates the best solution based on the individual classifier results. This result provides a more confident answer with less chance of false positive and false negative outcomes. Our system also generates a metadata topic summary for each document or email. \u00a9 2012 Published by Elsevier B.V.","keywords_author":["Categories","Cognitive","Decision Support System","DSS","Machine Learning","Natural Language Processing","NLP"],"keywords_other":null,"max_cite":1.0,"pub_year":2012.0,"sources":"['scp']","rawkeys":["decision support system","nlp","cognitive","machine learning","dss","natural language processing","categories"],"tags":["natural language processing","machine learning","categories","cognition","decision support systems"]},{"p_id":50153,"title":"A Method of discriminative features extraction for restricted boltzmann machines","abstract":"\u00a9 Springer International Publishing AG 2016.The Restricted Boltzmann Machine (RBM) is a kind of stochastic neural network. It can be used as basic building blocks to form deep architectures. Since Hinton solved the problem of computational inefficiency by using a so called greedy layer-wise unsupervised pretraining algorithm, much more attention is focused on deep learning and achieved significant success in areas of speech recognition, object recognition, natural language processing, etc. In addition to initializing deep networks, RBMs can also be used to learn features from the raw data. In this paper, we proposed a method to learn much better discriminative features for RBMs based on using a novel objective function. We test our idea on MNIST handwritten digit dataset. In our experiments, the features learnt by RBM were further fed to a multinomial logistic regression and results show that our objective function could result in much higher accuracy ratio of classification.","keywords_author":["Deep learning","Discriminative feature","Objective function","Restricted Boltzmann machines"],"keywords_other":["Deep learning","Restricted boltzmann machine","Multinomial logistic regression","Stochastic neural network","Basic building block","Objective functions","Discriminative features","NAtural language processing"],"max_cite":0.0,"pub_year":2016.0,"sources":"['scp', 'ieee']","rawkeys":["discriminative feature","restricted boltzmann machines","basic building block","multinomial logistic regression","deep learning","objective functions","natural language processing","objective function","discriminative features","stochastic neural network","restricted boltzmann machine"],"tags":["basic building block","objective functions","natural language processing","machine learning","multiple linear regressions","discriminative features","stochastic neural network","restricted boltzmann machine"]},{"p_id":48106,"title":"Intelligent medical data storage system using machine learning approach","abstract":"\u00a9 2017 IEEE.In recent days healthcare domains need a efficient storage and retrieval systems to provide a effective medical services to the health seekers. But there is a vocabulary gap in understanding the medical terminologies due to ambiguity. So, the existing systems need a intelligent medical storage using some natural language processing. Users post their queries in free text so it will result in complexity for analyzing and providing instant answers. The main aim of this paper is to develop an intelligent medical data warehousing and mining system. This system can be performed by assigning corpus-aware terminologies to the medical records. This system provides efficient answers to the health seekers by extracting the information related to their queries.","keywords_author":["Healthcare","Machine learning","Natural Language Processing (NLP)","Semantic similarity"],"keywords_other":["Medical terminologies","Medical services","Machine learning approaches","Storage and retrievals","Semantic similarity","Existing systems","Mining systems","Healthcare domains"],"max_cite":0.0,"pub_year":2017.0,"sources":"['scp']","rawkeys":["healthcare","storage and retrievals","machine learning","natural language processing (nlp)","healthcare domains","semantic similarity","medical terminologies","mining systems","medical services","existing systems","machine learning approaches"],"tags":["healthcare","storage and retrievals","machine learning","natural language processing","healthcare domains","semantic similarity","medical terminology","mining systems","medical services","existing systems","machine learning approaches"]},{"p_id":31726,"title":"Driver prediction to improve interaction with in-vehicle HMI","abstract":"Recently there has been a trend toward increasing the capability of the in-vehicle interface in terms of access to information and complex controls. This has been accompanied by an increase in the complexity of the car Human Machine Interface [HMI], At the same time, studies have shown that driver distraction can contribute to accidents. This paper provides some possible ways to reduce driver cognitive load by augmenting the interface. We use prediction of the driver's next action or intention in order to provide UI affordances for more quickly selecting actions. Two examples of this are presented: prediction of driver interaction with the car HMI based on the driving history, and prediction of driver intention from the driver speech. In the first example, we used signal processing techniques to extract meaningful features from vehicle CAN and history data, and then we used machine learning techniques to predict the driver's next action. In the second example, we used ASR and natural language processing to extract text features from driver speech, and predict user intention using a neural network and word embedding. The proposed prediction methods for user actions and intentions can be used to improve in-vehicle task performance.","keywords_author":["Car HMI","Machine learning","Prediction of driver intention","Prediction of driver interaction","Spoken language understanding"],"keywords_other":["Human Machine Interface","Spoken language understanding","Driver intention","Signal processing technique","Driver interaction","Driver distractions","NAtural language processing","Machine learning techniques"],"max_cite":3.0,"pub_year":2015.0,"sources":"['scp']","rawkeys":["driver intention","prediction of driver intention","machine learning techniques","machine learning","natural language processing","prediction of driver interaction","signal processing technique","human machine interface","driver distractions","car hmi","spoken language understanding","driver interaction"],"tags":["driver intention","prediction of driver intention","machine learning techniques","machine learning","natural language processing","prediction of driver interaction","signal processing technique","human machine interface","driver distractions","car hmi","spoken language understanding","driver interaction"]},{"p_id":44015,"title":"Identifying Security Requirements Based on Linguistic Analysis and Machine Learning","abstract":"\u00a9 2017 IEEE. Eliciting security requirements in early stage of system development has been widely recognized as an efficient way for minimizing security cost and avoiding recurring security problems. However, in many projects, security requirements are not explicitly specified but rather mixed with other requirements, requiring precise and fast identification of such security requirements. Although several probability-based approaches have been proposed to tackle this problem, they are either imprecise or domain-dependent. In this paper, we propose a tool-supported method to efficiently identify security requirements, which combines linguistic analysis with machine learning techniques. In particular, we apply a systematic approach to identify linguistic features of security requirements based on existing security requirements ontologies and linguistic knowledge. We automatically extract such features from textual requirements, which are then used to train security requirements classifiers using typical machine learning techniques. We have implemented a prototype tool to support our approach, and have systematically evaluated our approach based on three realistic requirements specifications. The evaluation results show that our approach has promising potential to train classifiers that can classify requirements specifications from different application domains.","keywords_author":["linguistic analysis","machine learning","natural language processing","prototype","security requirements classification","security requirements ontology"],"keywords_other":["Security requirements","System development","prototype","Linguistic analysis","Requirements specifications","Linguistic knowledge","Linguistic features","Machine learning techniques"],"max_cite":0.0,"pub_year":2018.0,"sources":"['scp']","rawkeys":["security requirements","prototype","system development","machine learning techniques","machine learning","natural language processing","linguistic features","security requirements ontology","linguistic analysis","requirements specifications","security requirements classification","linguistic knowledge"],"tags":["security requirements","system development","machine learning techniques","machine learning","natural language processing","linguistic features","security requirements ontology","prototypes","linguistic analysis","requirements specifications","security requirements classification","linguistic knowledge"]},{"p_id":29680,"title":"A computational framework for Tamil document classification using Random Kitchen Sink","abstract":"\u00a9 2015 IEEE. Along the prompt growth in World Wide Web, the availability and accessibility of regional language contents such as e-books, web pages, e-mails, and digital repositories has grown exponentially. As a result, the automatic document classification has become the hotspot for fetching information among the millions of web documents. The idea of classifying the text, forms the baseline for many NLP applications such as information extraction, query response, information summarization, etc. The main objective of this paper is to develop an computational framework for supervised Tamil document classification task. This paper highlights the performance of Random Kitchen Sink, a randomization algorithm, in Grand Unified Regularized Least Squares (GURLS), a Machine Learning Library, is proven to be comparably better than the conventional kernel based classifier in terms of accuracy. Henceforth, we claim that Random Kitchen Sink can be an effective alternative to the kernels for a classifier.","keywords_author":["Machine Learning","Natural Language Processing","Proximal Support Vector Machine","Random Kitchen Sink","Regularized Least Square","Support Vector Machine"],"keywords_other":["Randomization algorithm","Information summarization","Regularized least squares","Kernel based classifiers","Random Kitchen Sink","Proximal support vector machines","Computational framework","NAtural language processing"],"max_cite":4.0,"pub_year":2015.0,"sources":"['scp']","rawkeys":["random kitchen sink","regularized least square","kernel based classifiers","information summarization","natural language processing","machine learning","randomization algorithm","computational framework","regularized least squares","proximal support vector machine","support vector machine","proximal support vector machines"],"tags":["randomized algorithms","kernel based classifiers","information summarization","natural language processing","machine learning","computational framework","random kitchen sinks","recursive least square (rls)","proximal support vector machines"]},{"p_id":50161,"title":"Medic: An artificially intelligent system to provide healthcare services to society and medical assistance to doctors","abstract":"\u00a9 2016 IEEE.Since the last decade, number of applications of Artificial Intelligence in daily livelihood has drastically increased. This is primarily because of the inclusion of high-tech gadgets in our day-to-day lives. These gadgets provide high computational capabilities and geographical reach. These two features could be exerted to provide medical services to the society. This paper is based on a project which emphasizes on creating a software infrastructure which would provide healthcare services like diagnosis of diseases, advising medical tests to patients, providing medical prescription to patients by making use of personalized medicine problem solving algorithms etc., and providing medical assistance to doctors. This project, Medic, makes use of natural language processing, fuzzy logic, deep learning and a constantly evolving knowledge base to correctly diagnose diseases. It also provides various services to doctors which would help them while making decisions regarding any patient's medical treatment.","keywords_author":["Artificial Intelligence","Artificial neural networks","Convolution neural networks","Deep learning","Image recognition","Natural language processing"],"keywords_other":["Medical treatment","Medical prescription","Personalized medicines","Computational capability","Software infrastructure","Convolution neural network","Healthcare services","NAtural language processing"],"max_cite":0.0,"pub_year":2016.0,"sources":"['scp']","rawkeys":["artificial intelligence","healthcare services","convolution neural network","deep learning","personalized medicines","medical treatment","medical prescription","computational capability","natural language processing","convolution neural networks","software infrastructure","artificial neural networks","image recognition"],"tags":["healthcare services","neural networks","medical treatment","machine learning","computational capability","medical prescription","natural language processing","software infrastructure","convolutional neural network","image recognition","personalized medicine"]},{"p_id":33778,"title":"A survey on deep learning for natural language processing","abstract":"Copyright \u00a9 2016 Acta Automatica Sinica. All rights reserved.Recently, deep learning has made significant development in the fields of image and voice processing. However, there is no major breakthrough in natural language processing task which belongs to the same category of human cognition. In this paper, firstly the basic concepts of deep learning are introduced, such as application motivation, primary task and basic framework. Secondly, in terms of both data representation and learning model, this paper focuses on the current research progress and application strategies of deep learning for natural language processing, and further describes the current deep learning platforms and tools. Finally, the future development difficulties and suggestions for possible extensions are also discussed.","keywords_author":["Deep learning","Feature learning","Natural language processing","Neural network","Representation learning"],"keywords_other":["Deep learning","Data representations","Learning models","Representation learning","Human cognition","Application strategies","Feature learning","NAtural language processing","nocv1"],"max_cite":2.0,"pub_year":2016.0,"sources":"['scp']","rawkeys":["learning models","neural network","deep learning","natural language processing","representation learning","application strategies","feature learning","human cognition","data representations","nocv1"],"tags":["learning models","neural networks","machine learning","natural language processing","representation learning","application strategies","feature learning","human cognition","data representations","nocv1"]},{"p_id":50163,"title":"Automatic document classification based on J.S. mill\u2019s ideas","abstract":"\u00a9 Springer International Publishing Switzerland 2016. The paper describes an approach to automatic classification problem using J.S. Mill\u2019s ideas of inductive logic. The technique uses the general principles (but not the technical details) of the JSM method of automatic hypothesis generation. The proposed method uses some induction procedure to form generic positive and generic negative objects (represented as vectors) and an analogy procedure to classify new documents. With an optimal selection of text preprocessing options, the suggested approach shows better precision than other implemented text classification methods.","keywords_author":["Data mining","JSM method","Machine learning","Natural language processing"],"keywords_other":["Automatic classification","Technical details","Document Classification","Text preprocessing","JSM method","Hypothesis generation","NAtural language processing","Text classification methods"],"max_cite":0.0,"pub_year":2016.0,"sources":"['scp']","rawkeys":["text classification methods","data mining","automatic classification","jsm method","technical details","machine learning","natural language processing","text preprocessing","document classification","hypothesis generation"],"tags":["text classification methods","data mining","automatic classification","jsm method","technical details","machine learning","natural language processing","text preprocessing","document classification","hypothesis generation"]},{"p_id":44020,"title":"An event-extraction approach for business analysis from online Chinese news","abstract":"\u00a9 2018 Elsevier B.V. Extracting events from business news aids users to perceive market trends, be aware of competitors\u2019 strategies, and to make valuable investment decisions. Prior research lacks event extraction in the area of business and event based business analysis, especially in Chinese language. We propose a novel business event-extraction approach integrating patterns, machine learning models and word embedding technology in deep learning, which is applied to extract events from online Chinese news. Word embedding and a semantic lexicon are utilized to extend an event trigger dictionary with high accuracy. Then the trigger features in the dictionary are introduced into a machine learning classification algorithm to implement more refined event-type recognition. Based on a scalable pattern tree, the event type that is discovered is used to find the best-suited pattern for extracting event elements from online news. Experimental results show the effectiveness of the proposed approach. In addition, empirical studies demonstrate the practical value of extracted events, especially in finding the relationships between news events and excess returns for stock, and analyzing industry trends based on events in China.","keywords_author":["Business events","Business intelligence","Chinese text analytics","Event extraction","Explanatory econometrics","Machine learning models","Natural language processing","Online news","Patterns","Word embedding"],"keywords_other":["Event extraction","Explanatory econometrics","Machine learning models","Word embedding","Patterns","Chinese text","Online news"],"max_cite":0.0,"pub_year":2018.0,"sources":"['scp']","rawkeys":["online news","business intelligence","chinese text","machine learning models","natural language processing","patterns","business events","word embedding","explanatory econometrics","chinese text analytics","event extraction"],"tags":["online news","business intelligence","chinese text","machine learning models","natural language processing","patterns","business events","word embedding","explanatory econometrics","chinese text analytics","event extraction"]},{"p_id":23541,"title":"ChemDataExtractor: A Toolkit for Automated Extraction of Chemical Information from the Scientific Literature","abstract":"\u00a9 2016 American Chemical Society.The emergence of \"big data\" initiatives has led to the need for tools that can automatically extract valuable chemical information from large volumes of unstructured data, such as the scientific literature. Since chemical information can be present in figures, tables, and textual paragraphs, successful information extraction often depends on the ability to interpret all of these domains simultaneously. We present a complete toolkit for the automated extraction of chemical entities and their associated properties, measurements, and relationships from scientific documents that can be used to populate structured chemical databases. Our system provides an extensible, chemistry-aware, natural language processing pipeline for tokenization, part-of-speech tagging, named entity recognition, and phrase parsing. Within this scope, we report improved performance for chemical named entity recognition through the use of unsupervised word clustering based on a massive corpus of chemistry articles. For phrase parsing and information extraction, we present the novel use of multiple rule-based grammars that are tailored for interpreting specific document domains such as textual paragraphs, captions, and tables. We also describe document-level processing to resolve data interdependencies and show that this is particularly necessary for the autogeneration of chemical databases since captions and tables commonly contain chemical identifiers and references that are defined elsewhere in the text. The performance of the toolkit to correctly extract various types of data was evaluated, affording an F-score of 93.4%, 86.8%, and 91.5% for extracting chemical identifiers, spectroscopic attributes, and chemical property attributes, respectively; set against the CHEMDNER chemical name extraction challenge, ChemDataExtractor yields a competitive F-score of 87.8%. All tools have been released under the MIT license and are available to download from http:\/\/www.chemdataextractor.org.","keywords_author":null,"keywords_other":["Part of speech tagging","Chemical information","Named entity recognition","Automated extraction","Cluster Analysis","Humans","Pattern Recognition, Automated","Natural Language Processing","Scientific documents","Periodicals as Topic","Scientific literature","Valuable chemicals","Data Mining","Databases, Chemical","NAtural language processing"],"max_cite":10.0,"pub_year":2016.0,"sources":"['scp']","rawkeys":["data mining","periodicals as topic","automated","databases","chemical information","named entity recognition","natural language processing","automated extraction","cluster analysis","humans","part of speech tagging","valuable chemicals","pattern recognition","chemical","scientific literature","scientific documents"],"tags":["data mining","periodicals as topic","automated","databases","chemical information","named entity recognition","natural language processing","automated extraction","cluster analysis","humans","part of speech tagging","valuable chemicals","pattern recognition","chemical","scientific literature","scientific documents"]},{"p_id":9206,"title":"A unified framework for multioriented text detection and recognition","abstract":"\u00a9 1992-2012 IEEE.High level semantics embodied in scene texts are both rich and clear and thus can serve as important cues for a wide range of vision applications, for instance, image understanding, image indexing, video search, geolocation, and automatic navigation. In this paper, we present a unified framework for text detection and recognition in natural images. The contributions of this paper are threefold: 1) text detection and recognition are accomplished concurrently using exactly the same features and classification scheme; 2) in contrast to methods in the literature, which mainly focus on horizontal or near-horizontal texts, the proposed system is capable of localizing and reading texts of varying orientations; and 3) a new dictionary search method is proposed, to correct the recognition errors usually caused by confusions among similar yet different characters. As an additional contribution, a novel image database with texts of different scales, colors, fonts, and orientations in diverse real-world scenarios, is generated and released. Extensive experiments on standard benchmarks as well as the proposed database demonstrate that the proposed system achieves highly competitive performance, especially on multioriented texts.","keywords_author":["arbitrary orientations","dictionary search","natural image","text detection","text recognition","Unified framework"],"keywords_other":["Sensitivity and Specificity","Algorithms","Text detection","Reproducibility of Results","Writing","Pattern Recognition, Automated","Natural Language Processing","Reading","Image Interpretation, Computer-Assisted","Unified framework","Artificial Intelligence","Image Enhancement","Text recognition","Arbitrary orientation","Photography","Natural images","Documentation"],"max_cite":78.0,"pub_year":2014.0,"sources":"['ieee', 'scp', 'wos']","rawkeys":["automated","text detection","photography","dictionary search","image interpretation","algorithms","natural images","writing","reading","sensitivity and specificity","documentation","reproducibility of results","unified framework","natural image","text recognition","image enhancement","arbitrary orientation","artificial intelligence","computer-assisted","arbitrary orientations","natural language processing","pattern recognition"],"tags":["automated","text detection","photography","dictionary search","image interpretation","machine learning","algorithms","natural images","writing","reading","sensitivity and specificity","documentation","reproducibility of results","unified framework","text recognition","image enhancement","arbitrary orientation","computer-assisted","natural language processing","pattern recognition"]},{"p_id":33784,"title":"LightNet: A versatile, standalone matlab-based environment for deep learning: Simplify deep learning in hundreds of lines of code","abstract":"\u00a9 2016 Copyright held by the owner\/author(s).LightNet is a lightweight, versatile, purely Matlab-based deep learning framework. The idea underlying its design is to provide an easy-to-understand, easy-to-use and efficient computational platform for deep learning research. The implemented framework supports major deep learn-ing architectures such as Multilayer Perceptron Networks (MLP), Convolutional Neural Networks (CNN) and Recur-rent Neural Networks (RNN). The framework also supports both CPU and GPU computation, and the switch between them is straightforward. Different applications in computer vision, natural language processing and robotics are demon-strated as experiments. Availability: the source code and data is available at: https:\/\/github.com\/yechengxi\/LightNet.","keywords_author":["Computer vision","Convolutional neural networks","Deep learning","Image understanding","Machine learning","Multilayer perceptrons","Natural language processing","Recurrent neural networks","Reinforcement learning"],"keywords_other":["Deep learning","GPU computation","Lines of code","Convolutional neural network","Source codes","Computational platforms","NAtural language processing","Multilayer perceptron network (MLP)"],"max_cite":2.0,"pub_year":2016.0,"sources":"['scp']","rawkeys":["gpu computation","multilayer perceptrons","convolutional neural networks","deep learning","machine learning","natural language processing","recurrent neural networks","image understanding","lines of code","computational platforms","convolutional neural network","multilayer perceptron network (mlp)","reinforcement learning","computer vision","source codes"],"tags":["neural networks","gpu computing","machine learning","natural language processing","reinforcement learning","software","image understanding","lines of code","convolutional neural network","multi layer perceptron","computer vision","computing platform"]},{"p_id":48120,"title":"ENTERPRISE COGNITIVE COMPUTING APPLICATIONS: OPPORTUNITIES AND CHALLENGES","abstract":"IEEE Enterprise cognitive computing applications are generating a great deal of excitement for organizations. However their business impact is yet to emerge on a large scale. An important reason for this is a lack of understanding of how such applications can contribute to a company&#x2019;s business objectives, and of the challenges associated with implementing them. In this article we provide an overview of cognitive computing applications for the enterprise. In particular we provide a classification of opportunities for developing enterprise cognitive computing applications and describe challenges in implementing them. Our findings are based on a study of fifty-one initiatives of enterprise cognitive computing applications across a broad range of industries in North America, Europe and Asia-Pacific. Given the lack of systematic description regarding what is possible from enterprise cognitive computing, we believe this article will be valuable to researchers and practitioners in unpacking the black box of cognitive computing.","keywords_author":["Artifical Intelligence","Artificial intelligence","Cognitive computing tools","Enterprise cognitive computing","Enterprise cognitive computing challenges","Enterprise cognitive computing opportunities","Information systems","Machine Learning","Natural language processing","Object recognition","Organizations","Reliability"],"keywords_other":["Business impact","Cognitive Computing","Artifical intelligence","Business objectives","Black boxes"],"max_cite":0.0,"pub_year":2017.0,"sources":"['scp']","rawkeys":["artificial intelligence","black boxes","enterprise cognitive computing opportunities","business impact","machine learning","natural language processing","object recognition","reliability","enterprise cognitive computing","artifical intelligence","business objectives","information systems","cognitive computing tools","organizations","cognitive computing","enterprise cognitive computing challenges"],"tags":["cloud computing","black boxes","enterprise cognitive computing opportunities","machine learning","business impact","natural language processing","object recognition","organization","enterprise cognitive computing","reliability","artifical intelligence","business objectives","information systems","cognitive computing tools","enterprise cognitive computing challenges"]},{"p_id":50170,"title":"Optimizing authorship profiling of online messages","abstract":"Copyright \u00a9 2016 Ibadan ACM.Authorship profiling is of growing importance in the current information age, partly due to its application in digital forensics. Methodologies of profiling like any other authorship analysis consist majorly of feature extraction and application of analytical techniques. Choice of feature sets and analytical techniques may significantly affect the performance of authorship analysis. Hence, a need for methods that can help improve on the success of authorship profiling undertakings. The present study sought through experiments, the writing features, analytical technique and number of class labels that can help improve the effectiveness of profiling the country of affiliation of authors of online messages. The experiment showed that the most effective model was achieved when all feature set types in our study were used within a two-class dataset that was analysed with the Neural Network (Multilayer Perceptron) machine learning scheme. The study recommends a need for further studies in finding models that can maximize both effectiveness and efficiency in profiling the authorship of online messages.","keywords_author":["Authorship profiling","Computational linguistics","Machine learning","Natural Language Processing","Nigerian English"],"keywords_other":["ITS applications","Authorship analysis","Authorship profiling","Digital forensic","Nigerians","Number of class","Effectiveness and efficiencies","NAtural language processing"],"max_cite":0.0,"pub_year":2016.0,"sources":"['scp']","rawkeys":["digital forensic","nigerian english","effectiveness and efficiencies","authorship analysis","its applications","machine learning","natural language processing","number of class","computational linguistics","nigerians","authorship profiling"],"tags":["nigerian english","effectiveness and efficiencies","authorship analysis","its applications","machine learning","natural language processing","number of class","computational linguistics","digital forensics","nigerians","authorship profiling"]},{"p_id":23548,"title":"Automatic summarization assessment through a combination of semantic and syntactic information for intelligent educational systems","abstract":"\u00a9 2015 Elsevier Ltd. All rights reserved. Summary writing is a process for creating a short version of a source text. It can be used as a measure of understanding. As grading students' summaries is a very time-consuming task, computer-assisted assessment can help teachers perform the grading more effectively. Several techniques, such as BLEU, ROUGE, N-gram co-occurrence, Latent Semantic Analysis (LSA), LSA-Ngram and LSA-ERB, have been proposed to support the automatic assessment of students' summaries. Since these techniques are more suitable for long texts, their performance is not satisfactory for the evaluation of short summaries. This paper proposes a specialized method that works well in assessing short summaries. Our proposed method integrates the semantic relations between words, and their syntactic composition. As a result, the proposed method is able to obtain high accuracy and improve the performance compared with the current techniques. Experiments have displayed that it is to be preferred over the existing techniques. A summary evaluation system based on the proposed method has also been developed.","keywords_author":["Automatic grading","Automatic summary assessment","Content coverage","Intelligent tutoring systems","Natural language processing"],"keywords_other":["Automatic grading","Automatic summary assessment","Content coverage","Intelligent tutoring system","NAtural language processing"],"max_cite":10.0,"pub_year":2015.0,"sources":"['scp']","rawkeys":["natural language processing","intelligent tutoring systems","automatic grading","content coverage","intelligent tutoring system","automatic summary assessment"],"tags":["natural language processing","intelligent transportation systems","automatic grading","content coverage","automatic summary assessment"]},{"p_id":29693,"title":"Machine learning for holistic evaluation of scientific essays","abstract":"\u00a9 Springer International Publishing Switzerland 2015. In the US in particular, there is an increasing emphasis on the importance of science in education. To better understand a scientific topic, students need to compile information from multiple sources and determine the principal causal factors involved. We describe an approach for automatically inferring the quality and completeness of causal reasoning in essays on two separate scientific topics using a novel, twophase machine learning approach for detecting causal relations. For each core essay concept, we initially trained a window-based tagging model to predict which individual words belonged to that concept. Using the predictions from this first set of models, we then trained a second stacked model on all the predicted word tags present in a sentence to predict inferences between essay concepts. The results indicate we could use such a system to provide explicit feedback to students to improve reasoning and essay writing skills.","keywords_author":["Argumentation","Causal relation","Machine learning","Natural language inference","Natural language processing","NLP","Reading"],"keywords_other":["Argumentation","NLP","Causal relations","Natural languages","NAtural language processing","Reading"],"max_cite":4.0,"pub_year":2015.0,"sources":"['scp']","rawkeys":["nlp","natural languages","causal relation","machine learning","natural language processing","causal relations","argumentation","natural language inference","reading"],"tags":["natural languages","natural language processing","machine learning","causal relations","argumentation","natural language inference","reading"]},{"p_id":46078,"title":"Literature survey of statistical, deep and reinforcement learning in natural language processing","abstract":"\u00a9 2017 IEEE. This paper underlines the necessity to incorporate Deep learning and Neural networking in language models under scrutiny for Natural Language Processing. The paper describes various statistical models proposed and the limitations incurred in the same due to limited intelligence of a machine. We have discussed different neural networks highlighting the importance of Convolutional Neural Networking. We have discussed about open source software TensorFlow that works on Deep learning and the edge it has over the conventional models. Also we have recommended Reinforcement learning as an extension to neural networking which is widely used in gaming for the purpose of Natural Language Processing. We can utilize the reward-driven algorithm for better results.","keywords_author":["Deep Learning","Information Extraction","Natural Language Processing","Neural Networks","Reinforcement Learning","Statistical models","TensorFlow"],"keywords_other":["TensorFlow","Literature survey","Language model","Conventional models"],"max_cite":0.0,"pub_year":2017.0,"sources":"['scp']","rawkeys":["literature survey","deep learning","information extraction","neural networks","statistical models","natural language processing","reinforcement learning","tensorflow","language model","conventional models"],"tags":["literature survey","conventional modeling","information extraction","neural networks","statistical models","tensorflow","machine learning","natural language processing","language model","reinforcement learning"]},{"p_id":29696,"title":"Tuned and GPU-accelerated parallel data mining from comparable corpora","abstract":"\u00a9 Springer International Publishing Switzerland 2015. The multilingual nature of the world makes translation a crucial requirement today. Parallel dictionaries constructed by humans are a widely available resource, but they are limited and do not provide enough coverage for good quality translation purposes, due to out-of-vocabulary words and neologisms. This motivates the use of statistical translation systems, which are unfortunately dependent on the quantity and quality of training data. Such has a very limited availability especially for some languages and very narrow text domains. Is this research we present our improvements to Yalign\u2019s mining methodology by reimplementing the comparison algorithm, introducing a tuning scripts and by improving performance using GPU computing acceleration. The experiments are conducted on various text domains and bi-data is extracted from the Wikipedia dumps.","keywords_author":["Comparable corpora","Knowledge-free learning","Machine learning","Machine translation","NLP"],"keywords_other":["NLP","Improving performance","Parallel data mining","Statistical translation","Knowledge-free learning","Out of vocabulary words","Machine translations","Comparable corpora"],"max_cite":4.0,"pub_year":2015.0,"sources":"['scp']","rawkeys":["improving performance","nlp","out of vocabulary words","statistical translation","comparable corpora","machine learning","machine translation","parallel data mining","machine translations","knowledge-free learning"],"tags":["out of vocabulary words","statistical translation","comparable corpora","machine learning","natural language processing","parallel data mining","machine translations","improve performance","knowledge-free learning"]},{"p_id":23557,"title":"Textual risk mining for maritime situational awareness","abstract":"In this paper, we propose an auxiliary Machine Learning (ML) and Natural Language Processing (NLP) integrated system for maritime situational awareness (MSA) operations. We bring into account a new and influential asset - human intuition and perception - to the existing semi-automated decision support systems that mostly rely on numerical data collected by electronic sensors or cameras located either directly on the vessels or in the maritime command-and-control centers. For our project, we gathered weekly textual reports spanning twelve months from the United States Worldwide Threats to Shipping Reports repository that belongs to the National Geospatial-Intelligence Agency (NGA), We considered the maritime incident reports written by human operators as a valuable and accessible unstructured textual input source in which a span of text1 is called 'risk' if it expresses one of the following kinds of vessel incidents: fired, robbed, boarded, hijacked, attacked, chased, approached, kidnapped, boarding attempted, suspiciously approached or clashed with. Our approach benefits from probability distributions of some useful features annotated based on a list of lexicons that contain expressions denoting vessel types, risks types, risk associates, maritime geographical locations, dates and times. These distributions are captured and used to anchor the span of 'risks' as they are described in the textual reports. After some preprocessing steps that include tokenization, named entity extraction and part-of-speech tagging, the textual risk mining system applies a variety of sequence classification algorithms, e.g., Conditional Random Fields, Conditional Markov Models and Hidden Markov Models in order to compare the risk classification performance. Empirical results show that our NLP\/ML-based system can extract variable-length risk spans from the textual reports with about 90% correctness. \u00a9 2014 IEEE.","keywords_author":["machine learning","maritime domain awareness","maritime situational awareness","natural language processing","risk detection","sequence-based classifiers","text analysis"],"keywords_other":["Risk detections","Text analysis","Maritime domain awareness","Situational awareness","NAtural language processing"],"max_cite":10.0,"pub_year":2014.0,"sources":"['scp']","rawkeys":["risk detections","natural language processing","machine learning","risk detection","situational awareness","text analysis","maritime situational awareness","sequence-based classifiers","maritime domain awareness"],"tags":["natural language processing","machine learning","situational awareness","text analysis","maritime domain awareness","maritime situational awareness","sequence-based classifiers","risk detections"]},{"p_id":50181,"title":"Giving voice to office customers: Best practices in how office handles verbatim text feedback","abstract":"\u00a9 2016 IEEE. Microsoft Office users submit hundreds of thousands of pieces of verbatim feedback per month. How can an engineer or manager in Office find the signal in this data to make business decisions? This paper presents an overview of the Office Customer Voice (OCV) system. OCV combines classification, on-demand clustering and other machine learning techniques with a rich web UI to solve this problem. In this paper, we describe the different types of feedback received. Next, we outline the architecture used to build OCV. We then detail the text processing, classification and clustering done to reason on the data. Finally, we present challenges, future plans, and best practices that may be relevant to other teams analyzing customer feedback. We argue that this multi-pronged approach to handling customer feedback presents a pattern that other organizations can use to mature their handling of customer feedback.","keywords_author":["classification","clustering","Customer feedback","machine learning","natural language processing","nlp"],"keywords_other":["Customer feedback","Business decisions","Best practices","clustering","Classification and clustering","Microsoft Office","NAtural language processing","Machine learning techniques"],"max_cite":0.0,"pub_year":2016.0,"sources":"['scp']","rawkeys":["nlp","customer feedback","machine learning techniques","machine learning","natural language processing","clustering","classification","business decisions","best practices","microsoft office","classification and clustering"],"tags":["customer feedback","machine learning techniques","machine learning","natural language processing","clustering","classification","business decisions","best practices","microsoft office","classification and clustering"]},{"p_id":25609,"title":"A Neural Word Embeddings Approach for Multi-Domain Sentiment Analysis","abstract":"\u00a9 2010-2012 IEEE. Multi-domain sentiment analysis consists in estimating the polarity of a given text by exploiting domain-specific information. One of the main issues common to the approaches discussed in the literature is their poor capabilities of being applied on domains which are different from those used for building the opinion model. In this paper, we will present an approach exploiting the linguistic overlap between domains to build sentiment models supporting polarity inference for documents belonging to every domain. Word embeddings together with a deep learning architecture have been implemented into the \\sf {NeuroSent} tool for enabling the building of multi-domain sentiment model. The proposed technique is validated by following the \\sf{Dranziera} protocol in order to ease the repeatability of the experiments and the comparison of the results. The outcomes demonstrate the effectiveness of the proposed approach and also set a plausible starting point for future work.","keywords_author":["deep learning","multi-domain sentiment analysis","natural language processing","neural networks","Sentiment analysis","Sentiment analysis","natural language processing","neural networks","multi-domain sentiment analysis","deep learning"],"keywords_other":["Multi domains","Sentiment analysis","Learning architectures","CLASSIFICATION","MODEL","Embeddings","Domain-specific information"],"max_cite":7.0,"pub_year":2017.0,"sources":"['scp', 'wos']","rawkeys":["embeddings","learning architectures","model","domain-specific information","deep learning","multi-domain sentiment analysis","neural networks","natural language processing","classification","multi domains","sentiment analysis"],"tags":["embeddings","learning architectures","model","domain-specific information","multi-domain sentiment analysis","neural networks","machine learning","natural language processing","classification","multi domains","sentiment analysis"]},{"p_id":52233,"title":"Machine Learning for Emergent Middleware","abstract":"Highly dynamic and heterogeneous distributed systems are challenging today's middleware technologies. Existing middleware paradigms are unable to deliver on their most central promise, which is offering interoperability. In this paper, we argue for the need to dynamically synthesise distributed system infrastructures according to the current operating environment, thereby generating \"Emergent Middleware\" to mediate interactions among heterogeneous networked systems that interact in an ad hoc way. The paper outlines the overall architecture of Enablers underlying Emergent Middleware, and in particular focuses on the key role of learning in supporting such a process, spanning statistical learning to infer the semantics of networked system functions and automata learning to extract the related behaviours of networked systems. \u00a9 Springer-Verlag Berlin Heidelberg 2013.","keywords_author":["Automata learning","Automated Mediation","Interoperability","Machine learning","Natural language processing"],"keywords_other":["Automated Mediation","Statistical learning","Middleware technology","Heterogeneous networked systems","Automata learning","Operating environment","NAtural language processing","Heterogeneous distributed systems"],"max_cite":0.0,"pub_year":2013.0,"sources":"['scp']","rawkeys":["middleware technology","operating environment","automata learning","heterogeneous distributed systems","machine learning","natural language processing","heterogeneous networked systems","automated mediation","statistical learning","interoperability"],"tags":["middleware technology","operating environment","automata learning","heterogeneous distributed systems","machine learning","natural language processing","heterogeneous networked systems","automated mediation","statistical learning","interoperability"]},{"p_id":44047,"title":"DPPG: A Dynamic Password Policy Generation system","abstract":"\u00a9 2017 IEEE. To keep password users from creating simple and common passwords, major websites and applications provide a password-strength measure, namely a password checker. While critical requirements for a password checker to be stringent have prevailed in the study of password security, we show that regardless of the stringency, such static checkers can leak information and actually help the adversary enhance the performance of their attacks. To address this weakness, we propose and devise the Dynamic Password Policy Generator, namely DPPG, to be an effective and usable alternative to the existing password strength checker. DPPG aims to enforce an evenly-distributed password space and generate dynamic policies for users to create passwords that are diverse and that contribute to the overall security of the password database. Since DPPG is modular and can function with different underlying metrics for policy generation, we further introduce a diversity-based password security metric that evaluates the security of a password database in terms of password space and distribution. The metric is useful as a countermeasure to well-crafted offline cracking algorithms and theoretically illustrates why DPPG works well.","keywords_author":["Authentication","Computer security","Information security","Machine learning","Natural language processing","Network security"],"keywords_other":["LinkedIn","Training data","Password spaces","Dynamic policy","Password strength","Password security","Security","Policy generation"],"max_cite":0.0,"pub_year":2018.0,"sources":"['scp']","rawkeys":["password security","linkedin","authentication","computer security","machine learning","natural language processing","password spaces","security","information security","policy generation","password strength","dynamic policy","training data","network security"],"tags":["password security","linkedin","authentication","computer security","machine learning","natural language processing","password spaces","security","information security","policy generation","password strength","dynamic policy","training data","network security"]},{"p_id":39965,"title":"An intelligent system framework for an automated language tutoring tool","abstract":"Automated learning methodologies employing intelligent systems, have become increasingly popular in the internet due to advancements in the field of machine learning. In our paper, we examine the case of intelligent language tutoring system (ILTS), which has helped increase productivity and reduce overhead costs by automating teaching processes. However, current ILTSs are restrictive when it comes to integrating functionalities such as context-based understanding, and allowing text input from user. This paper proposes a framework, incorporated into an intelligent language tutoring system, based on a string search algorithm that extracts only vital word patterns from a pre-defined 'items bank'. This ensures that only required Basic English patterns are acquired, thereby facilitating the system to deliver accurate context based results and handle advanced structures with ease. Comprehensive evaluation of the system's performance indicates the system has proven to be efficient and robust structure, providing favorable alternatives to Natural Language Processing (NLP) systems. \u00a9 2011 IEEE.","keywords_author":["Artificial Intelligence","Automation","Computational Intelligence","Data gathering","Intelligent Systems","Machine Learning"],"keywords_other":["Comprehensive evaluation","Advanced structure","String search","Machine-learning","Automated learning","Overhead costs","System's performance","Data gathering","Teaching process","Tutoring system","Text input","Natural language processing","Context-based"],"max_cite":1.0,"pub_year":2011.0,"sources":"['scp']","rawkeys":["system's performance","artificial intelligence","automated learning","context-based","advanced structure","tutoring system","comprehensive evaluation","machine learning","intelligent systems","automation","natural language processing","data gathering","teaching process","string search","text input","machine-learning","computational intelligence","overhead costs"],"tags":["system's performance","automated learning","automated","context-based","advanced structure","comprehensive evaluation","machine learning","intelligent systems","natural language processing","data gathering","teaching process","string search","text input","tutoring system","computational intelligence","overhead costs"]},{"p_id":1053,"title":"Document level semantic comprehension of noisy text streams via convolutional neural networks","abstract":"Content comprehension in text is one of the challenges in natural language processing. Understanding text at a low level has become increasingly relevant due to the surge in the amount of content on the web space, where most of it is stream data. In our case, data streams are considered to be an ordered sequence of short and noisy textual messages that are read once or fewer number of times, for example tweets. Our approach entails processing and interpreting streaming texts at document level in mini-batches via deep convolutional networks for opinion, semantic or relationship analysis. Training our model is iterative and incremental, where documents are learnt by understanding the sentence structure and content in vector form based on a known offline model. The model however, incrementally adapts to the changing textual patterns. Our conceptual framework design is distributed in nature such that a pipeline of inputs, deep processing framework and output will be coordinated by the Apache Storm framework. This design is applicable in real time sentiment analysis, opinion mining, and stance detection among others.","keywords_author":null,"keywords_other":["deep convolutional networks","noisy textual messages","Storms","convolutional neural networks","Feature extraction","Training","Semantics","natural language processing","text understanding","Machine learning","text analysis","Neural networks","neural nets","noisy text streams","content comprehension","data streams","document level semantic comprehension"],"max_cite":null,"pub_year":2017.0,"sources":"['ieee']","rawkeys":["deep convolutional networks","noisy textual messages","convolutional neural networks","neural networks","semantics","storms","training","machine learning","natural language processing","text understanding","text analysis","feature extraction","neural nets","noisy text streams","content comprehension","data streams","document level semantic comprehension"],"tags":["noisy textual messages","neural networks","data stream","semantics","training","machine learning","natural language processing","text understanding","storm","text analysis","feature extraction","convolutional neural network","noisy text streams","content comprehension","document level semantic comprehension"]},{"p_id":46113,"title":"Mining the demographics of political sentiment from twitter using learning from label proportions","abstract":"\u00a9 2017 IEEE. Opinion mining and demographic attribute inference have many applications in social science. In this paper, we propose models to infer daily joint probabilities of multiple latent attributes from Twitter data, such as political sentiment and demographic attributes. Since it is costly and time-consuming to annotate data for traditional supervised classification, we instead propose scalable Learning from Label Proportions (LLP) models for demographic and opinion inference using U.S. Census, national and state political polls, and Cook partisan voting index as population level data. In LLP classification settings, the training data is divided into a set of unlabeled bags, where only the label distribution of each bag is known, removing the requirement of instance-level annotations. Our proposed LLP model, Weighted Label Regularization (WLR), provides a scalable generalization of prior work on label regularization to support weights for samples inside bags, which is applicable in this setting where bags are arranged hierarchically (e.g., county-level bags are nested inside of state-level bags). We apply our model to Twitter data collected in the year leading up to the 2016 U.S. presidential election, producing estimates of the relationships among political sentiment and demographics over time and place. We find that our approach closely tracks traditional polling data stratified by demographic category, resulting in error reductions of 28-44% over baseline approaches. We also provide descriptive evaluations showing how the model may be used to estimate interactions among many variables and to identify linguistic temporal variation, capabilities which are typically not feasible using traditional polling methods.","keywords_author":["Data Mining","Label Regularization","Learning from Label Proportions","LLP","Machine Learning","NLP"],"keywords_other":["Presidential election","Supervised classification","Temporal variation","Population levels","Error reduction","Joint probability","Label distribution","Learning from Label Proportions"],"max_cite":0.0,"pub_year":2017.0,"sources":"['scp']","rawkeys":["supervised classification","learning from label proportions","data mining","nlp","joint probability","llp","machine learning","presidential election","population levels","error reduction","label distribution","label regularization","temporal variation"],"tags":["supervised classification","learning from label proportions","data mining","joint probability","llp","machine learning","natural language processing","presidential election","population levels","error reduction","label distribution","label regularization","temporal variation"]},{"p_id":39970,"title":"Automatic cross-language plagiarism detection","abstract":"In this paper a new method to detect cross-language plagiarism in electronic documents is presented. The method is based on natural language processing techniques and machine learning methods. Preliminary results show that the system is able to identify the Internet sources of plagiarism of texts written in Spanish that have been copied (by humans and machines) from English sources. \u00a9 2011 IEEE.","keywords_author":["Cross-language plagiarism detection","machine learning","natural language processing","support vector machines"],"keywords_other":["Internet sources","Machine-learning","Support vector","Machine learning methods","Plagiarism detection","Electronic document","NAtural language processing"],"max_cite":1.0,"pub_year":2011.0,"sources":"['scp']","rawkeys":["machine learning methods","cross-language plagiarism detection","support vector","machine learning","natural language processing","support vector machines","machine-learning","plagiarism detection","electronic document","internet sources"],"tags":["machine learning methods","cross-language plagiarism detection","machine learning","natural language processing","support vector","plagiarism detection","electronic document","internet sources"]},{"p_id":48162,"title":"An independent-domain natural language interface for relational database: Case Arabic language","abstract":"\u00a9 2016 IEEE. Making information stored in database accessible for non expert users, has become one of the problems of great interest for the research community of database querying system. Hence for overriding the complexity of using database language such as Structured Query Language (SQL), the using of natural language can be a very important and simple method. But without helps computer cannot understand this language. For that its necessary to develop an interface able to translate natural language query into database query language. In this paper we present the architecture of generic interface for querying database using Arabic language. This interface functions independently of database domain and has the capacity to improve through experience its knowledge base.","keywords_author":["Arabic Natural Language","Extended Context Free Grammar (ECFG)","Machine Learning","Natural Language Processing (NLP)","Relational Databases","XML schema"],"keywords_other":["Extended Context Free Grammar (ECFG)","Relational Database","Structured query languages","XML schemas","Natural language interfaces","Database query language","Natural languages","Natural language queries"],"max_cite":0.0,"pub_year":2017.0,"sources":"['scp']","rawkeys":["structured query languages","natural language interfaces","extended context free grammar (ecfg)","xml schemas","natural languages","machine learning","natural language processing (nlp)","relational database","xml schema","relational databases","natural language queries","database query language","arabic natural language"],"tags":["natural language interfaces","extended context free grammar (ecfg)","natural languages","machine learning","natural language processing","relational database","xml schema","natural language queries","database query language","structured query language","arabic natural language"]},{"p_id":29733,"title":"Sentiment classification: An approach for Indian language tweets using decision tree","abstract":"\u00a9 Springer International Publishing Switzerland 2015. This paper describes the system we used for Shared Task on Sentiment Analysis in Indian Languages (SAIL) Tweets, at MIKE-2015. Twitter is one of the most popular platform which allows users to share their opinion in the form of tweets. Since it restricts the users with 140 characters, the tweets are actually very short to carry opinions and sentiments to analyze. We take the help of a twitter training dataset in Indian Language (Hindi) and apply data mining approaches for analyzing the sentiments. We used a state-of-the-art Data Mining tool Weka to automatically classify the sentiment of Hindi tweets into positive, negative or neutral.","keywords_author":["Machine learning","NLP","Polarity identification","Sentiment analysis"],"keywords_other":["Popular platform","NLP","Sentiment analysis","State of the art","Data-mining tools","Training dataset","Indian languages","Sentiment classification"],"max_cite":4.0,"pub_year":2015.0,"sources":"['scp']","rawkeys":["sentiment classification","nlp","data-mining tools","training dataset","state of the art","machine learning","indian languages","polarity identification","sentiment analysis","popular platform"],"tags":["sentiment classification","data-mining tools","state of the art","natural language processing","machine learning","indian languages","polarity identification","training data sets","sentiment analysis","popular platform"]},{"p_id":9254,"title":"SyMSS: A syntax-based measure for short-text semantic similarity","abstract":"Sentence and short-text semantic similarity measures are becoming an important part of many natural language processing tasks, such as text summarization and conversational agents. This paper presents SyMSS, a new method for computing short-text and sentence semantic similarity. The method is based on the notion that the meaning of a sentence is made up of not only the meanings of its individual words, but also the structural way the words are combined. Thus, SyMSS captures and combines syntactic and semantic information to compute the semantic similarity of two sentences. Semantic information is obtained from a lexical database. Syntactic information is obtained through a deep parsing process that finds the phrases in each sentence. With this information, the proposed method measures the semantic similarity between concepts that play the same syntactic role. Psychological plausibility is added to the method by using previous findings about how humans weight different syntactic roles when computing semantic similarity. The results show that SyMSS outperforms state-of-the-art methods in terms of rank correlation with human intuition, thus proving the importance of syntactic information in sentence semantic similarity computation. \u00a9 2011 Elsevier B.V. All rights reserved.","keywords_author":["Linguistic tools for IS modeling","Natural language processing (NLP)","Semantic similarity","Sentence similarity","Text DBs"],"keywords_other":["Conversational agents","Natural language processing","State-of-the-art methods","Syntactic information","Parsing process","Text DBs","Semantic similarity measures","Semantic similarity","Text summarization","Rank correlation","Sentence similarity","Lexical database","Semantic information"],"max_cite":77.0,"pub_year":2011.0,"sources":"['scp']","rawkeys":["lexical database","parsing process","linguistic tools for is modeling","state-of-the-art methods","text summarization","natural language processing","natural language processing (nlp)","rank correlation","semantic similarity","semantic similarity measures","conversational agents","text dbs","syntactic information","semantic information","sentence similarity"],"tags":["lexical database","parsing process","linguistic tools for is modeling","state-of-the-art methods","text summarization","natural language processing","rank correlation","semantic similarity","semantic similarity measures","conversational agents","text dbs","syntactic information","semantic information","sentence similarity"]},{"p_id":46127,"title":"Evaluating automatic methods to extract patients' supplement use from clinical reports","abstract":"\u00a9 2017 IEEE. The widespread prevalence of dietary supplements has drawn extensive attention due to the safety and efficacy issue. Clinical notes document a great amount of detailed information on dietary supplement usage, thus providing a rich source for clinical research on supplement safety surveillance. Identification the use status of dietary supplements is one of the initial steps for the ultimate goal of the supplement safety surveillance. In this study, we built rule-based and machine learning-based classifiers to automatically classify the use status of supplements into four categories: Continuing (C), Discontinued (D), Started (S), and Unclassified (U). In comparison to the machine learning classifier trained on the same datasets, the rule-based classifier showed a better performance with F-measure in the C, D, S, U status of 0.93, 0.98, 0.95, and 0.83, respectively. We further analyzed the errors generated by the rule-based classifier. The classifier can be potentially applied to extract supplement information from clinical notes for supporting research and clinical practice related to patient safety on supplement usage.","keywords_author":["Clinical Notes","Electronic Health records","Machine Learning","Natural Language Processing"],"keywords_other":["Clinical practices","Clinical notes","F measure","Patient safety","Electronic health record","Rule-based classifier","Automatic method","Rule based"],"max_cite":0.0,"pub_year":2017.0,"sources":"['scp']","rawkeys":["patient safety","clinical practices","automatic method","electronic health record","f measure","rule based","machine learning","electronic health records","natural language processing","rule-based classifier","clinical notes"],"tags":["patient safety","clinical practices","automatic method","rule based","machine learning","electronic health records","natural language processing","rule-based classifier","f-measure","clinical notes"]},{"p_id":52276,"title":"Machine learning with templates","abstract":"New methods are presented for the machine recognition and learning of categories, patterns, and knowledge. A probabilistic machine learning algorithm is described that scales favorably to extremely large datasets, avoids local minima problems, and provides fast learning and recognition speeds. Templates may be created using an evolutionary algorithm described here, constructed with other machine learning methods, designed by a human expert or synthesized using a combination of these methods. Each template has a prototype and matching function which can help improve generalization. These methods have applications in bioinformatics, financial data mining, goal-based planners, handwriting recognition, machine vision, natural language processing \/ understanding, search engines, strategy such as business and games and voice recognition.","keywords_author":["Evolution","Machine learning","Pattern recognition","Polymorphous","Template"],"keywords_other":["Template","Handwriting recognition","Probabilistic machines","Evolution","Polymorphous","Machine learning methods","Financial data minings","NAtural language processing"],"max_cite":0.0,"pub_year":2012.0,"sources":"['scp']","rawkeys":["machine learning methods","polymorphous","probabilistic machines","financial data minings","machine learning","natural language processing","pattern recognition","template","evolution","handwriting recognition"],"tags":["machine learning methods","probabilistic machines","financial data minings","machine learning","natural language processing","pattern recognition","template","biological","handwriting recognition","polymorphism"]},{"p_id":7221,"title":"Posterior regularization for structured latent variable models","abstract":"We present posterior regularization, a probabilistic framework for structured, weakly supervised learning. Our framework efficiently incorporates indirect supervision via constraints on posterior distributions of probabilistic models with latent variables. Posterior regularization separates model complexity from the complexity of structural constraints it is desired to satisfy. By directly imposing decomposable regularization on the posterior moments of latent variables during learning, we retain the computational efficiency of the unconstrained model while ensuring desired constraints hold in expectation. We present an efficient algorithm for learning with posterior regularization and illustrate its versatility on a diverse set of structural constraints such as bijectivity, symmetry and group sparsity in several large scale experiments, including multi-view learning, cross-lingual dependency grammar induction, unsupervised part-of-speech induction, and bitext word alignment.1 \u00a9 2010 Kuzman Ganchev, Jo\u00e3o Gra\u00e7a, Jennifer Gillenwater and Ben Taskar.","keywords_author":["Latent variables models","Natural language processing","Posterior regularization framework","Prior knowledge","Unsupervised learning"],"keywords_other":["Multi-view learning","Word alignment","Model complexity","Posterior distributions","Structural constraints","Part Of Speech","Latent variable models","Latent variables models","Regularization framework","Bijectivity","Latent variable","Probabilistic models","Large scale experiments","Prior knowledge","Efficient algorithm","Weakly supervised learning","Probabilistic framework","NAtural language processing","Cross-lingual","Dependency grammar"],"max_cite":158.0,"pub_year":2010.0,"sources":"['ieee', 'scp', 'wos']","rawkeys":["multi-view learning","bijectivity","latent variables models","efficient algorithm","weakly supervised learning","large scale experiments","cross-lingual","posterior distributions","model complexity","probabilistic framework","probabilistic models","structural constraints","unsupervised learning","posterior regularization framework","dependency grammar","latent variable models","word alignment","natural language processing","regularization framework","latent variable","part of speech","prior knowledge"],"tags":["multi-view learning","bijectivity","efficient algorithm","weakly supervised learning","large scale experiments","cross-lingual","posterior distributions","model complexity","probabilistic framework","probabilistic models","structural constraints","unsupervised learning","posterior regularization framework","dependency grammar","latent variable models","word alignment","natural language processing","regularization framework","latent variable","part of speech","prior knowledge"]},{"p_id":44086,"title":"Evaluating machine learning algorithms for fake news detection","abstract":"\u00a9 2017 IEEE. This paper explores the application of natural language processing techniques for the detection of 'fake news', that is, misleading news stories that come from non-reputable sources. Using a dataset obtained from Signal Media and a list of sources from OpenSources.co, we apply term frequency-inverse document frequency (TF-IDF) of bi-grams and probabilistic context free grammar (PCFG) detection to a corpus of about 11,000 articles. We test our dataset on multiple classification algorithms-Support Vector Machines, Stochastic Gradient Descent, Gradient Boosting, Bounded Decision Trees, and Random Forests. We find that TF-IDF of bi-grams fed into a Stochastic Gradient Descent model identifies non-credible sources with an accuracy of 77.2%, with PCFGs having slight effects on recall.","keywords_author":["Classification algorithms","Fake-news detection","Machine learning","Natural language processing"],"keywords_other":null,"max_cite":0.0,"pub_year":2018.0,"sources":"['scp']","rawkeys":["natural language processing","machine learning","classification algorithms","fake-news detection"],"tags":["fake news detection","natural language processing","classification algorithm","machine learning"]},{"p_id":21564,"title":"Evaluation of an integrated multi-task machine learning system with humans in the loop","abstract":"Performance of a cognitive personal assistant, RADAR, consisting of multiple machine learning components, natural language processing, and optimization was examined with a test explicitly developed to measure the impact of integrated machine learning when used by a human user in a real world setting. Three conditions (conventional tools, Radar without learning, and Radar with learning) were evaluated in a large-scale, between-subjects study. The study revealed that integrated machine learning does produce a positive impact on overall performance. This paper also discusses how specific machine learning components contributed to human-system performance.","keywords_author":["Evaluation","Intelligent systems","Machine learning","Mixed-initiative assistants"],"keywords_other":["Human users","Real world setting","system performances","Overall performance","machine-learning","Multi-task","Performance metrics","Natural Language Processing (NLP)","(I ,J) conditions"],"max_cite":14.0,"pub_year":2007.0,"sources":"['scp']","rawkeys":["performance metrics","machine learning","natural language processing (nlp)","evaluation","intelligent systems","(i ,j) conditions","multi-task","real world setting","system performances","overall performance","machine-learning","mixed-initiative assistants","human users"],"tags":["performance metrics","conditioning","machine learning","natural language processing","evaluation","intelligent systems","real world setting","multi-task","system performance","overall performance","mixed-initiative assistants","human users"]},{"p_id":33852,"title":"SparkCRF: A parallel implementation of CRFs algorithm with spark","abstract":"\u00a9 2016, Science Press. All right reserved. Condition random fields has been successfully applied to various applications in text analysis, such as sequence labeling, Chinese words segmentation, named entity recognition, and relation extraction in nature language processing. The traditional CRFs tools in single-node computer meet many challenges when dealing with large-scale texts. For one thing, the personal computer experiences the performance bottleneck; For another, the server fails to tackle the analysis efficiently. And upgrading hardware of the server to promote the capability of computing is not always feasible due to the cost constrains. To tackle these problems, in light of the idea of \"divide and conquer\", we design and implement SparkCRF, which is a kind of distributed CRFs running on cluster environment based on Apache Spark. We perform three experiments using NLPCC2015 and the 2nd International Chinese Word Segmentation Bakeoff datasets, to evaluate SparkCRF from the aspects of performance, scalability and accuracy. Results show that: 1)compared with CRF++, SparkCRF runs almost 4 times faster on our cluster in sequence labeling task; 2)it has good scalability by adjusting the number of working cores; 3)furthermore, SparkCRF has comparable accuracy to the state-of-the-art CRF tools, such as CRF++ in the task of text analysis.","keywords_author":["Big data","Condition random fields (CRFs)","Distributed computing","Machine learning","Spark"],"keywords_other":["Random fields","Named entity recognition","Performance bottlenecks","Chinese word segmentation","Nature language processing","Cluster environments","Parallel implementations","Design and implements"],"max_cite":2.0,"pub_year":2016.0,"sources":"['scp']","rawkeys":["design and implements","distributed computing","chinese word segmentation","big data","random fields","named entity recognition","machine learning","parallel implementations","nature language processing","performance bottlenecks","spark","condition random fields (crfs)","cluster environments"],"tags":["distributed computing","chinese word segmentation","big data","conditional random field","design and implementations","named entity recognition","machine learning","natural language processing","parallel implementations","performance bottlenecks","random forests","spark","cluster environments"]},{"p_id":39997,"title":"A several-step approach to de-identify clinical records Une approche \u00e0 plusieurs \u00e9tapes pour anonymiser des documents m\u00e9dicaux","abstract":"This paper describes a procedure of multi-step anonymization that was developed on French narrative clinical records in cardiology. Our approach is based upon a combination of several methods, using a rule-based approach followed by the application of a machinelearning system. The combination of the two outperformed each of them taken separately (0.881 F-measure), with a recall (0.912) higher than precision (0.851). \u00a9 2011 Lavoisier, Paris.","keywords_author":["Health insurance portability and accountability act","Machine learning","Medical records","Natural language processing","Privacy of patient data"],"keywords_other":["Machine-learning","F-measure","Medical record","Multi-step","Health insurance portability and accountability acts","Rule-based approach","Clinical records","NAtural language processing","Anonymization"],"max_cite":1.0,"pub_year":2011.0,"sources":"['scp']","rawkeys":["multi-step","health insurance portability and accountability acts","machine learning","medical record","natural language processing","f-measure","clinical records","privacy of patient data","machine-learning","anonymization","medical records","rule-based approach","health insurance portability and accountability act"],"tags":["multi-step","health insurance portability and accountability acts","machine learning","medical record","natural language processing","f-measure","clinical records","privacy of patient data","anonymization","rule-based approach"]},{"p_id":50238,"title":"Comparison of different approaches for hotels deduplication","abstract":"\u00a9 Springer International Publishing Switzerland 2016. The present article addresses the problem of a hotel deduplication. Obvious approaches, such as name or location comparisons, fail, because hotel descriptions differ among different databases. The most accurate approach to solve this problem is to use the professionally trained content managers, but it is expensive, hence an automatic solution should be implemented. We propose a method to improve a hypothesis that a pair of hotels is identical, and compare its performance with alternative solutions. The proposed method satisfies business requirements set for the precision and recall of the hotel deduplication task. The method is based on machine learning approach with the use of some unique features, including those built with the help of computer vision algorithms.","keywords_author":["Deduplication","Entity resolution","Machine learning","Natural language processing"],"keywords_other":["Alternative solutions","Computer vision algorithms","Business requirement","De duplications","Precision and recall","Entity resolutions","NAtural language processing","Unique features"],"max_cite":0.0,"pub_year":2016.0,"sources":"['scp']","rawkeys":["deduplication","unique features","entity resolutions","alternative solutions","machine learning","business requirement","natural language processing","precision and recall","de duplications","computer vision algorithms","entity resolution"],"tags":["deduplication","unique features","alternative solutions","machine learning","business requirement","natural language processing","precision and recall","de duplications","computer vision algorithms","entity resolution"]},{"p_id":44095,"title":"Integrated social media knowledge capture model in medical domain of Indonesia","abstract":"\u00a9 2017 IEEE. The Social Media Platforms, as the one of largest part of today data traffic on the Internet, disseminate a vast volume of information, including medical information in it. Knowledge management system (KMS) approach is applied with a purpose to capture, maintain, and manage tacit or explicit knowledge available and collected within the social media platforms, organization's database, knowledge base, or document repository. By adding Indonesian Natural Language Processing (InaNLP) and Data Mining approach, our research proposed a model which is theoretically designed to improve the previous research related to social media knowledge capture model and enhance its accuracy and reliability of knowledge retrieved compared to previous knowledge capture model. Despite the proposed framework is still a theoretical model, it can be applied in medical sector of Indonesia as a big picture of medical knowledge capture model. This model mainly aimed for medical practitioner to give a quick suggestion of the diseases regarding to the early diagnose which has been taken in the first place.","keywords_author":["Data Mining","Knowledge Capture","Knowledge Management System","Machine Learning","Medical Knowledge","Natural Language Processing"],"keywords_other":["Knowledge capture","Medical knowledge","Knowledge management system","Document repositories","Social media platforms","Medical information","Theoretical modeling","Medical practitioner"],"max_cite":0.0,"pub_year":2018.0,"sources":"['scp']","rawkeys":["medical information","document repositories","data mining","theoretical modeling","medical practitioner","social media platforms","machine learning","medical knowledge","natural language processing","knowledge capture","knowledge management system"],"tags":["medical information","document repositories","data mining","theoretical modeling","medical practitioner","social media platforms","machine learning","medical knowledge","natural language processing","knowledge capture","knowledge management system"]},{"p_id":23617,"title":"Semi-supervised learning and domain adaptation in natural language processing","abstract":"\u00a9 2013 by Morgan & Claypool. This book introduces basic supervised learning algorithms applicable to natural language processing (NLP) and shows how the performance of these algorithms can often be improved by exploiting the marginal distribution of large amounts of unlabeled data. One reason for that is data sparsity, i.e., the limited amounts of data we have available in NLP. However, in most real-world NLP applications our labeled data is also heavily biased. This book introduces extensions of supervised learning algorithms to cope with data sparsity and different kinds of sampling bias. This book is intended to be both readable by first-year students and interesting to the expert audience.My intention was to introduce what is necessary to appreciate the major challenges we face in contemporary NLP related to data sparsity and sampling bias, without wasting too much time on details about supervised learning algorithms or particular NLP applications. I use text classification, part-of-speech tagging, and dependency parsing as running examples, and limit myself to a small set of cardinal learning algorithms. I have worried less about theoretical guarantees (\"this algorithm never does too badly\") than about useful rules of thumb (\"in this case this algorithm may perform really well\"). In NLP, data is so noisy, biased, and non-stationary that few theoretical guarantees can be established and we are typically left with our gut feelings and a catalogue of crazy ideas. I hope this book will provide its readers with both. Throughout the book we include snippets of Python code and empirical evaluations, when relevant.","keywords_author":["Learning under bias","Machine learning","Natural language processing","Semi-supervised learning"],"keywords_other":["Part of speech tagging","Marginal distribution","Text classification","Learning under bias","Semi- supervised learning","Theoretical guarantees","Empirical evaluations","First year students"],"max_cite":10.0,"pub_year":2013.0,"sources":"['scp']","rawkeys":["first year students","semi- supervised learning","machine learning","natural language processing","theoretical guarantees","part of speech tagging","semi-supervised learning","empirical evaluations","marginal distribution","learning under bias","text classification"],"tags":["machine learning","natural language processing","theoretical guarantees","part of speech tagging","semi-supervised learning","first-year students","marginal distribution","empirical evaluations","learning under bias","text classification"]},{"p_id":37954,"title":"Differentiating communication styles of leaders on the Linux kernel mailing list","abstract":"Copyright 2016 ACM. Much communication between developers of free, libre, and open source software (FLOSS) projects happens on email mailing lists. Geographically and temporally dispersed development teams use email as an asynchronous, centralized, persistently stored institutional memory for sharing code samples, discussing bugs, and making decisions. Email is especially important to large, mature projects, such as the Linux kernel, which has thousands of developers and a multi-layered leadership structure. In this paper, we collect and analyze data to understand the communication patterns in such a community. How do the leaders of the Linux Kernel project write in email? What are the salient features of their writing, and can we discern one leader from another? We find that there are clear written markers for two leaders who have been particularly important to recent discussions of leadership style on the Linux Kernel Mailing List (LKML): Linux Torvalds and Greg Kroah-Hartman. Furthermore, we show that it is straightforward to use a machine learning strategy to automatically differentiate these two leaders based on their writing. Our findings will help researchers understand how this community works, and why there is occasional controversy regarding differences in communication styles on the LKML.","keywords_author":["Data mining","Email","Linux","Machine learning","Natural language processing","Open source software","Text mining"],"keywords_other":["Text mining","Salient features","Leadership style","Communication styles","Communication pattern","Linux kernel mailing lists","Development teams","NAtural language processing"],"max_cite":1.0,"pub_year":2016.0,"sources":"['scp']","rawkeys":["linux kernel mailing lists","email","data mining","leadership style","open source software","text mining","machine learning","communication styles","natural language processing","salient features","communication pattern","linux","development teams"],"tags":["linux kernel mailing lists","email","data mining","leadership style","open source software","text mining","machine learning","communication styles","natural language processing","salient features","communication pattern","linux","development teams"]},{"p_id":31811,"title":"Sentiment analysis and text summarization of online reviews: A survey","abstract":"\u00a9 2016 IEEE. Sentiment analysis and text summarization has evoke the interest of many scientists and researchers in last few years, since the textual data has become useful for many real world applications and problems. Sentiment analysis is a machine learning approach in which machine learns and analyze the sentiments, emotions etc about some text data like reviews about movies or products. These reviews are increasing day by day, due to which summarization of reviews comes in role where summarized form of text in needed, which provides useful information from the large number of reviews. It is very difficult for a human being to extract useful data or summarize it from the very large document. In Text summarization, importance of sentences is decided based on linguistic features of sentences. This paper provides the comprehensive overview of recent and past research on sentiment analysis and text summarization and provides excellent research queries and approaches for future aspects.","keywords_author":["Information extraction","Natural language processing","Opinion mining","Sentiment analysis","Text summarization"],"keywords_other":["Online reviews","Textual data","Machine learning approaches","Sentiment analysis","Text summarization","Linguistic features","NAtural language processing","Opinion mining"],"max_cite":3.0,"pub_year":2016.0,"sources":"['scp']","rawkeys":["information extraction","online reviews","textual data","text summarization","natural language processing","linguistic features","sentiment analysis","opinion mining","machine learning approaches"],"tags":["information extraction","online reviews","textual data","text summarization","natural language processing","linguistic features","sentiment analysis","opinion mining","machine learning approaches"]},{"p_id":31812,"title":"Text summarisation based on human language technologies and its applications","abstract":"The research work carried out in this thesis focuses on Text Summarisation, proposing and developing compendium Text Summarisation tool. This tool takes into account the cognitive perspective, that provides insights of how humans summarise, as well as computational issues needed for its automation. For evaluating compendium, we selected different corpora belonging to a wide range of domains and textual genres. Moreover, we also performed an extrinsic evaluation, applying compendium to three Human Languages Technologies tasks: question answering, opinion minng and text classification. The results obtained show that the generated summaries are very appropriate both for individual users as well as for other Human Language Technologies applications. \u00a9 2012 Sociedad Espa\u00e3ola para el Procesamiento de Lenguaje Natural.","keywords_author":["Human Language Technologies","Natural Language Processing","Text Summarisation"],"keywords_other":null,"max_cite":3.0,"pub_year":2012.0,"sources":"['scp']","rawkeys":["text summarisation","human language technologies","natural language processing"],"tags":["text summarisation","human language technologies","natural language processing"]},{"p_id":40003,"title":"Chinese event place phrase recognition of emergency event using maximum entropy","abstract":"This paper provides a new method combining Maximum Entropy with rules for identify event place phrase. Firstly, all phrases which not include event trigger are extracted from event mention, and a rule base about event place phrases analyzes and filters these phrases for obtaining the phrase candidate set. Secondly, we explore some rich text features from three kinds of linguistics features that contain phrase, event trigger and context information. Thirdly, in order to establish a train set, we use some feature words representing these text features to build feature vector space. Then, a machine learning model to identify event place phrase is trained by using L-BFGS functions algorithm. At last, this predictive model is used to classify the test set. The result shows that the method is efficient. In open test, the recall, precision and F-measure reach 0.6296296, 0.8095238 and 0.7083333 respectively. \u00a9 2011 IEEE.","keywords_author":["Chinese event place phrase recognition","Machine learning","Maximum Entropy","Natural language processing","Rule-based filtering"],"keywords_other":["Test sets","Maximum entropy","Predictive models","Event trigger","Machine-learning","A-train","Rule-based filtering","Feature words","Rule base","Chinese event place phrase recognition","F-measure","Feature vectors","Context information","Text feature","Natural language processing"],"max_cite":1.0,"pub_year":2011.0,"sources":"['scp']","rawkeys":["test sets","maximum entropy","rule base","predictive models","machine learning","natural language processing","feature vectors","text feature","event trigger","f-measure","chinese event place phrase recognition","machine-learning","a-train","rule-based filtering","feature words","context information"],"tags":["test sets","maximum entropy","predictive models","rule based","machine learning","natural language processing","feature vectors","text feature","f-measure","evapotranspiration","chinese event place phrase recognition","a-train","rule-based filtering","feature words","context information"]},{"p_id":37957,"title":"Sentiment classification for unlabeled dataset using Doc2vec with JST","abstract":"Copyright is held by the owner\/author(s). Supervised learning require sentiment labeled corpus for training. But it is hard to apply automatic sentiment classification system to new domain because labeled dataset construction costs a lot of time. Meanwhile, researches using Doc2vec based document representation beat out other sentiment classification researches. However, these document representation methods only represent documents' context or sentiment. In this paper, we proposed supervised learning scheme for unlabeled corpus and also proposed document representation method which can simultaneously represent documents' context and sentiment.","keywords_author":["Doc2vec","JST","Machine learning","Natural language processing","Sentiment analysis"],"keywords_other":["Construction costs","Doc2vec","Document Representation","Sentiment analysis","Labeled dataset","NAtural language processing","Sentiment classification"],"max_cite":1.0,"pub_year":2016.0,"sources":"['scp']","rawkeys":["doc2vec","sentiment classification","construction costs","labeled dataset","jst","machine learning","natural language processing","document representation","sentiment analysis"],"tags":["doc2vec","sentiment classification","construction costs","labeled dataset","jst","machine learning","natural language processing","document representation","sentiment analysis"]},{"p_id":52295,"title":"Evaluating NLP features for automatic prediction of language impairment using child speech transcripts","abstract":"Language impairment (LI) in children is pervasive in all walks of life. Automatic prediction of LI is useful as a first pass for speech language pathologists in identifying prospective children with LI. Previous work in the automatic prediction of LI has explored various features, mostly shallow and surface level features. In this paper, we evaluate deeper Natural Language Processing (NLP) features such as syntactic, semantic and entity grid model features, along with narrative structure and quality features in the prediction of LI using child language transcripts. Our experiments show that narrative structure and quality features along with a combination of other features are helpful in the prediction of LI in storytelling narratives.","keywords_author":["Language impairment","Machine learning","Natural language processing"],"keywords_other":["Child speech","Speech language pathologists","Automatic prediction","Grid model","Quality features","NAtural language processing","Language impairments","Narrative structures"],"max_cite":0.0,"pub_year":2012.0,"sources":"['scp']","rawkeys":["quality features","speech language pathologists","machine learning","natural language processing","language impairments","child speech","automatic prediction","grid model","language impairment","narrative structures"],"tags":["quality features","speech language pathologists","machine learning","natural language processing","child speech","automatic prediction","grid model","language impairment","narrative structures"]},{"p_id":52296,"title":"Machine learning approach for argument extraction of bio-molecular events","abstract":"The main goal of Biomedical Natural Language Processing (BioNLP) is to capture biomedical phenomena from textual data by extracting relevant entities and information or relations between biomedical entities such as proteins and genes. Previous research was focussed on extracting only binary relations, but in recent times the focus is shifted towards extracting more complex relations in the form of bio-molecular events that may include several entities or other relations. In this paper we propose a machine learning approach based on Conditional Random Field (CRF) to extract the arguments of bio-molecular events. The overall task involves identification of event triggers from texts, classification of them into some predefined categories and determining the arguments of these events. We identify and implement a set of features in the forms of statistical and linguistic features that represent various morphological, syntactic and contextual information. Experiments on the benchmark setup of BioNLP 2009 shared task show the recall, precision and F-measure values of 45.75%, 78.93% and 57.91%, respectively. \u00a9 2012 IEEE.","keywords_author":["Bio-molecular event","Conditional Random Field","Machine Learning"],"keywords_other":["Binary relation","Textual data","Event trigger","Contextual information","Bio-molecular","Machine learning approaches","F-measure","Linguistic features","Conditional random field","NAtural language processing"],"max_cite":0.0,"pub_year":2012.0,"sources":"['scp']","rawkeys":["bio-molecular","contextual information","conditional random field","textual data","machine learning","natural language processing","linguistic features","event trigger","f-measure","binary relation","bio-molecular event","machine learning approaches"],"tags":["bio-molecular","contextual information","conditional random field","textual data","machine learning","natural language processing","linguistic features","f-measure","evapotranspiration","binary relation","bio-molecular event","machine learning approaches"]},{"p_id":46152,"title":"Deep gramulator: Improving precision in the classification of personal health-experience tweets with deep learning","abstract":"\u00a9 2017 IEEE. Health surveillance is an important task to track the happenings related to human health, and one of its areas is pharmacovigilance. Pharmacovigilance tracks and monitors safe use of pharmaceutical products. Pharmacovigilance involves tracking side effects that may be caused by medicines and other health related drugs. Medical professionals have a difficult time collecting this information. It is anticipated that social media could help to collect this data and track side effects. Twitter data can be used for this task given that users post their personal health related experiences on-line. One problem with Twitter data, however, is that it contains a lot of noise. Therefore, an approach is needed to remove the noise. In this paper, several machine learning algorithms including deep neural nets are used to build classifiers that can help to detect these Personal Experience Tweets (PETs). Finally, we propose a method called the Deep Gramulator that improves results. Results of the analysis are presented and discussed.","keywords_author":["deep learning","natural language processing"],"keywords_other":["Health surveillances","Medical professionals","Personal experience","Pharmaceutical products","Pharmacovigilance","Twitter datum","Deep neural nets","Personal health"],"max_cite":0.0,"pub_year":2017.0,"sources":"['scp']","rawkeys":["deep neural nets","medical professionals","personal experience","deep learning","pharmacovigilance","natural language processing","health surveillances","pharmaceutical products","personal health","twitter datum"],"tags":["deep neural nets","medical professionals","personal experience","pharmacovigilance","natural language processing","health surveillances","machine learning","pharmaceutical products","personal health","twitter datum"]},{"p_id":44099,"title":"Design of an intelligent agent for stimulating brainstorming","abstract":"\u00a9 2018 Association for Computing Machinery. In recent years, brainstorming has gradually become a mainstream approach used by groups of people to collect ideas before making decisions. This approach is useful in handling the predicament of lacking ideas on specific topics for individual members, and breaking through the limitations of their own experiences. However, in the process of group brainstorming, there exists some common problems such as the lack of comprehensive discussion contents due to the limited experiences recalled by the members, the stagnation in the progress, and so on. Therefore, in this paper, we propose the design of an intelligent agent system that plays the vital role of facilitator to participate in the brainstorming process; in other words, our system can discuss a designated topic on an on-line chatroom with other members in a brainstorming discussion. This system collects textual data and establishes the knowledge model in a specific domain with machine learning methods prior to the brainstorming. Then, in the brainstorming process, it attempts to conjecture the topic of the current dialogues, determine the progress, and generate responses to the brainstorming chatroom in order to come up with diverse ideas or extend the present topic. The goal of this system is to increase the diversity and profundity of discussions, and make the whole process smooth and fruitful. The results of our experiments show that our intelligent agent is effective in helping on- line brainstorming, especially idea generations.","keywords_author":["Brainstorming","Intelligent agent","Knowledge model","Machine learning","Natural language processing"],"keywords_other":["Knowledge model","Textual data","Whole process","Making decision","Brainstorming","Machine learning methods","Idea generation","Intelligent agent system"],"max_cite":0.0,"pub_year":2018.0,"sources":"['scp']","rawkeys":["machine learning methods","making decision","intelligent agent system","textual data","brainstorming","machine learning","natural language processing","intelligent agent","whole process","idea generation","knowledge model"],"tags":["machine learning methods","knowledge modeling","making decision","intelligent agent system","textual data","brainstorming","machine learning","natural language processing","intelligent agents","whole process","idea generation"]},{"p_id":37965,"title":"Sentiment analysis and classification using lexicon-based approach and addressing polarity shift problem","abstract":"\u00a9 2005 - 2016 JATIT & LLS. All rights reserved.As we know that in recent years e-commerce has been growing, so volume of online reviews on the web is also increasing for different sides due to which we can understand that the particular product or things are good for use or not and their current status in market. In Natural Language Processing (NLP) and text mining, different models and methods are useful for text representation and categorization purposes. Bag- Of-Word (BOW) model is one such model used to model the text. But polarity shift problem is a major factor in Bag-Of-Word model which can effect on classification performance of Sentiment Analysis. In our methodology, we address the polarity shift problem by detecting, removing and modifying negation from extracted review to identify where the sentiment orientation is actually changing in given review. Our main idea is Sentiment analysis and classification which is based on machine learning approach using Lexicon based antonym dictionary. We build system for Sentence-level sentiment classification. We first extract product reviews from one of the customized shopping portal. When extracted reviews are simple sentences then system is trained to directly find its opinion target, and classify it according to its sentiment polarities i.e. is positive, negative and neutral class labels. When extracted reviews are compound and complex sentences then we first split it into subsentences and build a model based on some rules to detect, remove and modify polarity shift in contrast of negation to identify where the sentiment orientation is changing in compound or complex sentences. After that, we classify review according to its polarity and determine the targets of opinion given in review. Furthermore, we extend our system for opinion summarization based on opinion features or aspects and graphically represent overall summary of Positive, Negative and Neutral sentiments of customer for each product.","keywords_author":["Machine Learning","Natural Language Processing","Opinion Mining","Polarity Shift","Sentiment Analysis"],"keywords_other":null,"max_cite":1.0,"pub_year":2016.0,"sources":"['scp']","rawkeys":["natural language processing","machine learning","sentiment analysis","opinion mining","polarity shift"],"tags":["natural language processing","machine learning","sentiment analysis","opinion mining","polarity shift"]},{"p_id":35918,"title":"Machine learning approaches for chinese shallow parsers","abstract":"In this paper, we present two machine-learning algorithms, namely, transformation-based error-driven learning (TEL) and memory-based learning (MBL) to improve the performance of a Chinese shallow parser. The Algorithm not only can handle nested chunking data, but also different phrase types (e.g. NP, VP, S etc.). Results show that TEL can achieve better recall rate, yet MBL is less sensitive to nesting and requires much less computation.","keywords_author":["Machine learning algorithms","Natural language processing","Shallow parsers Introduction"],"keywords_other":["Machine learning algorithms","Shallow parsers","Natural language processing"],"max_cite":2.0,"pub_year":2003.0,"sources":"['scp']","rawkeys":["machine learning algorithms","natural language processing","shallow parsers","shallow parsers introduction"],"tags":["machine learning algorithms","natural language processing","shallow parsers","shallow parsers introduction"]},{"p_id":44110,"title":"Sentiment analysis model on weather related tweets with deep neural network","abstract":"\u00a9 2018 Association for Computing Machinery. Weather related tweets are user's comments about daily weather. We can gain useful information about how weather influence p eop le's mood by analyzing them. This is what we called opinion mining in natural language processing field. Traditional opinion mining algorithm use feature engineering to build sentence model, and classifier like naive bayes is used for further classification. However, these feature vectors can sometimes be insufficient to represent the text, and they are manually designed, highly relevant to the p roblem's background. In this work1, we propose a method modeling text based on deep learning approach, which can automatically extract text feature. As for word's vector representation, we incorporate linguistic knowled ge into word's representation, and use three different word representations in our model. The performance of the sentiment analysis system shows that our method is an efficient way analyzing user's sentiment on weather events.","keywords_author":["Deep learning","Natural language processing","Sentiment analysis"],"keywords_other":["Method model","Mining algorithms","Word representations","Weather events","Vector representations","Feature engineerings","Learning approach","Feature vectors"],"max_cite":0.0,"pub_year":2018.0,"sources":"['scp']","rawkeys":["method model","deep learning","natural language processing","feature vectors","feature engineerings","vector representations","weather events","mining algorithms","word representations","learning approach","sentiment analysis"],"tags":["method model","machine learning","natural language processing","feature vectors","feature engineerings","vector representations","weather events","mining algorithms","word representations","learning approach","sentiment analysis"]},{"p_id":33872,"title":"Text readability for Arabic as a foreign language","abstract":"\u00a9 2015 IEEE. In this study, we evaluate the informativeness of lexical, morphological and semantic features in determining the readability of texts geared towards learners of Arabic as a foreign language. We have gathered low-complexity features with the purpose of establishing a baseline for future research in readability assessment, using freely available natural language processing (NLP) and machine learning (ML) tools on a publicly accessible corpus. We tested common classification algorithms, as well as random forests-an ensemble learning method-and report on their results using several evaluation measures for comparability with similar work. Our results suggest that a small set of easily computed features can be indicative of the reading level of a text. Moreover, our findings will serve as a common ground, for ourselves and others, to evaluate and compare the performance of more elaborate techniques and feature sets.","keywords_author":["Arabic","foreign language learning","machine learning","natural language processing","text readability"],"keywords_other":["Arabic","Publicly accessible","Evaluation measures","Foreign language learning","Classification algorithm","Semantic features","text readability","NAtural language processing"],"max_cite":2.0,"pub_year":2016.0,"sources":"['scp']","rawkeys":["publicly accessible","machine learning","foreign language learning","evaluation measures","natural language processing","semantic features","arabic","text readability","classification algorithm"],"tags":["publicly accessible","machine learning","foreign language learning","evaluation measures","natural language processing","semantic features","arabic","text readability","classification algorithm"]},{"p_id":33873,"title":"Deep learning for information retrieval","abstract":"\u00a9 2016 ACM. Recent years have observed a significant progress in information retrieval and natural language processing with deep learning technologies being successfully applied into almost all of their major tasks. The key to the success of deep learning is its capability of accurately learning distributed representations (vector representations or structured arrangement of them) of natural language expressions such as sentences, and effectively utilizing the representations in the tasks. This tutorial aims at summarizing and introducing the results of recent research on deep learning for information retrieval, in order to stimulate and foster more significant research and development work on the topic in the future. The tutorial mainly consists of three parts. In the first part, we introduce the fundamental techniques of deep learning for natural language processing and information retrieval, such as word embedding, recurrent neural networks, and convolutional neural networks. In the second part, we explain how deep learning, particularly representation learning techniques, can be utilized in fundamental NLP and IR problems, including matching, translation, classification, and structured prediction. In the third part, we describe how deep learning can be used in specific application tasks in details. The tasks are search, question answering (from either documents, database, or knowledge base), and image retrieval.","keywords_author":["Deep learning","Image retrieval","Information retrieval","Question answering","Search"],"keywords_other":["Deep learning","Question Answering","Research and development","Natural language expressions","Search","Convolutional neural network","Distributed representation","NAtural language processing"],"max_cite":2.0,"pub_year":2016.0,"sources":"['scp']","rawkeys":["question answering","research and development","search","deep learning","image retrieval","natural language processing","information retrieval","distributed representation","convolutional neural network","natural language expressions"],"tags":["research and development","search","image retrieval","machine learning","natural language processing","information retrieval","distributed representation","convolutional neural network","natural language expressions"]},{"p_id":17490,"title":"Automated systems for the de-identification of longitudinal clinical narratives: Overview of 2014 i2b2\/UTHealth shared task Track 1","abstract":"\u00a9 2015 Elsevier Inc.The 2014 i2b2\/UTHealth Natural Language Processing (NLP) shared task featured four tracks. The first of these was the de-identification track focused on identifying protected health information (PHI) in longitudinal clinical narratives. The longitudinal nature of clinical narratives calls particular attention to details of information that, while benign on their own in separate records, can lead to identification of patients in combination in longitudinal records. Accordingly, the 2014 de-identification track addressed a broader set of entities and PHI than covered by the Health Insurance Portability and Accountability Act - the focus of the de-identification shared task that was organized in 2006. Ten teams tackled the 2014 de-identification task and submitted 22 system outputs for evaluation. Each team was evaluated on their best performing system output. Three of the 10 systems achieved F1 scores over .90, and seven of the top 10 scored over .75. The most successful systems combined conditional random fields and hand-written rules. Our findings indicate that automated systems can be very effective for this task, but that de-identification is not yet a solved problem.","keywords_author":["Machine learning","Medical records","Natural language processing","Shared task"],"keywords_other":["Cohort Studies","Shared task","Attention to details","Electronic Health Records","Computer Security","Pattern Recognition, Automated","Natural Language Processing","Longitudinal records","Vocabulary, Controlled","Narration","Medical record","Protected health informations","Health insurance portability and accountability acts","Data Mining","Longitudinal Studies","Conditional random field","NAtural language processing","Confidentiality"],"max_cite":29.0,"pub_year":2015.0,"sources":"['scp']","rawkeys":["vocabulary","automated","computer security","shared task","attention to details","conditional random field","machine learning","electronic health records","medical record","medical records","narration","cohort studies","data mining","health insurance portability and accountability acts","longitudinal records","confidentiality","controlled","protected health informations","longitudinal studies","natural language processing","pattern recognition"],"tags":["vocabulary","automated","computer security","shared task","attention to details","conditional random field","control","electronic health records","machine learning","medical record","narration","cohort studies","data mining","health insurance portability and accountability acts","longitudinal records","confidentiality","protected health informations","longitudinal studies","natural language processing","pattern recognition"]},{"p_id":31826,"title":"Discovering novel protein-protein interactions by measuring the protein semantic similarity from the biomedical literature","abstract":"\u00a9 2014 Imperial College Press.Protein-protein interactions (PPIs) are involved in the majority of biological processes. Identification of PPIs is therefore one of the key aims of biological research. Although there are many databases of PPIs, many other unidentified PPIs could be buried in the biomedical literature. Therefore, automated identification of PPIs from biomedical literature repositories could be used to discover otherwise hidden interactions. Search engines, such as Google, have been successfully applied to measure the relatedness among words. Inspired by such approaches, we propose a novel method to identify PPIs through semantic similarity measures among protein mentions. We define six semantic similarity measures as features based on the page counts retrieved from the MEDLINE database. A machine learning classifier, Random Forest, is trained using the above features. The proposed approach achieve an averaged micro-F of 71.28% and an averaged macro-F of 64.03% over five PPI corpora, an improvement over the results of using only the conventional co-occurrence feature (averaged micro-F of 68.79% and an averaged macro-F of 60.49%). A relation-word reinforcement further improves the averaged micro-F to 71.3% and averaged macro-F to 65.12%. Comparing the results of the current work with other studies on the AIMed corpus (ranging from 77.58% to 85.1% in micro-F, 62.18% to 76.27% in macro-F), we show that the proposed approach achieves micro-F of 81.88% and macro-F of 64.01% without the use of sophisticated feature extraction. Finally, we manually examine the newly discovered PPI pairs based on a literature review, and the results suggest that our approach could extract novel protein-protein interactions.","keywords_author":["Classification","machine learning","protein-protein interaction","text mining"],"keywords_other":["Protein Binding","Terminology as Topic","Pattern Recognition, Automated","Semantics","Natural Language Processing","Periodicals as Topic","Protein Interaction Mapping","Vocabulary, Controlled","Artificial Intelligence","Data Mining","Binding Sites","Proteins"],"max_cite":3.0,"pub_year":2014.0,"sources":"['scp']","rawkeys":["terminology as topic","artificial intelligence","vocabulary","data mining","periodicals as topic","automated","protein-protein interaction","proteins","semantics","text mining","machine learning","natural language processing","protein interaction mapping","classification","controlled","binding sites","pattern recognition","protein binding"],"tags":["terminology as topic","vocabulary","data mining","periodicals as topic","automated","proteins","text mining","semantics","control","machine learning","natural language processing","protein interaction mapping","classification","binding sites","pattern recognition","protein-protein interactions","protein binding"]},{"p_id":25686,"title":"Interleaved text\/image deep mining on a large-scale radiology database for automated image interpretation","abstract":"\u00a9 2016, Microtome Publishing. All rights reserved. Despite tremendous progress in computer vision, there has not been an attempt to apply machine learning on very large-scale medical image databases. We present an interleaved text\/image deep learning system to extract and mine the semantic interactions of radiology images and reports from a national research hospital's Picture Archiving and Communication System. With natural language processing, we mine a collection of \u223c216K representative two-dimensional images selected by clinicians for diagnostic reference and match the images with their descriptions in an automated manner. We then employ a weakly supervised approach using all of our available data to build models for generating approximate interpretations of patient images. Finally, we demonstrate a more strictly supervised approach to detect the presence and absence of a number of frequent disease types, providing more specific interpretations of patient scans. A relatively small amount of data is used for this part, due to the challenge in gathering quality labels from large raw text data. Our work shows the feasibility of large-scale learning and prediction in electronic patient records available in most modern clinical institutions. It also demonstrates the trade-offs to consider in designing machine learning systems for analyzing large medical data.","keywords_author":["Convolutional neural networks","Deep learning","Medical imaging","Natural language processing","Topic models","Deep learning","Convolutional Neural Networks","Topic Models","Natural Language Processing","Medical Imaging"],"keywords_other":["Deep learning","Medical image database","MEDICAL LANGUAGE SYSTEM","Automated image interpretations","Two dimensional images","Electronic patient record","Convolutional neural network","Topic model","NAtural language processing"],"max_cite":7.0,"pub_year":2016.0,"sources":"['scp', 'wos']","rawkeys":["automated image interpretations","convolutional neural networks","medical language system","deep learning","medical imaging","natural language processing","topic model","medical image database","convolutional neural network","electronic patient record","topic models","two dimensional images"],"tags":["automated image interpretations","medical language system","medical imaging","topic modeling","machine learning","natural language processing","medical image database","convolutional neural network","electronic patient record","two dimensional images"]},{"p_id":50265,"title":"Are deep learning approaches suitable for natural language processing?","abstract":"\u00a9 Springer International Publishing Switzerland 2016.In recent years, Deep Learning (DL) techniques have gained much attention from Artificial Intelligence (AI) and Natural Language Processing (NLP) research communities because these approaches can often learn features from data without the need for human design or engineering interventions. In addition, DL approaches have achieved some remarkable results. In this paper, we have surveyed major recent contributions that use DL techniques for NLP tasks. All these reviewed topics have been limited to show contributions to text understanding, such as sentence modelling, sentiment classification, semantic role labelling, question answering, etc. We provide an overview of deep learning architectures based on Artificial Neural Networks (ANNs), Convolutional Neural Networks (CNNs), Long Short-Term Memory (LSTM), and Recursive Neural Networks (RNNs).","keywords_author":["Artificial neural networks","Convolutional neural networks","Deep learning","Long short-term memory","Natural language processing","Recursive neural networks"],"keywords_other":["Deep learning","Recursive neural networks","Convolutional neural network","Long short term memory","NAtural language processing"],"max_cite":0.0,"pub_year":2016.0,"sources":"['scp']","rawkeys":["recursive neural networks","convolutional neural networks","deep learning","long short-term memory","natural language processing","artificial neural networks","convolutional neural network","long short term memory"],"tags":["neural networks","long short-term memory","machine learning","natural language processing","convolutional neural network","recursive neural networks"]},{"p_id":33882,"title":"Domain-specific hierarchical text classification for supporting automated environmental compliance checking","abstract":"\u00a9 2015 American Society of Civil Engineers.Automated environmental compliance checking requires automated extraction of rules from environmental regulatory textual documents such as energy conservation codes and EPA regulations. Automated rule extraction requires complex text processing and analysis for information extraction and subsequent formalization of the extracted information into computer-processable rules. In the proposed automated compliance checking (ACC) approach, the text is first classified into predefined categories before information extraction (IE). The advantages are that irrelevant text will be filtered out during text classification (TC) and text with similar semantic meaning will be grouped, thereby improving the efficiency and accuracy of further IE and compliance reasoning (CR). The categories used for TC are predefined in a semantic TC topic hierarchy, and the classified text is subsequently used in semantic IE and semantic CR. This paper presents the proposed machine learning (ML)-based TC algorithm for classifying clauses in environmental regulatory documents based on the TC topic hierarchy. In developing the algorithm, different text preprocessing techniques, ML algorithms, and performance improvement strategies were tested and used. The final TC algorithm was tested on 10 environmental regulatory documents and evaluated in terms of precision and recall. The algorithm achieved approximately 97 and 84% average recall and precision, respectively, on the testing data.","keywords_author":["Automated compliance checking","Automated construction management systems","Machine learning","Natural language processing","Semantic systems","Text classification"],"keywords_other":["Text classification","Semantic systems","Automated compliance checking","Automated construction","NAtural language processing"],"max_cite":2.0,"pub_year":2016.0,"sources":"['scp']","rawkeys":["natural language processing","machine learning","automated construction","semantic systems","automated compliance checking","automated construction management systems","text classification"],"tags":["natural language processing","machine learning","automated construction","semantic systems","automated compliance checking","automated construction management systems","text classification"]},{"p_id":27739,"title":"Automatic detection of satire in Twitter: A psycholinguistic-based approach","abstract":"\u00a9 2017 Elsevier B.V.In recent years, a substantial effort has been made to develop sophisticated methods that can be used to detect figurative language, and more specifically, irony and sarcasm. There is, however, an absence of new approaches and research works that analyze satirical texts. The recognition of satire by sentiment analysis and Natural Language Processing (NLP) applications is extremely important because it can influence and change the meaning of a statement in varied and complex ways. We used this understanding as a basis to propose a method that employs a wide variety of psycholinguistic features and which detects satirical and non-satirical text. We then went on to train a set of machine learning algorithms that would enable us to classify unknown data. Finally, we conducted several experiments in order to detect the most relevant features that generate a better pattern as regards detecting satirical texts. We evaluated the effectiveness of our method by obtaining a corpus of satirical and non-satirical news from Mexican and Spanish Twitter accounts. Our proposal obtained encouraging results, with an F-measure of 85.5% for Mexico and one of 84.0% for Spain. Moreover, the results of the experiment showed that there is no significant difference between Mexican and Spanish satire.","keywords_author":["Computational psycholinguistics","LIWC","Machine learning","Satire","Twitter"],"keywords_other":["Twitter","Satire","Sentiment analysis","Relevant features","LIWC","Computational psycholinguistics","Automatic Detection","NAtural language processing"],"max_cite":5.0,"pub_year":2017.0,"sources":"['scp', 'wos']","rawkeys":["relevant features","satire","liwc","machine learning","computational psycholinguistics","natural language processing","automatic detection","sentiment analysis","twitter"],"tags":["relevant features","satire","liwc","machine learning","computational psycholinguistics","natural language processing","automatic detection","sentiment analysis","twitter"]},{"p_id":31835,"title":"Automated Classification of Radiology Reports to Facilitate Retrospective Study in Radiology","abstract":"\u00a9 2014, Society for Imaging Informatics in Medicine. Retrospective research is an import tool in radiology. Identifying imaging examinations appropriate for a given research question from the unstructured radiology reports is extremely useful, but labor-intensive. Using the machine learning text-mining methods implemented in LingPipe [1], we evaluated the performance of the dynamic language model (DLM) and the Na\u00efve Bayesian (NB) classifiers in classifying radiology reports to facilitate identification of radiological examinations for research projects. The training dataset consisted of 14,325 sentences from 11,432 radiology reports randomly selected from a database of 5,104,594 reports in all disciplines of radiology. The training sentences were categorized manually into six categories (Positive, Differential, Post Treatment, Negative, Normal, and History). A 10-fold cross-validation [2] was used to evaluate the performance of the models, which were tested in classification of radiology reports for cases of sellar or suprasellar masses and colloid cysts. The average accuracies for the DLM and NB classifiers were 88.5 % with 95 % confidence interval (CI) of 1.9 % and 85.9 % with 95 % CI of 2.0 %, respectively. The DLM performed slightly better and was used to classify 1,397 radiology reports containing the keywords \u201csellar or suprasellar mass\u201d, or \u201ccolloid cyst\u201d. The DLM model produced an accuracy of 88.2 % with 95 % CI of 2.1 % for 959 reports that contain \u201csellar or suprasellar mass\u201d and an accuracy of 86.3 % with 95 % CI of 2.5 % for 437 reports of \u201ccolloid cyst\u201d. We conclude that automated classification of radiology reports using machine learning techniques can effectively facilitate the identification of cases suitable for retrospective research.","keywords_author":["Computer analysis","Machine learning","Natural language processing","Radiology Information Systems (RIS)","Radiology report classification","Radiology reporting","Retrospective studies"],"keywords_other":["Radiology reporting","Retrospective studies","Radiology information system","Sensitivity and Specificity","Datasets as Topic","Computer analysis","Humans","Reproducibility of Results","Natural Language Processing","Radiology","Databases, Factual","Retrospective Studies","Radiology Information Systems","Radiology reports","Research Report","NAtural language processing"],"max_cite":3.0,"pub_year":2014.0,"sources":"['scp']","rawkeys":["radiology information systems (ris)","databases","research report","machine learning","natural language processing","humans","reproducibility of results","radiology reports","radiology report classification","retrospective studies","radiology","radiology information system","radiology information systems","factual","radiology reporting","sensitivity and specificity","datasets as topic","computer analysis"],"tags":["computational analysis","databases","research report","machine learning","natural language processing","humans","reproducibility of results","radiology reports","radiology report classification","retrospective studies","radiology","radiology information systems","factual","sensitivity and specificity","datasets as topic"]},{"p_id":17502,"title":"Using NLP techniques for file fragment classification","abstract":"The classification of file fragments is an important problem in digital forensics. The literature does not include comprehensive work on applying machine learning techniques to this problem. In this work, we explore the use of techniques from natural language processing to classify file fragments. We take a supervised learning approach, based on the use of support vector machines combined with the bag-of-words model, where text documents are represented as unordered bags of words. This technique has been repeatedly shown to be effective and robust in classifying text documents (e.g., in distinguishing positive movie reviews from negative ones). In our approach, we represent file fragments as \"bags of bytes\" with feature vectors consisting of unigram and bigram counts, as well as other statistical measurements (including entropy and others). We made use of the publicly available Garfinkel data corpus to generate file fragments for training and testing. We ran a series of experiments, and found that this approach is effective in this domain as well. \u00a9 2012 Dykstra & Sherman. Published by Elsevier Ltd. All rights reserved.","keywords_author":["Bigrams","Digital forensics","File carving","File fragment classification","Machine learning","Natural language processing","Support vector machine"],"keywords_other":["File fragments","Supervised learning approaches","File carving","Bigrams","Bag-of-words models","Training and testing","NAtural language processing","Machine learning techniques"],"max_cite":29.0,"pub_year":2012.0,"sources":"['scp']","rawkeys":["bigrams","machine learning techniques","machine learning","file fragment classification","natural language processing","supervised learning approaches","training and testing","digital forensics","support vector machine","file carving","file fragments","bag-of-words models"],"tags":["bigrams","machine learning techniques","machine learning","file fragment classification","natural language processing","supervised learning approaches","training and testing","digital forensics","file carving","file fragments","bag-of-words models"]},{"p_id":23647,"title":"Phishing website detection using latent dirichlet allocation and AdaBoost","abstract":"One of the ways criminals steal identity in the cyberspace is using phishing. Attackers host phishing websites that resemble a legitimate website and entice users to click on hyperlinks which directs them to these fake websites. Attackers use these fake sites to capture personal information such as login, passwords and social security numbers from innocent victims, which they later use to commit crimes. We propose here a robust methodology to detect phishing websites that employs for semantic analysis a topic modeling technique, Latent Dirichlet Allocation, and for classification, AdaBoost. The methodology developed is a content driven approach that is device independent and language neutral. The website content of mobile and desktop clients are collected by employing an intelligent web crawler. The website contents that are not in English are translated to English using Google's language translator. Topic model is built using the translated contents of desktop and mobile clients. The phishing website classifier is built using (i) distribution probabilities for the topics found as features using Latent Dirichlet Allocation and (ii) AdaBoost voting technique. Experiments were conducted using one of the large public corpus of website data containing 47500 phishing websites and 52500 good websites. Results show that our method achieves a F-measure of 99%. \u00a9 2012 IEEE.","keywords_author":["boosting","detection","identity theft","machine learning","natural language processing","phishing website","semantic analysis"],"keywords_other":["Phishing","Identity theft","Semantic analysis","NAtural language processing","boosting"],"max_cite":10.0,"pub_year":2012.0,"sources":"['scp']","rawkeys":["machine learning","natural language processing","identity theft","semantic analysis","detection","phishing website","boosting","phishing"],"tags":["machine learning","natural language processing","identity theft","semantic analysis","detection","phishing websites","boosting","phishing"]},{"p_id":48223,"title":"Structured learning for spoken language understanding in human-robot interaction","abstract":"\u00a9 The Author(s) 2017. Robots are slowly becoming apart of everyday life, being marketed for commercial applications such as telepresence, cleaning or entertainment. Thus, the ability to interact via natural language with non-expert users is becoming a key requirement. Even if user utterances can be efficiently recognized and transcribed by automatic speech recognition systems, several issues arise in translating them into suitable robotic actions and most of the existing solutions are strictly related to a specific scenario. In this paper, we present an approach to the design of natural language interfaces for human robot interaction, to translate spoken commands into computational structures that enable the robot to execute the intended request. The proposed solution is achieved by combining a general theory of language semantics, i.e. frame semantics, with state-of-the-art methods for robust spoken language understanding, based on structured learning algorithms. The adopted data driven paradigm allows the development of a fully functional natural language processing chain, that can be initialized by re-using available linguistic tools and resources. In addition, it can be also specialized by providing small sets of examples representative of a target newer domain. A systematic benchmarking resource, in terms of a rich and multi-layered spoken corpus has also been created and it has been used to evaluate the natural language processing chain. Our results show that our processing chain, trained with generic resources, provides a solid baseline for command understanding in a service robot domain. Moreover, when domain-dependent resources are provided to the system, the accuracy of the achieved interpretation always improves.","keywords_author":["Human-robot interaction","Machine learning for natural language understanding","Natural language processing","Spoken language understanding"],"keywords_other":["Natural language understanding","State-of-the-art methods","Spoken language understanding","Commercial applications","Computational structure","Structured learning","Natural language interfaces","Automatic speech recognition system"],"max_cite":0.0,"pub_year":2017.0,"sources":"['scp']","rawkeys":["machine learning for natural language understanding","state-of-the-art methods","computational structure","natural language interfaces","structured learning","human-robot interaction","natural language processing","commercial applications","automatic speech recognition system","spoken language understanding","natural language understanding"],"tags":["machine learning for natural language understanding","state-of-the-art methods","computational structure","natural language interfaces","human-robot interaction","natural language processing","commercial applications","automatic speech recognition system","structure-learning","spoken language understanding","natural language understanding"]},{"p_id":37986,"title":"Text classification: The case of multiple labels","abstract":"\u00a9 2016 IEEE. Analysis of subjectivity is the actively developed direction of research in text mining. The paper presents machine learning experiments on classification of sentiments in forum texts. We explore the difficult task of classification when texts are labeled by several sentiment labels and in this condition we reach the average F-measure equal to 0.805.","keywords_author":["machine learning","multi-label classification","natural language processing","sentiment analysis","text classification"],"keywords_other":["Text mining","Text classification","F measure","Sentiment analysis","Multiple labels","Multi label classification","NAtural language processing"],"max_cite":1.0,"pub_year":2016.0,"sources":"['scp']","rawkeys":["multi-label classification","text classification","f measure","text mining","machine learning","natural language processing","multiple labels","multi label classification","sentiment analysis"],"tags":["text classification","text mining","natural language processing","machine learning","multiple labels","f-measure","multi label classification","sentiment analysis"]},{"p_id":25701,"title":"Classification of clinically useful sentences in clinical evidence resources","abstract":"\u00a9 2016 Elsevier Inc. Most patient care questions raised by clinicians can be answered by online clinical knowledge resources. However, important barriers still challenge the use of these resources at the point of care. Objective: To design and assess a method for extracting clinically useful sentences from synthesized online clinical resources that represent the most clinically useful information for directly answering clinicians' information needs. Materials and methods: We developed a Kernel-based Bayesian Network classification model based on different domain-specific feature types extracted from sentences in a gold standard composed of 18 UpToDate documents. These features included UMLS concepts and their semantic groups, semantic predications extracted by SemRep, patient population identified by a pattern-based natural language processing (NLP) algorithm, and cue words extracted by a feature selection technique. Algorithm performance was measured in terms of precision, recall, and F-measure. Results: The feature-rich approach yielded an F-measure of 74% versus 37% for a feature co-occurrence method (p < 0.001). Excluding predication, population, semantic concept or text-based features reduced the F-measure to 62%, 66%, 58% and 69% respectively (p < 0.01). The classifier applied to Medline sentences reached an F-measure of 73%, which is equivalent to the performance of the classifier on UpToDate sentences (p = 0.62). Conclusions: The feature-rich approach significantly outperformed general baseline methods. This approach significantly outperformed classifiers based on a single type of feature. Different types of semantic features provided a unique contribution to overall classification performance. The classifier's model and features used for UpToDate generalized well to Medline abstracts.","keywords_author":["Clinical decision support","Machine learning","Natural language processing","Text summarization"],"keywords_other":["Humans","Unified Medical Language System","Selection techniques","Language","MEDLINE","Bayesian network classification","Semantic predications","Algorithms","Bayes Theorem","Text summarization","NAtural language processing","Decision Support Systems, Clinical","Classification performance","Supervised Machine Learning","Terminology as Topic","Semantics","Natural Language Processing","Information Storage and Retrieval","Clinical decision support","Algorithm performance"],"max_cite":7.0,"pub_year":2016.0,"sources":"['scp']","rawkeys":["medline","unified medical language system","semantic predications","clinical decision support","language","decision support systems","terminology as topic","bayesian network classification","machine learning","clinical","algorithms","information storage and retrieval","classification performance","semantics","humans","bayes theorem","algorithm performance","selection techniques","text summarization","natural language processing","supervised machine learning"],"tags":["medline","semantic predications","clinical decision support","language","decision support systems","terminology as topic","bayesian network classification","machine learning","clinical","algorithms","information storage and retrieval","unified medical language systems","classification performance","semantics","humans","bayes theorem","algorithm performance","selection techniques","text summarization","natural language processing","supervised machine learning"]},{"p_id":31849,"title":"Disease and disorder template filling using rule-based and statistical approaches","abstract":"We present the participation of LIMSI in Task 2 of the 2014 ShARe\/CLEF eHealth Evaluation Lab.We used a hybrid approach based on a rule-based system and supervised classifiers depending on the properties of the attributes. The rule-based system identified course, severity and body location attributes based on the annotations of the training set and resources obtained from the UMLS. The Heideltime system was used to identify the dates. A MaxEnt model was trained to detect negation and uncertainty based on the disorder and surrounding words. A Decision Tree detected the relation to document time based on the position of the disorder in the document and on the words in the current sentence. Our system obtained a global 5th position out of ten ranked teams (accuracy of 0.804), and ranked 2nd for the detection of the relation to document time (accuracy of 0.322).","keywords_author":["Machine learning","Medical records","Natural language processing"],"keywords_other":["MaxEnt models","Statistical approach","Disease and disorders","Medical record","Hybrid approach","Training sets","Supervised classifiers","NAtural language processing"],"max_cite":3.0,"pub_year":2014.0,"sources":"['scp']","rawkeys":["training sets","machine learning","medical record","natural language processing","hybrid approach","disease and disorders","statistical approach","supervised classifiers","maxent models","medical records"],"tags":["training sets","machine learning","medical record","natural language processing","hybrid approach","disease and disorders","statistical approach","supervised classifiers","maxent models"]},{"p_id":31852,"title":"Unsupervised learning for syntactic disambiguation","abstract":"We present a methodology framework for syntactic disambiguation in natural language texts. The method takes advantage of an existing manually compiled non-probabilistic and non-lexicalized grammar, and turns it into a probabilistic lexicalized grammar by automatically learning a kind of subcategorization frames or selectional preferences for all words observed in the training corpus. The dictionary of subcategorization frames or selectional preferences obtained in the training process can be subsequently used for syntactic disambiguation of new unseen texts. The learning process is unsupervised and requires no manual markup. The learning algorithm proposed in this paper can take advantage of any existing disambiguation method, including linguistically motivated methods of filtering or weighting competing alternative parse trees or syntactic relations, thus allowing for integration of linguistic knowledge and unsupervised machine learning.","keywords_author":["Natural language processing","Syntactic disambiguation","Syntactic parsing","Unsupervised machine learning"],"keywords_other":null,"max_cite":3.0,"pub_year":2014.0,"sources":"['scp']","rawkeys":["syntactic disambiguation","unsupervised machine learning","natural language processing","syntactic parsing"],"tags":["syntactic disambiguation","unsupervised machine learning","natural language processing","syntactic parsing"]},{"p_id":33900,"title":"Large scale biomedical texts classification: A kNN and an ESA-based approaches","abstract":"\u00a9 2016 Dram\u00e9 et al.Background: With the large and increasing volume of textual data, automated methods for identifying significant topics to classify textual documents have received a growing interest. While many efforts have been made in this direction, it still remains a real challenge. Moreover, the issue is even more complex as full texts are not always freely available. Then, using only partial information to annotate these documents is promising but remains a very ambitious issue. Methods: We propose two classification methods: a k-nearest neighbours (kNN)-based approach and an explicit semantic analysis (ESA)-based approach. Although the kNN-based approach is widely used in text classification, it needs to be improved to perform well in this specific classification problem which deals with partial information. Compared to existing kNN-based methods, our method uses classical Machine Learning (ML) algorithms for ranking the labels. Additional features are also investigated in order to improve the classifiers' performance. In addition, the combination of several learning algorithms with various techniques for fixing the number of relevant topics is performed. On the other hand, ESA seems promising for this classification task as it yielded interesting results in related issues, such as semantic relatedness computation between texts and text classification. Unlike existing works, which use ESA for enriching the bag-of-words approach with additional knowledge-based features, our ESA-based method builds a standalone classifier. Furthermore, we investigate if the results of this method could be useful as a complementary feature of our kNN-based approach. Results: Experimental evaluations performed on large standard annotated datasets, provided by the BioASQ organizers, show that the kNN-based method with the Random Forest learning algorithm achieves good performances compared with the current state-of-the-art methods, reaching a competitive f-measure of 0.55 % while the ESA-based approach surprisingly yielded unsatisfactory results. Conclusions: We have proposed simple classification methods suitable to annotate textual documents using only partial information. They are therefore adequate for large multi-label classification and particularly in the biomedical domain. Thus, our work contributes to the extraction of relevant information from unstructured documents in order to facilitate their automated processing. Consequently, it could be used for various purposes, including document indexing, information retrieval, etc.","keywords_author":["Biomedical text classification","Explicit semantic analysis","Information extraction","K-nearest neighbours","Machine learning","Multi-label classification","Semantic indexing"],"keywords_other":["Humans","Semantics","Natural Language Processing","Biomedical Research","Machine Learning","Biological Ontologies","Data Mining"],"max_cite":2.0,"pub_year":2016.0,"sources":"['scp']","rawkeys":["multi-label classification","biomedical research","data mining","information extraction","semantics","k-nearest neighbours","machine learning","natural language processing","humans","semantic indexing","biomedical text classification","biological ontologies","explicit semantic analysis"],"tags":["biomedical research","data mining","multi label classification","information extraction","semantics","machine learning","natural language processing","humans","semantic indexing","biomedical text classification","biological ontologies","explicit semantic analysis","k-nearest neighbors"]},{"p_id":52332,"title":"Segmentation of natural language documents using term distance as discourse coherency measure","abstract":"Machine learning (ML) methods are used to extract new knowledge from existing datasets. Ensemble methods (EM) were introduced to improve the ML performance. While EM's offer performance improvements, they limit the amount of control and general understanding of how the final result was achieved. We are researching the possibility to verify ML results with formalization of research results accessible in natural language. We propose to use the extracted information from scientific papers for the verification of ML results. In order to be able to extract the information, relevant to the ML results, an approach to extract specific information rich segments from scientific papers is needed. We present an approach to automated selection of relevant segments from large repositories.","keywords_author":["Content segmentation","Information retrieval","Machine learning","Natural language processing"],"keywords_other":["Content segmentation","Research results","Specific information","Automated selection","Natural languages","Scientific papers","NAtural language processing","Performance improvements"],"max_cite":0.0,"pub_year":2012.0,"sources":"['scp']","rawkeys":["content segmentation","specific information","automated selection","natural language processing","machine learning","information retrieval","natural languages","performance improvements","research results","scientific papers"],"tags":["content segmentation","specific information","automated selection","natural language processing","machine learning","information retrieval","natural languages","performance improvements","research results","scientific papers"]},{"p_id":35951,"title":"Can multilinguality improve Biomedical Word Sense Disambiguation?","abstract":"\u00ef\u00bf\u00bd 2016 Elsevier Inc. Ambiguity in the biomedical domain represents a major issue when performing Natural Language Processing tasks over the huge amount of available information in the field. For this reason, Word Sense Disambiguation is critical for achieving accurate systems able to tackle complex tasks such as information extraction, summarization or document classification. In this work we explore whether multilinguality can help to solve the problem of ambiguity, and the conditions required for a system to improve the results obtained by monolingual approaches. Also, we analyze the best ways to generate those useful multilingual resources, and study different languages and sources of knowledge. The proposed system, based on co-occurrence graphs containing biomedical concepts and textual information, is evaluated on a test dataset frequently used in biomedicine. We can conclude that multilingual resources are able to provide a clear improvement of more than 7% compared to monolingual approaches, for graphs built from a small number of documents. Also, empirical results show that automatically translated resources are a useful source of information for this particular task.","keywords_author":["Biomedical Word Sense Disambiguation","Graph-based systems","Multilinguality","Parallel and comparable corpora","Unified Medical Language System","Unsupervised systems"],"keywords_other":["Unified medical language systems","Algorithms","Graph-based","Humans","Unified Medical Language System","Natural Language Processing","Word Sense Disambiguation","Knowledge Bases","Data Mining","Multilinguality","Comparable corpora"],"max_cite":2.0,"pub_year":2016.0,"sources":"['scp']","rawkeys":["knowledge bases","unified medical language systems","data mining","graph-based systems","parallel and comparable corpora","unified medical language system","graph-based","comparable corpora","natural language processing","humans","word sense disambiguation","algorithms","biomedical word sense disambiguation","multilinguality","unsupervised systems"],"tags":["multilingual","unified medical language systems","data mining","graph-based systems","parallel and comparable corpora","graph-based","comparable corpora","natural language processing","knowledge base","humans","word sense disambiguation","algorithms","biomedical word sense disambiguation","unsupervised systems"]},{"p_id":31856,"title":"AutoODC: Automated generation of orthogonal defect classifications","abstract":"\u00a9 2014, Springer Science+Business Media New York.Orthogonal defect classification (ODC), the most influential framework for software defect classification and analysis, provides valuable in-process feedback to system development and maintenance. Conducting ODC classification on existing organizational defect reports is human-intensive and requires experts\u2019 knowledge of both ODC and system domains. This paper presents AutoODC, an approach for automating ODC classification by casting it as a supervised text classification problem. Rather than merely applying the standard machine learning framework to this task, we seek to acquire a better ODC classification system by integrating experts\u2019 ODC experience and domain knowledge into the learning process via proposing a novel relevance annotation framework. We have trained AutoODC using two state-of-the-art machine learning algorithms for text classification, Naive Bayes (NB) and support vector machine (SVM), and evaluated it on both an industrial defect report from the social network domain and a larger defect list extracted from a publicly accessible defect tracker of the open source system FileZilla. AutoODC is a promising approach: not only does it leverage minimal human effort beyond the human annotations typically required by standard machine learning approaches, but it achieves overall accuracies of 82.9 % (NB) and 80.7 % (SVM) on the industrial defect report, and accuracies of 77.5 % (NB) and 75.2 % (SVM) on the larger, more diversified open source defect list.","keywords_author":["Machine learning","Natural language processing","Orthogonal defect classification (ODC)","Text classification"],"keywords_other":["Orthogonal defect classification","System development","Classification system","Text classification","Publicly accessible","Open source system","Automated generation","NAtural language processing"],"max_cite":3.0,"pub_year":2014.0,"sources":"['scp']","rawkeys":["classification system","system development","natural language processing","machine learning","orthogonal defect classification","orthogonal defect classification (odc)","automated generation","publicly accessible","text classification","open source system"],"tags":["classification system","system development","natural language processing","machine learning","orthogonal defect classification (odc)","automated generation","publicly accessible","text classification","open source system"]},{"p_id":48241,"title":"Information retrieval, machine learning, and Natural Language Processing for intellectual property information","abstract":"\u00a9 2017 Readers of this journal are well aware that automation technology has played a significant role in searching for patent information and, as artificial intelligence is once again (after the first, 1960s, and second, 1980s, golden eras of AI) a trending topic at both academic and industry conferences, the editorial team of this journal would like to encourage contributions that cover any aspect of automation as applied to intellectual property information. As a new Associate Editor of World Patent Information, I take the opportunity to advertise the availability of the editorial team to submissions from computer science teams, in addition to the existing contributions from the IP community. By way of introduction, I have taken the liberty to provide an extremely brief overview of efforts and contributions that the computer science community has made to the field of intellectual property. This summary focuses on patents, but that is not to say that trademarks or other forms of intellectual property have not triggered the interest of computer scientists. In fact, to call this summary short is already giving it too much credit. It is but a seed, upon which I hope that a forest of contributions and publications from my fellow computer scientists will grow.","keywords_author":["Editorial","Information retrieval","Machine learning","Natural Language Processing","Patent information"],"keywords_other":null,"max_cite":0.0,"pub_year":2017.0,"sources":"['scp']","rawkeys":["patent information","natural language processing","machine learning","information retrieval","editorial"],"tags":["patent information","natural language processing","machine learning","information retrieval","editorial"]},{"p_id":44147,"title":"Intelligent travel chatbot for predictive recommendation in echo platform","abstract":"\u00a9 2018 IEEE. Chatbot is a computer application that interacts with users using natural language in a similar way to imitate a human travel agent. A successful implementation of a chatbot system can analyze user preferences and predict collective intelligence. In most cases, it can provide better user-centric recommendations. Hence, the chatbot is becoming an integral part of the future consumer services. This paper is an implementation of an intelligent chatbot system in travel domain on Echo platform which would gather user preferences and model collective user knowledge base and recommend using the Restricted Boltzmann Machine (RBM) with Collaborative Filtering. With this chatbot based on DNN, we can improve human to machine interaction in the travel domain.","keywords_author":["Amazon Alexa Echo platform","Chatbot","DNN (Deep Neural Network)","Machine Learning (ML)","Natural Language Processing (NLP)","Neural Net(NN)","Restricted Boltzmann Machine (RBM)"],"keywords_other":["Restricted boltzmann machine","Consumer services","Chatbot","User knowledge","Amazon Alexa Echo platform","DNN (Deep Neural Network)","Collective intelligences","Natural languages"],"max_cite":0.0,"pub_year":2018.0,"sources":"['scp']","rawkeys":["machine learning (ml)","chatbot","natural languages","natural language processing (nlp)","restricted boltzmann machine (rbm)","consumer services","neural net(nn)","user knowledge","amazon alexa echo platform","dnn (deep neural network)","restricted boltzmann machine","collective intelligences"],"tags":["neural networks","chatbot","machine learning","natural language processing","natural languages","consumer services","user knowledge","convolutional neural network","amazon alexa echo platform","restricted boltzmann machine","collective intelligences"]},{"p_id":50294,"title":"Tip ranker: A M.L. approach to ranking short reviews","abstract":"Copyright is held by the author\/owner(s). Foursquare is a local search and discovery app where as part of the experience users leave tips, short reviews and suggestions to help other users find great places. This poster summarizes the strategy we use to select the best tips for a given venue. Our new ranking model leverages text, contextual and social signals to selects the tips that provide our users with the most informative and high quality content. The new model has numerous applications within the Foursquare app ecosystem and its introduction yielded significant and positive results in our metrics as measured by various A\/B tests.","keywords_author":["A\/B test","Context-aware recommenders","Foursquare","Machine learning","Natural language processing","Ranking","svm","Text classification"],"keywords_other":["Ranking","Foursquare","Text classification","Context-Aware","NAtural language processing"],"max_cite":0.0,"pub_year":2016.0,"sources":"['scp']","rawkeys":["a\/b test","context-aware","machine learning","natural language processing","svm","foursquare","ranking","context-aware recommenders","text classification"],"tags":["context-aware","standards","machine learning","natural language processing","context-aware recommendations","foursquare","a\/b testing","text classification"]},{"p_id":50295,"title":"Semantic node embeddings of distributed graphs using apache spark","abstract":"\u00a9 2016 IEEE.Graphs are widely used to model relationships between entities. Mining for information on large scale social network graphs poses a major challenge in terms of computation and performance due to the sheer size, scale and growth of the graphs involved. Therefore it is important to be able to work with graphs in a distributed setting. Recently, Natural language processing (NLP)-based graph representation algorithms have been developed. These algorithms obtain vector representation for the nodes of the graph in Rn while encoding information about each node's context. Such representational algorithms can be used to preprocess graph input for a variety of graph-based machine learning algorithms such as classification, link prediction etc. In our work, we introduce a distributed version of one such algorithm which can learn node representations from large scale distributed graphs using Apache Spark. Our algorithm is fast, scalable and supports online training for learning representations. We have also modeled the protein-protein interaction problem using graphs and obtained its semantic representation in Rn. The results we have obtained are quite interesting owing to the fact that even when we use just 1% of the data for training, we get an accuracy of close to 95% on prediction.","keywords_author":["Apache Spark","Graphs","Language modelling","Machine Learning","Word2Vec"],"keywords_other":["Encoding informations","Semantic representation","Word2Vec","Vector representations","Protein-protein interactions","Language modelling","Graphs","NAtural language processing"],"max_cite":0.0,"pub_year":2016.0,"sources":"['scp', 'ieee']","rawkeys":["apache spark","graphs","word2vec","language modelling","machine learning","natural language processing","semantic representation","vector representations","encoding informations","protein-protein interactions"],"tags":["apache spark","word2vec","graph","machine learning","natural language processing","semantic representation","language model","encoding informations","vector representations","protein-protein interactions"]},{"p_id":44150,"title":"Analyzing spread of influence in social networks for transportation applications","abstract":"\u00a9 2018 IEEE. The spread of influence on social networks has been extensively studied but has been primarily demonstrated on large networks such as author collaboration networks. It is not clear how well these approaches translate to real-world social networks comprised of users discussing region-specific topics related to transportation. This work attempts to utilize the concept of influence in social media to identify the individuals who exert the most influence on transportation-related topics. The work describes a web-accessible system that enables a user to select transportation-related topics and then retrieve in real-time the most influential potential influential individuals and\/or organizations on Twitter with respect to that specific topic. Influence is quantified using two simple measures of influence, the number of Twitter mentions and the number of retweets of a message. The locations of these influential entities are then indicated on a web-accessible front-end using Google Maps. Such a tool can eventually be used for many purposes including limiting misinformation and encouraging acceptance of a new transportation products or service.","keywords_author":["classification","crowdsourcing","machine learning","natural language processing","public transportation","sentiment analysis"],"keywords_other":["Real-world","Large networks","Social media","Public transportation","Google maps","Collaboration network","Influential individuals","Real time"],"max_cite":0.0,"pub_year":2018.0,"sources":"['scp']","rawkeys":["real time","social media","machine learning","crowdsourcing","large networks","natural language processing","classification","real-world","sentiment analysis","collaboration network","public transportation","google maps","influential individuals"],"tags":["real time","social media","machine learning","crowdsourcing","large networks","natural language processing","classification","real-world","sentiment analysis","collaboration network","public transportation","google maps","influential individuals"]},{"p_id":42107,"title":"Combination POS taggers on Amazigh texts","abstract":"\u00a9 2017 IEEE. Part of Speech (PoS) tagging is the task to assign the appropriate morphosyntactic category to each word according to the context. Several probabilistic methods have been adapted for PoS tagging such as Conditional Random Fields, Support Vector Machines, and Decision Trees. Based on these methods, language independent PoS taggers have been developed such as CRF++, Yamcha and TreeTagger. These POS taggers implement the process of assigning the correct PoS (noun, verb, adjective, adverb...) to each word of the sentence. PoS taggers are developed by modeling the morphosyntactic structure of natural language text. In this paper, we tried to improve the accuracy of existing Amazigh POS taggers using a voting algorithm. The three used Amazigh POS taggers are: (1) Conditional Random Fields (CRF) tagger (2) Support Vector Machines (SVM) tagger (3) TreeTagger (TT). These taggers are developed with an accuracy of 86.79 %, 84.64 % and 86.57 % respectively. An annotated corpus of 60,000 words is used to form all these taggers. An error analysis is done to find out the mistakes made by these taggers. Then, a voting algorithm is proposed to construct an Amazigh PoS tagger to achieve better results and we can reach an accuracy of 89.06 %. This accurate POS tagger could be used for a variety of NLP applications to offer the students and the researchers an opportunity to work with language data with variety of tools and techniques in terms of computational procedures and programs.","keywords_author":["Amazigh","Combination","Corpus","CRF","Machine Learning","NLP","POS tagging","SVM","TreeTagger"],"keywords_other":["TreeTagger","Corpus","Combination","Amazigh","PoS tagging"],"max_cite":0.0,"pub_year":2018.0,"sources":"['scp']","rawkeys":["nlp","amazigh","combination","machine learning","svm","treetagger","corpus","crf","pos tagging"],"tags":["amazigh","conditional random field","combination","machine learning","natural language processing","treetagger","corpus","pos tagging"]},{"p_id":42109,"title":"Survey on trends and methods of an intelligent answering system","abstract":"\u00a9 2017 IEEE. In Contemporary world, life styles and interactions have changed in all applications domain due to increasing advances of internet technology. Due to recent advances in information explosion, tries to build an intelligent question answering system where user may communicate with a machine in natural language to get response to user question using different strategies like Natural Language Processing (NLP), Artificial Intelligence, Information Retrieval and Human Computer Interaction. Natural Language Processing is a technique where computer behave like human, which helps people to talk to the computer in their own language rather than computer commands. The skills needed to build intelligent answering system includes tokenization, parsing, parts of speech tagging, question classification, query construction, sentence understanding, document retrieval, keyword ranking, classifier, answer extraction and validation. The current study intends to develop an intelligent system for user queries in natural language for precise answer.","keywords_author":["Artificial Intelligence (AI)","Intelligent System","Machine Learning","Natural Language Processing (NLP)","Question Answering (QA)"],"keywords_other":["Applications domains","Parts-of-speech tagging","Question classification","Question Answering","Intelligent question answering systems","Query construction","Information explosion","Internet technology"],"max_cite":0.0,"pub_year":2018.0,"sources":"['scp']","rawkeys":["question answering","intelligent question answering systems","applications domains","machine learning","natural language processing (nlp)","question classification","artificial intelligence (ai)","question answering (qa)","information explosion","parts-of-speech tagging","internet technology","query construction","intelligent system"],"tags":["intelligent question answering systems","machine learning","natural language processing","intelligent systems","part of speech tagging","information retrieval","question classification","information explosion","internet technology","query construction","application domains"]},{"p_id":42110,"title":"Authorship attribution for textual data on online social networks","abstract":"\u00a9 2017 IEEE. Authorship Attribution, (AA) is a process of determining a particular document is written by which author among a list of suspected authors. Authorship attribution has been the problem from last six decades; when there were handwritten documents needed to be identified for the genuine author. Due to the technology advancement and increase in cybercrime and unlawful activities, this problem of AA becomes forth most important to trace out the author behind online messages. Over the past, many years research has been conducted to attribute the authorship of an author on the basis of their writing style as all authors possess different distinctiveness while writing a piece of document. This paper presents a comparative study of various machine learning approaches on different feature sets for authorship attribution on short text. The Twitter dataset has been used for comparison with varying sample size of a dataset of 10 prolific authors with various combinations of feature sets. The significance and impact of combinations of features while inferring different stylometric features has been reflected. The results of different approaches are compared based on their accuracy and precision values.","keywords_author":["Authorship Attribution","Features","Machine Learning","NLP","Stylometry"],"keywords_other":["Authorship attribution","Machine learning approaches","On-line social networks","Features","Accuracy and precision","Handwritten document","Stylometry","Technology advancement"],"max_cite":0.0,"pub_year":2018.0,"sources":"['scp']","rawkeys":["on-line social networks","technology advancement","nlp","features","machine learning","handwritten document","accuracy and precision","authorship attribution","stylometry","machine learning approaches"],"tags":["on-line social networks","technological advances","features","machine learning","handwritten document","natural language processing","accuracy and precision","authorship attribution","stylometry","machine learning approaches"]},{"p_id":38023,"title":"Neu-IR: The SIGIR 2016 workshop on neural information retrieval","abstract":"\u00a9 2016 ACM.In recent years, deep neural networks have yielded significant performance improvements on speech recognition and computer vision tasks, as well as led to exciting breakthroughs in novel application areas such as automatic voice translation, image captioning, and conversational agents. Despite demonstrating good performance on natural language processing (NLP) tasks (e.g., language modelling and machine translation, the performance of deep neural networks on information retrieval (IR) tasks has had relatively less scrutiny. Recent work in this area has mainly focused on word embeddings and neural models for short text similarity. The lack of many positive results in this area of information retrieval is partially due to the fact that IR tasks such as ranking are fundamentally different from NLP tasks, but also because the IR and neural network communities are only beginning to focus on the application of these techniques to core information retrieval problems. Given that deep learning has made such a big impact, first on speech processing and computer vision and now, increasingly, also on computational linguistics, it seems clear that deep learning will have a major impact on information retrieval and that this is an ideal time for a workshop in this area. Neu-IR (pronounced \"new IR\") will be a forum for new research relating to deep learning and other neural network based approaches to IR. The purpose is to provide an opportunity for people to present new work and early results, compare notes on neural network toolkits, share best practices, and discuss the main challenges facing this line of research.","keywords_author":["Deep learning","Information retrieval","Neural networks"],"keywords_other":["Deep learning","Information retrieval problems","Network-based approach","Network communities","Machine translations","Deep neural networks","NAtural language processing","Conversational agents"],"max_cite":1.0,"pub_year":2016.0,"sources":"['scp']","rawkeys":["deep learning","deep neural networks","information retrieval problems","neural networks","natural language processing","network-based approach","information retrieval","conversational agents","network communities","machine translations"],"tags":["neural networks","information retrieval problems","machine learning","natural language processing","information retrieval","network-based approach","conversational agents","convolutional neural network","machine translations","network communities"]},{"p_id":27786,"title":"Deep learning and ensemble methods for domain adaptation","abstract":"\u00a9 2016 IEEE. Real world applications of machine learning in natural language processing can span many different domains and usually require a huge effort for the annotation of domain specific training data. For this reason, domain adaptation techniques have gained a lot of attention in the last years. In order to derive an effective domain adaptation, a good feature representation across domains is crucial as well as the generalisation ability of the predictive model. In this paper we address the problem of domain adaptation for sentiment classification by combining deep learning, for acquiring a cross-domain high-level feature representation, and ensemble methods, for reducing the cross-domain generalization error. The proposed adaptation framework has been evaluated on a benchmark dataset composed of reviews of four different Amazon category of products, significantly outperforming the state of the art methods.","keywords_author":["Deep learning","Domain adaptation","Ensemble","Sentiment analysis","Transfer learning"],"keywords_other":["Ensemble","Transfer learning","State-of-the-art methods","Sentiment analysis","Feature representation","Domain adaptation","Sentiment classification","NAtural language processing"],"max_cite":5.0,"pub_year":2017.0,"sources":"['scp']","rawkeys":["state-of-the-art methods","sentiment classification","deep learning","transfer learning","natural language processing","ensemble","domain adaptation","feature representation","sentiment analysis"],"tags":["state-of-the-art methods","sentiment classification","denoising autoencoder","transfer learning","natural language processing","machine learning","ensemble","feature representation","sentiment analysis"]},{"p_id":29834,"title":"Semantically-based priors and nuanced knowledge core for Big Data, Social AI, and language understanding","abstract":"Noise-resistant and nuanced, COGBASE makes 10 million pieces of commonsense data and a host of novel reasoning algorithms available via a family of semantically-driven prior probability distributions.Machine learning, Big Data, natural language understanding\/processing, and social AI can draw on COGBASE to determine lexical semantics, infer goals and interests, simulate emotion and affect, calculate document gists and topic models, and link commonsense knowledge to domain models and social, spatial, cultural, and psychological data.COGBASE is especially ideal for social Big Data, which tends to involve highly implicit contexts, cognitive artifacts, difficult-to-parse texts, and deep domain knowledge dependencies. \u00a9 2014 Elsevier Ltd.","keywords_author":["Big Data","Data mining","Knowledge representation","Machine learning","Natural language understanding","Nuanced commonsense reasoning","Social data"],"keywords_other":["Statistics as Topic","Cognitive artifacts","Natural language understanding","Algorithms","Humans","Language understanding","Reasoning algorithms","Social datum","Knowledge","Natural Language Processing","Semantics","Artificial Intelligence","Language","Social Support","Prior probability","Commonsense knowledge","Commonsense reasoning"],"max_cite":4.0,"pub_year":2014.0,"sources":"['scp']","rawkeys":["knowledge","big data","reasoning algorithms","cognitive artifacts","commonsense reasoning","social data","language","machine learning","nuanced commonsense reasoning","social datum","knowledge representation","algorithms","data mining","semantics","social support","humans","prior probability","natural language understanding","commonsense knowledge","artificial intelligence","natural language processing","statistics as topic","language understanding"],"tags":["knowledge","big data","reasoning algorithms","cognitive artifacts","commonsense reasoning","social data","language","machine learning","nuanced commonsense reasoning","social datum","knowledge representation","algorithms","data mining","semantics","social support","humans","prior probability","natural language understanding","commonsense knowledge","natural language processing","statistics as topic","language understanding"]},{"p_id":25742,"title":"Multimedia data mining using deep learning","abstract":"\u00a9 2015 IEEE. Due to the large amounts of Multimedia data on the Internet, Multimedia mining has become a very active area of research. Multimedia mining is a form of data mining. Data mining uses algorithms to segment data to identify useful patterns and to make predictions. Despite the successes in many areas, data mining remains a challenging task. In the past, multimedia mining was one of the fields where the results were often not satisfactory. Multimedia Data Mining extracts relevant data from multimedia files such as audio, video and still images to perform similarity searches, identify associations, entity resolution and for classification. As the mining techniques have matured, new techniques were developed. A lot of progress has been made in areas such as visual data mining and natural language processing using deep learning techniques. Deep learning is a branch of machine learning and has been used among other on Smartphones for face recognition and voice commands. Deep learners are a type of artificial neural networks with multiple data processing layers that learn representations by increasing the level of abstraction from one layer to the next. These methods have improved the state-of-the-art in multimedia mining, in speech recognition, visual object recognition, natural language processing and other areas such as genome mining and predicting the efficacy of drug molecules. This paper describes some of the deep learning techniques that have been used in recent research for multimedia data mining.","keywords_author":["artificial neural networks","data mining","deep learning","multimedia data mining","natural language processing","visual data mining"],"keywords_other":["Deep learning","Multimedia data mining","Recent researches","Visual data mining","Visual object recognition","Level of abstraction","Entity resolutions","NAtural language processing"],"max_cite":7.0,"pub_year":2015.0,"sources":"['scp', 'ieee']","rawkeys":["entity resolutions","data mining","deep learning","level of abstraction","natural language processing","multimedia data mining","visual data mining","artificial neural networks","visual object recognition","recent researches"],"tags":["levels of abstraction","data mining","neural networks","machine learning","natural language processing","multimedia data mining","visual data mining","visual object recognition","recent researches","entity resolution"]},{"p_id":29838,"title":"Aligned-parallel-corpora based semi-supervised learning for arabic mention detection","abstract":"In the last two decades, significant effort has been put into annotating linguistic resources in several languages. Despite this valiant effort, there are still many languages left that have only small amounts of such resources. The goal of this article is to present and investigate a method of propagating information (specifically mentions) from a resource-rich language such as English into a relatively less-resource language such as Arabic. We compare also this approach to its equivalent counterpart using monolingual resources. Part of the investigation is to quantify the contribution of propagating information in different conditions - based on the availability of resources in the target language. Experiments on the language pair Arabic-English show that one can achieve relatively decent performance by propagating information from a language with richer resources such as English into Arabic alone (no resources or models in the source language Arabic). Furthermore, results show that propagated features from English do help improve the Arabic system performance even when used in conjunction with all feature types built from the source language. Experiments also show that using propagated features in conjunction with lexically-derived features only (as can be obtained directly from a mention annotated corpus) brings the system performance at the one obtained in the target language by using feature derived from many linguistic resources, therefore improving the system when such resources are not available. \u00a9 2013 IEEE.","keywords_author":["Crosslingual NLP","Index terms","Information extraction","Machine learning","Mention detection","Natural language processing"],"keywords_other":["Index terms","Semi- supervised learning","Target language","Source language","Linguistic resources","Cross-lingual","Parallel corpora","Semi-supervised learning","Language pairs","Derived features","NAtural language processing"],"max_cite":4.0,"pub_year":2014.0,"sources":"['scp']","rawkeys":["cross-lingual","parallel corpora","semi- supervised learning","information extraction","target language","machine learning","natural language processing","index terms","semi-supervised learning","crosslingual nlp","linguistic resources","derived features","language pairs","mention detection","source language"],"tags":["cross-lingual","parallel corpora","information extraction","target language","machine learning","natural language processing","index terms","semi-supervised learning","crosslingual nlp","linguistic resources","derived features","language pairs","mention detection","source language"]},{"p_id":40079,"title":"Key concepts identification and weighting in search engine queries","abstract":"It has been widely observed that queries of search engine are becoming longer and closer to natural language. Actually, current search engines do not perform well with natural language queries. Accurately discovering the key concepts of these queries can dramatically improve the effectiveness of search engines. It has been shown that queries seem to be composed in a way that how users summarize documents, which is so much similar to anchor texts. In this paper, we present a technique for automatic extraction of key concepts from queries with anchor texts analysis. Compared with using web counts of documents, we proposed a supervised machine learning model to classify the concepts of queries into 3 sets according to their importance and types. In the end of this paper, we also demonstrate that our method has remarkable improvement over the baseline. \u00a9 2011 Springer-Verlag Berlin Heidelberg.","keywords_author":["anchor analysis","key concepts","Machine learning","NLP"],"keywords_other":["NLP","Machine learning","Automatic extraction","key concepts","Natural languages","Natural language queries","Supervised machine learning"],"max_cite":1.0,"pub_year":2011.0,"sources":"['scp']","rawkeys":["anchor analysis","nlp","natural languages","machine learning","automatic extraction","natural language queries","key concepts","supervised machine learning"],"tags":["anchor analysis","natural languages","machine learning","natural language processing","automatic extraction","natural language queries","key concepts","supervised machine learning"]},{"p_id":50319,"title":"Sentiment analysis in Arabic social media using association rule mining","abstract":"\u00a9 Medwell Journals, 2016. The fast-paced growth in worldwide webs has resulted in the development of sentiment analysis it involves the analysis of comments or web reviews. The sentiment classification of the Arabic social meda is an exciting and fascinating area of study. Hence this study brings fob a new method engaging association rules with three Feature Selection (FS) methods in the Sentiment Analysis (SA) of web reviews in the Arabic language. The feature selection methods used are (\u03c72), Gini Index (GI) and Information Gain (GI). This study reveals that the use of feature selection methods has enhanced the classifier results. This means that the proposed model shows a better result than the baseline result. Finally, the experimental results show that the Chi-square Feature Selection (FS) produces the best classification techmque with a high accuracy of f-measure (86.81 1).","keywords_author":["Arabic sentiment analysis","Association rule","Feature selection method","Machine learning","NLP"],"keywords_other":null,"max_cite":0.0,"pub_year":2016.0,"sources":"['scp']","rawkeys":["nlp","association rule","arabic sentiment analysis","feature selection method","machine learning"],"tags":["arabic sentiment analysis","feature selection methods","machine learning","natural language processing","association rules"]},{"p_id":44178,"title":"Literature survey of sarcasm detection","abstract":"\u00a9 2017 IEEE. Sarcasm is a form of language in which individual convey their message in an implicit way i.e. the opposite of what is implied. Sarcasm detection is the task of predicting sarcasm in text. This is the crucial step in sentiment analysis due to inherently ambiguous nature of sarcasm. With this ambiguity, sarcasm detection has always been a difficult task, even for humans. Therefore sarcasm detection has gained importance in many Natural Language Processing applications. In this paper, we describe approaches, issues, challenges and future scopes in sarcasm detection.","keywords_author":["Machine learning","Natural language processing","Sarcasm","Sentiment analysis"],"keywords_other":["Literature survey","Sarcasm"],"max_cite":0.0,"pub_year":2018.0,"sources":"['scp']","rawkeys":["literature survey","natural language processing","machine learning","sarcasm","sentiment analysis"],"tags":["literature survey","natural language processing","machine learning","sarcasm","sentiment analysis"]},{"p_id":27796,"title":"MultiWiBi: The multilingual Wikipedia bitaxonomy project","abstract":"\u00a9 2016 We present MultiWiBi, an approach to the automatic creation of two integrated taxonomies for Wikipedia pages and categories written in different languages. In order to create both taxonomies in an arbitrary language, we first build them in English and then project the two taxonomies to other languages automatically, without the help of language-specific resources or tools. The process crucially leverages a novel algorithm which exploits the information available in either one of the taxonomies to reinforce the creation of the other taxonomy. Our experiments show that the taxonomical information in MultiWiBi is characterized by a higher quality and coverage than state-of-the-art resources like DBpedia, YAGO, MENTA, WikiNet, LHD and WikiTaxonomy, also across languages. MultiWiBi is available online at http:\/\/wibitaxonomy.org\/multiwibi.","keywords_author":["Collaborative resources","Machine learning","Natural language processing","Taxonomy extraction","Taxonomy induction","Wikipedia"],"keywords_other":["Novel algorithm","Arbitrary languages","Wikipedia","State of the art","Automatic creations","Collaborative resources","Taxonomy extractions","NAtural language processing"],"max_cite":5.0,"pub_year":2016.0,"sources":"['scp']","rawkeys":["automatic creations","taxonomy extraction","novel algorithm","taxonomy extractions","state of the art","natural language processing","machine learning","arbitrary languages","wikipedia","collaborative resources","taxonomy induction"],"tags":["automatic creations","novel algorithm","taxonomy extractions","state of the art","natural language processing","machine learning","arbitrary languages","wikipedia","collaborative resources","taxonomy induction"]},{"p_id":31895,"title":"Mining until it hurts: Automatic extraction of usability issues from online reviews compared to traditional usability evaluation","abstract":"Large amounts of data available on the web, for example reviews, tweets, and forum postings, contain user narratives on interaction with products. Finding usability issues in such user narratives offers an interesting alternative to traditional usability testing. To leverage such data for identifying usability issues, we (I) devise a methodology for building automated extraction tools for usability issues; (II) perform empirical assessment of such tools by training a number of classifiers to extract sentences describing usability issues for two digital cameras and a children's tablet; (III) perform quantitative and qualitative comparisons between the usability issues identified by the classifiers and those identified and assessed by two traditional methods: heuristic evaluation and think aloud testing. Our results show that it is possible to build and train algorithms for extracting actionable usability issues, but raise serious concerns about the practical future prospects for supplementing traditional evaluation methods with automated extraction algorithms. Copyright 2014 ACM.","keywords_author":["Machine learning","Natural language processing","Usability","User experience"],"keywords_other":["Usability","Automated extraction","Empirical assessment","Heuristic evaluation","User experience","Large amounts of data","Usability evaluation","NAtural language processing"],"max_cite":3.0,"pub_year":2014.0,"sources":"['scp']","rawkeys":["empirical assessment","machine learning","natural language processing","large amounts of data","automated extraction","user experience","usability evaluation","heuristic evaluation","usability"],"tags":["empirical assessment","machine learning","natural language processing","large amounts of data","automated extraction","user experience","usability evaluation","heuristic evaluation","usability"]},{"p_id":19608,"title":"The CLSA model: A novel framework for concept-level sentiment analysis","abstract":"\u00a9 Springer International Publishing Switzerland 2015. Hitherto, sentiment analysis has been mainly based on algorithms relyingon the textual representation of online reviews and microblogging posts.Such algorithms are very good at retrieving texts, splitting them into parts, checkingthe spelling, and counting their words. But when it comes to interpretingsentences and extracting opinionated information, their capabilities are knownto be very limited. Current approaches to sentiment analysis are mainly basedon supervised techniques relying on manually labeled samples, such as movieor product reviews, where the overall positive or negative attitude was explicitlyindicated. However, opinions do not occur only at document-level, nor theyare limited to a single valence or target. Contrary or complementary attitudes towardthe same topic or multiple topics can be present across the span of a review.In order to overcome this and many other issues related to sentiment analysis,we propose a novel framework, termed concept-level sentiment analysis (CLSA)model, which takes into account all the natural-language-processing tasks necessaryfor extracting opinionated information from text, namely: microtext analysis,semantic parsing, subjectivity detection, anaphora resolution, sarcasm detection,topic spotting, aspect extraction, and polarity detection.","keywords_author":null,"keywords_other":["Topic spotting","NAtural language processing","Sentiment analysis","Textual representation","Retrieving texts","Anaphora resolution","Product reviews","Semantic parsing"],"max_cite":20.0,"pub_year":2015.0,"sources":"['scp', 'wos']","rawkeys":["semantic parsing","topic spotting","natural language processing","textual representation","retrieving texts","anaphora resolution","sentiment analysis","product reviews"],"tags":["semantic parsing","topic spotting","natural language processing","textual representation","retrieving texts","anaphora resolution","sentiment analysis","product reviews"]},{"p_id":19609,"title":"Multilingual Sentiment Analysis: State of the Art and Independent Comparison of Techniques","abstract":"\u00a9 2016, The Author(s).With the advent of Internet, people actively express their opinions about products, services, events, political parties, etc., in social media, blogs, and website comments. The amount of research work on sentiment analysis is growing explosively. However, the majority of research efforts are devoted to English-language data, while a great share of information is available in other languages. We present a state-of-the-art review on multilingual sentiment analysis. More importantly, we compare our own implementation of existing approaches on common data. Precision observed in our experiments is typically lower than the one reported by the original authors, which we attribute to the lack of detail in the original presentation of those approaches. Thus, we compare the existing works by what they really offer to the reader, including whether they allow for accurate implementation and for reliable reproduction of the reported results.","keywords_author":["Artificial intelligence","Natural language processing","Opinion mining","Sentic computing","Sentiment Analysis"],"keywords_other":["Political parties","Research efforts","English languages","Sentic Computing","Sentiment analysis","State-of-the art reviews","NAtural language processing","Opinion mining"],"max_cite":20.0,"pub_year":2016.0,"sources":"['scp']","rawkeys":["state-of-the art reviews","artificial intelligence","natural language processing","sentic computing","english languages","research efforts","political parties","opinion mining","sentiment analysis"],"tags":["state-of-the-art reviews","natural language processing","machine learning","sentic computing","english languages","research efforts","political parties","opinion mining","sentiment analysis"]},{"p_id":17564,"title":"Machine learning and word sense disambiguation in the biomedical domain: Design and evaluation issues","abstract":"Background: Word sense disambiguation (WSD) is critical in the biomedical domain for improving the precision of natural language processing (NLP), text mining, and information retrieval systems because ambiguous words negatively impact accurate access to literature containing biomolecular entities, such as genes, proteins, cells, diseases, and other important entities. Automated techniques have been developed that address the WSD problem for a number of text processing situations, but the problem is still a challenging one. Supervised WSD machine learning (ML) methods have been applied in the biomedical domain and have shown promising results, but the results typically incorporate a number of confounding factors, and it is problematic to truly understand the effectiveness and generalizability of the methods because these factors interact with each other and affect the final results. Thus, there is a need to explicitly address the factors and to systematically quantify their effects on performance. Results: Experiments were designed to measure the effect of \"sample size\" (i.e. size of the datasets), \"sense distribution\" (i.e. the distribution of the different meanings of the ambiguous word) and \"degree of difficulty\" (i.e. the measure of the distances between the meanings of the senses of an ambiguous word) on the performance of WSD classifiers. Support Vector Machine (SVM) classifiers were applied to an automatically generated data set containing four ambiguous biomedical abbreviations: BPD, BSA, PCA, and RSV, which were chosen because of varying degrees of differences in their respective senses. Results showed that: 1) increasing the sample size generally reduced the error rate, but this was limited mainly to well-separated senses (i.e. cases where the distances between the senses were large); in difficult cases an unusually large increase in sample size was needed to increase performance slightly, which was impractical, 2) the sense distribution did not have an effect on performance when the senses were separable, 3) when there was a majority sense of over 90%, the WSD classifier was not better than use of the simple majority sense, 4) error rates were proportional to the similarity of senses, and 5) there was no statistical difference between results when using a 5-fold or 10-fold cross-validation method. Other issues that impact performance are also enumerated. Conclusion: Several different independent aspects affect performance when using ML techniques for WSD. We found that combining them into one single result obscures understanding of the underlying methods. Although we studied only four abbreviations, we utilized a well-established statistical method that guarantees the results are likely to be generalizable for abbreviations with similar characteristics. The results of our experiments show that in order to understand the performance of these ML methods it is critical that papers report on the baseline performance, the distribution and sample size of the senses in the datasets, and the standard deviation or confidence intervals. In addition, papers should also characterize the difficulty of the WSD task, the WSD situations addressed and not addressed, as well as the ML methods and features used. This should lead to an improved understanding of the generalizablility and the limitations of the methodology. \u00a9 2006 Xu et al; licensee BioMed Central Ltd.","keywords_author":null,"keywords_other":["Numerical Analysis, Computer-Assisted","10-fold cross-validation","Animals","Humans","Unified Medical Language System","Vocabulary, Controlled","Automated techniques","Statistical differences","Pattern Recognition, Automated","Artificial Intelligence","Data Interpretation, Statistical","Algorithms","Base-line performance","Models, Statistical","NAtural language processing","Automatically generated","Terminology as Topic","Natural Language Processing","Word-sense disambiguation","Design and evaluations","Computational Biology"],"max_cite":29.0,"pub_year":2006.0,"sources":"['scp']","rawkeys":["statistical","vocabulary","base-line performance","unified medical language system","automated","10-fold cross-validation","terminology as topic","statistical differences","numerical analysis","automated techniques","models","algorithms","humans","controlled","animals","artificial intelligence","computer-assisted","computational biology","natural language processing","data interpretation","automatically generated","design and evaluations","pattern recognition","word-sense disambiguation"],"tags":["vocabulary","base-line performance","automated","10-fold cross-validation","terminology as topic","statistical differences","machine learning","control","numerical analysis","automated techniques","algorithms","unified medical language systems","humans","word sense disambiguation","animals","statistics","model","computer-assisted","computational biology","natural language processing","data interpretation","automatically generated","design and evaluations","pattern recognition"]},{"p_id":29852,"title":"Automatically assessing the expert degree of online health content using SVMS","abstract":"More and more people search for health information regarding diseases, diagnoses and treatments over the Web. However, lay people often have difficulties in assessing the understandability of related articles. Therefore, they could benefit from a system, which computes the medical expert degree of a corresponding piece of text in advance. In this paper we present an approach to automatically compute this expert degree using a machine learning approach. For evaluation purposes we constructed a large text corpus and tested our trained text classifier, which is based on Support Vector Machines. \u00a9 2014 The authors and IOS Press. All rights reserved.","keywords_author":["Classification system","Health information seeking","Internet","Machine learning","Medical expert degree","Support Vector Machine","SVM"],"keywords_other":["Consumer Health Information","Health Information Systems","Internet","Online Systems","Pattern Recognition, Automated","Natural Language Processing","Expert Testimony","Support Vector Machine"],"max_cite":4.0,"pub_year":2014.0,"sources":"['scp']","rawkeys":["online systems","medical expert degree","classification system","consumer health information","automated","health information systems","machine learning","natural language processing","svm","expert testimony","internet","pattern recognition","support vector machine","health information seeking"],"tags":["online systems","medical expert degree","classification system","consumer health information","automated","health information systems","machine learning","natural language processing","expert testimony","internet","pattern recognition","health information seeking"]},{"p_id":29856,"title":"Towards automatic detection of user influence in twitter by means of stylistic and behavioral features","abstract":"\u00a9 Springer International Publishing Switzerland 2014.Online communities are filled with comments of loyal readers or first-time viewers, that are constantly creating and sharing information at an unprecedented level, resulting in millions of messages containing opinions, ideas, needs and beliefs of Internet users. Therefore, businesses companies are very interested in finding influential users and encouraging them to create positive influence. Influential users represent users with the ability to influence individual\u2019s attitudes in a desired way with relative frequency. We present an empirical analysis on influential users identification problem in Twitter. Our proposed approach considers that the influential level of users can be detected by considering its communication patterns, by means of particular writing style features as well as behavioral features. Performed experiments on more that 7000 users profiles, indicate that it is possible to automatically identify influential users among the members of a social networking community, and also it obtains competitive results against several state-of-the-art methods.","keywords_author":["Author Profiling","Machine Learning","Natural Language Processing","Opinion Leaders","User Influence"],"keywords_other":["Author Profiling","Opinion leaders","State-of-the-art methods","Relative frequencies","Identification problem","User influences","Communication pattern","Author profiling","NAtural language processing"],"max_cite":4.0,"pub_year":2014.0,"sources":"['scp']","rawkeys":["state-of-the-art methods","natural language processing","machine learning","opinion leaders","user influences","communication pattern","author profiling","relative frequencies","user influence","identification problem"],"tags":["state-of-the-art methods","natural language processing","machine learning","opinion leaders","user influences","communication pattern","author profiling","relative frequencies","identification problem"]},{"p_id":50339,"title":"Named entity recognition model for Punjabi language: A survey","abstract":"\u00a9 2016 IEEE. Information extraction is the sub topic of Artificial Intelligence method. Recognition of named entity tags for computer using NLP (Natural language processing) is very important. It is very first step in recognition of unstructured content. For classification and identification of given number of tasks for any data, named entity recognition can be used as a subtask for extraction of information. There are numerous methods that help in applying NE process. In this paper various methods of NER have been presented and various issues related to Punjabi language has been discussed.","keywords_author":["machine learning & hybrid methods","Named Entity Recognition","Punjabi language Issues","rule based"],"keywords_other":["Rule based","Named entity recognition","Extraction of information","Classification and identifications","Artificial intelligence methods","Nlp (natural language processing)","Hybrid method","Punjabi language Issues"],"max_cite":0.0,"pub_year":2016.0,"sources":"['scp']","rawkeys":["extraction of information","nlp (natural language processing)","rule based","classification and identifications","named entity recognition","hybrid method","punjabi language issues","artificial intelligence methods","machine learning & hybrid methods"],"tags":["extraction of information","rule based","classification and identifications","named entity recognition","natural language processing","hybrid method","punjabi language issues","artificial intelligence methods","machine learning & hybrid methods"]},{"p_id":42155,"title":"Intelligence in the Cloud-We Need a Lot of it","abstract":"\u00a9 2014 IEEE. Artificial Intelligence (AI) has been progressing quite nicely in the last few years-thanks to much higher computation power, graphics processors and custom hardware such as neuromorphic chips, advances in data and computer sciences, but also the cloud. I have said in my many keynote speeches that cloud is becoming, if not already, the de-facto hosting platform for all innovations, whether social, industrial or for services of any kind. Cloud has helped extensively here by not only providing computational power, but also the flexibility to experiment as broadly as required without costing an arm or a leg.","keywords_author":["AIaaS","artificial intelligence","deep learning","machine learning","natural language processing"],"keywords_other":["Computational power","Neuromorphic chips","AIaaS","Computation power","Graphics processor","Custom hardwares"],"max_cite":0.0,"pub_year":2018.0,"sources":"['scp']","rawkeys":["artificial intelligence","deep learning","neuromorphic chips","machine learning","natural language processing","custom hardwares","computation power","aiaas","computational power","graphics processor"],"tags":["neuromorphic chips","machine learning","natural language processing","custom hardwares","aiaas","computational power","graphics processor"]},{"p_id":44607,"title":"New Word Pair Level Embeddings to Improve Word Pair Similarity","abstract":"\u00a9 2017 IEEE. We present a novel approach for computing similarity of English word pairs. While many previous approaches compute cosine similarity of individually computed word embeddings, we compute a single embedding for the word pair that is suited for similarity computation. Such embeddings are then used to train a machine learning model. Testing results on MEN and WordSim-353 datasets demonstrate that for the task of word pair similarity, computing word pair embeddings is better than computing word embeddings only.","keywords_author":["Machine learning","Natural language processing","Word pair embeddings","Word pair similarity"],"keywords_other":["Word-pairs","Machine learning models","English word","Cosine similarity","Embeddings","Similarity computation"],"max_cite":0.0,"pub_year":2018.0,"sources":"['scp']","rawkeys":["embeddings","similarity computation","word-pairs","cosine similarity","machine learning models","machine learning","natural language processing","word pair similarity","english word","word pair embeddings"],"tags":["embeddings","similarity computation","word-pairs","cosine similarity","machine learning models","machine learning","natural language processing","word pair similarity","english word","word pair embeddings"]},{"p_id":29870,"title":"Texterra: A framework for text analysis","abstract":"\u00a9 2014, Pleiades Publishing, Ltd.A framework for fast text analysis, which is developed as a part of the Texterra project, is described. Texterra provides a scalable solution for the fast text processing on the basis of novel methods that exploit knowledge extracted from the Web and text documents. For the developed tools, details of the project, use cases, and evaluation results are presented.","keywords_author":["computational linguistics","information search","knowledge bases","machine learning","natural language processing","semantic ontologies","terminology extraction","text analysis","Wikipedia"],"keywords_other":["Terminology extraction","Text analysis","Information search","Wikipedia","Semantic ontology","Knowledge basis","NAtural language processing"],"max_cite":4.0,"pub_year":2014.0,"sources":"['scp']","rawkeys":["knowledge bases","information search","machine learning","natural language processing","semantic ontologies","terminology extraction","text analysis","computational linguistics","semantic ontology","wikipedia","knowledge basis"],"tags":["information search","machine learning","knowledge base","natural language processing","terminology extraction","text analysis","computational linguistics","semantic ontology","wikipedia","knowledge basis"]},{"p_id":33968,"title":"Opinion mining using Na\u00efve Bayes","abstract":"\u00a9 2015 IEEE. The opinion of other person is an important information for decision making. People share useful information to others as the growth of social media through Internet. This information is used to organize, explore and analyze for better decision making. Opinion Mining is a Natural Language Processing task which aims to determine the attitude of a person by identifying and extracting information. The major task in opinion mining is to classify the polarity of a review at sentence level, whether the expressed opinion is positive or negative. In this paper, we design a classifier using Na\u00efve Bayes as a machine learning approach to determine the opinion expressed both in English and Bangla. We label the polarity of each opinion as weak, steady and strong. The performances are evaluated and the comparative results are analyzed.","keywords_author":["document probability","machine learning","Na\u00efve Bayes","opinion mining","sentiment detection","text classifier"],"keywords_other":["Machine learning approaches","Social media","Text classifiers","Sentence level","NAtural language processing","Extracting information","Opinion mining"],"max_cite":2.0,"pub_year":2016.0,"sources":"['scp']","rawkeys":["na\u00efve bayes","social media","natural language processing","document probability","machine learning","extracting information","text classifiers","text classifier","sentiment detection","opinion mining","machine learning approaches","sentence level"],"tags":["opinion mining","social media","natural language processing","document probability","machine learning","extracting information","text classifiers","sentiment detection","naive bayes","machine learning approaches","sentence level"]},{"p_id":50357,"title":"KEC@DPIL-FIRE2016: Detection of paraphrases on Indian Languages","abstract":"This paper presents a report on Detecting Paraphrases in Indian Languages (DPIL), in particular the Tamil language, by the team NLP@KEC of Kongu Engineering College. Automatic paraphrase detection is an intellectual task which has immense applications like plagiarism detection, new event detection, etc. Paraphrase is defined as the expression of a given fact in more than one way by means of different phrases. Paraphrase identification is a classic natural language processing task which is of classification type. Though there are several algorithms for paraphrase identification, reflecting the semantic relations between the constituent parts of a sentence plays a very important role. In this paper we utilize sixteen different features to best represent the similarity between sentences. The proposed approach utilizes machine learning algorithms like Support Vector Machine and Maximum Entropy for classification of given sentence pair. They have been classified into Paraphrase and Not-a-Paraphrase for task1 and Paraphrase, Not-a-Paraphrase and Semi-Paraphrase for task2. The accuracy and performance of these methods are measured on the basis of evaluation parameters like accuracy, precision, recall, f-measure and macro f-measure. Our methodology got 2nd place in DPIL evaluation track.","keywords_author":["Machine learning approach","Maximum entropy","Natural language processing","Paraphrase identification","Shallow parser","Support vector machine"],"keywords_other":["Paraphrase identifications","Machine learning approaches","Shallow parser","Evaluation parameters","Semantic relations","Plagiarism detection","NAtural language processing","Engineering colleges"],"max_cite":0.0,"pub_year":2016.0,"sources":"['scp']","rawkeys":["engineering colleges","paraphrase identification","machine learning approach","maximum entropy","semantic relations","natural language processing","paraphrase identifications","shallow parser","evaluation parameters","support vector machine","plagiarism detection","machine learning approaches"],"tags":["engineering colleges","semantic relations","maximum entropy","shallow parsers","natural language processing","machine learning","paraphrase identifications","evaluation parameters","plagiarism detection","machine learning approaches"]},{"p_id":38070,"title":"Machine learning classification of surgical pathology reports and chunk recognition for information extraction noise reduction","abstract":"\u00a9 2016 Elsevier B.V.. Background and aims: Machine learning techniques for the text mining of cancer-related clinical documents have not been sufficiently explored. Here some techniques are presented for the pre-processing of free-text breast cancer pathology reports, with the aim of facilitating the extraction of information relevant to cancer staging. Materials and methods: The first technique was implemented using the freely available software RapidMiner to classify the reports according to their general layout: 'semi-structured' and 'unstructured'. The second technique was developed using the open source language engineering framework GATE and aimed at the prediction of chunks of the report text containing information pertaining to the cancer morphology, the tumour size, its hormone receptor status and the number of positive nodes. The classifiers were trained and tested respectively on sets of 635 and 163 manually classified or annotated reports, from the Northern Ireland Cancer Registry. Results: The best result of 99.4% accuracy - which included only one semi-structured report predicted as unstructured - was produced by the layout classifier with the k nearest algorithm, using the binary term occurrence word vector type with stopword filter and pruning. For chunk recognition, the best results were found using the PAUM algorithm with the same parameters for all cases, except for the prediction of chunks containing cancer morphology. For semi-structured reports the performance ranged from 0.97 to 0.94 and from 0.92 to 0.83 in precision and recall, while for unstructured reports performance ranged from 0.91 to 0.64 and from 0.68 to 0.41 in precision and recall. Poor results were found when the classifier was trained on semi-structured reports but tested on unstructured. Conclusions: These results show that it is possible and beneficial to predict the layout of reports and that the accuracy of prediction of which segments of a report may contain certain information is sensitive to the report layout and the type of information sought.","keywords_author":["Cancer staging","Information extraction","Natural language processing","Supervised machine learning","Surgical pathology report"],"keywords_other":["Algorithms","Cancer staging","Noise","Breast Neoplasms","Humans","Extraction of information","Surgical pathology","Software","Machine Learning","Pathology, Surgical","Data Mining","Supervised machine learning","Freely available software","NAtural language processing","Machine learning techniques","Machine learning classification"],"max_cite":1.0,"pub_year":2016.0,"sources":"['scp']","rawkeys":["breast neoplasms","freely available software","surgical pathology","cancer staging","extraction of information","information extraction","surgical pathology report","machine learning","pathology","algorithms","machine learning classification","data mining","noise","machine learning techniques","humans","software","surgical","natural language processing","supervised machine learning"],"tags":["breast neoplasms","freely available software","surgical pathology","cancer staging","extraction of information","information extraction","surgical pathology report","machine learning","pathology","algorithms","machine learning classification","data mining","noise","machine learning techniques","humans","software","surgical","natural language processing","supervised machine learning"]},{"p_id":29879,"title":"Text analysis and information extraction from Spanish written documents","abstract":"Despite of the spread of Electronic Health Records (EHRs) in Spanish hospitals and Spanish occupying the second place in the ranking of number of speakers, to the best of our knowledge there are no natural language processing tools for medical texts written in Spanish. This paper presents an approach based on OpenNLP to process natural language texts written in Spanish for information extraction. The main goal is to integrate our development with cTAKES. As cTAKES has been specifically trained for the clinical domain, in this paper we will train the main modules from a general purpose annotated Spanish corpus and an in-house corpus developed with medical documents, testing both on a set of medical documents. Best performance of individual components when tested with medical documents: Sentence boundary detector accuracy = 0.872; Part-of-speech tagger accuracy = 0.946; chunker = 0.909. \u00a9 2014 Springer International Publishing.","keywords_author":["Electronic Health Record","Machine Learning","Natural Language Processing"],"keywords_other":["Natural Language Processing Tools","Electronic health record","Natural language text","Individual components","Sentence boundaries","Part-of-speech tagger","Electronic health record (EHRs)","NAtural language processing"],"max_cite":4.0,"pub_year":2014.0,"sources":"['scp']","rawkeys":["natural language processing tools","electronic health record","sentence boundaries","individual components","natural language processing","machine learning","part-of-speech tagger","electronic health record (ehrs)","natural language text"],"tags":["natural language processing tools","sentence boundaries","individual components","natural language processing","machine learning","electronic health records","part-of-speech tagger","natural language text"]},{"p_id":23737,"title":"A knowledge-rich approach to identifying semantic relations between nominals","abstract":"This paper describes a state-of-the-art supervised, knowledge-intensive approach to the automatic identification of semantic relations between nominals in English sentences. The system employs a combination of rich and varied sets of new and previously used lexical, syntactic, and semantic features extracted from various knowledge sources such as WordNet and additional annotated corpora. The system ranked first at the third most popular SemEval 2007 Task - Classification of Semantic Relations between Nominals and achieved an F-measure of 72.4% and an accuracy of 76.3%. We also show that some semantic relations are better suited for WordNet-based models than other relations. Additionally, we make a distinction between out-of-context (regular) examples and those that require sentence context for relation identification and show that contextual data are important for the performance of a noun-noun semantic parser. Finally, learning curves show that the task difficulty varies across relations and that our learned WordNet-based representation is highly accurate so the performance results suggest the upper bound on what this representation can do. \u00a9 2009 Elsevier Ltd. All rights reserved.","keywords_author":["Lexical semantics","Machine learning","Natural language processing","Semantic relations"],"keywords_other":["Automatic identification","Lexical semantics","Machine-learning","Knowledge sources","English sentences","Upper Bound","Task difficulty","F-measure","Semantic features","Semantic relations","Wordnet","NAtural language processing","Learning curves"],"max_cite":10.0,"pub_year":2010.0,"sources":"['scp']","rawkeys":["semantic relations","wordnet","machine learning","natural language processing","semantic features","task difficulty","f-measure","upper bound","lexical semantics","automatic identification","learning curves","machine-learning","english sentences","knowledge sources"],"tags":["semantic relations","wordnet","machine learning","natural language processing","semantic features","task difficulty","f-measure","learning-curve","lexical semantics","automatic identification","upper bound","english sentences","knowledge sources"]},{"p_id":27834,"title":"Detecting sarcasm in multimodal social platforms","abstract":"\u00a9 2016 Copyright held by the owner\/author(s). Publication rights licensed to ACM.Sarcasm is a peculiar form of sentiment expression, where the surface sentiment differs from the implied sentiment. The detection of sarcasm in social media platforms has been applied in the past mainly to textual utterances where lexical indicators (such as interjections and intensifiers), linguistic markers, and contextual information (such as user profiles, or past conversations) were used to detect the sarcastic tone. However, modern social media platforms allow to create multimodal messages where audiovisual content is integrated with the text, making the analysis of a mode in isolation partial. In our work, we first study the relationship between the textual and visual aspects in multimodal posts from three major social media platforms, i.e., Instagram, Tumblr and Twitter, and we run a crowdsourcing task to quantify the extent to which images are perceived as necessary by human annotators. Moreover, we propose two different computational frameworks to detect sarcasm that integrate the textual and visual modalities. The first approach exploits visual semantics trained on an external dataset, and concatenates the semantics features with stateof-the-art textual features. The second method adapts a visual neural network initialized with parameters trained on ImageNet to multimodal sarcastic posts. Results show the positive effect of combining modalities for the detection of sarcasm across platforms and methods.","keywords_author":["Deep learning","Multimodal","NLP","Sarcasm","Social media"],"keywords_other":["Deep learning","Audio-visual content","Sarcasm","Contextual information","Social media","Social media platforms","Multi-modal","Computational framework"],"max_cite":5.0,"pub_year":2016.0,"sources":"['scp']","rawkeys":["multimodal","nlp","contextual information","deep learning","social media","social media platforms","audio-visual content","computational framework","multi-modal","sarcasm"],"tags":["multimodal","contextual information","social media","social media platforms","machine learning","audio-visual content","natural language processing","computational framework","multi-modal","sarcasm"]},{"p_id":31929,"title":"A hybrid recommender combining user, item and interaction data","abstract":"While collaborative filtering often yields very good recommendation results, in many real-world recommendation scenarios cold-start and data sparseness remain important problems. This paper presents a hybrid recommender system that integrates user demographics and item characteristics, around a collaborative filtering core based on user-item interactions. The recommender system is evaluated on Movie lens data (including genre information and user data) as well as real-world data from a discount coupon provider. We show that the inclusion of additional item and user information can have great impact on recommendation quality, especially in settings where little interaction data is available. \u00a9 2014 IEEE.","keywords_author":["Information mining and applications","Machine learning applications","Natural language processing","Recommender systems"],"keywords_other":["Real-world","User information","Data sparseness","Cold-start","Information mining","Machine learning applications","NAtural language processing","Hybrid recommender systems"],"max_cite":3.0,"pub_year":2014.0,"sources":"['scp']","rawkeys":["data sparseness","hybrid recommender systems","machine learning applications","natural language processing","recommender systems","user information","cold-start","information mining","real-world","information mining and applications"],"tags":["data sparseness","hybrid recommender systems","cold start","machine learning applications","natural language processing","recommender systems","user information","information mining","real-world","information mining and applications"]},{"p_id":50363,"title":"Study of Sensitivity Knowledge for Quantitative Evaluations to the car Exterior Design","abstract":"\u00a9 2016 Elsevier B.V. All rights reserved.In recent years, the manufacturing industry has seen a shift of the domain of competition from \"performance\" which can easily be expressed numerically to \"design\" which is hard to be represented with numerical values. The rise of companies that focus on design, such as Apple, Samsung and IKEA, is remarkable. However, design presents two challenges for the manufacturing industry. Firstly, it is difficult to conduct a questionnaire survey on a design to external customers due to confidentiality. Secondary, subjectivity is involved as evaluators tend to use their experiences and feelings, which makes it quantitative evaluation almost impossible. Therefore, this study takes up automobile exterior design as a subject and aims to realize customer-oriented evaluation process by linking customers' sensibilities to feature values of automobile exterior designs to enable simulating new designs.","keywords_author":["Deep Learning","Design engineering","Natural language processing"],"keywords_other":["Deep learning","NAtural language processing","Manufacturing industries","Questionnaire surveys","Exterior designs","Quantitative evaluation","External customers","Design Engineering"],"max_cite":0.0,"pub_year":2016.0,"sources":"['scp']","rawkeys":["external customers","deep learning","natural language processing","questionnaire surveys","manufacturing industries","exterior designs","quantitative evaluation","design engineering"],"tags":["external customers","machine learning","natural language processing","design engineers","manufacturing industries","exterior designs","questionnaire surveys","quantitative evaluation"]},{"p_id":5312,"title":"Annotating expressions of opinions and emotions in language","abstract":"This paper describes a corpus annotation project to study issues in the manual annotation of opinions, emotions, sentiments, speculations, evaluations and other private states in language. The resulting corpus annotation scheme is described, as well as examples of its use. In addition, the manual annotation process and the results of an inter-annotator agreement study on a 10,000-sentence corpus of articles drawn from the world press are presented. \u00a9 Springer 2006.","keywords_author":["Affect","Attitudes","Corpus annotation","Emotion","Natural language processing","Opinions","Sentiment","Subjectivity"],"keywords_other":null,"max_cite":636.0,"pub_year":2005.0,"sources":"['ieee', 'scp', 'wos']","rawkeys":["corpus annotation","emotion","natural language processing","opinions","subjectivity","sentiment","affect","attitudes"],"tags":["corpus annotation","emotion","opinion","natural language processing","subjectivity","sentiment","affect","attitudes"]},{"p_id":31937,"title":"A synergistic framework for geographic question answering","abstract":"QA (question answering) systems designated for answering in-depth geographic questions are highly demanded but not quite available. Previous research has visited various individual aspects of a QA system but few synergistic frameworks have been proposed. This paper investigates the nature of geographic question formation and observes their unique linguistic structures that can be semantically translated into a spatial query. We create a new task of solving non-trivial questions using GIS (Geographic Information System) and test it with an associated corpus. A dynamic programming algorithm is developed for classification and voting algorithm for verification. Two types of ontologies are integrated for disambiguating and discriminating spatial terms. PostGIS serves as the GIS backend to provide domain expertise for spatial reasoning. Results show that exact answers can be returned quickly and correctly by our system. Contrast classification results in improved accuracy compared with the baseline which proves the effectiveness of proposed methods. \u00a9 2013 IEEE.","keywords_author":["Dynamic programming","Gis","Machine learning","Nlp","Ontology","Question answering","Spatial SQL","Voting algorithm"],"keywords_other":null,"max_cite":3.0,"pub_year":2013.0,"sources":"['scp']","rawkeys":["gis","question answering","spatial sql","nlp","ontology","machine learning","dynamic programming","voting algorithm"],"tags":["gis","spatial sql","machine learning","natural language processing","information retrieval","dynamic programming","voting algorithm"]},{"p_id":42179,"title":"Towards automatic filtering of fake reviews","abstract":"\u00a9 2018 Elsevier B.V. Online opinions significantly influence consumer purchase decisions. Unfortunately, this has led to a dramatic increase of fake (or spam) reviews that can damage the reputation of brands and artificially manipulate users\u2019 perceptions about products and companies. Despite the efforts of several studies on fake review detection, important questions still remain open. For instance, there is no consensus if the performance of the classification methods is affected when they are used in real-world scenarios that require online learning. Moreover, it is also not known if the performance of the methods decreases due to the time-ordered nature of the reviews. To answer these and other important open questions, this work presents a comprehensive analysis of content-based classification methods for fake review detection. The experiments were performed in multiple settings, employing different types of learning and datasets. A careful analysis of the results provided sufficient evidence to respond appropriately to the open questions, which can be used as a baseline for future studies.","keywords_author":["Fake review","Machine learning","Natural language processing","Spam review","Text categorization"],"keywords_other":["Classification methods","Consumer purchase","Text categorization","Real-world scenario","Content-based classification","Comprehensive analysis","Automatic filtering","Fake review detections"],"max_cite":0.0,"pub_year":2018.0,"sources":"['scp']","rawkeys":["fake review detections","classification methods","automatic filtering","machine learning","natural language processing","fake review","consumer purchase","comprehensive analysis","real-world scenario","spam review","text categorization","content-based classification"],"tags":["fake review detections","classification methods","automatic filtering","machine learning","natural language processing","fake reviews","consumer purchase","comprehensive analysis","real-world scenario","spam review","text categorization","content-based classification"]},{"p_id":23748,"title":"Using linguistic information and machine learning techniques to identify entities from juridical documents","abstract":"Information extraction from legal documents is an important and open problem. A mixed approach, using linguistic information and machine learning techniques, is described in this paper. In this approach, top-level legal concepts are identified and used for document classification using Support Vector Machines. Named entities, such as, locations, organizations, dates, and document references, are identified using semantic information from the output of a natural language parser. This information, legal concepts and named entities, may be used to populate a simple ontology, allowing the enrichment of documents and the creation of high-level legal information retrieval systems. The proposed methodology was applied to a corpus of legal documents - from the EUR-Lex site - and it was evaluated. The obtained results were quite good and indicate this may be a promising approach to the legal information extraction problem. \u00a9 2010 Springer-Verlag Berlin Heidelberg.","keywords_author":["Machine learning","Named entity recognition","Natural language processing"],"keywords_other":["Legal information","Linguistic information","Mixed approach","Named entities","Document Classification","Legal information retrieval","Machine-learning","Named entity recognition","Information Extraction","Legal documents","Natural languages","Open problems","Legal concepts","NAtural language processing","Machine learning techniques","Semantic information"],"max_cite":10.0,"pub_year":2010.0,"sources":"['scp']","rawkeys":["legal information retrieval","open problems","information extraction","machine learning techniques","machine learning","legal documents","named entity recognition","natural language processing","natural languages","document classification","legal concepts","mixed approach","machine-learning","semantic information","named entities","linguistic information","legal information"],"tags":["legal information retrieval","open problems","information extraction","machine learning techniques","machine learning","legal documents","named entity recognition","natural language processing","natural languages","document classification","legal concepts","mixed approach","semantic information","named entities","linguistic information","legal information"]},{"p_id":29892,"title":"Access control policy extraction from unconstrained natural language text","abstract":"While access control mechanisms have existed in computer systems since the 1960s, modern system developers often fail to ensure appropriate mechanisms are implemented within particular systems. Such failures allow for individuals, both benign and malicious, to view and manipulate information that they should not otherwise be able to access. The goal of our research is to help developers improve security by extracting the access control policies implicitly and explicitly defined in natural language project artifacts. Developers can then verify and implement the extracted access control policies within a system. We propose a machine-learning based process to parse existing, unaltered natural language documents, such as requirement or technical specifications to extract the relevant subjects, actions, and resources for an access control policy. To evaluate our approach, we analyzed a public requirements specification. We had a precision of 0.87 with a recall of 0.91 in classifying sentences as access control or not. Through a bootstrapping process utilizing dependency graphs, we correctly identified the subjects, actions, and objects elements of the access control policies with a precision of 0.46 and a recall of 0.54. \u00a9 2013 IEEE.","keywords_author":["Access control","Documentation","Machine learning","Natural language processing","Relation extraction","Security"],"keywords_other":["Relation extraction","Technical specifications","Natural language text","Security","Requirements specifications","Access control policies","Access control mechanism","NAtural language processing"],"max_cite":4.0,"pub_year":2013.0,"sources":"['scp']","rawkeys":["access control mechanism","documentation","machine learning","natural language processing","access control policies","security","technical specifications","requirements specifications","access control","relation extraction","natural language text"],"tags":["access control mechanism","documentation","machine learning","natural language processing","access control policies","security","technical specifications","biometrics","requirements specifications","relation extraction","natural language text"]},{"p_id":31942,"title":"Mining web technical discussions to identify malware capabilities","abstract":"The exponential growth of unique malware binary artifacts has led researchers to explore automated techniques for characterizing unknown malware binaries' capabilities. Thus far, automatic malware analysis systems have relied on labeled training data and analyst defined rules to identify malware samples' software features and functional categories. Such approaches require substantial expert analyst effort to maintain, as malware authors change programming languages, APIs, malicious tactics, and operating system targets. In this paper we present preliminary results demonstrating the viability of a new research direction for malware capability identification that addresses these issues, the concept of mining web technical documentation to automatically identify malware capabilities. This approach does not require expert generation of rules or training labels and automatically stays up to date with the latest software engineering trends. We make two contributions aimed at demonstrating the value of this research direction: first, with a corpus of 6 million web technical postings from the programming question and answer website StackOverflow.com, we show that symbols found in a corpus of malicious executable files, such as registry keys, file names, and API call names, also occur frequently in the StackOverflow data, suggesting that applying natural language processing to the StackOverflow posts (and other technical documents) may help us automatically generate characterizations of technical symbols, and, thereby, capabilities, found in malware. Our second contribution is to show that by analyzing function call symbol co-occurrence within StackOverflow posts, as well as the semantic tags associated with these posts, we can create function relationship graphs over the symbols which show promise in helping to identifying malware software capabilities. We argue that these early findings demonstrate the promise of a web technical document based approach to automating malware capability identification. \u00a9 2013 IEEE.","keywords_author":["computer security","data mining","machine learning","malware analysis","natural language processing","statistical modeling"],"keywords_other":["Statistical modeling","Malicious executable files","Labeled training data","Malware analysis","Function relationships","Technical documentations","NAtural language processing","Software engineering trends"],"max_cite":3.0,"pub_year":2013.0,"sources":"['scp']","rawkeys":["malicious executable files","data mining","labeled training data","software engineering trends","statistical modeling","computer security","machine learning","function relationships","natural language processing","malware analysis","technical documentations"],"tags":["malicious executable files","data mining","labeled training data","software engineering trends","computer security","statistical models","machine learning","natural language processing","malware analysis","functional relationship","technical documentations"]},{"p_id":52421,"title":"Concept chaining utilizing meronyms in text characterization","abstract":"For most, the web is the first source to answer a question formulated by curiosity, need, or research reasons. This phenomenon is due to the internet's ubiquitous access, ease of use, and the extensive and ever expanding content. The problem is no longer the need to acquire content to encourage use, but to provide organizational tools to support content categorization that will facilitate improved access methods. This paper presents the results of a new text characterization algorithm that combines semantic and linguistic techniques utilizing domain-based ontology background knowledge. It explores the combination of meronym, synonym, and hypernym linguistic relationships to create a set of concept chains used to represent concepts found in a document. The experiments show improved accuracy over bag-of-words based term weighting methods and reveal characteristics of the meronym contribution to document representation. \u00a9 2012 ACM.","keywords_author":["clustering","concept extraction","digital libraries","machine learning","natural language processing","ontology","text characterization"],"keywords_other":["Ubiquitous access","Document Representation","Text characterization","Term weighting","Organizational tools","Concept extraction","Background knowledge","clustering","Access methods","Ease-of-use","Bag of words","Linguistic techniques","NAtural language processing"],"max_cite":0.0,"pub_year":2012.0,"sources":"['scp']","rawkeys":["bag of words","digital libraries","organizational tools","ubiquitous access","ontology","ease-of-use","machine learning","natural language processing","clustering","linguistic techniques","text characterization","term weighting","background knowledge","concept extraction","document representation","access methods"],"tags":["bag of words","ease of use","organizational tools","ubiquitous access","machine learning","natural language processing","clustering","back-ground knowledge","linguistic techniques","text characterization","term weighting","digital libraries","concept extraction","document representation","access methods"]},{"p_id":33993,"title":"Identification of relations from IndoWordNet for Indian languages using Support Vector Machine","abstract":"\u00a9 2015 IEEE. Identification and classification of relations between synsets in a low resource language is a challenging and difficult task, which requires intensive Natural Language Processing (NLP). This paper presents Support Vector Machine (SVM) based approach for learning, classifying and automatically predicting relationships between Hindi Synsets. The average accuracy obtained using SVM is 71.87%, which can be further improved through introduction of language based knowledge. The system performance has been validated using the performance measures namely Precision, Recall and F-score.","keywords_author":["IndoWordNet","Machine learning","Ontology","Relations","Support Vector Machine"],"keywords_other":["Low resource languages","Synsets","F-score","Performance measure","Relations","IndoWordNet","Indian languages","NAtural language processing"],"max_cite":2.0,"pub_year":2016.0,"sources":"['scp']","rawkeys":["performance measure","indowordnet","relations","ontology","machine learning","natural language processing","indian languages","f-score","synsets","support vector machine","low resource languages"],"tags":["performance measure","indowordnet","relations","machine learning","natural language processing","indian languages","f-score","synsets","low resource languages"]},{"p_id":33994,"title":"Augmenting a classifier ensemble with automatically generated class level patterns for higher accuracy","abstract":"\u00a9 2015 IEEE. Different types of classifiers were investigated in the context of classification of problem tickets in the Enterprise domain. There were still challenges in building an accurate classifier post data cleaning and other accuracy improving pre-processing techniques. Creating an ensemble of classifiers gave better accuracy than individual classifiers. The maximum accuracy was got by enhancing the ensemble with an additional automatically generated domain specific class wise keyword list. Use of this system gave us greater than 4 percent improvement over the techniques of just using the ensemble classifier. A further improvement in accuracy was obtained when a semi-supervised approach was followed where the automatically generated class level keys are further reviewed by domain team before usage.","keywords_author":["Accuracy","Classification","Committee-based approach","ensemble","F-score","Feature extraction","Latent Semantic Analysis","Machine Learning","Natural Language Processing","Term Frequency Inverse Document Frequency"],"keywords_other":["Latent Semantic Analysis","Accuracy","ensemble","F-score","Committee-based approach","Term frequency-inverse document frequencies","NAtural language processing"],"max_cite":2.0,"pub_year":2016.0,"sources":"['scp']","rawkeys":["accuracy","machine learning","ensemble","natural language processing","f-score","committee-based approach","classification","feature extraction","term frequency inverse document frequency","term frequency-inverse document frequencies","latent semantic analysis"],"tags":["accuracy","machine learning","ensemble","natural language processing","f-score","committee-based approach","classification","feature extraction","term frequency-inverse document frequencies","latent semantic analysis"]},{"p_id":40144,"title":"Unmasking text plagiarism using syntactic-semantic based natural language processing techniques: Comparisons, analysis and challenges","abstract":"\u00a9 2018 Elsevier Ltd The proposed work aims to explore and compare the potency of syntactic-semantic based linguistic structures in plagiarism detection using natural language processing techniques. The current work explores linguistic features, viz., part of speech tags, chunks and semantic roles in detecting plagiarized fragments and utilizes a combined syntactic-semantic similarity metric, which extracts the semantic concepts from WordNet lexical database. The linguistic information is utilized for effective pre-processing and for availing semantically relevant comparisons. Another major contribution is the analysis of the proposed approach on plagiarism cases of various complexity levels. The impact of plagiarism types and complexity levels, upon the features extracted is analyzed and discussed. Further, unlike the existing systems, which were evaluated on some limited data sets, the proposed approach is evaluated on a larger scale using the plagiarism corpus provided by PAN1 competition from 2009 to 2014. The approach presented considerable improvement in comparison with the top-ranked systems of the respective years. The evaluation and analysis with various cases of plagiarism also reflected the supremacy of deeper linguistic features for identifying manually plagiarized data.","keywords_author":["Chunking","Natural language processing","Plagiarism detection","POS tagging","Semantic role labelling","Syntactic-semantic"],"keywords_other":["Linguistic information","Part-of-speech tags","Semantic roles","Linguistic structure","Evaluation and analysis","Plagiarism detection","PoS tagging","Chunking"],"max_cite":1.0,"pub_year":2018.0,"sources":"['scp']","rawkeys":["linguistic structure","syntactic-semantic","natural language processing","semantic roles","semantic role labelling","part-of-speech tags","pos tagging","chunking","plagiarism detection","evaluation and analysis","linguistic information"],"tags":["linguistic structure","syntactic-semantic","natural language processing","semantic roles","part of speech tagging","semantic role labeling","pos tagging","chunking","plagiarism detection","evaluation and analysis","linguistic information"]},{"p_id":3280,"title":"Comparison of Three Different CNN Architectures for Age Classification","abstract":"\u00a9 2017 IEEE. As one of the powerful tools of machine learning, Convolutional Neural Network (CNN) architectures are used tosolve complex problems like image recognition, video analysisand natural language processing. In this paper, three differentCNN architectures for age classification using face images arecompared. The Morph dataset containing over 55k images isused in experiments and success of a 6-layer CNN and 2 variantsof ResNet with different depths are compared. The images in thedataset are divided into 6 different age classes. While 80% of theimages are used in training of the networks, the rest of the 20% isused for testing. The performance of the networks are comparedaccording to two different criteria namely, the ability to makethe estimation pointing the exact age classes of test images andthe ability to make the estimation pointing the exact age classesor at most neighboring classes of the images. According to theperformance results obtained, with 6-layer network, it is possibleto estimate the exact or neighboring classes of the images withless than 5% error. It is shown that for a 6 class age classificationproblem 6-layer network is more successful than the deeperResNet counterparts since 6-layer network is less susceptible tooverfitting for this problem.","keywords_author":["Age classification","convolutional neural networks","deep learning"],"keywords_other":["Complex problems","Morph dataset","Convolutional neural network","Face images","Test images","NAtural language processing","Age classification"],"max_cite":1.0,"pub_year":2017.0,"sources":"['scp', 'ieee']","rawkeys":["morph dataset","age classification","convolutional neural networks","deep learning","natural language processing","face images","convolutional neural network","complex problems","test images"],"tags":["morph dataset","age classification","machine learning","natural language processing","face images","convolutional neural network","complex problems","test images"]},{"p_id":31954,"title":"A Named Entity Recognition approach for Albanian","abstract":"Named Entity Recognition (NER) deals with identifying personal, geographical, organizational or other entity types in a raw text. In this paper we propose the first NER model for the Albanian language. Our model is based on the maximum entropy approach. We manually annotate a corpus in the historical and political domains and train the models to generate classifiers that are able to recognize relevant entities in the text. We achieve good performance for precision and recall on the selected domains, despite the scarcity of Albanian corpora and the fact that this paper marks the first NER research for the Albanian language. Experiments demonstrate that the models can be further improved if richer training corpus is provided. \u00a9 2013 IEEE.","keywords_author":["Albanian","machine learning","named entity recognition","natural language processing"],"keywords_other":["Named entity recognition","Entity-types","Albanian languages","Training corpus","Precision and recall","Maximum-entropy approaches","NAtural language processing","Albanians"],"max_cite":3.0,"pub_year":2013.0,"sources":"['scp']","rawkeys":["albanians","albanian","machine learning","named entity recognition","natural language processing","training corpus","maximum-entropy approaches","entity-types","precision and recall","albanian languages"],"tags":["albanians","named entity recognition","machine learning","natural language processing","training corpus","maximum-entropy approaches","entity-types","precision and recall","albanian languages"]},{"p_id":19668,"title":"Using EHRs and Machine Learning for Heart Failure Survival Analysis","abstract":"\u00a9 2015 IMIA and IOS Press. 'Heart failure (HF) is a frequent health problem with high morbidity and mortality, increasing prevalence and escalating healthcare costs' [1]. By calculating a HF survival risk score based on patient-specific characteristics from Electronic Health Records (EHRs), we can identify high-risk patients and apply individualized treatment and healthy living choices to potentially reduce their mortality risk. The Seattle Heart Failure Model (SHFM) is one of the most popular models to calculate HF survival risk that uses multiple clinical variables to predict HF prognosis and also incorporates impact of HF therapy on patient outcomes. Although the SHFM has been validated across multiple cohorts [1-5], these studies were primarily done using clinical trials databases that do not reflect routine clinical care in the community. Further, the impact of contemporary therapeutic interventions, such as beta-blockers or defibrillators, was incorporated in SHFM by extrapolation from external trials. In this study, we assess the performance of SHFM using EHRs at Mayo Clinic, and sought to develop a risk prediction model using machine learning techiniques that applies routine clinical care data. Our results shows the models which were built using EHR data are more accurate (11% improvement in AUC) with the convenience of being more readily applicable in routine clinical care. Furthermore, we demonstrate that new predictive markers (such as co-morbidities) when incorporated into our models improve prognostic performance significantly (8% improvement in AUC).","keywords_author":["Electronic Health Records","Heart Failure","Machine Learning","Survival Score"],"keywords_other":["Population Surveillance","Male","Electronic Health Records","Humans","Aged","Pattern Recognition, Automated","Death, Sudden, Cardiac","Minnesota","Natural Language Processing","Machine Learning","Risk Factors","Survival Analysis","Prevalence","Data Mining","Heart Failure","Female"],"max_cite":19.0,"pub_year":2015.0,"sources":"['scp']","rawkeys":["cardiac","automated","survival analysis","aged","sudden","survival score","death","population surveillance","machine learning","electronic health records","minnesota","data mining","humans","prevalence","heart failure","risk factors","male","natural language processing","pattern recognition","female"],"tags":["cardiac","automated","survival analysis","aged","sudden","survival score","death","population surveillance","machine learning","electronic health records","minnesota","data mining","humans","prevalence","heart failure","risk factors","male","natural language processing","pattern recognition","female"]},{"p_id":46292,"title":"Hybrid ontology-learning materials engineering system for pharmaceutical products: Multi-label entity recognition and concept detection","abstract":"\u00a9 2017 Elsevier Ltd The dawn of a new era in knowledge management due to information explosion is making old habits of modeling knowledge and decision-making inadequate. In the search for new modeling paradigms, we expect ontologies to play a big role. One of the critical challenges we face is the scarcity of semantically rich, properly populated, ontologies in most application domains in chemical and materials engineering. Developing such ontologies is a very challenging task requiring considerable investment in time, effort, and expert knowledge. One needs automation tools that can assist an ontology engineer to quickly develop and curate domain-specific ontologies. We consider our conceptual framework in this paper, a general approach for populating scientific ontologies, and its implementation as the prototype HOLMES, as an early attempt towards such an automated knowledge management environment. Our approach integrates a variety of machine learning and natural language processing methods to extract information from journal articles and store them semantically in an ontology. In this work, identification of key terms (such as chemicals, drugs, processes, anatomical entities, etc.) from abstracts, and the classification of these terms into 25 classes are presented. Two methods, a multi-class classifier (SVM) and a multi-label classifier (HOMER), were tested on an annotated data set for the pharmaceutical industry. The test was done using two different versions of the same data set, one using the BIO notation and the other not. The F1 scores for HOMER, were better in the BIO notation (63.6% vs 48.5%) while SVM performed better in the non-BIO version (54.1% vs 53.2%). However, the standard metrics did not consider the effect of the multiple answers that the multi-label classifier is allowed to obtain. As the results of our computational experiments show, while the performance of multi-label classifier is encouraging, much more remains to be done in order to develop a practically viable automated ontology-based knowledge management system.","keywords_author":["Concept detection","Entity recognition","Machine learning","Natural language processing","Ontology"],"keywords_other":["Ontology-based knowledge managements","Computational experiment","Pharmaceutical products","Concept detection","Domain-specific ontologies","Pharmaceutical industry","NAtural language processing","Entity recognition"],"max_cite":0.0,"pub_year":2017.0,"sources":"['scp']","rawkeys":["entity recognition","domain-specific ontologies","ontology-based knowledge managements","concept detection","machine learning","natural language processing","ontology","pharmaceutical products","computational experiment","pharmaceutical industry"],"tags":["entity recognition","domain-specific ontologies","ontology-based knowledge managements","concept detection","machine learning","natural language processing","pharmaceutical products","computational experiment","pharmaceutical industry"]},{"p_id":27864,"title":"Negation scope detection in sentiment analysis: Decision support for news-driven trading","abstract":"\u00a9 2016 Elsevier B.V. Decision support for financial news using natural language processing requires robust methods that process all sentences correctly, including those that are negated. To predict the corresponding negation scope, related literature commonly utilizes rule-based algorithms and generative probabilistic models. In contrast, we propose the use of a tailored reinforcement learning method, since it can conquer learning task of arbitrary length. We then perform a thorough comparison with a two-pronged evaluation. First, we compare the predictive performance using a manually-labeled dataset. Here, reinforcement learning outperforms common approaches from the related literature, leading to a balanced classification accuracy of up to 70.17%. Second, we examine how detecting negation scopes can improve the accuracy of sentiment analysis for financial news, leading to an improvement of up to 10.63% in the correlation between news sentiment and stock market returns. This reveals negation scope detection as a crucial leverage in decision support from sentiment.","keywords_author":["Decision support","Financial news","Machine learning","Negation scope detection","Sentiment analysis"],"keywords_other":["Financial news","Predictive performance","Classification accuracy","Sentiment analysis","Rule based algorithms","Decision supports","Reinforcement learning method","NAtural language processing"],"max_cite":5.0,"pub_year":2016.0,"sources":"['scp', 'wos']","rawkeys":["predictive performance","financial news","classification accuracy","machine learning","natural language processing","reinforcement learning method","decision support","sentiment analysis","rule based algorithms","negation scope detection","decision supports"],"tags":["financial news","classification accuracy","natural language processing","machine learning","reinforcement learning method","sentiment analysis","rule based algorithms","prediction performance","negation scope detection","decision supports"]},{"p_id":29916,"title":"Research progress of probabilistic graphical models: A survey","abstract":"Probabilistic graphical models are powerful tools for compactly representing complex probability distributions, efficiently computing (approximate) marginal and conditional distributions, and conveniently learning parameters and hyperparameters in probabilistic models. As a result, they have been widely used in applications that require some sort of automated probabilistic reasoning, such as computer vision and natural language processing, as a formal approach to deal with uncertainty. This paper surveys the basic concepts and key results of representation, inference and learning in probabilistic graphical models, and demonstrates their uses in two important probabilistic models. It also reviews some recent advances in speeding up classic approximate inference algorithms, followed by a discussion of promising research directions. \u00a9Copyright 2013, Institute of Software, the Chinese Academy of Sciences.","keywords_author":["Machine learning","Probabilistic graphical model","Probabilistic reasoning"],"keywords_other":["Hyperparameters","Approximate inference","Conditional distribution","Probabilistic models","Probabilistic graphical models","Probabilistic reasoning","Learning parameters","NAtural language processing"],"max_cite":4.0,"pub_year":2013.0,"sources":"['scp']","rawkeys":["hyperparameters","probabilistic models","probabilistic reasoning","conditional distribution","approximate inference","machine learning","natural language processing","probabilistic graphical models","probabilistic graphical model","learning parameters"],"tags":["probabilistic models","probabilistic reasoning","conditional distribution","approximate inference","machine learning","natural language processing","probabilistic graphical models","hyper-parameter","learning parameters"]},{"p_id":34012,"title":"FPGA-Accelerated Hadoop Cluster for Deep Learning Computations","abstract":"\u00a9 2015 IEEE. Deep learning algorithms have received significant attention in the last few years. Their popularity is due to their ability to achieve higher accuracy than conventional machine learning in many research areas such as speech recognition, image processing and natural language processing. Deep learning algorithms rely on multiple cascaded layers of non-linear processing units, typically composed of hidden artificial neural networks for feature extraction and transformation. However, deep learning algorithms require a large amount of computational power and significant amount of time to train. Fortunately, the training and inference algorithms of deep learning architectures expose abundant data-parallelism. We aim in this work to develop technology that exploits deep learning data parallelism in 2 ways: 1) by distributing deep computation into a Hadoop cluster or cloud of computing nodes, and 2) by using field programmable gate arrays (FPGA) hardware acceleration to speed up computationally intensive deep learning kernels. In this paper, we describe a hardware prototype of our accelerated Hadoop deep learning system architecture and report initial performance and energy reduction results. By accelerating the convolutional layers of deep learning Convolutional Neural Network, we have observed a potential speed-up of 12.6 times and an energy reduction of 87.5% on a 6-node FPGA accelerated Hadoop cluster.","keywords_author":["convolutional neural network","deep learning","FPGA","Hadoop","map-reduce"],"keywords_other":["Deep learning","Map-reduce","Hardware acceleration","Nonlinear processing","Convolutional neural network","Conventional machines","Hadoop","NAtural language processing"],"max_cite":2.0,"pub_year":2016.0,"sources":"['scp']","rawkeys":["hadoop","deep learning","natural language processing","fpga","conventional machines","map-reduce","convolutional neural network","nonlinear processing","hardware acceleration"],"tags":["hardware accelerators","hadoop","machine learning","fpga","natural language processing","conventional machines","map-reduce","convolutional neural network","nonlinear processing"]},{"p_id":38109,"title":"Deep learning based primary user classification in Cognitive Radios","abstract":"\u00a9 2015 IEEE.Deep Belief Networks (DBN) is a very powerful algorithm in deep learning. The DBN has been effectively applied in many areas of machine learning, such as computer vision (CV) and natural language processing (NLP). With the help of deep architecture, their accuracy has been largely improved and their human annotation data which traditional machine learning algorithm extremely rely on could be reduced. In Cognitive Radios (CRs), learning is necessary for its cognition, while two of the key challenges are how to classify primary user agents with their performances and predict their behaviors. The CRs' performance has a positive correlation with the hit rate of learning algorithm's classification and prediction results. In this paper, we study the questions of classification and prediction of user agents. We apply the DBN model to improve accuracy rating of user agent's recognition in CRs with user-centered model, it's the first application of deep learning structure in CRs. The DBN model provides a primary user agent's classification, which is the foundation of the prediction to both idle frequency spectrums and time slots. Experimental results show that the cognitive engine finds a much better detection rate than the CRs engine with shallow learning and other traditional strategy. The simulation results are also tested on the WIFI channel with 5GHz and 2.4GHz.","keywords_author":["cognitive radio","deep belief networks","deep learning","spectrum sensing"],"keywords_other":["Deep learning","Positive correlations","User-centered modeling","Deep architectures","Spectrum sensing","Deep belief networks","Deep belief network (DBN)","NAtural language processing"],"max_cite":1.0,"pub_year":2016.0,"sources":"['scp']","rawkeys":["deep belief network (dbn)","deep learning","deep architectures","natural language processing","cognitive radio","user-centered modeling","positive correlations","spectrum sensing","deep belief networks"],"tags":["machine learning","natural language processing","deep architectures","cognitive radio","user-centered modeling","positive correlations","spectrum sensing","deep belief networks"]},{"p_id":46300,"title":"Semi-supervised approach for Persian word sense disambiguation","abstract":"\u00a9 2017 IEEE. Word-sense disambiguation is one of the key concepts in natural language processing. The main goal of a language is to present a specific concept to the audience. This concept is extracted from the meaning of words in that language. System should be able to identify role and meaning of words in order to identify the concepts in texts properly. This issue becomes more problematic if there are words that take different meanings because of their surrounding words. Regarding that different practical programs have been developed in Persian language, it is vital now to find a solution for word-sense disambiguation in Persian language. Lack of training data is the biggest challenge in the course of word-sense disambiguation in Persian language. In order to face this problem, machine learning approach with minimal supervision is employed in this research. The applied method tries to disambiguate word senses by considering defined features of target words and applying collaborative learning method. Extracted corpus from published news by news agencies is used as the reference corpus. Evaluating the program by the available corpus on three considered ambiguous words, the implemented method has been able to properly identify the meaning of 5368 documents with 88% recall, 95% precision and 93% accuracy rate.","keywords_author":["Machine Learning","Natural Language Processing","Semi-Supervised Learning","Text Mining","Word Sense Disambiguation"],"keywords_other":["Text mining","Semi-supervised","Collaborative learning","Machine learning approaches","Semi- supervised learning","Practical projects","Word Sense Disambiguation","Persian languages"],"max_cite":0.0,"pub_year":2017.0,"sources":"['scp']","rawkeys":["persian languages","semi- supervised learning","text mining","collaborative learning","semi-supervised","natural language processing","machine learning","practical projects","semi-supervised learning","word sense disambiguation","machine learning approaches"],"tags":["persian languages","text mining","collaborative learning","semi-supervised","natural language processing","machine learning","practical projects","semi-supervised learning","word sense disambiguation","machine learning approaches"]},{"p_id":1251,"title":"A comparative study on phonological feature detection from continuous speech with respect to variable corpus size","abstract":"In this paper, place and manner of articulation based phonological features have been successfully identified with high accuracy using very minimal amount of training data. In detection-based, bottom-up speech recognition approach, the phonological feature based acoustic-phonetic speech attributes are considered as a key component. After identifying the features, they are merged together to get the phonemes. So this type of feature detection using low corpus size shows a path with which continuous speech can be recognized using inadequate data repository also. To execute the experiment, both the language, Bengali and English have been considered. The sentences were trained using deep neural network. Training procedure is carried out for Bengali using three different corpus sizes with a number of 100, 200, and 500 sentences. The average frame level accuracies were obtained as 87.88%, 88.43% and 88.96% respectively for CDAC speech corpus. Whereas using the same training procedure for TIMIT corpus, the accuracies were 87.97%, 88.84%, and 89.39% respectively. So the average frame level accuracy is almost same irrespective of number of training data. This ensures, in case of small speech corpora, phonological feature based speech attributes can be detected with the bottom-up approach.","keywords_author":["Speech attribute","phonological features","place of articulation","manner of articulation","detection based approach","bottom-up speech recognition","deep neural network","automatic speech recognition"],"keywords_other":["detection-based bottom-up speech recognition","speech recognition","CDAC speech corpus","Neural networks","acoustics","phonological feature based acoustic-phonetic speech attributes","TIMIT corpus","training data","Bengali language","Feature extraction","Speech","English language","phonological feature detection","learning (artificial intelligence)","Training","phonemes","variable corpus size","Hidden Markov models","continuous speech","deep neural network training","neural nets","Detectors","natural language processing","speech processing","Speech recognition","feature extraction","data repository"],"max_cite":2.0,"pub_year":2016.0,"sources":"['ieee']","rawkeys":["automatic speech recognition","bengali language","detection-based bottom-up speech recognition","speech recognition","acoustics","phonological feature based acoustic-phonetic speech attributes","training data","detection based approach","timit corpus","bottom-up speech recognition","manner of articulation","speech","cdac speech corpus","english language","phonological feature detection","learning (artificial intelligence)","neural networks","phonemes","training","variable corpus size","continuous speech","deep neural network training","neural nets","hidden markov models","detectors","deep neural network","natural language processing","speech processing","speech attribute","feature extraction","phonological features","data repository","place of articulation"],"tags":["automatic speech recognition","bengali language","detection-based bottom-up speech recognition","data repositories","speech recognition","convolutional neural network","acoustics","phonological feature based acoustic-phonetic speech attributes","training data","detection based approach","timit corpus","bottom-up speech recognition","machine learning","phoneme","manner of articulation","speech","cdac speech corpus","phonological feature detection","neural networks","training","variable corpus size","continuous speech","deep neural network training","hidden markov models","detectors","natural language processing","speech processing","speech attribute","english languages","feature extraction","phonological features","place of articulation"]},{"p_id":34022,"title":"Weakly supervised learning of biomedical information extraction from curated data","abstract":"\u00a9 2015 Jain et al.Background: Numerous publicly available biomedical databases derive data by curating from literatures. The curated data can be useful as training examples for information extraction, but curated data usually lack the exact mentions and their locations in the text required for supervised machine learning. This paper describes a general approach to information extraction using curated data as training examples. The idea is to formulate the problem as cost-sensitive learning from noisy labels, where the cost is estimated by a committee of weak classifiers that consider both curated data and the text. Results: We test the idea on two information extraction tasks of Genome-Wide Association Studies (GWAS). The first task is to extract target phenotypes (diseases or traits) of a study and the second is to extract ethnicity backgrounds of study subjects for different stages (initial or replication). Experimental results show that our approach can achieve 87 % of Precision-at-2 (P@2) for disease\/trait extraction, and 0.83 of F1-Score for stage-ethnicity extraction, both outperforming their cost-insensitive baseline counterparts. Conclusions: The results show that curated biomedical databases can potentially be reused as training examples to train information extractors without expert annotation or refinement, opening an unprecedented opportunity of using \"big data\" in biomedical text mining.","keywords_author":["Biomedical text mining","Database curation","Information extraction","Machine learning","Natural language processing"],"keywords_other":["Risk Assessment","Biomedical information extractions","Abstracting and Indexing as Topic","Genetic Predisposition to Disease","Humans","Curation","Disease","Biomedical text minings","Databases, Factual","Genome-wide association studies","Genome-Wide Association Study","Cost-sensitive learning","Weakly supervised learning","Data Mining","Supervised machine learning","Data Curation","NAtural language processing"],"max_cite":2.0,"pub_year":2016.0,"sources":"['scp']","rawkeys":["databases","biomedical text minings","genome-wide association study","database curation","information extraction","machine learning","risk assessment","data curation","data mining","abstracting and indexing as topic","disease","biomedical text mining","humans","factual","biomedical information extractions","curation","genetic predisposition to disease","natural language processing","genome-wide association studies","cost-sensitive learning","supervised machine learning","weakly supervised learning"],"tags":["databases","biomedical text minings","genome-wide association study","database curation","information extraction","machine learning","risk assessment","data curation","data mining","abstracting and indexing as topic","disease","humans","factual","biomedical information extractions","curation","genetic predisposition to disease","natural language processing","cost-sensitive learning","supervised machine learning","weakly supervised learning"]},{"p_id":50406,"title":"Requirement mining for model-based product design","abstract":"Copyright \u00a9 2016 Inderscience Enterprises Ltd. PLM software applications should enable engineers to develop and manage requirements throughout the product's lifecycle. However, PLM activities of the beginning-of-life and end-of-life of a product mainly deal with a fastidious document-based approach. Indeed, requirements are scattered in many different prescriptive documents (reports, specifications, standards, regulations, etc.) that make the feeding of a requirements management tool laborious. Our contribution is two-fold. First, we propose a natural language processing (NLP) pipeline to extract requirements from prescriptive documents. Second, we show how machine learning techniques can be used to develop a text classifier that will automatically classify requirements into disciplines. Both contributions support companies willing to feed a requirements management tool from prescriptive documents. The NLP experiment shows an average precision of 0.86 and an average recall of 0.95, whereas the SVM requirements classifier outperforms that of naive Bayes with a 76% accuracy rate.","keywords_author":["Classification","Extraction","Natural language processing","NLP","Requirements","Supervised learning machine learning","Unstructured"],"keywords_other":null,"max_cite":0.0,"pub_year":2016.0,"sources":"['scp']","rawkeys":["requirements","nlp","unstructured","natural language processing","classification","supervised learning machine learning","extraction"],"tags":["requirements","unstructured","natural language processing","classification","supervised learning machine learning","extraction"]},{"p_id":48362,"title":"Automated annotation and classification of BI-RADS assessment from radiology reports","abstract":"\u00a9 2017 The Breast Imaging Reporting and Data System (BI-RADS) was developed to reduce variation in the descriptions of findings. Manual analysis of breast radiology report data is challenging but is necessary for clinical and healthcare quality assurance activities. The objective of this study is to develop a natural language processing (NLP) system for automated BI-RADS categories extraction from breast radiology reports. We evaluated an existing rule-based NLP algorithm, and then we developed and evaluated our own method using a supervised machine learning approach. We divided the BI-RADS category extraction task into two specific tasks: (1) annotation of all BI-RADS category values within a report, (2) classification of the laterality of each BI-RADS category value. We used one algorithm for task 1 and evaluated three algorithms for task 2. Across all evaluations and model training, we used a total of 2159 radiology reports from 18 hospitals, from 2003 to 2015. Performance with the existing rule-based algorithm was not satisfactory. Conditional random fields showed a high performance for task 1 with an F-1 measure of 0.95. Rules from partial decision trees (PART) algorithm showed the best performance across classes for task 2 with a weighted F-1 measure of 0.91 for BIRADS 0-6, and 0.93 for BIRADS 3-5. Classification performance by class showed that performance improved for all classes from Na\u00efve Bayes to Support Vector Machine (SVM), and also from SVM to PART. Our system is able to annotate and classify all BI-RADS mentions present in a single radiology report and can serve as the foundation for future studies that will leverage automated BI-RADS annotation, to provide feedback to radiologists as part of a learning health system loop.","keywords_author":["Breast Imaging Reporting and Data System (BI-RADS)","Imaging informatics","Information extraction","Machine learning","Natural language processing"],"keywords_other":["Classification performance","Partial decision trees","Bayes Theorem","Breast Neoplasms","Imaging informatics","Breast","Female","Humans","Breast imaging reporting and data systems","Mammography","Data Curation","BI-RADS","Radiology Information Systems","Supervised machine learning","Conditional random field","NAtural language processing"],"max_cite":0.0,"pub_year":2017.0,"sources":"['scp']","rawkeys":["breast neoplasms","breast","breast imaging reporting and data systems","partial decision trees","information extraction","conditional random field","imaging informatics","machine learning","radiology information systems","data curation","breast imaging reporting and data system (bi-rads)","classification performance","bi-rads","humans","mammography","bayes theorem","natural language processing","supervised machine learning","female"],"tags":["breast neoplasms","breast","classification performance","information extraction","conditional random field","bi-rads","imaging informatics","machine learning","humans","natural language processing","partial decision trees","radiology information systems","mammography","bayes theorem","supervised machine learning","female","data curation"]},{"p_id":48364,"title":"Towards generalizable entity-centric clinical coreference resolution","abstract":"\u00a9 2017 Objective This work investigates the problem of clinical coreference resolution in a model that explicitly tracks entities, and aims to measure the performance of that model in both traditional in-domain train\/test splits and cross-domain experiments that measure the generalizability of learned models. Methods The two methods we compare are a baseline mention-pair coreference system that operates over pairs of mentions with best-first conflict resolution and a mention-synchronous system that incrementally builds coreference chains. We develop new features that incorporate distributional semantics, discourse features, and entity attributes. We use two new coreference datasets with similar annotation guidelines \u2013 the THYME colon cancer dataset and the DeepPhe breast cancer dataset. Results The mention-synchronous system performs similarly on in-domain data but performs much better on new data. Part of speech tag features prove superior in feature generalizability experiments over other word representations. Our methods show generalization improvement but there is still a performance gap when testing in new domains. Discussion Generalizability of clinical NLP systems is important and under-studied, so future work should attempt to perform cross-domain and cross-institution evaluations and explicitly develop features and training regimens that favor generalizability. A performance-optimized version of the mention-synchronous system will be included in the open source Apache cTAKES software.","keywords_author":["Clinical NLP","Coreference","Generalizability","Machine learning","Portability"],"keywords_other":["Synchronous system","Generalizability","Electronic Health Records","Co-reference resolutions","Humans","Semantics","Word representations","Natural Language Processing","Clinical NLP","Conflict Resolution","Distributional semantics","Coreference","APACHE","Software"],"max_cite":0.0,"pub_year":2017.0,"sources":"['scp']","rawkeys":["generalizability","co-reference resolutions","distributional semantics","semantics","machine learning","electronic health records","humans","clinical nlp","natural language processing","coreference","portability","software","apache","synchronous system","word representations","conflict resolution"],"tags":["generalizability","distributional semantics","semantics","machine learning","electronic health records","humans","clinical nlp","natural language processing","coreference","portability","software","apache","synchronous system","word representations","conflict resolution","coreference resolution"]},{"p_id":17650,"title":"PhishGILLNET-phishing detection methodology using probabilistic latent semantic analysis, AdaBoost, and co-training","abstract":"Identity theft is one of the most profitable crimes committed by felons. In the cyber space, this is commonly achieved using phishing. We propose here robust server side methodology to detect phishing attacks, called phishGILLNET, which incorporates the power of natural language processing and machine learning techniques. phishGILLNET is a multi-layered approach to detect phishing attacks. The first layer (phishGILLNET1) employs Probabilistic Latent Semantic Analysis (PLSA) to build a topic model. The topic model handles synonym (multiple words with similar meaning), polysemy (words with multiple meanings), and other linguistic variations found in phishing. Intentional misspelled words found in phishing are handled using Levenshtein editing and Google APIs for correction. Based on term document frequency matrix as input PLSA finds phishing and non-phishing topics using tempered expectation maximization. The performance of phishGILLNET1 is evaluated using PLSA fold in technique and the classification is achieved using Fisher similarity. The second layer of phishGILLNET (phishGILLNET2) employs AdaBoost to build a robust classifier. Using probability distributions of the best PLSA topics as features the classifier is built using AdaBoost. The third layer (phishGILLNET3) further expands phishGILLNET2 by building a classifier from labeled and unlabeled examples by employing Co-Training. Experiments were conducted using one of the largest public corpus of email data containing 400,000 emails. Results show that phishGILLNET3 outperforms state of the art phishing detection methods and achieves F-measure of 100%. Moreover, phishGILLNET3 requires only a small percentage (10%) of data be annotated thus saving significant time, labor, and avoiding errors incurred in human annotation. \u00a9 2012 Ramanathan and Wechsler; licensee Springer.","keywords_author":["Boosting","Co-training","Identity theft","Machine learning","Natural language processing","Phishing","Probabilistic latent semantic analysis"],"keywords_other":["Co-training","Phishing","Identity theft","Probabilistic latent semantic analysis","boosting","NAtural language processing","Boosting"],"max_cite":28.0,"pub_year":2012.0,"sources":"['scp']","rawkeys":["probabilistic latent semantic analysis","machine learning","natural language processing","co-training","identity theft","boosting","phishing"],"tags":["probabilistic latent semantic analysis","machine learning","natural language processing","co-training","identity theft","boosting","phishing"]},{"p_id":1266,"title":"Deep neural network based phonological feature extraction for Bengali continuous speech","abstract":"Automatic Speech Attribute Transcription framework is a recently proposed paradigm for detection based bottom-up speech recognition. Speech signal contains a large set of related information, known as speech attributes that include a set of fundamental speech sound with their linguistic identification that is phonological features. In this study, a bank of Deep Neural Network based attribute detectors has been applied to Bengali continuous speech corpus to detect phonological features of Bengali language. 89.17% of average attribute detection accuracy was achieved for Bengali continuous speech. This experiment was repeated using TIMIT speech corpus to ensure the system robustness, and the average detection accuracy was 89.40%.","keywords_author":["Phonological features","place of articulation","manner of articulation","detection-based approach","deep neural network"],"keywords_other":["linguistics","speech recognition","Neural networks","automatic speech attribute transcription framework","phonological feature extraction","Feature extraction","Speech","Bengali continuous speech","phonological feature detection","attribute detectors","Training","Hidden Markov models","neural nets","Detectors","detection based bottom-up speech recognition","deep neural network","linguistic identification","fundamental speech sound","natural language processing","Speech recognition","phonological features","feature extraction"],"max_cite":2.0,"pub_year":2016.0,"sources":"['ieee']","rawkeys":["linguistics","speech recognition","automatic speech attribute transcription framework","phonological feature extraction","manner of articulation","speech","phonological feature detection","detection-based approach","attribute detectors","neural networks","training","neural nets","bengali continuous speech","detection based bottom-up speech recognition","hidden markov models","detectors","deep neural network","linguistic identification","fundamental speech sound","natural language processing","feature extraction","phonological features","place of articulation"],"tags":["linguistics","detection-based bottom-up speech recognition","speech recognition","convolutional neural network","automatic speech attribute transcription framework","phonological feature extraction","detection based approach","manner of articulation","speech","phonological feature detection","attribute detectors","neural networks","training","bengali continuous speech","hidden markov models","detectors","linguistic identification","fundamental speech sound","natural language processing","feature extraction","phonological features","place of articulation"]},{"p_id":27893,"title":"PDF text classification to leverage information extraction from publication reports","abstract":"\u00a9 2016 Elsevier Inc. Objectives: Data extraction from original study reports is a time-consuming, error-prone process in systematic review development. Information extraction (IE) systems have the potential to assist humans in the extraction task, however majority of IE systems were not designed to work on Portable Document Format (PDF) document, an important and common extraction source for systematic review. In a PDF document, narrative content is often mixed with publication metadata or semi-structured text, which add challenges to the underlining natural language processing algorithm. Our goal is to categorize PDF texts for strategic use by IE systems. Methods: We used an open-source tool to extract raw texts from a PDF document and developed a text classification algorithm that follows a multi-pass sieve framework to automatically classify PDF text snippets (for brevity, texts) into TITLE, ABSTRACT, BODYTEXT, SEMISTRUCTURE, and METADATA categories. To validate the algorithm, we developed a gold standard of PDF reports that were included in the development of previous systematic reviews by the Cochrane Collaboration. In a two-step procedure, we evaluated (1) classification performance, and compared it with machine learning classifier, and (2) the effects of the algorithm on an IE system that extracts clinical outcome mentions. Results: The multi-pass sieve algorithm achieved an accuracy of 92.6%, which was 9.7% (p < 0.001) higher than the best performing machine learning classifier that used a logistic regression algorithm. F-measure improvements were observed in the classification of TITLE (+15.6%), ABSTRACT (+54.2%), BODYTEXT (+3.7%), SEMISTRUCTURE (+34%), and MEDADATA (+14.2%). In addition, use of the algorithm to filter semi-structured texts and publication metadata improved performance of the outcome extraction system (F-measure +4.1%, p = 0.002). It also reduced of number of sentences to be processed by 44.9% (p < 0.001), which corresponds to a processing time reduction of 50% (p = 0.005). Conclusions: The rule-based multi-pass sieve framework can be used effectively in categorizing texts extracted from PDF documents. Text classification is an important prerequisite step to leverage information extraction from PDF documents.","keywords_author":["Document analysis","Machine learning","Natural language processing","Text classification"],"keywords_other":["Portable document formats","Cochrane collaboration","Classification performance","Algorithms","Information extraction systems","Text classification","Humans","Logistic regression algorithms","Publications","Document analysis","Natural Language Processing","Machine Learning","Review Literature as Topic","Information Storage and Retrieval","Narration","NAtural language processing"],"max_cite":5.0,"pub_year":2016.0,"sources":"['scp']","rawkeys":["narration","portable document formats","publications","classification performance","cochrane collaboration","machine learning","natural language processing","humans","review literature as topic","document analysis","logistic regression algorithms","information storage and retrieval","algorithms","information extraction systems","text classification"],"tags":["narration","portable document formats","publications","classification performance","cochrane collaboration","machine learning","natural language processing","humans","review literature as topic","document analysis","logistic regression algorithms","information storage and retrieval","algorithms","information extraction systems","text classification"]},{"p_id":34038,"title":"Needmining: Identifying micro blog data containing customer needs","abstract":"The design of new products and services starts with the identification of needs of potential customers or users. Many existing methods like observations, surveys, and experiments draw upon specific efforts to elicit unsatisfied needs from individuals. At the same time, a huge amount of user-generated content in micro blogs is freely accessible at no cost. While this information is already analyzed to monitor sentiments towards existing offerings, it has not yet been tapped for the elicitation of needs. In this paper, we lay an important foundation for this endeavor: we propose a Machine Learning approach to identify those posts that do express needs. Our evaluation of tweets in the e-mobility domain demonstrates that the small share of relevant tweets can be identified with remarkable precision or recall results. Applied to huge data sets, the developed method should enable scalable need elicitation support for innovation managers - across thousands of users, and thus augment the service design tool set available to him.","keywords_author":["Innovation management","Machine learning","Micro blogs","Needs elicitation","NLP","Service design","Text analysis"],"keywords_other":["Potential customers","User-generated content","Text analysis","Machine learning approaches","Needs elicitation","Products and services","Innovation management","Service design"],"max_cite":2.0,"pub_year":2016.0,"sources":"['scp']","rawkeys":["needs elicitation","nlp","potential customers","service design","innovation management","machine learning","user-generated content","products and services","text analysis","micro blogs","machine learning approaches"],"tags":["potential customers","service design","innovation management","need elicitation","machine learning","natural language processing","products and services","user-generated content","text analysis","microblogging","machine learning approaches"]},{"p_id":52472,"title":"Phrase chunking","abstract":"\u00a9 The Author(s) 2012. This chapter presents the application of ETL to language independent phrase chunking (PCK). The PCK task consists in dividing a text into non-overlapping phrases. We apply ETL and ETL committee to four different corpora in three different languages: Portuguese, English and Hindi. For the four corpora ETL system achieves very competitive results. For two copora ETL achieves state-of-the-art results. ETL committee significantly improves the ETL results for the four corpora. This chapter is organized as follows. In Sect. 6.1, we describe the task and the selected corpora. In Sect. 6.2, we detail some modeling configurations used in our PCK system. In Sect. 6.3, we show some configurations used in the machine learning algorithms. Section 6.4 presents the application of ETL for the SNR-CLIC Corpus. In Sect. 6.5, we detail the application of ETL for the Ramshaw and Marcus Corpus. Section 6.6 presents the application of ETL for the CoNLL-2000 Corpus. In Sect. 6.7, we present the application of ETL for the SPSAL-2007 Corpus. Finally, Sect. 6.8 presents some concluding remarks.","keywords_author":["Conditional random fields","Entropy guided transformation learning","ETL committee","Hidden Markov models","Machine learning","Natural language processing","Phrase chunking","Support vector machines","Transformation based learning"],"keywords_other":null,"max_cite":0.0,"pub_year":2012.0,"sources":"['scp']","rawkeys":["phrase chunking","hidden markov models","conditional random fields","machine learning","natural language processing","transformation based learning","entropy guided transformation learning","support vector machines","etl committee"],"tags":["phrase chunking","hidden markov models","conditional random field","machine learning","natural language processing","transformation based learning","entropy guided transformation learning","etl committee"]},{"p_id":19705,"title":"Learning domain-specific polarity lexicons","abstract":"Sentiment analysis aims to automatically estimate the sentiment in a given text as positive or negative. Polarity lexicons, often used in sentiment analysis, indicate how positive or negative each term in the lexicon is. However, since creating domain-specific polarity lexicons is expensive and time-consuming, researchers often use a general purpose or domain-independent lexicon. In this work, we address the problem of adapting a general purpose polarity lexicon to a specific domain and propose a simple yet effective adaptation algorithm. We experimented with two sets of reviews from the hotel and movie domains and observed that while our adaptation techniques changed the polarity values for only a small set of words, the overall test accuracy increased significantly: 77% to 83% in the hotel dataset and 61% to 66% in the movie dataset. \u00a9 2012 IEEE.","keywords_author":["Lexicon adaptation","Machine learning","Natural language processing","Polarity detection","Sentiment analysis"],"keywords_other":["Adaptation algorithms","Lexicon adaptation","Sentiment analysis","General purpose","Data sets","Test accuracy","Domain specific","Adaptation techniques","NAtural language processing"],"max_cite":19.0,"pub_year":2012.0,"sources":"['scp']","rawkeys":["test accuracy","polarity detection","adaptation techniques","machine learning","natural language processing","data sets","general purpose","domain specific","adaptation algorithms","lexicon adaptation","sentiment analysis"],"tags":["test accuracy","polarity detection","adaptation techniques","machine learning","natural language processing","data sets","general purpose","domain specific","adaptive algorithms","lexicon adaptation","sentiment analysis"]},{"p_id":38138,"title":"Automatic Summarization of Privacy Policies using Ensemble Learning","abstract":"When customers purchase a product or sign up for service from a company, they often are required to agree to a Privacy Policy or Terms of Service agreement. Many of these policies are lengthy, and a typical customer agrees to them without reading them carefully if at all. To address this problem, we have developed a prototype automatic text summarization system which is specifically designed for privacy policies. Our system generates a summary of a policy statement by identifying important sentences from the statement, categorizing these sentences by which of 5 \"statement categories\" the sentence addresses, and displaying to a user a list of the sentences which match each category. Our system incorporates keywords identified by a human domain expert and rules that were obtained by machine learning, and they are combined in an ensemble architecture. We have tested our system on a sample corpus of privacy statements, and preliminary results are promising.","keywords_author":["Machine learning","Natural language processing","Privacy policy"],"keywords_other":["Terms of services","Policy statements","Ensemble learning","Automatic text summarization","Privacy statements","Privacy policies","NAtural language processing","Automatic summarization"],"max_cite":1.0,"pub_year":2016.0,"sources":"['scp']","rawkeys":["automatic summarization","policy statements","terms of services","privacy policy","privacy statements","natural language processing","machine learning","ensemble learning","privacy policies","automatic text summarization"],"tags":["automatic summarization","policy statements","terms of services","privacy statements","natural language processing","machine learning","ensemble learning","privacy policies","automatic text summarization"]},{"p_id":46329,"title":"Bidirectional LSTM with a context input window for named entity recognition in tweets","abstract":"\u00a9 2017 Copyright held by the owner\/author(s). Lately, with the increasing popularity of social media technologies, applying natural language processing for mining information in tweets has posed itself as a challenging task and has attracted significant research efforts. In contrast with the news text and others formal content, tweets pose a number of new challenges, due to their short and noisy nature. Thus, over the past decade, different Named Entity Recognition (NER) architectures have been proposed to solve this problem. However, most of them are based on handcrafted-features and restricted to a particular domain, which imposes a natural barrier to generalize over different contexts. In this sense, despite the long line of work in NER on formal domains, there are no studies in NER for tweets in Portuguese (despite 17.97 million monthly active users). To bridge this gap, we present a new gold-standard corpus of tweets annotated for Person, Location, and Organization (PLO). Additionally, we also perform multiple NER experiments using a variety of Long Short-Term Memory (LSTM) based models without resorting to any handcrafted rules. Our approach with a centered context input window of word embeddings yields 52.78 F1 score, 38.68% higher compared to a state of the art baseline system.","keywords_author":["Deep learning","Informal text","Machine learning","Named entity recognition","Natural language processing","Neural networks"],"keywords_other":["Gold standards","Baseline systems","Research efforts","Named entity recognition","Informal text","Social media","State of the art","Handcrafted rules"],"max_cite":0.0,"pub_year":2017.0,"sources":"['scp']","rawkeys":["baseline systems","deep learning","neural networks","social media","named entity recognition","machine learning","natural language processing","state of the art","gold standards","research efforts","informal text","handcrafted rules"],"tags":["baseline systems","neural networks","social media","informational text","machine learning","named entity recognition","natural language processing","state of the art","gold standards","research efforts","handcrafted rules"]},{"p_id":48379,"title":"Effect of parameter variations on accuracy of Convolutional Neural Network","abstract":"\u00a9 2016 IEEE. In this paper, we implement a Convolutional Neural Network especially designed for Natural Language processing. With the help of this CNN, we try to classify sentences for sentiment analysis for which the embeddings used were learned from scratch rather than using pre-trained word2vec vectors. Here we try to vary the different parameters and learn how they effect on the performance of the CNN. From the observations we try to demonstrate that a fairly less-complex CNN that has a small amount of parameter adjustments and fine-tuning can achieve a significant growth in performance.","keywords_author":["Character Embedding","Convolutional Neural Network","Deep Learning","Feature Map","Learning Rate","Natural Language Processing","Pooling","Regularization","Sentence Classification"],"keywords_other":["Sentence classifications","Regularization","Pooling","Convolutional neural network","Learning rates","Feature map","Character Embedding","NAtural language processing"],"max_cite":0.0,"pub_year":2017.0,"sources":"['scp']","rawkeys":["feature map","deep learning","natural language processing","sentence classifications","learning rate","convolutional neural network","sentence classification","learning rates","pooling","character embedding","regularization"],"tags":["feature map","machine learning","natural language processing","sentence classifications","convolutional neural network","learning rates","pooling","character embedding","regularization"]},{"p_id":19709,"title":"Syndromic classification of twitter messages","abstract":"Recent studies have shown strong correlation between social networking data and national influenza rates. We expanded upon this success to develop an automated text mining system that classifies Twitter messages in real time into six syndromic categories based on key terms from a public health ontology. 10-fold cross validation tests were used to compare Naive Bayes (NB) and Support Vector Machine (SVM) models on a corpus of 7431 Twitter messages. SVM performed better than NB on 4 out of 6 syndromes. The best performing classifiers showed moderately strong F1 scores: respiratory = 86.2 (NB); gastrointestinal = 85.4 (SVM polynomial kernel degree 2); neurological = 88.6 (SVM polynomial kernel degree 1); rash = 86.0 (SVM polynomial kernel degree 1); constitutional = 89.3 (SVM polynomial kernel degree 1); hemorrhagic = 89.9 (NB). The resulting classifiers were deployed together with an EARS C2 aberration detection algorithm in an experimental online system. \u00a9 2012 ICST Institute for Computer Science, Social Informatics and Telecommunications Engineering.","keywords_author":["epidemic intelligence","machine learning","natural language processing","social networking"],"keywords_other":["Text mining","Naive bayes","Real time","Polynomial kernels","Cross-validation tests","Strong correlation","Detection algorithm","epidemic intelligence","NAtural language processing"],"max_cite":19.0,"pub_year":2012.0,"sources":"['scp']","rawkeys":["strong correlation","text mining","cross-validation tests","real time","machine learning","detection algorithm","natural language processing","polynomial kernels","epidemic intelligence","naive bayes","social networking"],"tags":["strong correlation","text mining","cross-validation tests","real time","machine learning","detection algorithm","natural language processing","polynomial kernels","social networks","epidemic intelligence","naive bayes"]},{"p_id":60670,"title":"Learning distributed word representation with multi-contextual mixed embedding","abstract":"Learning distributed word representations has been a popular method for various natural language processing applications such as word analogy and similarity, document classification and sentiment analysis. However, most existing word embedding models only exploit a shallow slide window as the context to predict the target word. Because the semantic of each word is also influenced by its global context, as the distributional models usually induced the. word representations from the global co-occurrence matrix, the window-based models are insufficient to capture semantic knowledge. In this paper, we propose a novel hybrid model called mixed word embedding (MWE) based on the well-known word2vec toolbox. Specifically, the proposed MWE model combines the two variants of word2vec, i.e., SKIP-GRAM and CBOW, in a seamless way via sharing a common encoding structure, which is able to capture the syntax information of words more accurately. Furthermore, it incorporates a global text vector into the CBOW variant so as to capture more semantic information. Our MWE preserves the same time complexity as the SKIP-GRAM. To evaluate our MWE model efficiently and adaptively, we study our model on linguistic and application perspectives with both English and Chinese dataset. For linguistics, we conduct empirical studies on word analogies and similarities. The learned latent representations on both document classification and sentiment analysis are considered for application point of view of this work. The experimental results show that our MWE model is very competitive in all tasks as compared with the state-of-the-art word embedding models such as CBOW, SKIP-GRAM, and GloVe. (C) 2016 Elsevier B.V. All rights reserved.","keywords_author":["Word embedding","Distributed word representation","Word2vec","Natural language processing"],"keywords_other":["SEMANTICS","MODELS","SENTIMENT ANALYSIS"],"max_cite":11.0,"pub_year":2016.0,"sources":"['ieee', 'wos']","rawkeys":["word2vec","semantics","natural language processing","word embedding","distributed word representation","models","sentiment analysis"],"tags":["model","word2vec","semantics","natural language processing","word embedding","distributed word representation","sentiment analysis"]},{"p_id":97534,"title":"GLA: Global-Local Attention for Image Description","abstract":"In recent years, the task of automatically generating image description has attracted a lot of attention in the field of artificial intelligence. Benefitting from the development of convolutional neural networks (CNNs) and recurrent neural networks (RNNs), many approaches based on the CNN-RNN framework have been proposed to solve this task and achieved remarkable process. However, two problems remain to be tackled in which the most existing methods use only the image-level representation. One problem is object missing, in which some important objects may he missing when generating the image description and the other is misprediction, when one object may be recognized in a wrong category. In this paper, to address these two problems, we propose a new method called global-local attention (GLA) for generating image description. The proposed GLA model utilizes an attention mechanism to integrate object-level features with image-level feature. Through this manner, our model can selectively pay attention to objects and context information concurrently. Therefore, our proposed GLA method can generate more relevant image description sentences and achieve the state-of-the-art performance on the well-known Microsoft COCO caption dataset with several popular evaluation metrics-CIDEr, METEOR, ROUGE-L and BLEU-1, 2,3, 4.","keywords_author":["Convolutional neural network","recurrent neural network","image description","natural language processing"],"keywords_other":["MODEL","NEURAL-NETWORKS","RETRIEVAL"],"max_cite":0.0,"pub_year":2018.0,"sources":"['wos']","rawkeys":["neural-networks","model","natural language processing","recurrent neural network","image description","convolutional neural network","retrieval"],"tags":["model","neural networks","natural language processing","image descriptions","convolutional neural network","retrieval"]},{"p_id":34049,"title":"Feature selection for movie recommendation","abstract":"\u00a9 T\u00dcB\u0130TAK.TV users have an abundance of different movies they could choose from, and with the quantity and quality of data available both on user behavior and content, better recommenders are possible. In this paper, we evaluate and combine different content-based and collaborative recommendation methods for a Turkish movie recommendation system. Our recommendation methods can make use of user behavior, different types of content features, and other users' behavior to predict movie ratings. We gather different types of data on movies, such as the description, actors, directors, year, and genre. We use natural language processing methods to convert the Turkish movie descriptions into keyword vectors. Then, for each user, we use the content features and the user's past implicit ratings to produce content feature-based user profiles. In order to have more reliable profiles, we do feature selection on these profiles. We show that for each feature space, such as actor, director, or keyword, a different amount of feature selection may be optimal. Different recommenders may also perform best for a different number of movies available as training data for a user. We also combine different content-based recommenders and collaborative recommenders using an aggregation or the best of the available recommenders. Experimental results on a dataset with hundreds of users and movies show that, especially for users who have watched a small number of movies in the past, feature selection can increase recommendation success.","keywords_author":["Content feature-based user profile","Content-based filtering","Feature selection","Hybrid recommenders","Machine learning"],"keywords_other":["Content based filtering","Collaborative recommender","Recommendation methods","User profile","Hybrid recommenders","Collaborative recommendation","Movie recommendations","NAtural language processing"],"max_cite":2.0,"pub_year":2016.0,"sources":"['scp']","rawkeys":["collaborative recommendation","collaborative recommender","hybrid recommenders","machine learning","movie recommendations","natural language processing","content feature-based user profile","content-based filtering","feature selection","content based filtering","recommendation methods","user profile"],"tags":["collaborative recommendation","machine learning","movie recommendations","natural language processing","content feature-based user profile","hybrid recommendation","feature selection","content based filtering","recommendation methods","user profile"]},{"p_id":52481,"title":"Semantic role labeling","abstract":"\u00a9 The Author(s) 2012. This chapter presents the application of the ETL approach to semantic role labeling (SRL). The SRL task consists in detecting basic event structures in a given text. Some of these event structures include who did what to whom, when and where. We evaluate the performance of ETL over two English language corpora: CoNLL-2004 and CoNLL-2005. ETL system achieves regular results for the two corpora. However, for the CoNLL-2004 Corpus, our ETL system outperforms the TBL system proposed by Higgins [4]. ETL committee significantly improves the ETL results for the two corpora. This chapter is organized as follows. In Sect. 8.1, we describe the selected corpora. In Sect. 8.2, we detail some modeling configurations used in our SRL system. In Sect. 8.3, we show some configurations used in the machine learning algorithms. Section 8.4 presents the application of ETL for the CoNLL-2004 Corpus. Section 8.5 presents the application of ETL for the CoNLL-2005 Corpus. Finally, Sect. 8.6 presents some concluding remarks.","keywords_author":["Adaboost","Entropy guided transformation learning","ETL committee","Machine learning","Natural language processing","Semantic role labeling","Support vector machines","Transformation based learning"],"keywords_other":null,"max_cite":0.0,"pub_year":2012.0,"sources":"['scp']","rawkeys":["machine learning","natural language processing","transformation based learning","entropy guided transformation learning","support vector machines","semantic role labeling","etl committee","adaboost"],"tags":["machine learning","natural language processing","transformation based learning","entropy guided transformation learning","semantic role labeling","etl committee","adaboost"]},{"p_id":27907,"title":"A study on Web security incidents in China by analyzing vulnerability disclosure platforms","abstract":"\u00a9 2015 Elsevier Ltd All rights reserved.Understanding the nature of a country's World Wide Web security can allow analysts to evaluate the security awareness of local organizations, the technology employed by researchers, and the defense capabilities of the whole country. In this paper, we put forward a new framework to evaluate the security situation in China with real vulnerability disclosure platforms. The focus of this research is to analyze the current situation of Chinese websites using 57,112 Web vulnerability incidents submitted by 5371 researchers from 2012 to 2015. The dataset is distributed into four types of organizations, including listed companies, government institutions, educational institutions, and startups. We present an approach, based on machine learning and natural language processing technologies, to classify the vulnerability type for each incident. Furthermore, our experimental results show that the vulnerability distribution and response speed toward important issues are so different among the four types of organizations that researchers at various levels of experience begin to take part in submitting vulnerabilities to public disclosure platforms. Based on the results, we propose security some best-practices for organizations and show that the security situation of Chinese websites has changed quickly in the last three years but is still facing several big problems.","keywords_author":["Machine learning","Security response","Vulnerability disclosure","Vulnerability management","Web security"],"keywords_other":["Security awareness","Educational institutions","WEB security","Security response","Government institutions","Vulnerability disclosure","Vulnerability management","NAtural language processing"],"max_cite":5.0,"pub_year":2016.0,"sources":"['scp']","rawkeys":["security awareness","web security","vulnerability management","natural language processing","machine learning","educational institutions","vulnerability disclosure","security response","government institutions"],"tags":["security awareness","web security","vulnerability management","natural language processing","machine learning","educational institutions","vulnerability disclosure","security response","government institutions"]},{"p_id":34051,"title":"The comprehension of figurative language: What is the influence of irony and sarcasm on NLP techniques?","abstract":"\u00a9 Springer International Publishing Switzerland 2016.Due to the growing volume of available textual information, there is a great demand for Natural Language Processing (NLP) techniques that can automatically process and manage texts, supporting the information retrieval and communication in core areas of society (e.g. healthcare, business, and science). NLP techniques have to tackle the often ambiguous and linguistic structures that people use in everyday speech. As such, there are many issues that have to be considered, for instance slang, grammatical errors, regional dialects, figurative language, etc. Figurative Language (FL), such as irony, sarcasm, simile, and metaphor, poses a serious challenge to NLP systems. FL is a frequent phenomenon within human communication, occurring both in spoken and written discourse including books, websites, for a, chats, social network posts, news articles and product reviews. Indeed, knowing what people think can help companies, political parties, and other public entities in strategizing and decision making polices. When people are engaged in an informal conversation, they almost inevitably use irony (or sarcasm) to express something else or different than stated by the literal sentence meaning. Sentiment analysis methods can be easily misled by the presence of words that have a strong polarity but are used sarcastically, which means that the opposite polarity was intended. Several efforts have been recently devoted to detect and tackle FL phenomena in social media. Many of applications rely on task-specific lexicons (e.g. dictionaries, word classifications) or Machine Learning algorithms. Increasingly, numerous companies have begun to leverage automated methods for inferring consumer sentiment from online reviews and other sources. A system capable of interpreting FL would be extremely beneficial to a wide range of practical NLP applications. In this sense, this chapter aims at evaluating how two specific domains of FL, sarcasm and irony, affect Sentiment Analysis (SA) tools. The study\u2019s ultimate goal is to find out if FL hinders the performance (polarity detection) of SA systems due to the presence of ironic context. Our results indicate that computational intelligence approaches are more suitable in presence of irony and sarcasm in Twitter classification.","keywords_author":["Figurative language","Irony","Machine learning","Natural language processing","Sarcasm","Sentiment analysis"],"keywords_other":null,"max_cite":2.0,"pub_year":2016.0,"sources":"['scp']","rawkeys":["natural language processing","machine learning","sarcasm","irony","figurative language","sentiment analysis"],"tags":["natural language processing","machine learning","sarcasm","irony","figurative language","sentiment analysis"]},{"p_id":52483,"title":"Clinician-driven automated classification of limb fractures from free-text radiology reports","abstract":"The aim of this research is to report initial experimental results and evaluation of a clinician-driven automated method that can address the issue of misdiagnosis from unstructured radiology reports. Timely diagnosis and reporting of patient symptoms in hospital emergency departments (ED) is a critical component of health services delivery. However, due to disperse information resources and vast amounts of manual processing of unstructured information, a point-of-care accurate diagnosis is often difficult. A rule-based method that considers the occurrence of clinician specified keywords related to radiological findings was developed to identify limb abnormalities, such as fractures. A dataset containing 99 narrative reports of radiological findings was sourced from a tertiary hospital. The rule-based method achieved an F-measure of 0.80 and an accuracy of 0.80. While our method achieves promising performance, a number of avenues for improvement were identified using advanced natural language processing (NLP) techniques.","keywords_author":["Classification","Emergency department","Limb fractures","Machine learning","Radiology reports","Rule-based method"],"keywords_other":["Hospital emergency departments","Health services deliveries","Rule-based method","Emergency departments","Unstructured radiology reports","Radiology reports","NAtural language processing","Automated classification"],"max_cite":0.0,"pub_year":2012.0,"sources":"['scp']","rawkeys":["automated classification","unstructured radiology reports","emergency departments","machine learning","natural language processing","hospital emergency departments","radiology reports","classification","rule-based method","limb fractures","emergency department","health services deliveries"],"tags":["automated classification","unstructured radiology reports","emergency departments","machine learning","natural language processing","hospital emergency departments","radiology reports","classification","rule-based method","limb fractures","health services deliveries"]},{"p_id":46345,"title":"Deep web crawling for insights from polar data","abstract":"\u00a9 2017 IEEE.We describe efforts to bring new methods of search analytics, machine learning, natural language processing and data visualization to address the challenge of finding and extracting meaning from unstructured text and multimedia content. We use the Polar domain to motivate the problem and our proposed solution. However our techniques are applicable and scalable to other domains.","keywords_author":["Information Retrieval","Machine Learning","Natural Language Processing","Polar Science","Search Analytics","Semantic Inference"],"keywords_other":null,"max_cite":0.0,"pub_year":2017.0,"sources":"['scp']","rawkeys":["natural language processing","machine learning","information retrieval","polar science","semantic inference","search analytics"],"tags":["natural language processing","machine learning","information retrieval","polar science","semantic inference","search analytics"]},{"p_id":50443,"title":"Multilevel syntactic parsing based on recursive restricted Boltzmann machines and learning to rank","abstract":"\u00a9 Springer International Publishing Switzerland 2016. Syntactic parsing is one of the central tasks in Natural Language Processing. In this paper, a multilevel syntactic parsing algorithm is proposed, which is a three-level model with innovative combinations of existing mature tools and algorithms. First, coarse-grained syntax trees are generated with general algorithms, such as Cocke-Younger-Kasami (CYK) algorithm based on Probabilistic Context Free Grammar (PCFG). Second, Recursive Restricted Boltzmann Machines (RRBM) are constructed, which aim at extracting feature vector through training syntax trees with deep learning methods. At last, Learning to Rank (LTR) model is trained to get the most satisfactory syntax tree and furthermore turn the parsing problem into a typical retrieval problem. Experiment results show that our method has achieved the state-of-the-art performance on syntactic parsing task.","keywords_author":["Deep learning","Learning to rank","Multilevel syntactic parsing","Recursive restricted Boltzmann machines"],"keywords_other":["Deep learning","State-of-the-art performance","Restricted boltzmann machine","Probabilistic context free grammars","Syntactic parsing","Learning to rank","Cocke-Younger-Kasami algorithm","NAtural language processing"],"max_cite":0.0,"pub_year":2016.0,"sources":"['scp']","rawkeys":["recursive restricted boltzmann machines","multilevel syntactic parsing","learning to rank","deep learning","syntactic parsing","natural language processing","probabilistic context free grammars","state-of-the-art performance","cocke-younger-kasami algorithm","restricted boltzmann machine"],"tags":["recursive restricted boltzmann machines","multilevel syntactic parsing","learning to rank","syntactic parsing","natural language processing","machine learning","probabilistic context free grammars","state-of-the-art performance","cocke-younger-kasami algorithm","restricted boltzmann machine"]},{"p_id":50452,"title":"Machine learning techniques for Myanmar word-sense disambiguation","abstract":"\u00a9 Springer International Publishing Switzerland 2016. Word Sense Disambiguation (WSD) is the vital of Natural Language processing such as machine translation, grammatical analysis, content analysis and information retrieval. WSD process is useful for automatically identifying the correct meaning of an ambiguous word in the sentence or the query when it has multiple meanings. In this paper, the supervised, semi-supervised, unsupervised and knowledge-based approaches for WSD are discussed. This work aim to explore the machine learning techniques for word sense disambiguation of Myanmar Nouns.","keywords_author":["Machine learning","Natural language processing","Word sense disambiguation"],"keywords_other":["Semi-supervised","Knowledge-based approach","Word Sense Disambiguation","Myanmars","Content analysis","Machine translations","NAtural language processing","Machine learning techniques"],"max_cite":0.0,"pub_year":2016.0,"sources":"['scp']","rawkeys":["machine learning techniques","machine learning","content analysis","natural language processing","myanmars","semi-supervised","knowledge-based approach","machine translations","word sense disambiguation"],"tags":["machine learning techniques","machine learning","content analysis","natural language processing","myanmars","semi-supervised","knowledge-based approach","machine translations","word sense disambiguation"]},{"p_id":38165,"title":"A Stochastic Approach for Finding Optimal Context in a Contextual Pattern Analysis Task","abstract":"\u00a9 2016 IEEE.This article concerns contextual pattern analysis tasks. As different contexts give different performances, models for finding the optimal context are revisited here. Random field models for the input data are assumed. An underlying random field is represented by a set of parameters capturing the spatial dependence. Next, a Bayesian approach is revisited to develop a decision rule for choosing appropriate context. The relevance of this approach is explored for three pattern analysis tasks, namely, handwriting analysis, image compression, and word sense disambiguation.","keywords_author":["Bayesian statistics","contextual information","image processing","intelligent systems","machine learning","natural language processing","optimal context","pattern recognition","random field models"],"keywords_other":["Contextual information","Random field model","Bayesian statistics","optimal context","NAtural language processing"],"max_cite":1.0,"pub_year":2016.0,"sources":"['scp']","rawkeys":["random field models","contextual information","image processing","random field model","machine learning","natural language processing","intelligent systems","optimal context","bayesian statistics","pattern recognition"],"tags":["contextual information","image processing","random field model","machine learning","natural language processing","intelligent systems","optimal context","bayesian statistics","pattern recognition"]},{"p_id":29974,"title":"Text window denoising autoencoder: Building deep architecture for Chinese word segmentation","abstract":"Deep learning is the new frontier of machine learning research, which has led to many recent breakthroughs in English natural language processing. However, there are inherent differences between Chinese and English, and little work has been done to apply deep learning techniques to Chinese natural language processing. In this paper, we propose a deep neural network model: text window denoising autoencoder, as well as a complete pre-training solution as a new way to solve classical Chinese natural language processing problems. This method does not require any linguistic knowledge or manual feature design, and can be applied to various Chinese natural language processing tasks, such as Chinese word segmentation. On the PKU dataset of Chinese word segmentation bakeoff 2005, applying this method decreases the F1 error rate by 11.9% for deep neural network based models. We are the first to apply deep learning methods to Chinese word segmentation to our best knowledge. \u00a9 Springer-Verlag Berlin Heidelberg 2013.","keywords_author":["Chinese natural language processing","Deep learning","Denoising autoencoder","Word segmentation"],"keywords_other":["Deep learning","Word segmentation","Chinese word segmentation","Machine learning research","Auto encoders","Chinese natural language processing","Deep neural networks","NAtural language processing"],"max_cite":4.0,"pub_year":2013.0,"sources":"['scp']","rawkeys":["word segmentation","chinese word segmentation","chinese natural language processing","deep neural networks","deep learning","denoising autoencoder","auto encoders","natural language processing","machine learning research"],"tags":["word segmentation","chinese word segmentation","chinese natural language processing","denoising autoencoder","machine learning","auto encoders","natural language processing","convolutional neural network","machine learning research"]},{"p_id":21783,"title":"Us and them: identifying cyber hate on Twitter across multiple protected characteristics","abstract":"\u00a9 2016, Burnap and Williams.Hateful and antagonistic content published and propagated via the World Wide Web has the potential to cause harm and suffering on an individual basis, and lead to social tension and disorder beyond cyber space. Despite new legislation aimed at prosecuting those who misuse new forms of communication to post threatening, harassing, or grossly offensive language - or cyber hate - and the fact large social media companies have committed to protecting their users from harm, it goes largely unpunished due to difficulties in policing online public spaces. To support the automatic detection of cyber hate online, specifically on Twitter, we build multiple individual models to classify cyber hate for a range of protected characteristics including race, disability and sexual orientation. We use text parsing to extract typed dependencies, which represent syntactic and grammatical relationships between words, and are shown to capture \u2018othering\u2019 language - consistently improving machine classification for different types of cyber hate beyond the use of a Bag of Words and known hateful terms. Furthermore, we build a data-driven blended model of cyber hate to improve classification where more than one protected characteristic may be attacked (e.g. race and sexual orientation), contributing to the nascent study of intersectionality in hate crime.","keywords_author":["cyber hate","hate speech","machine learning","NLP","Twitter"],"keywords_other":["Twitter","Offensive languages","Machine classifications","cyber hate","Individual models","Social tensions","Sexual orientations","Automatic Detection"],"max_cite":13.0,"pub_year":2016.0,"sources":"['scp']","rawkeys":["nlp","individual models","machine learning","social tensions","machine classifications","cyber hate","offensive languages","sexual orientations","hate speech","automatic detection","twitter"],"tags":["individual models","machine learning","natural language processing","social tensions","machine classifications","cyber hate","offensive languages","hate speech","automatic detection","sexual orientation","twitter"]},{"p_id":29976,"title":"Actor level emotion magnitude prediction in text and speech","abstract":"The digital universe is expanding at very high rates. New ways of retrieving and enriching text and audio content are required. In this work, a methodology for actor level emotion magnitude prediction in text and speech is proposed. A model is trained to predict emotion magnitudes per actor at any point in a story using previous emotion magnitudes plus current text and speech features which act on the actor's emotional state. The methodology compares linear and non-linear regression techniques to determine the optimal model that fits the data. Results of the analysis show that non-linear regression models based on Support Vector Regression (SVR) using a Radial Basis Function (RBF) kernel provide the most accurate prediction model. An analysis of the contribution of the features for emotion magnitude prediction is performed. \u00a9 Springer Science+Business Media, LLC 2011.","keywords_author":["Affect detection","Artificial intelligence","Machine learning","Multimedia semantic analysis","Natural language processing","Speech processing"],"keywords_other":["Affect detection","Radial Basis Function(RBF)","Support vector regression (SVR)","Emotional state","Accurate prediction","Non-linear regression","NAtural language processing","Multimedia semantics"],"max_cite":4.0,"pub_year":2013.0,"sources":"['scp']","rawkeys":["multimedia semantics","non-linear regression","artificial intelligence","support vector regression (svr)","machine learning","natural language processing","speech processing","multimedia semantic analysis","emotional state","affect detection","radial basis function(rbf)","accurate prediction"],"tags":["multimedia semantics","non-linear regression","support vector regression (svr)","machine learning","natural language processing","speech processing","multimedia semantic analysis","emotional state","affect detection","accurate prediction","radial basis functions"]},{"p_id":21785,"title":"Predicting the impact of scientific concepts using full-text features","abstract":"\u00ef\u00bf\u00bd 2016 ASIS&TNew scientific concepts, interpreted broadly, are continuously introduced in the literature, but relatively few concepts have a long-term impact on society. The identification of such concepts is a challenging prediction task that would help multiple parties\u2014including researchers and the general public\u2014focus their attention within the vast scientific literature. In this paper we present a system that predicts the future impact of a scientific concept, represented as a technical term, based on the information available from recently published research articles. We analyze the usefulness of rich features derived from the full text of the articles through a variety of approaches, including rhetorical sentence analysis, information extraction, and time-series analysis. The results from two large-scale experiments with 3.8 million full-text articles and 48 million metadata records support the conclusion that full-text features are significantly more useful for prediction than metadata-only features and that the most accurate predictions result from combining the metadata and full-text features. Surprisingly, these results hold even when the metadata features are available for a much larger number of documents than are available for the full-text features.","keywords_author":["machine learning","natural language processing","scientometrics"],"keywords_other":["Prediction tasks","Sentence analysis","Long-term impacts","Scientometrics","Scientific literature","Accurate prediction","Large scale experiments","NAtural language processing"],"max_cite":13.0,"pub_year":2016.0,"sources":"['scp']","rawkeys":["sentence analysis","large scale experiments","prediction tasks","natural language processing","machine learning","accurate prediction","scientometrics","scientific literature","long-term impacts"],"tags":["sentence analysis","large scale experiments","prediction tasks","natural language processing","machine learning","accurate prediction","scientometrics","scientific literature","long-term impacts"]},{"p_id":34072,"title":"Anuj@DPIL-FIRE2016: A novel paraphrase detection method in Hindi language using machine learning","abstract":"Every language possesses plausible several interpretations. With the evolution of web, smart devices and social media it has become a challenging task to identify these syntactic or semantic ambiguities. In Natural Language Processing, two statements written using different words having same meaning is termed as paraphrasing. At FIRE 2016, we have worked upon the problem of detecting paraphrases for the given Shared Task DPIL (Detecting Paraphrases in Indian Languages) in Hindi Language specifically. This paper proposed a novel approach to identify if two statements are paraphrased or not using various machine learning algorithms like Random Forest, Support Vector Machine, Gradient Boosting and Gaussian Na\u00efve Bayes on the given training data set of two subtasks. In cross validation experiments, Random Forest leads the other methods in terms of F1-score. The experimental results depicts that our algorithm gives better performance with the ensemble learning method than individual approaches for such classification problem. This can be used in various applications such as question-answering system, document clustering, machine translation, text summarization, plagiarism detection and many more.","keywords_author":["Machine learning","Natural Language Processing","Paraphrase detection","Random forest","Semantic similarity","Soundex"],"keywords_other":["Random forests","Semantic similarity","Question answering systems","Semantic ambiguities","Plagiarism detection","SoundEx","Machine translations","NAtural language processing"],"max_cite":2.0,"pub_year":2016.0,"sources":"['scp']","rawkeys":["semantic ambiguities","natural language processing","machine learning","question answering systems","random forests","semantic similarity","machine translations","plagiarism detection","soundex","random forest","paraphrase detection"],"tags":["semantic ambiguities","natural language processing","machine learning","question answering systems","random forests","semantic similarity","machine translations","plagiarism detection","soundex","paraphrase detection"]},{"p_id":50458,"title":"Automated similarity modeling for real-world applications","abstract":"Copyright \u00a92016 for this paper by its authors. Many Case-Based Reasoning (CBR) applications rely on experts opinions and input to design the knowledge base. Even though these experts are an integral part of the modeling process, they are human and cannot always provide the amounts of information that is needed. To that effect, the idea behind this thesis research is to utilize the data's struc-ture to extract relationships between knowledge entities in cases where expert knowledge is not enough. The goal is to automatically model the similarity measure between cases and their attributes using methods such as information retrieval (IR), natural language processing (NLP), machine learning, graph theory, and social network analysis (SNA), with an emphasis on SNA, to extract contextual knowledge from a dataset.","keywords_author":["Graph theory","Information retrieval","Machine learning","Sensitivity analysis","Similarity","Social network analysis"],"keywords_other":["Contextual knowledge","Modeling process","Similarity measure","Similarity","Expert knowledge","Casebased reasonings (CBR)","Similarity models","NAtural language processing"],"max_cite":0.0,"pub_year":2016.0,"sources":"['scp']","rawkeys":["sensitivity analysis","graph theory","similarity","machine learning","natural language processing","information retrieval","expert knowledge","similarity measure","social network analysis","modeling process","contextual knowledge","similarity models","casebased reasonings (cbr)"],"tags":["sensitivity analysis","graph theory","similarity","machine learning","natural language processing","information retrieval","expert knowledge","similarity measure","social network analysis","modeling process","contextual knowledge","similarity models","casebased reasonings (cbr)"]},{"p_id":34076,"title":"Comparison and synergy between fact-orientation and relation extraction for domain model generation in regulatory compliance","abstract":"\u00a9 Springer International Publishing AG 2016. Modern enterprises need to treat regulatory compliance in a holistic and maximally automated manner, given the stakes and complexity involved. The ability to derive the models of regulations in a given domain from natural language texts is vital in such a treatment. Existing approaches automate regulatory rule extraction with a restricted use of domain models counting on the knowledge and efforts of domain experts. We present a semi-automated treatment of regulatory texts by automating in unison, the key steps in fact-orientation and relation extraction. In addition, we utilize the domain models in learning to identify rules from the text. The key benefit of our approach is that it can be applied to any legal text with a considerably reduced burden on domain experts. Early results are encouraging and pave the way for further explorations.","keywords_author":["Fact-orientation","Machine learning","Natural language processing","Regulatory compliance","Relation extraction","Rule extraction"],"keywords_other":["Relation extraction","Fact orientation","Rule extraction","Natural language text","Domain model","Regulatory rules","NAtural language processing","Domain experts"],"max_cite":2.0,"pub_year":2016.0,"sources":"['scp']","rawkeys":["fact-orientation","domain model","rule extraction","domain experts","machine learning","natural language processing","regulatory compliance","regulatory rules","fact orientation","relation extraction","natural language text"],"tags":["fact-orientation","domain model","rule extraction","domain experts","natural language processing","machine learning","regulatory compliance","regulatory rules","relation extraction","natural language text"]},{"p_id":34077,"title":"Text Analytics for Supporting Stakeholder Opinion Mining for Large-scale Highway Projects","abstract":"\u00a9 2016 The Authors. For large-scale highway projects, late identification of stakeholder concerns often leads to design changes and duplication of effort, which may cause major project delays. This paper proposes a stakeholder opinion mining approach for helping transportation practitioners better identify the types of concerns in the early project stage. The proposed approach includes two major components: (1) stakeholder concern extraction, and (2) stakeholder concern classification. This paper focuses on presenting the proposed methodology and experimental results for stakeholder concern extraction, which extracts the words and phrases that describe stakeholder concerns from stakeholder comments on large-scale highway projects. In developing the proposed stakeholder concern extraction methodology, several supervised machine learning (ML) algorithms were tested and evaluated, and the effect of using a predefined name list as feature was also investigated. All the algorithms were tested on a testing data set of 200 comment sentences, which were selected from a comment collection including 1,849 stakeholder comments on five large-scale highway projects.","keywords_author":["Data analytics","Information extraction","Machine learning","Natural language processing","Opinion mining"],"keywords_other":["Highway projects","Data analytics","Project stages","Text analytics","Supervised machine learning","NAtural language processing","Major project","Opinion mining"],"max_cite":2.0,"pub_year":2016.0,"sources":"['scp']","rawkeys":["text analytics","information extraction","data analytics","highway projects","machine learning","natural language processing","major project","project stages","supervised machine learning","opinion mining"],"tags":["text analytics","information extraction","data analytics","highway projects","machine learning","natural language processing","major project","project stages","supervised machine learning","opinion mining"]},{"p_id":25886,"title":"Real-time statistical speech translation","abstract":"This research investigates the Statistical Machine Translation approaches to translate speech in real time automatically. Such systems can be used in a pipeline with speech recognition and synthesis software in order to produce a real-time voice communication system between foreigners. We obtained three main data sets from spoken proceedings that represent three different types of human speech. TED, Europarl, and OPUS parallel text corpora were used as the basis for training of language models, for developmental tuning and testing of the translation system. We also conducted experiments involving part of speech tagging, compound splitting, linear language model interpolation, TrueCasing and morphosyntactic analysis. We evaluated the effects of variety of data preparations on the translation results using the BLEU, NIST, METEOR and TER metrics and tried to give answer which metric is most suitable for PL-EN language pair. \u00a9 Springer International Publishing Switzerland 2014.","keywords_author":["Knowledge-free learning","Machine learning","Machine translation","NLP","Speech translation"],"keywords_other":["Part of speech tagging","Translation systems","Statistical machine translation","NLP","Real-time voice communications","Knowledge-free learning","Machine translations","Speech translation"],"max_cite":7.0,"pub_year":2014.0,"sources":"['scp']","rawkeys":["real-time voice communications","nlp","speech translation","machine learning","machine translation","part of speech tagging","translation systems","machine translations","knowledge-free learning","statistical machine translation"],"tags":["real-time voice communications","speech translation","natural language processing","machine learning","translation systems","part of speech tagging","machine translations","knowledge-free learning","statistical machine translation"]},{"p_id":29983,"title":"An application of hidden Markov models in subjectivity analysis","abstract":"Hidden Markov models are a powerful statistical tool and have been used in many areas of speech and natural language processing. In this work, we attempt to detect sentence-level subjectivity by means of hidden Markov model which hasn't been thoroughly investigated for subjectivity analysis. Our feature extraction algorithm calculates a feature vector based on the statistical occurrences of words in a corpus without any linguistic knowledge except tokenization. For this reason, this model can be applied to any language; i.e., there is no lexical, grammatical, syntactical analysis used in the classification process. \u00a9 2013 IEEE.","keywords_author":["Hidden Markov models","machine learning","opinion mining","sentiment analysis","subjectivity detection","text mining"],"keywords_other":["Text mining","Feature extraction algorithms","Classification process","Sentiment analysis","Linguistic knowledge","Statistical tools","NAtural language processing","Opinion mining"],"max_cite":4.0,"pub_year":2013.0,"sources":"['scp']","rawkeys":["hidden markov models","text mining","opinion mining","machine learning","natural language processing","subjectivity detection","statistical tools","feature extraction algorithms","classification process","sentiment analysis","linguistic knowledge"],"tags":["hidden markov models","text mining","opinion mining","machine learning","natural language processing","subjectivity detection","statistical tools","feature extraction algorithms","classification process","sentiment analysis","linguistic knowledge"]},{"p_id":42268,"title":"Classifying online Job Advertisements through Machine Learning","abstract":"\u00a9 2018 Elsevier B.V. The rapid growth of Web usage for advertising job positions provides a great opportunity for real-time labour market monitoring. This is the aim of Labour Market Intelligence (LMI), a field that is becoming increasingly relevant to EU Labour Market policies design and evaluation. The analysis of Web job vacancies, indeed, represents a competitive advantage to labour market stakeholders with respect to classical survey-based analyses, as it allows for reducing the time-to-market of the analysis by moving towards a fact-based decision making model. In this paper, we present our approach for automatically classifying million Web job vacancies on a standard taxonomy of occupations. We show how this problem has been expressed in terms of text classification via machine learning. We also show how our approach has been applied to certain real-life projects and we discuss the benefits provided to end users.","keywords_author":["Big data","Machine learning","NLP","Text classification"],"keywords_other":["Decision making models","Competitive advantage","Rapid growth","Labour market","Text classification","Design and evaluations","Time to market","Real time"],"max_cite":0.0,"pub_year":2018.0,"sources":"['scp']","rawkeys":["competitive advantage","nlp","big data","rapid growth","labour market","machine learning","real time","decision making models","design and evaluations","time to market","text classification"],"tags":["competitive advantage","big data","rapid growth","labour market","machine learning","natural language processing","real time","decision making models","design and evaluations","time to market","text classification"]},{"p_id":52505,"title":"Part-of-speech tagging","abstract":"\u00a9 The Author(s) 2012. This chapter presents the application of ETL to language independent part-of-speech (POS) tagging. The POS tagging task consists in assigning a POS or another lexical class marker to each word in a text. We apply ETL and ETL Committee to four different corpora in three different languages: Portuguese, German and English. ETL system achieves state-of-the-art results for the four corpora. The ETL Committee strategy slightly improves the ETL accuracy for all corpora. This chapter is organized as follows. In Sect. 5.1, we describe the task and the selected corpora. In Sect. 5.2, we detail some modeling configurations used in our POS tagger system. In Sect. 5.3, we show some configurations used in the machine learning algorithms. Section 5.4 presents the application of ETL for the Mac-Morpho Corpus. In Sect. 5.5, we describe the application of ETL for the Tycho Brahe Corpus. Section 5.6 presents the application of ETL for the TIGER Corpus. In Sect. 5.7, we show the application of ETL for the Brown Corpus. Finally, Sect. 5.8 presents some concluding remarks.","keywords_author":["Entropy guided transformation learning","ETL committee","Machine learning","Natural language processing","Part-of-speech tagging","Transformation based learning"],"keywords_other":null,"max_cite":0.0,"pub_year":2012.0,"sources":"['scp']","rawkeys":["part-of-speech tagging","natural language processing","machine learning","transformation based learning","entropy guided transformation learning","etl committee"],"tags":["natural language processing","machine learning","transformation based learning","part of speech tagging","entropy guided transformation learning","etl committee"]},{"p_id":52514,"title":"Conclusions","abstract":"\u00a9 The Author(s) 2012. Entropy guided transformation learning is a machine learning algorithm for classification tasks. In this book, we detail how ETL generalizes transformation based learning by solving the TBL bottleneck: the construction of good template sets. ETL relies on the use of the information gain measure to select feature combinations that provide effective template sets. In this work, we also present ETL committee, an ensemble method that uses ETL as the base learner. We describe the application of ETL to four language independent NLP tasks: part-of-speech tagging, phrase chunking, named entity recognition and semantic role labeling. Overall, we successfully apply it to thirteen different corpora in six different languages: Dutch, English, German, Hindi, Portuguese and Spanish. Our extensive experimental results demonstrate that ETL is an effective way to learn accurate transformation rules. In all experiments, ETL shows better results than TBL with hand-crafted templates. Our experimental results also demonstrate that ETL Committee is an effective way to improve the ETL effectiveness. We believe that by avoiding the use of handcrafted templates, ETL enables the use of transformation rules to a greater range of classification tasks.","keywords_author":["Entropy guided transformation learning","ETL committee","Machine learning","Named entity recognition","Natural language processing","Part-of-speech tagging","Phrase chunking","Semantic role labeling","Transformation based learning"],"keywords_other":null,"max_cite":0.0,"pub_year":2012.0,"sources":"['scp']","rawkeys":["phrase chunking","part-of-speech tagging","named entity recognition","natural language processing","machine learning","transformation based learning","entropy guided transformation learning","semantic role labeling","etl committee"],"tags":["phrase chunking","named entity recognition","natural language processing","machine learning","transformation based learning","part of speech tagging","entropy guided transformation learning","semantic role labeling","etl committee"]},{"p_id":32035,"title":"Implementation of ML using na\u00efve bayes algorithm for identifying disease-treatment relation in bio-science text","abstract":"In recent years many successful machine learning applications have been developed, ranging from datamining programs to information-filtering systems that learn users' reading preferences. At the same time, there have been important advances in the theory and algorithms that can be used identify the diseases and treatment relations in a Bio-Science text. Imagine a computer learns from medical records which treatments are most effective for new diseases. Having the machine learning concept behind we have proposed a Machine Learning (ML) approach based on Na\u00efve Bayes (NB) algorithm to improve the automatic disease identification in the medical field. And also we have improved text classification by using an integrated model. \u00a9 Maxwell Scientific Organization, 2013.","keywords_author":["Health care information","Machine learning","Natural language processing"],"keywords_other":["Bio-science","NAtural language processing","Text classification","Medical record","Bayes algorithms","Medical fields","Machine learning applications","Integrated models"],"max_cite":3.0,"pub_year":2013.0,"sources":"['scp']","rawkeys":["bio-science","machine learning applications","medical fields","health care information","bayes algorithms","machine learning","medical record","natural language processing","integrated models","text classification"],"tags":["bio-science","machine learning applications","medical fields","health care information","bayes algorithms","machine learning","medical record","natural language processing","integrated models","text classification"]},{"p_id":46364,"title":"Personal summarization from profile networks","abstract":"\u00a9 2016, Higher Education Press and Springer-Verlag Berlin Heidelberg. Personal profile information on social media like LinkedIn.com and Facebook.com is at the core of many interesting applications, such as talent recommendation and contextual advertising. However, personal profiles usually lack consistent organization confronted with the large amount of available information. Therefore, it is always a challenge for people to quickly find desired information from them. In this paper, we address the task of personal profile summarization by leveraging both textual information and social connection information in social networks from both unsupervised and supervised learning paradigms. Here, using social connection information is motivated by the intuition that people with similar academic, business or social background (e.g., comajor, co-university, and co-corporation) tend to have similar experiences and should have similar summaries. For unsupervised learning, we propose a collective ranking approach, called SocialRank, to combine textual information in an individual profile and social context information from relevant profiles in generating a personal profile summary. For supervised learning, we propose a collective factor graph model, called CoFG, to summarize personal profiles with local textual attribute functions and social connection factors. Extensive evaluation on a large dataset from LinkedIn.com demonstrates the usefulness of social connection information in personal profile summarization and the effectiveness of our proposed unsupervised and supervised learning approaches.","keywords_author":["machine learning","natural language processing","personal profile summarization","social networks"],"keywords_other":["Personal profile","Supervised learning approaches","Social connection","Personal profile information","Textual information","Attribute functions","NAtural language processing","Contextual advertisings"],"max_cite":0.0,"pub_year":2017.0,"sources":"['scp']","rawkeys":["personal profile","natural language processing","machine learning","personal profile information","social connection","social networks","supervised learning approaches","attribute functions","contextual advertisings","textual information","personal profile summarization"],"tags":["natural language processing","machine learning","personal profile information","social connection","social networks","supervised learning approaches","attribute functions","contextual advertisings","personality profiles","textual information","personal profile summarization"]},{"p_id":46366,"title":"Coupling an annotated corpus and a lexicon for Amazigh POS tagging","abstract":"\u00a9 Rinton Press. This paper investigates how to best couple hand-annotated data with information extracted from an external lexical resource to improve part-of-speech tagging performance. Focusing mostly on Amazigh tagging, we introduce a decision tree and Markov model using TreeTagger system. This system gives 92.3 % accuracy on the Amazigh corpus, an error reduction of 15 % (18.45 % on unknown words) over the same tagger without lexical information. We perform a series of experiments that help understanding how this lexical information helps improving tagging accuracy. We also conduct experiments on datasets and lexicons of varying sizes in order to assess the best tradeoff between annotating data versus developing a lexicon. We find that the use of a lexicon improves the quality of the tagger at any stage of development of either resource, and that for fixed performance levels the availability of the full lexicon consistently reduces the need for supervised data.","keywords_author":["Amazigh","Machine learning","NLP","POS tagging","Tagset","Treetagger"],"keywords_other":["Performance level","Part of speech tagging","Treetagger","Lexical resources","Lexical information","Amazigh","Tagset","PoS tagging"],"max_cite":0.0,"pub_year":2017.0,"sources":"['scp']","rawkeys":["lexical information","nlp","amazigh","machine learning","part of speech tagging","lexical resources","treetagger","tagset","performance level","pos tagging"],"tags":["lexical information","amazigh","natural language processing","machine learning","part of speech tagging","lexical resources","treetagger","tagset","performance level","pos tagging"]},{"p_id":27942,"title":"Sentiment classification of spanish reviews: An approach based on feature selection and machine learning methods","abstract":"\u00a9 J.UCS.Sentiment analysis aims to extract users\u2019 opinions from review documents. Nowadays, there are two main approaches for sentiment analysis: the semantic orientation and the machine learning. Sentiment analysis approaches based on Machine Learning (ML) methods work over a set of features extracted from the users\u2019 opinions. However, the high dimensionality of the feature vector reduces the effectiveness of this approach. In this sense, we propose a sentiment classification method based on feature selection mechanisms and ML methods. The present method uses a hybrid feature extraction method based on POS pattern and dependency parsing. The features obtained are enriched semantically through commonsense knowledge bases. Then, a feature selection method is applied to eliminate the noisy and irrelevant features. Finally, a set of classifiers is trained in order to classify unknown data. To prove the effectiveness of our approach, we have conducted an evaluation in the movies and technological products domains. Also, our proposal was compared with well-known methods and algorithms used on the sentiment classification field. Our proposal obtained encouraging results based on the F-measure metric, ranging from 0.786 to 0.898 for the aforementioned domains.","keywords_author":["Feature selection methods","Machine learning","Natural language processing","Opinion mining","Sentiment Analysis"],"keywords_other":null,"max_cite":5.0,"pub_year":2016.0,"sources":"['scp', 'wos']","rawkeys":["feature selection methods","natural language processing","machine learning","opinion mining","sentiment analysis"],"tags":["feature selection methods","natural language processing","machine learning","opinion mining","sentiment analysis"]},{"p_id":25895,"title":"DeepQA Jeopardy! gamification: A machine-learning perspective","abstract":"DeepQA is a large-scale natural language processing (NLP) question-and-answer system that responds across a breadth of structured and unstructured data, from hundreds of analytics that are combined with over 50 models, trained through machine learning. After the 2011 historic milestone of defeating the two best human players in the Jeopardy! game show, the technology behind IBM Watson, DeepQA, is undergoing gamification into real-world business problems. Gamifying a business domain for Watson is a composite of functional, content, and training adaptation for nongame play. During domain gamification for medical, financial, government, or any other business, each system change affects the machine-learning process. As opposed to the original Watson Jeopardy!, whose class distribution of positive-to-negative labels is 1:100, in adaptation the computed training instances, question-and-answer pairs transformed into true-false labels, result in a very low positive-to-negative ratio of 1:100 000. Such initial extreme class imbalance during domain gamification poses a big challenge for the Watson machine-learning pipelines. The combination of ingested corpus sets, question-and-answer pairs, configuration settings, and NLP algorithms contribute toward the challenging data state. We propose several data engineering techniques, such as answer key vetting and expansion, source ingestion, oversampling classes, and question set modifications to increase the computed true labels. In addition, algorithm engineering, such as an implementation of the Newton-Raphson logistic regression with a regularization term, relaxes the constraints of class imbalance during training adaptation. We conclude by empirically demonstrating that data and algorithm engineering are complementary and indispensable to overcome the challenges in this first Watson gamification for real-world business problems. \u00a9 2009-2012 IEEE.","keywords_author":["Gamification","machine learning","natural language processing (NLP)","pattern recognition"],"keywords_other":["Regularization terms","Business problems","Unstructured data","Logistic regressions","Class distributions","Algorithm engineering","NAtural language processing","Gamification"],"max_cite":7.0,"pub_year":2014.0,"sources":"['scp']","rawkeys":["gamification","regularization terms","algorithm engineering","machine learning","class distributions","natural language processing","natural language processing (nlp)","logistic regressions","unstructured data","business problems","pattern recognition"],"tags":["gamification","algorithm-engineering","regularization terms","machine learning","natural language processing","class distributions","logistic regressions","unstructured data","business problems","pattern recognition"]},{"p_id":44325,"title":"Programming challenges of chatbot: Current and future prospective","abstract":"\u00a9 2017 IEEE. In the modern Era of technology, Chatbots is the next big thing in the era of conversational services. Chatbots is a virtual person who can effectively talk to any human being using interactive textual skills. Currently, there are many cloud base Chatbots services which are available for the development and improvement of the chatbot sector such as IBM Watson, Microsoft bot, AWS Lambda, Heroku and many others. A virtual person is based on machine learning and Artificial Intelligence (AI) concepts and due to dynamic nature, there is a drawback in the design and development of these chatbots as they have built-in AI, NLP, programming and conversion services. This paper gives an overview of cloud-based chatbots technologies along with programming of chatbots and challenges of programming in current and future Era of chatbot.","keywords_author":["Artificial Intelligence (AI)","Chatbot","Machine Learning (ML)","NLP"],"keywords_other":["On-machines","Human being","Chatbot","Conversion services","Dynamic nature","Design and Development","Cloud-based","Conversational services"],"max_cite":0.0,"pub_year":2018.0,"sources":"['scp']","rawkeys":["machine learning (ml)","conversion services","nlp","on-machines","cloud-based","chatbot","conversational services","dynamic nature","design and development","artificial intelligence (ai)","human being"],"tags":["on-machines","cloud-based","conversational services","chatbot","machine learning","dynamic nature","natural language processing","design and development","human being"]},{"p_id":44329,"title":"Investigation of recurrent neural networks in the field of sentiment analysis","abstract":"\u00a9 2017 IEEE. Recurrent Neural Networks(RNNs) are popular deep learning architectures used in Natural Language Processing for analyzing sentiments in sentences. The recurrent nature of these networks enable them to use information from previous time steps. In this paper, we analyze the performance of three RNNs namely vanilla RNNs, Long Short-Term Memory(LSTM) and Gated Recurrent Units(GRU). Both unidirectional and bidirectional nature of these networks are considered. Pretrained word vectors from the Google News dataset are used. We evaluate the performance of these networks on the Amazon health product reviews dataset and sentiment analysis benchmark datasets SST-1 and SST-2.","keywords_author":["Deep learning","Natural language processing","Recurrent neural networks","Sentiment analysis"],"keywords_other":["Health products","Bidirectional nature","Learning architectures","Benchmark datasets","Word vectors","Time step","Recurrent neural network (RNNs)"],"max_cite":0.0,"pub_year":2018.0,"sources":"['scp']","rawkeys":["recurrent neural network (rnns)","bidirectional nature","benchmark datasets","learning architectures","health products","deep learning","natural language processing","word vectors","recurrent neural networks","time step","sentiment analysis"],"tags":["bidirectional nature","benchmark datasets","learning architectures","health products","neural networks","machine learning","natural language processing","word vectors","time step","sentiment analysis"]},{"p_id":48420,"title":"A framework for automated rating of online reviews against the underlying topics","abstract":"\u00a9 2017 ACM.Even though the most online review systems offer star rating in addition to free text reviews, this only applies to the overall review. However, different users may have different preferences in relation to different aspects of a product or a service and may struggle to extract relevant information from a massive amount of consumer reviews available online. In this paper, we present a framework for extracting prevalent topics from online reviews and automatically rating them on a 5-star scale. It consists of five modules, including linguistic pre-processing, topic modelling, text classification, sentiment analysis, and rating. Topic modelling is used to extract prevalent topics, which are then used to classify individual sentences against these topics. A state-of-the-art word embedding method is used to measure the sentiment of each sentence. The two types of information associated with each sentence - its topic and sentiment - are combined to aggregate the sentiment associated with each topic. The overall topic sentiment is then projected onto the 5-star rating scale. We use a dataset of Airbnb online reviews to demonstrate a proof of concept. The proposed framework is simple and fully unsupervised. It is also domain independent, and, therefore, applicable to any other domains of products and services.","keywords_author":["Data Mining","Latent Dirichlet Allocation","Machine Learning","Natural Language Processing","Sentiment Analysis","Topic Modelling","Weighted Word Embeddings"],"keywords_other":["Embedding method","Text classification","Sentiment analysis","Products and services","Domain independents","Embeddings","Proof of concept","Latent Dirichlet allocation"],"max_cite":0.0,"pub_year":2017.0,"sources":"['scp']","rawkeys":["embeddings","text classification","latent dirichlet allocation","data mining","weighted word embeddings","machine learning","domain independents","natural language processing","products and services","embedding method","proof of concept","topic modelling","sentiment analysis"],"tags":["embeddings","text classification","data mining","linear discriminant analysis","topic modeling","weighted word embeddings","machine learning","domain independents","natural language processing","products and services","embedding method","proof of concept","sentiment analysis"]},{"p_id":29995,"title":"A supervised named-entity extraction system for medical text","abstract":"We present our participation in Task 1a of the 2013 CLEFeHEALTH Challenge, whose goal was the identification of disorder named entities from electronic medical records. We developed a supervised CRF model that based on a rich set of features learns to predict disorder named entities. The CRF system uses external knowledge from specialized biomedical terminologies and Wikipedia. Our system performance was evaluated at 0.598 F-measure in the context of strict evaluation and 0.711 F-measure in the context of relaxed evaluation.","keywords_author":["Machine learning","Medical records","Named-entity recognition","Natural Language Processing"],"keywords_other":["Named entity extraction","Named entities","Named entity recognition","External knowledge","System use","Medical record","Electronic medical record","NAtural language processing"],"max_cite":4.0,"pub_year":2013.0,"sources":"['scp']","rawkeys":["named-entity recognition","named entity extraction","named entity recognition","machine learning","medical record","natural language processing","external knowledge","electronic medical record","medical records","named entities","system use"],"tags":["named entity extraction","named entity recognition","machine learning","medical record","natural language processing","external knowledge","electronic medical record","named entities","system use"]},{"p_id":27949,"title":"Cloud-Based AI for Pervasive Applications","abstract":"\u00a9 2015 IEEE. This is a brief introduction on how to include sophisticated computer vision, speech recognition, text analytics, and machine learning in pervasive computing applications.","keywords_author":["artificial intelligence","computer vision","intelligent systems","machine learning","pervasive computing","speech recognition","text analytics"],"keywords_other":["Pervasive computing applications","Keyword extraction","Language understanding","Application programmers","Pervasive applications","Text analytics","Language detection","Cloud-based","Semantic text analysis","NAtural language processing"],"max_cite":5.0,"pub_year":2016.0,"sources":"['scp']","rawkeys":["artificial intelligence","language understanding","pervasive applications","text analytics","cloud-based","keyword extraction","machine learning","intelligent systems","natural language processing","pervasive computing applications","speech recognition","semantic text analysis","pervasive computing","language detection","computer vision","application programmers"],"tags":["language understanding","pervasive applications","text analytics","cloud-based","machine learning","keyword extraction","intelligent systems","natural language processing","pervasive computing applications","speech recognition","semantic text analysis","pervasive computing","language detection","computer vision","application programmers"]},{"p_id":25902,"title":"LyS at CLEF RepLab 2014: Creating the state of the art in author influence ranking and reputation classification on Twitter","abstract":"This paper describes our participation at RepLab 2014, a competitive evaluation for reputation monitoring on Twitter. The following tasks were addressed: (1) categorisation of tweets with respect to standard reputation dimensions and (2) characterisation of Twitter profiles, which includes: (2.1) identifying the type of those profiles, such as journalist or investor, and (2.2) ranking the authors according to their level of influence on this social network. We consider an approach based on the application of natural language processing techniques in order to take into account part-of-speech, syntactic and semantic information. However, each task is addressed independently, since they respond to different requirements. The official results confirm the competitiveness of our approaches, which achieve the 2nd place, tied in practice with the 1st place, at the author ranking task; and 3rd place at the reputation dimensions classification tasks.","keywords_author":["Author ranking","Machine learning","Natural language processing","Reputation monitoring","Twitter"],"keywords_other":["Twitter","Classification tasks","State of the art","Part Of Speech","NAtural language processing","Author ranking","Semantic information"],"max_cite":7.0,"pub_year":2014.0,"sources":"['scp']","rawkeys":["author ranking","state of the art","natural language processing","machine learning","classification tasks","reputation monitoring","part of speech","semantic information","twitter"],"tags":["author ranking","state of the art","natural language processing","machine learning","classification tasks","reputation monitoring","part of speech","semantic information","twitter"]},{"p_id":48703,"title":"BITS-Pilani@INLI-FIRE-2017: Indian Native Language Identification using deep learning","abstract":"The task of Native Language Identification involves identifying the prior or first learnt language of a user based on his writing technique and\/or analysis of speech and phonetics in second language. There is a surplus of such data present on social media sites and organised dataset from bodies like Educational Testing Service(ETS), which can be exploited to develop language learning systems and forensic linguistics. In this paper we propose a deep neural network for this task using hierarchical paragraph encoder with attention mechanism to identify relevant features over tendencies and errors a user makes with second language for the INLI task in FIRE 2017. The task involves six Indian languages as prior\/native set and english as the second language which has been collected from user's social media account.","keywords_author":["Deep learning","Machine learning","Native Language Identification","Natural language processing","Neural network"],"keywords_other":["Social media","Second language","Relevant features","Educational testing","Language learning systems","Native language","Indian languages","Attention mechanisms"],"max_cite":0.0,"pub_year":2017.0,"sources":"['scp']","rawkeys":["neural network","relevant features","deep learning","educational testing","social media","attention mechanisms","machine learning","native language identification","natural language processing","indian languages","language learning systems","native language","second language"],"tags":["relevant features","neural networks","educational testing","social media","attention mechanisms","machine learning","native language identification","natural language processing","indian languages","language learning systems","native language","second language"]},{"p_id":1328,"title":"The Technology Behind Personal Digital Assistants: An overview of the system architecture and key components","abstract":"We have long envisioned that one day computers will understand natural language and anticipate what we need, when and where we need it, and proactively complete tasks on our behalf. As computers get smaller and more pervasive, how humans interact with them is becoming a crucial issue. Despite numerous attempts over the past 30 years to make language understanding (LU) an effective and robust natural user interface for computer interaction, success has been limited and scoped to applications that were not particularly central to everyday use. However, speech recognition and machine learning have continued to be refined, and structured data served by applications and content providers has emerged. These advances, along with increased computational power, have broadened the application of natural LU to a wide spectrum of everyday tasks that are central to a user's productivity. We believe that as computers become smaller and more ubiquitous [e.g., wearables and Internet of Things (IoT)], and the number of applications increases, both system-initiated and user-initiated task completion across various applications and web services will become indispensable for personal life management and work productivity. In this article, we give an overview of personal digital assistants (PDAs); describe the system architecture, key components, and technology behind them; and discuss their future potential to fully redefine human?computer interaction.","keywords_author":null,"keywords_other":["notebook computers","Web service","Mobile handsets","speech recognition","Magnetic sensors","Natural language processing","work productivity","natural user interface","machine learning","human-computer interaction","personal digital assistant","mobile computing","Web services","personal life management","user productivity","learning (artificial intelligence)","system-initiated task completion","user-initiated task completion","PDA","human computer interaction","Handheld computers","LU","language understanding","computer interaction"],"max_cite":4.0,"pub_year":2017.0,"sources":"['ieee']","rawkeys":["notebook computers","web services","speech recognition","work productivity","natural user interface","machine learning","human-computer interaction","personal digital assistant","mobile computing","web service","handheld computers","lu","personal life management","user productivity","learning (artificial intelligence)","magnetic sensors","mobile handsets","pda","natural language processing","human computer interaction","system-initiated task completion","user-initiated task completion","language understanding","computer interaction"],"tags":["notebook computers","web services","speech recognition","work productivity","machine learning","human-computer interaction","mobile computing","handheld computers","natural user interfaces","personal life management","user productivity","magnetic sensors","mobile handsets","personal digital assistants","pda","natural language processing","system-initiated task completion","user-initiated task completion","land use","language understanding","computer interaction"]},{"p_id":9520,"title":"Deep learning for sentiment analysis: A survey","abstract":"Deep learning has emerged as a powerful machine learning technique that learns multiple layers of representations or features of the data and produces state-of-the-art prediction results. Along with the success of deep learning in many application domains, deep learning is also used in sentiment analysis in recent years. This paper gives an overview of deep learning and then provides a comprehensive survey of its current applications in sentiment analysis. This article is categorized under:","keywords_author":["data mining","deep learning","machine learning","natural language processing","neural network","opinion mining","sentiment analysis","survey","data mining","deep learning","machine learning","natural language processing","neural network","opinion mining","sentiment analysis","survey"],"keywords_other":["Text mining","Multiple layers","CLASSIFICATION","State of the art","Fundamental concepts","NEURAL-NETWORKS","Machine learning techniques"],"max_cite":0.0,"pub_year":2018.0,"sources":"['ieee', 'scp', 'wos']","rawkeys":["neural-networks","neural network","data mining","text mining","survey","deep learning","machine learning techniques","machine learning","multiple layers","natural language processing","state of the art","fundamental concepts","classification","opinion mining","sentiment analysis"],"tags":["data mining","text mining","survey","neural networks","machine learning techniques","machine learning","multiple layers","natural language processing","state of the art","fundamental concepts","classification","opinion mining","sentiment analysis"]},{"p_id":1326,"title":"Improving Deep Belief Networks via Delta Rule for Sentiment Classification","abstract":"Sentiment classification has received much attention in both engineering and academic fields. Deep belief networks (DBN) has proved powerful in many domains including natural language processing. In this paper, DBN is applied in sentiment classification, while we propose a new way to improve the DBN based on the unsupervised training phase of restricted Boltzmann machines (RBM). That is, the RBM generates the hidden layer in an unsupervised fashion, and then we use this hidden layer as the output of a single-layer neural network, which is trained using the delta rule. The new weights trained from delta rule are then transmitted into the whole back propagation. This way keeps much more correction signal information for each layer in back propagation compared to that in the same network structure. Consequently, our experimental results demonstrate that the new learning method performs relatively better on ten sentiment datasets, which further proves the delta rule improves DBN performance for natural language processing tasks.","keywords_author":["Deep belief networks","Restricted Boltzmann machines","Single-layer network","Delta rule","Sentiment classification"],"keywords_other":["Biological neural networks","RBM","Periodic structures","sentiment analysis","delta rule","Twitter","sentiment classification","single-layer neural network","Training","Learning systems","back propagation","unsupervised learning","DBN","Neurons","restricted Boltzmann machines","natural language processing","Niobium","Boltzmann machines","deep belief networks","backpropagation"],"max_cite":null,"pub_year":2016.0,"sources":"['ieee']","rawkeys":["single-layer network","learning systems","sentiment analysis","rbm","dbn","delta rule","biological neural networks","neurons","sentiment classification","single-layer neural network","boltzmann machines","training","back propagation","unsupervised learning","restricted boltzmann machines","natural language processing","niobium","deep belief networks","periodic structures","backpropagation","twitter"],"tags":["sentiment classification","single-layer network","restricted boltzmann machine","single-layer neural network","boltzmann machines","training","natural language processing","niobium","delta rule","unsupervised learning","sentiment analysis","learning systems","biological neural networks","neurons","deep belief networks","periodic structures","backpropagation","twitter"]},{"p_id":27955,"title":"Using machine learning to identify major shifts in human gut microbiome protein family abundance in disease","abstract":"\u00a9 2016 IEEE.Inflammatory Bowel Disease (IBD) is an autoimmune condition that is observed to be associated with major alterations in the gut microbiome taxonomic composition. Here we classify major changes in microbiome protein family abundances between healthy subjects and IBD patients. We use machine learning to analyze results obtained previously from computing relative abundance of \u223c10,000 KEGG orthologous protein families in the gut microbiome of a set of healthy individuals and IBD patients. We develop a machine learning pipeline, involving the Kolomogorv-Smirnov test, to identify the 100 most statistically significant entries in the KEGG database. Then we use these 100 as a training set for a Random Forest classifier to determine \u223c5% the KEGGs which are best at separating disease and healthy states. Lastly, we developed a Natural Language Processing classifier of the KEGG description files to predict KEGG relative over-or under-abundance. As we expand our analysis from 10,000 KEGG protein families to one million proteins identified in the gut microbiome, scalable methods for quickly identifying such anomalies between health and disease states will be increasingly valuable for biological interpretation of sequence data.","keywords_author":["ibd","kegg","machine learning","microbiome","pca","random forest"],"keywords_other":["Autoimmune conditions","Random forests","Biological interpretation","Microbiome","kegg","Random forest classifier","Inflammatory bowel disease","NAtural language processing"],"max_cite":5.0,"pub_year":2016.0,"sources":"['scp']","rawkeys":["ibd","inflammatory bowel disease","machine learning","natural language processing","random forest classifier","random forests","kegg","autoimmune conditions","microbiome","biological interpretation","pca","random forest"],"tags":["principal component analysis","inflammatory bowel disease","machine learning","natural language processing","random forests","random forest classifier","kegg","autoimmune conditions","microbiome","biological interpretation"]},{"p_id":52532,"title":"Text classification in natural language using Wikipedia Clasificaci\u00f3n de textos en lenguaje natural usando la Wikipedia","abstract":"Automatic Text Classifiers are needed in environments where the amount of data to handle is so high that human classification would be ineffective. In our study, the proposed classifier takes advantage of the Wikipedia to generate the corpus defining each category. The text is then analyzed syntactically using Natural Language Processing software. The proposed classifier is highly accurate and outperforms Machine Learning trained classifiers.","keywords_author":["Machine learning","Natural language processing","Text categorization","Tf-idf","Wikipedia"],"keywords_other":["Highly accurate","Text categorization","Text classification","Tf-idf","Wikipedia","Natural languages","Text classifiers"],"max_cite":0.0,"pub_year":2011.0,"sources":"['scp']","rawkeys":["highly accurate","tf-idf","natural languages","natural language processing","machine learning","wikipedia","text categorization","text classification","text classifiers"],"tags":["highly accurate","tf-idf","natural languages","natural language processing","machine learning","wikipedia","text categorization","text classification","text classifiers"]},{"p_id":1329,"title":"Cascading Training for Relaxation CNN on Handwritten Character Recognition","abstract":"With the development of deep learning, many difficult recognition problems can be solved by deep learning models. For handwritten character recognition, the CNN is used the most. In order to improve the performance of CNN, many new models have been proposed and in which the relaxation CNN [35] is widely used. The relaxation CNN has more complicated structure than CNN while the recognition time is the same with which. However, the training of relaxation CNN needs much more time than CNN. In this paper, we propose the cascading training for relaxation CNN. Our method can train a relaxation CNN of better performance while using almost the same training time with normal CNN. The experimental results proved that the relaxation CNN trained by cascading training is able to achieve the state-of-the-art performance on handwritten Chinese character recognition.","keywords_author":["cascading training","character recognition","CNN"],"keywords_other":["Character recognition","recognition time","relaxation CNN training","learning (artificial intelligence)","Neurons","Distortion","handwritten Chinese character recognition","feedforward neural nets","Training","natural language processing","Machine learning","Kernel","handwritten character recognition","training time","deep learning models","Stochastic processes","relaxation CNN"],"max_cite":null,"pub_year":2016.0,"sources":"['ieee']","rawkeys":["cascading training","kernel","relaxation cnn training","handwritten character recognition","recognition time","machine learning","training time","neurons","feedforward neural nets","learning (artificial intelligence)","relaxation cnn","stochastic processes","training","handwritten chinese character recognition","distortion","cnn","character recognition","deep learning models","natural language processing"],"tags":["recognition time","cascading training","deep learning model","relaxation cnn","deformation","stochastic processes","machine learning","handwritten chinese character recognition","natural language processing","training","character recognition","kernel","convolutional neural network","training time","relaxation cnn training","handwritten character recognition","neurons","feedforward neural nets"]},{"p_id":21814,"title":"Predicting judicial decisions of the European court of human rights: A natural language processing perspective","abstract":"\u00a9 2016 Aletras et al.Recent advances in Natural Language Processing and Machine Learning provide us with the tools to build predictive models that can be used to unveil patterns driving judicial decisions. This can be useful, for both lawyers and judges, as an assisting tool to rapidly identify cases and extract patterns which lead to certain decisions. This paper presents the first systematic study on predicting the outcome of cases tried by the European Court of Human Rights based solely on textual content. We formulate a binary classification task where the input of our classifiers is the textual content extracted from a case and the target output is the actual judgment as to whether there has been a violation of an article of the convention of human rights. Textual information is represented using contiguous word sequences, i.e., N-grams, and topics. Our models can predict the court's decisions with a strong accuracy (79% on average). Our empirical analysis indicates that the formal facts of a case are the most important predictive factor. This is consistent with the theory of legal realism suggesting that judicial decision-making is significantly affected by the stimulus of the facts. We also observe that the topical content of a case is another important feature in this classification task and explore this relationship further by conducting a qualitative analysis.","keywords_author":["Artificial intelligence","Judicial decisions","Legal science","Machine learning","Natural language processing","Text mining"],"keywords_other":null,"max_cite":13.0,"pub_year":2016.0,"sources":"['scp']","rawkeys":["artificial intelligence","text mining","natural language processing","legal science","judicial decisions","machine learning"],"tags":["text mining","natural language processing","machine learning","judicial decisions","legal science"]},{"p_id":38199,"title":"Connecting Devices to Cookies via Filtering, Feature Engineering, and Boosting","abstract":"\u00a9 2015 IEEE.We present a supervised machine learning system capable of matching internet devices to web cookies through filtering, feature engineering, binary classification, and post processing. The system builds a reasonably sized training and testing data set through filtering and feature engineering. We build 415 features in total. Some of these features were engineered to be O(n) time, stand alone classifiers for this problem. Other features use various natural language processing (NLP) techniques. Meta features are created by ridge regression and Adaboost. Then binary classification through two different gradient boosting (XGBoost with logarithmic loss) models is performed. A post processing pipeline connects devices and cookies in a way that maximizes F0.5 score. Our machine learning system obtained a private F0.5 score of 0.849562 for a final rank of 12th\/340 on the ICDM 2015: Drawbridge Cross-Device Connections challenge.","keywords_author":["binary classification","cookies","feature engineering","Filtering","machine learning"],"keywords_other":["Binary classification","cookies","Training and testing","Gradient boosting","Feature engineerings","Internet devices","Supervised machine learning","NAtural language processing"],"max_cite":1.0,"pub_year":2016.0,"sources":"['scp']","rawkeys":["filtering","feature engineering","machine learning","natural language processing","cookies","binary classification","training and testing","feature engineerings","gradient boosting","internet devices","supervised machine learning"],"tags":["machine learning","natural language processing","cookies","binary classification","training and testing","feature engineerings","gradient boosting","internet devices","filter","supervised machine learning"]},{"p_id":44343,"title":"A machine Learning approach for sentiment analysis in the standard or dialectal Arabic Facebook comments","abstract":"\u00a9 2017 IEEE. Social networks like Facebook contain an enormous amount of data, called Big Data. Extracting valuable information and trends from these data allows a better understanding and decision-making. In general, there are two categories of approaches to address this problem: Machine Learning approaches and lexicon based approaches. This work deals with the sentiment analysis for Facebook's comments written and shared in Arabic language (Modern Standard or Dialectal) from a Machine Learning perspective. The process starts by collecting and preparing the Arabic Facebook comments. Then, several combinations of extraction (n-grams) and weighting schemes (TF \/ TF-IDF) for features construction are conducted to ensure the highest performance of the developed classification models. In addition, to reduce the dimensionality and improve the classification performance, a features selection method is applied. Three supervised classification algorithms have been used: Naive Bayes, Random Forests and Support Vectors Machines using R software. Our Machine Learning approach using sentiment analysis was implemented with the purpose of analyzing the Facebook comments, written in Modern Standard Arabic or in Moroccan Dialectal Arabic, on the Morocco's Legislative Elections of 2016. The results obtained are promising and encourage us to continue working on this subject.","keywords_author":["Feature construction","Feature selection","Machine learning approach","Modern Standard Arabic","Moroccan Dialectal","Natural Language Processing","Sentiment Analysis"],"keywords_other":["Supervised classification","Classification performance","Moroccan Dialectal","Feature construction","Machine learning approaches","Support vectors machine","Features constructions","Modern standards"],"max_cite":0.0,"pub_year":2018.0,"sources":"['scp']","rawkeys":["moroccan dialectal","supervised classification","machine learning approach","classification performance","feature construction","modern standards","natural language processing","support vectors machine","feature selection","modern standard arabic","sentiment analysis","machine learning approaches","features constructions"],"tags":["moroccan dialectal","supervised classification","classification performance","feature construction","modern standards","natural language processing","machine learning","feature selection","modern standard arabic","sentiment analysis","machine learning approaches"]},{"p_id":50492,"title":"Bayesian Analysis in Natural Language Processing","abstract":"\u00a9 2016 by Morgan & Claypool. Natural language processing (NLP) went through a profound transformation in the mid-1980s when it shifted to make heavy use of corpora and data-driven techniques to analyze language. Since then, the use of statistical techniques in NLP has evolved in several ways. One such example of evolution took place in the late 1990s or early 2000s, when full-fledged Bayesian machinery was introduced to NLP. This Bayesian approach to NLP has come to accommodate for various shortcomings in the frequentist approach and to enrich it, especially in the unsupervised setting, where statistical learning is done without target prediction examples. We cover the methods and algorithms that are needed to fluently read Bayesian learning papers in NLP and to do research in the area. These methods and algorithms are partially borrowed from both machine learning and statistics and are partially developed \"in-house\" in NLP. We cover inference techniques such as Markov chain Monte Carlo sampling and variational inference, Bayesian estimation, and nonparametric modeling. We also cover fundamental concepts in Bayesian statistics such as prior distributions, conjugacy, and generative modeling. Finally, we cover some of the fundamental modeling techniques in NLP, such as grammar modeling and their use with Bayesian analysis.","keywords_author":["Bayesian NLP","Bayesian statistics","computational linguistics","grammar modeling in NLP","inference in NLP","machine learning","natural language processing","statistical learning","unsupervised learning"],"keywords_other":["Statistical learning","inference in NLP","Bayesian","Bayesian statistics","grammar modeling in NLP"],"max_cite":0.0,"pub_year":2016.0,"sources":"['scp']","rawkeys":["inference in nlp","machine learning","natural language processing","bayesian nlp","unsupervised learning","computational linguistics","statistical learning","bayesian statistics","grammar modeling in nlp","bayesian"],"tags":["inference in nlp","machine learning","natural language processing","bayesian nlp","unsupervised learning","computational linguistics","statistical learning","bayesian statistics","grammar modeling in nlp","bayesian"]},{"p_id":25917,"title":"Implementation of evolutionary algorithms for deep architectures","abstract":"\u00a9 2014, International Workshop on Artificial Intelligence and Cognition.Deep learning is becoming an increasingly interesting and powerful machine learning method with successful applications in many domains, such as natural language processing, image recognition, and hand-written character recognition. Despite of its eminent success, limitations of traditional learning approach may still prevent deep learning from achieving a wide range of realistic learning tasks. Due to the flexibility and proven effectiveness of evolutionary learning techniques, they may therefore play a crucial role towards unleashing the full potential of deep learning in practice. Unfortunately, many researchers with a strong background on evolutionary computation are not fully aware of the stateof-the-art research on deep learning. To close this knowledge gap and to promote the research on evolutionary inspired deep learning techniques, this paper presents a comprehensive review of the latest deep architectures and surveys important evolutionary algorithms that can potentially be explored for training these deep architectures.","keywords_author":["Deep architectures","Deep learning","Evolutionary algorithms"],"keywords_other":["Deep learning","Deep architectures","Learning tasks","Machine learning methods","Evolutionary Learning","Hand-written characters","Traditional learning","NAtural language processing"],"max_cite":7.0,"pub_year":2014.0,"sources":"['scp', 'ieee']","rawkeys":["machine learning methods","hand-written characters","deep learning","learning tasks","deep architectures","natural language processing","traditional learning","evolutionary algorithms","evolutionary learning"],"tags":["machine learning methods","hand-written characters","learning tasks","deep architectures","machine learning","natural language processing","traditional learning","evolutionary algorithms","evolutionary learning"]},{"p_id":21823,"title":"A study of active learning methods for named entity recognition in clinical text","abstract":"\u00a9 2015. Objectives: Named entity recognition (NER), a sequential labeling task, is one of the fundamental tasks for building clinical natural language processing (NLP) systems. Machine learning (ML) based approaches can achieve good performance, but they often require large amounts of annotated samples, which are expensive to build due to the requirement of domain experts in annotation. Active learning (AL), a sample selection approach integrated with supervised ML, aims to minimize the annotation cost while maximizing the performance of ML-based models. In this study, our goal was to develop and evaluate both existing and new AL methods for a clinical NER task to identify concepts of medical problems, treatments, and lab tests from the clinical notes. Methods: Using the annotated NER corpus from the 2010 i2b2\/VA NLP challenge that contained 349 clinical documents with 20,423 unique sentences, we simulated AL experiments using a number of existing and novel algorithms in three different categories including uncertainty-based, diversity-based, and baseline sampling strategies. They were compared with the passive learning that uses random sampling. Learning curves that plot performance of the NER model against the estimated annotation cost (based on number of sentences or words in the training set) were generated to evaluate different active learning and the passive learning methods and the area under the learning curve (ALC) score was computed. Results: Based on the learning curves of F-measure vs. number of sentences, uncertainty sampling algorithms outperformed all other methods in ALC. Most diversity-based methods also performed better than random sampling in ALC. To achieve an F-measure of 0.80, the best method based on uncertainty sampling could save 66% annotations in sentences, as compared to random sampling. For the learning curves of F-measure vs. number of words, uncertainty sampling methods again outperformed all other methods in ALC. To achieve 0.80 in F-measure, in comparison to random sampling, the best uncertainty based method saved 42% annotations in words. But the best diversity based method reduced only 7% annotation effort. Conclusion: In the simulated setting, AL methods, particularly uncertainty-sampling based approaches, seemed to significantly save annotation cost for the clinical NER task. The actual benefit of active learning in clinical NER should be further evaluated in a real-time setting.","keywords_author":["Active learning","Clinical named entity recognition","Clinical natural language processing","Machine learning"],"keywords_other":["Active learning methods","Uncertainty samplings","Learning","Named entity recognition","Humans","Natural Language Processing","Real-time settings","Machine Learning","Passive learning","Active Learning","NAtural language processing","Sampling strategies"],"max_cite":13.0,"pub_year":2015.0,"sources":"['scp']","rawkeys":["real-time settings","active learning methods","named entity recognition","learning","active learning","humans","clinical natural language processing","machine learning","natural language processing","passive learning","uncertainty samplings","sampling strategies","clinical named entity recognition"],"tags":["real-time settings","active learning methods","named entity recognition","machine learning","natural language processing","humans","clinical natural language processing","passive learning","uncertainty samplings","sampling strategies","clinical named entity recognition"]},{"p_id":52545,"title":"Taxonomy and evaluation of markers for computational stylistics","abstract":"Currently, stylistic analysis of natural language texts is achieved through a wide variety of techniques containing many different algorithms, feature sets and collection methods. Most machine-learning methods rely on feature extraction to model the text and perform classification. But what are the best features for making style based distinctions? While many researchers have developed particular collections of style features-called style markers - no definitive list exists. In this paper we present an organized collection of such style markers with performance data on a diverse set of texts. We show that for each training document, one or more markers exist that can distinguish it from others, providing a basis for a weighted, combined set of markers that outperform any of the individual ones. We examine and categorize 502 style markers, both individually and as a set, and evaluate their performance on several English language text collections.","keywords_author":["Artificial intelligence","Computational linguistics","Computational stylistics","Machine learning","Natural language processing","Style processing"],"keywords_other":["Collection methods","English languages","Machine-learning","Computational stylistics","Natural language text","Style markers","Training documents","Text collection","Performance data","NAtural language processing","Feature sets"],"max_cite":0.0,"pub_year":2011.0,"sources":"['scp']","rawkeys":["artificial intelligence","computational stylistics","natural language text","style processing","machine learning","natural language processing","performance data","text collection","collection methods","english languages","computational linguistics","training documents","machine-learning","style markers","feature sets"],"tags":["computational stylistics","natural language text","style processing","machine learning","natural language processing","performance data","text collection","collection methods","english languages","computational linguistics","training documents","style markers","feature sets"]},{"p_id":30022,"title":"Using syntactic dependencies and WordNet classes for noun event recognition","abstract":"The goal of this research is to devise a method for recognizing TimeML noun events in a more effective way. TimeML is the most recent an-notation scheme for processing the event and temporal expressions in natural language processing fields. In this paper, we argue and demonstrate that the de-pendencies and the deep-level WordNet classes are useful for recognizing events. We formulate the event recognition problem as a classification task us-ing various features including lexical semantic and dependency-based features. The experimental results show that our proposed method outperforms signifi-cantly a state-of-the-art approach. Our analysis of the results demonstrates that the dependencies of direct object and the deep-level WordNet hypernyms play pivotal roles for recognizing noun events.","keywords_author":["Event recognition","Machine LEARNING","Natural language processing","TimeBank","TimeML","WordNet"],"keywords_other":["Event recognition","TimeML","TimeBank","Wordnet","NAtural language processing"],"max_cite":4.0,"pub_year":2012.0,"sources":"['scp']","rawkeys":["event recognition","wordnet","timeml","natural language processing","machine learning","timebank"],"tags":["event recognition","wordnet","timeml","natural language processing","machine learning","timebank"]},{"p_id":87366,"title":"Development of an automated phenotyping algorithm for hepatorenal syndrome","abstract":"Objective: Hepatorenal Syndrome (HRS) is a devastating form of acute kidney injury (MU) in advanced liver disease patients with high morbidity and mortality, but phenotyping algorithms have not yet been developed using large electronic health record (EHR) databases. We evaluated and compared multiple phenotyping methods to achieve an accurate algorithm for HRS identification.","keywords_author":["Cirrhosis","Phenotyping","Hepatorenal syndrome","Acute kidney injury","Dimension reduction","Natural language processing"],"keywords_other":["HIGH-THROUGHPUT","CIRRHOSIS","FRAMEWORK","ASCITES","INTRAHEPATIC PORTOSYSTEMIC SHUNT","LIVER-FAILURE","BETA-BLOCKERS","TERLIPRESSIN PLUS ALBUMIN","ELECTRONIC HEALTH RECORDS","UNITED-STATES"],"max_cite":0.0,"pub_year":2018.0,"sources":"['wos']","rawkeys":["ascites","beta-blockers","phenotyping","hepatorenal syndrome","framework","acute kidney injury","natural language processing","electronic health records","terlipressin plus albumin","liver-failure","cirrhosis","united-states","high-throughput","dimension reduction","intrahepatic portosystemic shunt"],"tags":["ascites","beta-blockers","hepatorenal syndrome","framework","acute kidney injury","natural language processing","electronic health records","terlipressin plus albumin","liver-failure","cirrhosis","united-states","intrahepatic portosystemic shunt","phenotype","high-throughput","dimensionality reduction"]},{"p_id":52552,"title":"The third personal pronoun anaphora resolution in texts from narrow subject domains with grammatical errors and mistypings","abstract":"The third personal pronoun anaphora resolution in texts from the Internet sources (forum comments, opinions) with a given subject domain (cars, household appliances etc) is being discussed. A concrete solution to the task is offered. High precision with acceptable recall (and vice versa) is shown by an example of opinions about mobile phones.","keywords_author":["Anaphora resolution","Computational linguistics","Machine learning","Natural language processing","Opinion mining"],"keywords_other":["Grammatical errors","Internet sources","Anaphora resolution","NAtural language processing","Opinion mining"],"max_cite":0.0,"pub_year":2011.0,"sources":"['scp']","rawkeys":["opinion mining","machine learning","natural language processing","computational linguistics","anaphora resolution","grammatical errors","internet sources"],"tags":["opinion mining","machine learning","natural language processing","computational linguistics","anaphora resolution","grammatical errors","internet sources"]},{"p_id":27980,"title":"Twitter opinion mining for adverse drug reactions","abstract":"\u00a9 2015 IEEE. Although rigorous clinical studies are required before a drug is placed on the market, it is impossible to predict all side effects for the approved medication. The United States Food and Drug Administration actively monitors approved drugs to identify adverse events. The FDA Adverse Event Reporting System (FAERS) contains a database of adverse drug events reported by the healthcare providers and consumers. The ubiquitous online social networks, such as Twitter, can provide complementary information about adverse drug events. Short Twitter postings, or tweets, are often used to express an opinion about drugs, as well as solicit and receive feedback from consumers of a drug. Thus, adverse drug events can be discovered by extracting from tweets users' opinions about drugs. Here, we developed a computational pipeline for collecting, processing, and analyzing tweets to find signals about adverse drug reactions, defined as drug side effects caused by a drug at a normal dose during normal use. Manual examination of processed tweets identified several known side effects of four drugs.","keywords_author":["Adverse Drug Reactions","Machine Learning","Nature Language Processing","Opinion Mining","Sentiment Analysis"],"keywords_other":["On-line social networks","Sentiment analysis","Nature language processing","Health care providers","Manual examination","United states food and drug administrations","Adverse drug reactions","Opinion mining"],"max_cite":5.0,"pub_year":2015.0,"sources":"['scp']","rawkeys":["on-line social networks","united states food and drug administrations","manual examination","machine learning","nature language processing","health care providers","adverse drug reactions","opinion mining","sentiment analysis"],"tags":["on-line social networks","united states food and drug administrations","manual examination","natural language processing","machine learning","health care providers","adverse drug reactions","opinion mining","sentiment analysis"]},{"p_id":25934,"title":"Convolution neural network for relation extraction","abstract":"Deep Neural Network has been applied to many Natural Language Processing tasks. Instead of building hand-craft features, DNN builds features by automatic learning, fitting different domains well. In this paper, we propose a novel convolution network, incorporating lexical features, applied to Relation Extraction. Since many current deep neural networks use word embedding by word table, which, however, neglects semantic meaning among words, we import a new coding method, which coding input words by synonym dictionary to integrate semantic knowledge into the neural network. We compared our Convolution Neural Network (CNN) on relation extraction with the state-of-art tree kernel approach, including Typed Dependency Path Kernel and Shortest Dependency Path Kernel and Context-Sensitive tree kernel, resulting in a 9% improvement competitive performance on ACE2005 data set. Also, we compared the synonym coding with the one-hot coding, and our approach got 1.6% improvement. Moreover, we also tried other coding method, such as hypernym coding, and give some discussion according the result. \u00a9 2013 Springer-Verlag.","keywords_author":["Convolution Network","Deep Learning","Relation Extraction","Word Embedding"],"keywords_other":["Deep learning","Relation extraction","Automatic-learning","Competitive performance","Convolution neural network","Word Embedding","Deep neural networks","NAtural language processing"],"max_cite":7.0,"pub_year":2013.0,"sources":"['scp']","rawkeys":["competitive performance","convolution neural network","deep learning","deep neural networks","natural language processing","automatic-learning","word embedding","convolution network","relation extraction"],"tags":["competitive performance","natural language processing","machine learning","automatic-learning","word embedding","convolutional neural network","relation extraction"]},{"p_id":27987,"title":"Comparison of machine learning classifiers for influenza detection from emergency department free-text reports","abstract":"\u00a9 2015 Elsevier Inc. Influenza is a yearly recurrent disease that has the potential to become a pandemic. An effective biosurveillance system is required for early detection of the disease. In our previous studies, we have shown that electronic Emergency Department (ED) free-text reports can be of value to improve influenza detection in real time. This paper studies seven machine learning (ML) classifiers for influenza detection, compares their diagnostic capabilities against an expert-built influenza Bayesian classifier, and evaluates different ways of handling missing clinical information from the free-text reports. We identified 31,268 ED reports from 4 hospitals between 2008 and 2011 to form two different datasets: training (468 cases, 29,004 controls), and test (176 cases and 1620 controls). We employed Topaz, a natural language processing (NLP) tool, to extract influenza-related findings and to encode them into one of three values: Acute, Non-acute, and Missing. Results show that all ML classifiers had areas under ROCs (AUC) ranging from 0.88 to 0.93, and performed significantly better than the expert-built Bayesian model. Missing clinical information marked as a value of missing (not missing at random) had a consistently improved performance among 3 (out of 4) ML classifiers when it was compared with the configuration of not assigning a value of missing (missing completely at random). The case\/control ratios did not affect the classification performance given the large number of training cases. Our study demonstrates ED reports in conjunction with the use of ML and NLP with the handling of missing value information have a great potential for the detection of infectious diseases.","keywords_author":["Bayesian","Case detection","Emergency department reports","Influenza","Machine learning"],"keywords_other":["Classification performance","Influenza, Human","Humans","Emergency departments","Bayesian classifier","Bayesian","Clinical information","Machine Learning","Diagnostic capabilities","Emergency Service, Hospital","NAtural language processing","Influenza"],"max_cite":5.0,"pub_year":2015.0,"sources":"['scp']","rawkeys":["diagnostic capabilities","hospital","classification performance","clinical information","human","machine learning","bayesian classifier","emergency service","humans","natural language processing","emergency department reports","influenza","case detection","bayesian","emergency departments"],"tags":["diagnostic capabilities","classification performance","clinical information","machine learning","bayesian classifier","emergency service","humans","natural language processing","emergency department reports","influenza","case detection","hospitals","bayesian","emergency departments"]},{"p_id":27988,"title":"An automatic system to identify heart disease risk factors in clinical texts over time","abstract":"\u00a9 2015 Elsevier Inc. Despite recent progress in prediction and prevention, heart disease remains a leading cause of death. One preliminary step in heart disease prediction and prevention is risk factor identification. Many studies have been proposed to identify risk factors associated with heart disease; however, none have attempted to identify all risk factors. In 2014, the National Center of Informatics for Integrating Biology and Beside (i2b2) issued a clinical natural language processing (NLP) challenge that involved a track (track 2) for identifying heart disease risk factors in clinical texts over time. This track aimed to identify medically relevant information related to heart disease risk and track the progression over sets of longitudinal patient medical records. Identification of tags and attributes associated with disease presence and progression, risk factors, and medications in patient medical history were required. Our participation led to development of a hybrid pipeline system based on both machine learning-based and rule-based approaches. Evaluation using the challenge corpus revealed that our system achieved an F1-score of 92.68%, making it the top-ranked system (without additional annotations) of the 2014 i2b2 clinical NLP challenge.","keywords_author":["Clinical information extraction","Heart disease","Machine learning","Risk factor identification"],"keywords_other":["Humans","Heart disease","Vocabulary, Controlled","Medical history","Risk factors","China","Aged","Pattern Recognition, Automated","Pipe-line systems","Rule-based approach","Female","Automatic systems","Cohort Studies","Cardiovascular Diseases","Recent progress","Comorbidity","Diabetes Complications","Narration","Longitudinal Studies","Incidence","NAtural language processing","Confidentiality","Male","Risk Assessment","Electronic Health Records","Computer Security","Middle Aged","Natural Language Processing","Data Mining"],"max_cite":5.0,"pub_year":2015.0,"sources":"['scp']","rawkeys":["vocabulary","heart disease","automated","computer security","automatic systems","aged","medical history","rule-based approach","comorbidity","machine learning","electronic health records","middle aged","risk assessment","diabetes complications","narration","cohort studies","data mining","incidence","humans","confidentiality","risk factor identification","cardiovascular diseases","controlled","pipe-line systems","china","risk factors","longitudinal studies","male","recent progress","natural language processing","pattern recognition","female","clinical information extraction"],"tags":["vocabulary","heart disease","automated","computer security","automatic systems","aged","medical history","rule-based approach","comorbidity","control","electronic health records","machine learning","middle aged","risk assessment","cardiovascular disease","diabetes complications","narration","cohort studies","data mining","incidence","humans","confidentiality","risk factor identification","pipe-line systems","china","risk factors","longitudinal studies","male","recent progress","natural language processing","pattern recognition","female","clinical information extraction"]},{"p_id":50517,"title":"Development of a machine learning framework for biomedical text mining","abstract":"\u00a9 Springer International Publishing Switzerland 2016.Biomedical text mining (BTM) aims to create methods for searching and structuring knowledge extracted from biomedical literature. Named entity recognition (NER), a BTM task, seeks to identify mentions to biological entities in texts. Dictionaries, regular expressions, natural language processing and machine learning (ML) algorithms are used in this task. Over the last years, @Note2, an open-source software framework, which includes user-friendly interfaces for important tasks in BTM, has been developed, but it did not include ML-based methods. In this work, the development of a framework, BioTML, including a number of ML-based approaches for NER is proposed, to fill the gap between @Note2 and state-of-the-art ML approaches. BioTML was integrated in @Note2 as a novel plug-in, where Hidden Markov Models, Conditional Random Fields and Support Vector Machines were implemented to address NER tasks, working with a set of over 60 feature types used to train ML models. The implementation was supported in open-source software, such as MALLET, LibSVM, ClearNLP or OpenNLP. Several manually annotated corpora were used in the validation of BioTML. The results are promising, while there is room for improvement.","keywords_author":["Biomedical text mining","Machine learning","Named entity recognition"],"keywords_other":["Biomedical literature","Named entity recognition","User friendly interface","Regular expressions","Biomedical text minings","Structuring knowledge","Conditional random field","NAtural language processing"],"max_cite":0.0,"pub_year":2016.0,"sources":"['scp']","rawkeys":["conditional random field","named entity recognition","machine learning","natural language processing","biomedical text mining","user friendly interface","biomedical text minings","structuring knowledge","regular expressions","biomedical literature"],"tags":["conditional random field","named entity recognition","machine learning","natural language processing","structural knowledge","biomedical text minings","user friendly interface","regular expressions","biomedical literature"]},{"p_id":50520,"title":"Opinion mining: Analysis of comments written in Arabic colloquial","abstract":"In Arab nations, people used to express their opinions using colloquial dialects depending on the country to which they belong to. Analyzing reviews written in various Arabic dialects is a challenging problem. This is because some words could have different meanings in various dialects. Furthermore, dialects could contain words that do not belong to classical Arabic language. This research tackles the problem of sentiment analysis of reviews and comments written in colloquial dialects of Arabic language, at which the ability of different machine learning algorithms and features are examined in polarity determination. In this work, people's reviews (written in different dialects) are classified into positive or negative opinions. Each dialect comes with its own stop-words list. Consequently, a list of stop-words that suits different dialects in addition to modern standard Arabic (MSA) is suggested. In this paper, a light stemmer that suits dialects is developed. Two feature sets are utilized (bag of words (BoW), and N-gram of words) to investigate their effectiveness in sentiment analysis. Finally, Na\u00efve-Bayes, Support vector machine (SVM), and Maximum Entropy machine learning algorithms are applied to study their performance in opinion mining. Fl-measure is used to evaluate the performance of these machine learning algorithms. To train and test the suggested system performance, we built a corpus1 of reviews by collecting reviews written in two dialects (Saudi dialect and Jordanian dialect). The testing results show that Maximum Entropy outperforms the other two machine learning algorithms. Using N-gram (with N=3) as features set improves the performance of the three machine learning algorithms.","keywords_author":["Arabic colloquial dialects","Machine learning","Natural language processing","Opinion mining","Sentiment analysis"],"keywords_other":["Polarity determination","Arabic dialects","Sentiment analysis","Arabic colloquial dialects","Modern standards","Arabic languages","NAtural language processing","Opinion mining"],"max_cite":0.0,"pub_year":2016.0,"sources":"['scp']","rawkeys":["arabic colloquial dialects","polarity determination","machine learning","modern standards","natural language processing","arabic languages","opinion mining","sentiment analysis","arabic dialects"],"tags":["arabic colloquial dialects","polarity determination","machine learning","modern standards","natural language processing","arabic languages","opinion mining","sentiment analysis","arabic dialects"]},{"p_id":50523,"title":"Machine-learning methods for text named entity recognition","abstract":"\u00a9 2016, CEUR-WS. All rights reserved. The article describes machine learning methods for the named entity recognition. To build named entity classifiers two basic models of machine learning, The Na\u00efve Bayes and Conditional Random Fields, were used. A model for multi-classification of named entities using Error Correcting Output Codes was also researched. The paper describes a method for classifiers' training and the results of test experiments. Conditional Random Fields overcome other models in precision and recall evaluations.","keywords_author":["Machine learning","Named entity recognition","Natural language processing"],"keywords_other":["Multi-classification","Named entities","Named entity recognition","Precision and recall","Error correcting output code","Machine learning methods","Conditional random field","NAtural language processing"],"max_cite":0.0,"pub_year":2016.0,"sources":"['scp']","rawkeys":["machine learning methods","conditional random field","named entity recognition","machine learning","natural language processing","error correcting output code","multi-classification","named entities","precision and recall"],"tags":["machine learning methods","conditional random field","named entity recognition","machine learning","natural language processing","error correcting output code","multi-classification","named entities","precision and recall"]},{"p_id":21852,"title":"Study on feature selection and machine learning algorithms for Malay sentiment classification","abstract":"\u00a9 2014 IEEE. Online social media is used to show the sentiments of different individuals about various subjects. Sentiment analysis or opinion mining has recently been considered as one of the highly dynamic research fields in natural language processing, web mining, and machine learning. There has been a very limited amount of research that focuses on sentiment analysis in the Malay language. This study investigates how feature selection methods contribute to the improvement of Malay sentiment classification performance. Three supervised machine-learning classifiers and seven feature selection methods are used to conduct a series of experiments for the effective selection of the appropriate methods for the automatic sentiment classification of online Malay-written reviews. Findings show that the classifications of Malay sentiment improve using feature selections approaches. This work demonstrates that all feature reduction methods generally improve classifier performance. Support Vector Machine (SVM) approach provide the highest accuracy performance of features selection in order to classify Malay sentiment comparing with other classifications approaches such as PCA and CHI square. SVM records 87% as experimental accuracy result of feature selection.","keywords_author":["Classifications","Feature Selection","Machine Learning","NLP","Sentiment analysis"],"keywords_other":["Feature selection methods","Online social medias","Classifier performance","NLP","Sentiment analysis","Supervised machine learning","Sentiment classification","NAtural language processing"],"max_cite":13.0,"pub_year":2015.0,"sources":"['scp']","rawkeys":["sentiment classification","nlp","feature selection methods","machine learning","natural language processing","online social medias","classifier performance","feature selection","supervised machine learning","sentiment analysis","classifications"],"tags":["sentiment classification","feature selection methods","machine learning","natural language processing","online social medias","classifier performance","feature selection","classification","supervised machine learning","sentiment analysis"]},{"p_id":48478,"title":"A modeling framework for the Moroccan sociolect recognition used on the social media","abstract":"\u00a9 2017 Association for Computing Machinery. In this paper, we propose a new modeling methodology for Moroccan sociolect recognition used on the social media. It is based on detecting the language of each word in the text: classical Arabic, Tamazight, French or English, determination of the dominant language and processing the words belonging to the Moroccan sociolect. The Interest in this area comes from the huge and simultaneous use of, numbers, Latin script or figures and \/ or emoticons to speak in Arabic in Morocco which is the result of the country's history.","keywords_author":["Linguistic classifier","Machine Learning","Moroccan sociolect","NLP","Opinion mining","Social networks"],"keywords_other":["Modeling methodology","Social media","Model framework","Moroccan sociolect","Simultaneous use","Opinion mining"],"max_cite":0.0,"pub_year":2017.0,"sources":"['scp']","rawkeys":["moroccan sociolect","nlp","linguistic classifier","model framework","social media","machine learning","social networks","simultaneous use","modeling methodology","opinion mining"],"tags":["moroccan sociolect","linguistic classifier","social media","machine learning","natural language processing","social networks","simultaneous use","modeling methodology","modelling framework","opinion mining"]},{"p_id":25951,"title":"Person name recognition using the hybrid approach","abstract":"Arabic Person Name Recognition has been tackled mostly using either of two approaches: a rule-based or Machine Learning (ML) based approach, with their strengths and weaknesses. In this paper, the problem of Arabic Person Name Recognition is tackled through integrating the two approaches together in a pipelined process to create a hybrid system with the aim of enhancing the overall performance of Person Name Recognition tasks. Extensive experiments are conducted using three different ML classifiers to evaluate the overall performance of the hybrid system. The empirical results indicate that the hybrid approach outperforms both the rule-based and the ML-based approaches. Moreover, our system outperforms the state-of-the-art of Arabic Person Name Recognition in terms of accuracy when applied to ANERcorp dataset, with precision 0.949, recall 0.942 and f-measure 0.945. \u00a9 2013 Springer-Verlag Berlin Heidelberg.","keywords_author":["Hybrid Approach","Machine Learning Approach","Natural Language Processing","Person Name Recognition","Rulebased Approach"],"keywords_other":["Machine learning approaches","Name recognition","Hybrid approach","Rule-based approach","NAtural language processing"],"max_cite":7.0,"pub_year":2013.0,"sources":"['scp']","rawkeys":["machine learning approach","name recognition","natural language processing","rulebased approach","hybrid approach","person name recognition","rule-based approach","machine learning approaches"],"tags":["name recognition","natural language processing","rulebased approach","hybrid approach","machine learning approaches","rule-based approach","person name recognition"]},{"p_id":44383,"title":"Sentimental analysis using fuzzy and naive bayes","abstract":"\u00a9 2017 IEEE. Sentimental Analysis is the best way to judge people's opinion regarding a particular post. In this paper we present analysis for sentiment behavior of Twitter data. The proposed work utilizes the naive Bayes and fuzzy Classifier to classify Tweets into positive, negative or neural behavior of a particular person. We present experimental evaluation of our dataset and classification results which proved that combined proposed method is more efficient in terms of Accuracy, Precision and Recall.","keywords_author":["Artificial Intelligence","Behaviour Analysis","classification","deep learning","machine learning","Natural language processing","neural networks","opinion mining","Sentiment analysis"],"keywords_other":["Naive bayes","Experimental evaluation","Behaviour analysis","Classification results","Twitter datum","Precision and recall","Fuzzy classifiers"],"max_cite":0.0,"pub_year":2018.0,"sources":"['scp']","rawkeys":["artificial intelligence","opinion mining","experimental evaluation","deep learning","neural networks","machine learning","natural language processing","fuzzy classifiers","classification","sentiment analysis","twitter datum","behaviour analysis","naive bayes","precision and recall","classification results"],"tags":["opinion mining","experimental evaluation","neural networks","machine learning","natural language processing","fuzzy classifiers","classification","sentiment analysis","twitter datum","behaviour analysis","naive bayes","precision and recall","classification results"]},{"p_id":48480,"title":"Multi-label categorization of French death certificates using NLP and machine learning","abstract":"\u00a9 2017 Copyright is held by the wner\/author(s). Publication rights licensed to ACM.The medical information represents an invaluable source of knowledge concerning the medical history of the patient, but the manner of their presentation make it badly exploited. The idea of this paper is based on the analysis of the death reports written in natural language, which are rich of information, and can be exploited in the calculation of mortality statistics, giving preventive solutions, as well as, help medical professional in their research. This paper proposes our approach to the task of Multi-label Categorization of French death certificates according to ICD-10 (International Classification of Diseases) codes. This approach is based on Machine learning techniques, which is evaluated over C\u00e9piDC corpus. The experiment showed that our approach gives interesting results, with an average F1-measue of 79.02%.","keywords_author":["French Death Certificates","Machine learning","Multi-label categorization","NLP","Text Mining"],"keywords_other":["French Death Certificates","Text mining","Medical professionals","Mortality statistics","Multi-label categorization","International classification of disease","Medical information","Natural languages"],"max_cite":0.0,"pub_year":2017.0,"sources":"['scp']","rawkeys":["medical information","medical professionals","french death certificates","international classification of disease","nlp","text mining","natural languages","machine learning","multi-label categorization","mortality statistics"],"tags":["medical information","french death certificates","medical professionals","text mining","natural languages","machine learning","natural language processing","multi-label categorization","mortality statistics","international classification of diseases"]},{"p_id":48483,"title":"Build a morphosyntaxically annotated amazigh corpus","abstract":"\u00a9 2017 Association for Computing Machinery. Language resources are important for those working on computational methods to analyze and study languages. These resources are needed to help advancing the research in fields such as natural language processing, machine learning, information retrieval and text analysis in general. We describe the creation of morphosyntactically annotated corpus for Amazigh language that currently lacks them. We illustrate our approach for creating this corpus, that is more expensive but of high quality, using crowdsourcing and manual effort with appropriately skilled human participants. Qualitative and quantitative evaluations of the resources are also presented.","keywords_author":["Amazigh","Annotation","Corpus","CRF","Machine Learning","NLP","POS tagging","Standardization"],"keywords_other":["High quality","Text analysis","Language resources","Corpus","Quantitative evaluation","Annotation","Amazigh","PoS tagging"],"max_cite":0.0,"pub_year":2017.0,"sources":"['scp']","rawkeys":["nlp","amazigh","machine learning","pos tagging","text analysis","corpus","high quality","language resources","quantitative evaluation","crf","annotation","standardization"],"tags":["amazigh","conditional random field","standards","machine learning","natural language processing","text analysis","corpus","high quality","pos tagging","quantitative evaluation","language resources","annotation"]},{"p_id":32100,"title":"Featured based sentiment classification for hotel reviews using NLP and Bayesian classification","abstract":"The internet revolution has brought about a new way of expressing an individual's opinion. It has become a medium through which people openly express their views on various subjects. These opinions contain useful information which can be utilized in many sectors which require constant customer feedback. Analysis of the opinion and it's classification into different sentiment classes is gradually emerging as a key factor in decision making. There has been extensive research on automatic text analysis for sentiments such as sentiment classifiers, affect analysis, automatic survey analysis, opinion extraction, or recommender systems. These methods typically try to extract the overall sentiment revealed in a sentence or document, either positive or negative, or somewhere in between. However, a drawback of these methods is that the information can be degraded, especially in texts where a loss of information can also occur. The proposed method attempts to overcome the problem of the loss of text information by using well trained training sets. Also, recommendation of a product or request for a product as per the user's requirements have achieved with the proposed method. \u00a9 2012 IEEE.","keywords_author":["machine learning","naive bayes classification","natural language processing","online traveller reviews","ontology","sentiment analysis"],"keywords_other":["Customer feedback","Bayesian classification","Text information","Key factors","Internet revolution","Text analysis","Sentiment analysis","Opinion extraction","Training sets","Affect analysis","Naive Bayes classification","NAtural language processing","Sentiment classification"],"max_cite":3.0,"pub_year":2012.0,"sources":"['scp']","rawkeys":["bayesian classification","online traveller reviews","sentiment classification","customer feedback","text information","training sets","internet revolution","ontology","machine learning","natural language processing","naive bayes classification","opinion extraction","text analysis","key factors","affect analysis","sentiment analysis"],"tags":["bayesian classification","online traveller reviews","sentiment classification","customer feedback","text information","training sets","internet revolution","machine learning","natural language processing","naive bayes classification","opinion extraction","text analysis","key factors","affect analysis","sentiment analysis"]},{"p_id":32101,"title":"Some methods to address the problem of unbalanced sentiment classification in an arabic context","abstract":"The rise of social media (such as online web forums and social networking sites) has attracted interests to mining and analyzing opinions available on the web. The online opinion has become the object of studies in many research areas; especially that called 'Opinion Mining and Sentiment Analysis'. Several interesting and advanced works were performed on few languages (in particular English). However, there were very few studies on some languages such as Arabic. This paper presents the study we have carried out to address the problem of unbalanced data sets in supervised sentiment classification in an Arabic context. We propose three different methods to under-sample the majority class documents. Our goal is to compare the effectiveness of the proposed methods with the common random under-sampling. We also aim to evaluate the behavior of the classifier toward different under-sampling rates. We use two different common classifiers, namely Na\u00efve Bayes and Support Vector Machines. The experiments are carried out on an Arabic data set that we have built from Aljazeera's web site and labeled manually. The results show that Na\u00efve Bayes is sensitive to data set size, the more we reduce the data the more the results degrade. However, it is not sensitive to unbalanced data sets on the contrary of Support Vector Machines which is highly sensitive to unbalanced data sets. The results show also that we can rely on the proposed techniques and that they are typically competitive with random under-sampling. \u00a9 2012 IEEE.","keywords_author":["Arabic Language","Corpora","Machine Learning","Natural Language Processing","Opinion Mining","Sentiment Analysis","Text Classification","Unbalanced Data sets"],"keywords_other":["Text classification","Corpora","Sentiment analysis","Arabic languages","Unbalanced data","NAtural language processing","Opinion mining"],"max_cite":3.0,"pub_year":2012.0,"sources":"['scp']","rawkeys":["corpora","unbalanced data","text classification","arabic language","natural language processing","machine learning","unbalanced data sets","arabic languages","opinion mining","sentiment analysis"],"tags":["corpora","unbalanced data","text classification","unbalanced datasets","natural language processing","machine learning","arabic languages","opinion mining","sentiment analysis"]},{"p_id":1382,"title":"Mispronunciation Detection and Diagnosis in L2 English Speech Using Multidistribution Deep Neural Networks","abstract":"This paper investigates the use of multidistribution deep neural networks (DNNs) for mispronunciation detection and diagnosis (MDD), to circumvent the difficulties encountered in an existing approach based on extended recognition networks (ERNs). The ERNs leverage existing automatic speech recognition technology by constraining the search space via including the likely phonetic error patterns of the target words in addition to the canonical transcriptions. MDDs are achieved by comparing the recognized transcriptions with the canonical ones. Although this approach performs reasonably well, it has the following issues: 1) Learning the error patterns of the target words to generate the ERNs remains a challenging task. Phones or phone errors missing from the ERNs cannot be recognized even if we have well-trained acoustic models; and 2) acoustic models and phonological rules are trained independently, and hence, contextual information is lost. To address these issues, we propose an acoustic-graphemic-phonemic model (AGPM) using a multidistribution DNN, whose input features include acoustic features, as well as corresponding graphemes and canonical transcriptions (encoded as binary vectors). The AGPM can implicitly model both grapheme-to-likely-pronunciation and phoneme-to-likely-pronunciation conversions, which are integrated into acoustic modeling. With the AGPM, we develop a unified MDD framework, which works much like free-phone recognition. Experiments show that our method achieves a phone error rate (PER) of 11.1%. The false rejection rate (FRR), false acceptance rate (FAR), and diagnostic error rate (DER) for MDD are 4.6%, 30.5%, and 13.5%, respectively. It outperforms the ERN approach using DNNs as acoustic models, whose PER, FRR, FAR, and DER are 16.8%, 11.0%, 43.6%, and 32.3%, respectively.","keywords_author":["Deep neural networks","L2 English speech","mispronunciation detection","mispronunciation diagnosis","speech recognition","Deep neural networks","L2 English speech","mispronunciation detection","mispronunciation diagnosis","speech recognition"],"keywords_other":["Acoustics","DER","REPRESENTATIONS","ERN","multidistribution deep neural network","speech recognition","PER","PRONUNCIATION ERROR PATTERNS","search space","Neural networks","false rejection rate","likely phonetic error pattern","false acceptance rate","automatic speech recognition technology","mispronunciation detection and diagnosis","AGREEMENT","extended recognition network","word processing","Speech","Context modeling","L2 english speech","AGPM","phoneme-to-likely-pronunciation conversion","MARKOV-MODELS","target word","RECOGNITION","Hidden Markov models","grapheme-to-likely-pronunciation conversion","neural nets","multidistribution DNN","canonical transcription","diagnostic error rate","well-trained acoustic model","phone error rate","FAR","natural language processing","speech processing","voice activity detection","MDD","Speech recognition","TO-PHONEME CONVERSION","LANGUAGE","UNSUPERVISED DISCOVERY","acoustic-graphemic-phonemic model","FRR"],"max_cite":4.0,"pub_year":2016.0,"sources":"['wos', 'ieee']","rawkeys":["frr","markov-models","mdd","speech recognition","multidistribution deep neural network","agreement","context modeling","search space","ern","acoustics","language","agpm","false rejection rate","likely phonetic error pattern","false acceptance rate","automatic speech recognition technology","der","extended recognition network","l2 english speech","mispronunciation detection and diagnosis","word processing","speech","per","phoneme-to-likely-pronunciation conversion","target word","recognition","unsupervised discovery","neural networks","far","grapheme-to-likely-pronunciation conversion","mispronunciation detection","neural nets","canonical transcription","diagnostic error rate","hidden markov models","well-trained acoustic model","mispronunciation diagnosis","deep neural networks","phone error rate","natural language processing","speech processing","voice activity detection","representations","pronunciation error patterns","multidistribution dnn","acoustic-graphemic-phonemic model","to-phoneme conversion"],"tags":["target words","speech recognition","markov model","multidistribution deep neural network","agreement","context modeling","convolutional neural network","ern","acoustics","language","agpm","false rejection rate","likely phonetic error pattern","false acceptance rate","automatic speech recognition technology","der","extended recognition network","l2 english speech","mispronunciation detection and diagnosis","word processing","speech","per","phoneme-to-likely-pronunciation conversion","false alarm rate","recognition","unsupervised discovery","search spaces","neural networks","grapheme-to-likely-pronunciation conversion","mispronunciation detection","canonical transcription","diagnostic error rate","hidden markov models","well-trained acoustic model","mispronunciation diagnosis","phone error rate","natural language processing","representation","speech processing","voice activity detection","pronunciation error patterns","multidistribution dnn","major depressive disorder","acoustic-graphemic-phonemic model","to-phoneme conversion"]},{"p_id":109926,"title":"Method for unconstrained text detection in natural scene image","abstract":"Text detection in natural scene images is an important prerequisite for many content-based multimedia understanding applications. The authors present a simple and effective text detection method in natural scene image. Firstly, MSERs are extracted by the V-MSER algorithm from channels of G, H, S, O-1, and O-2, as component candidates. Since text is composed of character candidates, the authors design an MRF model to exploit the relationship between characters. Secondly, in order to filter out non-text components, they design a set of two-layers filtering scheme: most of the non-text components can be filtered by the first layer of the filtering scheme; the second layer filtering scheme is an AdaBoost classifier, which is trained by the features of compactness, horizontal variance and vertical variance, and aspect ratio. Then, only four simple features are adopted to generate component pairs. Finally, according to the orientation similarity of the component pairs, component pairs which have roughly the same orientation are merged into text lines. The proposed method is evaluated on two public datasets: ICDAR 2011 and MSRA-TD500. It achieves 82.94 and 75% F-measure, respectively. Especially, the experimental results, on their URMQ_LHASA-TD220 dataset which contains 220 images for multi-orientation and multi-language text lines evaluation, show that the proposed method is general for detecting scene text lines in different languages.","keywords_author":["text detection","multimedia systems","image filtering","image classification","natural language processing","unconstrained text detection","natural scene image","content-based multimedia understanding applications","V-MSER algorithm","character candidates","MRF model","nontext component filterig","two-layers filtering scheme","first layer filtering scheme","second layer filtering scheme","AdaBoost classifier","compactness","horizontal variance","vertical variance","aspect ratio","component pair orientation similarity","ICDAR 2011","MSRA-TD500","URMQ_LHASA-TD220 dataset","multiorientation text lines evaluation","multilanguage text lines evaluation"],"keywords_other":["STABLE EXTREMAL REGIONS","RECOGNITION"],"max_cite":1.0,"pub_year":2017.0,"sources":"['ieee', 'wos']","rawkeys":["horizontal variance","image filtering","text detection","second layer filtering scheme","two-layers filtering scheme","urmq_lhasa-td220 dataset","image classification","content-based multimedia understanding applications","first layer filtering scheme","multilanguage text lines evaluation","aspect ratio","stable extremal regions","compactness","adaboost classifier","icdar 2011","recognition","nontext component filterig","natural scene image","unconstrained text detection","v-mser algorithm","multimedia systems","character candidates","msra-td500","vertical variance","natural language processing","mrf model","component pair orientation similarity","multiorientation text lines evaluation"],"tags":["horizontal variance","image filtering","text detection","second layer filtering scheme","two-layers filtering scheme","urmq_lhasa-td220 dataset","image classification","content-based multimedia understanding applications","first layer filtering scheme","multilanguage text lines evaluation","aspect ratio","stable extremal regions","natural scene images","compactness","icdar 2011","recognition","nontext component filterig","unconstrained text detection","v-mser algorithm","multimedia systems","character candidates","msra-td500","ada boost classifiers","vertical variance","natural language processing","mrf model","component pair orientation similarity","multiorientation text lines evaluation"]},{"p_id":1380,"title":"Partial discharge patterns recognition with deep Convolutional Neural Networks","abstract":"Traditional methods of partial discharge (PD) patterns recognition often rely on much prior knowledge about PD mechanism and signal processing techniques to construct appropriate features. Therefore the performance is not stable. Recent progress in deep neural networks, which contain more than one hidden layer, has shown state-of-art performance in speech recognition, image classification and natural language processing. Besides, the deep neural networks have the ability to handle large dataset, which is the technique for the future as more and more condition monitoring data accumulate. In this paper, a Convolutional Neural Network (CNN) with deep architecture is established to extrapolate new features automatically to realize ultra-high frequency (UHF) signals recognition in GIS. Firstly, a two-dimension spectral frames representation of the UHF signals is obtained by the time-frequency analysis. Then the spectral frames are used to train a deep CNN. It is shown that the proposed method can identify different sources of PD successfully. A comparison with other PD pattern recognition techniques is also discussed.","keywords_author":["partial discharge","Convolutional Neural Network","ultra-high frequency","pattern recognition"],"keywords_other":["Pattern recognition","2D spectral frames representation","PD pattern recognition","speech recognition","time-frequency analysis","extrapolation","Finite difference methods","gas insulated switchgear","image classification","Time-frequency analysis","partial discharge","CNN","signal processing techniques","Convolution","partial discharges","Time-domain analysis","condition monitoring","ultra-high frequency signals","UHF signal recognition","signal representation","condition monitoring data","PD mechanism","deep convolutional neural networks","natural language processing","Partial discharges","GIS","pattern recognition"],"max_cite":null,"pub_year":2016.0,"sources":"['ieee']","rawkeys":["extrapolation","speech recognition","time-frequency analysis","convolutional neural network","2d spectral frames representation","time-domain analysis","gas insulated switchgear","image classification","convolution","partial discharge","signal processing techniques","partial discharges","gis","pd mechanism","condition monitoring","cnn","ultra-high frequency signals","signal representation","ultra-high frequency","finite difference methods","condition monitoring data","deep convolutional neural networks","natural language processing","pattern recognition","uhf signal recognition","pd pattern recognition"],"tags":["extrapolation","speech recognition","time-frequency analysis","convolutional neural network","2d spectral frames representation","time-domain analysis","gas insulated switchgear","image classification","convolution","gis","parkinson's disease","pd mechanism","condition monitoring","ultra-high frequency signals","condition-monitoring data","signal representation","ultra-high frequency","finite difference methods","natural language processing","signal processing technique","pattern recognition","uhf signal recognition","pd pattern recognition"]},{"p_id":7531,"title":"An experimental study of graph connectivity for unsupervised word sense disambiguation","abstract":"Word sense disambiguation (WSD), the task of identifying the intended meanings (senses) of words in context, has been a long-standing research objective for natural language processing. In this paper, we are concerned with graph-based algorithms for large-scale WSD. Under this framework, finding the right sense for a given word amounts to identifying the most important node among the set of graph nodes representing its senses. We introduce a graph-based WSD algorithm which has few parameters and does not require sense-annotated data for training. Using this algorithm, we investigate several measures of graph connectivity with the aim of identifying those best suited for WSD. We also examine how the chosen lexicon and its connectivity influences WSD performance. We report results on standard data sets and show that our graph-based approach performs comparably to the state of the art. \u00a9 2010 IEEE.","keywords_author":["Graph connectivity","Semantic networks","Social network analysis.","Word sense disambiguation"],"keywords_other":["Semantic network","Graph-based","Research objectives","State of the art","Data sets","Graph connectivity","Experimental studies","Word Sense Disambiguation","Social Network Analysis","WSD algorithms","NAtural language processing"],"max_cite":140.0,"pub_year":2010.0,"sources":"['scp']","rawkeys":["graph connectivity","semantic network","graph-based","state of the art","natural language processing","experimental studies","data sets","research objectives","wsd algorithms","social network analysis","word sense disambiguation","semantic networks"],"tags":["graph connectivity","semantic network","graph-based","state of the art","natural language processing","experimental studies","data sets","research objectives","wsd algorithms","social network analysis","word sense disambiguation"]},{"p_id":32117,"title":"Automatic detection of inconsistencies between free text and coded data in sarcoma discharge letters","abstract":"Discordance between data stored in Electronic Health Records (EHR) may have a harmful effect on patient care. Automatic identification of such situations is an important yet challenging task, especially when the discordance involves information stored in free text fields. Here we present a method to automatically detect inconsistencies between data stored in free text and related coded fields. Using EHR data we train an ensemble of classifiers to predict the value of coded fields from the free text fields. Cases in which the classifiers predict with high confidence a code different from the clinicians' choice are marked as potential inconsistencies. Experimental results over discharge letters of sarcoma patients, verified by a domain expert, demonstrate the validity of our method. \u00a9 2012 European Federation for Medical Informatics and IOS Press. All rights reserved.","keywords_author":["Clinical decision support","EHR","Machine learning","NLP"],"keywords_other":null,"max_cite":3.0,"pub_year":2012.0,"sources":"['scp']","rawkeys":["ehr","clinical decision support","nlp","machine learning"],"tags":["natural language processing","clinical decision support","electronic health records","machine learning"]},{"p_id":40310,"title":"Intelligent agent for information extraction from arabic text without machine translation","abstract":"The process of classifying text into two opposing opinions is known as sentiment polarity classification. It has been shown in the literature that this problem cannot reach accuracy higher than 80-85%. This paper shows that a higher accuracy (96%) can be achieved without the need to translate text into English language. More specifically, our case study is: Islamic Hadith Narration. The problem is to tell whether a person is trustworthy or not based on his biographical data. With such high accuracy, the agent can be used to create new books in the area of Hadith automatically instead of manual classification done before. The results of our experiments encourage the use of an intelligent agent for information extraction using supervised learning, domain knowledge and number of natural language processing techniques.","keywords_author":["Arabic","Information extraction","Machine learning","Machine translation","Natural language processing","Sentiment analysis","Supervised learning"],"keywords_other":["English languages","Arabic","Polarity classification","Domain knowledge","Sentiment analysis","Manual classification","Machine translations","NAtural language processing"],"max_cite":1.0,"pub_year":2010.0,"sources":"['scp']","rawkeys":["supervised learning","information extraction","manual classification","machine learning","natural language processing","machine translation","arabic","english languages","domain knowledge","machine translations","sentiment analysis","polarity classification"],"tags":["supervised learning","information extraction","manual classification","machine learning","natural language processing","arabic","english languages","domain knowledge","machine translations","sentiment analysis","polarity classification"]},{"p_id":32119,"title":"Identifying named entities on a university intranet","abstract":"Named entities (NEs) are textual references via proper names, such as people names, company names, places and so on. The importance of NEs has been observed in intranet search engines, including university web sites. In this paper, a mechanism is built exclusively to recognize the three named entities, which are constantly referenced in the University of Essex domain: names, course codes, and room numbers. While a person name is considered a common named entity, course codes and room numbers are specific to the University domain. We developed a technique specifically to train three different classifiers on electronic corpora, consisting of 16,629 examples in total, which were collected and annotated manually from the University domain. The resulting models were then incorporated into the NER system that was built to use pre-trained classifiers in the detection process, mark these NEs, and cross-reference them to the related documents. The proposed method performed well on a test corpus, with the average precision reaching nearly 0.97. The recall varied, but was lower overall than precision with an average of 0.82. Moreover, in terms of name recognition in the University domain, our system outperformed two other systems: the OpenNLP name finder and ANNIE system. \u00a9 2012 IEEE.","keywords_author":["Corpus-based methods","Information Extraction from the web","Machine learning","Named Entity Recognition","Natural Language Processing","Statistical approach"],"keywords_other":["Named entity recognition","Statistical approach","Information Extraction","NAtural language processing","Corpus-based methods"],"max_cite":3.0,"pub_year":2012.0,"sources":"['scp']","rawkeys":["information extraction","named entity recognition","machine learning","natural language processing","statistical approach","corpus-based methods","information extraction from the web"],"tags":["information extraction","named entity recognition","machine learning","natural language processing","statistical approach","corpus-based methods","information extraction from the web"]},{"p_id":34168,"title":"On one approach of solving sentiment analysis task for Kazakh and Russian languages using deep learning","abstract":"\u00a9 Springer International Publishing Switzerland 2016. The given research paper describes modern approaches of solving the task of sentiment analysis of the news articles in Kazakh and Russian languages by using deep recurrent neural networks. Particularly, we used Long-Short Term Memory (LSTM) in order to consider long term dependencies of the whole text. Thereby, research shows that good results can be achieved even without knowing linguistic features of particular language. Here we are going to use word embedding (word2vec, GloVes) as the main feature in our machine learning algorithms. The main idea of word embedding is the representations of words with the help of vectors in such manner that semantic relationships between words preserved as basic linear algebra operations.","keywords_author":["Deep learning","Machine learning","NLP","Sentiment analysis","Text classification"],"keywords_other":["Deep learning","Text classification","Semantic relationships","Linear algebra operations","Sentiment analysis","Long short term memory","Long-term dependencies","Linguistic features"],"max_cite":2.0,"pub_year":2016.0,"sources":"['scp']","rawkeys":["text classification","nlp","deep learning","linear algebra operations","machine learning","linguistic features","long-term dependencies","semantic relationships","sentiment analysis","long short term memory"],"tags":["text classification","linear algebra operations","long short-term memory","machine learning","natural language processing","linguistic features","long-term dependencies","semantic relationships","sentiment analysis"]},{"p_id":40317,"title":"Recognition of gene\/protein names using conditional random fields","abstract":"With the overwhelming amount of publicly available data in the biomedical field, traditional tasks performed by expert database annotators rapidly became hard and very expensive. This situation led to the development of computerized systems to extract information in a structured manner. The first step of such systems requires the identification of named entities (e.g. gene\/protein names), a task called Named Entity Recognition (NER). Much of the current research to tackle this problem is based on Machine Learning (ML) techniques, which demand careful and sensitive definition of the several used methods. This article presents a NER system using Conditional Random Fields (CRFs) as the machine learning technique, combining the best techniques recently described in the literature. The proposed system uses biomedical knowledge and a large set of orthographic and morphological features. An F-measure of 0,7936 was obtained on the BioCreative II Gene Mention corpus, achieving a significantly better performance than similar baseline systems.","keywords_author":["Gene\/Protein names","Machine learning","Named entity recognition","Natural language processing","Text mining"],"keywords_other":["Text mining","Named entity recognition","Machine-learning","Gene\/Protein names","NAtural language processing"],"max_cite":1.0,"pub_year":2010.0,"sources":"['scp']","rawkeys":["text mining","gene\/protein names","natural language processing","machine learning","named entity recognition","machine-learning"],"tags":["text mining","gene\/protein names","natural language processing","machine learning","named entity recognition"]},{"p_id":1406,"title":"A Convolutional Neural Network-Based Chinese Text Detection Algorithm via Text Structure Modeling","abstract":"Text detection in a natural environment plays an important role in many computer vision applications. While existing text detection methods are focused on English characters, there are strong application demands on text detection in other languages, such as Chinese. In this paper, we present a novel text detection algorithm for Chinese characters based on a specific designed convolutional neural network (CNN). The CNN contains a text structure component detector layer, a spatial pyramid layer, and a multi-input-layer deep belief network (DBN). The CNN is pre-trained via a convolutional sparse auto-encoder, specifically designed for extracting complex features from Chinese characters. In particular, the text structure component detectors enhance the accuracy and uniqueness of feature descriptors by extracting multiple text structure components in various ways. The spatial pyramid layer enhances the scale invariability of the CNN for detecting texts in multiple scales. Finally, the multi-input-layer DBN replaces the fully connected layers in the CNN to ensure features from multiple scales are comparable. A multilingual text detection dataset, in which texts in Chinese, English, and digits are labeled separately, is set up to evaluate the proposed text detection algorithm. The proposed algorithm shows a significant performance improvement over the baseline CNN algorithms. In addition the proposed algorithm is evaluated over a public multilingual benchmark and achieves state-of-the-art result under multiple languages. Furthermore, a simplified version of the proposed algorithm with only general components is evaluated on the ICDAR 2011 and 2013 datasets, showing comparable detection performance to the existing general text detection algorithms.","keywords_author":["Chinese text detection","convolutional neural network (CNN)","text structure detector","unsupervised learning","Chinese text detection","convolutional neural network (CNN)","text structure detector","unsupervised learning"],"keywords_other":["Detection algorithms","English","convolutional neural network","Neural networks","READING TEXT","Unsupervised learning","spatial pyramid layer","Feature extraction","multilingual text detection dataset","CNN","complex feature extraction","Machine learning","chinese text detection algorithm","text structure modeling","text structure component detector layer","convolutional sparse auto-encoder","general text detection algorithms","English characters","computer vision applications","multi-input-layer deep belief network","RECOGNITION","DBN","computer vision","neural nets","Detectors","Chinese","LOCALIZATION","multiple text structure components","Chinese characters","VIDEOS","Image edge detection","natural language processing","text analysis","NATURAL SCENE IMAGES"],"max_cite":3.0,"pub_year":2016.0,"sources":"['ieee', 'wos']","rawkeys":["detection algorithms","localization","english","convolutional neural network","spatial pyramid layer","dbn","multilingual text detection dataset","machine learning","complex feature extraction","chinese text detection algorithm","text structure modeling","text structure component detector layer","convolutional sparse auto-encoder","general text detection algorithms","natural scene images","chinese characters","image edge detection","recognition","reading text","neural networks","computer vision applications","multi-input-layer deep belief network","cnn","convolutional neural network (cnn)","unsupervised learning","computer vision","neural nets","chinese","chinese text detection","english characters","text structure detector","multiple text structure components","detectors","natural language processing","text analysis","videos","feature extraction"],"tags":["localization","english","convolutional neural network","spatial pyramid layer","multilingual text detection dataset","machine learning","complex feature extraction","video","chinese text detection algorithm","text structure modeling","text structure component detector layer","natural scene images","general text detection algorithms","chinese characters","image edge detection","recognition","reading text","neural networks","computer vision applications","multi-input-layer deep belief network","detection algorithm","unsupervised learning","computer vision","chinese","chinese text detection","english characters","convolutional sparse autoencoders","text structure detector","multiple text structure components","detectors","natural language processing","text analysis","feature extraction","deep belief networks"]},{"p_id":9599,"title":"Predicting process behaviour using deep learning","abstract":"Predicting business process behaviour is an important aspect of business process management. Motivated by research in natural language processing, this paper describes an application of deep learning with recurrent neural networks to the problem of predicting the next event in a business process. This is both a novel method in process prediction, which has largely relied on explicit process models, and also a novel application of deep learning methods. The approach is evaluated on two real datasets and our results surpass the state-of-the-art in prediction precision. (C) 2017 Elsevier B.V. All rights reserved.","keywords_author":["Deep learning","Neural networks","Process management","Process prediction","Runtime support","Process management","Runtime support","Process prediction","Deep learning","Neural networks"],"keywords_other":["Prediction precision","Novel applications","Business process management","Process management","State of the art","Runtime support","NEURAL-NETWORKS","Process prediction","TIME","MODELS","NAtural language processing","BUSINESS PROCESSES"],"max_cite":16.0,"pub_year":2017.0,"sources":"['scp', 'wos']","rawkeys":["neural-networks","novel applications","process prediction","deep learning","neural networks","prediction precision","runtime support","natural language processing","state of the art","time","business process management","business processes","process management","models"],"tags":["model","novel applications","process prediction","neural networks","runtime support","prediction precision","state of the art","natural language processing","machine learning","time","business process management","process management","business process"]},{"p_id":32127,"title":"Effective unsupervised Arabic word stemming: Towards an unsupervised radicals extraction","abstract":"This paper presents a new totally unsupervised and 90% effective stemming approach for classical Arabic. This stemming is meant to be a preparatory step to an unsupervised root (i.e., radicals) extraction. As a learning input, our stemming system requires no linguistic knowledge but a plain classical Arabic text. Once the learning input analyzed, our stemming system is able to extract the strongest segment of a given length, namely the stem. We start by a definition of the targeted stem, then, we show how our system performs about 90% true positives after a leaning of less than 15000 words. Unlike the other unsupervised approaches, ours does not suppose the perfectness of the input text and deals efficiently with the eventual (practically very frequent) misspellings. The test corpus we have used is an ultimate reference in the classical Arabic and its labeling has been rigorously done by a team of experts.","keywords_author":["Classical Arabic","Computational morphology","Machine learning","Natural language processing","Semitic languages"],"keywords_other":null,"max_cite":3.0,"pub_year":2012.0,"sources":"['scp']","rawkeys":["classical arabic","natural language processing","machine learning","computational morphology","semitic languages"],"tags":["classical arabic","natural language processing","machine learning","computational morphology","semitic languages"]},{"p_id":50563,"title":"Neural abstractive text summarization","abstract":"\u00a9 2016, CEUR-WS. All rights reserved. Abstractive text summarization is a complex task whose goal is to generate a concise version of a text without necessarily reusing the sentences from the original source, but still preserving the meaning and the key contents. We address this issue by modeling the problem as a sequence to sequence learning and exploiting Recurrent Neural Networks (RNNs). This work is a discussion about our ongoing research on abstractive text summarization, where our aim is to investigate methods to infuse prior knowledge into deep neural networks. We believe that these approaches can obtain better performance than the state-of-the-art models for generating well-formed and meaningful summaries.","keywords_author":["Abstractive text summarization","Deep learning","Natural language processing","Recurrent neural networks"],"keywords_other":["Deep learning","Prior knowledge","State of the art","Text summarization","Sequence learning","Recurrent neural network (RNNs)","Deep neural networks","NAtural language processing"],"max_cite":0.0,"pub_year":2016.0,"sources":"['scp']","rawkeys":["recurrent neural network (rnns)","deep learning","deep neural networks","state of the art","natural language processing","text summarization","abstractive text summarization","recurrent neural networks","sequence learning","prior knowledge"],"tags":["neural networks","state of the art","natural language processing","machine learning","abstractive text summarization","text summarization","convolutional neural network","sequence learning","prior knowledge"]},{"p_id":1419,"title":"Offline Arabic Handwritten recognition system with dropout applied in Deep networks based-SVMs","abstract":"As a machine learning algorithms, deep learning algorithms developed in recent years, have been successfully practiced in many fields of computer vision, like face recognition, object detection and image classification. These Deep algorithms look for drawing out a very performing representation of the data, among which image and speech, through multi-layers in a deep hierarchical structure. In this study, a deep learning model based on Support Vector Machine (SVM) named Deep SVM (DSVM) is represented. We applied the dropout technique on the Deep SVM (DSVM). It is worth noting that this model has an inherent capacity to choose data points crucial to classify good generalization capacities. The deep SVM is built by a stack of SVMs permitting to extracting\/learning automatically features from the raw images and to realize classification, too. We chose and tested the Multi-class Support Vector Machine with an RBF kernel, as non-linear discriminative features for classification, on Handwritten Arabic Characters Database (HACDB). Further to these advantages, our model is safeguarded against over-fitting because of strong performance of dropout. Simulation outcomes prove the efficiency of the suggested model.","keywords_author":["Arabic handwritten script","Deep learning","Deep SVM","Dropout","Hand-crafted feature","Over-fitting","Deep learning","arabic handwritten script","Deep SVM","dropout","hand-crafted feature","over-fitting"],"keywords_other":["Overfitting","data structures","Hand-crafted feature","Text recognition","nonlinear discriminative feature","Neural networks","feature learning","handwritten character recognition","image classification","Handwriting recognition","Deep learning","multiclass support vector machine","Feature extraction","Machine learning","handwritten Arabic characters database","speech","Dropout","Deep SVM","machine learning algorithm","deep SVM","RBF kernel","learning (artificial intelligence)","Training","computer vision","dropout","deep learning algorithms","Arabic handwritten script","data representation","offline Arabic handwritten recognition system","DSVM","natural language processing","feature extraction","support vector machines","HACDB","Support vector machines"],"max_cite":3.0,"pub_year":2016.0,"sources":"['ieee', 'scp']","rawkeys":["rbf kernel","data structures","nonlinear discriminative feature","feature learning","handwritten character recognition","arabic handwritten script","image classification","overfitting","multiclass support vector machine","machine learning","speech","handwriting recognition","machine learning algorithm","learning (artificial intelligence)","deep learning","neural networks","training","hand-crafted feature","dsvm","text recognition","computer vision","over-fitting","deep svm","dropout","data representation","deep learning algorithms","handwritten arabic characters database","natural language processing","hacdb","offline arabic handwritten recognition system","feature extraction","support vector machines"],"tags":["rbf kernels","machine learning algorithms","data structures","nonlinear discriminative feature","feature learning","hand-crafted features","handwritten character recognition","arabic handwritten script","image classification","data representations","overfitting","machine learning","speech","handwriting recognition","deep learning algorithm","neural networks","training","dsvm","text recognition","computer vision","deep svm","dropout","handwritten arabic characters database","natural language processing","hacdb","offline arabic handwritten recognition system","feature extraction"]},{"p_id":48523,"title":"Sentiment mining: An approach for Bengali and Tamil tweets","abstract":"\u00a9 2016 IEEE. This paper presents a proposed work for extracting the sentiments from tweets in Indian Language. We proposed a system that deal with the goal to extract the sentiments from Bengali & Tamil tweets. Our aim is to classify a given Bengali or Tamil tweets into three sentiment classes namely positive, negative or neutral. In recent time, Twitter gain much attention to NLP researchers as it is most widely used platform that allows the user to share there opinion in form of tweets. The proposed methodology used unigram and bi-gram models along with different supervised machine learning techniques. We also consider the use of features generated from lexical resources such as Wordnets and Emoticons Tagger.","keywords_author":["Information Retrieval","Machine learning","Natural Language Processing","Polarity Identification","Sentiment Mining"],"keywords_other":["Bengalis","Gram models","Lexical resources","Supervised machine learning","Indian languages","NAtural language processing"],"max_cite":0.0,"pub_year":2017.0,"sources":"['scp']","rawkeys":["sentiment mining","bengalis","machine learning","natural language processing","information retrieval","indian languages","gram models","lexical resources","polarity identification","supervised machine learning"],"tags":["sentiment mining","bengalis","machine learning","natural language processing","information retrieval","indian languages","gram models","lexical resources","polarity identification","supervised machine learning"]},{"p_id":17806,"title":"Designing for the deluge: Understanding & supporting the distributed, collaborative work of crisis volunteers","abstract":"Social media are a potentially valuable source of situational awareness information during crisis events. Consistently, \"digital volunteers\" and others are coming together to filter and process this data into usable resources, often coordinating their work within distributed online groups. However, current tools and practices are frequently unable to keep up with the speed and volume of incoming data during large events. Through contextual interviews with emergency response professionals and digital volunteers, this research examines the ad hoc, collaborative practices that have emerged to help process this data and outlines strategies for supporting and leveraging these efforts in future designs. We argue for solutions that align with current group values, work practices, volunteer motivations, and organizational structures, but also allow these groups to increase the scale and efficiency of their operations. Copyright \u00a9 2014 ACM.","keywords_author":["Civic participation","Crisis informatics","Crowdsourcing","Digital volunteers","Disaster response","Machine learning","Natural language processing","Social computing"],"keywords_other":["Social computing","Digital volunteers","Civic participation","Crisis informatics","Disaster response","NAtural language processing","Crowdsourcing"],"max_cite":27.0,"pub_year":2014.0,"sources":"['scp']","rawkeys":["digital volunteers","disaster response","machine learning","crowdsourcing","natural language processing","social computing","crisis informatics","civic participation"],"tags":["digital volunteers","disaster response","machine learning","crowdsourcing","natural language processing","social computing","crisis informatics","civic participation"]},{"p_id":32142,"title":"Building up lexical sample dataset for Turkish word sense disambiguation","abstract":"Word Sense Disambiguation (WSD) has become even more important research area in recent years with the widespread usage of Natural Language Processing (NLP) applications. WSD task has two variants: \"Lexical Sample\" and \"All Words\" approaches. Lexical Sample approach disambiguates the occurrences of a small sample of target words that were previously selected, while in the latter all the words in a piece of text are disambiguated. In the scope of this work, a Lexical Sample Dataset for Turkish has been prepared. As a first step, highly ambiguous words in Turkish have been selected. Collection of text samples for chosen words has been completed. Five taggers have annotated the word senses. This paper summarizes the step-by-step building-up process of a Lexical Sample Dataset in Turkish and presents the results of some experiments on it. \u00a9 2012 IEEE.","keywords_author":["Feature Selection","Lexical Sample","Machine Learning","Natural Language Processing","Word Sense Disambiguation"],"keywords_other":["Turkishs","Word sense","Small samples","Word Sense Disambiguation","Lexical Sample","Sample dataset","NAtural language processing"],"max_cite":3.0,"pub_year":2012.0,"sources":"['scp']","rawkeys":["turkishs","natural language processing","machine learning","feature selection","word sense","word sense disambiguation","sample dataset","small samples","lexical sample"],"tags":["turkishs","natural language processing","machine learning","feature selection","word sense","word sense disambiguation","sample dataset","small samples","lexical sample"]},{"p_id":38288,"title":"Tackling the Winograd Schema Challenge through machine logical inferences","abstract":"\u00a9 2016 The authors and IOS Press. Levesque has argued that the problem of resolving difficult pronouns in a carefully chosen set of twin sentences, which he refers to as the Winograd Schema Challenge (WSC), could serve as a conceptually and practically appealing alternative to the well-known Turing Test. As he said, probably anything that answers correctly a series of these questions is thinking in the full-bodied sense we usually reserve for people. In this paper we examine the task of resolving cases of definite pronouns. Specifically, we examine those for which traditional linguistic constraints on co-reference as well as commonly-used resolution heuristics are not useful, or the procedure they follow is very similar to a statistical approach, without invoking common logic like humans do.","keywords_author":["Knowledge and reasoning","Machine learning","NLP","Winograd schema challenge","WSC"],"keywords_other":["Winograd","Linguistic constraints","Knowledge and reasoning","Statistical approach","Logical inference","Common logic","Turing tests"],"max_cite":1.0,"pub_year":2016.0,"sources":"['scp']","rawkeys":["nlp","wsc","linguistic constraints","machine learning","statistical approach","logical inference","winograd schema challenge","turing tests","winograd","common logic","knowledge and reasoning"],"tags":["wsc","linguistic constraints","machine learning","natural language processing","logical inference","statistical approach","winograd schema challenge","turing tests","winograd","common logic","knowledge and reasoning"]},{"p_id":52625,"title":"Classification based on specific vocabulary","abstract":"Assuming a binomial distribution for word occurrence, we propose computing a standardized Z score to define the specific vocabulary of a subset compared to that of the entire corpus. This approach is applied to weight terms characterizing a document (or a sample of texts). We then show how these Z score values can be used to derive an efficient categorization scheme. To evaluate this proposition we categorize speeches given by B. Obama as either electoral or presidential. The results tend to show that the suggested classification scheme performs better than a Support Vector Machine scheme, and a Na\u00efve Bayes classifier (10-fold cross validation). \u00a9 2011 IEEE.","keywords_author":["Lexical analysis","Machine learning","Natural language processing","Political discourse","Text categorization"],"keywords_other":["Text categorization","Machine-learning","Political discourse","Natural language processing","Lexical analysis"],"max_cite":0.0,"pub_year":2011.0,"sources":"['scp']","rawkeys":["political discourse","natural language processing","machine learning","lexical analysis","machine-learning","text categorization"],"tags":["political discourse","natural language processing","machine learning","lexical analysis","text categorization"]},{"p_id":32146,"title":"Extracting epidemiologic exposure and outcome terms from literature using machine learning approaches","abstract":"Much epidemiologic information resides in literature, which is not in a computable format. To extract information and build knowledge bases of epidemiologic studies, we developed a system to extract noun phrases about epidemiologic exposures and outcomes. The system consists of two components: a natural language processing (NLP) engine a machine learning (ML) based classifier. Four ML algorithms were applied and compared over different feature sets. To evaluate the performance of the system, we manually constructed an annotated dataset. The system achieved the highest F-measure of 82.0% for extracting exposure terms, and 70% for extracting outcome terms. Copyright \u00a9 2012 Inderscience Enterprises Ltd.","keywords_author":["Biomedical literature mining","Epidemiologic exposure","Epidemiologic outcome","Epidemiology","Evidence-based medicine","Machine learning","ML","Natural language processing","NLP","Term extraction"],"keywords_other":null,"max_cite":3.0,"pub_year":2012.0,"sources":"['scp']","rawkeys":["biomedical literature mining","nlp","evidence-based medicine","epidemiology","epidemiologic exposure","machine learning","natural language processing","ml","term extraction","epidemiologic outcome"],"tags":["biomedical literature mining","evidence-based medicine","epidemiology","epidemiologic exposure","machine learning","natural language processing","term extraction","epidemiologic outcome"]},{"p_id":81300,"title":"Evaluating Report Text Variation and Informativeness: Natural Language Processing of CT Chest Imaging for Pulmonary Embolism","abstract":"Objective: The aim of this study was to quantify the variability of language in free text reports of pulmonary embolus (PE) studies and to gauge the informativeness of free text to predict PE diagnosis using machine learning as proxy for human understanding.","keywords_author":["Structured reporting","text analysis","pulmonary embolus","machine learning","variability","prediction","natural language processing","NLP"],"keywords_other":["RADIOLOGY REPORTS","CLASSIFICATION","BIG DATA","IMPACT","MEDICINE","ENHANCEMENT","CONVOLUTIONAL NEURAL-NETWORKS"],"max_cite":1.0,"pub_year":2018.0,"sources":"['wos']","rawkeys":["structured reporting","nlp","enhancement","big data","prediction","machine learning","natural language processing","variability","medicine","pulmonary embolus","radiology reports","text analysis","classification","convolutional neural-networks","impact"],"tags":["structured reporting","enhancement","big data","prediction","machine learning","natural language processing","variability","medicine","pulmonary embolus","radiology reports","text analysis","classification","convolutional neural network","impact"]},{"p_id":50581,"title":"A multi-layer system for semantic textual similarity","abstract":"Copyright \u00a9 2016 by SCITEPRESS - Science and Technology Publications, Lda. All rights reserved. Building a system able to cope with various phenomena which falls under the umbrella of semantic similarity is far from trivial. It is almost always the case that the performances of a system do not vary consistently or predictably from corpora to corpora. We analyzed the source of this variance and found that it is related to the word-pair similarity distribution among the topics in the various corpora. Then we used this insight to construct a 4-module system that would take into consideration not only string and semantic word similarity, but also word alignment and sentence structure. The system consistently achieves an accuracy which is very close to the state of the art, or reaching a new state of the art. The system is based on a multi-layer architecture and is able to deal with heterogeneous corpora which may not have been generated by the same distribution.","keywords_author":["Machine Learning","Natural Language Processing (NLP)","Semantic Textual Similarity (STS)."],"keywords_other":["Multi-layer system","Multi-layer architectures","Textual similarities","Semantic similarity","Similarity distribution","State of the art","Sentence structures","NAtural language processing"],"max_cite":0.0,"pub_year":2016.0,"sources":"['scp']","rawkeys":["multi-layer architectures","semantic textual similarity (sts)","similarity distribution","state of the art","natural language processing","machine learning","natural language processing (nlp)","semantic similarity","multi-layer system","sentence structures","textual similarities"],"tags":["multi-layer architectures","semantic textual similarity (sts)","similarity distribution","state of the art","natural language processing","machine learning","semantic similarity","sentence structures","textual similarities","multi-layered systems"]},{"p_id":30104,"title":"Interdisciplinary contributions to flame modeling","abstract":"The world-wide emerging e-society generates new ways to communicate among people with different cultures and backgrounds. Communication systems as forums, blogs, and comments are widely used being easily accessible to end users. Studying and interpreting user generated data\/text available on the Internet is a complex and time consuming duty for any human analyst. This study proposes an interdisciplinary approach to modeling the flaming phenomenon (hot, aggressive discussions) in on-line Italian forums. The model is based on the analysis of psycho\/cognitive\/linguistic interaction modalities among participants to web communities and on state-of-the art machine learning techniques and natural language processing technology. This research gives the opportunity to better understand and model the dynamics of web forums, including the language involved, the interaction between users, the relation between topic and users, language intensity and differences in behavior by age and gender. \u00a9 2011 Springer-Verlag Berlin Heidelberg.","keywords_author":["flame wars","flames identification","flaming","machine learning","natural language processing","opinion mining","web forums"],"keywords_other":["flame wars","Machine-learning","Web Forums","flaming","Flame wars","NAtural language processing","Flaming","Opinion mining"],"max_cite":4.0,"pub_year":2011.0,"sources":"['scp']","rawkeys":["flames identification","flame wars","machine learning","natural language processing","web forums","machine-learning","flaming","opinion mining"],"tags":["flames identification","flame wars","natural language processing","machine learning","web forums","flaming","opinion mining"]},{"p_id":48540,"title":"CKIP Valence-Arousal Predictor for IALP 2016 Shared Task","abstract":"\u00a9 2016 IEEE. Sentiment analysis is an important task in natural language processing and computational linguistics. Automatic sentiment analysis has been widely applied to opinion reviews and social media for a variety of applications, such as marketing and customer services. The dimensional approach can provide more fine-grained sentiment analysis in which each vocabulary is assigned two continuous numerical values-valence and arousal. Our goal is to predict the both values for the unseen vocabularies. In this paper we propose a combination of three rating predictors-E-HowNet knowledge based, word embedding based and single character based predictors to predict Chinese vocabularies. In the IALP 2016 Shared Task (Dimensional Sentiment Analysis for Chinese Words), out of 32 teams, our system ranks top1 on the prediction of valence, and ranks top14 on the prediction of arousal.","keywords_author":["Deep Learning","Dimensional Approach","Knowledge Base","Sentiment Analysis","Word Embedding"],"keywords_other":["Dimensional Approach","Knowledge based","Numerical values","Sentiment analysis","Customer services","Word Embedding","Knowledge base","NAtural language processing"],"max_cite":0.0,"pub_year":2017.0,"sources":"['scp']","rawkeys":["knowledge based","deep learning","dimensional approach","natural language processing","knowledge base","word embedding","customer services","sentiment analysis","numerical values"],"tags":["dimensional approach","machine learning","knowledge base","natural language processing","word embedding","customer services","sentiment analysis","numerical values"]},{"p_id":38301,"title":"An approach for semantic business process model matching using supervised machine learning","abstract":"Matching business process models and their node labels plays an important role for business process management. Many matching algorithms using natural language processing (NLP) techniques exist but do not exploit the opportunities of machine learning though it is generally agreed that a learning approach has great potential in the field of NLP. Therefore, we develop a matching approach based on supervised learning using a language-driven similarity function in order to reproduce a human judgement. Additionally, we implement and evaluate our approach using established quality measures, consisting of precision, recall and F-measure. We conduct an evaluation based on real world process models that demonstrates the potential and the limitations of our machine learning approach. The results show a significant learning effect for matching unknown models without predefined rules. The matching quality is comparable to existing matchers. However, the matching quality seems to depend on the one hand on the available training data and on the other hand on the complexity of the chosen similarity function. Further research efforts have to be undertaken in order to improve our approach. This will include developing a more elaborate similarity function containing more linguistic characteristics of a label as well as integrating contextual information.","keywords_author":["BPM","Machine learning","Process model matching"],"keywords_other":["Contextual information","Similarity functions","Machine learning approaches","Process model matching","Business process management","Business process model","Supervised machine learning","NAtural language processing"],"max_cite":1.0,"pub_year":2016.0,"sources":"['scp']","rawkeys":["contextual information","process model matching","similarity functions","machine learning","natural language processing","bpm","business process management","business process model","supervised machine learning","machine learning approaches"],"tags":["contextual information","process model matching","similarity functions","machine learning","natural language processing","business process management","business process model","supervised machine learning","machine learning approaches","border pairs method (bpm)"]},{"p_id":46494,"title":"Deep Learning in NLP: Methods and Applications","abstract":"\u00a9 2017, Editorial Board of Journal of the University of Electronic Science and Technology of China. All right reserved. With the rise of deep learning waves, the full force of deep learning methods has hit the Natural Language Process (NLP) and ushered in amazing technological advances in many different application areas of NLP. In this article, we firstly present the development history, main advantages and research situation of deep learning. Secondly, in terms of both feature representation and model theory, we introduces the neural language model and word embedding as the entry point, and present an overview of modeling and implementations of Deep Neural Network (DNN). Then we focus on the newest deep learning models with their wonderful and competitive performances related to different NLP tasks. At last, we discuss and summarize the existing problems of deep learning in NLP with the possible future directions.","keywords_author":["Deep learning","Deep neural networks","Language models","Nature language process","Word embedding"],"keywords_other":["Competitive performance","Language model","Natural language process","Word embedding","Feature representation","Development history","Technological advances","Research situations"],"max_cite":0.0,"pub_year":2017.0,"sources":"['scp']","rawkeys":["competitive performance","technological advances","natural language process","research situations","deep learning","deep neural networks","word embedding","language model","nature language process","feature representation","development history","language models"],"tags":["competitive performance","technological advances","research situations","machine learning","natural language processing","word embedding","language model","convolutional neural network","feature representation","development history"]},{"p_id":40353,"title":"Question answering for not yet semantic web","abstract":"In this paper we present a prototype implementation of the question answering system for one of the inflectional languages - Czech. The presented open domain system is especially effective in answering factual wh-questions about people, dates, names and locations. The answer is constructed on-the-fly from data gathered from the Internet, public ontologies, knowledge of the Czech language, and extensible template system. The system is capable of semiautomatic learning of new templates as well as both statistical and semantic processing of Internet content. \u00a9 2010 Springer-Verlag Berlin Heidelberg.","keywords_author":["information extraction","machine learning","morphology","NLP","question answering","semantic compatibility"],"keywords_other":["semantic compatibility","Machine-learning","Question Answering","NLP","Information Extraction"],"max_cite":1.0,"pub_year":2010.0,"sources":"['scp']","rawkeys":["question answering","semantic compatibility","nlp","information extraction","machine learning","morphology","machine-learning"],"tags":["semantic compatibility","information extraction","natural language processing","machine learning","information retrieval","morphology"]},{"p_id":34210,"title":"Unlock big data emotions: Weighted word embeddings for sentiment classification","abstract":"\u00a9 2016 IEEE. Sentiment classification has gained much attention in big data era. Most existing methods rely on bag-of-words model, which disregard contextual information. In many cases however, the sentiment strength of a word is implicitly associated with its part of speech and context. In this paper, we present a WWE (weighted word embeddings) method that combines word embeddings and part-of-speech (POS) tagging. First, we used a continuous word representations algorithm (Word2Vec) to train a vector model. The algorithm learns the optimal vectors from the context of surrounding words. According to the cosine similarity between the vector of a word and the vectors of seed words, a polarity score of this word can be calculated. The state-of-the-art SyntaxNet was used for POS tagging. We then computed an overall polarity score of the whole sentence by POS weighted polarity scores of words. At the end, majority voting was applied to determine the final polarity. Our experimental results show that the WWE method is performed with promising outcomes. Additionally, the methodology was demonstrated on the 3 Twitter datasets from different domains. The robustness recommends that this method can be applied on other sentiment classification problems or domains. We also compared the performance on various dimensions of the trained models. A higher dimension achieved a better performance.","keywords_author":["Big Data","Cosine Similarity","Machine Learning","Natural Language Processing","Natural Language Understanding","NLP","NLU","Parsey McParseface","Part of Speech","Sentiment Classification","Social Media","SyntaxNet","Twitter","Word Embeddings","Word2Vec"],"keywords_other":["Twitter","Natural language understanding","SyntaxNet","Word2Vec","Social media","Cosine similarity","Sentiment classification","Embeddings","Part Of Speech","NAtural language processing","Parsey McParseface"],"max_cite":2.0,"pub_year":2016.0,"sources":"['scp']","rawkeys":["embeddings","parsey mcparseface","sentiment classification","nlp","syntaxnet","big data","cosine similarity","word2vec","social media","natural language processing","machine learning","twitter","word embeddings","part of speech","nlu","natural language understanding"],"tags":["embeddings","parsey mcparseface","sentiment classification","syntaxnet","big data","cosine similarity","word2vec","social media","natural language processing","machine learning","twitter","word embedding","part of speech","nlu","natural language understanding"]},{"p_id":32163,"title":"Linking DICOM pixel data with radiology reports using automatic semantic annotation","abstract":"Improved access to DICOM studies to both physicians and patients is changing the ways medical imaging studies are visualized and interpreted beyond the confines of radiologists' PACS workstations. While radiologists are trained for viewing and image interpretation, a non-radiologist physician relies on the radiologists' reports. Consequently, patients historically have been typically informed about their imaging findings via oral communication with their physicians, even though clinical studies have shown that patients respond to physician's advice significantly better when the individual patients are shown their own actual data. Our previous work on automated semantic annotation of DICOM Computed Tomography (CT) images allows us to further link radiology report with the corresponding images, enabling us to bridge the gap between image data with the human interpreted textual description of the corresponding imaging studies. The mapping of radiology text is facilitated by natural language processing (NLP) based search application. When combined with our automated semantic annotation of images, it enables navigation in large DICOM studies by clicking hyperlinked text in the radiology reports. An added advantage of using semantic annotation is the ability to render the organs to their default window level setting thus eliminating another barrier to image sharing and distribution. We believe such approaches would potentially enable the consumer to have access to their imaging data and navigate them in an informed manner. \u00a9 2012 SPIE.","keywords_author":["Classification","DICOM","Machine learning","Natural language processing","Regression","Semantic association"],"keywords_other":["Clinical study","NAtural language processing","Textual description","Imaging data","Semantic associations","Image sharing","Semantic annotations","Window level","Image interpretation","Oral communication","Regression","Radiology reports","DICOM","Computed Tomography","Search application","Hyperlinked texts","Image data"],"max_cite":3.0,"pub_year":2012.0,"sources":"['scp']","rawkeys":["oral communication","imaging data","dicom","image sharing","classification","computed tomography","semantic associations","image interpretation","textual description","semantic annotations","machine learning","radiology reports","clinical study","regression","image data","window level","search application","hyperlinked texts","natural language processing","semantic association"],"tags":["oral communication","image interpretation","window level","hyperlinked texts","textual description","dicom","machine learning","natural language processing","image sharing","semantic annotations","radiology reports","clinical study","classification","computed tomography","semantic associations","search application","regression","image data"]},{"p_id":38306,"title":"The relationship between disclosing purchase information and reputation systems in electronic markets","abstract":"In this work we investigate how the introduction of the Verified Purchase (VP) badge on Amazon.com affected both the review helpfulness and the product ratings. We first conduct a propensity score matching study and find that all else equal, camera reviews are on average ranked 7 positions higher than non-VP reviews, while book VP reviews are on average ranked 11 positions higher than non-VP reviews. Next, we use a natural experiment setting to identify whether the entry of the VP feature had an effect on the (1) overall review helpfulness (both VP and non-VP reviews), and (2) average product rating. Our results show that the introduction of VP caused an increase in review helpfulness of 7.7% for books, and 1.7% for electronics. Furthermore, it caused on average an increase of 20 and 18 positions in the ranks on book and electronic products respectively.","keywords_author":["Data analysis","Data mining","Econometric analyses","Electronic markets","Empirical analysis","Experimental economics","Machine learning","Natural language processing","Text mining"],"keywords_other":["Text mining","Empirical analysis","Econometric analysis","Electronic market","Experimental economics","NAtural language processing"],"max_cite":1.0,"pub_year":2016.0,"sources":"['scp']","rawkeys":["data mining","econometric analyses","text mining","electronic market","machine learning","natural language processing","econometric analysis","experimental economics","empirical analysis","data analysis","electronic markets"],"tags":["data mining","econometric analyses","text mining","electronic market","machine learning","natural language processing","econometric analysis","experimental economics","empirical analysis","data analysis"]},{"p_id":38308,"title":"Mention detection for improving coreference resolution in Russian texts: A machine learning approach","abstract":"Coreference resolution task is a well-known NLP application that was proven helpful for all high-level NLP applications: machine translation, summarization, and others. Mention detection is the sub-task of detecting the discourse status of each noun phrase, classifying it as a discourse-new, singleton (mentioned only once) or discourse-old occurrence. It has been shown that this task applied to a coreference resolution system may increase its overall performance. So, we decided to adapt current approaches for English language into Russian. We present some quality results of experiments regarding classifiers for mention detection and their application into the coreference resolution task in Russian languages.","keywords_author":["Coreference resolution","Discourse processing","Discourse-new detection","Machine learning","Natural language processing","Singleton detection"],"keywords_other":null,"max_cite":1.0,"pub_year":2016.0,"sources":"['scp']","rawkeys":["singleton detection","discourse-new detection","natural language processing","discourse processing","machine learning","coreference resolution"],"tags":["singleton detection","discourse-new detection","natural language processing","discourse processing","machine learning","coreference resolution"]},{"p_id":42401,"title":"Machine Learning Approaches on Diagnostic Term Encoding with the ICD for Clinical Documentation","abstract":"\u00a9 2013 IEEE. This work focuses on data mining applied to the clinical documentation domain. Diagnostic terms (DTs) are used as keywords to retrieve valuable information from electronic health records. Indeed, they are encoded manually by experts following the International Classification of Diseases (ICD). The goal of this work is to explore the aid of text mining on DT encoding. From the machine learning (ML) perspective, this is a high-dimensional classification task, as it comprises thousands of codes. This work delves into a robust representation of the instances to improve ML results. The proposed system is able to find the right ICD code among more than 1500 possible ICD codes with 92% precision for the main disease (primary class) and 88% for the main disease together with the nonessential modifiers (fully specified class). The methodology employed is simple and portable. According to the experts from public hospitals, the system is very useful in particular for documentation and pharmacosurveillance services. In fact, they reported an accuracy of 91.2% on a small randomly extracted test. Hence, together with this paper, we made the software publicly available in order to help the clinical and research community.","keywords_author":["Clinical text mining","International Classification of Diseases","machine learning","natural language processing"],"keywords_other":["Text mining","International classification of disease","Google","Informatics","Encyclopedias"],"max_cite":0.0,"pub_year":2018.0,"sources":"['scp']","rawkeys":["informatics","international classification of disease","text mining","machine learning","natural language processing","clinical text mining","international classification of diseases","encyclopedias","google"],"tags":["informatics","text mining","machine learning","natural language processing","clinical text mining","international classification of diseases","encyclopedias","google"]},{"p_id":52647,"title":"A review of statistical and cognitive learning for sentence processing","abstract":"In the research field of natural language processing (NLP), sentence stands a very prominent position in text processing. The procedure of sentence processing acquires technologies at a higher level, such as computing the meaning of a sentence based on analysis of meanings of its individual words. This paper reviews several typical research works for statistical learning, highlights relevant cognitive concepts and models, introduces a cognitive sentence parser based on Simple Recurrent Networks (SRNs), and concludes that further research should be implemented with more cognitive models. \u00a9 2011 IEEE.","keywords_author":["Cognitive","Machine learning","Sentence processing","Statistical"],"keywords_other":["Machine-learning","Statistical","Cognitive model","Simple recurrent networks","Statistical learning","Cognitive","Research fields","Cognitive learning","Natural language processing"],"max_cite":0.0,"pub_year":2011.0,"sources":"['scp']","rawkeys":["statistical","simple recurrent networks","cognitive","research fields","machine learning","natural language processing","sentence processing","cognitive model","cognitive learning","statistical learning","machine-learning"],"tags":["statistics","simple recurrent networks","cognitive modeling","research fields","machine learning","natural language processing","sentence processing","cognitive learning","statistical learning","cognition"]},{"p_id":38312,"title":"Acceleration of word2vec using GPUs","abstract":"\u00a9 Springer International Publishing AG 2016. Word2vec is a widely used word embedding toolkit which generates word vectors by training input corpus. Since word vector can represent an exponential number of word cluster and enables reasoning of words with simple algebraic operations, it has become a widely used representation for the subsequent NLP tasks. In this paper, we present an efficient parallelization of word2vec using GPUs that preserves the accuracy. With two K20 GPUs, the proposed acceleration technique achieves 1.7M words\/sec, which corresponds to about 20\u00d7 of speedup compared to a single-threaded CPU execution.","keywords_author":["CUDA","Machine learning","Natural language processing","Neural network","Word embedding","Word2vec"],"keywords_other":["Word2vec","Word embedding","Exponential numbers","Algebraic operations","Acceleration technique","CUDA","NAtural language processing","Parallelizations"],"max_cite":1.0,"pub_year":2016.0,"sources":"['scp']","rawkeys":["neural network","cuda","word2vec","machine learning","natural language processing","parallelizations","word embedding","acceleration technique","algebraic operations","exponential numbers"],"tags":["cuda","neural networks","word2vec","machine learning","natural language processing","parallelizations","word embedding","acceleration technique","algebraic operations","exponential numbers"]},{"p_id":1441,"title":"An experimental study on joint modeling of mixed-bandwidth data via deep neural networks for robust speech recognition","abstract":"We propose joint modeling strategies leveraging upon large-scale mixed-band training speech for recognition of both narrowband and wideband data based on deep neural networks (DNNs). We utilize conventional down-sampling and up-sampling schemes to go between narrowband and wideband data. We also explore DNN-based speech bandwidth expansion (BWE) to map some acoustic features from narrowband to wideband speech. By arranging narrowband and wideband features at the input or the output level of BWE-DNN, and combining down-sampling and up-sampling data, different DNNs can be established. Our experiments on a Mandarin speech recognition task show that the hybrid DNNs for joint modeling of mixed-band speech yield significant performance gains over both the narrowband and wideband speech models, well-trained separately, with a relative character error rate reduction of 7.9% and 3.9% on narrowband and wideband data, respectively. Furthermore, the proposed strategies also consistently outperform other conventional DNN-based methods.","keywords_author":null,"keywords_other":["Mandarin speech recognition","DNNs","Narrowband","speech recognition","Wideband","acoustic features","Speech","acoustic signal processing","wideband data","narrowband data","Training","bandwidth expansion","Hidden Markov models","neural nets","BWE","joint modeling","deep neural networks","natural language processing","Speech recognition","Acoustics","mixed-bandwidth data"],"max_cite":null,"pub_year":2016.0,"sources":"['ieee']","rawkeys":["speech recognition","bwe","acoustics","acoustic features","acoustic signal processing","speech","wideband data","narrowband data","narrowband","dnns","training","bandwidth expansion","neural nets","hidden markov models","joint modeling","deep neural networks","mandarin speech recognition","natural language processing","wideband","mixed-bandwidth data"],"tags":["speech recognition","convolutional neural network","bwe","acoustics","wide-band","acoustic features","acoustic signal processing","speech","wideband data","narrowband data","neural networks","training","bandwidth expansion","narrow bands","hidden markov models","joint modeling","mandarin speech recognition","natural language processing","mixed-bandwidth data"]},{"p_id":7594,"title":"Learning to rank short text pairs with convolutional deep neural networks","abstract":"\u00a9 2015 ACM. Learning a similarity function between pairs of objects is at the core of learning to rank approaches. In information retrieval tasks we typically deal with query-document pairs, in question answering - question-answer pairs. However, before learning can take place, such pairs needs to be mapped from the original space of symbolic words into some feature space encoding various aspects of their relatedness, e.g. lexical, syntactic and semantic. Feature engineering is often a laborious task and may require external knowledge sources that are not always available or difficult to obtain. Recently, deep learning approaches have gained a lot of attention from the research community and industry for their ability to automatically learn optimal feature representation for a given task, while claiming state-of-the-art performance in many tasks in computer vision, speech recognition and natural language processing. In this paper, we present a convolutional neural network architecture for reranking pairs of short texts, where we learn the optimal representation of text pairs and a similarity function to relate them in a supervised way from the available training data. Our network takes only words in the input, thus requiring minimal preprocessing. In particular, we consider the task of reranking short text pairs where elements of the pair are sentences. We test our deep learning system on two popular retrieval tasks from TREC: Question Answering and Microblog Retrieval. Our model demonstrates strong performance on the first task beating previous state-of-the-art systems by about 3% absolute points in both MAP and MRR and shows comparable results on tweet reranking, while enjoying the benefits of no manual feature engineering and no additional syntactic parsers.","keywords_author":["Convolutional neural networks","Learning to rank","Microblog search","Question answering"],"keywords_other":["Micro-blog","State-of-the-art performance","State-of-the-art system","Question Answering","Feature representation","Convolutional neural network","Learning to rank","NAtural language processing"],"max_cite":136.0,"pub_year":2015.0,"sources":"['ieee', 'scp', 'wos']","rawkeys":["question answering","convolutional neural networks","learning to rank","microblog search","state-of-the-art performance","natural language processing","micro-blog","state-of-the-art system","convolutional neural network","feature representation"],"tags":["microblog search","learning to rank","state-of-the-art performance","natural language processing","information retrieval","state-of-the-art system","convolutional neural network","microblogging","feature representation"]},{"p_id":21931,"title":"A multi-strategy approach to biological named entity recognition","abstract":"Recognizing and disambiguating bio-entities (genes, proteins, cells, etc.) names are very challenging tasks as some biologica databases can be outdated, names may not be normalized, abbreviations are used, syntactic and word order is modified, etc. Thus, the same bio-entity might be written into different ways making searching tasks a key obstacle as many candidate relevant literature containing those entities might not be found. As consequence, the same protein mention but using different names should be looked for or the same discovered protein name is being used to name a new protein using completely different features hence named-entity recognition methods are required. In this paper, we developed a bio-entity recognition model which combines different classification methods and incorporates simple pre-processing tasks for bio-entities (genes and proteins) recognition is presented. Linguistic pre-processing and feature representation for training and testing is observed to positively affect the overall performance of the method, showing promising results. Unlike some state-of-the-art methods, the approach does not require additional knowledge bases or specific-purpose tasks for post processing which make it more appealing. Experiments showing the promise of the model compared to other state-of-the-art methods are discussed. \u00a9 2012 Elsevier Ltd. All rights reserved.","keywords_author":["Bioinformatics","Machine learning","Markov models","Named entity recognition","Natural language processing"],"keywords_other":["Pre-processing","NAtural language processing","Classification methods","State-of-the-art methods","Named entity recognition","Recognition models","Training and testing","Feature representation","Searching task","Post processing","Word orders","Bio-entities","Protein names","Markov model","Additional knowledge"],"max_cite":13.0,"pub_year":2012.0,"sources":"['scp']","rawkeys":["state-of-the-art methods","pre-processing","classification methods","additional knowledge","machine learning","named entity recognition","natural language processing","markov model","training and testing","searching task","post processing","word orders","markov models","bioinformatics","protein names","feature representation","bio-entities","recognition models"],"tags":["state-of-the-art methods","pre-processing","classification methods","search tasks","additional knowledge","machine learning","named entity recognition","natural language processing","markov model","training and testing","word orders","post processing","protein names","bioinformatics","feature representation","bio-entities","recognition models"]},{"p_id":26027,"title":"Morphological analyzer for Malayalam using machine learning","abstract":"An efficient and reliable method for implementing Morphological Analyzer for Malayalam using Machine Learning approach has been presented here. A Morphological Analyzer segments words into morphemes and analyze word formation. Morphemes are smallest meaning bearing units in a language. Morphological Analysis is one of the techniques used in formal reading and writing. Rule based approaches are generally used for building Morphological Analyzer. The disadvantage of using rule based approaches are that if one rule fails it will affect the entire rule that follows, that is each rule works on the output of previous rule. The significance of using machine learning approach arises from the fact that rules are learned automatically from data, uses learning and classification algorithms to learn models and make predictions. The result shows that the system is very effective and after learning it predicts correct grammatical features even forwords which are not in the training set. \u00a9 2012 Springer-Verlag.","keywords_author":["Machine Learning","Morphemes","Morphology","Natural Language Processing"],"keywords_other":["Morphological analysis","Techniques used","Machine-learning","One-rule","Morphemes","Classification algorithm","Morphological analyzer","Rule-based approach","Training sets","NAtural language processing"],"max_cite":7.0,"pub_year":2012.0,"sources":"['scp']","rawkeys":["morphological analyzer","training sets","morphological analysis","machine learning","natural language processing","morphology","techniques used","machine-learning","morphemes","rule-based approach","classification algorithm","one-rule"],"tags":["morpheme","morphological analyzer","training sets","morphological analysis","machine learning","natural language processing","morphology","techniques used","rule-based approach","classification algorithm","one-rule"]},{"p_id":1451,"title":"Evolving deep neural networks: A new prospect","abstract":"The success of Deep Neural Networks (DNNs) for various applications like language processing (NLP), image processing, character recognition inspired to use machine learning (ML) and Evolutionary Computation (EC) techniques for improving learning process. Using evolutionary algorithms to improve the efficiency of deep learning attained some success. However, these techniques are unable to reduce the learning time which is the key concern for deep learning. The main problem with DNN is that, it uses a random topology to start with (similar to artificial neural networks). If the topology is not suitable, training procedure will restart with a new topology and this process continues till expected results are obtained. In this paper, we propose, for the first time, a new prospect for evolving optimized deep neural networks which can provide a warm start to the training process compared to heuristic random initial architecture. We discuss the theoretical approach towards possibility of optimizing the learning process inspired from the existing un-conventional approaches. The training process of DNN with EC approach is faster than regular approach by a considerable difference of over 6 hours for MNIST data set. Further, we observed a considerable improvement in the classification accuracies. Our approaches resulted in an improved classification accuracy of 2% and 4.4%for MNIST data set and 1.2% and 1.4% for IRIS data set compared to heuristic random weights approach. Our initial experimental results prove that evolutionary approaches provides a warm start to the deep learning, thus, reducing the training time.","keywords_author":["Evolving Neural Networks","Deep Neural Networks","Multi-population evolution","Co-evolution"],"keywords_other":["Topology","Encoding","Neural networks","ML","Computer architecture","machine learning","Machine learning","evolutionary algorithms","topology","EC","time training reduction","Training","character recognition","Network topology","neural nets","pattern classification","image processing","deep neural networks","NLP","classification accuracy","natural language processing","MNIST data set","evolutionary computation"],"max_cite":5.0,"pub_year":2016.0,"sources":"['ieee']","rawkeys":["encoding","ec","machine learning","co-evolution","evolutionary algorithms","topology","computer architecture","evolving neural networks","neural networks","time training reduction","training","character recognition","ml","neural nets","pattern classification","network topology","nlp","image processing","deep neural networks","classification accuracy","mnist data set","natural language processing","multi-population evolution","evolutionary computation"],"tags":["convolutional neural network","encoding","evolving neural network","machine learning","evolutionary algorithms","topology","computer architecture","neural networks","time training reduction","training","character recognition","pattern classification","network topology","image processing","classification accuracy","natural language processing","coevolution","multi-population evolution","mnist dataset","evolutionary computation"]},{"p_id":21934,"title":"Applying active learning to assertion classification of concepts in clinical text","abstract":"Supervised machine learning methods for clinical natural language processing (NLP) research require a large number of annotated samples, which are very expensive to build because of the involvement of physicians. Active learning, an approach that actively samples from a large pool, provides an alternative solution. Its major goal in classification is to reduce the annotation effort while maintaining the quality of the predictive model. However, few studies have investigated its uses in clinical NLP. This paper reports an application of active learning to a clinical text classification task: to determine the assertion status of clinical concepts. The annotated corpus for the assertion classification task in the 2010 i2b2\/VA Clinical NLP Challenge was used in this study. We implemented several existing and newly developed active learning algorithms and assessed their uses. The outcome is reported in the global ALC score, based on the Area under the average Learning Curve of the AUC (Area Under the Curve) score. Results showed that when the same number of annotated samples was used, active learning strategies could generate better classification models (best ALC - 0.7715) than the passive learning method (random sampling) (ALC - 0.7411). Moreover, to achieve the same classification performance, active learning strategies required fewer samples than the random sampling method. For example, to achieve an AUC of 0.79, the random sampling method used 32 samples, while our best active learning algorithm required only 12 samples, a reduction of 62.5% in manual annotation effort. \u00a9 2011 Elsevier Inc.","keywords_author":["Active learning","Clinical text classification","Machine learning","Natural language processing"],"keywords_other":["Classification performance","Predictive models","Text classification","Machine-learning","Classification models","Classification tasks","Random sampling","Random sampling method","Manual annotation","Supervised machine learning","Active-learning algorithm","Passive learning","Area under the curves","Active Learning","Natural language processing","Learning curves"],"max_cite":13.0,"pub_year":2012.0,"sources":"['scp']","rawkeys":["manual annotation","area under the curves","random sampling method","text classification","classification performance","active-learning algorithm","predictive models","machine learning","active learning","natural language processing","passive learning","classification tasks","clinical text classification","learning curves","random sampling","machine-learning","supervised machine learning","classification models"],"tags":["manual annotation","text classification","classification performance","active-learning algorithm","predictive models","machine learning","natural language processing","passive learning","classification tasks","learning-curve","clinical text classification","random sampling","random sampling method","supervised machine learning","roc curve","classification models"]},{"p_id":1455,"title":"The effects of deep network topology on mortality prediction","abstract":"Deep learning has achieved remarkable results in the areas of computer vision, speech recognition, natural language processing and most recently, even playing Go. The application of deep-learning to problems in healthcare, however, has gained attention only in recent years, and it's ultimate place at the bedside remains a topic of skeptical discussion. While there is a growing academic interest in the application of Machine Learning (ML) techniques to clinical problems, many in the clinical community see little incentive to upgrade from simpler methods, such as logistic regression, to deep learning. Logistic regression, after all, provides odds ratios, p-values and confidence intervals that allow for ease of interpretation, while deep nets are often seen as `black-boxes' which are difficult to understand and, as of yet, have not demonstrated performance levels far exceeding their simpler counterparts. If deep learning is to ever take a place at the bedside, it will require studies which (1) showcase the performance of deep-learning methods relative to other approaches and (2) interpret the relationships between network structure, model performance, features and outcomes. We have chosen these two requirements as the goal of this study. In our investigation, we utilized a publicly available EMR dataset of over 32,000 intensive care unit patients and trained a Deep Belief Network (DBN) to predict patient mortality at discharge. Utilizing an evolutionary algorithm, we demonstrate automated topology selection for DBNs. We demonstrate that with the correct topology selection, DBNs can achieve better prediction performance compared to several bench-marking methods.","keywords_author":null,"keywords_other":["Topology","health care","Humans","Reproducibility of Results","Machine Learning","electronic medical record","Bioinformatics","Neural networks","network structure","Genomics","healthcare","deep network topology","electronic health records","Machine learning","evolutionary algorithm","deep belief network","intensive care unit patients","Female","learning (artificial intelligence)","Algorithms","Training","Odds Ratio","medical computing","Mortality","belief networks","Network topology","Models, Statistical","Logistic Models","Male","EMR dataset","Electronic Health Records","Neural Networks (Computer)","deep-learning methods","patient mortality prediction","Natural Language Processing","evolutionary computation","Regression Analysis"],"max_cite":1.0,"pub_year":2016.0,"sources":"['ieee']","rawkeys":["statistical","health care","regression analysis","electronic medical record","mortality","network structure","healthcare","deep network topology","machine learning","electronic health records","evolutionary algorithm","logistic models","deep belief network","models","algorithms","evolutionary computation","intensive care unit patients","topology","genomics","learning (artificial intelligence)","neural networks (computer)","neural networks","training","reproducibility of results","humans","medical computing","belief networks","network topology","deep-learning methods","emr dataset","male","natural language processing","patient mortality prediction","odds ratio","bioinformatics","female"],"tags":["health care","regression analysis","electronic medical record","mortality","healthcare","deep network topology","network structures","machine learning","electronic health records","logistic models","evolutionary algorithms","algorithms","evolutionary computation","intensive care unit patients","topology","genomics","neural networks","training","reproducibility of results","humans","deep learning methods","medical computing","belief networks","statistics","network topology","model","emr dataset","male","natural language processing","patient mortality prediction","odds ratios","bioinformatics","female","deep belief networks"]},{"p_id":50602,"title":"PRHLT at PR-SOCO: A regression model for predicting personality traits from source code","abstract":"This paper describes our participation in the PAN@FIRE Personality Recognition in Source Code (PR-SOCO) 2016 shared task. We have proposed two different approaches to tackle this task, on the one hand, each code sample from each author was taken as an independent sample and it was vectorized using word n-grams; on the other hand, all the code from an author was taken as a unique sample, and it was vectorized using word n-grams together with handcrafted features that may determine the personality traits of an author. Regardless of the approach, a regression model was trained to classify the personality traits of the author of a sample of source code. All the systems we have submitted to be evaluated have achieved a root mean square error (RMSE) below the mean RMSE of the participants of the shared task. Moreover, one of our runs, the one that included the hand-crafted features, held the best result in the personality trait Agreeableness. This suggests that in the absence of enough independent samples to train a machine learning system, hand-crafted features are able to obtain better results.","keywords_author":["Author profiling","Machine learning","Natural language processing","Personality recognition","PR-SOCO","Regression","Source code"],"keywords_other":["Regression","Source codes","Personality recognition","Author profiling","NAtural language processing"],"max_cite":0.0,"pub_year":2016.0,"sources":"['scp']","rawkeys":["source codes","natural language processing","machine learning","personality recognition","author profiling","pr-soco","regression","source code"],"tags":["natural language processing","machine learning","personality recognition","author profiling","pr-soco","software","regression"]},{"p_id":21940,"title":"Toward automated consumer question answering: Automatically separating consumer questions from professional questions in the healthcare domain","abstract":"Objective: Both healthcare professionals and healthcare consumers have information needs that can be met through the use of computers, specifically via medical question answering systems. However, the information needs of both groups are different in terms of literacy levels and technical expertise, and an effective question answering system must be able to account for these differences if it is to formulate the most relevant responses for users from each group. In this paper, we propose that a first step toward answering the queries of different users is automatically classifying questions according to whether they were asked by healthcare professionals or consumers. Design: We obtained two sets of consumer questions (\u223c10,000 questions in total) from Yahoo answers. The professional questions consist of two question collections: 4654 point-of-care questions (denoted as PointCare) obtained from interviews of a group of family doctors following patient visits and 5378 questions from physician practices through professional online services (denoted as OnlinePractice). With more than 20,000 questions combined, we developed supervised machine-learning models for automatic classification between consumer questions and professional questions. To evaluate the robustness of our models, we tested the model that was trained on the Consumer-PointCare dataset on the Consumer-OnlinePractice dataset. We evaluated both linguistic features and statistical features and examined how the characteristics in two different types of professional questions (PointCare vs. OnlinePractice) may affect the classification performance. We explored information gain for feature reduction and the back-off linguistic category features. Results: The 10-fold cross-validation results showed the best F1-measure of 0.936 and 0.946 on Consumer-PointCare and Consumer-OnlinePractice respectively, and the best F1-measure of 0.891 when testing the Consumer-PointCare model on the Consumer-OnlinePractice dataset. Conclusion: Healthcare consumer questions posted at Yahoo online communities can be reliably classified from professional questions posted by point-of-care clinicians and online physicians. The supervised machine-learning models are robust for this task. Our study will significantly benefit further development in automated consumer question answering. \u00a9 2011 Elsevier Inc.","keywords_author":["Medical question answering","Natural language processing","Question classification","Supervised machine learning","Support vector machines"],"keywords_other":["Question classification","Question Answering","Support vector","Supervised machine learning","NAtural language processing"],"max_cite":13.0,"pub_year":2011.0,"sources":"['scp', 'wos']","rawkeys":["question answering","medical question answering","natural language processing","question classification","support vector machines","support vector","supervised machine learning"],"tags":["medical question answering","natural language processing","machine learning","information retrieval","question classification","support vector","supervised machine learning"]},{"p_id":19893,"title":"Sentic LDA: Improving on LDA with semantic similarity for aspect-based sentiment analysis","abstract":"\u00a9 2016 IEEE. The advent of the Social Web has provided netizens with new tools for creating and sharing, in a time- and cost-efficient way, their contents, ideas, and opinions with virtually the millions of people connected to the World Wide Web. This huge amount of information, however, is mainly unstructured as specifically produced for human consumption and, hence, it is not directly machine-processable. In order to enable a more efficient passage from unstructured information to structured data, aspect-based opinion mining models the relations between opinion targets contained in a document and the polarity values associated with these. Because aspects are often implicit, however, spotting them and calculating their respective polarity is an extremely difficult task, which is closer to natural language understanding rather than natural language processing. To this end, Sentic LDA exploits common-sense reasoning to shift LDA clustering from a syntactic to a semantic level. Rather than looking at word co-occurrence frequencies, Sentic LDA leverages on the semantics associated with words and multi-word expressions to improve clustering and, hence, outperform state-of-the-art techniques for aspect extraction.","keywords_author":null,"keywords_other":["Multi-word expressions","Natural language understanding","Word co-occurrence","Amount of information","Semantic similarity","State-of-the-art techniques","NAtural language processing","Commonsense reasoning"],"max_cite":19.0,"pub_year":2016.0,"sources":"['scp', 'wos']","rawkeys":["word co-occurrence","amount of information","natural language processing","semantic similarity","commonsense reasoning","multi-word expressions","state-of-the-art techniques","natural language understanding"],"tags":["word co-occurrence","amount of information","natural language processing","semantic similarity","commonsense reasoning","multi-word expressions","state-of-the-art techniques","natural language understanding"]},{"p_id":34228,"title":"Identifying named entities from PubMed\u00ae for enriching semantic categories","abstract":"\u00a9 2015 Kim et al.; licensee BioMed Central. Background: Controlled vocabularies such as the Unified Medical Language System (UMLS\u00ae) and Medical Subject Headings (MeSH\u00ae) are widely used for biomedical natural language processing (NLP) tasks. However, the standard terminology in such collections suffers from low usage in biomedical literature, e.g. only 13% of UMLS terms appear in MEDLINE\u00ae. Results: We here propose an efficient and effective method for extracting noun phrases for biomedical semantic categories. The proposed approach utilizes simple linguistic patterns to select candidate noun phrases based on headwords, and a machine learning classifier is used to filter out noisy phrases. For experiments, three NLP rules were tested and manually evaluated by three annotators. Our approaches showed over 93% precision on average for the headwords, \"gene\", \"protein\", \"disease\", \"cell\" and \"cells\". Conclusions: Although biomedical terms in knowledge-rich resources may define semantic categories, variations of the controlled terms in literature are still difficult to identify. The method proposed here is an effort to narrow the gap between controlled vocabularies and the entities used in text. Our extraction method cannot completely eliminate manual evaluation, however a simple and automated solution with high precision performance provides a convenient way for enriching semantic categories by incorporating terms obtained from the literature.","keywords_author":["Machine learning","Natural language processing","Semantic term extraction"],"keywords_other":["Extraction method","Unified medical language systems","Biomedical literature","Semantic terms","Medical Subject Headings","Humans","Unified Medical Language System","Linguistic patterns","Semantics","Natural Language Processing","PubMed","Automated solutions","Vocabulary, Controlled","Artificial Intelligence","NAtural language processing","Medical subject headings"],"max_cite":2.0,"pub_year":2015.0,"sources":"['scp']","rawkeys":["artificial intelligence","unified medical language systems","unified medical language system","vocabulary","semantics","machine learning","natural language processing","humans","semantic terms","extraction method","automated solutions","linguistic patterns","controlled","medical subject headings","semantic term extraction","biomedical literature","pubmed"],"tags":["unified medical language systems","vocabulary","semantics","machine learning","control","humans","natural language processing","semantic terms","extraction method","automated solutions","linguistic patterns","medical subject headings","semantic term extraction","biomedical literature","pubmed"]},{"p_id":5567,"title":"GENIA corpus - A semantically annotated corpus for bio-textmining","abstract":"Motivation: Natural language processing (NLP) methods are regarded as being useful to raise the potential of text mining from biological literature. The lack of an extensively annotated corpus of this literature, however, causes a major bottleneck for applying NLP techniques. GENIA corpus is being developed to provide reference materials to let NLP techniques work for bio-textmining. Results: GENIA corpus version 3.0 consisting of 2000 MEDLINE abstracts has been released with more than 400000 words and almost 100000 annotations for biological terms. Availability: GENIA corpus is freely available at http:\/\/www-tsujii.is.s.u-tokyo.ac.jp\/GENIA. \u00a9 Oxford University Press 2003; all rights reserved.","keywords_author":["Computational Molecular Biology","Corpus","Information Extraction","Natural Language Processing","Text Mining"],"keywords_other":null,"max_cite":466.0,"pub_year":2003.0,"sources":"['scp', 'wos']","rawkeys":["text mining","information extraction","natural language processing","computational molecular biology","corpus"],"tags":["text mining","information extraction","natural language processing","computational molecular biology","corpus"]},{"p_id":15807,"title":"Bio-medical entity extraction using support vector machines","abstract":"Objective: Support vector machines (SVMs) have achieved state-of-the-art performance in several classification tasks. In this article we apply them to the identification and semantic annotation of scientific and technical terminology in the domain of molecular biology. This illustrates the extensibility of the traditional named entity task to special domains with large-scale terminologies such as those in medicine and related disciplines. Methods and materials: The foundation for the model is a sample of text annotated by a domain expert according to an ontology of concepts, properties and relations. The model then learns to annotate unseen terms in new texts and contexts. The results can be used for a variety of intelligent language processing applications. We illustrate SVMs capabilities using a sample of 100 journal abstracts texts taken from the {human, blood cell, transcription factor} domain of MEDLINE. Results: Approximately 3400 terms are annotated and the model performs at about 74% F-score on cross-validation tests. A detailed analysis based on empirical evidence shows the contribution of various feature sets to performance. Conclusion: Our experiments indicate a relationship between feature window size and the amount of training data and that a combination of surface words, orthographic features and head noun features achieve the best performance among the feature sets tested. \u00a9 2004 Elsevier B.V. All rights reserved.","keywords_author":["Machine learning","MEDLINE","Multi-classifier","Named entity","Natural language processing","Support vector machines","Text mining"],"keywords_other":["Feature window size","Support vector machines (SVM)","Cross-validation tests","Intelligent language processing"],"max_cite":42.0,"pub_year":2005.0,"sources":"['scp']","rawkeys":["medline","named entity","support vector machines (svm)","text mining","cross-validation tests","feature window size","machine learning","natural language processing","multi-classifier","intelligent language processing","support vector machines"],"tags":["medline","text mining","cross-validation tests","feature window size","machine learning","natural language processing","multi-classifier","intelligent language processing","named entities"]},{"p_id":17857,"title":"Exploring hedge identification in biomedical literature","abstract":"We investigate automatic identification of speculative language, or 'hedging', in scientific literature from the biomedical domain. Our contributions include a precise description of the task including annotation guidelines, theoretical analysis and discussion. We show that good agreement can be achieved using our guidelines and present a publicly available benchmark dataset for the task. We argue for separation of the acquisition and classification phases in semi-supervised machine learning, and present a probabilistic acquisition model which is evaluated both theoretically and experimentally. We explore the impact of different sample representations on classification accuracy across the learning curve and demonstrate the effectiveness of using machine learning for the hedge identification task. Finally, we examine the errors made by our approach and point toward avenues for future research. \u00a9 2008 Elsevier Inc. All rights reserved.","keywords_author":["Agreement","Annotation","Classification","Hedging","Machine learning","Natural language processing","Semi-supervised learning"],"keywords_other":["Biomedical literature","Semi-supervised","Classification accuracies","Benchmark dataset","Biomedical domains","Scientific literature","Elsevier (CO)","automatic identification","future research","machine-learning","Precise description","Learning curves"],"max_cite":27.0,"pub_year":2008.0,"sources":"['scp']","rawkeys":["agreement","classification","precise description","biomedical literature","biomedical domains","semi-supervised","machine learning","semi-supervised learning","automatic identification","learning curves","hedging","annotation","classification accuracies","machine-learning","scientific literature","benchmark dataset","natural language processing","future research","elsevier (co)"],"tags":["benchmark datasets","future-research","classification accuracy","semi-supervised","machine learning","natural language processing","semi-supervised learning","agreement","learning-curve","biomedical domain","automatic identification","classification","hedging","precise description","annotation","scientific literature","biomedical literature","elsevier (co)"]},{"p_id":50624,"title":"Research on the text classification based on natural language processing and machine learning","abstract":"\u00a9 2016, Scibulcom Ltd. All rights reserved. In this paper, the author researched on the text classification based on natural language processing and machine learning. Text classification can solve the information chaos to a great extent as a key technology of processing and organizing vast text data. It has very realistic significance for efficient management and effective utilization of information and has gradually been an important research direction in the field of data mining. The author deeply discussed the relationship between text classification and machine learning. The experiment result shows the performance of text classification can be improved by using natural language processing and machine learning.","keywords_author":["Information chaos","Machine learning","Natural language processing","Text classification"],"keywords_other":null,"max_cite":0.0,"pub_year":2016.0,"sources":"['scp']","rawkeys":["natural language processing","information chaos","machine learning","text classification"],"tags":["natural language processing","information chaos","machine learning","text classification"]},{"p_id":13763,"title":"Pattern for python","abstract":"Pattern is a package for Python 2.4+ with functionality for web mining (Google + Twitter + Wikipedia, web spider, HTML DOM parser), natural language processing (tagger\/chunker, n-gram search, sentiment analysis, WordNet), machine learning (vector space model, k-means clustering, Naive Bayes + k-NN + SVM classifiers) and network analysis (graph centrality and visualization). It is well documented and bundled with 30+ examples and 350+ unit tests. The source code is licensed under BSD and available from http:\/\/www.clips.ua.ac.be\/pages\/ pattern.\u00a9 2012 Tom De Smedt and Walter Daelemans.","keywords_author":["Data mining","Graph networks","Machine learning","Natural language processing","Python"],"keywords_other":["Graph networks","Web spiders","Naive bayes","Python","Vector space models","Source codes","Sentiment analysis","Wikipedia","Html doms","SVM classifiers","Web Mining","K-means clustering","Unit tests","Wordnet","NAtural language processing"],"max_cite":71.0,"pub_year":2012.0,"sources":"['scp', 'wos']","rawkeys":["wikipedia","wordnet","data mining","source codes","html doms","graph networks","machine learning","natural language processing","python","svm classifiers","unit tests","vector space models","web mining","k-means clustering","naive bayes","sentiment analysis","web spiders"],"tags":["wikipedia","wordnet","data mining","vector space models","html doms","graph networks","machine learning","natural language processing","python","svm classifiers","web spider","unit testing","software","web mining","k-means clustering","naive bayes","sentiment analysis"]},{"p_id":50625,"title":"GTI at TASS 2016: Supervised approach for aspect based sentiment analysis in twitter","abstract":"This paper describes the participation of the GTI research group of AtlantTIC, University of Vigo, in tass 2016. This workshop is framed within the XXXII edition of the Annual Congress of the Spanish Society for Natural Language Processing event. In this work we propose a supervised approach based on classifiers, for the aspect based sentiment analysis task. Using this technique we managed to improve the performance of previous years, obtaining a solution reecting the actual state-of-The-Art.","keywords_author":["Aspects","Machine learning","Sentiment analysis","SVM","Twitter"],"keywords_other":["Twitter","Previous year","Research groups","Sentiment analysis","State of the art","Aspects","NAtural language processing"],"max_cite":0.0,"pub_year":2016.0,"sources":"['scp']","rawkeys":["state of the art","natural language processing","machine learning","svm","twitter","research groups","sentiment analysis","previous year","aspects"],"tags":["state of the art","natural language processing","machine learning","twitter","research groups","sentiment analysis","previous year","aspects"]},{"p_id":32197,"title":"Flames recognition for opinion mining","abstract":"The emerging world-wide e-society creates new ways of interaction between people with different cultures and backgrounds. Communication systems as forums, blogs, and comments are easily accessible to end users. In this context, user generated content management revealed to be a difficult but necessary task. Studying and interpreting user generated data\/text available on the Internet is a complex and time consuming task for any human analyst. This study proposes an interdisciplinary approach to modelling the flaming phenomena (hot, aggressive discussions) in online Italian forums. The model is based on the analysis of psycho\/cognitive\/linguistic interaction modalities among web communities' participants, state-of-the art machine learning techniques and natural language processing technology. Virtual communities' administrators, moderators and users could benefit directly from this research. A further positive outcome of this research is the opportunity to better understand and model the dynamics of web forums as the base for developing opinion mining applications focused on commercial applications.","keywords_author":["Flame wars","Flaming","Machine learning","Natural language processing","Opinion mining","Web forums"],"keywords_other":null,"max_cite":3.0,"pub_year":2011.0,"sources":"['scp']","rawkeys":["flame wars","natural language processing","machine learning","web forums","flaming","opinion mining"],"tags":["flame wars","natural language processing","machine learning","web forums","flaming","opinion mining"]},{"p_id":26055,"title":"Classifier-based acronym extraction for business documents","abstract":"Acronym extraction for business documents has been neglected in favor of acronym extraction for biomedical documents. Although there are overlapping challenges, the semi-structured and non-predictive nature of business documents hinder the effectiveness of the extraction methods used on biomedical documents and fail to deliver the expected performance. A classifier-based extraction subsystem is presented as part of the wider project, Binocle, for the analysis of French business corpora. Explicit and implicit acronym presentation cases are identified using textual and syntactical hints. Among the 7 features extracted from each candidate instance, we introduce \"similarity\" features, which compare a candidate's characteristics with average length-related values calculated from a generic acronym repository. Commonly used rules for evaluating the candidate (matching first letters, ordered instances, etc.) are scored and aggregated in a single composite feature that permits a supple classification. One hundred and thirty-eight French business documents from 14 public organizations were used for the training and evaluation corpora, yielding a recall of 90. 9% at a precision level of 89. 1% for a search space size of 3 sentences. \u00a9 2010 Springer-Verlag London Limited.","keywords_author":["Acronym extraction","Business document mining","Classification","Information extraction","Machine learning","Natural language processing","Similarity feature"],"keywords_other":null,"max_cite":7.0,"pub_year":2011.0,"sources":"['scp']","rawkeys":["information extraction","business document mining","machine learning","natural language processing","acronym extraction","classification","similarity feature"],"tags":["information extraction","business document mining","machine learning","natural language processing","acronym extraction","classification","similarity feature"]},{"p_id":48584,"title":"Verb Sense Disambiguation: A study about the performance of argumental semantic information Desambiguaci\u00f3n Verbal Autom\u00e1tica: Un estudio sobre el rendimiento de la informaci\u00f3n sem\u00e1ntica argumental","abstract":"\u00a9 2017 Sociedad Espa\u00f1ola para el Procesamiento del Lenguaje Natural. One of the key tasks for resolving the ambiguity in the field of Natural Language Processing is Word Sense Disambiguation; especially the specific task of Verb Sense Disambiguation (VSD). In the present study an experimental task is performed in order to test the feasibility of an approach to VSD based on semantic information about verbal arguments. The good results obtained indicate the need to take into account this information in future proposals for VSD.","keywords_author":["Machine learning","Natural language processing","Semantics","Verb sense disambiguation"],"keywords_other":null,"max_cite":0.0,"pub_year":2017.0,"sources":"['scp']","rawkeys":["natural language processing","semantics","verb sense disambiguation","machine learning"],"tags":["natural language processing","semantics","verb sense disambiguation","machine learning"]},{"p_id":28105,"title":"Reviewing classification approaches in sentiment analysis","abstract":"\u00a9 Springer Science+Business Media Singapore 2015. The advancement of web technologies has changed the way people share and express their opinions. People enthusiastically shared their thoughts and opinions via online media such as forums, blogs and social networks. The overwhelmed of online opinionated data have gained much attention by researchers especially in the field of text mining and natural language processing (NLP) to study in depth about sentiment analysis. There are several methods in classifying sentiment, including lexicon-based approach and machine learning approach. Each approach has its own advantages and disadvantages. However, there are not many literatures deliberate on the comparison of both approaches. This paper presents an overview of classification approaches in sentiment analysis. Various advantages and limitations of the sentiment classification approaches based on several criteria such as domain, classification type and accuracy are also discussed in this paper.","keywords_author":["Lexicon-based","Machine learning","Sentiment classification","Subjectivity sentiment analysis"],"keywords_other":["Web technologies","Classification approach","Online media","Machine learning approaches","Lexicon-based","Sentiment analysis","Sentiment classification","NAtural language processing"],"max_cite":5.0,"pub_year":2015.0,"sources":"['scp']","rawkeys":["sentiment classification","online media","natural language processing","machine learning","classification approach","web technologies","subjectivity sentiment analysis","sentiment analysis","lexicon-based","machine learning approaches"],"tags":["sentiment classification","online media","natural language processing","machine learning","classification approach","web technologies","subjectivity sentiment analysis","sentiment analysis","lexicon-based","machine learning approaches"]},{"p_id":26058,"title":"Feature selection for high dimensional data: An evolutionary filter approach","abstract":"Problem statement: Feature selection is a task of crucial importance for the application of machine learning in various domains. In addition, the recent increase of data dimensionality poses a severe challenge to many existing feature selection approaches with respect to efficiency and effectiveness. As an example, genetic algorithm is an effective search algorithm that lends itself directly to feature selection; however this direct application is hindered by the recent increase of data dimensionality. Therefore adapting genetic algorithm to cope with the high dimensionality of the data becomes increasingly appealing. Approach: In this study, we proposed an adapted version of genetic algorithm that can be applied for feature selection in high dimensional data. The proposed approach is based essentially on a variable length representation scheme and a set of modified and proposed genetic operators. To assess the effectiveness of the proposed approach, we applied it for cues phrase selection and compared its performance with a number of ranking approaches which are always applied for this task. Results and Conclusion: The results provide experimental evidences on the effectiveness of the proposed approach for feature selection in high dimensional data. \u00a9 2011 Science Publications.","keywords_author":["Evaluation function","Feature selection","Filter approach","Genetic algorithm","High dimensional data","Machine Learning (ML)","Mutation operator","Natural language processing","Proposed approach","Search algorithm"],"keywords_other":null,"max_cite":7.0,"pub_year":2011.0,"sources":"['scp']","rawkeys":["high dimensional data","machine learning (ml)","search algorithm","mutation operator","natural language processing","genetic algorithm","evaluation function","feature selection","filter approach","proposed approach"],"tags":["high dimensional data","search algorithms","mutation operator","machine learning","natural language processing","genetic algorithm","evaluation function","feature selection","filter approach","proposed approach"]},{"p_id":34253,"title":"360-MAM-Affect: Sentiment analysis with the Google prediction API and EmoSenticNet","abstract":"\u00a9 2015 ICST.Online recommender systems are useful for media asset management where they select the best content from a set of media assets. We have developed an architecture for 360-MAM-Select, a recommender system for educational video content. 360-MAM-Select will utilise sentiment analysis and gamification techniques for the recommendation of media assets. 360-MAM-Select will increase user participation with digital content through improved video recommendations. Here, we discuss the architecture of 360-MAM-Select and the use of the Google Prediction API and EmoSenticNet for 360-MAM-Affect, 360-MAM-Select's sentiment analysis module. Results from testing two models for sentiment analysis, SentimentClassifer (Google Prediction API) and EmoSenticNetClassifer (Google Prediction API + EmoSenticNet) are promising. Future work includes the implementation and testing of 360-MAM-Select on video data from YouTube EDU and Head Squeeze.","keywords_author":["360-MAM-Affect","360-MAM-Select","affective computing","EmoSenticNet","gamification","Google Prediction API","Head Squeeze","machine learning","natural language processing","recommender system","sentiment analysis","YouTube"],"keywords_other":["Affective Computing","NAtural language processing","360-MAM-Select","360-MAM-Affect","Sentiment analysis","YouTube","EmoSenticNet","Google Prediction API","Head Squeeze","Gamification"],"max_cite":2.0,"pub_year":2015.0,"sources":"['scp']","rawkeys":["gamification","affective computing","machine learning","natural language processing","360-mam-affect","google prediction api","360-mam-select","youtube","head squeeze","sentiment analysis","recommender system","emosenticnet"],"tags":["gamification","affective computing","machine learning","natural language processing","360-mam-affect","google prediction api","360-mam-select","recommender systems","youtube","head squeeze","sentiment analysis","emosenticnet"]},{"p_id":19920,"title":"Automatic ICD-10 classification of cancers from free-text death certificates","abstract":"\u00a9 2015 Elsevier Ireland Ltd. Objective: Death certificates provide an invaluable source for cancer mortality statistics; however, this value can only be realised if accurate, quantitative data can be extracted from certificates - an aim hampered by both the volume and variable nature of certificates written in natural language. This paper proposes an automatic classification system for identifying cancer related causes of death from death certificates. Methods: Detailed features, including terms, n-grams and SNOMED CT concepts were extracted from a collection of 447,336 death certificates. These features were used to train Support Vector Machine classifiers (one classifier for each cancer type). The classifiers were deployed in a cascaded architecture: the first level identified the presence of cancer (i.e., binary cancer\/nocancer) and the second level identified the type of cancer (according to the ICD-10 classification system). A held-out test set was used to evaluate the effectiveness of the classifiers according to precision, recall and F-measure. In addition, detailed feature analysis was performed to reveal the characteristics of a successful cancer classification model. Results: The system was highly effective at identifying cancer as the underlying cause of death (F-measure 0.94). The system was also effective at determining the type of cancer for common cancers (F-measure 0.7). Rare cancers, for which there was little training data, were difficult to classify accurately (F-measure 0.12). Factors influencing performance were the amount of training data and certain ambiguous cancers (e.g., those in the stomach region). The feature analysis revealed a combination of features were important for cancer type classification, with SNOMED CT concept and oncology specific morphology features proving the most valuable. Conclusion: The system proposed in this study provides automatic identification and characterisation of cancers from large collections of free-text death certificates. This allows organisations such as Cancer Registries to monitor and report on cancer mortality in a timely and accurate manner. In addition, the methods and findings are generally applicable beyond cancer classification and to other sources of medical text besides death certificates.","keywords_author":["Cancer classification","Death certificates","Machine learning","Natural language processing"],"keywords_other":["Automatic identification","Cause of Death","Registries","Classification system","Death Certificates","Humans","Cancer classification","Neoplasms","Natural Language Processing","Program Evaluation","International Classification of Diseases","Machine Learning","New South Wales","Support vector machine classifiers","Death certificates","Automatic classification systems","Type classifications","NAtural language processing"],"max_cite":19.0,"pub_year":2015.0,"sources":"['scp']","rawkeys":["automatic classification systems","support vector machine classifiers","classification system","program evaluation","type classifications","registries","machine learning","natural language processing","humans","neoplasms","cancer classification","new south wales","automatic identification","international classification of diseases","cause of death","death certificates"],"tags":["automatic classification systems","support vector machine classifiers","classification system","program evaluation","type classifications","registry","machine learning","natural language processing","humans","neoplasms","cancer classification","causes of death","new south wales","automatic identification","international classification of diseases","death certificates"]},{"p_id":38352,"title":"A deep two-stream network for bidirectional cross-media information retrieval","abstract":"\u00a9 Springer International Publishing AG 2016. The recent development in deep learning techniques has showed its wide applications in traditional vision tasks like image classification and object detection. However, as a fundamental problem in artificial intelligence that connects computer vision and natural language processing, bidirectional retrieval of images and sentences is not as popular as the traditional problems, and the results are far from satisfying. In this paper, we consider learning a cross-media representation model with a deep two-stream network. Previous models generally use image label information to train the dataset or strictly correspond the local features in images and texts. Unlike those models, we learn globalized local features, which can reflect the salient objects as well as the details in the images and sentences. After mapping the cross-media data into a common feature space, we use max-margin as the criterion function to update the network. The experiment on the dataset of Flickr8k shows that our approach achieves superior performance compared with the state-of-the-art methods.","keywords_author":["Cross-media","Deep learning","Two-stream network"],"keywords_other":["Deep learning","Two-stream","Cross-media information retrieval","State-of-the-art methods","Label information","Cross-media","Criterion functions","NAtural language processing"],"max_cite":1.0,"pub_year":2016.0,"sources":"['scp', 'ieee']","rawkeys":["state-of-the-art methods","cross-media","two-stream","deep learning","natural language processing","label information","cross-media information retrieval","criterion functions","two-stream network"],"tags":["state-of-the-art methods","cross-media","two-stream","machine learning","natural language processing","label information","cross-media information retrieval","criterion functions","two-stream network"]},{"p_id":52691,"title":"Extracts cognitive artifacts from text through combining human and machine learning in an iterative fashion","abstract":"The world network of information is complex and not always organized in a structure useful for human understanding. This paper investigates the need and methods for creating an artificial system that categorizing information similar to the way humans categorize. The system will use Bayesian modeling to model text sentiment. The categorization of text sentiment will be done both by machines and by humans. The hypothesis is that the resultant system will not differ significantly from the accuracy of a control group of human categorizers. This represents a non-standard approach to learning that involves the human and the machine in an iterative learning process. \u00a9 2011 Springer-Verlag.","keywords_author":["categorization","Cognition","learning","machine learning","natural-language processing","sentiment analysis"],"keywords_other":["Machine-learning","natural-language processing","learning","Sentiment analysis","categorization","Cognition"],"max_cite":0.0,"pub_year":2011.0,"sources":"['scp']","rawkeys":["natural-language processing","learning","machine learning","categorization","machine-learning","cognition","sentiment analysis"],"tags":["natural language processing","machine learning","categorization","cognition","sentiment analysis"]},{"p_id":32212,"title":"Evaluating Lexicographer Controlled Semi-automatic Word Sense Disambiguation method in a large scale experiment","abstract":"Word Sense Disambiguation in text remains a difficult problem as the best supervised methods require laborious and costly manual preparation of training data. On the other hand, the unsupervised methods yield significantly lower precision and produce results that are not satisfying for many applications. Recently, an algorithm based on weakly-supervised learning for WSD called Lexicographer-Controlled Semi-automatic Sense Disambiguation (LexCSD) was proposed. The method is based on clustering of text snippets including words in focus. For each cluster we find a core, which is labelled with a word sense by a human, and is used to produce a classifier. Classifiers, constructed for each word separately, are applied to text. The goal of this work is to evaluate LexCSD trained on large volume of untagged text. A comparison showed that the approach is better than most frequent sense baseline in most cases.","keywords_author":["Natural language processing","Semi-supervised machine learning","Word Sense Disambiguation"],"keywords_other":null,"max_cite":3.0,"pub_year":2011.0,"sources":"['scp']","rawkeys":["semi-supervised machine learning","natural language processing","word sense disambiguation"],"tags":["semi-supervised machine learning","natural language processing","word sense disambiguation"]},{"p_id":34261,"title":"Can Topic Modeling Shed Light on Climate Extremes?","abstract":"\u00a9 1999-2011 IEEE.Understanding changes in climate extremes is an urgent challenge. Topic modeling techniques from natural language processing can help scientists learn climate patterns from data. The authors' work extracts global climate patterns from multivariate climate data, modeling relations between variables via latent topics and discovering the probability of each climate topic appearing at different geographical locations.","keywords_author":["climate extremes","climate informatics","latent Dirichlet allocation","machine learning","Scientific computing","topic models","unsupervised learning"],"keywords_other":["Modeling relations","Informatics","Climate extremes","Geographical locations","Topic model","Climate patterns","NAtural language processing","Latent Dirichlet allocation"],"max_cite":2.0,"pub_year":2015.0,"sources":"['scp']","rawkeys":["informatics","scientific computing","latent dirichlet allocation","climate informatics","modeling relations","geographical locations","machine learning","natural language processing","topic model","climate patterns","climate extremes","unsupervised learning","topic models"],"tags":["informatics","scientific computing","linear discriminant analysis","climate informatics","geographic location","modeling relations","topic modeling","machine learning","natural language processing","climate patterns","climate extremes","unsupervised learning"]},{"p_id":26070,"title":"Wikipedia vandalism detection","abstract":"Wikipedia is an online encyclopedia that anyone can access and edit. It has become one of the most important sources of knowledge online and many third party projects rely on it for a wide-range of purposes. The open model of Wikipedia allows pranksters, lobbyists and spammers to attack the integrity of the encyclopedia and this endangers it as a public resource. This is known in the community as vandalism. A plethora of methods have been developed within the Wikipedia and the scientific community to tackle this problem. We have participated in this effort and developed one of the leading approaches. Our research aims to create a fully-working antivandalism system and get it working in the real world. \u00a9 2011 ACM.","keywords_author":["machine learning","natural language processing","reputation","Wikipedia vandalism detection"],"keywords_other":["Online encyclopedia","Third parties","Machine-learning","Scientific community","Wikipedia","Spammers","reputation","Public resources","NAtural language processing"],"max_cite":7.0,"pub_year":2011.0,"sources":"['scp']","rawkeys":["public resources","spammers","third parties","natural language processing","machine learning","online encyclopedia","wikipedia vandalism detection","scientific community","machine-learning","reputation","wikipedia"],"tags":["public resources","spammers","third parties","natural language processing","machine learning","online encyclopedia","wikipedia vandalism detection","scientific community","reputation","wikipedia"]},{"p_id":19927,"title":"Natural language processing technologies in radiology research and clinical applications","abstract":"\u00a9 RSNA, 2016. The migration of imaging reports to electronic medical record systems holds great potential in terms of advancing radiology research and practice by leveraging the large volume of data continuously being updated, integrated, and shared. However, there are significant challenges as well, largely due to the heterogeneity of how these data are formatted. Indeed, although there is movement toward structured reporting in radiology (ie, hierarchically itemized reporting with use of standardized terminology), the majority of radiology reports remain unstructured and use free-form language. To effectively \u201cmine\u201d these large datasets for hypothesis testing, a robust strategy for extracting the necessary information is needed. Manual extraction of information is a time-consuming and often unmanageable task. \u201cIntelligent\u201d search engines that instead rely on natural language processing (NLP), a computer-based approach to analyzing free-form text or speech, can be used to automate this data mining task. The overall goal of NLP is to translate natural human language into a structured format (ie, a fixed collection of elements), each with a standardized set of choices for its value, that is easily manipulated by computer programs to (among other things) order into subcategories or query for the presence or absence of a finding. The authors review the fundamentals of NLP and describe various techniques that constitute NLP in radiology, along with some key applications.","keywords_author":null,"keywords_other":["Electronic Health Records","Humans","Pattern Recognition, Automated","Natural Language Processing","Biomedical Research","Radiology","Machine Learning","Vocabulary, Controlled","Data Mining"],"max_cite":19.0,"pub_year":2016.0,"sources":"['scp', 'wos']","rawkeys":["biomedical research","vocabulary","data mining","automated","machine learning","electronic health records","humans","natural language processing","radiology","controlled","pattern recognition"],"tags":["biomedical research","vocabulary","data mining","automated","control","electronic health records","humans","machine learning","natural language processing","radiology","pattern recognition"]},{"p_id":38361,"title":"Identifying Disease -Treatment Relations Using Machine Learning Approach","abstract":"\u00ef\u00bf\u00bd 2016 The Authors. Identifying the disease treatment relation enables to find what disease a person suffers from and what appropriate treatment can be given to that person. The semantic relation tags namely Cure, Prevent and Sideeffects helps to find out the relationship between disease and treatment. Many methodologies like co-occurrence analysis, rule based methodologies and statistical methods are used in disease treatment relation. However, machine learning is widely used in many applications like protein-protein interaction, extraction of medical knowledge and in health care field. we propose a machine learning approach termed as SMO classification, which uses several features namely medical papers, medical abstracts. Our approach identifies the features namely disease-treat, cure, prevent and sideeffects. The performance can be measured by Accuracy, Precision, F-measure and Recall.","keywords_author":["Machine Learning","Natural Language Processing","SMO Classifcation"],"keywords_other":["Disease treatment","Medical knowledge","Classifcation","Machine learning approaches","Co-occurrence analysis","Protein-protein interactions","Semantic relations","NAtural language processing"],"max_cite":1.0,"pub_year":2016.0,"sources":"['scp']","rawkeys":["classifcation","semantic relations","co-occurrence analysis","smo classifcation","machine learning","disease treatment","medical knowledge","natural language processing","protein-protein interactions","machine learning approaches"],"tags":["classifcation","semantic relations","co-occurrence analysis","smo classifcation","machine learning","disease treatment","medical knowledge","natural language processing","protein-protein interactions","machine learning approaches"]},{"p_id":32219,"title":"Text classification using word sequence kernel methods","abstract":"This paper presents a comparison study of two sequence kernels for text classification, namely, all common subsequences and sequence kernel. We consider some variations of the two kernels kernels based on individual features, linear combination of individual kernels and kernels with a factored representation of features and evaluate them in text classification by employing them as similarity functions in a support vector machine. A sentence is represented as a sequence of words along with their lemma and part-of-speech tags. Experiments show that sequence kernel has a clear advantage over all common subsequences. Since the main difference between the two kernels lies in the fact that the frequency of words (objects) is considered in sequence kernel but not in all common subsequences, we conclude that the frequency of words is an important factor in the successful application of kernels to text classification. \u00a9 2011 IEEE.","keywords_author":["Information retrieval","Kernel methods","Machine learning","Natural language processing","Text classification"],"keywords_other":["Part-of-speech tags","Text classification","Machine-learning","Similarity functions","Comparison study","Kernel methods","Common subsequence","Linear combinations","NAtural language processing"],"max_cite":3.0,"pub_year":2011.0,"sources":"['scp']","rawkeys":["linear combinations","common subsequence","similarity functions","machine learning","natural language processing","information retrieval","comparison study","machine-learning","kernel methods","part-of-speech tags","text classification"],"tags":["linear combinations","common subsequence","similarity functions","machine learning","natural language processing","information retrieval","part of speech tagging","comparison study","kernel methods","text classification"]},{"p_id":19932,"title":"Automated risk identification using NLP in cloud based development environments","abstract":"\u00a9 2017 Springer-Verlag Berlin Heidelberg In typical software development practice, the risk assessment is not being done in an integrated manner along with the SDLC life cycle. Mostly, risk assessment is a reactive process carried out either during the deployment process or during the evaluation of software for business. Humans are involved in the risk assessment process which is time consuming, error prone and expensive. The risks identified is also not immediately reflected upon the various people in the software development value chain. This causes the churning rate to find or alleviate risks in the future. In typical SDLC, risks may be developed while coding and it may be evident and would takes different shape as different version of the software gets updated over a period of time. Security aspects of the software solution depends on the code which needs to be consistently tracked on a continuous basis to monitor changes and its related risks, without which vulnerabilities and weakness identification will be reactive. It is always essential to identify risks based on the experience from others that is where the use of risk assessment frameworks would be handy along with vulnerability and weakness database such as common weakness enumeration, common vulnerability enumeration and Exploit DB. In this paper, NLP is implemented using deep learning techniques. This paper addresses the need for automated risk assessments with the help of NLP to auto identify the risks on the analysis of weakness and vulnerabilities.","keywords_author":["Deep learning","NLP","Risk assessment","Risk identification","Vulnerabilities","Weakness"],"keywords_other":["Development environment","Vulnerabilities","Weakness","Learning techniques","Risk assessment framework","Risk Identification","Software development practices","Deployment process"],"max_cite":18.0,"pub_year":2017.0,"sources":"['scp']","rawkeys":["software development practices","nlp","development environment","deep learning","weakness","risk assessment","learning techniques","vulnerabilities","risk identification","risk assessment framework","deployment process"],"tags":["software development practices","development environment","natural language processing","machine learning","weakness","risk assessment","learning techniques","vulnerability","risk identification","risk assessment framework","deployment process"]},{"p_id":32221,"title":"Japanese named entity recognition for question answering system","abstract":"Current question answering (QA) systems usually contain named entity recognizer (NER) as a core component. NER is an important and difficult task in computational linguistics. It plays an important role in natural language processing application such as Question Answering, Machine Translation, and Information Retrieval etc. NER includes the identification and classification of certain proper nouns (like location, organization, person, data, money and others) in a text. The purpose of our study is to recognize and extract the exact Japanese sightseeing domain named entities. It is a basic step for the following processing: question analysis and keyword extraction information retrieval. As well as, through doing the named entity recognition, we consider that it can mine exact information from text document to respond to user. This paper describes how to do the Japanese sightseeing named entity recognition due to we are constructing a Japanese sightseeing question answering system. We adopt the hybrid method which combined with machine learning and rule-base method. In the experiment of Japanese sightseeing domain named entity recognition we have got excellent precision and recalling rates. It shows that our method is effective and can be used in a practical question answering system. \u00a9 2011 IEEE.","keywords_author":["machine learning","named entity recognition","question answering system","rule-base"],"keywords_other":["Text document","Named entities","NAtural language processing","Keyword extraction","Named entity recognition","Question Answering","Proper nouns","Question answering systems","rule-base","Hybrid method","Question analysis","Machine translations","Core components"],"max_cite":3.0,"pub_year":2011.0,"sources":"['scp']","rawkeys":["question answering","question analysis","question answering system","keyword extraction","machine learning","named entity recognition","natural language processing","proper nouns","question answering systems","hybrid method","machine translations","rule-base","named entities","text document","core components"],"tags":["question analysis","rule based","keyword extraction","machine learning","named entity recognition","natural language processing","proper nouns","information retrieval","question answering systems","hybrid method","machine translations","named entities","text document","core components"]},{"p_id":44507,"title":"An exploratory approach to find a novel metric based optimum language model for automatic bangla word prediction","abstract":"\u00a9 2018 MECS.Word completion and word prediction are two important phenomena in typing that have intense effect on aiding disable people and students while using keyboard or other similar devices. Such auto completion technique also helps students significantly during learning process through constructing proper keywords during web searching. A lot of works are conducted for English language, but for Bangla, it is still very inadequate as well as the metrics used for performance computation is not rigorous yet. Bangla is one of the mostly spoken languages (3.05% of world population) and ranked as seventh among all the languages in the world. In this paper, word prediction on Bangla sentence by using stochastic, i.e. N-gram based language models are proposed for auto completing a sentence by predicting a set of words rather than a single word, which was done in previous work. A novel approach is proposed in order to find the optimum language model based on performance metric. In addition, for finding out better performance, a large Bangla corpus of different word types is used.","keywords_author":["Corpus","Eager learning","Language model","Machine learning","N-gram","Natural language processing","Performance metric","Word prediction"],"keywords_other":null,"max_cite":0.0,"pub_year":2018.0,"sources":"['scp']","rawkeys":["n-gram","performance metric","machine learning","natural language processing","language model","eager learning","corpus","word prediction"],"tags":["n-grams","performance metrics","machine learning","natural language processing","language model","eager learning","corpus","word prediction"]},{"p_id":40416,"title":"Information extraction from Wikipedia using pattern learning","abstract":"In this paper we present solutions for the crucial task of extracting structured information from massive free-text resources, such as Wikipedia, for the sake of semantic databases serving upcoming Semantic Web technologies. We demonstrate both a verb frame-based approach using deep natural language processing techniques with extraction patterns developed by human knowledge experts and machine learning methods using shallow linguistic processing. We also propose a method for learning verb frame-based extraction patterns automatically from labeled data. We show that labeled training data can be produced with only minimal human effort by utilizing existing semantic resources and the special characteristics of Wikipedia. Custom solutions for named entity recognition are also possible in this scenario. We present evaluation and comparison of the different approaches for several different relations.","keywords_author":["Information extraction","Machine learning","Natural language processing"],"keywords_other":["Named entity recognition","Semantic resources","Labeled training data","Structured information","Machine learning methods","Semantic Web technology","Linguistic processing","Extraction patterns"],"max_cite":1.0,"pub_year":2010.0,"sources":"['scp']","rawkeys":["machine learning methods","extraction patterns","labeled training data","information extraction","semantic web technology","linguistic processing","machine learning","named entity recognition","natural language processing","semantic resources","structured information"],"tags":["machine learning methods","extraction patterns","labeled training data","information extraction","semantic web technology","linguistic processing","machine learning","named entity recognition","natural language processing","semantic resources","structural information"]},{"p_id":28129,"title":"Extracting Meaningful Entities from Human-generated Tactical Reports","abstract":"\u00a9 2015 The Authors. Published by Elsevier B.V. Military intelligence analysts use automated tools to exploit physics-based sensor data to construct a spatio-temporal picture of adversary entities, networks, and behaviors on the battlefield. Traditionally, tools did not exploit human generated, textual reports, leaving analysts to manually map dots on the map into meaningful entities using background knowledge about adversary equipment, organization, and activity. Current off-the-shelf text extraction techniques underperform on tactical reports due to unique characteristics of the text. Tactical reports typically feature short sentences with simple grammar, but also tend to include jargon and abbreviations, do not follow grammatical rules, and are likely to have spelling errors. Likewise, named entity recognizers have low recall, because few of the names in reports appear in standard dictionaries. We have developed an entity extraction capability tailored to these challenges, and to the specific needs of analysts, as part of a comprehensive exploitation and fusion system. With fewer cues from syntax, our approach uses semantic constraints to disambiguate syntactic patterns, implemented by a hybrid system that post-processes the output from a standard Natural Language Processing (NLP) engine with our custom semantic pattern analysis. Additional functionality extracts military time and location formats - essential elements that enable downstream fusion of extracted entities with sensor information resulting in a compact and meaningful representation of the battlefield situation.","keywords_author":["Entity Extraction","Event Coding","Information Fusion","Machine Learning","NLP"],"keywords_other":["Military intelligence","Semantic constraints","Sensor informations","Entity extractions","Event Coding","Syntactic patterns","Back-ground knowledge","NAtural language processing"],"max_cite":5.0,"pub_year":2015.0,"sources":"['scp']","rawkeys":["semantic constraints","syntactic patterns","military intelligence","nlp","event coding","machine learning","natural language processing","back-ground knowledge","entity extractions","sensor informations","entity extraction","information fusion"],"tags":["semantic constraints","syntactic patterns","military intelligence","event coding","machine learning","natural language processing","back-ground knowledge","entity extractions","sensor informations","information fusion"]},{"p_id":7653,"title":"Data-intensive text processing with MapReduce","abstract":"\u00a9 2010 by Morgan & Claypool. All rights reserved. Our world is being revolutionized by data-driven methods: access to large amounts of data has generated new insights and opened exciting new opportunities in commerce, science, and computing applications. Processing the enormous quantities of data necessary for these advances requires large clusters, making distributed computing paradigms more crucial than ever. MapReduce is a programming model for expressing distributed computations on massive datasets and an execution framework for large-scale data processing on clusters of commodity servers. The programming model provides an easy-to-understand abstraction for designing scalable algorithms, while the execution framework transparently handles many system-level details, ranging from scheduling to synchronization to fault tolerance. This book focuses on MapReduce algorithm design, with an emphasis on text processing algorithms common in natural language processing, information retrieval, and machine learning. We introduce the notion of MapReduce design patterns, which represent general reusable solutions to commonly occurring problems across a variety of problem domains. This book not only intends to help the reader \"think in MapReduce\", but also discusses limitations of the programming model as well.","keywords_author":["Algorithm design","Hadoop","Information retrieval","Machine learning","Natural language processing","Parallel and distributed programming","Text processing"],"keywords_other":["Scalable algorithms","Computing applications","Large amounts of data","Large-scale data processing","Distributed computations","Parallel and distributed programming","Hadoop","Algorithm design"],"max_cite":133.0,"pub_year":2010.0,"sources":"['scp']","rawkeys":["algorithm design","distributed computations","hadoop","scalable algorithms","computing applications","machine learning","natural language processing","information retrieval","large amounts of data","parallel and distributed programming","large-scale data processing","text processing"],"tags":["algorithm design","distributed computing","hadoop","scalable algorithms","computing applications","machine learning","natural language processing","information retrieval","large amounts of data","parallel and distributed programming","large-scale data processing","text processing"]},{"p_id":38375,"title":"Information extraction for personalised services based on conference alerts","abstract":"\u00a9 2016 Inderscience Enterprises Ltd.Text mining is moderately new research area at the interaction of data mining, natural language processing (NLP), machine learning and information retrieval. The interconnected task, information extraction is a text transforming that places a specified set of significant items in a natural-language document. It distils organised data or knowledge from unstructured text by recognising references to named entities and additionally expressed relationships between such entities. We present a new schema for text mining as information extraction for prediction, which uses a learn information extraction system to transform text into more structures data which is then be further analysed or mine for discovering more general patterns and interesting relationships. This paper presents the work obtained by applying information extraction (IE) technique to a corpus of conference announcement posted on conference web newsgroups. The work is analysis of extracted essential name entities that were used to find the patterns of recent trends in research area and it also provide a platform to explore more on NLP aspects.","keywords_author":["Data mining","Information extraction","Information retrieval","Machine learning","Natural language processing","NLP","Text mining","Web mining"],"keywords_other":null,"max_cite":1.0,"pub_year":2016.0,"sources":"['scp']","rawkeys":["data mining","nlp","text mining","information extraction","machine learning","natural language processing","information retrieval","web mining"],"tags":["data mining","text mining","information extraction","machine learning","natural language processing","information retrieval","web mining"]},{"p_id":40427,"title":"Sales intelligence using web mining","abstract":"This paper presents a knowledge extraction system for providing sales intelligence based on information downloaded from the WWW. The information is first located and downloaded from relevant companies' websites and then machine learning is used to find these web pages that contain useful information where useful is defined as containing news about orders for specific products. Several machine learning algorithms were tested from which k-nearest neighbour, support vector machines, multi-layer perceptron and C4.5 decision tree produced best results in one or both experiments however k-nearest neighbour and support vector machines proved to be most robust which is a highly desired characteristic in the particular application. K-nearest neighbour slightly outperformed the support vector machines in both experiments which contradicts the results reported previously in the literature. \u00a9 2009 Springer Berlin Heidelberg.","keywords_author":["Machine learning","Natural language processing","Text mining","Web content mining"],"keywords_other":["Text mining","NAtural language processing","Knowledge extraction","K-nearest neighbours","Machine-learning","Machine learning algorithms","Web content mining","Web Mining","Web page","Multi layer perceptron"],"max_cite":1.0,"pub_year":2009.0,"sources":"['scp']","rawkeys":["text mining","machine learning algorithms","k-nearest neighbours","machine learning","natural language processing","web content mining","knowledge extraction","web page","machine-learning","web mining","multi layer perceptron"],"tags":["text mining","machine learning algorithms","web content mining","machine learning","natural language processing","knowledge extraction","web page","multi layer perceptron","web mining","k-nearest neighbors"]},{"p_id":13804,"title":"Recognizing Textual Entailment: Models and Applications","abstract":"\u00a9 Morgan and Claypool Publishers. All rights reserved. Download Free Sample In the last few years, a number of NLP researchers have developed and participated in the task of Recognizing Textual Entailment (RTE). This task encapsulates Natural Language Understanding capabilities within a very simple interface: Recognizing when the meaning of a text snippet is contained in the meaning of a second piece of text. This simple abstraction of an exceedingly complex problem has broad appeal partly because it can be conceived also as a component in other NLP applications, from Machine Translation to Semantic Search to Information Extraction. It also avoids commitment to any specific meaning representation and reasoning framework, broadening its appeal within the research community. This level of abstraction also facilitates evaluation, a crucial component of any technological advancement program. This book explains the RTE task formulation adopted by the NLP research community, and gives a clear overview of research in this area. It draws out commonalities in this research, detailing the intuitions behind dominant approaches and their theoretical underpinnings. This book has been written with a wide audience in mind, but is intended to inform all readers about the state of the art in this fascinating field, to give a clear understanding of the principles underlying RTE research to date, and to highlight the short- A nd long-term research goals that will advance this technology.","keywords_author":["knowledge acquisition","machine learning","natural language processing","textual entailment","textual inference"],"keywords_other":["Natural language understanding","Machine translations","Textual entailment","Technological advancement","Research communities","textual inference","Recognizing textual entailments","Level of abstraction"],"max_cite":70.0,"pub_year":2013.0,"sources":"['scp']","rawkeys":["knowledge acquisition","technological advancement","level of abstraction","recognizing textual entailments","natural language processing","machine learning","textual entailment","textual inference","machine translations","research communities","natural language understanding"],"tags":["technological advances","levels of abstraction","knowledge acquisition","recognizing textual entailments","textual entailment","natural language processing","machine learning","textual inference","machine translations","research communities","natural language understanding"]},{"p_id":38387,"title":"Clinical named entity recognition: Challenges and opportunities","abstract":"\u00a9 2016 IEEE. Information Extraction (IE), one of the important tasks in text analysis and Natural Language Processing (NLP), involves extracting meaningful pieces of knowledge from unstructured information sources, as unstructured data is computationally opaque. The intent of IE is to produce a knowledge base i.e. organize the information in a way that it is useful to people and arrange the information in a semantic way so that algorithms can make certain useful inferences from it. Named Entity Recognition (NER) is a sub-task of IE which finds and classifies the names\/entities. Once these Named Entities (NE) are extracted, they can then be indexed and made searchable, relations can be derived, questions can be answered and many more. NER techniques are different for different domains, because of the uniqueness that exists in each of the domains, although the process depends on a number of fundamental Natural Language Processing (NLP) steps such as tokenization, part-of-speech tagging, parsing and model building. As an example, NER in the medical domain involves handling of a number of vital tasks such as identification of medical terms, attributes such as negation, severity, identification of relationships between entities and mapping terms in the document to concepts in domain specific ontologies. There is also a heavy dependence on domain specific resources such as medical dictionaries and ontologies such as the Unified Medical Language System (UMLS)[34]. In this paper, we focus on NER in the clinical domain. In particular, we will focus on the NER challenges and the qualitative analysis of clinical reports on the approaches we took for the named entities: anatomies, findings, location qualifier, and procedures.","keywords_author":["Deep Learning","Language Modelling","Long short-term memory","Machine Learning","Natural Language Processing","Ontologies","Text Analytics"],"keywords_other":["Part of speech tagging","Unified medical language systems","Named entity recognition","Domain-specific ontologies","Relationships between entities","Language modelling","Text analytics","NAtural language processing"],"max_cite":1.0,"pub_year":2016.0,"sources":"['scp']","rawkeys":["relationships between entities","unified medical language systems","domain-specific ontologies","text analytics","deep learning","language modelling","long short-term memory","named entity recognition","machine learning","natural language processing","part of speech tagging","ontologies"],"tags":["relationships between entities","unified medical language systems","domain-specific ontologies","text analytics","long short-term memory","named entity recognition","machine learning","natural language processing","part of speech tagging","language model"]},{"p_id":50675,"title":"Algorithmic content generation for products","abstract":"\u00a9 2015 IEEE.Content is one of the most essential parts of products on e-commerce websites such as eBay. It not only drives user-engagement but also traffic from various search engine websites based on the relevance. Generating the content for the products, however comes with a wide set of challenges, due to the complexity of commerce at scale, and requires new applications in text processing and information extraction to address some core issues. Some of the factors which need to be addressed are: scalability (millions of products), dynamism (products change with time), removal of item-specific or seller specific information (maintain generality), size of the content etc. Generally, curators are hired for writing the product descriptions manually, which is not cost-effective and is not scalable. In the current work, an algorithmic framework based on Natural Language Processing and Deep Learning is proposed and used to generate the content for ecommerce products. Seller descriptions for multiple items aggregated at a product level are used for content generation. Furthermore, a combination of behavioral and text signals such as search queries are also used to understand the user intent. Two different approaches are proposed in this work: Extraction (sentence retrieval) and Abstraction (sentence generation). The results of both the methods are analyzed and it is depicted that algorithmic content generation is scalable, fast and has potential to cut down the manualcuration cost dramatically.","keywords_author":["content generation","data mining","Deep Learning","e-commerce","Natural Language Processing","Recurrent Neural Networks","summarization","text mining","word vectors"],"keywords_other":["Deep learning","Text mining","Word vectors","summarization","content generation","NAtural language processing"],"max_cite":0.0,"pub_year":2015.0,"sources":"['scp']","rawkeys":["e-commerce","data mining","text mining","deep learning","natural language processing","word vectors","recurrent neural networks","summarization","content generation"],"tags":["e-commerce","data mining","text mining","neural networks","natural language processing","machine learning","word vectors","summarization","content generation"]},{"p_id":46581,"title":"Learning Data Privacy and Terms of Service from Different Cloud Service Providers","abstract":"\u00a9 2017 IEEE. People are using on daily basis websites, mobile applications, and online software nowadays. Major mobile and desktop software applications have been moved to cloud computing environment that allows users to interact with a variety of applications on the move and pay only for additional usage of services on-demand. Each online application has its own terms of service, data privacy agreement that needs to be signed by users even if users are not able to read all documents. In addition, the online service providers collect a variety of information from online users depending on their agreement. This variety of agreements, terms and policies might be challenging for users who use several online applications every day. Data privacy plays a key role in the age of online information centric. It might be more challenging for the users who subscribed to several accounts from different online service providers. In this study, we proposed a machine learning-based method that generates a vector representation of the terms of service. Then, it generates a matrix of the terms of service for different cloud service providers. Finally, it employs a clustering algorithm to analyze the collected data and it monitors data privacy of users in real-time according to written terms of service from cloud service providers.","keywords_author":["data privacy","machine learning","natural language processing","terms of service"],"keywords_other":["Cloud service providers","Cloud computing environments","Mobile applications","On-line applications","Vector representations","On-line information","Terms of services","Privacy agreements"],"max_cite":0.0,"pub_year":2017.0,"sources":"['scp']","rawkeys":["terms of services","mobile applications","on-line applications","data privacy","machine learning","natural language processing","cloud service providers","terms of service","on-line information","vector representations","privacy agreements","cloud computing environments"],"tags":["terms of services","mobile applications","on-line applications","data privacy","machine learning","natural language processing","cloud service providers","on-line information","vector representations","privacy agreements","cloud computing environments"]},{"p_id":22007,"title":"Automated identification of LTL patterns in natural language requirements","abstract":"Analyzing requirements for consistency and checking them for correctness can require significant effort, particularly if they have not been maintained with a requirements management tool (e.g., DOORS) or specified in a machine-readable notation. By restricting the number of requirements being analyzed, fewer opportunities exist for introducing errors into the analysis. This can be accomplished by subsetting the requirements and analyzing one subset at a time. Previous work showed that simple natural language processing and machine learning techniques can be used to identify temporal requirements within a set of natural language requirements. This paper builds on that work by detailing our results in applying these techniques to a set of natural-language temporal requirements taken from a current JPL mission and determining whether a requirement is one of the most frequently occurring types of temporal requirements. The ability to distinguish between different LTL patterns in natural-language requirements raises the possibility of automating the transformation of natural-language temporal requirements into LTL expressions. This would allow automated consistency checking and tracing of naturallanguage temporal requirements. Since correctness properties are often specified as LTL expressions, this would also provide a set of correctness properties against which abstract models of the system could be verified. \u00a9 2009 IEEE.","keywords_author":["Machine learning","Natural language processing","Requirements analysis","Temporal requirements"],"keywords_other":["Automated identification","Consistency checking","Machine-learning","Language requirements","Natural language requirements","Correctness properties","Abstract models","Requirements management tool","Requirements analysis","NAtural language processing","Machine learning techniques"],"max_cite":13.0,"pub_year":2009.0,"sources":"['scp']","rawkeys":["consistency checking","language requirements","natural language requirements","correctness properties","machine learning","machine learning techniques","natural language processing","requirements management tool","requirements analysis","temporal requirements","machine-learning","abstract models","automated identification"],"tags":["consistency checking","language requirements","natural language requirements","correctness properties","machine learning","machine learning techniques","natural language processing","requirements management tool","requirements analysis","temporal requirements","abstract models","automated identification"]},{"p_id":19960,"title":"Sentiment analysis of Turkish political news","abstract":"In this paper, sentiment classification techniques are incorporated into the domain of political news from columns in different Turkish news sites. We compared four supervised machine learning algorithms of Na\u00efve Bayes, Maximum Entropy, SVM and the character based N-Gram Language Model for sentiment classification of Turkish political columns. We also discussed in detail the problem of sentiment classification in the political news domain. We observe from empirical findings that the Maximum Entropy and N-Gram Language Model outperformed the SVM and Na\u00efve Bayes. Using different features, all the approaches reached accuracies of 65% to 77%. \u00a9 2012 IEEE.","keywords_author":["Machine Learning","News Domain","NLP","Sentiment Analysis","Turkish"],"keywords_other":["Turkishs","NLP","Sentiment analysis","News domain","N-gram language models","Supervised machine learning","Sentiment classification","Empirical findings"],"max_cite":18.0,"pub_year":2012.0,"sources":"['scp']","rawkeys":["turkishs","news domain","n-gram language models","sentiment classification","nlp","machine learning","turkish","supervised machine learning","empirical findings","sentiment analysis"],"tags":["turkishs","news domain","n-gram language models","sentiment classification","natural language processing","machine learning","supervised machine learning","empirical findings","sentiment analysis"]},{"p_id":19961,"title":"Comparison of feature selection methods for sentiment analysis","abstract":"Sentiment analysis is a sub-field of Natural Language Processing and involves automatically classifying input text according to the sentiment expressed in it. Sentiment analysis is similar to topical text classification but has a significant contextual difference that needs to be handled. Based on this observation we propose a new feature selection method called Document Frequency Difference to automatically identify the words which are more useful for classifying sentiment. We further compare it to three other feature selection methods and show that it can help improve sentiment classification performance. \u00a9 2010 Springer-Verlag Berlin Heidelberg.","keywords_author":["Feature selection","Maximum entropy modeling","Natural language processing","Sentiment analysis","Text classification"],"keywords_other":["Maximum entropy modeling","Text classification","Sentiment analysis","NAtural language processing","Feature selection"],"max_cite":18.0,"pub_year":2010.0,"sources":"['scp']","rawkeys":["natural language processing","feature selection","sentiment analysis","maximum entropy modeling","text classification"],"tags":["natural language processing","maximum entropy models","feature selection","sentiment analysis","text classification"]},{"p_id":24058,"title":"De-identification of psychiatric intake records: Overview of 2016 CEGS N-GRID shared tasks Track 1","abstract":"\u00a9 2017 Elsevier Inc.The 2016 CEGS N-GRID shared tasks for clinical records contained three tracks. Track 1 focused on de-identification of a new corpus of 1000 psychiatric intake records. This track tackled de-identification in two sub-tracks: Track 1.A was a \u201csight unseen\u201d task, where nine teams ran existing de-identification systems, without any modifications or training, on 600 new records in order to gauge how well systems generalize to new data. The best-performing system for this track scored an F1 of 0.799. Track 1.B was a traditional Natural Language Processing (NLP) shared task on de-identification, where 15 teams had two months to train their systems on the new data, then test it on an unannotated test set. The best-performing system from this track scored an F1 of 0.914. The scores for Track 1.A show that unmodified existing systems do not generalize well to new data without the benefit of training data. The scores for Track 1.B are slightly lower than the 2014 de-identification shared task (which was almost identical to 2016 Track 1.B), indicating that these new psychiatric records pose a more difficult challenge to NLP systems. Overall, de-identification is still not a solved problem, though it is important to the future of clinical NLP.","keywords_author":["Clinical records","Machine learning","Natural language processing","Shared task"],"keywords_other":["Test sets","Shared task","NLP systems","Training data","Existing systems","Clinical records","De-identification"],"max_cite":9.0,"pub_year":2017.0,"sources":"['scp']","rawkeys":["training data","test sets","nlp systems","de-identification","natural language processing","machine learning","shared task","clinical records","existing systems"],"tags":["training data","test sets","nlp systems","de-identification","natural language processing","machine learning","shared task","clinical records","existing systems"]},{"p_id":44538,"title":"Entity disambiguation with memory network","abstract":"\u00a9 2017 Elsevier B.V. We develop a computational approach based on memory network for entity disambiguation. The approach automatically finds important clues of a mention from surrounding contexts with attention mechanism, and leverages these clues to facilitate entity disambiguation. Unlike existing feature-based methods, this approach does not rely on any manually designed features. Unlike existing neural models such as recurrent or convolutional neural network, this approach leverages the importance of context words in an explicit way. The model could be easily trained with back-propagation. To effectively learn the model parameters, we automatically collect large-scale mention-entity pairs from Wikipedia as training data. We verify the effectiveness of the proposed approach on a benchmark dataset from TAC-KBP evaluation in 2010. Experimental results demonstrate that our approach empirically surpasses strong feature based and neural network based methods. Model analysis further reveals that our approach has the capacity to discover important clues from contexts.","keywords_author":["Deep learning","Entity disambiguation","Memory network","Natural language processing"],"keywords_other":["Entity disambiguation","Model parameters","Benchmark datasets","Feature-based method","Convolutional neural network","Memory network","Computational approach","Attention mechanisms"],"max_cite":0.0,"pub_year":2018.0,"sources":"['scp']","rawkeys":["benchmark datasets","entity disambiguation","deep learning","memory network","attention mechanisms","natural language processing","convolutional neural network","computational approach","feature-based method","model parameters"],"tags":["benchmark datasets","entity disambiguation","memory network","attention mechanisms","machine learning","natural language processing","convolutional neural network","computational approach","feature-based method","model parameters"]},{"p_id":48635,"title":"Intent understanding in a virtual agent","abstract":"\u00a9 2017 ACM. This paper discusses the intent recognition system we have built, this system is to be used as part of a virtual agent that can help resolve end user queries. The end user queries are of different intents - request for action, request for information, report of some issue, general greetings. Intent detection is a key component of the virtual agent o decide which type the query belongs to and to further invoke the appropriate action modules. The system uses a combination of machine learning and rules based techniques. The rules based component can be used in an unsupervised mode with only the dictionary databases to be loaded upfront. Classifier is a supervised block which requires training data. The system has a feedback based learning which enables the system's performance to improve with use. This paper brings out the architecture of the intent recognition system, alternate configurations, results obtained and conclusions. The key differentiator of this system is the ability to use this system for different domains with minimal supervision.","keywords_author":["Classifier","Dependency graph","Detection","Ensemble","Feedback","Intent","Learning","Learning","Machine learning","Natural language processing","Scoring","Stemming","Supervised","Tokenization","Unsupervised","Virtual agent"],"keywords_other":["Ensemble","Virtual agent","Supervised","Learning","Stemming","Tokenization","Scoring","Dependency graphs","Unsupervised","Intent"],"max_cite":0.0,"pub_year":2017.0,"sources":"['scp']","rawkeys":["dependency graphs","tokenization","virtual agent","dependency graph","learning","ensemble","machine learning","natural language processing","supervised","stemming","classifier","detection","feedback","unsupervised","intent","scoring"],"tags":["dependency graphs","tokenization","virtual agent","machine learning","ensemble","natural language processing","supervised","score","classifier","detection","feedback","intention","stem"]},{"p_id":30205,"title":"Toward extracting information from public health statutes using text classification machine learning","abstract":"This paper presents preliminary results in extracting semantic information from US state public health legislative provisions using natural language processing techniques and machine learning classifiers. Challenges in the density and distribution of the data as well as the structure of the prediction task are described. Decision tree models trained on a unigram representation with TFIDF measures in most cases outperform the baselines by varying margins, leaving room for further improvement. \u00a9 2011 The authors and IOS Press. All rights reserved.","keywords_author":["machine learning","natural language processing","semantic extraction"],"keywords_other":["Prediction tasks","Text classification","Legislative provisions","Decision tree models","Semantic extraction","NAtural language processing","Extracting information","Semantic information"],"max_cite":4.0,"pub_year":2011.0,"sources":"['scp']","rawkeys":["prediction tasks","semantic extraction","machine learning","natural language processing","extracting information","semantic information","decision tree models","text classification","legislative provisions"],"tags":["prediction tasks","semantic extraction","natural language processing","machine learning","extracting information","semantic information","decision tree modeling","text classification","legislative provisions"]},{"p_id":50685,"title":"Introducing XGL - A lexicalised probabilistic graphical lemmatiser for isiXhosa","abstract":"\u00a9 2015 IEEE. In this paper, a lexicalized probabilistic graphical lemmatiser for isiXhosa, XGL, is presented. An overview of isiXhosa lemmatisation issues is given, followed by a discussion on previous work in automated lemmatisation for isiXhosa. The paper continues to motivate for a machine learning lemmatiser for isiXhosa. IsiXhosa data used to train the lemmatiser is analyzed and the best features are identified from the analysis. The inner workings of XGL are detailed and evaluation results presented. XGL is shown to have achieved accuracy rates of 83.19% on a gold standard of word-lemma pairs, thereby outperforming similar lemmatisers such as LemmaGen's 80.6% and 73.13% from the CST lemmatiser when trained with 35000 word-lemma pairs.","keywords_author":["IsiXhosa","Lemmatisation","Machine Learning","Natural Language Processing"],"keywords_other":["Gold standards","Lemmatisation","Evaluation results","Accuracy rate","IsiXhosa","NAtural language processing"],"max_cite":0.0,"pub_year":2015.0,"sources":"['scp']","rawkeys":["isixhosa","machine learning","natural language processing","accuracy rate","lemmatisation","evaluation results","gold standards"],"tags":["isixhosa","machine learning","natural language processing","accuracy rate","lemmatisation","evaluation results","gold standards"]},{"p_id":44543,"title":"Sentiment analysis approaches for movie reviews forecasting: A survey","abstract":"\u00a9 2017 IEEE. Human psychology has always influenced by others suggestion and reviews. Our reviews about something are very much influenced by other's reviews, and whenever we need to make a decision or solution, we often seek out other's reviews, so people are excited to know other's reviews for their profit that is why automated sentiment analysis systems are required. This Survey Paper is describes basics about sentiment analysis, polarity types, sentiment types, sentiment documentation types, levels of sentiment, General Flow Diagram of System, and also different Approaches and Related Work for better understanding about the system.","keywords_author":["Data Mining","Deep Learning (DL)","Information Retrieval","Machine Learning (ML)","Natural Language Processing (NLP)","Opinion Mining","Predictive Analysis","Sentiment Analysis","SentiWordNet (SWN)","Text Mining"],"keywords_other":["Text mining","Analysis approach","Related works","Flow diagram","Human psychology","Movie reviews","SentiWordNet"],"max_cite":0.0,"pub_year":2018.0,"sources":"['scp']","rawkeys":["related works","human psychology","data mining","machine learning (ml)","text mining","movie reviews","predictive analysis","natural language processing (nlp)","information retrieval","sentiwordnet","sentiwordnet (swn)","analysis approach","flow diagram","opinion mining","sentiment analysis","deep learning (dl)"],"tags":["related works","human psychology","data mining","text mining","movie reviews","predictive analysis","machine learning","natural language processing","information retrieval","sentiwordnet","analysis approach","flow diagram","opinion mining","sentiment analysis"]},{"p_id":38400,"title":"In or out?: Real-time monitoring of BREXIT sentiment on Twitter","abstract":"\u00a9 2016 Copyright held by the author\/owner(s).s). The SSIX (Social Sentiment analysis financial IndeXes) project is a European Innovation Project sponsored by the European Commission under the Horizon 2020 framework. SSIX aims to provide European SMEs with a collection of easy to interpret tools to analyse and understand social media sentiment for any given topic regardless of locale or language. The United Kingdom's recent referendum on European Union membership i.e. staying (\"Bremain\") or leaving the EU (\"Brexit\") was selected for the initial real-world test case for the validating the SSIX methodology and platform. In this paper, we describe the SSIX architecture in brief as well as analysis of the platforms X-Scores metrics and their application to Brexit, our initial experimental results and lessons learned.","keywords_author":["Brexit","Machine learning","Natural language processing","Opinion Mining","Political opinion mining","Sentiment analysis","SSIX","Twitter"],"keywords_other":["Twitter","SSIX","Sentiment analysis","Brexit","NAtural language processing","Opinion mining"],"max_cite":1.0,"pub_year":2016.0,"sources":"['scp']","rawkeys":["brexit","ssix","natural language processing","machine learning","political opinion mining","opinion mining","sentiment analysis","twitter"],"tags":["brexit","ssix","natural language processing","machine learning","political opinion mining","opinion mining","sentiment analysis","twitter"]},{"p_id":48638,"title":"Utilizing hashtags for sentiment analysis of tweets in the political domain","abstract":"\u00a9 2017 ACM. The objective of this research is to investigate the benefit of utilizing hashtags to determine sentiment polarity of tweets in the political domain. We used the sentiment polarity of hashtags as the features in classification, proposed rules for automatically annotating dataset based on the number of positive and negative hashtags in the tweets, and proposed a method to enrich terms in the tweet by extracting hashtag terms. We named the number of positive and negative hashtags as SentiHT feature. The experiments and evaluation show that sentiment classification using SentiHT feature and the automatically labeled dataset using SentiHT has a very good accuracy of more than 95%. Moreover, SentiHT outperforms unigram feature when combined with Na\u00efve Bayes, SVM or Logistic Regression algorithms, but the opposite occurs when using Random Forest algorithm. Based on computing time to build the model, we recommend using SentiHT feature combined with Na\u00efve Bayes algorithm.","keywords_author":["Machine learning","NLP","Politics","Sentiment analysis","Twitter"],"keywords_other":["Twitter","Politics","Logistic regression algorithms","Sentiment analysis","Random forest algorithm","Bayes algorithms","Labeled dataset","Sentiment classification"],"max_cite":0.0,"pub_year":2017.0,"sources":"['scp']","rawkeys":["politics","sentiment classification","nlp","random forest algorithm","labeled dataset","machine learning","bayes algorithms","logistic regression algorithms","sentiment analysis","twitter"],"tags":["politics","sentiment classification","random forest algorithm","labeled dataset","machine learning","natural language processing","bayes algorithms","logistic regression algorithms","sentiment analysis","twitter"]},{"p_id":65027,"title":"A deep source-context feature for lexical selection in statistical machine translation","abstract":"This paper presents a methodology to address lexical disambiguation in a standard phrase-based statistical machine translation system. Similarity among source contexts is used to select appropriate translation units. The information is introduced as a novel feature of the phrase-based model and it is used to select the translation units extracted from the training sentence more similar to the sentence to translate. The similarity is computed through a deep autoencoder representation, which allows to obtain effective low-dimensional embedding of data and statistically significant BLEU score improvements on two different tasks (English-to-Spanish and English-to-Hindi). (C) 2016 Elsevier B.V. All rights reserved.","keywords_author":["Natural language processing","Neural nets and related approaches","Semantics"],"keywords_other":null,"max_cite":2.0,"pub_year":2016.0,"sources":"['wos']","rawkeys":["semantics","natural language processing","neural nets and related approaches"],"tags":["semantics","natural language processing","neural nets and related approaches"]},{"p_id":44549,"title":"Onto-based sentiment classification using machine learning techniques","abstract":"\u00a9 2017 IEEE.Sentiment analysis is a methodology used to analyse the emotion or view of an individual to a situation or topic. In present scenario, Social media is the source for the collection of individual's feedbacks, user's emotions, reviews and personal experiences which lead to a need for efficient mining of the text to derive knowledge. An optimal classification of text based on emotion is an unsolved problem in text mining. To extract knowledge from text many machine learning tools and techniques were proposed. An onto-based process is proposed to analyse the customer's emotion in this paper. The input emotional text that needs to be classified is given as input to the NLP and processed and an emotional ontology is created for better understanding of the semantics and relationships. When adding new instances, Ontology can be automatically classify them based on emotional relationship. The Emowords from ontology can be further classified using any of the standard machine learning techniques which definitively gives a better performance. This paper is a review of all the machine learning techniques that can be applied on the semantic analysis of sentiments.","keywords_author":["machine learning","NLP","ontology","semantics","sentimental analysis"],"keywords_other":["Efficient minings","Personal experience","Semantic analysis","sentimental analysis","Standard machines","Optimal classification","Sentiment classification","Machine learning techniques"],"max_cite":0.0,"pub_year":2018.0,"sources":"['scp']","rawkeys":["efficient minings","sentiment classification","nlp","personal experience","semantics","optimal classification","machine learning techniques","machine learning","ontology","semantic analysis","sentimental analysis","standard machines"],"tags":["efficient minings","sentiment classification","personal experience","semantics","optimal classification","machine learning techniques","natural language processing","machine learning","semantic analysis","sentiment analysis","standard machines"]},{"p_id":32264,"title":"Representation learning via Dual-Autoencoder for recommendation","abstract":"\u00a9 2017 Elsevier Ltd Recommendation has provoked vast amount of attention and research in recent decades. Most previous works employ matrix factorization techniques to learn the latent factors of users and items. And many subsequent works consider external information, e.g., social relationships of users and items\u2019 attributions, to improve the recommendation performance under the matrix factorization framework. However, matrix factorization methods may not make full use of the limited information from rating or check-in matrices, and achieve unsatisfying results. Recently, deep learning has proven able to learn good representation in natural language processing, image classification, and so on. Along this line, we propose a new representation learning framework called Recommendation via Dual-Autoencoder (ReDa). In this framework, we simultaneously learn the new hidden representations of users and items using autoencoders, and minimize the deviations of training data by the learnt representations of users and items. Based on this framework, we develop a gradient descent method to learn hidden representations. Extensive experiments conducted on several real-world data sets demonstrate the effectiveness of our proposed method compared with state-of-the-art matrix factorization based methods.","keywords_author":["Dual-Autoencoder","Matrix factorization","Recommendation","Representation learning"],"keywords_other":["Representation learning","Learning","Humans","Attention","Gradient Descent method","Databases, Factual","Recommendation","External informations","Machine Learning","Auto encoders","Recommendation performance","Matrix factorizations","NAtural language processing"],"max_cite":3.0,"pub_year":2017.0,"sources":"['scp']","rawkeys":["recommendation","dual-autoencoder","recommendation performance","databases","auto encoders","learning","humans","machine learning","attention","gradient descent method","matrix factorization","matrix factorizations","natural language processing","representation learning","factual","external informations"],"tags":["recommendation","dual-autoencoder","recommendation performance","databases","nonnegative matrix factorization","auto encoders","machine learning","humans","natural language processing","attention","gradient descent method","representation learning","factual","external informations"]},{"p_id":9737,"title":"Text feature extraction based on deep learning: a review","abstract":"Selection of text feature item is a basic and important matter for text mining and information retrieval. Traditional methods of feature extraction require handcrafted features. To hand-design, an effective feature is a lengthy process, but aiming at new applications, deep learning enables to acquire new effective feature representation from training data. As a new feature extraction method, deep learning has made achievements in text mining. The major difference between deep learning and conventional methods is that deep learning automatically learns features from big data, instead of adopting handcrafted features, which mainly depends on priori knowledge of designers and is highly impossible to take the advantage of big data. Deep learning can automatically learn feature representation from big data, including millions of parameters. This thesis outlines the common methods used in text feature extraction first, and then expands frequently used deep learning methods in text feature extraction and its applications, and forecasts the application of deep learning in feature extraction.","keywords_author":["Deep learning","Feature extraction","Natural language processing","Text characteristic","Text mining","Deep learning","Feature extraction","Text characteristic","Natural language processing","Text mining"],"keywords_other":["Text mining","DIMENSION REDUCTION","ITS applications","Feature extraction methods","Learning methods","FEATURE-SELECTION","Conventional methods","CLASSIFICATION","ALGORITHM","Feature representation","RECOGNITION","NEURAL-NETWORK","Priori knowledge","Text characteristic"],"max_cite":1.0,"pub_year":2017.0,"sources":"['scp', 'wos']","rawkeys":["algorithm","recognition","text mining","feature extraction methods","deep learning","its applications","learning methods","text characteristic","natural language processing","priori knowledge","conventional methods","feature-selection","classification","feature extraction","neural-network","feature representation","dimension reduction"],"tags":["recognition","text mining","feature extraction methods","neural networks","its applications","learning methods","text characteristic","machine learning","natural language processing","conventional methods","priori knowledge","feature selection","classification","feature extraction","algorithms","feature representation","dimensionality reduction"]},{"p_id":36361,"title":"Visual Question Authentication Protocol (VQAP)","abstract":"\u00a9 2017 Elsevier Ltd Many conventional methods exist to authenticate a user, including text-based systems and graphical systems. While text-based authentication is secure, it is difficult for users to remember very robust passwords. Conversely, while graphical-based passwords that require selection of a \u201ccorrect\u201d image against \u201cincorrect\u201d (or distractor) ones have proven easier to remember, they are vulnerable to an attacker exploiting direct access to the selection (shoulder-surfing) or prior knowledge about the user. In this work, we propose a novel authentication method that combines both graphical-based and text-based features that seeks to mitigate these risks. Our system, Visual Question Authentication Protocol (VQAP) offers enhanced security by introducing (1) a question about a registered image as a cue for a text-based password, (2) a novel machine-learning based classifier for selecting distractor images that are related (or relevant) to the given question, and (3) multiple authentication scenarios, some of which present incorrect information that only the true user should be able to identify. We present experiments for our classifier that validate our ability to separate relevant and irrelevant images suitably for authentication purposes, given a question.","keywords_author":["Computer vision","Cued recall","Deep learning","Graphical authentication","Machine learning","Natural language processing","Visual question answering"],"keywords_other":["Text-based features","Multiple authentications","Cued recalls","Question Answering","Authentication protocols","Authentication methods","Conventional methods","Graphical authentications"],"max_cite":1.0,"pub_year":2018.0,"sources":"['scp']","rawkeys":["graphical authentications","question answering","authentication methods","deep learning","cued recalls","machine learning","conventional methods","natural language processing","graphical authentication","text-based features","visual question answering","cued recall","multiple authentications","computer vision","authentication protocols"],"tags":["graphical authentications","authentication methods","cued recalls","machine learning","conventional methods","natural language processing","information retrieval","text-based features","multiple authentications","computer vision","authentication protocols","video quality assessment"]},{"p_id":52744,"title":"Bag of meta-words: A novel method to represent document for the sentiment classification","abstract":"\u00a9 2018 Elsevier Ltd It is crucial to represent the semantic information of a document in sentiment classification. Various semantic information representation models have been proposed, however existing approaches have their setbacks. Notable weaknesses among these are: (1) tradition VSM methods, completely ignore the semantic information; (2) averaging word embedding methods, cannot depict the synthetical semantic meaning of the given document; (3) neural network methods, require complex structure and are notoriously difficult to be trained. To overcome these limitations, we introduce a simple but novel method which we call bag of meta-words (BoMW). In our method, the semantic information of the document is indicated by a meta-words vector in which every single meta-word element denotes particular semantic information. Especially, these meta-words are extracted from pre-trained word embeddings through two different but complemental models, naive interval meta-words (NIM) and feature combination meta-words (FCM). In general, our new model BoMW is as simple as traditional VSM model but it can capture the synthetical semantic meanings of the document. Numerous experiments on two benchmarks (IMDB dataset and Pang's dataset) are carried out to verify the effectiveness of the proposed method, and the results show that the performance of our method can exceed the traditional VSM methods and methods using pre-trained word embedding.","keywords_author":["Document representation","Natural language processing","Sentiment analysis","Sentiment classification","Word embedding"],"keywords_other":["Feature combination","Document Representation","Complex structure","Neural network method","Word embedding","Embeddings","Sentiment classification","Semantic information"],"max_cite":0.0,"pub_year":2018.0,"sources":"['scp']","rawkeys":["embeddings","sentiment classification","complex structure","natural language processing","word embedding","neural network method","semantic information","document representation","sentiment analysis","feature combination"],"tags":["embeddings","sentiment classification","complex structure","natural language processing","word embedding","neural network method","semantic information","document representation","sentiment analysis","feature combination"]},{"p_id":44555,"title":"Image captioning using deep neural architectures","abstract":"\u00a9 2017 IEEE. Automatically creating the description of an image using any natural language sentences is a very challenging task. It requires expertise of both image processing as well as natural language processing. This paper discusses about different available models for image captioning task. We have discussed about how the advancement in the task of object recognition and machine translation has greatly improved the performance of image captioning model in recent years. In addition to that we have discussed how this model can be implemented. At the end, we have also evaluated the performance of model using standard evaluation matrices.","keywords_author":["Deep Learning","Deep Neural Network","Image Captioning","Machine Translation","Natural Language Generation","Natural Language Processing","Object Recognition"],"keywords_other":["Standard evaluations","Natural language generation","Image captioning","Natural languages","Machine translations","Neural architectures"],"max_cite":0.0,"pub_year":2018.0,"sources":"['scp']","rawkeys":["deep neural network","image captioning","deep learning","natural languages","natural language processing","machine translation","object recognition","natural language generation","machine translations","neural architectures","standard evaluations"],"tags":["image captioning","natural languages","machine learning","natural language processing","object recognition","natural language generation","convolutional neural network","machine translations","neural architectures","standard evaluations"]},{"p_id":48653,"title":"Bidirectional LSTMs - CRFs networks for bangla POS tagging","abstract":"\u00a9 2016 IEEE. Part-of-speech (POS) information is one of the fundamental components in the natural language processing pipeline, which helps in extracting higher-level information such as named entities, discourse, and syntactic structure of a sentence. For some languages, such as English, Dutch, and Chinese, it is considered as a solved problem due to the higher accuracy (97%) of the predicted system. Significant efforts have been made for such languages in terms of making the data publicly accessible and also organizing evaluation campaigns. Compared to that there are very fewer efforts for Bangla (ethnonym: Bangla; exonym: Bengali). In this paper, we present a knowledge poor approach for POS tagging, which we evaluated using publicly accessible dataset from LDC. The motivation of our approach is that we did not want to rely on any existing resources such as lexicon or named entity recognizer for designing the system as they are not publicly available and difficult to develop. We have not used any handcrafted features, rather we employed distributed representations of word and characters. We designed the system using Long Short Term Memory (LSTM) neural networks followed by Conditional Random Fields (CRFs) for designing the model with an inclusion of pre-trained word embedded model. We obtained promising results with an accuracy of 86:0%.","keywords_author":["Bangla","Deep learning","POS tagging"],"keywords_other":["Word and characters","PoS tagging","Bangla","Conditional Random Fields(CRFs)","Fundamental component","Higher-level information","Distributed representation","NAtural language processing"],"max_cite":0.0,"pub_year":2017.0,"sources":"['scp']","rawkeys":["higher-level information","word and characters","fundamental component","deep learning","natural language processing","distributed representation","bangla","conditional random fields(crfs)","pos tagging"],"tags":["higher-level information","word and characters","fundamental component","conditional random field","machine learning","natural language processing","distributed representation","bangla","pos tagging"]},{"p_id":38414,"title":"Code mixed cross script question classification","abstract":"With the growth in our society, one of the most affected aspect of our routine life is language. We tend to mix our conversations in more than one language, often mixing up regional language with English language is a lot more common practice. This mixing of languages is referred as code mixing, where we mix different linguistic constituents such as phrases, proper nouns, morphemes etc. to come up code mixed script. With exponential growth of social media, we are using more and more code mixed cross script for our conversation on Facebook, WhatsApp, or Twitter. On the other hand, the language should be understood by the automated question answering system which is one of the most import application of AI. And now the trend is code mixed languages but current work is around a single language. At FIRE 2016, as a part of Shared Task1 CMCS (Code Mixed Cross Script Question Classification), we have worked on the problem of classify a code mixed question into 9 given classes. Shared Task is focused on Indian regional languages, wherein we worked on Bengali-English code mixed cross script questions classification. As scripting used in training data is English only, so all Bengali text was also written using English script only. We have used Machine Learning for question classification and used ensemble based Random Forest algorithm. As it's a code-mixed script, so traditional NLP components may not work well, so worked on a custom solution using own set of features for Classification.","keywords_author":["Classification","Code switching","Code-mixing","Machine learning","Natural Language Processing","Question answering","Random Forest","Stemming","TFIDF"],"keywords_other":["Random forests","Code-switching","Question Answering","Stemming","TFIDF","Code-mixing","NAtural language processing"],"max_cite":1.0,"pub_year":2016.0,"sources":"['scp', 'wos']","rawkeys":["question answering","tfidf","machine learning","natural language processing","random forests","code switching","stemming","classification","code-switching","code-mixing","random forest"],"tags":["tf-idf","machine learning","natural language processing","information retrieval","code switching","random forests","classification","code-mixing","stem"]},{"p_id":26127,"title":"Combining relevance assignment with quality of the evidence to support guideline development","abstract":"Clinical practice guidelines are used to disseminate best practice to clinicians. Successful guidelines depend on literature that is both relevant to the questions posed and based on high quality research in accordance with evidence-based medicine. Meeting these standards requires extensive manual review. We describe a system that combines symbolic semantic processing with a statistical method for selecting both relevant and high quality studies. We focused on a cardiovascular risk factor guideline, and the overall performance of the system was 56% recall, 91% precision (F 0.5-score 0.81). If quality of the evidence is not taken into account, performance drops to 62% recall, 79% precision (F 0.5-score 0.75). We suggest that this system can potentially improve the efficiency of the literature review process in guideline development. \u00a9 2010 IMIA and SAHIA. All rights reserved.","keywords_author":["Clinical guidelines","Machine learning","Natural language processing"],"keywords_other":null,"max_cite":7.0,"pub_year":2010.0,"sources":"['scp']","rawkeys":["natural language processing","clinical guidelines","machine learning"],"tags":["natural language processing","clinical guidelines","machine learning"]},{"p_id":30223,"title":"Morphological Analyzer for Telugu Using Support Vector Machine","abstract":"In this paper, we presented a morphological analyzer for the classical Dravidian language Telugu using machine learning approach. Morphological analyzer is a computer program that analyses the words belonging to Natural Languages and produces its grammatical structure as output. Telugu language is highly inflection and suffixation oriented, therefore developing the morphological analyzer for Telugu is a significant task. The developed morphological analyzer is based on sequence labeling and training by kernel methods, it captures the non-linear relationships and various morphological features of Telugu language in a better and simpler way. This approach is more efficient than other morphological analyzers which were based on rules. In rule based approach every rule is depends on the previous rule. So if one rule fails, it will affect the entire rule that follows. Regarding the accuracy our system significantly achieves a very competitive accuracy of 94% and 97% in case of Telugu Verbs and nouns. Morphological analyzer for Tamil and Malayalam was also developed by using this approach. \u00a9 Springer-Verlag Berlin Heidelberg 2010.","keywords_author":["Machine learning","Morphemes","Natural language processing","Paradigms and classification","Support vector machine"],"keywords_other":["Non-linear relationships","Grammatical structure","Machine learning approaches","Morphemes","Morphological features","Morphological analyzer","Rule-based approach","NAtural language processing"],"max_cite":4.0,"pub_year":2010.0,"sources":"['scp']","rawkeys":["morphological features","morphological analyzer","machine learning","natural language processing","non-linear relationships","paradigms and classification","rule-based approach","morphemes","support vector machine","grammatical structure","machine learning approaches"],"tags":["morpheme","morphological features","morphological analyzer","machine learning","natural language processing","non-linear relationships","paradigms and classification","rule-based approach","grammatical structure","machine learning approaches"]},{"p_id":42513,"title":"Emotion Based Automated Priority Prediction for Bug Reports","abstract":"OAPA Issue tracking systems allow users to report bugs. Bug reports often contain product name, product component, description, and severity. Based on such information, triagers often manually prioritize the bug reports for investigation. However, manual prioritization is time-consuming and cumbersome. DRONE is an automated state-of-the-art approach that recommends the priority level information of the bug reports. However, its performance for all levels of priorities are not uniform and may be improved. To this end, in this paper we propose an emotion-based automatic approach to predict the priority for a report. First, we exploit natural language processing techniques to preprocess the bug report. Second, we identify the emotion-words that are involved in the description of the bug report and assign it an emotion-value. Third, we create a feature vector for the bug report and predict its priority with a machine learning classifier that is trained with history data collected from the Internet. We evaluate the proposed approach on Eclipse open-source projects and the results of the cross-project evaluation suggest that the proposed approach outperforms the state-of-the-art. On average it improves the F1-score by more than six percent.","keywords_author":["Bug Reports","Classification","Computer bugs","Feature extraction","History","Machine Learning","Manuals","Natural language processing","Priority Prediction","Software","Software Maintenance","Task analysis"],"keywords_other":["Manuals","Automatic approaches","Bug reports","Computer bugs","State-of-the-art approach","Open source projects","Project evaluation","Task analysis"],"max_cite":0.0,"pub_year":2018.0,"sources":"['scp']","rawkeys":["task analysis","computer bugs","open source projects","bug reports","machine learning","natural language processing","software","classification","feature extraction","software maintenance","history","automatic approaches","manuals","project evaluation","priority prediction","state-of-the-art approach"],"tags":["task analysis","computer bugs","open source projects","bug reports","machine learning","natural language processing","software","classification","feature extraction","software maintenance","history","automatic approaches","manuals","project evaluation","priority prediction","state-of-the-art approach"]},{"p_id":24082,"title":"Software Feature Request Detection in Issue Tracking Systems","abstract":"\u00a9 2016 IEEE. Communication about requirements is often handled in issue tracking systems, especially in a distributed setting. As issue tracking systems also contain bug reports or programming tasks, the software feature requests of the users are often difficult to identify. This paper investigates natural language processing and machine learning features to detect software feature requests in natural language data of issue tracking systems. It compares traditional linguistic machine learning features, such as \"bag of words\", with more advanced features, such as subject-action-object, and evaluates combinations of machine learning features derived from the natural language and features taken from the issue tracking system meta-data. Our investigation shows that some combinations of machine learning features derived from natural language and the issue tracking system meta-data outperform traditional approaches. We show that issues or data fields (e.g. descriptions or comments), which contain software feature requests, can be identified reasonably well, but hardly the exact sentence. Finally, we show that the choice of machine learning algorithms should depend on the goal, e.g. maximization of the detection rate or balance between detection rate and precision. In addition, the paper contributes a double coded gold standard and an open-source implementation to further pursue this topic.","keywords_author":["Machine Learning","Mining Software Repositories","Natural Language Processing","Software Feature Request Detection"],"keywords_other":["Software features","Traditional approaches","Subject action objects","Mining software repositories","Programming tasks","Natural languages","NAtural language processing","Open source implementation"],"max_cite":9.0,"pub_year":2016.0,"sources":"['scp']","rawkeys":["open source implementation","natural languages","natural language processing","machine learning","software feature request detection","mining software repositories","software features","subject action objects","programming tasks","traditional approaches"],"tags":["open source implementation","natural languages","natural language processing","machine learning","software feature request detection","mining software repositories","software features","subject action objects","programming tasks","traditional approaches"]},{"p_id":46606,"title":"Predicting political opinions in social networks with user embeddings","abstract":"\u00a9 2017 IEEE. As social media becomes the prevalent way of expressing one's opinions, the capacity to analyze user behavioral patterns and trends becomes increasingly important. In this work we propose a method for embedding the opinions of social media users in low-dimensional vectors and explore some potential applications. Our proposal is general, in the sense that it can be easily adapted to a variety of use cases and social networks.","keywords_author":["Deep learning","Natural language processing","Representation learning","Word embeddings"],"keywords_other":["Representation learning","Social media","Behavioral patterns","Low dimensional","Embeddings"],"max_cite":0.0,"pub_year":2017.0,"sources":"['scp']","rawkeys":["embeddings","deep learning","low dimensional","social media","natural language processing","behavioral patterns","word embeddings","representation learning"],"tags":["embeddings","low dimensional","social media","machine learning","behavioral patterns","natural language processing","representation learning","word embedding"]},{"p_id":40469,"title":"Text classification through time: Efficient label propagation in time-based graphs","abstract":"One of the fundamental assumptions for machine-learning based text classification systems is that the underlying distribution from which the set of labeled-text is drawn is identical to the distribution from which the text-to-be-labeled is drawn. However, in live news aggregation sites, this assumption is rarely correct. Instead, the events and topics discussed in news stories dramatically change over time. Rather than ignoring this phenomenon, we attempt to explicitly model the transitions of news stories and classifications over time to label stories that may be acquired months after the initial examples are labeled. We test our system, based on efficiently propagating labels in time-based graphs, with recently published news stories collected over an eighty day period. Experiments presented in this paper include the use of training labels from each story within the first several days of gathering stories, to using a single story as a label.","keywords_author":["Adsorption","Graph algorithms","Machine learning","Natural language processing","Preference propagation","Semi supervised learning","Text analysis","Text classification"],"keywords_other":["Text classification","Machine-learning","Graph algorithms","Semi-supervised learning","NAtural language processing"],"max_cite":1.0,"pub_year":2009.0,"sources":"['scp']","rawkeys":["preference propagation","natural language processing","machine learning","adsorption","semi-supervised learning","text analysis","machine-learning","semi supervised learning","text classification","graph algorithms"],"tags":["preference propagation","natural language processing","machine learning","adsorption","semi-supervised learning","text analysis","text classification","graph algorithms"]},{"p_id":1559,"title":"A review on advances in deep learning","abstract":"Over the years conventional neural networks has shown state-of-art performance on many problems. However, their performance on recognition system is still not widely accepted in the machine learning community because these networks are unable to handle selectivity-invariance dilemma and also suffer from the problem of vanishing gradients. Some of these issues have been addressed by deep learning. Deep learning approaches attempt to disentangle intricate aspects of input by creating multiple levels of representation. These approaches have shown astonishing results in problem domains like recognition system, natural language processing, medical sciences, and in many other fields. The paper presents an overview of different deep learning approaches in a nutshell and also highlights some limitations which are restricting performance of deep neural networks in order to handle more realistic problems.","keywords_author":["conventional neural networks","deep learning","deep neural networks","conventional neural networks","deep learning","deep neural networks"],"keywords_other":["Deep learning","State-of-art performance","Feature extraction","Supervised learning","Machine learning communities","Biological neural networks","Machine learning","Vanishing gradient","Medical science","Unsupervised learning","Deep neural networks","NAtural language processing","Computer architecture","Recognition systems"],"max_cite":5.0,"pub_year":2016.0,"sources":"['ieee', 'scp']","rawkeys":["vanishing gradient","supervised learning","deep learning","deep neural networks","state-of-art performance","machine learning","natural language processing","medical science","machine learning communities","conventional neural networks","unsupervised learning","feature extraction","biological neural networks","computer architecture","recognition systems"],"tags":["vanishing gradient","supervised learning","state-of-art performance","machine learning","natural language processing","medical science","machine learning communities","conventional neural networks","unsupervised learning","convolutional neural network","feature extraction","biological neural networks","computer architecture","recognition systems"]},{"p_id":24090,"title":"A machine-learning approach to negation and speculation detection for sentiment analysis","abstract":"\u00a9 2015 ASIS&T Recognizing negative and speculative information is highly relevant for sentiment analysis. This paper presents a machine-learning approach to automatically detect this kind of information in the review domain. The resulting system works in two steps: in the first pass, negation\/speculation cues are identified, and in the second phase the full scope of these cues is determined. The system is trained and evaluated on the Simon Fraser University Review corpus, which is extensively used in opinion mining. The results show how the proposed method outstrips the baseline by as much as roughly 20% in the negation cue detection and around 13% in the scope recognition, both in terms of F1. In speculation, the performance obtained in the cue prediction phase is close to that obtained by a human rater carrying out the same task. In the scope detection, the results are also promising and represent a substantial improvement on the baseline (up by roughly 10%). A detailed error analysis is also provided. The extrinsic evaluation shows that the correct identification of cues and scopes is vital for the task of sentiment analysis.","keywords_author":["machine learning","natural language processing"],"keywords_other":["Simon Fraser University","Machine learning approaches","Sentiment analysis","Second phase","NAtural language processing","Opinion mining"],"max_cite":9.0,"pub_year":2016.0,"sources":"['scp']","rawkeys":["second phase","natural language processing","machine learning","simon fraser university","sentiment analysis","opinion mining","machine learning approaches"],"tags":["second phase","natural language processing","machine learning","simon fraser university","sentiment analysis","opinion mining","machine learning approaches"]},{"p_id":26139,"title":"Machine learning and rule-based automated coding of qualitative data","abstract":"Large volumes of textual data pose considerable challenges for manual qualitative analysis. We explore semi-automatic coding of textual data by leveraging Natural Language Processing (NLP). We compare the performance of human-developed NLP rules to those inferred by machine learning (ML) algorithms. The results suggest that NLP with ML may be useful to support researchers coding qualitative data.","keywords_author":["Machine learning","Natural language processing","Qualitative data analysis"],"keywords_other":null,"max_cite":7.0,"pub_year":2010.0,"sources":"['scp']","rawkeys":["natural language processing","qualitative data analysis","machine learning"],"tags":["natural language processing","qualitative data analysis","machine learning"]},{"p_id":1565,"title":"A deep convolutional neural wavelet network to supervised Arabic letter image classification","abstract":"In this paper, a new approach to supervised image classification is suggested. It's conducted by the combination of two techniques of learning: the wavelet network and the deep learning. This new approach consists of performing the classification of one class versus all the other classes of the dataset by the reconstruction of a convolutional deep neural wavelet network. This network is obtained using a series of stacked auto-encoders and a linear classifier. Finally, a local contrast normalization and an intelligent pooling are applied to our network. The experimental test of our approach performed on Arabic Printed Text Image (APTI) dataset demonstrates that our model is remarkably efficient for image classification compared to a known classifier.","keywords_author":["Convolutional Neural Network","Deep learning","Stacked Auto-Encoders","Wavelet Network","Wavelet Network","Deep learning","Stacked Auto-Encoders","Convolutional Neural Network"],"keywords_other":["Supervised image classifications","wavelet neural nets","Arabic printed text image dataset","image classification","Deep learning","APTI","Experimental test","Auto encoders","linear classifier","learning (artificial intelligence)","Local contrast","supervised Arabic letter image classification","Neurons","deep convolutional neural wavelet network","Image resolution","Semantics","Linear classifiers","natural language processing","Wavelet network","Convolutional neural network","stacked auto-encoders"],"max_cite":5.0,"pub_year":2015.0,"sources":"['wos', 'scp', 'ieee']","rawkeys":["wavelet network","supervised image classifications","wavelet neural nets","arabic printed text image dataset","convolutional neural network","supervised arabic letter image classification","apti","image classification","experimental test","linear classifier","neurons","image resolution","linear classifiers","learning (artificial intelligence)","deep learning","semantics","local contrast","deep convolutional neural wavelet network","auto encoders","natural language processing","stacked auto-encoders"],"tags":["wavelet network","supervised image classifications","wavelet neural nets","arabic printed text image dataset","convolutional neural network","supervised arabic letter image classification","apti","image classification","experimental test","machine learning","neurons","image resolution","linear classifiers","semantics","local contrast","deep convolutional neural wavelet network","auto encoders","natural language processing","stacked autoencoders"]},{"p_id":40477,"title":"Classifiers combination to Arabic MorphoSyntactic Disambiguation","abstract":"Parts of speech tagging forms the important preprocessing step in many of the natural language processing applications like text summarization, question answering and information retrieval system. MorphoSyntactic disambiguation (part of speech tagging) is the process of classifying every word in a given context to its appropriate part of speech. In this paper, we first review all the supervised machine learning approaches that have been used in the part of speech tagging. Then we review all the Arabic works to compare and to confirm our need to develop an accurate and efficient Arabic MorphoSyntactic Disambiguation system. Finally we propose a classifiers combination experimental framework for Arabic part of speech tagger in which three diverse probabilistic classifiers (Hidden Markov, Maximum Entropy and Transformation Based Learning) are combined using many different combination strategies to exploit their advantages. \u00a9 2009 IEEE.","keywords_author":["Machine learning","MorphoSyntactic disambiguation","Natural language processing"],"keywords_other":["Part of speech tagging","Maximum entropy","Probabilistic classifiers","Transformation-based learning","Machine-learning","Pre-processing step","Question Answering","Speech tagging","Text summarization","Part-of-speech tagger","Combination strategies","Part Of Speech","NAtural language processing","Supervised machine learning"],"max_cite":1.0,"pub_year":2009.0,"sources":"['scp']","rawkeys":["question answering","transformation-based learning","maximum entropy","morphosyntactic disambiguation","combination strategies","machine learning","natural language processing","part of speech","part of speech tagging","part-of-speech tagger","probabilistic classifiers","text summarization","machine-learning","pre-processing step","supervised machine learning","speech tagging"],"tags":["maximum entropy","morphosyntactic disambiguation","natural language processing","combination strategies","machine learning","part of speech tagging","part-of-speech tagger","information retrieval","probabilistic classifiers","speech tags","text summarization","transformation based learning","part of speech","pre-processing step","supervised machine learning"]},{"p_id":40479,"title":"A novel method of Chinese web information extraction and applications","abstract":"One promising application of natural language processing (NLP) research is in the area of information extraction (IE). In this paper, we present work flow of our IE system for the extraction of semantically rich information from the unstructured or semi-structured Chinese web pages. Knowledge engineering approach and automatic training approach are used to extract pattern and built knowledge repository. General IE system needs to label the unlabeled training web pages. A novel methodology that does not need to label text is developed, including hierarchy filtration pattern matching based on syntax in best distance method and maximum forward boundary recognition using organization suffix repository and part of speech tagging method. As for applications of IE, a new application system based on IE is built. It is object-level vertical search system and object here is Chinese people, so IE is concerned with extracting people's related attributes from a collection of web pages about Chinese people. The results are displayed as hierarchy directory tree according to people's attributes. The system makes user find people quickly and easily. \u00a9 2009 IEEE.","keywords_author":["Information extraction (IE)","Machine learning(ML)","Natural language processing (NLP)"],"keywords_other":["Boundary recognition","Search system","Distance method","Information extraction","Work-flows","Natural language processing","Knowledge engineering approach","Machine learning(ML)","Novel methods","Directory trees","Automatic training","Chinese people","New applications","Web page","Natural language processing (NLP)","Part of speech tagging","Knowledge repository","Chinese web","Information extraction (IE)","Novel methodology","Semi-structured"],"max_cite":1.0,"pub_year":2009.0,"sources":"['scp']","rawkeys":["web page","knowledge engineering approach","new applications","chinese people","information extraction","boundary recognition","search system","novel methodology","information extraction (ie)","natural language processing (nlp)","distance method","work-flows","chinese web","novel methods","semi-structured","natural language processing","knowledge repository","part of speech tagging","directory trees","machine learning(ml)","automatic training"],"tags":["web page","knowledge engineering approach","new applications","chinese people","information extraction","machine learning","boundary recognition","search system","novel methodology","distance method","chinese web","novel methods","workflow","semi-structured","natural language processing","knowledge repository","part of speech tagging","directory trees","automatic training"]},{"p_id":52773,"title":"Searching, translating and classifying information in cyberspace","abstract":"In this paper we describe current search technologies available on the web, explain underlying difficulties and show their limits, related to either current technologies or to the intrinsic properties of all natural languages. We then analyze the effectiveness of freely available machine translation services and demonstrate that under certain conditions these translation systems can operate at the same performance levels as manual translators. Searching for factual information with commercial search engines also allows the retrieval of facts, user comments and opinions on target items. In the third part we explain how the principle machine learning strategies are able to classify short passages of text extracted from the blogosphere as factual or opinionated and then classify their polarity (positive, negative or mixed). \u00a9 2011 Springer-Verlag Berlin Heidelberg.","keywords_author":["automatic text classification","machine learning","machine translation","natural language processing (NLP)","Search technology","web"],"keywords_other":["Search technology","Machine-learning","Automatic text classification","Machine translations","Natural language processing","web"],"max_cite":0.0,"pub_year":2011.0,"sources":"['scp']","rawkeys":["search technology","machine learning","automatic text classification","machine translation","natural language processing","natural language processing (nlp)","machine translations","machine-learning","web"],"tags":["search technology","natural language processing","machine learning","automatic text classification","machine translations","web"]},{"p_id":24106,"title":"Towards Extracting Drug-Effect Relation from Twitter: A Supervised Learning Approach","abstract":"\u00a9 2016 IEEE.Advancements in social media technology have resulted in the booming of massive public data. The availability of these huge data sets offers numerous research opportunities for deriving meaningful cause-effect relationships for many applications. One important application domain is the cause of side effects of drugs. In this paper, we applied supervised learning to extract useful cause-and-effect information related to drugs from Twitter. To filter out unrelated information and to increase the accuracy of classification, a spam filter and a preprocessing procedure have been developed. Validation experiments were performed using a manually labeled data set based on streamed tweets collected continuously on Twitter in real-time for 48 hours, and exploiting six different supervised machine-learning classifiers. Results have shown that these classifiers have achieved up to 77% accuracy in identifying drugs' cause-effect relations on Twitter data. This result has shown a positive feasibility for collecting drug side effect information from Twitter. The proposed method may be applied to other areas such as food, beverages, and other daily consumer products for finding their side effects and people's opinions concerning them.","keywords_author":["Cause-effect analysis","classification","machine learning","microblogging","natural language processing","online social networks","opinion mining","sentimental analysis","supervised learning","Twitter"],"keywords_other":["Twitter","Microblogging","On-line social networks","sentimental analysis","Cause-effect analysis","NAtural language processing","Opinion mining"],"max_cite":9.0,"pub_year":2016.0,"sources":"['scp']","rawkeys":["on-line social networks","online social networks","supervised learning","cause-effect analysis","machine learning","natural language processing","sentimental analysis","classification","microblogging","opinion mining","twitter"],"tags":["on-line social networks","online social networks","supervised learning","cause-effect analysis","machine learning","natural language processing","classification","microblogging","opinion mining","sentiment analysis","twitter"]},{"p_id":30251,"title":"RelHunter: A machine learning method for relation extraction from text","abstract":"We propose RelHunter, a machine learning-based method for the extraction of structured information from text. RelHunter's key idea is to model the target structures as a relation over entities. Hence, the modeling effort is reduced to the identification of entities and the generation of a candidate relation, which are simpler problems than the original one. RelHunter fits a very broad spectrum of complex computational linguistic problems. We apply it to five tasks: phrase chunking, clause identification, hedge detection, quotation extraction, and dependency parsing. We compare RelHunter to token classification approaches through several computational experiments on seven multilingual corpora. RelHunter outperforms the token classification approaches by 2.14% on average. Moreover, we compare the derived systems against state-of-the-art systems for each corpus. Our systems achieve state-of-the-art performances for three corpora: Portuguese phrase chunking, Portuguese clause identification, and English quotation extraction. Additionally, the derived systems show good quality performance for the other four corpora. \u00a9 2010 The Brazilian Computer Society.","keywords_author":["Entity relation extraction","Entropy Guided Transformation Learning","Machine learning","Natural language processing"],"keywords_other":["State-of-the-art performance","State-of-the-art system","Computational experiment","Classification approach","Clause identifications","Entity relation extractions","Machine learning methods","NAtural language processing"],"max_cite":4.0,"pub_year":2010.0,"sources":"['scp']","rawkeys":["machine learning methods","clause identifications","machine learning","entity relation extractions","natural language processing","classification approach","entropy guided transformation learning","state-of-the-art system","computational experiment","state-of-the-art performance","entity relation extraction"],"tags":["machine learning methods","clause identifications","machine learning","entity relation extractions","natural language processing","classification approach","entropy guided transformation learning","state-of-the-art system","computational experiment","state-of-the-art performance"]},{"p_id":32300,"title":"Post or Block? Advances in Automatically Filtering Undesired Comments","abstract":"\u00a9 2014, Springer Science+Business Media Dordrecht.Currently, a great volume of the available information on several websites comes from the interaction with users, such as social networks, forums and blogs, where readers can post comments and sometimes develop habits of frequenting them. Some blogs specialized in certain subjects, gain the users credibility and become references in the field. Nevertheless, the ease of inserting content through text comments makes room for unwanted messages, which affect the user experience, reduce the quality of the information provided by the websites and indirectly cause personal and economic losses. In this scenario, this paper presents a comprehensive study of established machine learning techniques applied to automatically detect undesired comments posted on blogs. Furthermore, different sets of attributes were evaluated along with text normalization techniques. Experiments carried out with a real and public database indicate that support vector machines, logistic regression and stacking ensemble methods, trained with both attributes extracted from the text messages and posting information, are promising for the task of blocking undesired comments.","keywords_author":["Classification","Natural language processing","Supervised learning","Undesired messages"],"keywords_other":["Public database","Text normalizations","User experience","Undesired messages","Logistic regressions","Ensemble methods","NAtural language processing","Machine learning techniques"],"max_cite":3.0,"pub_year":2015.0,"sources":"['scp']","rawkeys":["undesired messages","text normalizations","supervised learning","public database","machine learning techniques","natural language processing","ensemble methods","logistic regressions","classification","user experience"],"tags":["undesired messages","text normalizations","supervised learning","public database","machine learning techniques","natural language processing","ensemble methods","logistic regressions","classification","user experience"]},{"p_id":17967,"title":"A review of natural language processing techniques for opinion mining systems","abstract":"\u00a9 2016 Elsevier B.V. As the prevalence of social media on the Internet, opinion mining has become an essential approach to analyzing so many data. Various applications appear in a wide range of industrial domains. Meanwhile, opinions have diverse expressions which bring along research challenges. Both of the practical demands and research challenges make opinion mining an active research area in recent years. In this paper, we present a review of Natural Language Processing (NLP) techniques for opinion mining. First, we introduce general NLP techniques which are required for text preprocessing. Second, we investigate the approaches of opinion mining for different levels and situations. Then we introduce comparative opinion mining and deep learning approaches for opinion mining. Opinion summarization and advanced topics are introduced later. Finally, we discuss some challenges and open problems related to opinion mining.","keywords_author":["Deep learning","Machine learning","Natural language processing","Opinion mining","Sentiment analysis"],"keywords_other":["Deep learning","Research challenges","Text preprocessing","Nlp techniques","Sentiment analysis","Social media","NAtural language processing","Opinion mining"],"max_cite":26.0,"pub_year":2017.0,"sources":"['scp']","rawkeys":["research challenges","deep learning","nlp techniques","social media","natural language processing","machine learning","text preprocessing","opinion mining","sentiment analysis"],"tags":["research challenges","nlp techniques","social media","natural language processing","machine learning","text preprocessing","opinion mining","sentiment analysis"]},{"p_id":38447,"title":"Identification of long bone fractures in radiology reports using natural language processing to support healthcare quality improvement","abstract":"\u00ef\u00bf\u00bd Schattauer 2016. Background: Important information to support healthcare quality improvement is often recorded in free text documents such as radiology reports. Natural language processing (NLP) methods may help extract this information, but these methods have rarely been applied outside the research laboratories where they were developed. Objective: To implement and validate NLP tools to identify long bone fractures for pediatric emergency medicine quality improvement. Methods: Using freely available statistical software packages, we implemented NLP methods to identify long bone fractures from radiology reports. A sample of 1,000 radiology reports was used to construct three candidate classification models. A test set of 500 reports was used to validate the model performance. Blinded manual review of radiology reports by two independent physicians provided the reference standard. Each radiology report was segmented and word stem and bigram features were constructed. Common English \u201cstop words\u201d and rare features were excluded. We used 10-fold cross-validation to select optimal configuration parameters for each model. Accuracy, recall, precision and the F1 score were calculated. The final model was compared to the use of diagnosis codes for the identification of patients with long bone fractures. Results: There were 329 unique word stems and 344 bigrams in the training documents. A support vector machine classifier with Gaussian kernel performed best on the test set with accuracy=0.958, recall=0.969, precision=0.940, and F1 score=0.954. Optimal parameters for this model were cost=4 and gamma=0.005. The three classification models that we tested all performed better than diagnosis codes in terms of accuracy, precision, and F1 score (diagnosis code accuracy=0.932, recall= 0.960, precision=0.896, and F1 score=0.927). Conclusions: NLP methods using a corpus of 1,000 training documents accurately identified acute long bone fractures from radiology reports. Strategic use of straightforward NLP methods, implemented with freely available software, offers quality improvement teams new opportunities to extract information from narrative documents.","keywords_author":["Emergency medicine","Machine learning","Natural language processing","Pediatrics","Quality improvement"],"keywords_other":["Humans","Quality Improvement","Natural Language Processing","Radiology","Child","Emergency Medicine","Fractures, Bone","Research Report","Clinical Decision-Making","Documentation"],"max_cite":1.0,"pub_year":2016.0,"sources":"['scp']","rawkeys":["research report","documentation","machine learning","natural language processing","humans","emergency medicine","fractures","quality improvement","bone","radiology","pediatrics","clinical decision-making","child"],"tags":["research report","clinical decision making","documentation","machine learning","natural language processing","humans","emergency medicine","fracture","quality improvement","radiology","bone","pediatrics","child"]},{"p_id":30258,"title":"A latent discriminative variable model for automatic identification of Chinese base phrases","abstract":"In the fields of natural language processing such as information processing and machine translation, recognizing simple and non-recursive Chinese base phrases is an important task. In stead of rule-based model, we adopt the statistical machine learning method, newly proposed Latent semi-CRF model to solve the Chinese base phrase chunking problem. The Chinese base phrases could be treated as the sequence labeling problem, which involve the prediction of a class label for each frame in an unsegmented sequence. The Chinese base phrases have sub-structures which could not be observed in training data. Latent semi-CRF, which incorporates the advantages of Latent Dynamic Conditional Random Fields and semi-CRF that model the sub-structure of a class sequence and learn dynamics between class labels, in detecting the Chinese base phrases. Our results demonstrate that the latent dynamic discriminative model compares favorably to Support Vector Machines, Maximum Entropy Model, and Conditional Random Fields (including LDCRF and semi-CRF) on Chinese base phrases chunking. Copyright \u00a9 2010 Binary Information Press.","keywords_author":["Chinese base phrases chunking","Chinese natural language processing","Latent semi-CRF model","Machine learning"],"keywords_other":["Maximum entropy models","Class labels","Rule-based models","Dynamic conditional random fields","Variable model","Machine translations","Sequence Labeling","Discriminative models","Information processing","Automatic identification","Training data","Latent semi-CRF model","Machine learning methods","Chinese base phrases chunking","NAtural language processing","Sub-structures","Machine-learning","Chinese natural language processing","Conditional random field"],"max_cite":4.0,"pub_year":2010.0,"sources":"['scp']","rawkeys":["latent semi-crf model","sub-structures","maximum entropy models","variable model","training data","chinese base phrases chunking","chinese natural language processing","conditional random field","machine learning","class labels","automatic identification","machine translations","information processing","sequence labeling","machine-learning","rule-based models","discriminative models","machine learning methods","natural language processing","dynamic conditional random fields"],"tags":["latent semi-crf model","maximum entropy models","substructure","training data","chinese base phrases chunking","chinese natural language processing","conditional random field","variability modelling","machine learning","class labels","automatic identification","machine translations","information processing","sequence labeling","rule-based models","discriminative models","machine learning methods","natural language processing","dynamic conditional random fields"]},{"p_id":52786,"title":"Empirical evaluation of three machine learning method for automatic classification of neoplastic diagnoses Evaluaci\u00f3n emp\u00edrica de tres m\u00e9todos de aprendizaje autom\u00e1tico para clasificar autom\u00e1ticamente diagn\u00f3sticos de neoplasias","abstract":"Diagnoses are a valuable source of information for evaluating a health system. However, they are not used extensively by information systems because diagnoses are normally written in natural language. This work empirically evaluates three machine learning methods to automatically assign codes from the International Classification of Diseases (10th Revision) to 3,335 distinct diagnoses of neoplasms obtained from UMLS\u00ae. This evaluation is conducted on three different types of preprocessing. The results are encouraging: a well-known rule induction method and maximum entropy models achieve 90% accuracy in a balanced cross-validation experiment.","keywords_author":["Clinical coding","Controlled vocabulary","International classification of diseases","Machine learning","Natural language processing"],"keywords_other":null,"max_cite":0.0,"pub_year":2011.0,"sources":"['scp']","rawkeys":["controlled vocabulary","machine learning","natural language processing","international classification of diseases","clinical coding"],"tags":["controlled vocabulary","machine learning","natural language processing","international classification of diseases","clinical coding"]},{"p_id":38455,"title":"Is this a joke? Detecting humor in Spanish tweets","abstract":"\u00a9 Springer International Publishing AG 2016. While humor has been historically studied from a psychological, cognitive and linguistic standpoint, its study from a computational perspective is an area yet to be explored in Computational Linguistics. There exist some previous works, but a characterization of humor that allows its automatic recognition and generation is far from being specified. In this work we build a crowdsourced corpus of labeled tweets, annotated according to its humor value, letting the annotators subjectively decide which are humorous. A humor classifier for Spanish tweets is assembled based on supervised learning, reaching a precision of 84% and a recall of 69 %.","keywords_author":["Computational humor","Humor","Humor recognition","Machine learning","Natural language processing"],"keywords_other":["Computational humor","Automatic recognition","Humor recognition","Humor","NAtural language processing"],"max_cite":1.0,"pub_year":2016.0,"sources":"['scp']","rawkeys":["machine learning","humor","natural language processing","humor recognition","automatic recognition","computational humor"],"tags":["machine learning","humor","natural language processing","humor recognition","automatic recognition","computational humor"]},{"p_id":48697,"title":"Business perception based on sentiment analysis through deep neuronal networks for natural language processing","abstract":"\u00a9 2017, Springer International Publishing AG.In recent years, the machine-learning field, deep neural networks has been an important topic of research, used in several disciplines such as pattern recognition, information retrieval, classification and natural language processing. Is in the last that this paper it\u2019s going to be our principal topic, in this branch exist an specific task that in literature is called Sentiment Analysis were the principal function is to detect if an opinion is positive or negative. In the paper we show how use this subset of the machine learning knowledge and use it for give us an insight in the question: what is the perception in a business or a product by means of the opinion of the consumers in social networks?","keywords_author":["Deep learning","Machine learning","Natural language processing","Neural networks"],"keywords_other":["Perception-based","Sentiment analysis","Neuronal networks","Principal functions","Specific tasks"],"max_cite":0.0,"pub_year":2017.0,"sources":"['scp']","rawkeys":["deep learning","neural networks","principal functions","specific tasks","natural language processing","machine learning","neuronal networks","sentiment analysis","perception-based"],"tags":["neural networks","principal functions","specific tasks","natural language processing","machine learning","neuronal networks","sentiment analysis","perception-based"]},{"p_id":20026,"title":"Opinion classification techniques applied to a Spanish corpus","abstract":"Sentiment analysis is a new challenging task related to Text Mining and Natural Language Processing. Although there are some current works, most of them only focus on English texts. Web pages, information and opinions on the Internet are increasing every day, and English is not the only language used to write them. Other languages like Spanish are increasingly present so we have carried out some experiments over a Spanish film reviews corpus. In this paper we present several experiments using five classification algorithms (SVM, Nave Bayes, BBR, KNN, C4.5). The results obtained are very promising and encourage us to continue investigating in this line. \u00a9 2011 Springer-Verlag.","keywords_author":["machine learning algorithms","Opinion mining","sentiment polarity classification","subjective corpora"],"keywords_other":["Text mining","Classification technique","subjective corpora","Polarity classification","Sentiment analysis","Classification algorithm","Spanish corpora","NAtural language processing","Opinion mining"],"max_cite":18.0,"pub_year":2011.0,"sources":"['scp']","rawkeys":["spanish corpora","subjective corpora","sentiment polarity classification","opinion mining","text mining","machine learning algorithms","natural language processing","classification technique","sentiment analysis","classification algorithm","polarity classification"],"tags":["spanish corpora","subjective corpora","sentiment polarity classification","opinion mining","text mining","machine learning algorithms","natural language processing","classification technique","sentiment analysis","classification algorithm","polarity classification"]},{"p_id":26174,"title":"A comparison of data-driven automatic syllabification methods","abstract":"Although automatic syllabification is an important component in several natural language tasks, little has been done to compare the results of data-driven methods on a wide range of languages. This article compares the results of five data-driven syllabification algorithms (Hidden Markov Support Vector Machines, IB1, Liang's algorithm, the Look Up Procedure, and Syllabification by Analogy) on nine European languages in order to determine which algorithm performs best over all. Findings show that all algorithms achieve a mean word accuracy across all lexicons of over 90%. However, Syllabification by Analogy performs better than the other algorithms tested with a mean word accuracy of 96.84% (standard deviation of 2.93) whereas Liang's algorithm, the standard for hyphenation (used in ), produces the second best results with a mean of 95.67% (standard deviation of 5.70). \u00a9 2009 Springer.","keywords_author":["Automatic syllabification","Machine learning","Natural language processing"],"keywords_other":["European languages","Automatic syllabification","Data-driven","Machine learning","Standard deviation","Natural languages","Word accuracies","Data-driven methods","Other algorithms","Natural language processing"],"max_cite":7.0,"pub_year":2009.0,"sources":"['scp']","rawkeys":["data-driven methods","standard deviation","word accuracies","natural languages","machine learning","natural language processing","data-driven","european languages","automatic syllabification","other algorithms"],"tags":["data-driven methods","standard deviation","word accuracies","natural languages","machine learning","natural language processing","european languages","automatic syllabification","data driven","other algorithms"]},{"p_id":7743,"title":"Machine learning and radiology","abstract":"In this paper, we give a short introduction to machine learning and survey its applications in radiology. We focused on six categories of applications in radiology: medical image segmentation, registration, computer aided detection and diagnosis, brain function or activity analysis and neurological disease diagnosis from fMR images, content-based image retrieval systems for CT or MRI images, and text analysis of radiology reports using natural language processing (NLP) and natural language understanding (NLU). This survey shows that machine learning plays a key role in many radiology applications. Machine learning identifies complex patterns automatically and helps radiologists make intelligent decisions on radiology data such as conventional radiographs, CT, MRI, and PET images and radiology reports. In many applications, the performance of machine learning-based automatic detection and diagnosis systems has shown to be comparable to that of a well-trained and experienced radiologist. Technology development in machine learning and radiology will benefit from each other in the long run. Key contributions and common characteristics of machine learning techniques in radiology are discussed. We also discuss the problem of translating machine learning applications to the radiology clinical setting, including advantages and potential barriers. \u00a9 2012 .","keywords_author":["Computer-aided detection and diagnosis","Image segmentation","Machine learning","Radiology","Survey"],"keywords_other":["Computer-aided detection and diagnosis","Intelligent decisions","Brain functions","Radiology reports","Machine learning applications","Automatic Detection","Natural language understanding","Text analysis","MRI Image","Activity analysis","Complex pattern","Machine learning techniques","PET images","Content-based image retrieval system","Technology development","NAtural language processing","Medical image segmentation","Potential barriers","Clinical settings","Neurological disease"],"max_cite":128.0,"pub_year":2012.0,"sources":"['scp', 'wos']","rawkeys":["intelligent decisions","potential barriers","clinical settings","computer-aided detection and diagnosis","activity analysis","survey","pet images","machine learning","radiology reports","neurological disease","machine learning applications","machine learning techniques","mri image","radiology","medical image segmentation","image segmentation","natural language understanding","complex pattern","content-based image retrieval system","natural language processing","text analysis","brain functions","automatic detection","technology development"],"tags":["intelligent decisions","potential barriers","clinical settings","computer-aided detection and diagnosis","technological development","activity analysis","survey","pet images","machine learning","radiology reports","neurological disease","content-based image retrieval","machine learning applications","machine learning techniques","mri image","radiology","medical image segmentation","image segmentation","natural language understanding","complex pattern","natural language processing","text analysis","brain functions","automatic detection"]},{"p_id":34367,"title":"Named Entity Recognition for Malayalam language: A CRF based approach","abstract":"\u00a9 2015 IEEE. Named Entity Recognition is an important application area of Natural Language Processing. It is the process of identifying the designators which are present in a sentence called as named entities. Named Entity Recognition can be performed using rule based approaches, machine learning based approaches and hybrid approaches. This paper proposes a method for Named Entity Recognition of Malayalam language using one of the supervised machine learning approach called Conditional Random field approach.","keywords_author":["Conditional Random Field Approach","Machine learning","Named Entity Recognition","Supervised Machine Learning"],"keywords_other":["Named entities","Named entity recognition","Application area","Hybrid approach","Rule-based approach","Supervised machine learning","Conditional random field","NAtural language processing"],"max_cite":2.0,"pub_year":2015.0,"sources":"['scp']","rawkeys":["application area","conditional random field","named entity recognition","machine learning","natural language processing","hybrid approach","conditional random field approach","named entities","rule-based approach","supervised machine learning"],"tags":["application area","conditional random field","named entity recognition","machine learning","natural language processing","hybrid approach","conditional random field approach","named entities","rule-based approach","supervised machine learning"]},{"p_id":9791,"title":"Deep learning based spell checker for Malayalam language","abstract":"Spell checking plays an important role in conveying correct information and hence helps in clear communication. Spell checkers for English language are well established. But in case of Indian languages, especially Malayalam lacks a well developed spell checker. The spell checkers that currently exist for Indian languages are based on traditional approaches such as rule based or dictionary based. The rich morphological nature of Malayalam makes spell checking a difficult task. The proposed work is a novel attempt and first of its kind that focuses on implementing a spell checker for Malayalam using deep learning. The spell checker comprises of two processes: error detection and error correction. The error detection section employs a LSTM based neural network which is trained to identify the misspelled words and the position where the error has occurred. The error detection accuracy is measured using the F1 score. Error correction is achieved by the selecting the most probable word from the candidate word suggestions.","keywords_author":["deep learning","long short term memory","Malayalam","natural language processing","spell checker","Malayalam","spell checker","deep learning","natural language processing","long short term memory"],"keywords_other":["Traditional approaches","English languages","spell checker","Spell-checking","Malayalams","Indian languages","Dictionary-based","Detection accuracy"],"max_cite":1.0,"pub_year":2018.0,"sources":"['scp', 'wos']","rawkeys":["malayalams","deep learning","detection accuracy","natural language processing","indian languages","dictionary-based","spell checker","english languages","spell-checking","malayalam","traditional approaches","long short term memory"],"tags":["malayalams","long short-term memory","detection accuracy","machine learning","natural language processing","indian languages","dictionary-based","spell checker","english languages","spell-checking","traditional approaches"]},{"p_id":50750,"title":"Sentiment Analysis for Polish Using Transfer Learning Approach","abstract":"\u00a9 2015 IEEE. A method for sentiment polarity assignment for textual content written in Polish using supervised machine learning approach with transfer learning scheme is proposed in the paper. It has been shown that performing simple natural language processing steps prior to classification, provides inspiring results without redundant computation overhead. The documents containing subjective opinions were classified using N-gram and Bi-gram language model that is able to encode some of complex word phrases. The experiments carried out on two real datasets taken from different domains proved that learning on one dataset and testing on another, which is commonly called transfer learning, can be effective and may result in very high classification quality.","keywords_author":["classification","machine learning","opinions","Polish","sentiment analysis","support vector machine","transfer learning"],"keywords_other":["Transfer learning","Sentiment analysis","opinions","Classification quality","Redundant computation","Bi-gram language models","Supervised machine learning","NAtural language processing"],"max_cite":0.0,"pub_year":2015.0,"sources":"['scp']","rawkeys":["transfer learning","natural language processing","machine learning","opinions","polish","classification","redundant computation","classification quality","support vector machine","bi-gram language models","sentiment analysis","supervised machine learning"],"tags":["transfer learning","opinion","natural language processing","machine learning","polish","bigram language model","classification","redundant computation","classification quality","supervised machine learning","sentiment analysis"]},{"p_id":46657,"title":"Experience Report: Log Mining Using Natural Language Processing and Application to Anomaly Detection","abstract":"\u00a9 2017 IEEE. Event logging is a key source of information on a system state. Reading logs provides insights on its activity, assess its correct state and allows to diagnose problems. However, reading does not scale: With the number of machines increasingly rising, and the complexification of systems, the task of auditing systems' health based on logfiles is becoming overwhelming for system administrators. This observation led to many proposals automating the processing of logs. However, most of these proposal still require some human intervention, for instance by tagging logs, parsing the source files generating the logs, etc.In this work, we target minimal human intervention for logfile processing and propose a new approach that considers logs as regular text (as opposed to related works that seek to exploit at best the little structure imposed by log formatting). This approach allows to leverage modern techniques from natural language processing. More specifically, we first apply a word embedding technique based on Google's word2vec algorithm: Logfiles' words are mapped to a high dimensional metric space, that we then exploit as a feature space using standard classifiers. The resulting pipeline is very generic, computationally efficient, and requires very little intervention.We validate our approach by seeking stress patterns on an experimental platform. Results show a strong predictive performance (2248; 90% accuracy) using three out-of-the-box classifiers.","keywords_author":["Anomaly detection","Logfile","Machine learning","NLP","VNF","Word2vec"],"keywords_other":["Word2vec","High-dimensional metric space","System administrators","Predictive performance","Anomaly detection","Experimental platform","Logfile","Computationally efficient"],"max_cite":0.0,"pub_year":2017.0,"sources":"['scp']","rawkeys":["logfile","predictive performance","vnf","nlp","anomaly detection","word2vec","machine learning","experimental platform","system administrators","computationally efficient","high-dimensional metric space"],"tags":["logfile","vnf","anomaly detection","word2vec","system administrators","machine learning","experimental platform","natural language processing","prediction performance","computationally efficient","high-dimensional metric space"]},{"p_id":42564,"title":"Teaching data science: an objective approach to curriculum validation","abstract":"\u00a9 2018 Informa UK Limited, trading as Taylor & Francis Group Emerging careers in technology-focused fields such as data science coupled with necessary graduate outcomes mandate the need for a truly interdisciplinary pedagogical approach. However, the rapid pace of curriculum development in this field of inquiry has meant that curricula across universities has largely evolved in line with the internal disciplinary strengths of each institution rather than in response to the needs of graduates. To assist with the development of data science subjects the themes and content that contribute to each subject should be objectively validated. We propose the use of an objective test for data science curricula to quantify whether a particular degree programme maintains an interdisciplinary perspective unconstrained by single discipline bias. The test analyses a given curriculum and quantifies the subject components by category using natural language processing (NLP) techniques.","keywords_author":["curriculum development","data science","machine learning","natural language processing","Text mining"],"keywords_other":null,"max_cite":0.0,"pub_year":2018.0,"sources":"['scp']","rawkeys":["text mining","natural language processing","machine learning","data science","curriculum development"],"tags":["text mining","natural language processing","machine learning","data science","curriculum development"]},{"p_id":15941,"title":"A primer on neural network models for natural language processing","abstract":"\u00a9 2016 AI Access Foundation. All rights reserved. Over the past few years, neural networks have re-emerged as powerful machine-learning models, yielding state-of-the-art results in fields such as image recognition and speech processing. More recently, neural network models started to be applied also to textual natural language signals, again with very promising results. This tutorial surveys neural network models from the perspective of natural language processing research, in an attempt to bring natural-language researchers up to speed with the neural techniques. The tutorial covers input encoding for natural language tasks, feed-forward networks, convolutional networks, recurrent networks and recursive networks, as well as the computation graph abstraction for automatic gradient computation.","keywords_author":null,"keywords_other":["Convolutional networks","Feed-forward network","Machine learning models","Neural techniques","Neural network model","Recurrent networks","Gradient computation","NAtural language processing"],"max_cite":41.0,"pub_year":2016.0,"sources":"['scp', 'wos']","rawkeys":["neural techniques","machine learning models","natural language processing","feed-forward network","convolutional networks","gradient computation","neural network model","recurrent networks"],"tags":["neural techniques","machine learning models","natural language processing","feed-forward network","gradient computation","neural network model","convolutional neural network","recurrent networks"]},{"p_id":26182,"title":"Specializing for predicting obesity and its co-morbidities","abstract":"We present specializing, a method for combining classifiers for multi-class classification. Specializing trains one specialist classifier per class and utilizes each specialist to distinguish that class from all others in a one-versus-all manner. It then supplements the specialist classifiers with a catch-all classifier that performs multi-class classification across all classes. We refer to the resulting combined classifier as a specializing classifier. We develop specializing to classify 16 diseases based on discharge summaries. For each discharge summary, we aim to predict whether each disease is present, absent, or questionable in the patient, or unmentioned in the discharge summary. We treat the classification of each disease as an independent multi-class classification task. For each disease, we develop one specialist classifier for each of the present, absent, questionable, and unmentioned classes; we supplement these specialist classifiers with a catch-all classifier that encompasses all of the classes for that disease. We evaluate specializing on each of the 16 diseases and show that it improves significantly over voting and stacking when used for multi-class classification on our data. \u00a9 2008 Elsevier Inc. All rights reserved.","keywords_author":["Classification","Combination of classifiers","Machine learning","Natural language processing"],"keywords_other":["Combination of classifiers","Combining classifiers","Natural language processing","Multi-class classification","Machine learning","Classification","Discharge summary"],"max_cite":7.0,"pub_year":2009.0,"sources":"['scp']","rawkeys":["combination of classifiers","machine learning","natural language processing","discharge summary","classification","multi-class classification","combining classifiers"],"tags":["combined classifiers","combination of classifiers","machine learning","natural language processing","discharge summary","classification","multi-class classification"]},{"p_id":15943,"title":"Natural language processing: State of the art and prospects for significant progress, a workshop sponsored by the National Library of Medicine","abstract":"Natural language processing (NLP) is crucial for advancing healthcare because it is needed to transform relevant information locked in text into structured data that can be used by computer processes aimed at improving patient care and advancing medicine. In light of the importance of NLP to health, the National Library of Medicine (NLM) recently sponsored a workshop to review the state of the art in NLP focusing on text in English, both in biomedicine and in the general language domain. Specific goals of the NLM-sponsored workshop were to identify the current state of the art, grand challenges and specific roadblocks, and to identify effective use and best practices. This paper reports on the main outcomes of the workshop, including an overview of the state of the art, strategies for advancing the field, and obstacles that need to be addressed, resulting in recommendations for a research agenda intended to advance the field. \u00a9 2013 The Authors.","keywords_author":["Biomedical language processing","Natural language processing"],"keywords_other":["Best practices","Language processing","State of the art","National library of medicines","Research agenda","Structured data","NAtural language processing","Grand Challenge"],"max_cite":41.0,"pub_year":2013.0,"sources":"['scp']","rawkeys":["biomedical language processing","language processing","state of the art","natural language processing","structured data","research agenda","best practices","national library of medicines","grand challenge"],"tags":["biomedical language processing","language processing","state of the art","natural language processing","structured data","research agenda","best practices","national library of medicines","grand challenge"]},{"p_id":52807,"title":"Recent patents on information retrieval using natural language and keyword query","abstract":"With the explosion of data and information, researchers and practitioners have started to rethink how users can best interact with the massive text data found on the World Wide Web and in relational data warehouses. Traditional structured query languages, such as SQL and XQuery, are simply too inflexible and cumbersome for the mass public. In recent days, we have seen a resurgence of information retrieval and natural language processing techniques in information management for structured data. In this article, we emphasize on patents that utilize natural language queries and keyword queries as means of information retrieval. Without the declarative grammar structure, natural language and keyword queries pose unique challenges to query interpretation and document retrieval. Query interpretation and evaluation of such queries have received much attention in the past decade. We have selected four inventions that claim methods of improving the task of natural language and keyword query processing. We review the technical details and features of the patents, and compare them in a unified context. \u00a9 2010 Bentham Science Publishers Ltd.","keywords_author":["Information retrieval","Keyword queries","Machine learning","Natural language processing","Text analysis","Unstructured data","Web data"],"keywords_other":["Text analysis","Web data","Machine learning","Unstructured data","Keyword queries","Natural language processing"],"max_cite":0.0,"pub_year":2010.0,"sources":"['scp']","rawkeys":["keyword queries","natural language processing","machine learning","information retrieval","text analysis","unstructured data","web data"],"tags":["keyword queries","natural language processing","machine learning","information retrieval","text analysis","unstructured data","web data"]},{"p_id":28233,"title":"Toward automatic inference of causal structure in student essays","abstract":"With an increasing focus on science and technology in education comes an awareness that students must be able to understand and integrate scientific explanations from multiple sources. As part of a larger project aimed at deepening our understanding of student processes for integrating multiple sources of information, we are developing machine learning and natural language processing techniques for evaluating students' argumentative essays. In previous work, we have focused on identifying conceptual elements of the essays. In this paper, we present a method for inferring the causal structure of student essays. We used a standard parser to derive grammatical dependencies of the essay and converted them to logic statements. Then a simple inference mechanism was used to identify concepts linked to syntactic connectors by these dependencies. The results suggest that we will soon be able to provide explicit feedback that enables teachers and students to improve comprehension. \u00a9 2014 Springer International Publishing Switzerland.","keywords_author":["Argumentation","Machine learning","Natural language processing","Reading"],"keywords_other":["Conceptual elements","NAtural language processing","Argumentation","Inference mechanism","Automatic inference","Science and Technology","Integrating multiple sources","Reading"],"max_cite":5.0,"pub_year":2014.0,"sources":"['scp']","rawkeys":["science and technology","inference mechanism","machine learning","automatic inference","natural language processing","argumentation","reading","integrating multiple sources","conceptual elements"],"tags":["science and technology","inference mechanisms","machine learning","natural language processing","automatic inference","argumentation","reading","integrating multiple sources","conceptual elements"]},{"p_id":38476,"title":"Word embeddings for morphologically complex languages","abstract":"Recent methods for learning word embeddings, like GloVe or Word2- Vec, succeeded in spatial representation of semantic and syntactic relations. We extend GloVe by introducing separate vectors for base form and grammatical form of a word, using morphosyntactic dictionary for this. This allows vectors to capture properties of words better. We also present model results for word analogy test and introduce a new test based on WordNet.","keywords_author":["Machine learning","Morphology","Natural language processing","Word embeddings"],"keywords_other":null,"max_cite":1.0,"pub_year":2016.0,"sources":"['scp']","rawkeys":["natural language processing","word embeddings","machine learning","morphology"],"tags":["natural language processing","machine learning","word embedding","morphology"]},{"p_id":52812,"title":"Linear text segmentation using classification techniques","abstract":"Automatic segmentation of a text stream into topically coherent segments is an important component in natural language processing tasks such as information retrieval and document summarization. Machine learning techniques can play a vital role in building an efficient system for text segmentation. This paper describes a method for identifying segment boundaries of an unstructured text document with the aid of multiple linguistic features. Linguistic features include word repetition, lexical chains, presence of pronouns, conversation, named entities, paragraph and so on. The task of segmentation is modeled as a binary classification problem, where the classes correspond to the presence or the absence of a segment boundary. An experiment in text segmentation using an efficient classifier function is presented to show the effectiveness of the new approach. \u00a9 2010 ACM.","keywords_author":["classification techniques","machine learning","natural language processing","text segmentation"],"keywords_other":["Binary classification problems","Lexical Chain","Classification technique","Named entities","Text document","Document summarization","Text segmentation","Automatic segmentations","Efficient systems","machine learning","natural language processing","New approaches","In-buildings","Text streams","Linguistic features","Machine learning techniques"],"max_cite":0.0,"pub_year":2010.0,"sources":"['scp']","rawkeys":["new approaches","lexical chain","document summarization","in-buildings","text streams","text segmentation","machine learning techniques","machine learning","natural language processing","classification techniques","linguistic features","classification technique","binary classification problems","named entities","text document","efficient systems","automatic segmentations"],"tags":["new approaches","lexical chain","document summarization","in-buildings","text streams","text segmentation","machine learning techniques","machine learning","natural language processing","linguistic features","classification technique","automatic segmentation","binary classification problems","named entities","text document","efficient systems"]},{"p_id":50768,"title":"The impact of structured event embeddings on scalable stock forecasting models","abstract":"\u00a9 2015 ACM. According to the efficient market hypothesis, financial prices are unpredictable. However, meaningful advances have been achieved on anticipating market movements using machine learning techniques. In this work, we propose a novel method to represent the input for a stock price forecaster. The forecaster is able to predict stock prices from time series and additional information from web pages. Such information is extracted as structured events and represented in a compressed concept space. By using such representation with scalable forecasters, we reduced prediction error by about 10%, when compared to the traditional auto regressive models.","keywords_author":["Deep learning","Natural language processing","Open information extraction","Stocks forecast"],"keywords_other":["Deep learning","Prediction errors","Stock forecasting","Efficient market hypothesis","Auto regressive models","Concept space","NAtural language processing","Machine learning techniques"],"max_cite":0.0,"pub_year":2015.0,"sources":"['scp']","rawkeys":["efficient market hypothesis","concept space","deep learning","prediction errors","machine learning techniques","natural language processing","open information extraction","stock forecasting","auto regressive models","stocks forecast"],"tags":["efficient market hypothesis","concept space","prediction errors","machine learning techniques","machine learning","natural language processing","open information extraction","stock forecasting","auto regressive models"]},{"p_id":52816,"title":"An exploration of native speakers' eye fixations in reading Chinese text","abstract":"We collected the locations of eye fixations of Chinese native speakers when they read four Chinese articles, and attempted to analyze how the contextual linguistic and personal information influence the landing positions within the landing sites. In addition, we employed machine learning techniques to build models for the prediction of the landing positions. The models performed well for the closed tests, achieving 78% in accuracy in predicting whether a reader's eyes landed on the first or the second character within a word that contained two characters. Unfortunately, the accuracy for the same task in the 10-fold cross validations dropped to 60%, indicating the necessity of more future work. \u00a9 2010 IEEE.","keywords_author":["Chinese segmentation","Eye tracking","Machine learning","Natural language processing","Reading patterns"],"keywords_other":["Machine-learning","Chinese segmentation","Eye tracking","Reading patterns","NAtural language processing"],"max_cite":0.0,"pub_year":2010.0,"sources":"['scp']","rawkeys":["natural language processing","machine learning","reading patterns","machine-learning","eye tracking","chinese segmentation"],"tags":["natural language processing","machine learning","reading patterns","eye-tracking","chinese segmentation"]},{"p_id":38484,"title":"Identification of singleton mentions in Russian","abstract":"This paper describes a pilot study of the problem of detecting singleton mentions in Russian texts. A noun phrase is considered a singleton mention if it is the only referent of some entity. We discuss various morphosyntactic and lexical features, some of which were used for analogous tasks for English and propose new features derived from the discourse analysis. Testing the machine learning classifiers trained with the use of proposed features, we conclude that although the quality of classifiers is significantly lower than for English, they still have rather high precision and thus can be helpful in various tasks of mention tracking.","keywords_author":["Coreference resolution","Discourse processing","Machine learning","Mention detection","Natural language processing"],"keywords_other":["Noun phrase","Co-reference resolutions","Lexical features","Pilot studies","High-precision","Discourse processing","Discourse analysis"],"max_cite":1.0,"pub_year":2016.0,"sources":"['scp']","rawkeys":["co-reference resolutions","discourse processing","machine learning","natural language processing","pilot studies","discourse analysis","noun phrase","high-precision","lexical features","mention detection","coreference resolution"],"tags":["discourse processing","machine learning","natural language processing","pilot studies","discourse analysis","noun phrase","high-precision","lexical features","mention detection","coreference resolution"]},{"p_id":52821,"title":"An automatic normalized CUT topic segmentation approach","abstract":"This paper presents an automatic topic segmentation approach based on subwords normalized cut (Ncut) for Chinese broadcast news, since the classical Ncut has a limitation that the number of segments has to be set as a prior. We abstract a text into a weighted undirected graph, where the nodes correspond to sentences and the weights of edges describe inter-sentence lexical similarities at Chinese subwords level, thus the segmentation task is formalized as a graph-partitioning problem under the Ncut criterion. In order to break through the limitation, we proposed a text dotplot-ting inspired method, which can evaluate the segmentation results and select the optimal number of segments automatically. Lastly, we put the whole approach into a machine learning framework, learning the best arguments on train set. Our method achieved relative improvement of 3% over non-automatic subwords Ncut, also the previous best method. \u00a9 2010 IEEE.","keywords_author":["Intelligent information processing","Machine learning","Natural language processing","Normalized cut","Topic segmentation"],"keywords_other":["Machine-learning","Topic segmentation","Intelligent information processing","Normalized cut","Natural language processing"],"max_cite":0.0,"pub_year":2010.0,"sources":"['scp']","rawkeys":["natural language processing","machine learning","normalized cut","intelligent information processing","machine-learning","topic segmentation"],"tags":["normalized cuts","natural language processing","machine learning","intelligent information processing","topic segmentation"]},{"p_id":1622,"title":"Learning Contextual Dependence With Convolutional Hierarchical Recurrent Neural Networks","abstract":"Deep convolutional neural networks (CNNs) have shown their great success on image classification. CNNs mainly consist of convolutional and pooling layers, both of which are performed on local image areas without considering the dependence among different image regions. However, such dependence is very important for generating explicit image representation. In contrast, recurrent neural networks (RNNs) are well known for their ability of encoding contextual information in sequential data, and they only require a limited number of network parameters. Thus, we proposed the hierarchical RNNs (HRNNs) to encode the contextual dependence in image representation. In HRNNs, each RNN layer focuses on modeling spatial dependence among image regions from the same scale but different locations. While the cross RNN scale connections target on modeling scale dependencies among regions from the same location but different scales. Specifically, we propose two RNN models: 1) hierarchical simple recurrent network (HSRN), which is fast and has low computational cost and 2) hierarchical long-short term memory recurrent network, which performs better than HSRN with the price of higher computational cost. In this paper, we integrate CNNs with HRNNs, and develop end-to-end convolutional hierarchical RNNs (C-HRNNs) for image classification. C-HRNNs not only utilize the discriminative representation power of CNNs, but also utilize the contextual dependence learning ability of our HRNNs. On four of the most challenging object\/scene image classification benchmarks, our C-HRNNs achieve the state-of-the-art results on Places 205, SUN 397, and MIT indoor, and the competitive results on ILSVRC 2012.","keywords_author":["Convolutional Neural Networks","Deep Learning","Image Classification","Recurrent Neural Networks","Deep learning","image classification","recurrent neural networks","convolutional neural networks","Deep Learning","Image Classification","Recurrent Neural Networks","Convolutional Neural Networks"],"keywords_other":["Simple recurrent networks","recurrent neural networks","hierarchical long-short term memory recurrent network","Image representation","image classification","Deep learning","recurrent neural nets","computational complexity","Context modeling","Long short term memory","Recurrent neural network (RNNs)","image representation","computational cost","convolutional neural networks","Contextual information","convolutional hierarchical recurrent neural networks","hierarchical simple recurrent network","Computer vision","Image representations","Representation power","scene image classification","Logic gates","C-HRNN","Recurrent neural networks","CLASSIFICATION","object image classification","Convolutional neural network","image regions spatial dependence","contextual dependence learning","Natural language processing"],"max_cite":14.0,"pub_year":2016.0,"sources":"['ieee', 'scp', 'wos']","rawkeys":["contextual information","recurrent neural networks","classification","context modeling","convolutional neural network","hierarchical long-short term memory recurrent network","logic gates","image classification","image representations","recurrent neural network (rnns)","recurrent neural nets","computational complexity","image representation","representation power","computational cost","simple recurrent networks","convolutional neural networks","convolutional hierarchical recurrent neural networks","deep learning","hierarchical simple recurrent network","scene image classification","computer vision","c-hrnn","natural language processing","object image classification","image regions spatial dependence","contextual dependence learning","long short term memory"],"tags":["contextual information","computational costs","classification","context modeling","convolutional neural network","hierarchical long-short term memory recurrent network","logic gates","image classification","recurrent neural nets","computational complexity","machine learning","image representation","representation power","simple recurrent networks","convolutional hierarchical recurrent neural networks","neural networks","long short-term memory","hierarchical simple recurrent network","scene image classification","computer vision","c-hrnn","natural language processing","object image classification","image regions spatial dependence","contextual dependence learning"]},{"p_id":26199,"title":"A hybrid framework towards the solution for people with disability effectively using computer keyboard","abstract":"A hybrid framework based on machine learning model has been introduced. This is to offer an efficient solution for people with disability using QWERTY keyboard. It integrates neural network, language model and natural language processing technologies and provides users with two fundamental functions, namely, word prediction and typing correction. A development of a pilot application as an English input method has also been introduced. \u00a9 2008 IADIS.","keywords_author":["Language model","Machine learning","Neural network"],"keywords_other":["Hybrid frameworks","Language model","Machine learning","Word prediction","Neural network","Natural language process (NLP)"],"max_cite":7.0,"pub_year":2008.0,"sources":"['scp']","rawkeys":["natural language process (nlp)","neural network","machine learning","hybrid frameworks","language model","word prediction"],"tags":["neural networks","natural language processing","machine learning","hybrid framework","language model","word prediction"]},{"p_id":18008,"title":"Automated extraction of non-functional requirements in available documentation","abstract":"While all systems have non-functional requirements (NFRs), they may not be explicitly stated in a formal requirements specification. Furthermore, NFRs may also be externally imposed via government regulations or industry standards. As some NFRs represent emergent system proprieties, those NFRs require appropriate analysis and design efforts to ensure they are met. When the specified NFRs are not met, projects incur costly re-work to correct the issues. The goal of our research is to aid analysts in more effectively extracting relevant non-functional requirements in available unconstrained natural language documents through automated natural language processing. Specifically, we examine which document types (data use agreements, install manuals, regulations, request for proposals, requirements specifications, and user manuals) contain NFRs categorized to 14 NFR categories (e.g. capacity, reliability, and security). We measure how effectively we can identify and classify NFR statements within these documents. In each of the documents evaluated, we found NFRs present. Using a word vector representation of the NFRs, a support vector machine algorithm performed twice as effectively compared to the same input to a multinomial na\u00efve Bayes classifier. Our k-nearest neighbor classifier with a unique distance metric had an F1 measure of 0.54, outperforming in our experiments the optimal na\u00efve Bayes classifier which had a F1 measure of 0.32. We also found that stop word lists beyond common determiners had no minimal performance effect. \u00a9 2013 IEEE.","keywords_author":["classification","documentation","machine learning","natural language processing","non-functional requirements"],"keywords_other":["Automated extraction","K-nearest neighbor classifier","Requirements specifications","Government regulation","Request for proposals","Non-functional requirements","NAtural language processing","Support vector machine algorithm"],"max_cite":26.0,"pub_year":2013.0,"sources":"['scp']","rawkeys":["k-nearest neighbor classifier","support vector machine algorithm","documentation","machine learning","natural language processing","automated extraction","classification","government regulation","requirements specifications","request for proposals","non-functional requirements"],"tags":["support vector machine algorithm","documentation","machine learning","natural language processing","automated extraction","classification","government regulation","requirements specifications","request for proposals","non-functional requirements","k-nearest neighbors"]},{"p_id":24152,"title":"Creation of a new longitudinal corpus of clinical narratives","abstract":"\u00a9 2015 Elsevier Inc.The 2014 i2b2\/UTHealth Natural Language Processing (NLP) shared task featured a new longitudinal corpus of 1304 records representing 296 diabetic patients. The corpus contains three cohorts: patients who have a diagnosis of coronary artery disease (CAD) in their first record, and continue to have it in subsequent records; patients who do not have a diagnosis of CAD in the first record, but develop it by the last record; patients who do not have a diagnosis of CAD in any record. This paper details the process used to select records for this corpus and provides an overview of novel research uses for this corpus. This corpus is the only annotated corpus of longitudinal clinical narratives currently available for research to the general research community.","keywords_author":["Corpus","Machine learning","Medical records","NLP"],"keywords_other":["Humans","Vocabulary, Controlled","Aged","Corpus","Research communities","Coronary Artery Disease","Female","Boston","Cohort Studies","Diabetic patient","Coronary artery disease","Comorbidity","Diabetes Complications","Narration","Medical record","Incidence","NAtural language processing","Confidentiality","Male","Risk Assessment","Electronic Health Records","Computer Security","Middle Aged","NLP","Natural Language Processing","Data Mining"],"max_cite":9.0,"pub_year":2015.0,"sources":"['scp']","rawkeys":["vocabulary","computer security","aged","boston","diabetic patient","comorbidity","machine learning","electronic health records","medical record","middle aged","risk assessment","medical records","research communities","diabetes complications","narration","cohort studies","data mining","incidence","humans","confidentiality","controlled","nlp","male","natural language processing","coronary artery disease","corpus","female"],"tags":["vocabulary","computer security","aged","boston","diabetic patient","comorbidity","computer-aided diagnosis","control","electronic health records","machine learning","medical record","middle aged","risk assessment","research communities","diabetes complications","narration","cohort studies","data mining","incidence","humans","confidentiality","male","natural language processing","corpus","female"]},{"p_id":34391,"title":"Automating Risk of Bias Assessment for Clinical Trials","abstract":"\u00a9 2015 IEEE.Systematic reviews, which summarize the entirety of the evidence pertaining to a specific clinical question, have become critical for evidence-based decision making in healthcare. But such reviews have become increasingly onerous to produce due to the exponentially expanding biomedical literature base. This study proposes a step toward mitigating this problem by automating risk of bias assessment in systematic reviews, in which reviewers determine whether study results may be affected by biases (e.g., poor randomization or blinding). Conducting risk of bias assessment is an important but onerous task. We thus describe a machine learning approach to automate this assessment, using the standard Cochrane Risk of Bias Tool which assesses seven common types of bias. Training such a system would typically require a large labeled corpus, which would be prohibitively expensive to collect here. Instead, we use distant supervision, using data from the Cochrane Database of Systematic Reviews (a large repository of systematic reviews), to pseudoannotate a corpus of 2200 clinical trial reports in PDF format. We then develop a joint model which, using the full text of a clinical trial report as input, predicts the risks of bias while simultaneously extracting the text fragments supporting these assessments. This study represents a step toward automating or semiautomating extraction of data necessary for the synthesis of clinical trials.","keywords_author":["Evidence-based medicine","health informatics","machine learning","natural language processing"],"keywords_other":["Systematic Review","Biomedical literature","Risk Assessment","Cochrane database","Clinical Trials as Topic","Health informatics","Humans","Machine learning approaches","Natural Language Processing","Medical Informatics","Evidence- based decisions","Bias (Epidemiology)","NAtural language processing","Evidence-based medicine"],"max_cite":2.0,"pub_year":2015.0,"sources":"['scp']","rawkeys":["bias (epidemiology)","evidence-based medicine","machine learning","natural language processing","health informatics","humans","machine learning approaches","medical informatics","risk assessment","evidence- based decisions","clinical trials as topic","cochrane database","biomedical literature","systematic review"],"tags":["evidence-based medicine","epidemiology","machine learning","health informatics","humans","machine learning approaches","natural language processing","medical informatics","risk assessment","evidence- based decisions","clinical trials as topic","cochrane database","biomedical literature","systematic review"]},{"p_id":18011,"title":"A framework for automatic TRIZ level of invention estimation of patents using natural language processing, knowledge-transfer and patent citation metrics","abstract":"Patents provide a wealth of information about design concepts, their physical realization, and their relationship to prior designs in the form of citations. Patents can provide useful input for several goals of next-generation computer-aided design (CAD) systems, yet more efficient tools are needed to facilitate patent search and ranking. In this paper, a novel framework is presented and implemented for classifying patents according to level of invention (LOI) as defined in the theory of inventive problem solving (TRIZ). Level of invention characterizes the creativity of a design concept based on the resolution of a design conflict and the disciplines used in resolving the conflict. The assessment of LOI for a series of patents provides a useful input for screening and ranking patents in databases to identify high-impact patents. However, the manual effort required for assigning LOI to each patent is laborious and time-consuming. In this paper, a novel method that combines text mining, natural language processing, creation of knowledge-transfer metrics, and application of machine learning approaches is presented and implemented for classifying patents according to LOI. Two case studies are presented in which LOI data is compiled for patents: dynamic magnetic information storage or retrieval using Giant Magnetoresistive (GMR) or Colossal Magnetoresistive (CMR) sensors formed of multiple thin films (USPC 360\/324) and arbitration for access to a channel (USPC 370\/462). The peak performance in 5-fold stratified cross-validation was found to be 73.38% in the first case study and 77.12% for the second. \u00a9 2012 Elsevier Ltd. All rights reserved.","keywords_author":["Computer-aided Design (CAD)","Data mining","Level of invention (LOI)","Machine learning","Natural language processing (NLP)","Theory of inventive problem solving (TRIZ)"],"keywords_other":["Text mining","Patent citation","Cross validation","Level of invention (LOI)","Colossal magnetoresistive","Physical realization","Magnetic information storage","Peak performance","Theory of inventive problem solving","Wealth of information","Learning approach","Giant magnetoresistive","Design concept","NAtural language processing","Patent search"],"max_cite":26.0,"pub_year":2012.0,"sources":"['scp']","rawkeys":["patent citation","theory of inventive problem solving (triz)","peak performance","theory of inventive problem solving","computer-aided design (cad)","text mining","design concept","machine learning","patent search","giant magnetoresistive","magnetic information storage","learning approach","data mining","natural language processing (nlp)","wealth of information","physical realization","colossal magnetoresistive","cross validation","level of invention (loi)","natural language processing"],"tags":["patent citation","magnetic information storage","data mining","text mining","physical realization","computer-aided diagnosis","design concept","peak performance","level of invention (loi)","machine learning","natural language processing","wealth of information","patent search","theory of inventive problem solving","giant magnetoresistive","computer vision","learning approach","colossal magnetoresistive"]},{"p_id":30301,"title":"The value of parsing as feature generation for gene mention recognition","abstract":"We measured the extent to which information surrounding a base noun phrase reflects the presence of a gene name, and evaluated seven different parsers in their ability to provide information for that purpose. Using the GENETAG corpus as a gold standard, we performed machine learning to recognize from its context when a base noun phrase contained a gene name. Starting with the best lexical features, we assessed the gain of adding dependency or dependency-like relations from a full sentence parse. Features derived from parsers improved performance in this partial gene mention recognition task by a small but statistically significant amount. There were virtually no differences between parsers in these experiments.","keywords_author":["Biological text","Gene mention recognition","Machine learning","Named entity recognition","Natural language processing","Parsers","Support vector machines"],"keywords_other":["Named entity recognition","Machine learning","Biological text","Parsers","Natural language processing"],"max_cite":4.0,"pub_year":2009.0,"sources":"['scp']","rawkeys":["gene mention recognition","parsers","named entity recognition","natural language processing","machine learning","biological text","support vector machines"],"tags":["gene mention recognition","parsers","named entity recognition","natural language processing","machine learning","biological text"]},{"p_id":30302,"title":"Increasing quality of the corpus of frequency dictionary of contemporary polish for morphosyntactic tagging of the polish Language","abstract":"The paper is devoted to the issue of correction of the erroneous and ambiguous corpus of Frequency Dictionary of Contemporary Polish (FDCP) and its application to morphosyntactic tagging of the Polish language. Several stages of corpus transformation are presented and baseline part-of-speech tagging algorithms arc evaluated, too.","keywords_author":["Corpora preparation","Machine learning","Natural language processing","Part-of-speech tagging"],"keywords_other":null,"max_cite":4.0,"pub_year":2009.0,"sources":"['scp']","rawkeys":["natural language processing","corpora preparation","machine learning","part-of-speech tagging"],"tags":["natural language processing","corpora preparation","machine learning","part of speech tagging"]},{"p_id":26209,"title":"A case study of algorithms for morphosyntactic tagging of polish language","abstract":"The paper presents an evaluation of several part-of-speech taggers, representing main tagging algorithms, applied to corpus of frequency dictionary of the contemporary Polish language. We report our results considering two tagging schemes: IPI PAN positional tagset and its simplified version. Tagging accuracy is calculated for different training sets and takes into account many subcategories (accuracy on known and unknown tokens, word segments, sentences etc.) The comparison of results with other inflecting and analytic languages is done. Performance aspects (time demands) of used tagging tools are also discussed.","keywords_author":["Machine learning","Natural language processing","Part-of-speech tagging"],"keywords_other":null,"max_cite":7.0,"pub_year":2007.0,"sources":"['scp']","rawkeys":["natural language processing","part-of-speech tagging","machine learning"],"tags":["natural language processing","machine learning","part of speech tagging"]},{"p_id":24163,"title":"What Goes Around Comes Around: Learning Sentiments in Online Medical Forums","abstract":"\u00a9 2015, Springer Science+Business Media New York. It has been shown that online health-related discussions significantly influence the attitudes and behavioral intentions of the discussion participants. Although empirical evidence strongly supports the importance of emotions in health-related online discussions, there are few studies of the relationship between a subjective language and online discussions of personal health. In this work, we study sentiments expressed on online medical forums. Individual posts are classified into one of five categories. We identified three categories as sentimental (encouragement, gratitude, confusion) and two categories as neutral (facts, endorsement). A total of 1438 messages were annotated manually by two annotators with a strong inter-annotator agreement (Fleiss kappa = 0.737 when the posts were annotated in the context of discussion and Fleiss kappa = 0.763 when the posts were annotated as individual entities). Using machine learning multi-class classification approach, we assess the feasibility of automated recognition of the five sentiment categories. As well as considering the predominant sentiments expressed in individual posts, we analyze transitions between sentiments in online discussions.","keywords_author":["Discourse analysis","Machine learning","Natural language processing","Sentiment analysis","Sentiment transitions"],"keywords_other":["Behavioral intention","Online discussions","Multi-class classification","Sentiment analysis","Three categories","NAtural language processing","Automated recognition","Discourse analysis"],"max_cite":9.0,"pub_year":2015.0,"sources":"['scp']","rawkeys":["three categories","machine learning","natural language processing","online discussions","discourse analysis","sentiment analysis","automated recognition","sentiment transitions","multi-class classification","behavioral intention"],"tags":["three categories","machine learning","natural language processing","online discussions","discourse analysis","sentiment analysis","automated recognition","sentiment transitions","multi-class classification","behavioral intention"]},{"p_id":15972,"title":"Extracting usability and user experience information from online user reviews","abstract":"Internet review sites allow consumers to write detailed reviews of products potentially containing information related to user experience (UX) and usability. Using 5198 sentences from 3492 online reviews of software and video games, we investigate the content of online reviews with the aims of (i) charting the distribution of information in reviews among different dimensions of usability and UX, and (ii) extracting an associated vocabulary for each dimension using techniques from natural language processing and machine learning. We (a) find that 13%-49% of sentences in our online reviews pool contain usability or UX information; (b) chart the distribution of four sets of dimensions of usability and UX across reviews from two product categories; (c) extract a catalogue of important word stems for a number of dimensions. Our results suggest that a greater understanding of users' preoccupation with different dimensions of usability and UX may be inferred from the large volume of self-reported experiences online, and that research focused on identifying pertinent dimensions of usability and UX may benefit further from empirical studies of user-generated experience reports. Copyright 2013 ACM.","keywords_author":["End user reviews","Machine learning","Natural language processing","Usability","User experience"],"keywords_other":["Usability","User experiences (ux)","End users","User experience","Experience report","User-generated","NAtural language processing","Empirical studies"],"max_cite":40.0,"pub_year":2013.0,"sources":"['scp']","rawkeys":["user experiences (ux)","user-generated","empirical studies","machine learning","natural language processing","end users","end user reviews","user experience","experience report","usability"],"tags":["user-generated","empirical studies","machine learning","natural language processing","end users","end user reviews","user experience","experience report","usability"]},{"p_id":26211,"title":"A machine learning approach for indonesian question answering system","abstract":"Our research is to investigate a machine learning approach in order to build an Indonesian Question Answering System. Based on our experiments result on the question classification task, we choose to use SVM as the machine learning algorithm. Similar with ordinary QA systems, we divide our system into three subcomponents: question classifier, passage retriever and answer finder. The SVM algorithm is employed in the question classifier and answer finder modules. To overcome the language resource poorness problem of Indonesian language, we introduce a bi-gram frequency attribute extracted from a downloaded newspaper corpus. The comparison among attribute combination is shown in our question classifier experiment. The t-test shows that the question shallow parser result attribute joined with bi-gram frequency attribute gives significant improvement compared to the baseline (bag of words). Our question classifier achieves 96% accuracy. We also compare some attribute combinations in the answer finder module. We find that the join attribute between the expected answer type (EAT) and the attributes of the question classifier gives higher MRR score than using only the EAT attribute or only the attribute of the question classifiers. Our QA system achieves MRR (Mean Reciprocal Rank) of 0.52 for exact answers.","keywords_author":["Answer finder","Indonesian","Machine learning","Natural language processing","Question answering","Question classifier"],"keywords_other":["Question classifier","Answer finders","Question answering"],"max_cite":7.0,"pub_year":2007.0,"sources":"['scp']","rawkeys":["indonesian","question answering","answer finders","natural language processing","machine learning","question classifier","answer finder"],"tags":["indonesian","answer finders","natural language processing","machine learning","information retrieval","question classifier"]},{"p_id":36452,"title":"Integrating Natural Language Processing and Machine Learning Algorithms to Categorize Oncologic Response in Radiology Reports","abstract":"\u00a9 2017, Society for Imaging Informatics in Medicine. A significant volume of medical data remains unstructured. Natural language processing (NLP) and machine learning (ML) techniques have shown to successfully extract insights from radiology reports. However, the codependent effects of NLP and ML in this context have not been well-studied. Between April 1, 2015 and November 1, 2016, 9418 cross-sectional abdomen\/pelvis CT and MR examinations containing our internal structured reporting element for cancer were separated into four categories: Progression, Stable Disease, Improvement, or No Cancer. We combined each of three NLP techniques with five ML algorithms to predict the assigned label using the unstructured report text and compared the performance of each combination. The three NLP algorithms included term frequency-inverse document frequency (TF-IDF), term frequency weighting (TF), and 16-bit feature hashing. The ML algorithms included logistic regression (LR), random decision forest (RDF), one-vs-all support vector machine (SVM), one-vs-all Bayes point machine (BPM), and fully connected neural network (NN). The best-performing NLP model consisted of tokenized unigrams and bigrams with TF-IDF. Increasing N-gram length yielded little to no added benefit for most ML algorithms. With all parameters optimized, SVM had the best performance on the test dataset, with 90.6 average accuracy and F score of 0.813. The interplay between ML and NLP algorithms and their effect on interpretation accuracy is complex. The best accuracy is achieved when both algorithms are optimized concurrently.","keywords_author":["Informatics","Machine learning","Natural language processing","Structured reporting"],"keywords_other":["Fully connected neural network","Radiology reports","Decision forest","Structured reporting","Informatics","Term frequencyinverse document frequency (TF-IDF)","Feature hashing","Logistic regressions"],"max_cite":1.0,"pub_year":2018.0,"sources":"['scp']","rawkeys":["informatics","structured reporting","term frequencyinverse document frequency (tf-idf)","machine learning","fully connected neural network","natural language processing","decision forest","radiology reports","logistic regressions","feature hashing"],"tags":["informatics","structured reporting","term frequencyinverse document frequency (tf-idf)","machine learning","fully connected neural network","natural language processing","decision forest","radiology reports","logistic regressions","feature hashing"]},{"p_id":48742,"title":"DataBros@Information retrieval from microblogs during disasters(IRMiDis)","abstract":"Microblogging sites like Twiter are increasingly being used for aiding relief operations during disaster events. In such situations, identifying actionable information like needs and availabilities of various types of resources is critical for effective coordination of post disaster relief operations. However, such critical information is usually submerged within a lot of conversational content, such as sympathy for the victims of the disaster. Hence, automated IR techniques are needed to find and process such information[1].","keywords_author":["Data mining","Machine learning","NLP"],"keywords_other":["IR techniques","Microblogging","Post disasters","Relief operations","Microblogs"],"max_cite":0.0,"pub_year":2017.0,"sources":"['scp']","rawkeys":["data mining","nlp","microblogs","machine learning","ir techniques","post disasters","microblogging","relief operations"],"tags":["data mining","natural language processing","machine learning","ir techniques","post disasters","microblogging","relief operations"]},{"p_id":1642,"title":"Deep neural networks: A case study for music genre classification","abstract":"Music classification is a challenging problem with many applications in today's large-scale datasets with Gigabytes of music files and associated metadata and online streaming services. Recent success with deep neural network architectures on large-scale datasets has inspired numerous studies in the machine learning community for various pattern recognition and classification tasks such as automatic speech recognition, natural language processing, audio classification and computer vision. In this paper, we explore a two-layer neural network with manifold learning techniques for music genre classification. We compare the classification accuracy rate of deep neural networks with a set of well-known learning models including support vector machines (SVM and '1-SVM), logistic regression and '1-regression in combination with hand-crafted audio features for a genre classification task on a public dataset. Our experimental results show that neural networks are comparable with classic learning models when the data is represented in a rich feature space.","keywords_author":null,"keywords_other":["Neural networks","Music","Manifolds","Feature extraction","pattern recognition task","Music genre classification","Mel frequency cepstral coefficient","Deep neural networks","associated metadata","two-layer neural network","learning (artificial intelligence)","deep neural network architectures","manifold learning","online streaming services","machine learning community","neural nets","pattern classification","NAtural language processing","Music classification","Audio classification","music genre classification","music files","Spectrogram","deep neural networks","Large-scale datasets","Automatic speech recognition","Support vector machines","Classification accuracy","music"],"max_cite":4.0,"pub_year":2016.0,"sources":"['ieee', 'scp']","rawkeys":["automatic speech recognition","manifolds","pattern recognition task","audio classification","associated metadata","two-layer neural network","music classification","learning (artificial intelligence)","neural networks","spectrogram","deep neural network architectures","manifold learning","machine learning community","online streaming services","neural nets","pattern classification","music files","music genre classification","deep neural networks","classification accuracy","large-scale datasets","natural language processing","mel frequency cepstral coefficient","feature extraction","support vector machines","music"],"tags":["automatic speech recognition","manifolds","convolutional neural network","machine learning","audio classification","associated metadata","two-layer neural network","music classification","neural networks","deep neural network architectures","manifold learning","machine learning communities","mel-frequency cepstral coefficients","online streaming services","pattern classification","music files","music genre classification","pattern recognition tasks","classification accuracy","large-scale datasets","natural language processing","feature extraction","spectrograms","music"]},{"p_id":26219,"title":"World knowledge in broad-coverage information filtering","abstract":null,"keywords_author":["Financial news filtering","Machine learning","NLP","World knowledge"],"keywords_other":null,"max_cite":7.0,"pub_year":2007.0,"sources":"['scp']","rawkeys":["world knowledge","financial news filtering","nlp","machine learning"],"tags":["natural language processing","world knowledge","financial news filtering","machine learning"]},{"p_id":50794,"title":"Malayalam morphological analysis using MBLP approach","abstract":"\u00a9 2015 IEEE. This paper presents an approach to morphological analysis of Malayalam words as a classification Problem. The idea here is to use Memory Based Language Processing (MBLP) algorithm for Malayalam morphological analysis. MBLP is an approach to language processing based on exemplar storage during learning, and analogical reasoning during processing. The aim of the system is to find the citation forms (or meaningful parts) of words rather than a detailed morphological analysis. Training instances created from words are manually annotated for their segmentation and the system is trained using TiMBL (Tilburg Memory based Learner). The paper presents memory based model of Malayalam morphological analysis and its generalization accuracy.","keywords_author":["Machine Learning","Malayalam Morphological Analysis","Memory Based Language Processing","Morphology","Natural Language Processing","Natural Languages","Supervised Learning"],"keywords_other":["Morphological analysis","Generalization accuracy","Language processing","Memory-based modeling","Malayalams","Natural languages","Analogical reasoning","NAtural language processing"],"max_cite":0.0,"pub_year":2015.0,"sources":"['scp']","rawkeys":["supervised learning","malayalams","morphological analysis","natural languages","machine learning","natural language processing","morphology","malayalam morphological analysis","memory based language processing","analogical reasoning","language processing","generalization accuracy","memory-based modeling"],"tags":["supervised learning","malayalams","morphological analysis","natural languages","machine learning","natural language processing","morphology","malayalam morphological analysis","memory based language processing","analogical reasoning","language processing","generalization accuracy","memory-based modeling"]},{"p_id":28269,"title":"Transfer learning based clinical concept extraction on data from multiple sources","abstract":"\u00a9 2014 Elsevier Inc.Machine learning methods usually assume that training data and test data are drawn from the same distribution. However, this assumption often cannot be satisfied in the task of clinical concept extraction. The main aim of this paper was to use training data from one institution to build a concept extraction model for data from another institution with a different distribution. An instance-based transfer learning method, TrAdaBoost, was applied in this work. To prevent the occurrence of a negative transfer phenomenon with TrAdaBoost, we integrated it with Bagging, which provides a \"softer\" weights update mechanism with only a tiny amount of training data from the target domain. Two data sets named BETH and PARTNERS from the 2010 i2b2\/VA challenge as well as BETHBIO, a data set we constructed ourselves, were employed to show the effectiveness of our work's transfer ability. Our method outperforms the baseline model by 2.3% and 4.4% when the baseline model is trained by training data that are combined from the source domain and the target domain in two experiments of BETH vs. PARTNERS and BETHBIO vs. PARTNERS, respectively. Additionally, confidence intervals for the performance metrics suggest that our method's results have statistical significance. Moreover, we explore the applicability of our method for further experiments. With our method, only a tiny amount of labeled data from the target domain is required to build a concept extraction model that produces better performance.","keywords_author":["Bagging","Clinical concept extraction","Machine learning","TrAdaBoost","Transfer learning"],"keywords_other":["Statistical significance","Transfer learning","Electronic Health Records","Humans","Concept extraction","Natural Language Processing","Medical Informatics","Bagging","Vocabulary, Controlled","Tradaboost","Artificial Intelligence","Machine learning methods","Data Mining","Transfer learning methods","Different distributions"],"max_cite":5.0,"pub_year":2014.0,"sources":"['scp']","rawkeys":["machine learning methods","artificial intelligence","tradaboost","data mining","vocabulary","transfer learning","machine learning","clinical concept extraction","electronic health records","humans","natural language processing","medical informatics","controlled","bagging","statistical significance","concept extraction","transfer learning methods","different distributions"],"tags":["machine learning methods","tradaboost","vocabulary","data mining","transfer learning","machine learning","clinical concept extraction","control","electronic health records","humans","natural language processing","medical informatics","bagging","statistical significance","concept extraction","transfer learning methods","different distributions"]},{"p_id":52845,"title":"Tuning machine-learning algorithms for battery-operated portable devices","abstract":"Machine learning algorithms in various forms are now increasingly being used on a variety of portable devices, starting from cell phones to PDAs. They often form a part of standard applications (e.g. for grammar-checking in email clients) that run on these devices and occupy a significant fraction of processor and memory bandwidth. However, most of the research within the machine learning community has ignored issues like memory usage and power consumption of processors running these algorithms. In this paper we investigate how machine learned models can be developed in a power-aware manner for deployment on resource-constrained portable devices. We show that by tolerating a small loss in accuracy, it is possible to dramatically improve the energy consumption and data cache behavior of these algorithms. More specifically, we explore a typical sequential labeling problem of part-of-speech tagging in natural language processing and show that a power-aware design can achieve up to 50% reduction in power consumption, trading off a minimal decrease in tagging accuracy of 3%. \u00a9 2010 Springer-Verlag.","keywords_author":["Low-power Machine Learned Models","Mobile Machine Learning Applications","Part-of-speech Tagging","Power-aware Design"],"keywords_other":["Power-aware design","Memory bandwidths","Energy consumption","Data caches","Email clients","Machine learning algorithms","Portable device","Resource-constrained","Low Power","Low-power Machine Learned Models","Power Consumption","NAtural language processing","Power-aware","Part of speech tagging","Memory usage","Cell phone","Mobile machines","Machine learning communities","Tuning machine"],"max_cite":0.0,"pub_year":2010.0,"sources":"['scp']","rawkeys":["energy consumption","machine learning algorithms","resource-constrained","mobile machine learning applications","memory bandwidths","mobile machines","part-of-speech tagging","power-aware design","data caches","low-power machine learned models","cell phone","tuning machine","power consumption","power-aware","machine learning communities","memory usage","portable device","natural language processing","part of speech tagging","email clients","low power"],"tags":["energy consumption","machine learning algorithms","resource-constrained","mobile machine learning applications","memory bandwidths","cell phones","mobile machines","power-aware design","data caches","low-power machine learned models","tuning machine","power consumption","power-aware","machine learning communities","memory usage","portable device","natural language processing","part of speech tagging","email clients","low power"]},{"p_id":30320,"title":"Grammar-based classifier system: A universal tool for grammatical inference","abstract":"Grammatical Inference deals with the problem of learning structural models, such as grammars, from different sort of data patterns, such as artificial languages, natural languages, biosequences, speech and so on. This article describes a new grammatical inference tool, Grammar-based Classifier System (GCS) dedicated to learn grammar from data. GCS is a new model of Learning Classifier Systems in which the population of classifiers has a form of a context-free grammar rule set in a Chomsky Normal Form. GCS has been proposed to address both regular language induction and the natural language grammar induction as well as learning formal grammar for DNA sequence. In all cases near-optimal solutions or better than reported in the literature were obtained.","keywords_author":["DFA induction","Grammatical inference","Learning classifier systems","Machine learning","Natural language processing","Promoter recognition","Regular language induction"],"keywords_other":["DFA induction","Learning classifier systems","Regular language induction","Machine learning","Grammatical inference","Natural language processing","Promoter recognition"],"max_cite":4.0,"pub_year":2008.0,"sources":"['scp']","rawkeys":["natural language processing","machine learning","grammatical inference","learning classifier systems","regular language induction","promoter recognition","dfa induction"],"tags":["learning classifier system","natural language processing","machine learning","grammatical inference","regular language induction","promoter recognition","dfa induction"]},{"p_id":28274,"title":"Combining textual pre-game reports and statistical data for predicting success in the National Hockey League","abstract":"In this paper, we create meta-classifiers to forecast success in the National Hockey League. We combine three classifiers that use various types of information. The first one uses as features numerical data and statistics collected during previous games. The last two classifiers use pre-game textual reports: one classifier uses words as features (unigrams, bigrams and trigrams) in order to detect the main ideas expressed in the texts and the second one uses features based on counts of positive and negative words in order to detect the opinions of the pre-game report writers. Our results show that meta classifiers that use the two data sources combined in various ways obtain better prediction accuracies than classifiers that use only numerical data or only textual data. \u00a9 2014 Springer International Publishing.","keywords_author":["ice hockey","machine learning","monte carlo method","national hockey league","natural language processing","NHL","sentiment analysis"],"keywords_other":["national hockey league","Sentiment analysis","Ice hockey","NHL","NAtural language processing"],"max_cite":5.0,"pub_year":2014.0,"sources":"['scp']","rawkeys":["national hockey league","natural language processing","machine learning","nhl","ice hockey","monte carlo method","sentiment analysis"],"tags":["national hockey league","natural language processing","machine learning","nhl","ice hockey","sentiment analysis","monte carlo methods"]},{"p_id":48757,"title":"Development of a model to predict intention using deep learning","abstract":"This paper presents a method to analyze discussions from social network by using deep learning. We have prepared a new dataset by collecting discussions from a social network and annotating remarks of the discussion. The annotation consists of two types of labels for each message: intention type and direction of intention. Using this dataset and pre- Trained word embeddings we have evaluated two neural network structures. On the basis of evaluation, we chose a model to automatically predict intention types and direction of intention of an arbitrary message from any social network.","keywords_author":["Deep learning","Intention analysis","Natural language processing"],"keywords_other":["Embeddings","Neural network structures","Intention analysis"],"max_cite":0.0,"pub_year":2017.0,"sources":"['scp']","rawkeys":["embeddings","deep learning","natural language processing","neural network structures","intention analysis"],"tags":["embeddings","machine learning","natural language processing","neural network structures","intention analysis"]},{"p_id":1655,"title":"Distance metric learning for kernel density-based acoustic model under limited training data conditions","abstract":"Kernel density model works well for limited training data in acoustic modeling. In this paper, we improve the kernel density-based acoustic model for low resource language speech recognition. In our previous study, we demonstrated the effectiveness of the kernel density-based acoustic model on discriminative features such as cross-lingual bottleneck features. In this paper, we propose to learn a Mahalanobis-based distance, which is equivalent to a full rank linear feature transformation, to minimize training data frame classification error. Experimental results on the Wall Street Journal (WSJ) task show that the proposed Mahalanobis-based distance learning results in significant improvements over the Euclidean distance. The kernel density acoustic model with the Mahalanobis-based distance also outperforms deep neural network acoustic model significantly in limited training data cases.","keywords_author":null,"keywords_other":["training data frame classification error","Measurement","kernel density-based acoustic model","speech recognition","Kernel","WSJ task","kernel density acoustic model","limited training data conditions","full rank linear feature transformation","acoustic signal processing","Training data","Training","Hidden Markov models","signal classification","Data models","Wall Street Journal task","natural language processing","acoustic modeling","distance metric learning","language speech recognition","Mahalanobis-based distance learning","Acoustics"],"max_cite":1.0,"pub_year":2015.0,"sources":"['ieee']","rawkeys":["training data frame classification error","kernel density-based acoustic model","speech recognition","kernel","acoustics","training data","kernel density acoustic model","limited training data conditions","full rank linear feature transformation","acoustic signal processing","mahalanobis-based distance learning","measurement","training","signal classification","hidden markov models","wsj task","natural language processing","acoustic modeling","distance metric learning","language speech recognition","data models","wall street journal task"],"tags":["training data frame classification error","kernel density-based acoustic model","speech recognition","kernel","acoustics","training data","kernel density acoustic model","limited training data conditions","full rank linear feature transformation","acoustic signal processing","mahalanobis-based distance learning","measurement","training","signal classification","hidden markov models","wsj task","acoustic model","natural language processing","distance metric learning","language speech recognition","data models","wall street journal task"]},{"p_id":38519,"title":"Syntax-based Statistical Machine Translation","abstract":"Copyright \u00a9 2016 by Morgan & Claypool. This unique book provides a comprehensive introduction to the most popular syntax-based statistical machine translation models, filling a gap in the current literature for researchers and developers in human language technologies. While phrase-based models have previously dominated the field, syntax-based approaches have proved a popular alternative, as they elegantly solve many of the shortcomings of phrase-based models. The heart of this book is a detailed introduction to decoding for syntax-based models. The book begins with an overview of synchronous-context free grammar (SCFG) and synchronous tree-substitution grammar (STSG) along with their associated statistical models. It also describes how three popular instantiations (Hiero, SAMT, and GHKM) are learned from parallel corpora. It introduces and details hypergraphs and associated general algorithms, as well as algorithms for decoding with both tree and string input. Special attention is given to efficiency, including search approximations such as beam search and cube pruning, data structures, and parsing algorithms. The book consistently highlights the strengths (and limitations) of syntax-based approaches, including their ability to generalize phrase-based translation units, their modeling of specific linguistic phenomena, and their function of structuring the search space. Table of Contents: Preface \/ Acknowledgments \/ Models \/ Learning from Parallel Text \/ Decoding I: Preliminaries \/ Decoding II: Tree Decoding \/ Decoding III: String Decoding \/ Selected Topics \/ Closing Remarks \/ Bibliography \/ Authors' Biographies \/ Author Index \/ Index.","keywords_author":["computational linguistics","machine learning","natural language processing","statistical machine translation","statistical modeling","synchronous grammar formalisms","syntax"],"keywords_other":["Statistical modeling","Statistical machine translation","Tree substitution grammars","syntax","Synchronous grammars","Human language technologies","Syntax-based approach","Synchronous context-free grammars"],"max_cite":1.0,"pub_year":2016.0,"sources":"['scp']","rawkeys":["synchronous grammars","human language technologies","synchronous context-free grammars","synchronous grammar formalisms","statistical modeling","natural language processing","machine learning","syntax","computational linguistics","syntax-based approach","tree substitution grammars","statistical machine translation"],"tags":["synchronous grammars","human language technologies","synchronous context-free grammars","synchronous grammar formalisms","statistical models","natural language processing","machine learning","syntax","computational linguistics","syntax-based approach","tree substitution grammars","statistical machine translation"]},{"p_id":73337,"title":"Natural language based financial forecasting: a survey","abstract":"Natural language processing (NLP), or the pragmatic research perspective of computational linguistics, has become increasingly powerful due to data availability and various techniques developed in the past decade. This increasing capability makes it possible to capture sentiments more accurately and semantics in a more nuanced way. Naturally, many applications are starting to seek improvements by adopting cutting-edge NLP techniques. Financial forecasting is no exception. As a result, articles that leverage NLP techniques to predict financial markets are fast accumulating, gradually establishing the research field of natural language based financial forecasting (NLFF), or from the application perspective, stock market prediction. This review article clarifies the scope of NLFF research by ordering and structuring techniques and applications from related work. The survey also aims to increase the understanding of progress and hotspots in NLFF, and bring about discussions across many different disciplines.","keywords_author":["Financial forecasting","Natural language processing","Text mining","Predictive analytics","Knowledge engineering","Computational finance"],"keywords_other":["PREDICTION","MICROBLOGGING DATA","RETURNS","NEWS","TIME","MODELS","SENTIMENT ANALYSIS","NEURAL-NETWORK","TEXTUAL ANALYSIS","STOCK-MARKET"],"max_cite":5.0,"pub_year":2017.0,"sources":"['ieee', 'wos']","rawkeys":["computational finance","microblogging data","stock-market","text mining","predictive analytics","prediction","natural language processing","returns","news","textual analysis","time","financial forecasting","knowledge engineering","models","neural-network","sentiment analysis"],"tags":["computational finance","microblogging data","stock market","text mining","model","neural networks","predictive analytics","prediction","natural language processing","returns","news","textual analysis","time","financial forecasting","knowledge engineering","sentiment analysis"]},{"p_id":34425,"title":"Interpreting Medical Information Using Machine Learning and Individual Conditional Expectation","abstract":"\u00a9 2015 IMIA and IOS Press.Recently, machine-learning techniques have spread many fields. However, machine-learning is still not popular in medical research field due to difficulty of interpreting. In this paper, we introduce a method of interpreting medical information using machine learning technique. The method gave new explanation of partial dependence plot and individual conditional expectation plot from medical research field.","keywords_author":["Individual conditional expectation","Machine learning","Partial dependence plot"],"keywords_other":["Likelihood Functions","Algorithms","Electronic Health Records","Effect Modifier, Epidemiologic","Humans","Natural Language Processing","Machine Learning","Data Mining","Outcome Assessment (Health Care)","Data Interpretation, Statistical","Decision Support Systems, Clinical"],"max_cite":2.0,"pub_year":2015.0,"sources":"['scp']","rawkeys":["outcome assessment (health care)","statistical","data mining","effect modifier","partial dependence plot","machine learning","data interpretation","electronic health records","humans","individual conditional expectation","natural language processing","clinical","likelihood functions","algorithms","decision support systems","epidemiologic"],"tags":["statistics","data mining","health care","effect modifier","partial dependence plot","epidemiology","data interpretation","electronic health records","humans","individual conditional expectation","machine learning","clinical","likelihood functions","natural language processing","algorithms","decision support systems"]},{"p_id":50811,"title":"Using character valence in computer generated music to produce variation aligned to a storyline","abstract":"\u00a9 2015 ACM.This paper is to describes a method for interposing computer generated melody with tone linked to unique entities within the text of a novel. Background: A recent study describing a piece of software called \"TransProse\" has already shown that sentiment in the text of a novel can be used to automatically generate simple piano music that reflects the same sentiment as the novel. This study wished to establish a method whereby, if after aligning the text with the melody, the sentiment in the words surrounding particular characters as they occurred within the novel could produce another melody line, for each character, that could reflect the individual characters' tone and distinguish the melodies ascribed to each character from each other. Method: The sentiment in the text of the novel is extracted by looking up the words in a database that groups the words into emotional groups called \"Ekman categories\". Simplistic relations between aspects of music such as pitch and tempo are chosen based on the two categories that contained the most words. These chosen attributes are then used to generate the first two melody lines. The paragraphs within which the named entities referring to characters are found is manually determined and the top \"Ekman category\" of the named entities is obtained through simplistic methods of extraction. Each bar of the melody is aligned with individual paragraphs of text and an additional melody line is generated for each character. Results: Adjusting the fitness function of the Genetic algorithm (GA) that was used was not sufficient to link the tone of the characters to the melody. Assigning each character their own short melodic phrase and varying the phrase appropriately achieved the desired outcome but requires additional work to harmonise better with the first two melody lines.","keywords_author":["Applications of Machine Learning","Artificial Intelligence","Music","Natural Language Processing","Opinion mining","Sentiment analysis"],"keywords_other":["Named entities","Fitness functions","Computer generated music","Sentiment analysis","Music","NAtural language processing","Computer generated","Opinion mining"],"max_cite":0.0,"pub_year":2015.0,"sources":"['scp']","rawkeys":["fitness functions","artificial intelligence","opinion mining","computer generated","natural language processing","applications of machine learning","named entities","computer generated music","sentiment analysis","music"],"tags":["fitness functions","opinion mining","computer generated","machine learning","natural language processing","applications of machine learning","named entities","computer generated music","sentiment analysis","music"]},{"p_id":73339,"title":"Extracting medical events from clinical records using conditional random fields and parameter tuning for hidden Markov models","abstract":"Recently, the extraction of clinical events from unstructured medical texts has attracted much attention of the research community. Machine learning approaches are popular for this task, due to their ability to solve the problem of sequence tagging effectively. It has been suggested previously that simple features, such as word unigrams, part-of-speech tags, chunk tags, among others, are sufficient for this task. We show that more careful preprocessing and feature selection can significantly improve the results. We used conditional random field classifier with more linguistically oriented features and outperformed the current state-of-the-art approaches. We also show that the popular and much simpler Viterbi algorithm (hidden Markov model-based classification algorithm) can produce competitive results, when its parameters are tuned using specific optimization techniques. We evaluate these algorithms for the task of extraction of medical events from the corpus developed for SemEval shared Task 12: Clinical TempEval (Temporal Evaluation) 2016, namely, for its two subtasks: (i) event detection and (ii) event classification based on contextual modality.","keywords_author":["Clinical reports","medical information extraction","natural language processing","machine learning","feature selection","conditional random field","Viterbi algorithm"],"keywords_other":["TEXT","FEATURES","NAMED ENTITY RECOGNITION","NEURAL-NETWORK","LANGUAGE"],"max_cite":0.0,"pub_year":2018.0,"sources":"['wos']","rawkeys":["clinical reports","features","conditional random field","medical information extraction","named entity recognition","machine learning","natural language processing","text","feature selection","neural-network","language","viterbi algorithm"],"tags":["clinical reports","features","neural networks","conditional random field","medical information extraction","named entity recognition","machine learning","natural language processing","text","feature selection","language","viterbi algorithm"]},{"p_id":9853,"title":"Query-oriented unsupervised multi-document summarization via deep learning model","abstract":"Capturing the compositional process from words to documents is a key challenge in natural language processing and information retrieval: Extractive style query-oriented multi-document summarization generates a summary by extracting a proper set of sentences from multiple documents based on pre-given query. This paper proposes a novel document summarization framework based on deep learning model, which has been shown outstanding extraction ability in many real-world applications. The framework consists of three parts: concepts extraction, summary generation, and reconstruction validation. A new query-oriented extraction technique is proposed to extract information distributed in multiple documents. Then, the whole deep architecture is fine-tuned by minimizing the information loss in reconstruction validation. According to the concepts extracted from deep architecture layer by layer, dynamic programming is used to seek most informative set of sentences for the summary. Experiment on three benchmark datasets (DUC 2005, 2006, and 2007) assess and confirm the effectiveness of the proposed framework and algorithms. Experiment results show that the proposed method outperforms state-of-the-art extractive summarization approaches. Moreover, we also provide the statistical analysis of query words based on Amazon's Mechanical Turk (MTurk) crowdsourcing platform. There exists underlying relationships from topic words to the content which can contribute to summarization task. (C) 2015 Elsevier Ltd. All rights reserved.","keywords_author":["Deep learning","Multi-document","Neocortex simulation","Query-oriented summarization","Deep learning","Query-oriented summarization","Multi-document","Neocortex simulation"],"keywords_other":["Deep learning","Multi-document","Multi-document summarization","Neocortex simulation","ALGORITHM","VISUAL-CORTEX","Extractive summarizations","Query-oriented summarization","NEURAL-NETWORKS","INFERENCE","Amazon's mechanical turks","NAtural language processing"],"max_cite":13.0,"pub_year":2015.0,"sources":"['ieee', 'scp', 'wos']","rawkeys":["neural-networks","algorithm","visual-cortex","deep learning","multi-document summarization","natural language processing","amazon's mechanical turks","inference","query-oriented summarization","extractive summarizations","neocortex simulation","multi-document"],"tags":["visual-cortex","neural networks","multi-document summarization","machine learning","natural language processing","amazon's mechanical turks","inference","algorithms","query-oriented summarization","extractive summarizations","neocortex simulation","multi-document"]},{"p_id":7806,"title":"Arabic natural language processing: Challenges and solutions","abstract":"The Arabic language presents researchers and developers of natural language processing (NLP) applications for Arabic text and speech with serious challenges. The purpose of this article is to describe some of these challenges and to present some solutions that would guide current and future practitioners in the field of Arabic natural language processing (ANLP). We begin with general features of the Arabic language in Sections 1, 2, and 3 and then we move to more specific properties of the language in the rest of the article. In Section 1 of this article we highlight the significance of the Arabic language today and describe its general properties. Section 2 presents the feature of Arabic Diglossia showing how the sociolinguistic aspects of the Arabic language differ from other languages. The stability of Arabic Diglossia and its implications for ANLP applications are discussed and ways to deal with this problematic property are proposed. Section 3 deals with the properties of the Arabic script and the explosion of ambiguity that results from the absence of short vowel representations and overt case markers in contemporary Arabic texts. We present in Section 4 specific features of the Arabic language such as the nonconcatenative property of Arabic morphology, Arabic as an agglutinative language, Arabic as a pro-drop language, and the challenge these properties pose to ANLP. We also present solutions that have already been adopted by some pioneering researchers in the field. In Section 5 we point out to the lack of formal and explicit grammars of Modern Standard Arabic which impedes the progress of more advanced ANLP systems. In Section 6 we draw our conclusion. \u00a9 2009 ACM.","keywords_author":["Arabic dialects","Arabic script","Modern Standard Arabic"],"keywords_other":["Agglutinative language","Arabic natural language processing","Arabic texts","Arabic scripts","Non-concatenative","Modern standards","Arabic languages","Natural language processing"],"max_cite":125.0,"pub_year":2009.0,"sources":"['scp', 'wos']","rawkeys":["arabic scripts","arabic dialects","modern standards","agglutinative language","natural language processing","arabic natural language processing","modern standard arabic","non-concatenative","arabic languages","arabic texts","arabic script"],"tags":["arabic scripts","modern standards","agglutinative language","natural language processing","arabic natural language processing","modern standard arabic","non-concatenative","arabic languages","arabic texts","arabic dialects"]},{"p_id":48761,"title":"Author identification using deep learning","abstract":"\u00a9 2016 IEEE.Authorship identification is the task of identifying the author of a given text from a set of suspects. The main concern of this task is to define an appropriate characterization of texts that captures the writing style of authors. Although deep learning was recently used in different natural language processing tasks, it has not been used in author identification (to the best of our knowledge). In this paper, deep learning is used for feature extraction of documents represented using variable size character n-grams. We apply A Stacked Denoising Auto-Encoder (SDAE) for extracting document features with different settings, and then a support vector machine classifier is used for classification. The results show that the proposed system outperforms its counterparts.","keywords_author":["Author identification","Deep learning","Denoising autoencoder"],"keywords_other":["Authorship identification","Variable sizes","De-noising","Writing style","Support vector machine classifiers","Auto encoders","NAtural language processing","Author identification"],"max_cite":0.0,"pub_year":2017.0,"sources":"['scp']","rawkeys":["support vector machine classifiers","writing style","authorship identification","deep learning","author identification","denoising autoencoder","auto encoders","natural language processing","variable sizes","de-noising"],"tags":["support vector machine classifiers","writing style","authorship identification","author identification","denoising autoencoder","auto encoders","machine learning","natural language processing","variable sizes","de-noising"]},{"p_id":1656,"title":"Multilingual exemplar-based acoustic model for the NIST Open KWS 2015 evaluation","abstract":"In this paper, we investigate the use of the proposed non-parametric exemplar-based acoustic modeling for the NIST Open Keyword Search 2015 Evaluation. Specifically, kernel-density model is used to replace GMM in HMM\/GMM (Hidden Markov Model \/ Gaussian Mixture Model) or DNN in HMM\/DNN (Hidden Markov Model \/ Deep Neural Network) acoustic model to predict the emission probability of HMM states. To get further improvement, likelihood score generated by the kernel-density model is discriminatively tuned by the score tuning module realized by a neural network. Various configurations for score tuning module have been examined to show that simple neural network with 1 hidden layer is sufficient to fine tune the likelihood score generated by the kernel-density model. With this architecture, our exemplar-based model outperforms the 9-layer-DNN acoustic model significantly for both the speech recognition and keyword search tasks. In addition, our proposed exemplar-based system provides complementary information to other systems and we can further benefit from system combination.","keywords_author":null,"keywords_other":["neural network","automatic speech recognition","kernel-density model","score tuning module","NIST Open KWS 2015 Evaluation","speech recognition","Kernel","Neural networks","multilingual exemplar-based acoustic model","Tuning","hidden Markov models","HMM state","Training","hidden Markov model","emission probability prediction","Hidden Markov models","neural nets","probability","natural language processing","Speech recognition","ASR system","Acoustics"],"max_cite":null,"pub_year":2015.0,"sources":"['ieee']","rawkeys":["neural network","automatic speech recognition","kernel-density model","score tuning module","speech recognition","kernel","hidden markov model","acoustics","multilingual exemplar-based acoustic model","asr system","neural networks","training","emission probability prediction","hmm state","neural nets","probability","nist open kws 2015 evaluation","hidden markov models","natural language processing","tuning"],"tags":["hidden markov models","automatic speech recognition","neural networks","kernel-density model","score tuning module","multilingual exemplar-based acoustic model","natural language processing","speech recognition","training","kernel","emission probability prediction","asr system","hmm state","acoustics","probability","nist open kws 2015 evaluation","tuning"]},{"p_id":7809,"title":"Comparative analysis of five protein-protein interaction corpora","abstract":"Background: Growing interest in the application of natural language processing methods to biomedical text has led to an increasing number of corpora and methods targeting protein-protein interaction (PPI) extraction. However, there is no general consensus regarding PPI annotation and consequently resources are largely incompatible and methods are difficult to evaluate. Results: We present the first comparative evaluation of the diverse PPI corpora, performing quantitative evaluation using two separate information extraction methods as well as detailed statistical and qualitative analyses of their properties. For the evaluation, we unify the corpus PPI annotations to a shared level of information, consisting of undirected, untyped binary interactions of non-static types with no identification of the words specifying the interaction, no negations, and no interaction certainty. We find that the F-score performance of astate-of-the-art PPI extraction method varies on average 19 percentage units and in some cases over 30 percentage units between the different evaluated corpora. The differences stemming from the choice of corpus can thus be substantially larger than differences between the performance of PPI extraction methods, which suggests definite limits on the ability to compare methods evaluated on different resources. We analyse a number of potential sources for these differences and identify factors explaining approximately half of the variance. We further suggest ways in which the difficulty of the PPI extraction tasks codified by different corpora can be determined to advance comparability. Our analysis also identifies points of agreement and disagreement in PPI corpus annotation that are rarely explicitly stated by the authors of the corpora. Conclusions: Our comparative analysis uncovers key similarities and differences between the diverse PPI corpora, thus taking an important step towards standardization. In the course of this study we have created a major practical contribution in converting the corpora into a shared format. The conversion software is freely available at http:\/\/mars.cs.utu.fi\/PPICorpora. \u00a9 2008 Pyysalo et al.; licensee BioMed Central Ltd.","keywords_author":null,"keywords_other":["Information extraction methods","Qualitative analysis","Comparative evaluations","Quantitative evaluation","Comparative analysis","Protein-protein interactions","Binary interactions","NAtural language processing"],"max_cite":125.0,"pub_year":2008.0,"sources":"['scp', 'wos']","rawkeys":["binary interactions","information extraction methods","natural language processing","comparative analysis","quantitative evaluation","protein-protein interactions","qualitative analysis","comparative evaluations"],"tags":["binary interactions","information extraction methods","natural language processing","comparative analysis","quantitative evaluation","protein-protein interactions","qualitative analysis","comparative evaluations"]},{"p_id":34433,"title":"Natural language processing to quantify security effort in the software development lifecycle","abstract":"Copyright \u00a9 2015 by KSI Research Inc. and Knowledge Systems Institute Graduate School. Addressing security in the software development lifecycle is an ever-present concern for software engineers and organizations. From a management and monitoring perspective, it is difficult to measure 1) the amount of effort being focused on security concerns during active development and 2) the success of security related design and development efforts. Such data is simply not recorded. If reliable measurements were available, software project leaders would have a powerful tool to assess risk and inform decision making. This would enable managers to direct development and testing to assure a desired level of security in their software products, to protect both their organizations and customers. To fill this need and provide such data, we propose a technique for performing topic detection on data commonly available in most software development projects: text artifacts from issue tracking and version control systems. We apply machine learning and natural language processing techniques to create classifiers capable of accurately detecting whether a given text snippet is related to the topic of security. Realization of such a capability will give software teams the ability to analyze current and past levels of security effort, revealing immediate project focus and the long-term impacts of security tasking. We validate our approach via experiments on data from the large-scale open source Chromium software project. Our results show that a Na\u00efve Bayes classification scheme using an n-gram feature-space is an appropriate and effective approach to automated topic detection of software security text snippets, and that effective training data can be derived from public data sources without the need for manual intervention.","keywords_author":["Classification","Machine Learning","Natural Language Processing","Na\u00efve Bayes","Security","Software Security","Topic Defection"],"keywords_other":["Software security","Software development life cycle","Software development projects","Topic Defection","Security","Design and Development","Development and testing","NAtural language processing"],"max_cite":2.0,"pub_year":2015.0,"sources":"['scp']","rawkeys":["na\u00efve bayes","software development life cycle","machine learning","natural language processing","software development projects","security","software security","topic defection","classification","design and development","development and testing"],"tags":["software development life cycle","machine learning","natural language processing","software development projects","security","software security","topic defection","classification","design and development","development and testing","naive bayes"]},{"p_id":13955,"title":"Deep learning: yesterday, today, and tomorrow","abstract":"Machine learning is an important area of artificial intelligence. Since 1980s, huge success has been achieved in terms of algorithms, theory, and applications. From 2006, a new machine learning paradigm, named deep learning, has been popular in the research community, and has become a huge wave of technology trend for big data and artificial intelligence. Deep learning simulates the hierarchical structure of human brain, processing data from lower level to higher level, and gradually composing more and more semantic concepts. In recent years, Google, Microsoft, IBM, and Baidu have invested a lot of resources into the R&D of deep learning, making significant progresses on speech recognition, image understanding, natural language processing, and online advertising. In terms of the contribution to real-world applications, deep learning is perhaps the most successful progress made by the machine learning community in the last 10 years. In this article, we will give a high-level overview about the past and current stage of deep learning, discuss the main challenges, and share our views on the future development of deep learning.","keywords_author":["Deep learning","Image recognition","Machine learning","Natural language processing","Online advertising","Speech recognition"],"keywords_other":["Deep learning","Learning paradigms","Hierarchical structures","Online advertising","Research communities","Technology trends","Machine learning communities","NAtural language processing"],"max_cite":67.0,"pub_year":2013.0,"sources":"['scp', 'ieee']","rawkeys":["hierarchical structures","deep learning","online advertising","machine learning","natural language processing","speech recognition","technology trends","machine learning communities","image recognition","research communities","learning paradigms"],"tags":["hierarchical structures","online advertising","technological trends","machine learning","natural language processing","speech recognition","machine learning communities","image recognition","research communities","learning paradigms"]},{"p_id":52871,"title":"Building a biomedical tokenizer using the token lattice design pattern and the adapted viterbi algorithm","abstract":"Proper tokenization of biomedical text is a nontrivial problem. Problematic characteristics of current biomedical tokenizers include idiosyncratic tokenizer output and poor tokenizer extensibility and reuse. To address these problematic characteristics, we identified and completed a novel tokenizer design pattern for biomedical tokenizers. We separated a tokenizer into three components: a token lattice and lattice constructor, a best lattice-path chooser and token transducers. Token transducers create tokens from text. These tokens are assembled into a token lattice by the lattice constructor. The best path (tokenization) is selected from the token lattice, tokenizing the text. We applied our design pattern and our token transducer identification guidelines in the creation of a tokenizer for SNOMED CT concept descriptions and compared our tokenizer to three other tokenizer methods. Medpost and our adapted Viterbi tokenizer perform best with a 90.1% and 93.7% accuracy respectively. \u00a9 2010 IEEE.","keywords_author":["Machine learning","Medicine and science","Natural language processing","Patterns"],"keywords_other":["Concept description","Viterbi","Machine-learning","Tokenization","Three component","Patterns","Tokenizing","Design Patterns","Lattice design","Tokenizer","Biomedical text","Natural language processing"],"max_cite":0.0,"pub_year":2010.0,"sources":"['scp']","rawkeys":["three component","tokenizing","concept description","tokenization","machine learning","natural language processing","patterns","viterbi","lattice design","design patterns","biomedical text","machine-learning","medicine and science","tokenizer"],"tags":["three component","concept description","tokenization","machine learning","natural language processing","patterns","viterbi","lattice design","design patterns","biomedical text","medicine and science"]},{"p_id":46727,"title":"Entity recognition in the biomedical domain using a hybrid approach","abstract":"\u00a9 2017 The Author(s).Background: This article describes a high-recall, high-precision approach for the extraction of biomedical entities from scientific articles. Method: The approach uses a two-stage pipeline, combining a dictionary-based entity recognizer with a machine-learning classifier. First, the OGER entity recognizer, which has a bias towards high recall, annotates the terms that appear in selected domain ontologies. Subsequently, the Distiller framework uses this information as a feature for a machine learning algorithm to select the relevant entities only. For this step, we compare two different supervised machine-learning algorithms: Conditional Random Fields and Neural Networks. Results: In an in-domain evaluation using the CRAFT corpus, we test the performance of the combined systems when recognizing chemicals, cell types, cellular components, biological processes, molecular functions, organisms, proteins, and biological sequences. Our best system combines dictionary-based candidate generation with Neural-Network-based filtering. It achieves an overall precision of 86% at a recall of 60% on the named entity recognition task, and a precision of 51% at a recall of 49% on the concept recognition task. Conclusion: These results are to our knowledge the best reported so far in this particular task.","keywords_author":["Machine learning","Named entity recognition","Natural language processing","Text mining"],"keywords_other":["Algorithms","Neural Networks (Computer)","Reproducibility of Results","Terminology as Topic","Pattern Recognition, Automated","Semantics","Machine Learning","Vocabulary, Controlled","Data Mining"],"max_cite":0.0,"pub_year":2017.0,"sources":"['scp']","rawkeys":["terminology as topic","vocabulary","data mining","neural networks (computer)","automated","text mining","semantics","named entity recognition","machine learning","natural language processing","reproducibility of results","controlled","algorithms","pattern recognition"],"tags":["terminology as topic","vocabulary","data mining","text mining","automated","neural networks","semantics","named entity recognition","machine learning","control","natural language processing","reproducibility of results","algorithms","pattern recognition"]},{"p_id":52878,"title":"Giving shape to an N-version dependency parserimproving dependency parsing accuracy for spanish using maltparser","abstract":"Maltparser is a contemporary dependency parsing machine learning-based system that shows great accuracy. However 90% of the Labelled Attachment Score (LAS) seems to be a de facto limit for these kinds of parsers. In this paper we present an n-version dependency parser that will work as follows: we found that there is a small set of words that are more frequently incorrectly parsed so the n-version dependency parser consists of n different parsers trained specifically to parse those difficult words. An algorithm will send each word to each parser and combined with the action of a general parser we will achieve better overall accuracy. This work has been developed specifically for Spanish using Maltparser.","keywords_author":["Corpus-based training","Dependency parsing","Machine learning","Natural language processing"],"keywords_other":["Machine-learning","Corpus-based training","Dependency parser","De facto","Dependency parsing","NAtural language processing"],"max_cite":0.0,"pub_year":2010.0,"sources":"['scp']","rawkeys":["de facto","machine learning","natural language processing","dependency parsing","dependency parser","machine-learning","corpus-based training"],"tags":["de facto","machine learning","natural language processing","dependency parsing","dependency parser","corpus-based training"]},{"p_id":28304,"title":"Emotion detection from \"the SMS of the internet\"","abstract":"Due to the sudden eruption of activity in the social networking domain, analysts, social media as well as general public are drawn to Sentiment Analysis domain to gain invaluable information. In this paper, we go beyond basic sentiment classification (positive, negative and neutral) and target deeper emotion classification of Twitter data. We have focused on emotion identification into Ekman's six basic emotions i.e. JOY, SURPRISE, ANGER, DISGUST, FEAR and SADNESS. We have employed two diverse machine learning algorithms with three varied datasets and analyzed their outcomes. We show how equal distribution of emotions in training tweets results in better learning accuracies and hence better performance in the classification task. \u00a9 2013 IEEE.","keywords_author":["emotion identification","machine learning","Na\u00efve Bayes","natural language processing","SVM","tweets"],"keywords_other":["Emotion classification","SVM","Classification tasks","Sentiment analysis","Emotion identifications","Sentiment classification","NAtural language processing","tweets"],"max_cite":5.0,"pub_year":2013.0,"sources":"['scp']","rawkeys":["sentiment classification","emotion classification","na\u00efve bayes","machine learning","natural language processing","svm","classification tasks","emotion identification","emotion identifications","sentiment analysis","tweets"],"tags":["sentiment classification","emotion classification","natural language processing","machine learning","classification tasks","emotion identifications","naive bayes","sentiment analysis","tweets"]},{"p_id":50837,"title":"Code Comment Analysis for Improving Software Quality","abstract":"\u00a9 2015 Elsevier Inc. All rights reserved. Code comments contain a rich amount of information that can be leveraged to improve software maintainability and reliability. This chapter on studying and analyzing free-form and semistructured code comments will help practitioners, researchers, and students learn about (1) the characteristics and content of code comments, (2) the techniques, tools, and measures for studying and analyzing code comments automatically or semiautomatically, and (3) future research directions and challenges. Readers will acquire a unique blend of interdisciplinary techniques, including natural language processing, machine learning, and program analysis, which are also useful for analyzing other software engineering text. This chapter can serve as an introduction to comment study and analysis for practitioners, researchers, and students who are interested in conducting research in this area, applying comment analysis techniques in industry, and learning about this area.","keywords_author":["Annotations","Bug detection","Code comment analysis","Machine learning","Natural language processing","Software quality","Specification mining","Text analytics"],"keywords_other":["Bug detection","Comment analysis","Specification mining","Text analytics","Annotations","Software Quality"],"max_cite":0.0,"pub_year":2015.0,"sources":"['scp']","rawkeys":["comment analysis","specification mining","text analytics","bug detection","code comment analysis","machine learning","natural language processing","annotations","software quality"],"tags":["comment analysis","specification mining","text analytics","bug detection","code comment analysis","machine learning","natural language processing","annotation","software quality"]},{"p_id":18071,"title":"A survey on the application of recurrent neural networks to statistical language modeling","abstract":"\u00a9 2014 The Authors. Published by Elsevier Ltd. In this paper, we present a survey on the application of recurrent neural networks to the task of statistical language modeling. Although it has been shown that these models obtain good performance on this task, often superior to other state-of-the-art techniques, they suffer from some important drawbacks, including a very long training time and limitations on the number of context words that can be taken into account in practice. Recent extensions to recurrent neural network models have been developed in an attempt to address these drawbacks. This paper gives an overview of the most important extensions. Each technique is described and its performance on statistical language modeling, as described in the existing literature, is discussed. Our structured overview makes it possible to detect the most promising techniques in the field of recurrent neural networks, applied to language modeling, but it also highlights the techniques for which further research is required.","keywords_author":["Language modeling","Machine translation","Natural language processing","Recurrent neural networks","Speech recognition"],"keywords_other":["NAtural language processing","Training time","Language model","Statistical language modeling","Context-word","Machine translations","Recurrent neural network model","State-of-the-art techniques"],"max_cite":26.0,"pub_year":2015.0,"sources":"['scp']","rawkeys":["recurrent neural network model","natural language processing","machine translation","speech recognition","recurrent neural networks","statistical language modeling","language model","machine translations","language modeling","training time","state-of-the-art techniques","context-word"],"tags":["recurrent neural network model","neural networks","natural language processing","speech recognition","statistical language modeling","language model","machine translations","training time","state-of-the-art techniques","context-word"]},{"p_id":42647,"title":"A deep learning approach for extracting attributes of ABAC policies","abstract":"\u00a9 2018 Association for Computing Machinery. The National Institute of Standards and Technology (NIST) has identified natural language policies as the preferred expression of policy and implicitly called for an automated translation of ABAC natural language access control policy (NLACP) to a machine-readable form. An essential step towards this automation is to automate the extraction of ABAC attributes from NLACPs, which is the focus of this paper. We, therefore, raise the question of: how can we automate the task of attributes extraction from natural language documents? Our proposed solution to this question is built upon the recent advancements in natural language processing and machine learning techniques. For such a solution, the lack of appropriate data often poses a bottleneck. Therefore, we decouple the primary contributions of this work into: (1) developing a practical framework to extract ABAC attributes from natural language artifacts, and (2) generating a set of realistic synthetic natural language access control policies (NLACPs) to evaluate the proposed framework. The experimental results are promising with regard to the potential automation of the task of interest. Using a convolutional neural network (CNN), we achieved - in average - an F1-score of 0.96 when extracting the attributes of subjects, and 0.91 when extracting the objects' attributes from natural language access control policies.","keywords_author":["Access control policy","Attribute based access control","Deep learning","Natural language processing","Policy authoring","Relation extraction"],"keywords_other":["Relation extraction","Attributes extractions","Attribute based access control","Convolutional Neural Networks (CNN)","Access control policies","Natural language policy","National Institute of Standards and Technology","Machine learning techniques"],"max_cite":0.0,"pub_year":2018.0,"sources":"['scp']","rawkeys":["policy authoring","deep learning","national institute of standards and technology","access control policy","machine learning techniques","natural language processing","access control policies","attribute based access control","attributes extractions","natural language policy","relation extraction","convolutional neural networks (cnn)"],"tags":["policy authoring","national institute of standards and technology","machine learning techniques","machine learning","natural language processing","access control policies","attribute based access control","attribute extraction","convolutional neural network","natural language policy","relation extraction"]},{"p_id":1691,"title":"Object recognition base on deep belief network","abstract":"Event ontology is a general knowledge base constructed by event as the basic knowledge unit for computer communication. Event contains six elements which are action, object, time, environment, assertion and language performance. In this paper, we mainly discuss object elements recognition. There are several mainly existing way to recognize object: methods based on rule, statistical and shallow machine learning. Although these methods can get better recognition results in a particular environment, they have nature defects. For instance, it is difficult for them to do feature extraction and they can not achieve complex function approximation, leading to low recognition accuracy and scalability. Aiming at problems of existing object recognition methods, we present a Chinese emergency object recognition model based on deep learning (CEORM). Firstly, we use word segmentation system (LTP) to segment sentence, and classify words according to annotating elements in CEC2.0 corpus, and then obtain each word's vectorization of multiple features, which include part of speech, dependency grammar, length, location. We obtain word's deep semantic characteristics from the collection after vectorization using deep belief network, finally, object elements are classified and recognized. Extensive testing analysis shows that our proposed method can achieve better recognition effect.","keywords_author":["CEORM","DBN","deep learning","event ontology","event recognition","event ontology","deep learning","event recognition","DBN","CEORM"],"keywords_other":["Word segmentation systems","event ontology","action element","object element","Event ontology","Deep learning","environment element","Event recognition","Feature extraction","CEORM model","language performance element","Machine learning","statistical learning","deep belief network","time element","learning (artificial intelligence)","Training","Deep belief networks","CEORM","CEC2.0 corpus","Computer Communications","belief networks","Object recognition","shallow machine learning","Chinese emergency object recognition model","rule learning","Semantics","assertion element","word segmentation system","natural language processing","object recognition","Recognition accuracy","Grammar","LTP","deep semantic characteristics","Object oriented modeling"],"max_cite":0.0,"pub_year":2016.0,"sources":"['scp', 'ieee']","rawkeys":["event recognition","event ontology","recognition accuracy","action element","object element","chinese emergency object recognition model","environment element","dbn","machine learning","cec2.0 corpus","language performance element","statistical learning","deep belief network","time element","learning (artificial intelligence)","ltp","deep learning","semantics","training","computer communications","object oriented modeling","belief networks","word segmentation systems","shallow machine learning","rule learning","grammar","assertion element","natural language processing","word segmentation system","object recognition","ceorm model","ceorm","feature extraction","deep semantic characteristics","deep belief networks"],"tags":["event recognition","event ontology","recognition accuracy","action element","object element","chinese emergency object recognition model","environment element","machine learning","cec2.0 corpus","language performance element","statistical learning","long-term potentiation","time element","semantics","training","computer communications","object oriented modeling","belief networks","word segmentation systems","shallow machine learning","rule learning","grammar","assertion element","natural language processing","object recognition","ceorm model","ceorm","feature extraction","deep semantic characteristics","deep belief networks"]},{"p_id":48796,"title":"Recognition of empathy seeking questions in one of the largest woman CQA in Japan","abstract":"\u00a9 Springer International Publishing AG 2017. Many questions are posted on community websites in the world. Some of these questions are actually asked in order to receive empathy for the feelings of questioners, instead of getting specific answers to the questions asked. However, it is difficult to receive answers for these questions compared with questions that are asked for seeking responses other than for empathy. If such questions that are asked for the purpose of receiving empathy can get responses, it serves as an important factor to increase satisfaction of users. This paper reports on our attempt to improve response rate to the questions by classifying those questions that are asked for seeking empathy and those that are not by using machine learning and showing the questions classified as the ones seeking empathy to the prospective respondents who have been answered to these questions with higher rate.","keywords_author":["Community","CQA","Machine learning","NLP"],"keywords_other":["Response rate","Community"],"max_cite":0.0,"pub_year":2017.0,"sources":"['scp']","rawkeys":["community","response rate","nlp","cqa","machine learning"],"tags":["community","response rate","natural language processing","machine learning","community question answering"]},{"p_id":50845,"title":"Big data and the regulation of financial markets","abstract":"The development of computational data science techniques in natural language processing (NLP) and machine learning (ML) algorithms to analyze large and complex textual information opens new avenues to study intricate processes, such as government regulation of financial markets, at a scale unimaginable even a few years ago. This paper develops scalable NLP and ML algorithms (classification, clustering and ranking methods) that automatically classify laws into various codes\/labels, rank feature sets based on use case, and induce best structured representation of sentences for various types of computational analysis. The results provide standardized coding labels of policies to assist regulators to better understand how key policy features impact financial markets.","keywords_author":["Big data","Financial regulation","Machine learning","Natural language processing","Political economics"],"keywords_other":["Ranking methods","Computational analysis","Ml algorithms","Textual information","Government regulation","Computational data","NAtural language processing","Financial regulations"],"max_cite":0.0,"pub_year":2015.0,"sources":"['scp']","rawkeys":["political economics","computational analysis","computational data","big data","machine learning","financial regulations","natural language processing","ml algorithms","financial regulation","government regulation","ranking methods","textual information"],"tags":["political economics","computational analysis","computational data","big data","machine learning","natural language processing","ml algorithms","financial regulation","government regulation","ranking methods","textual information"]},{"p_id":42659,"title":"A single-model approach for Arabic segmentation, POS tagging, and named entity recognition","abstract":"\u00a9 2018 IEEE. This paper presents an entirely new, one-million-word annotated corpus for a comprehensive, machine-learning-based preprocessing of text in Modern Standard Arabic. Contrary to the conventional pipeline architecture, we solve the NLP tasks of word segmentation, POS tagging and named entity recognition as a single sequence labeling task. This single-component configuration results in a faster operation and is able to provide state-of-the-art precision and recall according to our evaluations. The fine-grained output tag set output by our annotator greatly simplifies downstream tasks such as lemmatization. Provided as a trained OpenNLP component, the annotator is free for research purposes.","keywords_author":["Lemmatization","Machine learning","Named entity recognition","NLP","POS tagging","Segmentation"],"keywords_other":["Named entity recognition","Word segmentation","Single components","Single model approach","Pipeline architecture","Precision and recall","Lemmatization","PoS tagging"],"max_cite":0.0,"pub_year":2018.0,"sources":"['scp']","rawkeys":["word segmentation","nlp","lemmatization","segmentation","single model approach","named entity recognition","machine learning","single components","pos tagging","pipeline architecture","precision and recall"],"tags":["word segmentation","single model approach","lemmatization","segmentation","named entity recognition","natural language processing","machine learning","single components","pos tagging","pipeline architecture","precision and recall"]},{"p_id":1705,"title":"Robust Chinese traffic sign detection and recognition with deep convolutional neural network","abstract":"Detection and recognition of traffic sign, including various road signs and text, play an important role in autonomous driving, mapping\/navigation and traffic safety. In this paper, we proposed a traffic sign detection and recognition system by applying deep convolutional neural network (CNN), which demonstrates high performance with regard to detection rate and recognition accuracy. Compared with other published methods which are usually limited to a predefined set of traffic signs, our proposed system is more comprehensive as our target includes traffic signs, digits, English letters and Chinese characters. The system is based on a multi-task CNN trained to acquire effective features for the localization and classification of different traffic signs and texts. In addition to the public benchmarking datasets, the proposed approach has also been successfully evaluated on a field-captured Chinese traffic sign dataset, with performance confirming its robustness and suitability to real-world applications.","keywords_author":["component","convolutional neural networks","deep learning","multi task CNN","traffic sign detection","traffic sign recognition","component","traffic sign detection","traffic sign recognition","deep learning","convolutional neural networks","multi task CNN"],"keywords_other":["traffic sign classification","traffic safety","Neural networks","image classification","traffic sign localization","Deep learning","multi task CNN","Feature extraction","robust Chinese traffic sign detection system","Machine learning","object detection","Proposals","component","feedforward neural nets","deep convolutional neural network","Object detection","Traffic sign detection","Computer vision","autonomous driving","public benchmarking datasets","Neurons","Traffic sign recognition","Chinese characters","English letters","natural language processing","traffic engineering computing","object recognition","robust Chinese traffic sign recognition system","text analysis","Convolutional neural network"],"max_cite":12.0,"pub_year":2015.0,"sources":"['wos', 'scp', 'ieee']","rawkeys":["traffic sign classification","multi task cnn","convolutional neural network","traffic sign detection","traffic safety","image classification","traffic sign localization","english letters","robust chinese traffic sign recognition system","machine learning","proposals","object detection","component","neurons","feedforward neural nets","chinese characters","convolutional neural networks","deep learning","neural networks","deep convolutional neural network","robust chinese traffic sign detection system","autonomous driving","computer vision","public benchmarking datasets","traffic sign recognition","natural language processing","traffic engineering computing","object recognition","text analysis","feature extraction"],"tags":["traffic sign classification","multi task cnn","convolutional neural network","traffic sign detection","traffic safety","image classification","traffic sign localization","english letters","robust chinese traffic sign recognition system","machine learning","proposals","object detection","component","neurons","feedforward neural nets","public benchmark datasets","chinese characters","neural networks","robust chinese traffic sign detection system","autonomous driving","computer vision","traffic sign recognition","natural language processing","traffic engineering computing","object recognition","text analysis","feature extraction"]},{"p_id":42666,"title":"A comparison of language-dependent and language-independent models for violence prediction","abstract":"\u00a9 2018 IEEE. The nature of violence changes with developments in politics, religion, and technology. This poses challenges to governmental and non-governmental organizations responsible for bringing forth timely strategies for engaging with groups advocating violence. While some groups are well-known for their violence, other groups' characteristics vary throughout time and across regions, which hampers traditional decision-making processes, taking time and resources that organizations do not always have. As such, a scalable and effective method for identifying violent groups becomes imperative. This paper applies text analysis techniques to differentiate violent and non-violent groups using English text from various value-based groups. The models presented in this paper achieved accuracies of at least 71% and as high as 83%. The results demonstrate that text analysis provides a powerful predictive solution for winnowing the violent groups from the non-violent ones. In addition, by incorporating natural language processing tools, language-dependent models show a slight 2% improvement in accuracy over language-independent models. The overall similar performance of language-dependent and language-independent models suggests the two approaches are comparable alternatives to each other.","keywords_author":["Machine learning","Natural language processing","Religious conflict","Text mining"],"keywords_other":["Language independents","Religious conflict","Text mining","Text analysis","Value-based","Nongovernmental organizations","Predictive solutions","Decision making process"],"max_cite":0.0,"pub_year":2018.0,"sources":"['scp']","rawkeys":["text mining","value-based","predictive solutions","natural language processing","machine learning","text analysis","religious conflict","nongovernmental organizations","language independents","decision making process"],"tags":["text mining","value-based","predictive solutions","natural language processing","machine learning","text analysis","religious conflict","nongovernmental organizations","language independents","decision making process"]},{"p_id":42667,"title":"Multi-classifier system for authorship verification task using word embeddings","abstract":"\u00a9 2018 IEEE. Authorship Verification is considered as a topic of growing interest in research, which has shown excellent development in recent years. We want to know if an unknown document belongs to the documents set known to an author or not. Classical text classifiers often focus on many human designed features, such as dictionaries, knowledge bases and special tree kernels. Other studies use the N-gram function that often leads to the curse of dimensionality. Contrary to traditional approaches, this article proposes a new scheme of Machine Learning model based on fusion of three different architectures namely, Convolutional Neural Networks, Recurrent-Convolutional Neural Networks and Support Vector Machine classifiers without human-designed features. Word2vec based Word Embeddings is proposed to learn the best word representations for automatic authorship verification. Word Embeddings provides semantic vectors and extracts the most relevant information about raw text with a relatively small dimension. As well as the classifiers generally make different errors on the same learning samples which results in a combination of several points of view to maintain relevant information contained in different classifiers. The final decision of our system is obtained by combining the results of the three models using the voting method.","keywords_author":["Authorship Verification","Convolutional Neural Networks (CNN)","Deep Learning","Natural Language Processing (NLP)","Recurrent-Convolutional Neural Networks R-CNN","Word Embeddings"],"keywords_other":["Authorship verification","Convolutional Neural Networks (CNN)","Machine learning models","Curse of dimensionality","Support vector machine classifiers","Convolutional neural network","Embeddings","Different architectures"],"max_cite":0.0,"pub_year":2018.0,"sources":"['scp']","rawkeys":["embeddings","support vector machine classifiers","deep learning","machine learning models","natural language processing (nlp)","curse of dimensionality","word embeddings","recurrent-convolutional neural networks r-cnn","convolutional neural network","different architectures","authorship verification","convolutional neural networks (cnn)"],"tags":["embeddings","support vector machine classifiers","machine learning models","machine learning","natural language processing","curse of dimensionality","word embedding","recurrent-convolutional neural networks r-cnn","convolutional neural network","different architectures","authorship verification"]},{"p_id":32428,"title":"Automatically assessing resource quality for educational digital libraries","abstract":"With the rise of community-generated web content, the need for automatic assessment of resource quality has grown. We demonstrate how developing a concrete characterization of quality for web-based resources can make machine learning approaches to automating quality assessment in the realm of educational digital libraries tractable. Using data from several previous studies of quality, we gathered a set of key dimensions and indicators of quality that were commonly identified by educators. We then performed a mixed-method study of digital library quality experts, showing that our characterization of quality captured the subjective processes used by the experts when assessing resource quality. Using key indicators of quality selected from a statistical analysis of our expert study data, we developed a set of annotation guidelines and annotated a corpus of 1000 digital resources for the presence or absence of the key quality indicators. Agreement among annotators was high, and initial machine learning models trained from this corpus were able to identify some indicators of quality with as much as an 18% improvement over the baseline. Copyright 2009 ACM.","keywords_author":["Digital library","Digital resource","Machine learning","Natural language processing","Quality"],"keywords_other":["Key indicator","Automatic assessment","Key quality indicators","Digital resources","Web-based resources","Learning approach","Web content","Key dimensions","Educational digital libraries","Quality assessment","NAtural language processing"],"max_cite":3.0,"pub_year":2009.0,"sources":"['scp']","rawkeys":["key indicator","web content","quality","digital library","key dimensions","key quality indicators","machine learning","digital resources","natural language processing","quality assessment","web-based resources","digital resource","learning approach","automatic assessment","educational digital libraries"],"tags":["key indicator","web content","quality","key dimensions","key quality indicators","machine learning","digital resources","natural language processing","information retrieval","web-based resources","digital libraries","learning approach","automatic assessment","educational digital libraries"]},{"p_id":32431,"title":"Is stock BBS content correlated with the stock market? - A Japanese case","abstract":"We analyze the relations between the stock market and a stock bulletin board system (BBS) in Japan. Previous studies in the USA found that the characteristics of messages posted on stock BBSs can predict market volatility and trading volume. We develop hypotheses based on the results of those analyses and apply statistical analysis to the data about companies mentioned in a large number of messages posted on the Yahoo! stock message board in Japan in 2005-2006. We analyze the contents of these messages using natural language processing. We find a significant correlation between the number of postings and market volatility and trading volume, and also find significant correlation between the amount of bullish and bearish opinion and the stock return. \u00a92009 IEEE.","keywords_author":["Machine learning","Natural language processing","Stock BBS","Stock market","Support vector machine"],"keywords_other":["Stock BBS","Machine-learning","Stock market","Message boards","Stock returns","Bulletin board systems","Trading volumes","Statistical analysis","Market volatility","NAtural language processing"],"max_cite":3.0,"pub_year":2009.0,"sources":"['scp']","rawkeys":["stock market","statistical analysis","bulletin board systems","market volatility","machine learning","natural language processing","stock bbs","trading volumes","machine-learning","stock returns","support vector machine","message boards"],"tags":["stock market","statistical analysis","bulletin board systems","market volatility","natural language processing","machine learning","stock bbs","trading volumes","stock returns","message boards"]},{"p_id":14000,"title":"How social media will change public health","abstract":"Recent work in machine learning and natural language processing has studied the health content of tweets and demonstrated the potential for extracting useful public health information from their aggrega-tion. This article examines the types of health topics discussed on Twitter, and how tweets can both augment existing public health capabilities and enable new ones. The author also discusses key chal-lenges that researchers must address to deliver high-quality tools to the public health community. \u00a9 2012 IEEE.","keywords_author":["health","machine learning","natural language processing","social media","twitter"],"keywords_other":["High quality","Social media","Health informations","NAtural language processing","twitter"],"max_cite":66.0,"pub_year":2012.0,"sources":"['scp']","rawkeys":["health informations","social media","machine learning","natural language processing","high quality","health","twitter"],"tags":["health informations","social media","machine learning","natural language processing","high quality","health","twitter"]},{"p_id":32433,"title":"Towards the measurement of Arabic weblogs credibility automatically","abstract":"Due to the large amount of information available on the web that is not necessarily true or believable, credibility of web information is becoming an increasingly important area to understand. Recently, most research has been available to develop automatic measures for web information credibility in languages such as Japanese, Germen and English websites. Unfortunately, there is no research for the credibility of Arabic web content. In this paper we will propose a system to measure information credibility of Arabic web content automatically. The focus will be on Weblogs; since they are considered a significant part of the rapid growth of Arabic content on the web. \u00a9 2010 ACM.","keywords_author":["Arabic language","credibility","machine learning","natural language processing","weblogs"],"keywords_other":["Rapid growth","Machine-learning","Amount of information","Weblogs","Web content","Arabic languages","Web information","NAtural language processing"],"max_cite":3.0,"pub_year":2009.0,"sources":"['scp']","rawkeys":["web content","arabic language","amount of information","rapid growth","machine learning","natural language processing","weblogs","web information","machine-learning","credibility","arabic languages"],"tags":["web content","amount of information","rapid growth","machine learning","natural language processing","weblogs","web information","credibility","arabic languages"]},{"p_id":24252,"title":"Improving opinion retrieval in social media by combining features-based coreferencing and memory-based learning","abstract":"\u00a9 2014 Elsevier Inc. Social networks messaging typically contains a lot of implicit linguistic information partially due to restrictions on a message's length (i.e., few named entities, short sentences, no discourse structure, etc.). This may significantly impact several applications including opinion mining, sentiment analysis, etc., as data collection tasks such as opinion retrieval tasks will fail to obtain all the relevant messages whenever the target topic, objects, or features are not explicit within the texts. In order to address these issues, in this paper a novel adaptive approach for opinion retrieval is proposed. It combines natural-language co-referencing techniques, features-based linguistic preprocessing and memory-based learning to resolving implicit co-referencing within informal opinion texts by using underlying hierarchies of thread messages. Experiments were conducted to assess the ability of the model to improve opinion retrieval by resolving implicit entities and features, showing the promise of our opinion retrieval approach when compared to state-of-the-art methods using text data from social networks.","keywords_author":["Linguistic coreferencing","Memory-based learning","Natural language processing","Opinion mining","Opinion retrieval","Text mining"],"keywords_other":["Text mining","Opinion retrievals","Memory-based learning","NAtural language processing","Opinion mining"],"max_cite":9.0,"pub_year":2015.0,"sources":"['scp']","rawkeys":["opinion retrievals","text mining","opinion mining","natural language processing","opinion retrieval","memory-based learning","linguistic coreferencing"],"tags":["text mining","opinion mining","natural language processing","opinion retrieval","memory-based learning","linguistic coreferencing"]},{"p_id":50877,"title":"Extracting biomedical events from pairs of text entities","abstract":"\u00a9 2015 Liu et al.; licensee BioMed Central Ltd. Background: Huge amounts of electronic biomedical documents, such as molecular biology reports or genomic papers are generated daily. Nowadays, these documents are mainly available in the form of unstructured free texts, which require heavy processing for their registration into organized databases. This organization is instrumental for information retrieval, enabling to answer the advanced queries of researchers and practitioners in biology, medicine, and related fields. Hence, the massive data flow calls for efficient automatic methods of text-mining that extract high-level information, such as biomedical events, from biomedical text. The usual computational tools of Natural Language Processing cannot be readily applied to extract these biomedical events, due to the peculiarities of the domain. Indeed, biomedical documents contain highly domain-specific jargon and syntax. These documents also describe distinctive dependencies, making text-mining in molecular biology a specific discipline. Results: We address biomedical event extraction as the classification of pairs of text entities into the classes corresponding to event types. The candidate pairs of text entities are recursively provided to a multiclass classifier relying on Support Vector Machines. This recursive process extracts events involving other events as arguments. Compared to joint models based on Markov Random Fields, our model simplifies inference and hence requires shorter training and prediction times along with lower memory capacity. Compared to usual pipeline approaches, our model passes over a complex intermediate problem, while making a more extensive usage of sophisticated joint features between text entities. Our method focuses on the core event extraction of the Genia task of BioNLP challenges yielding the best result reported so far on the 2013 edition.","keywords_author":["Information Extraction","Machine Learning","Natural Language Processing"],"keywords_other":["Computational Biology","Humans","Computational tools","Natural Language Processing","Biomedical Research","Databases, Factual","Recursive process","High-level information","Automatic method","Biomedical documents","Data Mining","Markov Random Fields","Knowledge Bases","Models, Theoretical","Multi-class classifier","Support Vector Machine","NAtural language processing"],"max_cite":0.0,"pub_year":2015.0,"sources":"['scp']","rawkeys":["markov random fields","databases","support vector machine","information extraction","machine learning","multi-class classifier","models","biomedical documents","knowledge bases","biomedical research","data mining","humans","factual","automatic method","computational biology","natural language processing","high-level information","recursive process","theoretical","computational tools"],"tags":["markov random fields","databases","knowledge base","information extraction","machine learning","multi-class classifier","biomedical documents","biomedical research","data mining","humans","factual","automatic method","model","computational biology","natural language processing","high-level information","recursive process","theoretical","computational tools"]},{"p_id":46783,"title":"Text based user comments as a signal for automatic language identification of online videos","abstract":"\u00a9 2017 ACM.Identifying the audio language of online videos is crucial for industrial multi-media applications. Automatic speech recognition systems can potentially detect the language of the audio. However, such systems are not available for all languages. Moreover, background noise, music and multi-party conversations make audio language identification hard. Instead, we utilize text based user comments as a new signal to identify audio language of YouTube videos. First, we detect the language of the text based comments. Augmenting this information with video meta-data features, we predict the language of the videos with an accuracy of 97% on a set of publicly available videos. The subject matter discussed in this research is patent pending.","keywords_author":["Automatic language identification","Machine learning","Multi-media content","Natural language processing","Signal processing","YouTube"],"keywords_other":["Automatic language identification","Subject matters","YouTube","Language identification","Multi-party conversations","Multimedia contents","Background noise","Automatic speech recognition system"],"max_cite":0.0,"pub_year":2017.0,"sources":"['scp']","rawkeys":["multi-party conversations","background noise","subject matters","machine learning","natural language processing","language identification","signal processing","automatic language identification","multimedia contents","automatic speech recognition system","multi-media content","youtube"],"tags":["multi-party conversations","background noise","machine learning","natural language processing","subject matter","language identification","signal processing","automatic language identification","multimedia contents","automatic speech recognition system","youtube"]},{"p_id":16075,"title":"Development of phenotype algorithms using electronic medical records and incorporating natural language processing","abstract":"\u00a9 BMJ Publishing Group Ltd 2015.Electronic medical records are emerging as a major source of data for clinical and translational research studies, although phenotypes of interest need to be accurately defined first. This article provides an overview of how to develop a phenotype algorithm from electronic medical records, incorporating modern informatics and biostatistics methods.","keywords_author":null,"keywords_other":["Algorithms","Electronic Health Records","Phenotype","Humans","Natural Language Processing","Translational Medical Research","Information Storage and Retrieval","Interdisciplinary Communication"],"max_cite":40.0,"pub_year":2015.0,"sources":"['scp', 'wos']","rawkeys":["natural language processing","electronic health records","humans","algorithms","phenotype","information storage and retrieval","translational medical research","interdisciplinary communication"],"tags":["natural language processing","electronic health records","humans","algorithms","phenotype","information storage and retrieval","translational medical research","interdisciplinary communication"]},{"p_id":24267,"title":"A comparative study of feature selection and machine learning algorithms for arabic sentiment classification","abstract":"\u00a9 Springer International Publishing Switzerland 2014. Sentiment analysis is a very challenging and important task that involves natural language processing, web mining, and machine learning. Sentiment analysis in the Arabic language is a more challenging task than in other languages due to the morphological complexity of the Arabic and the large variation of its dialects. This paper presents an empirical comparison of seven feature selection methods (Information Gain, Principal Components Analysis, Relief-F, Gini Index, Uncertainty, Chi-squared, and Support Vector Machines (SVMs)), and three machine learning classifiers (SVM, Naive Bayes, and K-nearest neighbor) for Arabic sentiment classification. A wide range of comparative experiments are conducted on an opinion corpus for Arabic (OCA). This paper demonstrates that feature selection does improve the performance of Arabic sentiment-based classification, but the result depends on the method used and the number of features selected. The experimental results demonstrate that feature reduction methods are found to improve the classifier performance. Moreover, the experimental results indicate that SVM-based feature selection yields the best performance for feature selection and that the SVM classifier outperforms the other techniques for Arabic sentimentbased classification. Finally, the experiments indicate that the SVM classifier with the SVM-based feature selection method yields the best classification method, with an accuracy of 92.4%.","keywords_author":["Arabic sentiment analysis","Feature selection","Machine learning","Opinion mining"],"keywords_other":["Feature selection methods","Sentiment-based classification","Sentiment analysis","Principal components analysis","Morphological complexity","Support vector machine (SVMs)","NAtural language processing","Sentiment classification","Opinion mining"],"max_cite":9.0,"pub_year":2014.0,"sources":"['scp']","rawkeys":["sentiment-based classification","sentiment classification","arabic sentiment analysis","feature selection methods","machine learning","natural language processing","feature selection","morphological complexity","principal components analysis","support vector machine (svms)","opinion mining","sentiment analysis"],"tags":["sentiment-based classification","principal component analysis","sentiment classification","arabic sentiment analysis","feature selection methods","machine learning","natural language processing","feature selection","morphological complexity","opinion mining","sentiment analysis"]},{"p_id":34512,"title":"A survey on automating configuration and parameterization in evolutionary design exploration","abstract":"\u00a9 Cambridge University Press 2015. Configuration and parameterization of optimization frameworks for the computational support of design exploration can become an exclusive barrier for the adoption of such systems by engineers. This work addresses the problem of defining the elements that constitute a multiple-objective design optimization problem, that is, design variables, constants, objective functions, and constraint functions. In light of this, contributions are reviewed from the field of evolutionary design optimization with respect to their concrete implementation for design exploration. Machine learning and natural language processing are supposed to facilitate feasible approaches to the support of configuration and parameterization. Hence, the authors further review promising machine learning and natural language processing methods for automatic knowledge elicitation and formalization with respect to their implementation for evolutionary design optimization. These methods come from the fields of product attribute extraction, clustering of design solutions, relationship discovery, computation of objective functions, metamodeling, and design pattern extraction.","keywords_author":["Conceptual Design","Design Automation","Design Optimization","Evolutionary Computing","Knowledge Engineering","Machine Learning"],"keywords_other":["Design optimization","Design automations","Constraint functions","Optimization framework","Evolutionary design optimization","Design optimization problem","NAtural language processing","Evolutionary computing"],"max_cite":2.0,"pub_year":2015.0,"sources":"['scp']","rawkeys":["constraint functions","evolutionary computing","design automations","machine learning","natural language processing","optimization framework","design optimization","conceptual design","knowledge engineering","design optimization problem","design automation","evolutionary design optimization"],"tags":["constraint functions","machine learning","natural language processing","optimization framework","design optimization","conceptual design","knowledge engineering","design optimization problem","design automation","evolutionary design optimization","evolutionary computation"]},{"p_id":42706,"title":"Neural named entity recognition using a self-attention mechanism","abstract":"\u00a9 2017 IEEE. We propose a novel supervised approach for text tagging and multi-label text classification based on a multi-head encoder-decoder neural network architecture. Our method predicts which subset of possible tags best matches an input text. It efficiently spends computational resources, exploiting dependencies between tags by encoding an input text into a compact representation which is then passed to multiple decoder classifier heads. We test our architecture on a Twitter hashtag prediction task, comparing it to a baseline model with multiple feedforward networks and a baseline model with multiple recurrent neural networks with GRU cells. We show that our approach achieves a significantly better performance than baselines with an equivalent number of parameters.","keywords_author":["Attention Mechanism","Deep Learning","Named Entity Recognition","Natural Language Processing","NER","Neural Networks"],"keywords_other":["Computational resources","Prediction tasks","Feed-forward network","Multiple recurrent neural networks","Named entity recognition","Multi-label text classification","Compact representation","Attention mechanisms"],"max_cite":0.0,"pub_year":2018.0,"sources":"['scp']","rawkeys":["multiple recurrent neural networks","ner","deep learning","neural networks","prediction tasks","attention mechanisms","feed-forward network","named entity recognition","natural language processing","compact representation","computational resources","attention mechanism","multi-label text classification"],"tags":["multiple recurrent neural networks","neural networks","prediction tasks","attention mechanisms","feed-forward network","machine learning","named entity recognition","natural language processing","compact representation","computational resources","multi-label text classification"]},{"p_id":28371,"title":"When Does Disengagement Correlate with Performance in Spoken Dialog Computer Tutoring?","abstract":"In this paper we investigate how student disengagement relates to two performance metrics in a spoken dialog computer tutoring corpus, both when disengagement is measured through manual annotation by a trained human judge, and also when disengagement is measured through automatic annotation by the system based on a machine learning model. First, we investigate whether manually labeled overall disengagement and six different disengagement types are predictive of learning and user satisfaction in the corpus. Our results show that although students' percentage of overall disengaged turns negatively correlates both with the amount they learn and their user satisfaction, the individual types of disengagement correlate differently: some negatively correlate with learning and user satisfaction, while others don't correlate with either metric at all. Moreover, these relationships change somewhat depending on student prerequisite knowledge level. Furthermore, using multiple disengagement types to predict learning improves predictive power. Overall, these manual label-based results suggest that although adapting to disengagement should improve both student learning and user satisfaction in computer tutoring, maximizing performance requires the system to detect and respond differently based on disengagement type. Next, we present an approach to automatically detecting and responding to user disengagement types based on their differing correlations with correctness. Investigation of our machine learning model of user disengagement shows that its automatic labels negatively correlate with both performance metrics in the same way as the manual labels. The similarity of the correlations across the manual and automatic labels suggests that the automatic labels are a reasonable substitute for the manual labels. Moreover, the significant negative correlations themselves suggest that redesigning ITSPOKE to automatically detect and respond to disengagement has the potential to remediate disengagement and thereby improve performance, even in the presence of noise introduced by the automatic detection process. \u00a9 2013 IOS Press and the authors. All rights reserved.","keywords_author":["correlations","learning","machine learning","manual and automatic annotation","natural language processing","spoken dialog computer tutors","Types of disengagement"],"keywords_other":["spoken dialog computer tutors","correlations","learning","Automatic annotation","Types of disengagement","NAtural language processing"],"max_cite":5.0,"pub_year":2013.0,"sources":"['scp']","rawkeys":["types of disengagement","automatic annotation","correlations","machine learning","learning","natural language processing","spoken dialog computer tutors","manual and automatic annotation"],"tags":["types of disengagement","automatic annotation","natural language processing","machine learning","spoken dialog computer tutors","manual and automatic annotation","correlation"]},{"p_id":50901,"title":"Clinical text analysis using interactive natural language processing","abstract":"\u00a9 Copyright 2015 by the Association for Computing Machinery, Inc. (ACM).Natural Language Processing (NLP) systems are typically developed by informaticists skilled in machine learning techniques that are unfamiliar to end-users. Although NLP has been widely used in extracting information from clinical text, current systems generally do not provide any provisions for incorporating feedback and revising models based on input from domain experts. The goal of this research is to close this gap by building highly-usable tools suitable for the analysis of free text reports.","keywords_author":["Electronic medical records","Interactive machine learning","Visualization"],"keywords_other":["Domain experts","Text analysis","Extracting information","Interactive machine learning","Revising model","Electronic medical record","NAtural language processing","Machine learning techniques"],"max_cite":0.0,"pub_year":2015.0,"sources":"['scp']","rawkeys":["interactive machine learning","domain experts","machine learning techniques","natural language processing","revising model","extracting information","electronic medical records","text analysis","electronic medical record","visualization"],"tags":["interactive machine learning","machine learning techniques","domain experts","natural language processing","revising model","extracting information","text analysis","electronic medical record","visualization"]},{"p_id":42709,"title":"An attention mechanism for neural answer selection using a combined global and local view","abstract":"\u00a9 2017 IEEE. We propose a new attention mechanism for neural based question answering, which depends on varying granularities of the input. Previous work focused on augmenting recurrent neural networks for question answering systems with simple attention mechanisms which are a function of the similarity between a question embedding and an answer embeddings across time. We extend this by making the attention mechanism dependent on a global embedding of the answer attained using a separate network. We evaluate our system on InsuranceQA, a large question answering dataset. Our model outperforms current state-of-The-Art results on InsuranceQA. Further, we examine which sections of text our attention mechanism focuses on, and explore its performance across different parameter settings.","keywords_author":["Answer Selection","Attention Mechanism","Deep Learning","Natural Language-Processing","Neural Networks","Question Answering","Recurrent Neural-Networks"],"keywords_other":["Question-embedding","Question Answering","Across time","Question answering systems","State of the art","Answer Selection","Parameter setting","Attention mechanisms"],"max_cite":0.0,"pub_year":2018.0,"sources":"['scp']","rawkeys":["question answering","recurrent neural-networks","deep learning","neural networks","answer selection","attention mechanisms","across time","parameter setting","question answering systems","state of the art","attention mechanism","question-embedding","natural language-processing"],"tags":["neural networks","answer selection","attention mechanisms","natural language processing","across time","machine learning","parameter setting","information retrieval","question answering systems","state of the art","question-embedding"]},{"p_id":44758,"title":"Automatic generation and recommendation for API mashups","abstract":"\u00a9 2017 IEEE. Until today, finding the most suitable APIs to use in an application was burdensome, requiring manual and time-consuming searches across a diverse set of websites, in particular regarding how multiple APIs could be combined and worked together (i.e. API mashups). In this paper, we propose a new method to automatically generate API mashups through real-world data collection, text mining and natural language processing (NLP ) techniques. The generated API mashups are further ranked and recommended to developers based on a quantitative indicator of whether the given API mashup is plausible. To evaluate the overall accuracy of the proposed method, we use the generated API mashups to train several machine learning and deep learning models, and then use an independent mashup dataset collected from Github projects for testing. The experimental results show that our proposed method is feasible and accurate for automatic API mashup generation and recommendation.","keywords_author":["AP Imashups","deep learning","machine learning","NLP"],"keywords_other":["Text mining","Real-world","AP Imashups","Learning models","Quantitative indicators","Data collection","Overall accuracies","Automatic Generation"],"max_cite":0.0,"pub_year":2018.0,"sources":"['scp']","rawkeys":["learning models","text mining","nlp","overall accuracies","deep learning","machine learning","quantitative indicators","automatic generation","data collection","real-world","ap imashups"],"tags":["learning models","text mining","machine learning","natural language processing","quantitative indicators","data collection","automatically generated","optimization algorithms","real-world","ap imashups"]},{"p_id":48854,"title":"A deep learning approach to deal with data uncertainty in sentiment analysis","abstract":"\u00a9 Springer International Publishing AG 2017. Sentiment Analysis refers to the process of computationally identifying and categorizing opinions expressed in a piece of text, in order to determine whether the writer\u2019s attitude towards a particular topic or product is positive, negative, or even neutral. Recently, deep learning approaches emerge as powerful computational models that discover intricate semantic representations of texts automatically from data without hand-made feature engineering. These approaches have improved the state-of-the-art in many Sentiment Analysis tasks including sentiment classification of sentences or documents. In this paper we propose a semi-supervised neural network model, based on Deep Belief Networks, able to deal with data uncertainty for text sentences and adopting the Italian language as a reference language.We test this model against some datasets from literature related to movie reviews, adopting a vectorized representation of text and exploiting methods from Natural Language Processing (NLP) pre-processing.","keywords_author":["Deep belief networks","Deep learning","Sentiment analysis"],"keywords_other":["Semantic representation","Computational model","Sentiment analysis","Deep belief networks","Feature engineerings","Learning approach","Sentiment classification","NAtural language processing"],"max_cite":0.0,"pub_year":2017.0,"sources":"['scp']","rawkeys":["sentiment classification","deep learning","natural language processing","semantic representation","computational model","feature engineerings","learning approach","deep belief networks","sentiment analysis"],"tags":["computational modeling","sentiment classification","machine learning","natural language processing","semantic representation","feature engineerings","learning approach","deep belief networks","sentiment analysis"]},{"p_id":46808,"title":"Towards accurate duplicate bug retrieval using deep learning techniques","abstract":"\u00a9 2017 IEEE. Duplicate Bug Detection is the problem of identifying whether a newly reported bug is a duplicate of an existing bug in the system and retrieving the original or similar bugs from the past. This is required to avoid costly rediscovery and redundant work. In typical software projects, the number of duplicate bugs reported may run into the order of thousands, making it expensive in terms of cost and time for manual intervention. This makes the problem of duplicate or similar bug detection an important one in Software Engineering domain. However, an automated solution for the same is not quite accurate yet in practice, in spite of many reported approaches using various machine learning techniques. In this work, we propose a retrieval and classification model using Siamese Convolutional Neural Networks (CNN) and Long Short Term Memory (LSTM) for accurate detection and retrieval of duplicate and similar bugs. We report an accuracy close to 90% and recall rate close to 80%, which makes possible the practical use of such a system. We describe our model in detail along with related discussions from the Deep Learning domain. By presenting the detailed experimental results, we illustrate the effectiveness of the model in practical systems, including for repositories for which supervised training data is not available.","keywords_author":["Convolutional neural networks","Deep learning","Duplicate bug detection","Information retrieval","Long short term memory","Natural language processing","Siamese networks","Word embeddings"],"keywords_other":["Bug detection","Software engineering domain","Classification models","Supervised trainings","Learning techniques","Convolutional neural network","Embeddings","Machine learning techniques"],"max_cite":0.0,"pub_year":2017.0,"sources":"['scp']","rawkeys":["embeddings","duplicate bug detection","convolutional neural networks","bug detection","deep learning","machine learning techniques","natural language processing","information retrieval","word embeddings","supervised trainings","learning techniques","software engineering domain","convolutional neural network","siamese networks","classification models","long short term memory"],"tags":["embeddings","siamese network","duplicate bug detection","bug detection","long short-term memory","machine learning techniques","machine learning","natural language processing","information retrieval","word embedding","supervised trainings","learning techniques","software engineering domain","convolutional neural network","classification models"]},{"p_id":34524,"title":"Extracting local event information from micro-blogs for trip planning","abstract":"\u00a9 2015 IPSJ.This paper describes a method to extract local event information from the micro-blog service Twitter. Twitter holds innumerable user-posted short messages called tweets that cover various topics including local events. Our proposal is composed of three steps: 1) extract tweets related to local events from local tweets by the Support Vector Machine (SVM) approach, 2) identify and extract the venues, names and times of local events mentioned in the tweets by applying Conditional Random Fields (CRF), 3) use the venues and similarity of names to aggregate duplicate local event information. We implement the proposed method and confirm that it extracts local event information with higher precision than the conventional methods.","keywords_author":["local event","local information service","machine learning","natural language processing","Twitter"],"keywords_other":["Micro-blog","Twitter","Conventional methods","Short message","Trip planning","local event","Conditional random field","NAtural language processing"],"max_cite":2.0,"pub_year":2015.0,"sources":"['scp']","rawkeys":["local information service","conditional random field","machine learning","natural language processing","conventional methods","short message","micro-blog","twitter","local event","trip planning"],"tags":["local information service","conditional random field","machine learning","natural language processing","conventional methods","short message","twitter","microblogging","local event","trip planning"]},{"p_id":42716,"title":"SDF-NN: A deep neural network with semantic dropping and fusion for natural language inference","abstract":"\u00a9 2017 IEEE. Natural language inference (NLI) is an important task in natural language processing (NLP), and recently, several deep neural network based models have been proposed for NLI. In this work, we first make two important observations regarding NLI: (1) the existence of extra\/interfering semantics and its negative impact on the correctness of final inference; and (2) the unbalanced importance of local inference results and the need to combine all local results for aggregation. Motivated by these two observations, we have designed SDF-NN, a new NLI model with two novel components: (1) a Semantic Dropping Network (SDN) to automatically discard some of the interfering semantics; and (2) a Semantic Fusion Alignment (SFA) method to effectively fuse all local inference results. Our model has achieved 88.2% accuracy on the SNLI corpus, which is currently the best performing single model.","keywords_author":["Deep learning","Natural language inference","Natural language processing","Neural network","SNLI"],"keywords_other":["Novel component","SNLI","Single models","Natural languages","Semantic fusion"],"max_cite":0.0,"pub_year":2018.0,"sources":"['scp']","rawkeys":["neural network","deep learning","single models","natural languages","natural language processing","novel component","semantic fusion","snli","natural language inference"],"tags":["neural networks","single models","natural languages","natural language processing","machine learning","novel component","semantic fusion","snli","natural language inference"]},{"p_id":44764,"title":"A comparative study on opinion mining algorithms of social media statuses","abstract":"\u00a9 2017 IEEE. The Social Media (SM) is affecting clients' preferences by modeling their thoughts, attitudes, opinions, views and public mood. Observing the SM activities is a decent approach to measure clients' loyalty, keeping a track on their opinion towards products preferences or social event. Opinion Mining (OM) is the most rising research field of text mining using Machine Learning (ML) algorithms and Natural Language Processing (NLP). Several algorithms such as Support Vector Machines (SVM), Na\u00efve Bayes (NB) and Maximum Entropy (ME), were utilized to extract information that differentiates the user's opinion whether it's positive, negative or neutral. User's opinions and reviews are very beneficial information for individuals, businesses, and governments. In this paper, we compare the intelligent algorithms, which are utilized for OM in SM data over the last five years. The results show that using SVM with Part Of Speech (POS) or POS, Unigram and Bigram with J48 accomplish Sentiment Classification (SC) accuracy 92%.","keywords_author":["Facebook","Machine Learning","Natural Language Processing","Opinion Mining","Sentiment Classification","Social Media Mining","Twitter"],"keywords_other":["Twitter","Mining algorithms","Social media minings","Extract informations","Comparative studies","Facebook","Intelligent Algorithms","Sentiment classification"],"max_cite":0.0,"pub_year":2018.0,"sources":"['scp']","rawkeys":["facebook","sentiment classification","intelligent algorithms","social media mining","extract informations","comparative studies","machine learning","natural language processing","twitter","mining algorithms","opinion mining","social media minings"],"tags":["facebook","sentiment classification","intelligent algorithms","social media mining","comparative studies","machine learning","natural language processing","extracting information","mining algorithms","opinion mining","twitter"]},{"p_id":46817,"title":"Predictive modeling for classification of positive valence system symptom severity from initial psychiatric evaluation records","abstract":"\u00a9 2017 Elsevier Inc. In response to the challenges set forth by the CEGS N-GRID 2016 Shared Task in Clinical Natural Language Processing, we describe a framework to automatically classify initial psychiatric evaluation records to one of four positive valence system severities: absent, mild, moderate, or severe. We used a dataset provided by the event organizers to develop a framework comprised of natural language processing (NLP) modules and 3 predictive models (two decision tree models and one Bayesian network model) used in the competition. We also developed two additional predictive models for comparison purpose. To evaluate our framework, we employed a blind test dataset provided by the 2016 CEGS N-GRID. The predictive scores, measured by the macro averaged-inverse normalized mean absolute error score, from the two decision trees and Na\u00efve Bayes models were 82.56%, 82.18%, and 80.56%, respectively. The proposed framework in this paper can potentially be applied to other predictive tasks for processing initial psychiatric evaluation records, such as predicting 30-day psychiatric readmissions.","keywords_author":["Computer-assisted diagnosis","Natural language processing","Psychiatry","Research Domain Criteria (RDoC)","Supervised machine learning"],"keywords_other":["Bayesian network models","Computer assisted diagnosis","Mean absolute error","Decision tree models","Research domains","Psychiatric evaluation","Psychiatry","Supervised machine learning"],"max_cite":0.0,"pub_year":2017.0,"sources":"['scp']","rawkeys":["computer-assisted diagnosis","bayesian network models","natural language processing","psychiatric evaluation","computer assisted diagnosis","research domains","psychiatry","mean absolute error","decision tree models","supervised machine learning","research domain criteria (rdoc)"],"tags":["natural language processing","bayesian network models","psychiatric evaluation","computer assisted diagnosis","research domains","psychiatry","decision tree modeling","mean absolute error","supervised machine learning","research domain criteria (rdoc)"]},{"p_id":38626,"title":"Novel topic diffusion prediction using latent semantic and user behavior","abstract":"Predicting diffusions on big social media data using natural language processing (NLP) and social network analysis (SNA) techniques is an emerging research domain. To predict diffusions of novel topics, previous studies focus on predicting the diffusions on cross-topicobserved diffusions (the diffusions between the source and target user of the diffusion are not observed for the topic to be predicted, but still observed for other topics). However, in real world social network, many diffusions to be predicted are actually unobserved. For example, the diffusions may be unseen (the diffusions between the source and target user of the diffusion are not observed in training data), or even with silence users (one or both of the users of the diffusion never participate a diffusion before). In this paper, we generalize the diffusion prediction on novel topic problem to predict both cross-topic-observed and unobserved diffusions, which is very challenging because of lacking previous diffusion records. We design a learning-based framework to solve the problem. Leveraging NLP and SNA techniques to deal with such Big Data, we exploit the latent semantic derived from diverse information sources (e.g., user, topic, user-topic, and topological), and utilize the idea that \"users with the same attribute value tend to have similar behavior for similar topics\", to extract features for prediction. Our framework is evaluated on realworld microblog data, and the experiments show that we can achieve 73% AUC in this difficult prediction task. Our dataset is also publicly available at http:\/\/mslab.csie.ntu.edu.tw\/~tim\/ase-big-data-2015.zip.","keywords_author":["Big Data","Big data analytics","Big data mining","Data mining","Machine learning","Natural language processing","Social network analysis"],"keywords_other":["Prediction tasks","Information sources","Latent semantics","Social media datum","Topic diffusions","Attribute values","NAtural language processing","Data analytics"],"max_cite":1.0,"pub_year":2015.0,"sources":"['scp']","rawkeys":["attribute values","data mining","big data","topic diffusions","data analytics","latent semantics","prediction tasks","machine learning","natural language processing","social media datum","information sources","social network analysis","big data mining","big data analytics"],"tags":["attribute values","data mining","big data","topic diffusions","data analytics","latent semantics","prediction tasks","machine learning","natural language processing","social media datum","information sources","social network analysis","big data mining","big data analytics"]},{"p_id":32484,"title":"Natural language generation for sponsored-search advertisements","abstract":"In sponsored search, advertisers bid on phrases representative of offered products or services. For large advertisers, these phrases often come from quasi-algorithmically generated lists of thousands of terms prone to poor linguistic construction. A bidded term by itself is usually unsuitable for direct insertion into an ad copy template; it must be rephrased and capitalized properly to fit the template, possibly with additional language to avoid semantic ambiguity. We develop a natural language generation system to automate these steps, preparing a list of terms for insertion into an ad template. For each input term, our system first finds a proper word ordering by mining a corpus of Web search query logs. Next it determines whether the term is ambiguous and - if semantics dictate - attaches a clarifying modifier culled from query logs. Finally, it applies proper capitalization by analyzing pages from Web search engine results. Each step yields a plausible set of displayable forms from which a machine-learned model selects the best. The models are trained and tested on a large set of human-labeled data. The overall system significantly outperforms baseline systems that use simple heuristics. Copyright 2008 ACM.","keywords_author":["Automation","Machine learning","Natural language generation","Natural language processing","Pay-per-click","Query log","Sponsored search"],"keywords_other":["Query logs","Machine-learning","Natural language generation","Pay-per-click","NAtural language processing"],"max_cite":3.0,"pub_year":2008.0,"sources":"['scp']","rawkeys":["query log","natural language processing","machine learning","automation","natural language generation","pay-per-click","query logs","machine-learning","sponsored search"],"tags":["automated","natural language processing","machine learning","natural language generation","pay-per-click","query logs","sponsored search"]},{"p_id":34532,"title":"Domain-specific relation extraction: Using distant supervision machine learning","abstract":"\u00a9 2015 by SCITEPRESS - Science and Technology Publications, Lda.The increasing accessibility and availability of online data provides a valuable knowledge source for information analysis and decision-making processes. In this paper we argue that extracting information from this data is better guided by domain knowledge of the targeted use-case and investigate the integration of a knowledge-driven approach with Machine Learning techniques in order to improve the quality of the Relation Extraction process. Targeting the financial domain, we use Semantic Web Technologies to build the domain Knowledgebase, which is in turn exploited to collect distant supervision training data from semantic linked datasets such as DBPedia and Freebase. We conducted a serious of experiments that utilise the number of Machine Learning algorithms to report on the favourable implementations\/configuration for successful Information Extraction for our targeted domain.","keywords_author":["Information extraction","Knowledge-base","Natural language processing","Relation extraction","Semantic web","Supervised machine learning"],"keywords_other":["Relation extraction","Extracting information","Decision making process","Semantic Web technology","Knowledge base","NAtural language processing","Machine learning techniques","Supervised machine learning"],"max_cite":2.0,"pub_year":2015.0,"sources":"['scp']","rawkeys":["information extraction","semantic web technology","knowledge-base","machine learning techniques","knowledge base","natural language processing","extracting information","semantic web","supervised machine learning","relation extraction","decision making process"],"tags":["information extraction","semantic web technology","machine learning techniques","natural language processing","knowledge base","extracting information","semantic web","supervised machine learning","relation extraction","decision making process"]},{"p_id":18151,"title":"Extracting information from the text of electronic medical records to improve case detection: A systematic review","abstract":"\u00ef\u00bf\u00bd The Author 2016. Published by Oxford University Press on behalf of the American Medical Informatics Association. Background Electronic medical records (EMRs) are revolutionizing health-related research. One key issue for study quality is the accurate identification of patients with the condition of interest. Information in EMRs can be entered as structured codes or unstructured free text. The majority of research studies have used only coded parts of EMRs for case-detection, which may bias findings, miss cases, and reduce study quality. This review examines whether incorporating information from text into case-detection algorithms can improve research quality.Methods A systematic search returned 9659 papers, 67 of which reported on the extraction of information from free text of EMRs with the stated purpose of detecting cases of a named clinical condition. Methods for extracting information from text and the technical accuracy of case-detection algorithms were reviewed.Results Studies mainly used US hospital-based EMRs, and extracted information from text for 41 conditions using keyword searches, rule-based algorithms, and machine learning methods. There was no clear difference in case-detection algorithm accuracy between rule-based and machine learning methods of extraction. Inclusion of information from text resulted in a significant improvement in algorithm sensitivity and area under the receiver operating characteristic in comparison to codes alone (median sensitivity 78% (codes + text) vs 62% (codes), P = .03; median area under the receiver operating characteristic 95% (codes + text) vs 88% (codes), P = .025).Conclusions Text in EMRs is accessible, especially with open source information extraction algorithms, and significantly improves case detection when combined with codes. More harmonization of reporting within EMR studies is needed, particularly standardized reporting of algorithm accuracy metrics like positive predictive value (precision) and sensitivity (recall).","keywords_author":["Case detection","Data quality","Electronic health records","Review","Text mining"],"keywords_other":["Sensitivity and Specificity","Algorithms","Electronic Health Records","Humans","Natural Language Processing","Machine Learning","Information Storage and Retrieval","Data Mining","Diagnosis"],"max_cite":26.0,"pub_year":2016.0,"sources":"['scp', 'wos']","rawkeys":["diagnosis","data mining","text mining","data quality","electronic health records","humans","machine learning","natural language processing","case detection","algorithms","review","information storage and retrieval","sensitivity and specificity"],"tags":["diagnosis","data mining","text mining","data quality","electronic health records","humans","machine learning","natural language processing","case detection","algorithms","review","information storage and retrieval","sensitivity and specificity"]},{"p_id":32487,"title":"\"i know what you feel\": Analyzing the role of conjunctions in automatic sentiment analysis","abstract":"We are interested in finding how people feel about certain topics. This could be considered as a task of classifying the sentiment: sentiment could be positive, negative or neutral. In this paper, we examine the problem of automatic sentiment analysis at sentence level. We observe that sentence structure has a fair contribution towards sentiment determination, and conjunctions play a major role in defining the sentence structure. Our assumption is that in presence of conjunctions, not all phrases have equal contribution towards overall sentiment. We compile a set of conjunction rules to determine relevant phrases for sentiment analysis. Our approach is a representation of the idea to use linguistic resources at phrase level for the analysis at sentence level. We incorporate our approach with support vector machines to conclude that linguistic analysis plays a significant role in sentiment determination. Finally, we verify our results on movie, car and book reviews. \u00a9 2008 Springer-Verlag Berlin Heidelberg.","keywords_author":["Linguistic analysis","Machine learning","Natural language processing","Sentiment analysis","Support vector machines"],"keywords_other":["Linguistic analysis","Sentiment analysis","Machine learning","Linguistic resources","International conferences","Sentence level","Natural language processing","Book reviews"],"max_cite":3.0,"pub_year":2008.0,"sources":"['scp']","rawkeys":["book reviews","machine learning","international conferences","natural language processing","support vector machines","linguistic analysis","linguistic resources","sentiment analysis","sentence level"],"tags":["book reviews","machine learning","international conferences","natural language processing","linguistic analysis","linguistic resources","sentiment analysis","sentence level"]},{"p_id":24297,"title":"Aspect based Sentiment Analysis using support vector machine classifier","abstract":"Sentiment Analysis involves the process of identifying the polarity of opinionated texts. Lots of social networking sites are being used for expressing thoughts and opinions by users to rate products. These user opinionated text is highly unstructured in nature and thus involves the application of various natural language processing techniques. In aspect based sentiment analysis, the various features of a product is identified through the training process. For e.g. the aspects of a camera are picture quality, size, resolution etc. The quantitative analysis of each aspect is done using support vector machine classifier. In most of the previous works, a product review is analysed as a whole rather than considering each aspect of it. Aspect based opinion mining is tedious since the identification of individual features is in itself a challenging task. \u00a9 2013 IEEE.","keywords_author":["Aspect Selection","Machine Learning","Natural Language Processing","Sentiment Analysis"],"keywords_other":["NAtural language processing","Sentiment analysis","Aspect Selection","Social networking sites","Support vector machine classifiers","Identification of individuals","Product reviews","Training process"],"max_cite":9.0,"pub_year":2013.0,"sources":"['scp']","rawkeys":["support vector machine classifiers","training process","identification of individuals","natural language processing","machine learning","social networking sites","sentiment analysis","product reviews","aspect selection"],"tags":["support vector machine classifiers","training process","identification of individuals","natural language processing","machine learning","social networking sites","sentiment analysis","product reviews","aspect selection"]},{"p_id":48872,"title":"Deep text generation \u2013 Using hierarchical decomposition to mitigate the effect of rare data points","abstract":"\u00a9 Springer International Publishing AG 2017. Deep learning has recently been adopted for the task of natural language generation (NLG) and shown remarkable results. However, learning can go awry when the input dataset is too small or not well balanced with regards to the examples it contains for various input sequences. This is relevant to naturally occurring datasets such as many that were not prepared for the task of natural language processing but scraped off the web and originally prepared for a different purpose. As a mitigation to the problem of unbalanced training data, we therefore propose to decompose a large natural language dataset into several subsets that \u201ctalk about\u201d the same thing. We show that the decomposition helps to focus each learner\u2019s attention during training. Results from a proof-of-concept study show 73% times faster learning over a flat model and better results.","keywords_author":["Artificial intelligence","Deep learning","Natural language processing"],"keywords_other":["Training data","Input sequence","Natural language generation","Well balanced","Natural languages","Hierarchical decompositions","Proof of concept","Naturally occurring"],"max_cite":0.0,"pub_year":2017.0,"sources":"['scp']","rawkeys":["artificial intelligence","well balanced","deep learning","natural languages","hierarchical decompositions","natural language processing","natural language generation","input sequence","proof of concept","naturally occurring","training data"],"tags":["well balanced","natural languages","hierarchical decompositions","machine learning","natural language processing","natural language generation","input sequence","proof of concept","naturally occurring","training data"]},{"p_id":1767,"title":"An Associative Generated Model for Multi-signals Based on Deep Learning","abstract":"During exploring the emergence of language, we found that the brain can extract some common features from the same thing in different representations by pattern recognition and association. Consequently, the brain would establish a connection for identical concept from multi-signals. An associative generated model primarily based on Restricted Boltzmann Machine (RBM) and Deep Belief Network (DBN) is set up for multi-signals to simulate the brain's ability. The first step is to use the DBNs for extracting features from multiple input signal sources. The second is using top-level RBM to achieve the goal of associating and generating mutually by fusing each feature. Finally, we verify the feasibility of the model through the realization of generating Arabic digital pictures and Chinese characters digital images reciprocally.","keywords_author":["association","generation","feature extraction","Deep Belief network"],"keywords_other":["restricted Boltzmann machine","Biological neural networks","brain","Yttrium","Computational modeling","Feature extraction","brain ability","deep belief network","associative generated model","input signal sources","learning (artificial intelligence)","top-level RBM","Training","pattern association","character recognition","belief networks","DBN","Arabic digital picture generation","Digital images","Chinese character digital images","natural language processing","Brain modeling","feature extraction","Boltzmann machines","multisignals","pattern recognition"],"max_cite":null,"pub_year":2015.0,"sources":"['ieee']","rawkeys":["brain","brain modeling","association","dbn","brain ability","deep belief network","associative generated model","biological neural networks","computational modeling","input signal sources","learning (artificial intelligence)","boltzmann machines","training","pattern association","character recognition","belief networks","arabic digital picture generation","restricted boltzmann machine","chinese character digital images","top-level rbm","natural language processing","feature extraction","multisignals","pattern recognition","yttrium","generation","digital images"],"tags":["brain","deep belief networks","brain modeling","association","machine learning","brain ability","associative generated model","biological neural networks","digital image","computational modeling","input signal sources","boltzmann machines","training","pattern association","character recognition","belief networks","arabic digital picture generation","restricted boltzmann machine","chinese character digital images","top-level rbm","natural language processing","feature extraction","multisignals","pattern recognition","yttrium","generation"]},{"p_id":32493,"title":"Scaling conditional random fields by one-against-the-other decomposition","abstract":"As a powerful sequence labeling model, conditional random fields (CRFs) have had successful applications in many natural language processing (NLP) tasks. However, the high complexity of CRFs training only allows a very small tag (or label) set, because the training becomes intractable as the tag set enlarges. This paper proposes an improved decomposed training and joint decoding algorithm for CRF learning. Instead of training a single CRF model for all tags, it trains a binary sub-CRF independently for each tag. An optimal tag sequence is then produced by a joint decoding algorithm based on the probabilistic output of all sub-CRFs involved. To test its effectiveness, we apply this approach to tackling Chinese word segmentation (CWS) as a sequence labeling problem. Our evaluation shows that it can reduce the computational cost of this language processing task by 40-50% without any significant performance loss on various large-scale data sets. \u00a9 2008 Springer.","keywords_author":["Chinese word segmentation","Conditional random fields","Machine learning","Natural language processing"],"keywords_other":["Probabilistic output","Large-scale data","Language processing","Conditional random fields","Chinese word segmentation","Machine learning","Sequence labeling","Computational costs","Performance losses","Joint decoding","Natural language processing"],"max_cite":3.0,"pub_year":2008.0,"sources":"['scp']","rawkeys":["large-scale data","probabilistic output","conditional random fields","chinese word segmentation","language processing","machine learning","natural language processing","sequence labeling","computational costs","joint decoding","performance losses"],"tags":["probabilistic output","chinese word segmentation","conditional random field","language processing","machine learning","natural language processing","sequence labeling","computational costs","large scale data","joint decoding","performance loss"]},{"p_id":48877,"title":"Content in-context: Automatic news contextualization","abstract":"\u00a9 Springer International Publishing AG 2017. News content usually refers to specific facts, people or situations. Understanding the whole relationships and context about the actual text may require the user to manually connect, search and filter other sources, with a considerable effort. This work considers the use of deep learning techniques to analyze the news content to automatically build context and ultimately to provide a valuable solution for news readers and news editors using a real dataset from the most important online newspaper. Using a news article as a seed, we relate and add valuable information to news articles, providing understanding and comprehensiveness to put information into users\u2019 perspective, based on semantic, unobvious and time changing relationships. Context is constructed by modeling news and ontological information using deep learning. Ontological information is extracted from knowledge base sources. Content In-context is a complete solution applying this approach to a Colombian real, online news dataset, produced in Spanish. Tests and results are performed considering new articles using unknown data. Results prove to be interesting compared to classical machine learning methods.","keywords_author":["Deep learning","Natural language processing","Neural networks"],"keywords_other":["Complete solutions","Online newspaper","News articles","Learning techniques","News content","Machine learning methods","Contextualization","Knowledge base"],"max_cite":0.0,"pub_year":2017.0,"sources":"['scp']","rawkeys":["machine learning methods","complete solutions","deep learning","neural networks","natural language processing","knowledge base","online newspaper","contextualization","learning techniques","news articles","news content"],"tags":["machine learning methods","complete solutions","neural networks","machine learning","knowledge base","natural language processing","online newspaper","contextualization","learning techniques","news articles","news content"]},{"p_id":30448,"title":"Visual question answering: A survey of methods and datasets","abstract":"\u00a9 2017 Elsevier Inc.Visual Question Answering (VQA) is a challenging task that has received increasing attention from both the computer vision and the natural language processing communities. Given an image and a question in natural language, it requires reasoning over visual elements of the image and general knowledge to infer the correct answer. In the first part of this survey, we examine the state of the art by comparing modern approaches to the problem. We classify methods by their mechanism to connect the visual and textual modalities. In particular, we examine the common approach of combining convolutional and recurrent neural networks to map images and questions to a common feature space. We also discuss memory-augmented and modular architectures that interface with structured knowledge bases. In the second part of this survey, we review the datasets available for training and evaluating VQA systems. The various datatsets contain questions at different levels of complexity, which require different capabilities and types of reasoning. We examine in depth the question\/answer pairs from the Visual Genome project, and evaluate the relevance of the structured annotations of images with scene graphs for VQA. Finally, we discuss promising future directions for the field, in particular the connection to structured knowledge bases and the use of natural language processing models.","keywords_author":["Knowledge bases","Natural language processing","Recurrent neural networks","Visual question answering"],"keywords_other":["Question Answering","State of the art","Modular architectures","General knowledge","Knowledge basis","Natural languages","NAtural language processing","Structured knowledge"],"max_cite":4.0,"pub_year":2017.0,"sources":"['scp', 'wos']","rawkeys":["knowledge bases","question answering","modular architectures","structured knowledge","natural languages","natural language processing","state of the art","recurrent neural networks","visual question answering","general knowledge","knowledge basis"],"tags":["modular architectures","neural networks","natural languages","natural language processing","knowledge base","information retrieval","state of the art","structural knowledge","general knowledge","knowledge basis","video quality assessment"]},{"p_id":50935,"title":"Automatic knowledge extraction and data mining from echo reports of pediatric heart disease: Application on clinical decision support","abstract":"\u00a9 Springer International Publishing Switzerland 2015.Echocardiography (Echo) reports of the patients with pediatric heart disease contain many disease related information, which provide great support to physicians for clinical decision. Such as treatment customization based on the risk level of the specific patient. With the help of natural language processing (NLP), information can be automatically extracted from free-text reports. Those structured data is much easier to analyze with the existing data mining approaches. In this study, we extract the entity\/anatomic site-feature-value (EFV) triples in the Echo reports and predict the risk level on this basis. The prediction accuracy of machine learning and rule-based method are compared based on a manual prepared ideal data, to explore the application of automatic knowledge extraction on clinical decision support.","keywords_author":["Clinical decision support","Echo reports","Knowledge extraction","Machine learning","Natural language processing"],"keywords_other":["Knowledge extraction","Rule-based method","Clinical decision","Echo reports","Clinical decision support","Prediction accuracy","Structured data","NAtural language processing"],"max_cite":0.0,"pub_year":2015.0,"sources":"['scp']","rawkeys":["machine learning","natural language processing","knowledge extraction","structured data","clinical decision support","prediction accuracy","echo reports","clinical decision","rule-based method"],"tags":["machine learning","natural language processing","knowledge extraction","structured data","clinical decision support","prediction accuracy","echo reports","clinical decision","rule-based method"]},{"p_id":22267,"title":"Sentiment Analysis Is a Big Suitcase","abstract":"\u00a9 2017 IEEE. Although most works approach it as a simple categorization problem, sentiment analysis is actually a suitcase research problem that requires tackling many natural language processing (NLP) tasks. The expression 'sentiment analysis' itself is a big suitcase (like many others related to affective computing, such as emotion recognition or opinion mining) that all of us use to encapsulate our jumbled idea about how our minds convey emotions and opinions through natural language. The authors address the composite nature of the problem via a three-layer structure inspired by the 'jumping NLP curves' paradigm. In particular, they argue that there are (at least) 15 NLP problems that need to be solved to achieve human-like performance in sentiment analysis.","keywords_author":["artificial intelligence","computational linguistics","intelligent systems","knowledge representation and reasoning","machine learning","natural language processing","sentiment analysis"],"keywords_other":["Affective Computing","Research problems","TEXT","Three-layer structures","Emotion recognition","Sentiment analysis","Knowledge representation and reasoning","Natural languages","PERSONALITY","Opinion mining"],"max_cite":12.0,"pub_year":2017.0,"sources":"['ieee', 'scp', 'wos']","rawkeys":["three-layer structures","artificial intelligence","affective computing","knowledge representation and reasoning","natural languages","machine learning","natural language processing","intelligent systems","text","personality","emotion recognition","computational linguistics","research problems","opinion mining","sentiment analysis"],"tags":["three-layer structures","affective computing","knowledge representation and reasoning","natural languages","machine learning","natural language processing","intelligent systems","text","emotion recognition","computational linguistics","research problems","personalizations","opinion mining","sentiment analysis"]},{"p_id":50941,"title":"Effective utilization and implementation of toolkits in Natural language processing","abstract":"\u00a9 Research India Publications. Natural Language Processing holds great promise for making computer interfaces that are easier to use for people, since people will (hopefully) be able to talk to the computer in their own language, rather than learn a specialized language of computer commands. For programming, however, the necessity of a formal programming language for communicating with a computer has always been taken for granted. Natural language processing aims to design and build software that will analyze, understand, and generate languages that humans use naturally, so that eventually you will be able to address your computer as though you were addressing another person. The availability of enormous online documents, textual group discussion among various forums gained a massive attraction of researchers in the field of natural language processing. There are many NLP tools accessible in many platforms with different programming structures and usage meter. These tool kits channelize users to work on NLP based on their impending programming knowledge. This paper gives an elaborate description about the role played by the tool kits in developing NLP applications. It also identifies the unique features of the toolkit with the fair narration on their technical aspects.","keywords_author":["Artificial intelligence (AI)","Linguistics","Machine learning","Natural language processing","Sentimental analysis","Tamil language"],"keywords_other":null,"max_cite":0.0,"pub_year":2015.0,"sources":"['scp']","rawkeys":["tamil language","linguistics","natural language processing","machine learning","sentimental analysis","artificial intelligence (ai)"],"tags":["tamil language","linguistics","natural language processing","machine learning","sentiment analysis"]},{"p_id":46851,"title":"Review on Research and Development of Memory Neural Networks","abstract":"\u00a9 Copyright 2017, Institute of Software, the Chinese Academy of Sciences. All rights reserved. Firstly, in this paper, the key features of memory neural networks in the strongly supervised model and the weakly supervised model and introduced. Then the corresponding application scenarios and processing methods, as well as the advantages and disadvantages of the two models are summarized. Next, a brief survey on the development and application of the two models (including the innovation on the model and the innovation in application) is provided, and the key roles of individual innovative models in the natural language processing are summarized. Finally, the complex challenges of memory neural networks in the natural language processing and the future development of memory neural networks are also discussed.","keywords_author":["Artificial intelligence","Machine learning","Memory neural network","Natural language processing","Question answering","Recurrent neural network"],"keywords_other":["Application scenario","Question Answering","Research and development","Processing method","Innovative models","Key feature","Development and applications"],"max_cite":0.0,"pub_year":2017.0,"sources":"['scp']","rawkeys":["development and applications","question answering","artificial intelligence","research and development","memory neural network","innovative models","application scenario","machine learning","natural language processing","processing method","recurrent neural network","key feature"],"tags":["development and applications","research and development","memory neural network","neural networks","innovative models","application scenario","machine learning","natural language processing","processing method","information retrieval","key feature"]},{"p_id":32520,"title":"Selecting the best feature set for Thai word sense disambiguation using support vector machines","abstract":"This paper proposes a method of selecting the best feature set for Thai word sense disambiguation by using Support Vector Machines (SVM) algorithm. This research focuses on Thai verb sense disambiguation. Many approaches have been employed to resolve the word sense ambiguity with a reasonable degree of accuracy. Our research focuses on the corpus-based approach that employs a supervised machine learning method for disambiguation. The machine learning method has the ability of selecting the suitable feature. In order to find the best feature set for resolving Thai word sense ambiguity, our method uses characteristics of the words co-occur with the ambiguous word in sentences extracted from Thai corpus for determining sense of the ambiguous word. The ambiguous words are evaluated with 30 feature sets under \"word\" \"part of speech (POS)\" and \"semantic concept (SM)\" features. The result shows that \"word & SM\" feature set gives the best result as the best feature set of sense indicator and the accuracy rate is approximately 90-96%.","keywords_author":["Machine learning","Natural language processing","Support vector machines","Word sense disambiguation"],"keywords_other":["Word sense disambiguation"],"max_cite":3.0,"pub_year":2007.0,"sources":"['scp']","rawkeys":["natural language processing","machine learning","support vector machines","word sense disambiguation"],"tags":["natural language processing","machine learning","word sense disambiguation"]},{"p_id":34569,"title":"Challenges with label quality for supervised learning","abstract":"\u00a9 2015 ACM 1936-1955\/2015\/03-ART2 $15.00. Organizations that develop and use technologies around information retrieval, machine learning, recommender systems, and natural language processing depend on labels for engineering and experimentation. These labels, usually gathered via human computation, are used in machine-learned models for prediction and evaluation purposes. In such scenarios, collecting high-quality labels is a very important part of the overall process. We elaborate on these challenges and discuss research directions.","keywords_author":["Crowdsourcing","Human computation","Label quality","Machine learning"],"keywords_other":["High quality","Overall process","Human computation","NAtural language processing","Crowdsourcing"],"max_cite":2.0,"pub_year":2015.0,"sources":"['scp']","rawkeys":["human computation","machine learning","crowdsourcing","natural language processing","overall process","label quality","high quality"],"tags":["human computation","machine learning","crowdsourcing","natural language processing","overall process","label quality","high quality"]},{"p_id":44813,"title":"Deep learning enabled national cancer surveillance","abstract":"\u00a9 2017 IEEE. Pathology reports are a primary source of information for cancer registries which process high volumes of free-text reports annually. Information extraction and coding is a manual, labor-intensive process. In this talk I will discuss the latest deep learning technology, presenting both theoretical and practical perspectives that are relevant to natural language processing of clinical pathology reports. Using different deep learning architectures, I will present benchmark studies for various information extraction tasks and discuss their importance in supporting a comprehensive and scalable national cancer surveillance program.","keywords_author":["cancer","deep learning","natural language processing","surveillance"],"keywords_other":["Primary sources","cancer","Benchmark study","Cancer registries","Learning architectures","Learning technology","Surveillance program","Labor intensive process"],"max_cite":0.0,"pub_year":2018.0,"sources":"['scp']","rawkeys":["learning architectures","primary sources","learning technology","cancer","deep learning","natural language processing","benchmark study","surveillance program","cancer registries","surveillance","labor intensive process"],"tags":["learning architectures","primary sources","learning technology","cancer","machine learning","natural language processing","benchmark study","surveillance program","cancer registries","surveillance","labor intensive process"]},{"p_id":26382,"title":"Disambiguation of ambiguous biomedical terms using examples generated from the UMLS Metathesaurus","abstract":"Researchers have access to a vast amount of information stored in textual documents and there is a pressing need for the development of automated methods to enable and improve access to this resource. Lexical ambiguity, the phenomena in which a word or phrase has more than one possible meaning, presents a significant obstacle to automated text processing. Word Sense Disambiguation (WSD) is a technology that resolves these ambiguities automatically and is an important stage in text understanding. The most accurate approaches to WSD rely on manually labeled examples but this is usually not available and is prohibitively expensive to create. This paper offers a solution to that problem by using information in the UMLS Metathesaurus to automatically generate labeled examples. Two approaches are presented. The first is an extension of existing work (Liu et al., 2002 [1]) and the second a novel approach that exploits information in the UMLS that has not been used for this purpose. The automatically generated examples are evaluated by comparing them against the manually labeled ones in the NLM-WSD data set and are found to outperform the baseline. The examples generated using the novel approach produce an improvement in WSD performance when combined with manually labeled examples. \u00a9 2010 Elsevier Inc.","keywords_author":["Natural Language Processing","NLP","UMLS","Unified Medical Language System","Word Sense Disambiguation","WSD"],"keywords_other":["Unified medical language systems","NLP","Word Sense Disambiguation","UMLS","WSD","NAtural language processing"],"max_cite":7.0,"pub_year":2010.0,"sources":"['scp']","rawkeys":["umls","unified medical language systems","unified medical language system","nlp","natural language processing","word sense disambiguation","wsd"],"tags":["unified medical language systems","natural language processing","word sense disambiguation"]},{"p_id":34575,"title":"Cross-Lingual Preposition Disambiguation for Machine Translation","abstract":"\u00a9 2015 The Authors. This paper presents a supervised prepositional ambiguity resolution method for machine translation models in which the target language is Tamil and source language is English. We restrict our transfer ambiguity resolution problem with few prepositions only. This resolution method is based on supervised models which exploit collocation occurrences and linguistic information as features. This attempt will rectify the challenges in handling prepositions in English to Tamil automatic translation system. The preliminary results obtained from the evaluation shows that the proposed method is suitable for preposition resolution problem.","keywords_author":["Feature extraction","Machine learning","Machine translation","Natural language processing","Preposition disambiguation"],"keywords_other":["Linguistic information","Resolution methods","Automatic translation","Ambiguity resolution","Machine translation models","Machine translations","NAtural language processing","Preposition disambiguation"],"max_cite":2.0,"pub_year":2015.0,"sources":"['scp']","rawkeys":["automatic translation","resolution methods","preposition disambiguation","machine translation models","ambiguity resolution","machine learning","natural language processing","machine translation","feature extraction","machine translations","linguistic information"],"tags":["automatic translation","resolution methods","preposition disambiguation","machine translation models","ambiguity resolution","machine learning","natural language processing","feature extraction","machine translations","linguistic information"]},{"p_id":9997,"title":"Deep learning of mutation-gene-drug relations from the literature","abstract":"Background: Molecular biomarkers that can predict drug efficacy in cancer patients are crucial components for the advancement of precision medicine. However, identifying these molecular biomarkers remains a laborious and challenging task. Next-generation sequencing of patients and preclinical models have increasingly led to the identification of novel gene-mutation-drug relations, and these results have been reported and published in the scientific literature.","keywords_author":["BioNLP","Convolutional neural networks","Deep learning","Information extraction","Mutation","NLP","Precision medicine","Text mining","Deep learning","Convolutional neural networks","Information extraction","Text mining","NLP","BioNLP","Mutation","Precision medicine"],"keywords_other":["Mutation","Text mining","SENSITIVITY","State-of-the-art methods","TEXT","KNOWLEDGE","EXTRACTION","Next-generation sequencing","BioNLP","BIOMEDICAL LITERATURE","SYSTEM","VEMURAFENIB","Convolutional neural network","DISCOVERY","CANCER","Relation classifications","VARIANTS","Classification accuracy"],"max_cite":0.0,"pub_year":2018.0,"sources":"['scp', 'wos']","rawkeys":["state-of-the-art methods","knowledge","text","convolutional neural network","extraction","biomedical literature","next-generation sequencing","text mining","information extraction","mutation","system","relation classifications","convolutional neural networks","deep learning","variants","precision medicine","nlp","vemurafenib","cancer","discovery","classification accuracy","bionlp","sensitivity"],"tags":["state-of-the-art methods","knowledge","text","convolutional neural network","extraction","biomedical literature","next-generation sequencing","text mining","information extraction","machine learning","system","relation classifications","variants","precision medicine","vemurafenib","cancer","discovery","classification accuracy","natural language processing","bionlp","sensitivity","mutations"]},{"p_id":48912,"title":"A software agent for social networks using natural language processing techniques","abstract":"\u00a9 2016 IEEE.Machine-learning techniques are widely used in the computer processing of natural language. Software agents are programs that use machine learning and natural language processing to communicate with users and to perform certain tasks or provide specific information. This paper provides an overview of basic software agents and describes the implementation of an intelligent software agent for social network Facebook Finally, the results of the research propose potential improvements of implemented system.","keywords_author":["machine learning","natural language processing","social networks"],"keywords_other":["Specific information","Intelligent software agent","Computer processing","Facebook","Natural languages","NAtural language processing","Machine learning techniques"],"max_cite":0.0,"pub_year":2017.0,"sources":"['scp']","rawkeys":["facebook","specific information","machine learning techniques","machine learning","natural language processing","natural languages","social networks","intelligent software agent","computer processing"],"tags":["facebook","specific information","computation process","machine learning techniques","machine learning","natural language processing","natural languages","social networks","intelligent software agent"]},{"p_id":46867,"title":"A Resistant Strain: Revealing the Online Grassroots Rise of the Antivaccination Movement","abstract":"\u00a9 2017 IEEE. An analysis of more than eight years of data from vaccination forums on mothering.com shows that the antivaccination movement is well-organized and widely dispersed, and that it emerged long before concerns about immunity were expressed. The findings are evidence of a formidable challenge to the social norms surrounding vaccination.","keywords_author":["AI","antivaccination movement","artificial intelligence","computers and society","computing methodologies","computing methodologies","healthcare","herd immunity","human safety","immunity","machine learning","mothering.com","natural language processing","public policy issues","social media","vaccination","vaccines","web text analysis"],"keywords_other":["Herd immunities","Human safety","immunity","Social media","vaccination","Web text analysis","mothering.com","Computers and societies","antivaccination movement","Computing methodologies"],"max_cite":0.0,"pub_year":2017.0,"sources":"['scp']","rawkeys":["vaccines","public policy issues","ai","human safety","herd immunity","computers and society","antivaccination movement","herd immunities","computers and societies","healthcare","machine learning","vaccination","mothering.com","web text analysis","immunity","social media","artificial intelligence","natural language processing","computing methodologies"],"tags":["web text analysis","healthcare","immunity","social media","machine learning","natural language processing","vaccination","public policy issues","human safety","herd immunity","mothering.com","computers and society","antivaccination movement","computing methodologies"]},{"p_id":3860,"title":"Co-training for demographic classification using deep learning from label proportions","abstract":"\u00a9 2017 IEEE. Deep learning algorithms have recently produced state-of-the-art accuracy in many classification tasks, but this success is typically dependent on access to many annotated training examples. For domains without such data, an attractive alternative is to train models with light, or distant supervision. In this paper, we introduce a deep neural network for the Learning from Label Proportion (LLP) setting, in which the training data consist of bags of unlabeled instances with associated label distributions for each bag. We introduce a new regularization layer, Batch Averager, that can be appended to the last layer of any deep neural network to convert it from supervised learning to LLP. This layer can be implemented readily with existing deep learning packages. To further support domains in which the data consist of two conditionally independent feature views (e.g. image and text), we propose a co-training algorithm that iteratively generates pseudo bags and refits the deep LLP model to improve classification accuracy. We demonstrate our models on demographic attribute classification (gender and race\/ethnicity), which has many applications in social media analysis, public health, and marketing. We conduct experiments to predict demographics of Twitter users based on their tweets and profile image, without requiring any user-level annotations for training. We find that the deep LLP approach outperforms baselines for both text and image features separately. Additionally, we find that co-training algorithm improves image and text classification by 4% and 8% absolute F1, respectively. Finally, an ensemble of text and image classifiers further improves the absolute F1 measure by 4% on average.","keywords_author":["Data Mining","Deep Learning","Image Classification","Learning from Label Proportions","LLP","Machine Learning","NLP","Social Media Analysis"],"keywords_other":null,"max_cite":0.0,"pub_year":2017.0,"sources":"['scp', 'ieee']","rawkeys":["learning from label proportions","data mining","nlp","deep learning","llp","social media analysis","machine learning","image classification"],"tags":["learning from label proportions","data mining","llp","social media analysis","machine learning","natural language processing","image classification"]},{"p_id":50967,"title":"Genetic algorithm as machine learning for profiles recognition","abstract":"Copyright \u00a9 2015 by SCITEPRESS - Science and Technology Publications, Lda. All rights reserved.Persons are often asked to provide information about themselves. These data are very heterogeneous and result in as many \"profiles\" as contexts. Sorting a large amount of profiles from different contexts and assigning them back to a specific individual is quite a difficult problem. Semantic processing and machine learning are key tools to achieve this goal. This paper describes a framework to address this issue by means of concepts and algorithms selected from different Artificial Intelligence fields. Indeed, a Vector Space Model is customized to first transpose semantic information into a mathematical model. Then, this model goes through a Genetic Algorithm (GA) which is used as a supervised learning algorithm for training a computer to determine how much two profiles are similar. Amongst the GAs, this study introduces a new reproduction method (Best Together), and compare it to some usual ones (Wheel, Binary Tournament).This paper also evaluates the accuracy of the GAs predictions for profiles clustering with the computation of a similarity score, as well as its ability to classify two profiles are similar or non-similar. We believe that the overall methodology can be used for any kind of sources using profiles and, more generally, for similar data recognition.","keywords_author":["Clustering","Genetic Algorithm","Machine Learning","Natural Language Processing","Profiles Recognition"],"keywords_other":null,"max_cite":0.0,"pub_year":2015.0,"sources":"['scp']","rawkeys":["profiles recognition","natural language processing","machine learning","clustering","genetic algorithm"],"tags":["profiles recognition","natural language processing","machine learning","clustering","genetic algorithm"]},{"p_id":44825,"title":"Automated knowledge extraction from the federal acquisition regulations system (FARS)","abstract":"\u00a9 2017 IEEE. With increasing regulation of Big Data, it is becoming essential for organizations to ensure compliance with various data protection standards. The Federal Acquisition Regulations System (FARS) within the Code of Federal Regulations (CFR) includes facts and rules for individuals and organizations seeking to do business with the US Federal government. Parsing and gathering knowledge from such lengthy regulation documents is currently done manually and is time and human intensive. Hence, developing a cognitive assistant for automated analysis of such legal documents has become a necessity. We have developed semantically rich approach to automate the analysis of legal documents and have implemented a system to capture various facts and rules contributing towards building an efficient legal knowledge base that contains details of the relationships between various legal elements, semantically similar terminologies, deontic expressions and cross-referenced legal facts and rules. In this paper, we describe our framework along with the results of automating knowledge extraction from the FARS document (Title 48, CFR). Our approach can be used by Big Data Users to automate knowledge extraction from Large Legal documents.","keywords_author":["Code of Federal Regulations","Deep Learning","Information Retrieval","NLP"],"keywords_other":["Legal knowledge","Knowledge extraction","Code of Federal Regulations","Legal documents","Automated analysis","Federal governments","Protection standards","Deontic"],"max_cite":0.0,"pub_year":2018.0,"sources":"['scp']","rawkeys":["automated analysis","nlp","deep learning","federal governments","legal documents","information retrieval","protection standards","knowledge extraction","code of federal regulations","deontic","legal knowledge"],"tags":["automated analysis","machine learning","federal governments","information retrieval","legal documents","natural language processing","knowledge extraction","protection standards","code of federal regulations","deontic","legal knowledge"]},{"p_id":50972,"title":"Generation of infotips from interface labels","abstract":"\u00a9 Springer International Publishing Switzerland 2015.A method is presented for generating informative and naturalsounding infotips for graphical elements of a user interface. A domain-specific corpus is prepared using natural language processing techniques, and a termfrequency\/ inverse-document-frequency transform is used for vectorization of features. A k-means algorithm is then used to cluster the corpus by semantic similarity and retrieve the most similar infotips for any inputted interface label. We demonstrate the feasibility of this method and conclude by proposing several approaches to improve the selection of infotips by incorporating natural language processing and machine learning techniques.","keywords_author":["Graphical user interfaces","Instructional design","Machine learning","Natural language processing"],"keywords_other":["Inverse Document Frequency","Instructional designs","k-Means algorithm","Semantic similarity","Graphical elements","Domain specific","NAtural language processing","Machine learning techniques"],"max_cite":0.0,"pub_year":2015.0,"sources":"['scp']","rawkeys":["graphical elements","inverse document frequency","graphical user interfaces","instructional designs","machine learning techniques","machine learning","natural language processing","semantic similarity","k-means algorithm","instructional design","domain specific"],"tags":["graphical elements","inverse document frequency","graphical user interfaces","machine learning techniques","machine learning","natural language processing","semantic similarity","k-means algorithm","instructional design","domain specific"]},{"p_id":18205,"title":"An empirical study to address the problem of unbalanced data sets in sentiment classification","abstract":"With the emergence of Web 2.0, Sentiment Analysis is receiving more and more attention. Several interesting works were performed to address different issues in Sentiment Analysis. Nevertheless, the problem of Unbalanced Data Sets was not enough tackled within this research area. This paper presents the study we have carried out to address the problem of unbalanced data sets in supervised sentiment classification in a multi-lingual context. We propose three different methods to under-sample the majority class documents. These methods are Remove Similar, Remove Farthest and Remove by Clustering. Our goal is to compare the effectiveness of the proposed methods with the common random under-sampling. We also aim to evaluate the behavior of the classifiers toward different under-sampling rates. We use three different common classifiers, namely Na\u00efve Bayes, Support Vector Machines and k-Nearest Neighbors. The experiments are carried out on two Arabic data sets and an English data set. We show that the four under-sampling methods are typically competitive. Na\u00efve Bayes is shown as insensitive to unbalanced data sets. But Support Vector Machines seems to be highly sensitive to unbalanced data sets; k-Nearest Neighbors shows a slight sensitivity to imbalance in comparison with Support Vector Machines. \u00a9 2012 IEEE.","keywords_author":["Machine Learning","Natural Language Processing","Opinion Mining","Sentiment Analysis","Text Classification","Unbalanced Data sets"],"keywords_other":["Text classification","Sentiment analysis","Unbalanced data","NAtural language processing","Opinion mining"],"max_cite":25.0,"pub_year":2012.0,"sources":"['scp']","rawkeys":["unbalanced data","natural language processing","machine learning","unbalanced data sets","sentiment analysis","opinion mining","text classification"],"tags":["unbalanced data","unbalanced datasets","natural language processing","machine learning","sentiment analysis","opinion mining","text classification"]},{"p_id":44830,"title":"Develop method to predict the increase in the Nikkei VI index","abstract":"\u00a9 2017 IEEE. We propose a method of predicting an increase in the Nikkei VI index by analyzing social media based on the premise that investor sentiment is posted on social media. Since the VI index expresses the fear of investors, it is a closely related index to the risk of depression. Therefore, the VI index is an important indicator as an instrument for investment judgment. To predict the increase in the VI index more accurately, we divide messages by topic models specific to social media of stock trading and predict such the increase by machine learning using those topics. As a result of leave-one-day-out cross-validation, precision of our method was 0.45. We also found that the daily fluctuation in the VI index and the number of messages are as effective as feature quantities as the topic-posting frequency.","keywords_author":["LDA topic model","Machine Learning","Natural Language Processing","Stock BBS","VI Index"],"keywords_other":["Cross validation","Daily fluctuations","VI Index","Social media","Investor sentiments","Topic model","Stock trading","Topic Modeling"],"max_cite":0.0,"pub_year":2018.0,"sources":"['scp']","rawkeys":["cross validation","topic modeling","social media","machine learning","natural language processing","lda topic model","daily fluctuations","stock bbs","stock trading","topic model","vi index","investor sentiments"],"tags":["topic modeling","lda topic modelling","social media","machine learning","natural language processing","daily fluctuations","stock bbs","stock trading","vi index","investor sentiment","computer vision"]},{"p_id":34591,"title":"AMRITA-CEN@FIRE-2014: Morpheme extraction and lemmatization for Tamil using machine learning","abstract":"\u00a9 2015 ACM. This paper presents the method of Morpheme Extraction and lemmatization for Tamil language in Morpheme Extraction Task (MET) of FIRE-2014. Tamil is a morphologically rich and agglutinative language. Such a language needs deeper analysis at the word level to capture the meaning of the word from its morphemes and its categories. In this attempt, the methodology employed to extract Tamil morphemes and lemmas are based on a supervised machine learning algorithm for nouns and verbs and simple suffix stripping for pronouns and proper nouns. Morphemes are extracted for other Part-of-Speech categories using Tamil Part of Speech tagger. In supervised learning, Morphological analyzer problem is redefined as a classification problem. We decompose the problem of noun and verb morpheme extraction into two sub-problems: learning to perform morpheme identification of words in a text, and learning to perform morpheme tagging. In addition to the Morpheme extraction task results of FIRE-2014, we have carried out different experiments to show the effectiveness of the proposed method.","keywords_author":["Lemmatization","Machine Learning","Morpheme Extraction","Natural Language Processing","Support Vector Machines"],"keywords_other":["Agglutinative language","Tamil language","Morphological analyzer","Supervised machine learning","Part-of-speech tagger","Lemmatization","NAtural language processing","Nouns and verbs"],"max_cite":2.0,"pub_year":2014.0,"sources":"['scp']","rawkeys":["lemmatization","morphological analyzer","nouns and verbs","tamil language","machine learning","agglutinative language","natural language processing","part-of-speech tagger","support vector machines","morpheme extraction","supervised machine learning"],"tags":["lemmatization","morphological analyzer","nouns and verbs","tamil language","machine learning","agglutinative language","natural language processing","part-of-speech tagger","morpheme extraction","supervised machine learning"]},{"p_id":22304,"title":"Semantic Text Classification for Supporting Automated Compliance Checking in Construction","abstract":"\u00a9 2014 American Society of Civil Engineers. Automated regulatory and contractual compliance checking requires automated rule extraction from regulatory and contractual textual documents (e.g., contract specifications). Automated rule extraction is a challenging task that requires complex processing of text. In the proposed automated compliance checking (ACC) approach, the first step in automating the rule extraction process is automatically classifying the different documents and parts of documents (e.g., contract clauses) into predefined categories (environmental, safety, health, etc.) for preparing it for further text analysis and rule extraction. These categories are defined in a semantic model for normative reasoning. This paper presents a semantic, machine learning-based text classification algorithm for classifying clauses and subclauses of general conditions for supporting ACC in construction. The multilabel classification problem was transformed into a set of binary classification problems. Different machine learning algorithms, text preprocessing techniques, methods of text feature scoring, methods of feature weighting, and feature sizes were implemented and evaluated at different thresholds. The developed classifier achieved 100 and 96% recall and precision, respectively, on the testing data.","keywords_author":["Automated compliance checking","Automated construction management systems","Machine learning","Natural language processing","Semantic systems","Text classification"],"keywords_other":null,"max_cite":12.0,"pub_year":2016.0,"sources":"['scp']","rawkeys":["natural language processing","machine learning","semantic systems","automated compliance checking","automated construction management systems","text classification"],"tags":["natural language processing","machine learning","semantic systems","automated compliance checking","automated construction management systems","text classification"]},{"p_id":46878,"title":"The UAB Informatics Institute and 2016 CEGS N-GRID de-identification shared task challenge","abstract":"\u00a9 2017 Elsevier Inc.Clinical narratives (the text notes found in patients\u2019 medical records) are important information sources for secondary use in research. However, in order to protect patient privacy, they must be de-identified prior to use. Manual de-identification is considered to be the gold standard approach but is tedious, expensive, slow, and impractical for use with large-scale clinical data. Automated or semi-automated de-identification using computer algorithms is a potentially promising alternative. The Informatics Institute of the University of Alabama at Birmingham is applying de-identification to clinical data drawn from the UAB hospital's electronic medical records system before releasing them for research. We participated in a shared task challenge by the Centers of Excellence in Genomic Science (CEGS) Neuropsychiatric Genome-Scale and RDoC Individualized Domains (N-GRID) at the de-identification regular track to gain experience developing our own automatic de-identification tool. We focused on the popular and successful methods from previous challenges: rule-based, dictionary-matching, and machine-learning approaches. We also explored new techniques such as disambiguation rules, term ambiguity measurement, and used multi-pass sieve framework at a micro level. For the challenge's primary measure (strict entity), our submissions achieved competitive results (f-measures: 87.3%, 87.1%, and 86.7%). For our preferred measure (binary token HIPAA), our submissions achieved superior results (f-measures: 93.7%, 93.6%, and 93%). With those encouraging results, we gain the confidence to improve and use the tool for the real de-identification task at the UAB Informatics Institute.","keywords_author":["Automatic de-identification","Clinical natural language processing","Machine learning","Shared task"],"keywords_other":["Information sources","Shared task","Machine learning approaches","University of Alabama at Birmingham","Electronic medical record","NAtural language processing","Centers of excellence","De-identification"],"max_cite":0.0,"pub_year":2017.0,"sources":"['scp']","rawkeys":["centers of excellence","de-identification","machine learning","natural language processing","clinical natural language processing","information sources","automatic de-identification","shared task","university of alabama at birmingham","electronic medical record","machine learning approaches"],"tags":["centers of excellence","de-identification","machine learning","natural language processing","clinical natural language processing","information sources","automatic de-identification","shared task","university of alabama at birmingham","electronic medical record","machine learning approaches"]},{"p_id":42787,"title":"Using convolution control block for Chinese sentiment analysis","abstract":"\u00a9 2017 Elsevier Inc. Convolutional neural network (CNN) has lately received great attention because of its good performance in the field of computer vision and speech recognition. It has also been widely used in natural language processing. But those methods for English cannot be transplanted due to phrase segmentation. Those for Chinese are not good enough for poorly semantic retrieving. We propose a Chinese sentiment classification model on the concept of convolution control block (CCB). It aims at classifying Chinese sentences into the positive or the negative. CCB based model considers short and long context dependencies. Parallel convolution of different kernel sizes is designed for phrase segmentation, gate convolution for merging and filtering abstract features, and tiering 5 layers of CCBs for word connection in sentence. Our model is evaluated on Million Chinese Hotel Review dataset. Its positive emotion accuracy reaches 92.58%, which outperforms LR_all and DCN by 2.89% and 4.03%, respectively. Model depth and sentence length are positively related to the accuracy. Gate convolution indeed improves model accuracy.","keywords_author":["Convolutional neural network","Deep learning","Natural language processing","Sentiment analysis"],"keywords_other":["Phrase segmentations","Context dependency","Chinese sentence","Positive emotions","Sentiment analysis","Convolutional neural network","Sentiment classification","Sentence length"],"max_cite":0.0,"pub_year":2018.0,"sources":"['scp']","rawkeys":["phrase segmentations","sentiment classification","positive emotions","deep learning","chinese sentence","natural language processing","sentence length","convolutional neural network","context dependency","sentiment analysis"],"tags":["phrase segmentations","sentiment classification","positive emotions","chinese sentence","natural language processing","machine learning","sentence length","convolutional neural network","context dependent","sentiment analysis"]},{"p_id":7973,"title":"How do users like this feature? A fine grained sentiment analysis of App reviews","abstract":"\u00a9 2014 IEEE.App stores allow users to submit feedback for downloaded apps in form of star ratings and text reviews. Recent studies analyzed this feedback and found that it includes information useful for app developers, such as user requirements, ideas for improvements, user sentiments about specific features, and descriptions of experiences with these features. However, for many apps, the amount of reviews is too large to be processed manually and their quality varies largely. The star ratings are given to the whole app and developers do not have a mean to analyze the feedback for the single features. In this paper we propose an automated approach that helps developers filter, aggregate, and analyze user reviews. We use natural language processing techniques to identify fine-grained app features in the reviews. We then extract the user sentiments about the identified features and give them a general score across all reviews. Finally, we use topic modeling techniques to group fine-grained features into more meaningful high-level features. We evaluated our approach with 7 apps from the Apple App Store and Google Play Store and compared its results with a manually, peer-conducted analysis of the reviews. On average, our approach has a precision of 0.59 and a recall of 0.51. The extracted features were coherent and relevant to requirements evolution tasks. Our approach can help app developers to systematically analyze user opinions about single features and filter irrelevant reviews.","keywords_author":null,"keywords_other":["Fine grained","Automated approach","High-level features","Requirements evolution","Sentiment analysis","User requirements","NAtural language processing","Topic Modeling"],"max_cite":117.0,"pub_year":2014.0,"sources":"['scp']","rawkeys":["automated approach","high-level features","topic modeling","fine grained","natural language processing","requirements evolution","user requirements","sentiment analysis"],"tags":["automated approach","high-level features","topic modeling","fine grained","natural language processing","requirements evolution","user requirements","sentiment analysis"]},{"p_id":44838,"title":"The cybernetics thought collective project: Using computational methods to reveal intellectual context in archival material","abstract":"\u00a9 2017 IEEE. This paper discusses 'The Cybernetics Thought Collective: A History of Science and Technology Portal Project,' a collaborative effort among four institutions that maintain archival records vital to the exploration of cybernetic history - the University of Illinois at Urbana-Champaign, the American Philosophical Society, the British Library, and MIT. With recent grant funding from the NEH, the multi-institutional team is developing a prototype web-portal and analysis-engine to provide access to archival material related to the development of the field of cybernetics, which influenced the development of modern computing and provided a common language to articulate similar questions about behavior across disciplines - regardless of whether the subject of study was animal, machine, or social group. The project is also enabling the digitization of the personal archives of four founders of cybernetics - Heinz von Foerster, W. Ross Ashby, Warren S. McCulloch, and Norbert Wiener. Using computational methods based on advanced machine-learning algorithms to yield network and entity relationships maps from the digitized texts, this project seeks to create access to archival material that enables humanities scholars to better understand the development of cybernetic ideas and to enable scientists and engineers to reuse and access cybernetic data.","keywords_author":["cybernetics","digital archives","machine learning","named entities","natural language processing"],"keywords_other":["Archival materials","Named entities","Scientists and engineers","History of science and technologies","Entity-relationship","Intellectual context","University of Illinois","Digital archives"],"max_cite":0.0,"pub_year":2018.0,"sources":"['scp']","rawkeys":["named entities","history of science and technologies","archival materials","machine learning","natural language processing","university of illinois","cybernetics","digital archives","intellectual context","entity-relationship","scientists and engineers"],"tags":["named entities","history of science and technologies","archival materials","machine learning","natural language processing","university of illinois","cybernetics","digital archives","intellectual context","entity-relationship","scientists and engineers"]},{"p_id":18217,"title":"A machine learning approach for identifying disease-treatment relations in short texts","abstract":"The Machine Learning (ML) field has gained its momentum in almost any domain of research and just recently has become a reliable tool in the medical domain. The empirical domain of automatic learning is used in tasks such as medical decision support, medical imaging, protein-protein interaction, extraction of medical knowledge, and for overall patient management care. ML is envisioned as a tool by which computer-based systems can be integrated in the healthcare field in order to get a better, more efficient medical care. This paper describes a ML-based methodology for building an application that is capable of identifying and disseminating healthcare information. It extracts sentences from published medical papers that mention diseases and treatments, and identifies semantic relations that exist between diseases and treatments. Our evaluation results for these tasks show that the proposed methodology obtains reliable outcomes that could be integrated in an application to be used in the medical care domain. The potential value of this paper stands in the ML settings that we propose and in the fact that we outperform previous results on the same data set. \u00a9 2011 IEEE.","keywords_author":["Healthcare","machine learning","natural language processing."],"keywords_other":["Automatic-learning","Medical knowledge","Machine-learning","Potential values","Evaluation results","Medical papers","Patient management","Computer-based system","Data sets","Semantic relations","Protein-protein interactions","Medical domains","Medical decisions","Medical care","NAtural language processing"],"max_cite":25.0,"pub_year":2011.0,"sources":"['scp']","rawkeys":["medical care","semantic relations","healthcare","potential values","machine learning","medical knowledge","automatic-learning","medical decisions","natural language processing","data sets","evaluation results","patient management","computer-based system","machine-learning","medical papers","protein-protein interactions","medical domains"],"tags":["medical care","semantic relations","healthcare","potential values","machine learning","medical knowledge","automatic-learning","medical decisions","natural language processing","data sets","evaluation results","patient management","computer-based system","medical papers","protein-protein interactions","medical domains"]},{"p_id":5930,"title":"The unreasonable effectiveness of data","abstract":"Natural language processing problems are solved by the use of unreasonable effectiveness of data. The biggest successes in natural-language-related machine learning is statistical speech recognition and statistical machine translation. The first lesson of Web-scale learning is to use available large-scale data rather than hoping for annotated data that is not available. The statistical language models used in speech recognition and machine translation consist of a huge database of probabilities of short sequences of consecutive words. Natural language processing require choosing a representation language, encoding a model in that language, and performing inference on the model. Semantic interpretation deals with imprecise, ambiguous natural languages, and service interoperability deals with making data precise enough so that the programs operating on the data functions effectively.","keywords_author":null,"keywords_other":["Statistical machine translation","Representation languages","Semantic interpretation","Natural languages","Statistical language models","Service interoperability","Machine translations","NAtural language processing"],"max_cite":340.0,"pub_year":2009.0,"sources":"['wos', 'scp', 'ieee']","rawkeys":["representation languages","natural languages","natural language processing","service interoperability","semantic interpretation","machine translations","statistical language models","statistical machine translation"],"tags":["representation languages","natural languages","natural language processing","service interoperability","statistical language modeling","semantic interpretation","machine translations","statistical machine translation"]},{"p_id":26410,"title":"Machine learning based English-to-Korean transliteration using grapheme and phoneme information","abstract":"Machine transliteration is an automatic method to generate characters or words in one alphabetical system for the corresponding characters in another alphabetical system. Machine transliteration can play an important role in natural language application such as information retrieval and machine translation, especially for handling proper nouns and technical terms. The previous works focus on either a grapheme-based or phoneme-based method. However, transliteration is an orthographical and phonetic converting process. Therefore, both grapheme and phoneme information should be considered in machine transliteration. In this paper, we propose a grapheme and phoneme-based transliteration model and compare it with previous grapheme-based and phoneme-based models using several machine learning techniques. Our method shows about 13-78% performance improvement. Copyright \u00a9 2005 The Institute of Electronics, Information and Communication Engineers.","keywords_author":["Information retrieval","Machine learning","Machine translation","Machine transliteration","Natural language processing"],"keywords_other":["Alphabetical system","Machine transliteration","Machine translation","Character generation"],"max_cite":7.0,"pub_year":2005.0,"sources":"['scp']","rawkeys":["machine learning","natural language processing","information retrieval","machine translation","machine transliteration","alphabetical system","character generation"],"tags":["machine learning","natural language processing","information retrieval","machine transliteration","alphabetical system","character generation","machine translations"]},{"p_id":48954,"title":"A Case Study on Sepsis Using PubMed and Deep Learning for Ontology Learning","abstract":"\u00a9 2017 European Federation for Medical Informatics (EFMI) and IOS Press. We investigate the application of distributional semantics models for facilitating unsupervised extraction of biomedical terms from unannotated corpora. Term extraction is used as the first step of an ontology learning process that aims to (semi-)automatic annotation of biomedical concepts and relations from more than 300K PubMed titles and abstracts. We experimented with both traditional distributional semantics methods such as Latent Semantic Analysis (LSA) and Latent Dirichlet Allocation (LDA) as well as the neural language models CBOW and Skip-gram from Deep Learning. The evaluation conducted concentrates on sepsis, a major life-Threatening condition, and shows that Deep Learning models outperform LSA and LDA with much higher precision.","keywords_author":["Deep Learning","Ontology Learning","OWL","PubMed","SPARQL"],"keywords_other":["Humans","Semantics","Natural Language Processing","PubMed","Machine Learning","Information Storage and Retrieval","Sepsis"],"max_cite":0.0,"pub_year":2017.0,"sources":"['scp']","rawkeys":["pubmed","deep learning","semantics","natural language processing","machine learning","humans","sepsis","sparql","owl","information storage and retrieval","ontology learning"],"tags":["pubmed","semantics","natural language processing","machine learning","humans","sepsis","sparql","owl","information storage and retrieval","ontology learning"]},{"p_id":44859,"title":"Social media based NPL system to find and retrieve ARM data: Concept paper","abstract":"\u00a9 2017 IEEE. Information connectivity and retrieval has a role in our daily lives. The most pervasive source of online information is databases. The amount of data is growing at rapid rate and database technology is improving and having a profound effect. Almost all online applications are storing and retrieving information from databases. One challenge in supplying the public with wider access to informational databases is the need for knowledge of database languages like Structured Query Language (SQL). Although the SQL language has been published in many forms, not everybody is able to write SQL queries. Another challenge is that it may not be practical to make the public aware of the structure of the database. There is a need for novice users to query relational databases using their natural language. To solve this problem, many natural language interfaces to structured databases have been developed. The goal is to provide more intuitive method for generating database queries and delivering responses. Social media makes it possible to interact with a wide section of the population. Through this medium, and with the help of Natural Language Processing (NLP) we can make the data of the Atmospheric Radiation Measurement Data Center (ADC) more accessible to the public. We propose an architecture for using Apache Lucene\/Solr [1], OpenML [2,3], and Kafka [4] to generate an automated query\/response system with inputs from Twitter5, our Cassandra DB, and our log database. Using the Twitter API and NLP we can give the public the ability to ask questions of our database and get automated responses.","keywords_author":["machine learning","natural language processing","social media interaction","stream pipelining"],"keywords_other":["Atmospheric radiation measurements","Structured database","On-line applications","Database technology","Social media","On-line information","Natural language interfaces","Structured query languages"],"max_cite":0.0,"pub_year":2018.0,"sources":"['scp']","rawkeys":["structured query languages","natural language interfaces","on-line applications","social media","natural language processing","machine learning","on-line information","stream pipelining","structured database","social media interaction","atmospheric radiation measurements","database technology"],"tags":["natural language interfaces","on-line applications","social media","natural language processing","machine learning","on-line information","stream pipelining","structured database","structured query language","social media interaction","atmospheric radiation measurements","database technology"]},{"p_id":26428,"title":"Risk factor detection for heart disease by applying text analytics in electronic medical records","abstract":"\u00a9 2015 Elsevier Inc.In the United States, about 600,000 people die of heart disease every year. The annual cost of care services, medications, and lost productivity reportedly exceeds 108.9 billion dollars. Effective disease risk assessment is critical to prevention, care, and treatment planning. Recent advancements in text analytics have opened up new possibilities of using the rich information in electronic medical records (EMRs) to identify relevant risk factors. The 2014 i2b2\/UTHealth Challenge brought together researchers and practitioners of clinical natural language processing (NLP) to tackle the identification of heart disease risk factors reported in EMRs. We participated in this track and developed an NLP system by leveraging existing tools and resources, both public and proprietary. Our system was a hybrid of several machine-learning and rule-based components. The system achieved an overall F1 score of 0.9185, with a recall of 0.9409 and a precision of 0.8972.","keywords_author":["Medical records","Natural language processing","Risk assessment","Text classification"],"keywords_other":["Humans","Vocabulary, Controlled","Lost productivities","California","Text classification","Aged","Pattern Recognition, Automated","Electronic medical record","Female","Cohort Studies","Cardiovascular Diseases","Comorbidity","Rule-based components","Diabetes Complications","Narration","Medical record","Longitudinal Studies","Electronic medical records (EMRs)","Incidence","NAtural language processing","Confidentiality","Disease risk assessment","Male","Risk Assessment","Electronic Health Records","Computer Security","Middle Aged","Natural Language Processing","Data Mining"],"max_cite":7.0,"pub_year":2015.0,"sources":"['scp', 'wos']","rawkeys":["vocabulary","automated","computer security","aged","electronic medical record","text classification","comorbidity","middle aged","electronic health records","medical record","risk assessment","medical records","lost productivities","diabetes complications","narration","cohort studies","data mining","electronic medical records (emrs)","incidence","humans","confidentiality","cardiovascular diseases","controlled","disease risk assessment","longitudinal studies","male","natural language processing","california","pattern recognition","female","rule-based components"],"tags":["vocabulary","automated","computer security","aged","electronic medical record","text classification","comorbidity","control","electronic health records","medical record","middle aged","disease risk-assessment","risk assessment","lost productivities","cardiovascular disease","diabetes complications","narration","cohort studies","data mining","incidence","humans","confidentiality","longitudinal studies","male","natural language processing","california","pattern recognition","female","rule-based components"]},{"p_id":34622,"title":"Protocol design contests","abstract":"In fields like data mining and natural language processing, design contests have been successfully used to advance the state of the art. Such contests offer an opportunity to bring the excitement and challenges of protocol design-one of the core intellectual elements of research and practice in networked systems-to a broader group of potential contributors, whose ideas may prove important. Moreover, it may lead to an increase in the number of students, especially undergraduates or those learning via online courses, interested in pursuing a career in the field. We describe the creation of the infrastructure and our experience with a protocol design contest conducted in MIT's graduate Computer Networks class. This contest involved the design and evaluation of a congestion-control protocol for paths traversing cellular wireless networks. One key to the success of a design contest is an unambiguous, measurable objective to compare protocols. In practice, protocol design is the art of trading off conflicting goals with each other, but in this contest, we specified that the goal was to maximize log(throughput=delay). This goal is a good match for applications such as video streaming or videoconferencing that care about high throughput and low interactive delays. Some students produced protocols whose performance was better than published protocols tackling similar goals. Furthermore, the convex hull of the set of all student protocols traced out a tradeoff curve in the throughput-delay space, providing useful insights into the entire space of possible protocols. We found that student protocols diverged in performance between the training and testing traces, indicating that some students had overtrained (\"overfitted\") their protocols to the training trace. Our conclusion is that, if designed properly, such contests could benefit networking research by making new proposals more easily reproducible and amenable to such \"gamification,\" improve networked systems, and provide an avenue for outreach.","keywords_author":["Congestion control","Design contest","Gamification","Machine learning","Protocol"],"keywords_other":["Cellular wireless networks","Design contests","Training and testing","Trade-off curves","Networked systems","Design and evaluations","NAtural language processing","Gamification"],"max_cite":2.0,"pub_year":2014.0,"sources":"['scp']","rawkeys":["cellular wireless networks","gamification","trade-off curves","design contests","machine learning","natural language processing","training and testing","design contest","design and evaluations","networked systems","congestion control","protocol"],"tags":["cellular wireless networks","gamification","design contests","machine learning","natural language processing","training and testing","design and evaluations","protocols","networked systems","congestion control","trade-off curves"]},{"p_id":28479,"title":"A bootstrapping approach for training a ner with conditional random fields","abstract":"In this paper we present a bootstrapping approach for training a Named Entity Recognition (NER) system. Our method starts by annotating persons' names on a dataset of 50,000 news items. This is performed using a simple dictionary-based approach. Using such training set we build a classification model based on Conditional Random Fields (CRF). We then use the inferred classification model to perform additional annotations of the initial seed corpus, which is then used for training a new classification model. This cycle is repeated until the NER model stabilizes. We evaluate each of the bootstrapping iterations by calculating: (i) the precision and recall of the NER model in annotating a small gold-standard collection (HAREM); (ii) the precision and recall of the CRF bootstrapping annotation method over a small sample of news; and (iii) the correctness and the number of new names identified. Additionally, we compare the NER model with a dictionary-based approach, our baseline method. Results show that our bootstrapping approach stabilizes after 7 iterations, achieving high values of precision (83%) and recall (68%). \u00a9 2011 Springer-Verlag.","keywords_author":["Conditional Random Fields","Machine Learning","Named Entity Recognition","Natural Language Processing"],"keywords_other":["Named entity recognition","Machine-learning","Classification models","Data sets","Small samples","Precision and recall","Training sets","Annotation methods","Conditional random field","NAtural language processing","Baseline methods"],"max_cite":5.0,"pub_year":2011.0,"sources":"['scp']","rawkeys":["conditional random fields","conditional random field","training sets","named entity recognition","machine learning","natural language processing","precision and recall","data sets","annotation methods","machine-learning","small samples","classification models","baseline methods"],"tags":["conditional random field","training sets","named entity recognition","machine learning","natural language processing","precision and recall","data sets","annotation methods","small samples","classification models","baseline methods"]},{"p_id":42818,"title":"Artificial Intelligence and Intellectual Property","abstract":null,"keywords_author":["Artificial Intelligence","Editorial","Information Retrieval","Machine Learning","Natural Language Processing","Patent Information"],"keywords_other":null,"max_cite":0.0,"pub_year":2018.0,"sources":"['scp']","rawkeys":["artificial intelligence","patent information","natural language processing","machine learning","information retrieval","editorial"],"tags":["patent information","natural language processing","machine learning","information retrieval","editorial"]},{"p_id":53059,"title":"Close integration of ML and NLP tools in BioAlvis for semantic search in bacteriology","abstract":"This paper focuses on the use of corpus-based machine learning (ML) methods for fine-grained semantic annotation of text. The state of the art in semantic annotation in Life Science as in other technical and scientific domains, takes advantage of recent breakthroughs in the development of natural language processing (NLP) platforms. The resources required to run such platforms include named entity dictionaries, terminologies, grammars and ontologies. The demand for domain-specific, comprehensive and low cost resources led to the intensive use of ML methods. The precise specification of the ML task goal and target knowledge, and the adequate normalization of the training corpus representation can notably increase the quality of the acquired knowledge. We argue in this paper that integrated ML-NLP architectures facilitate such specifications. We illustrate our demonstration with four representative NLP tasks that are part of the BioAlvis semantic annotation platform. Their impact on the quality of the semantic annotation is qualified through the evaluation of an IR application in Bacteriology.","keywords_author":["Machine learning","Natural language processing","Ontology learning","Semantic annotation"],"keywords_other":["Ontology learning","Semantic search","Semantic annotations","Training corpus","State of the art","Domain specific","Close integration","NAtural language processing"],"max_cite":0.0,"pub_year":2009.0,"sources":"['scp']","rawkeys":["semantic search","semantic annotations","natural language processing","machine learning","state of the art","training corpus","close integration","domain specific","semantic annotation","ontology learning"],"tags":["semantic search","semantic annotations","natural language processing","machine learning","state of the art","training corpus","close integration","domain specific","ontology learning"]},{"p_id":1858,"title":"Image retrieval using latent feature learning by deep architecture","abstract":"The explosive growth of data, images in the World Wide Web makes it critical to the information retrievals. Image retrieval has been recognized as an elementary problem in the retrieval tasks and this exercise has got a wide attention based on the underlying domain characteristics. For instance, in social media data encompasses of noisy, diverse, heterogeneous, interconnected data. To confront these numerous characteristics and employ image retrieval the widely accepted deep architecture concept is utilized with the help of natural language latent query features. In this paper, we are introducing a novel approach for image retrieval task which collaboratively make use of the technicalities of natural language processing and deep architecture.","keywords_author":["Natural language processing","Deep architecture","Latent features"],"keywords_other":["natural language latent query feature","social media data","learning (artificial intelligence)","Natural language processing","World Wide Web","Training","image retrieval","natural language processing","information retrieval","Image retrieval","deep architecture","Web sites","Kernel","feature extraction","Context","Neural networks","Computer architecture","latent feature learning"],"max_cite":null,"pub_year":2014.0,"sources":"['ieee']","rawkeys":["natural language latent query feature","web sites","latent features","learning (artificial intelligence)","social media data","neural networks","training","image retrieval","natural language processing","information retrieval","deep architecture","kernel","feature extraction","context","world wide web","computer architecture","latent feature learning"],"tags":["natural language latent query feature","web sites","latent features","social media data","neural networks","training","image retrieval","deep architectures","machine learning","information retrieval","natural language processing","kernel","feature extraction","context","world wide web","computer architecture","latent feature learning"]},{"p_id":42821,"title":"Prerequisites between learning objects: Automatic extraction based on a machine learning approach","abstract":"\u00a9 2017 Elsevier Ltd One standing problem in the area of web-based e-learning is how to support instructional designers to effectively and efficiently retrieve learning materials, appropriate for their educational purposes. Learning materials can be retrieved from structured repositories, such as repositories of Learning Objects and Massive Open Online Courses; they could also come from unstructured sources, such as web hypertext pages. Platforms for distance education often implement algorithms for recommending specific educational resources and personalized learning paths to students. But choosing and sequencing the adequate learning materials to build adaptive courses may reveal to be quite a challenging task. In particular, establishing the prerequisite relationships among learning objects, in terms of prior requirements needed to understand and complete before making use of the subsequent contents, is a crucial step for faculty, instructional designers or automated systems whose goal is to adapt existing learning objects to delivery in new distance courses. Nevertheless, this information is often missing. In this paper, an innovative machine learning-based approach for the identification of prerequisites between text-based resources is proposed. A feature selection methodology allows us to consider the attributes that are most relevant to the predictive modeling problem. These features are extracted from both the input material and weak-taxonomies available on the web. Input data undergoes a Natural language process that makes finding patterns of interest more easy for the applied automated analysis. Finally, the prerequisite identification is cast to a binary statistical classification task. The accuracy of the approach is validated by means of experimental evaluations on real online coursers covering different subjects.","keywords_author":["Curriculum sequencing","E-learning","Learning object","Machine learning","Prerequisite"],"keywords_other":["Curriculum sequencing","Experimental evaluation","Machine learning approaches","Natural language process","Statistical classification","Massive open online course","Learning objects","Prerequisite"],"max_cite":0.0,"pub_year":2018.0,"sources":"['scp']","rawkeys":["natural language process","experimental evaluation","curriculum sequencing","massive open online course","machine learning","prerequisite","statistical classification","learning objects","learning object","machine learning approaches","e-learning"],"tags":["experimental evaluation","curriculum sequencing","machine learning","natural language processing","statistical classification","mooc","learning objects","prerequisite","machine learning approaches","e-learning"]},{"p_id":16198,"title":"Uncovering and improving upon the inherent deficiencies of radiology reporting through data mining","abstract":"Uncertainty has been the perceived Achilles heel of the radiology report since the inception of the free-text report. As a measure of diagnostic confidence (or lack thereof), uncertainty in reporting has the potential to lead to diagnostic errors, delayed clinical decision making, increased cost of healthcare delivery, and adverse outcomes. Recent developments in data mining technologies, such as natural language processing (NLP), have provided the medical informatics community with an opportunity to quantify report concepts, such as uncertainty. The challenge ahead lies in taking the next step from quantification to understanding, which requires combining standardized report content, data mining, and artificial intelligence; thereby creating Knowledge Discovery Databases (KDD). The development of this database technology will expand our ability to record, track, and analyze report data, along with the potential to create data-driven and automated decision support technologies at the point of care. For the radiologist community, this could improve report content through an objective and thorough understanding of uncertainty, identifying its causative factors, and providing data-driven analysis for enhanced diagnosis and clinical outcomes. \u00a9 2010 Society for Imaging Informatics in Medicine.","keywords_author":["Data mining","Reporting","Uncertainty"],"keywords_other":["Medical informatics","Radiology reporting","Clinical decision making","Clinical outcome","Adverse outcomes","Data-driven","Radiology reports","Achilles heel","Data-driven analysis","Database technology","Knowledge discovery database","Point of care","Decision supports","Data mining technology","Healthcare delivery","Natural language processing"],"max_cite":39.0,"pub_year":2010.0,"sources":"['scp']","rawkeys":["clinical decision making","point of care","data-driven analysis","adverse outcomes","data mining technology","achilles heel","uncertainty","radiology reports","radiology reporting","data mining","knowledge discovery database","reporting","data-driven","medical informatics","database technology","natural language processing","clinical outcome","healthcare delivery","decision supports"],"tags":["data mining","knowledge discovery database","clinical decision making","point of care","reporting","uncertainty","data-driven analysis","natural language processing","adverse outcomes","clinical outcome","radiology reports","medical informatics","achilles heel","healthcare delivery","data driven","data mining technology","decision supports","database technology"]},{"p_id":34632,"title":"Learning word representations for Turkish T\u00fcrk\u00e7e i\u00e7in kelime temsillerinin \u00f6\u01e7renimi","abstract":"High-quality word representations have been very successful in recent years at improving performance across a variety of NLP tasks. These word representations are the mappings of each word in the vocabulary to a real vector in the Euclidean space. Besides high performance on specific tasks, learned word representations have been shown to perform well on establishing linear relationships among words. The recently introduced skip-gram model improved performance on unsupervised learning of word embeddings that contains rich syntactic and semantic word relations both in terms of accuracy and speed. Word embeddings that have been used frequently on English language, is not applied to Turkish yet. In this paper, we apply the skip-gram model to a large Turkish text corpus and measured the performance of them quantitatively with the 'question' sets that we generated. The learned word embeddings and the question sets are publicly available at our website. \u00a9 2014 IEEE.","keywords_author":["Deep Learning","Natural Language Processing","Word embeddings"],"keywords_other":["Deep learning","English languages","Euclidean spaces","Linear relationships","Word representations","Improving performance","Embeddings","NAtural language processing"],"max_cite":2.0,"pub_year":2014.0,"sources":"['scp']","rawkeys":["embeddings","euclidean spaces","improving performance","deep learning","natural language processing","word embeddings","english languages","word representations","linear relationships"],"tags":["embeddings","euclidean spaces","machine learning","natural language processing","word embedding","english languages","word representations","improve performance","linear relationships"]},{"p_id":51017,"title":"Cross domain sentiment analysis using different machine learning techniques","abstract":"\u00a9 Springer International Publishing Switzerland 2015.Sentiment analysis is the field of study that focuses on finding effectively the conduct of subjective text by analyzing people\u2019s opinions, sentiments, evaluations, attitudes and emotions towards entities. The analysis of data and extracting the opinion word from the data is a challenging task especially when it involves reviews from completely different domains. We perform cross domain sentiment analysis on Amazon product reviews (books, dvd, kitchen appliances, electronics) and TripAdvisor hotel reviews, effectively classify the reviews to positive and negative polarities by applying various preprocessing techniques like Tokenization, POS Tagging, Lemmatization and Stemming which can enhance the performance of sentiment analysis in terms of accuracy and time to train the classifier. Various methods proposed for document-level sentiment classification like Naive Bayes, k-Nearest Neighbor, Support Vector Machines and Decision Tree are analysed in this work. Cross domain sentiment classification is useful because many times we might not have training corpus of specific domains for which we need to classify the data and also cross domain is favoured by lower computation cost and time. Despite poor performance in accuracy, the time consumed for sentiment classification when multiple testing datasets of different domains are present is far less in case of cross domain as compared to single domain. This work aims to define methods to overcome the problem of lower accuracy in cross-domain sentiment classification using different techniques and taking the benefit of being a faster method.","keywords_author":["Cross domain","Machine learning","Natural language processing"],"keywords_other":["Cross-domain","Negative polarity","Sentiment analysis","Sentiment classification","Preprocessing techniques","K-nearest neighbors","NAtural language processing","Machine learning techniques"],"max_cite":0.0,"pub_year":2015.0,"sources":"['scp']","rawkeys":["sentiment classification","machine learning techniques","machine learning","natural language processing","cross-domain","preprocessing techniques","cross domain","sentiment analysis","negative polarity","k-nearest neighbors"],"tags":["sentiment classification","machine learning techniques","natural language processing","machine learning","cross-domain","preprocessing techniques","sentiment analysis","negative polarity","k-nearest neighbors"]},{"p_id":1870,"title":"Multi-Layer and Recursive Neural Networks for Metagenomic Classification","abstract":"Recent advances in machine learning, specifically in deep learning with neural networks, has made a profound impact on fields such as natural language processing, image classification, and language modeling; however, feasibility and potential benefits of the approaches to metagenomic data analysis has been largely under-explored. Deep learning exploits many layers of learning nonlinear feature representations, typically in an unsupervised fashion, and recent results have shown outstanding generalization performance on previously unseen data. Furthermore, some deep learning methods can also represent the structure in a data set. Consequently, deep learning and neural networks may prove to be an appropriate approach for metagenomic data. To determine whether such approaches are indeed appropriate for metagenomics, we experiment with two deep learning methods: i) a deep belief network, and ii) a recursive neural network, the latter of which provides a tree representing the structure of the data. We compare these approaches to the standard multi-layer perceptron, which has been well-established in the machine learning community as a powerful prediction algorithm, though its presence is largely missing in metagenomics literature. We find that traditional neural networks can be quite powerful classifiers on metagenomic data compared to baseline methods, such as random forests. On the other hand, while the deep learning approaches did not result in improvements to the classification accuracy, they do provide the ability to learn hierarchical representations of a data set that standard classification methods do not allow. Our goal in this effort is not to determine the best algorithm in terms accuracy-as that depends on the specific application-but rather to highlight the benefits and drawbacks of each of the approach we discuss and provide insight on how they can be improved for predictive metagenomic analysis.","keywords_author":["Comparative metagenomics","Metagenomics","Microbiome","Neural networks","Comparative metagenomics","metagenomics","microbiome","neural networks"],"keywords_other":["multilayer perceptrons","Metagenomics","predictive metagenomic analysis","Hierarchical representation","multilayer-recursive neural networks","Neural networks","Organisms","molecular biophysics","image classification","metagenomic literature","metagenomic data analysis","Feature extraction","machine learning","Machine learning","Recursive neural networks","deep belief network","unsupervised fashion","genomics","image representation","Nanobioscience","Algorithms","learning (artificial intelligence)","Microbiota","Microbiome","neural networks","prediction algorithm","Training","medical image processing","deep learning methods","machine learning community","Vegetation","neurophysiology","DNA","NAtural language processing","nonlinear feature representations","Neural Networks (Computer)","multilayer perceptron","classification accuracy","data structure","Generalization performance","natural language processing","language modeling","hierarchical representations","Machine learning communities","Classification accuracy","metagenomic classification","generalization performance"],"max_cite":15.0,"pub_year":2015.0,"sources":"['scp', 'wos', 'ieee']","rawkeys":["multilayer perceptrons","predictive metagenomic analysis","multilayer-recursive neural networks","microbiome","molecular biophysics","image classification","nanobioscience","metagenomic literature","metagenomic data analysis","metagenomics","machine learning","deep belief network","comparative metagenomics","algorithms","microbiota","unsupervised fashion","genomics","image representation","recursive neural networks","learning (artificial intelligence)","neural networks (computer)","neural networks","prediction algorithm","training","dna","medical image processing","deep learning methods","machine learning communities","machine learning community","neurophysiology","hierarchical representation","nonlinear feature representations","vegetation","multilayer perceptron","organisms","classification accuracy","data structure","natural language processing","feature extraction","language modeling","hierarchical representations","metagenomic classification","generalization performance"],"tags":["predictive metagenomic analysis","data structures","multilayer-recursive neural networks","microbiome","molecular biophysics","image classification","nanobioscience","metagenomic literature","metagenomic data analysis","metagenomics","organization","machine learning","comparative metagenomics","algorithms","microbiota","unsupervised fashion","genomics","image representation","recursive neural networks","neural networks","training","dna","medical image processing","deep learning methods","language model","machine learning communities","neurophysiology","hierarchical representation","nonlinear feature representations","vegetation","classification accuracy","natural language processing","prediction algorithms","feature extraction","multi layer perceptron","deep belief networks","metagenomic classification","generalization performance"]},{"p_id":51022,"title":"Automated Detection of Health Websites' HONcode Conformity: Can N-gram Tokenization Replace Stemming?","abstract":"\u00a9 2015 IMIA and IOS Press. Authors evaluated supervised automatic classification algorithms for determination of health related web-page compliance with individual HONcode criteria of conduct using varying length character n-gram vectors to represent healthcare web page documents. The training\/testing collection comprised web page fragments extracted by HONcode experts during the manual certification process. The authors compared automated classification performance of n-gram tokenization to the automated classification performance of document words and Porter-stemmed document words using a Naive Bayes classifier and DF (document frequency) dimensionality reduction metrics. The study attempted to determine whether the automated, language-independent approach might safely replace word-based classification. Using 5-grams as document features, authors also compared the baseline DF reduction function to Chi-square and Z-score dimensionality reductions. Overall study results indicate that n-gram tokenization provided a potentially viable alternative to document word stemming.","keywords_author":["HONcode","Machine learning","N-gram"],"keywords_other":["Consumer Health Information","Internet","Feasibility Studies","Supervised Machine Learning","Pattern Recognition, Automated","Natural Language Processing","Guideline Adherence","Practice Guidelines as Topic","Internationality"],"max_cite":0.0,"pub_year":2015.0,"sources":"['scp']","rawkeys":["guideline adherence","n-gram","consumer health information","automated","internationality","feasibility studies","practice guidelines as topic","machine learning","natural language processing","internet","pattern recognition","supervised machine learning","honcode"],"tags":["guideline adherence","n-grams","consumer health information","automated","feasibility studies","practice guidelines as topic","machine learning","natural language processing","pattern recognition","internet","international (co)","supervised machine learning","honcode"]},{"p_id":51025,"title":"Personality mining from biographical data with the \"adjectival Marker \" technique","abstract":"The last decade has witnessed significant work in personality mining from lexical cues in social media data. Not much work has yet been undertaken in extracting these lexical cues from biographical data populating social media. Most of this work involves a large crowd of researchers leveraging dictionary-based approaches such as LIWC (which primarily focus on function words). By means of this paper we intend to introduce a novel method of personality mining from social media data called \"Adjectival-marker Technique\". This method involves extracting lexical features from descriptive texts (e.g. biographical data) to train a learning model, so as to predict the respective personality traits of the subject. Conceptually, it draws heavily from the last 78 years of work in lexical psychology and the Big Five personality test. However, it is not only a computational variant of the primordial theories of lexical psychology, but is also competent in conferring a substantial accuracy of personality prediction, matching that obtained by psychometric tests. In this study, we propose a variant of the Lexical Hypothesis from psychology. This modified hypothesis is validated by the computational results of personality prediction achieved by the Adjectival Marker Technique discussed below. The paper also discusses some insights illustrating the coherence of people's judgments about the subject's personality (virtual personality). The average accuracy (i.e. matching that achieved by psychometric tests for Big 5) for prediction approximated to Extraversion - 82.82% Agreeableness - 89.62%, Conscientiousness - 92.48% and Imaginativeness\/Intellect - 81.67%.","keywords_author":["Machine Learning","Natural Language Processing","Psychology","Social Computing","User Personality Determination"],"keywords_other":["Social computing","Social media datum","Personality minings","Psychology","Personality predictions","User personalities","NAtural language processing","Computational results"],"max_cite":0.0,"pub_year":2015.0,"sources":"['scp']","rawkeys":["social media datum","user personality determination","natural language processing","machine learning","social computing","computational results","personality predictions","user personalities","personality minings","psychology"],"tags":["recognition","social media datum","user personality determination","natural language processing","machine learning","social computing","computational results","personality predictions","user personalities","personality minings"]},{"p_id":44881,"title":"Estimating skill fungibility and forecasting services labor demand","abstract":"\u00a9 2017 IEEE. We present an approach for forecasting labor demand in a services business. We introduce an arrangement of machine learning techniques, each constructed by necessity to overcome issues with data veracity and high dimensionality.","keywords_author":["Forecasting","Labor Economics","Machine Learning","Natural Language Processing","Workforce"],"keywords_other":["Data veracities","High dimensionality","Workforce","Labor demands","Machine learning techniques"],"max_cite":0.0,"pub_year":2018.0,"sources":"['scp']","rawkeys":["labor economics","forecasting","labor demands","machine learning","machine learning techniques","natural language processing","high dimensionality","data veracities","workforce"],"tags":["labor economics","forecasting","labor demands","machine learning","machine learning techniques","high-dimensional","natural language processing","data veracities","workforce"]},{"p_id":20309,"title":"Sentiment-specific representation learning for document-level sentiment analysis","abstract":"Copyright \u00a9 2015 ACM. In this paper, we propose a representation learning research framework for document-level sentiment analysis. Given a document as the input, document-level sentiment analysis aims to automatically classify its sentiment\/opinion (such as thumbs up or thumbs down ) based on the textural in-formation. Despite the success of feature engineering in many previous studies, the hand-coded features do not well capture the semantics of texts. In this research, we argue that learning sentiment-specific semantic representations of documents is crucial for document-level sentiment analysis. We decompose the document semantics into four cascad-ed constitutes: (1) word representation, (2) sentence struc-ture, (3) sentence composition and (4) document compo-sition. Specifically, we learn sentiment-specific word rep-resentations, which simultaneously encode the contexts of words and the sentiment supervisions of texts into the con-tinuous representation space. According to the principle of compositionality, we learn sentiment-specific sentence struc-tures and sentence-level composition functions to produce the representation of each sentence based on the representa-tions of the words it contains. The semantic representations of documents are obtained through document composition, which leverages the sentiment-sensitive discourse relations and sentence representations.","keywords_author":["Deep learning","Natural language processing","Sentiment analysis"],"keywords_other":["Deep learning","Semantic representation","Sentiment analysis","Composition functions","Feature engineerings","Representation space","NAtural language processing","Document composition"],"max_cite":17.0,"pub_year":2015.0,"sources":"['scp', 'wos']","rawkeys":["representation space","deep learning","natural language processing","semantic representation","feature engineerings","document composition","composition functions","sentiment analysis"],"tags":["representation space","natural language processing","machine learning","semantic representation","feature engineerings","document composition","composition functions","sentiment analysis"]},{"p_id":28503,"title":"Parsing citations in biomedical articles using conditional random fields","abstract":"Citations are used ubiquitously in biomedical full-text articles and play an important role for representing both the rhetorical structure and the semantic content of the articles. As a result, text mining systems will significantly benefit from a tool that automatically extracts the content of a citation. In this study, we applied the supervised machine-learning algorithms Conditional Random Fields (CRFs) to automatically parse a citation into its fields (e.g., Author, Title, Journal, and Year). With a subset of html format open-access PubMed Central articles, we report an overall 97.95% F1-score. The citation parser can be accessed at: http:\/\/www.cs.uwm.edu\/~qing\/projects\/cithit\/index.html. \u00a9 2011 Elsevier Ltd.","keywords_author":["Biomedical text mining","Citation indexing","Citation parsing","Conditional random fields","Information extraction","Machine learning","Natural language processing"],"keywords_other":["Citation parsing","Biomedical text minings","Machine learning","Citation indexing","Information extraction","Conditional random field","NAtural language processing"],"max_cite":5.0,"pub_year":2011.0,"sources":"['scp']","rawkeys":["conditional random fields","citation parsing","information extraction","conditional random field","machine learning","natural language processing","biomedical text mining","biomedical text minings","citation indexing"],"tags":["citation parsing","information extraction","conditional random field","machine learning","natural language processing","biomedical text minings","citation indexing"]},{"p_id":8025,"title":"Sentic patterns: Dependency-based rules for concept-level sentiment analysis","abstract":"\u00a9 2014 Elsevier B.V. All rights reserved. The Web is evolving through an era where the opinions of users are getting increasingly important and valuable. The distillation of knowledge from the huge amount of unstructured information on the Web can be a key factor for tasks such as social media marketing, branding, product positioning, and corporate reputation management. These online social data, however, remain hardly accessible to computers, as they are specifically meant for human consumption. The automatic analysis of online opinions involves a deep understanding of natural language text by machines, from which we are still very far. To this end, concept-level sentiment analysis aims to go beyond a mere word-level analysis of text and provide novel approaches to opinion mining and sentiment analysis that enable a more efficient passage from (unstructured) textual information to (structured) machine-processable data. A recent knowledge-based technology in this context is sentic computing, which relies on the ensemble application of common-sense computing and the psychology of emotions to infer the conceptual and affective information associated with natural language. Sentic computing, however, is limited by the richness of the knowledge base and by the fact that the bag-of-concepts model, despite more sophisticated than bag-of-words, misses out important discourse structure information that is key for properly detecting the polarity conveyed by natural language opinions. In this work, we introduce a novel paradigm to concept-level sentiment analysis that merges linguistics, common-sense computing, and machine learning for improving the accuracy of tasks such as polarity detection. By allowing sentiments to flow from concept to concept based on the dependency relation of the input sentence, in particular, we achieve a better understanding of the contextual role of each concept within the sentence and, hence, obtain a polarity detection engine that outperforms state-of-the-art statistical methods.","keywords_author":["Artificial intelligence","Linguistic rules","Natural language processing","Opinion mining","Sentic computing"],"keywords_other":["Knowledge-based technology","Corporate reputations","Sentic Computing","Natural language text","Social media marketings","Linguistic rules","NAtural language processing","Ensemble applications","Opinion mining"],"max_cite":116.0,"pub_year":2014.0,"sources":"['scp', 'ieee']","rawkeys":["artificial intelligence","knowledge-based technology","ensemble applications","linguistic rules","natural language processing","sentic computing","corporate reputations","social media marketings","opinion mining","natural language text"],"tags":["knowledge-based technology","ensemble applications","linguistic rules","machine learning","natural language processing","sentic computing","corporate reputations","social media marketings","opinion mining","natural language text"]},{"p_id":51036,"title":"Improving relation extraction by using an ontology class hierarchy feature","abstract":"\u00a9 Springer International Publishing Switzerland 2015. Relation extraction is a key step to address the problem of structuring natural language text. This paper proposes a new ontology class hierarchy feature to improve relation extraction when applying a method based on the distant supervision approach. It argues in favour of the expressiveness of the feature, in multi-class perceptrons, by experimentally showing its effectiveness when compared with combinations of (regular) lexical features.","keywords_author":["Distant supervision","Machine learning","Natural language processing","Relation extraction","Semantic Web"],"keywords_other":["Relation extraction","Class hierarchies","Natural language text","Lexical features","NAtural language processing","Distant supervision"],"max_cite":0.0,"pub_year":2015.0,"sources":"['scp']","rawkeys":["class hierarchies","natural language processing","machine learning","distant supervision","semantic web","lexical features","relation extraction","natural language text"],"tags":["class hierarchies","natural language processing","machine learning","distant supervision","semantic web","lexical features","relation extraction","natural language text"]},{"p_id":42845,"title":"Local ensemble learning from imbalanced and noisy data for word sense disambiguation","abstract":"\u00a9 2017 Elsevier LtdNatural Language Processing plays a key role in man-machine interactions, allowing computers to understand and analyze human language. One of its more challenging sub-domains is word sense disambiguation, the task of automatically identifying the intended sense (or concept) of an ambiguous word based on the context in which the word is used. This requires proper feature extraction to capture specific data properties and a dedicated machine learning solution to allow for the accurate labeling of the appropriate sense. However, the pattern classification problem posed here is highly challenging, as we must deal with high-dimensional and multi-class imbalanced data that additionally may be corrupted with class label noise. To address these issues, we propose a local ensemble learning solution. It uses a one-class decomposition of the multi-class problem, assigning an ensemble of one-class classifiers to each of the distributions. The classifiers are trained on the basis of low-dimensional subsets of features and a kernel feature space transformation to obtain a more compact representation. Instance weighting is used to filter out potentially noisy instances and reduce overlapping among classes. Finally, a two-level classifier fusion technique is used to reconstruct the original multi-class problem. Our results show that the proposed learning approach displays robustness to both multi-class skewed distributions and class label noise, making it a useful tool for the considered task.","keywords_author":["Class label noise","Ensemble learning","Imbalanced classification","Machine learning","Multi-class imbalance","Natural language processing","One-class classification","Word sense disambiguation"],"keywords_other":["Class imbalance","One-class Classification","Class labels","Ensemble learning","Word Sense Disambiguation","Imbalanced classification"],"max_cite":0.0,"pub_year":2018.0,"sources":"['scp']","rawkeys":["ensemble learning","machine learning","natural language processing","class labels","word sense disambiguation","multi-class imbalance","imbalanced classification","class imbalance","one-class classification","class label noise"],"tags":["ensemble learning","machine learning","natural language processing","class labels","word sense disambiguation","multi-class imbalance","imbalanced classification","class imbalance","one-class classification","class label noise"]},{"p_id":8033,"title":"Deep neural nets as a method for quantitative structure-activity relationships","abstract":"\u00a9 2015 American Chemical Society.Neural networks were widely used for quantitative structure-activity relationships (QSAR) in the 1990s. Because of various practical issues (e.g., slow on large problems, difficult to train, prone to overfitting, etc.), they were superseded by more robust methods like support vector machine (SVM) and random forest (RF), which arose in the early 2000s. The last 10 years has witnessed a revival of neural networks in the machine learning community thanks to new methods for preventing overfitting, more efficient training algorithms, and advancements in computer hardware. In particular, deep neural nets (DNNs), i.e. neural nets with more than one hidden layer, have found great successes in many applications, such as computer vision and natural language processing. Here we show that DNNs can routinely make better prospective predictions than RF on a set of large diverse QSAR data sets that are taken from Mercks drug discovery effort. The number of adjustable parameters needed for DNNs is fairly large, but our results show that it is not necessary to optimize them for individual data sets, and a single set of recommended parameters can achieve better performance than RF for most of the data sets we studied. The usefulness of the parameters is demonstrated on additional data sets not used in the calibration. Although training DNNs is still computationally intensive, using graphical processing units (GPUs) can make this issue manageable.","keywords_author":null,"keywords_other":["Algorithms","Neural Networks (Computer)","Training algorithms","Prospective Studies","Drug Discovery","Machine Learning","Quantitative structure-activity relationships","Support Vector Machine","Graphical processing unit (GPUs)","Workflow","Better performance","Adjustable parameters","Quantitative Structure-Activity Relationship","Quantitative structure activity relationship","Machine learning communities","NAtural language processing"],"max_cite":115.0,"pub_year":2015.0,"sources":"['ieee', 'scp', 'wos']","rawkeys":["adjustable parameters","quantitative structure-activity relationships","workflow","neural networks (computer)","training algorithms","better performance","drug discovery","machine learning","natural language processing","prospective studies","machine learning communities","algorithms","graphical processing unit (gpus)","support vector machine","quantitative structure-activity relationship","quantitative structure activity relationship"],"tags":["adjustable parameters","workflow","graphics processing units","training algorithms","neural networks","better performance","drug discovery","machine learning","natural language processing","prospective studies","machine learning communities","algorithms","qsar"]},{"p_id":48994,"title":"Microblog geolocation estimation with recurrent neural networks","abstract":"\u00a9 2017, Japanese Society for Artificial Intelligence. All rights reserved. People collect and use information about real world from internet to help their daily activities. In particular, the number of users in microblog such as Twitter is so large that users can get a diversity of information. They can elicit not only the information which they need from microblog posts but also the location which is indicated by the contents posted in microblog. While previous approaches apply corpus-based or machine learning that require various prior knowledge such as natural language processing and feature engineering, our approach is able to estimate the location without those requirements with extension of long-short term memory (LSTM). In our experiment, we apply our approach to geo-tagged tweets posted in Twitter and show that this approach is effective in outperforming corpus-based and previous works that use support vector machine (SVM) with bag-of-words (BoW).","keywords_author":["Deep learning","Location estimatation","Long-short term memory","Twitter"],"keywords_other":["Deep learning","Twitter","Daily activity","Prior knowledge","Geolocations","Feature engineerings","Long short term memory","NAtural language processing"],"max_cite":0.0,"pub_year":2017.0,"sources":"['scp']","rawkeys":["deep learning","long-short term memory","daily activity","natural language processing","location estimatation","twitter","feature engineerings","geolocations","prior knowledge","long short term memory"],"tags":["long short-term memory","daily activity","machine learning","location estimatation","natural language processing","feature engineerings","geolocations","prior knowledge","twitter"]},{"p_id":51043,"title":"Applying natural language processing, information retrieval and machine learning to decision support in medical coordination in an emergency medicine context","abstract":"\u00a9 2015 IEEE.The Medical Coordination, which is the application of logistics techniques to emergency context, is responsible for providing appropriate resources, in appropriate conditions to appropriate patients. A system for medical coordination of emergency requests was developed in 2009, although some activities related to medical coordination decision making are extremely subjective. Aiming to decrease subjectivity on activities like prioritization of requests and coordination flow, new technologies of decision support were incorporated to that system. These technologies include textual and semantic processing of clinical summaries and machine learning tools. Results indicate that automated tools could support decision on medical coordination process, allowing coordinators to focus attention on critical cases. These features may streamline the medical coordination, avoiding mistakes and increasing the chances of saving lives.","keywords_author":["Emergency Medicine","Information Retrieval","Machine Learning","Medical Coordination","Natural Language Processing"],"keywords_other":["Emergency contexts","Medical coordination","Prioritization","Decision supports","Emergency medicine","Semantic processing","Automated tools","NAtural language processing"],"max_cite":0.0,"pub_year":2015.0,"sources":"['scp']","rawkeys":["prioritization","machine learning","natural language processing","information retrieval","emergency medicine","semantic processing","medical coordination","emergency contexts","automated tools","decision supports"],"tags":["prioritization","machine learning","natural language processing","information retrieval","emergency medicine","semantic processing","medical coordination","emergency contexts","automated tools","decision supports"]},{"p_id":51046,"title":"Style & identity recognition","abstract":"\u00a9 Tribun EU 2015.Knowledge of the author's identity and style can by used in the fight against forged and and anonymous documents and illegal actions in the Internet. Nowadays, there are many systems dedicated to solving stylometric tasks, but they are predominantly designed only for a specific task; they are used exclusively by their owners; or they do not natively support any Slavic languages. Therefore, we present new open-source modular system Style & Identity Recognition (SIR). The system is designed to support any stylometric tasks with minimal efforts (or event by default) by combining dynamic stylometry features selection and prediction driven by input data labels. The system is free for non-commercial applications and easy to use, therefore it can be helpful for people dealing with threatening e-mails or sms, children forum protection against pedophiles and other tasks. Being customizable and freely accessible, it can be also used as a baseline for other systems solving stylometry tasks. System combines machine learning techniques and nature language processing tools. It is written in Python and it is dependent on other opensource Python libraries.","keywords_author":["Authorship recognition","Machine learning","Opensource","Stylometry"],"keywords_other":["Authorship recognition","Features selection","Commercial applications","Nature language processing","Identity recognition","Open-source","Stylometry","Machine learning techniques"],"max_cite":0.0,"pub_year":2015.0,"sources":"['scp']","rawkeys":["opensource","authorship recognition","machine learning","features selection","commercial applications","machine learning techniques","nature language processing","open-source","stylometry","identity recognition"],"tags":["authorship recognition","machine learning","machine learning techniques","commercial applications","natural language processing","open sources","feature selection","stylometry","identity recognition"]},{"p_id":20327,"title":"Improving sentiment analysis via sentence type classification using BiLSTM-CRF and CNN","abstract":"\u00a9 2016 The Authors Different types of sentences express sentiment in very different ways. Traditional sentence-level sentiment classification research focuses on one-technique-fits-all solution or only centers on one special type of sentences. In this paper, we propose a divide-and-conquer approach which first classifies sentences into different types, then performs sentiment analysis separately on sentences from each type. Specifically, we find that sentences tend to be more complex if they contain more sentiment targets. Thus, we propose to first apply a neural network based sequence model to classify opinionated sentences into three types according to the number of targets appeared in a sentence. Each group of sentences is then fed into a one-dimensional convolutional neural network separately for sentiment classification. Our approach has been evaluated on four sentiment classification datasets and compared with a wide range of baselines. Experimental results show that: (1) sentence type classification can improve the performance of sentence-level sentiment analysis; (2) the proposed approach achieves state-of-the-art results on several benchmarking datasets.","keywords_author":["Deep neural network","Natural language processing","Sentiment analysis","Natural language processing","Sentiment analysis","Deep neural network"],"keywords_other":["Sentiment analysis","Divide-and-conquer approach","Convolutional neural network","Sequence modeling","LEVEL","LANGUAGE","NETWORK","Deep neural networks","NAtural language processing","Sentiment classification","Type classifications"],"max_cite":17.0,"pub_year":2017.0,"sources":"['scp', 'wos']","rawkeys":["type classifications","network","sentiment classification","deep neural network","sequence modeling","deep neural networks","level","natural language processing","convolutional neural network","language","sentiment analysis","divide-and-conquer approach"],"tags":["type classifications","sentiment classification","sequence modeling","level","natural language processing","networks","convolutional neural network","language","sentiment analysis","divide-and-conquer approach"]},{"p_id":28520,"title":"Paraphrase identification and semantic text similarity analysis in Arabic news tweets using lexical, syntactic, and semantic features","abstract":"\u00a9 2017 Elsevier Ltd The rapid growth in digital information has raised considerable challenges in particular when it comes to automated content analysis. Social media such as twitter share a lot of its users\u2019 information about their events, opinions, personalities, etc. Paraphrase Identification (PI) is concerned with recognizing whether two texts have the same\/similar meaning, whereas the Semantic Text Similarity (STS) is concerned with the degree of that similarity. This research proposes a state-of-the-art approach for paraphrase identification and semantic text similarity analysis in Arabic news tweets. The approach adopts several phases of text processing, features extraction and text classification. Lexical, syntactic, and semantic features are extracted to overcome the weakness and limitations of the current technologies in solving these tasks for the Arabic language. Maximum Entropy (MaxEnt) and Support Vector Regression (SVR) classifiers are trained using these features and are evaluated using a dataset prepared for this research. The experimentation results show that the approach achieves good results in comparison to the baseline results.","keywords_author":["Arabic language","Natural language processing","Paraphrase identification","Semantic analysis","Semantic text similarity"],"keywords_other":["Text similarity","Paraphrase identifications","Semantic analysis","Arabic languages","NAtural language processing"],"max_cite":5.0,"pub_year":2017.0,"sources":"['scp']","rawkeys":["paraphrase identification","arabic language","natural language processing","semantic analysis","semantic text similarity","paraphrase identifications","text similarity","arabic languages"],"tags":["natural language processing","semantic text similarity","semantic analysis","paraphrase identifications","text similarity","arabic languages"]},{"p_id":34669,"title":"Extracting Information from Electronic Medical Records to Identify the Obesity Status of a Patient Based on Comorbidities and Bodyweight Measures","abstract":"\u00a9 2016, Springer Science+Business Media New York.Obesity is a chronic disease with an increasing impact on the world\u2019s population. In this work, we present a method of identifying obesity automatically using text mining techniques and information related to body weight measures and obesity comorbidities. We used a dataset of 3015 de-identified medical records that contain labels for two classification problems. The first classification problem distinguishes between obesity, overweight, normal weight, and underweight. The second classification problem differentiates between obesity types: super obesity, morbid obesity, severe obesity and moderate obesity. We used a Bag of Words approach to represent the records together with unigram and bigram representations of the features. We implemented two approaches: a hierarchical method and a nonhierarchical one. We used Support Vector Machine and Na\u00efve Bayes together with ten-fold cross validation to evaluate and compare performances. Our results indicate that the hierarchical approach does not work as well as the nonhierarchical one. In general, our results show that Support Vector Machine obtains better performances than Na\u00efve Bayes for both classification problems. We also observed that bigram representation improves performance compared with unigram representation.","keywords_author":["Comorbidities","Machine learning","Natural language processing","Obesity"],"keywords_other":["Electronic Health Records","Bayes Theorem","Humans","Natural Language Processing","Obesity","Comorbidity","Artificial Intelligence","Support Vector Machine","Data Mining","Overweight"],"max_cite":2.0,"pub_year":2016.0,"sources":"['scp']","rawkeys":["obesity","artificial intelligence","comorbidity","data mining","overweight","machine learning","electronic health records","humans","natural language processing","comorbidities","support vector machine","bayes theorem"],"tags":["obesity","comorbidity","data mining","overweight","machine learning","electronic health records","humans","natural language processing","bayes theorem"]},{"p_id":12144,"title":"Segment convolutional neural networks (Seg-CNNs) for classifying relations in clinical notes","abstract":"We propose Segment Convolutional Neural Networks (Seg-CNNs) for classifying relations from clinical notes. Seg-CNNs use only word-embedding features without manual feature engineering. Unlike typical CNN models, relations between 2 concepts are identified by simultaneously learning separate representations for text segments in a sentence: preceding, concept(1), middle, concept(2), and succeeding. We evaluate Seg-CNN on the i2b2\/VA relation classification challenge dataset. We show that Seg-CNN achieves a state-of-the-art micro-average F-measure of 0.742 for overall evaluation, 0.686 for classifying medical problem-treatment relations, 0.820 for medical problem-test relations, and 0.702 for medical problem-medical problem relations. We demonstrate the benefits of learning segment-level representations. We show that medical domain word embeddings help improve relation classification. Seg-CNNs can be trained quickly for the i2b2\/VA dataset on a graphics processing unit (GPU) platform. These results support the use of CNNs computed over segments of text for classifying medical relations, as they show state-of-the-art performance while requiring no manual feature engineering.","keywords_author":["natural language processing","medical relation classification","convolutional neural network","machine learning"],"keywords_other":["OF-THE-ART","ADVERSE DRUG-REACTIONS","TEXT","KNOWLEDGE","EXTRACTION","SEMANTICS","DESIDERATA"],"max_cite":0.0,"pub_year":2018.0,"sources":"['wos']","rawkeys":["knowledge","desiderata","semantics","machine learning","natural language processing","of-the-art","text","medical relation classification","convolutional neural network","adverse drug-reactions","extraction"],"tags":["knowledge","desiderata","semantics","machine learning","natural language processing","of-the-art","text","medical relation classification","adverse drug reactions","convolutional neural network","extraction"]},{"p_id":44920,"title":"LAWBO: A smart lawyer chatbot","abstract":"\u00a9 2018 ACM. When it comes to conversing and understanding like humans, one of the most intricate domains for chatbots is the judicial system. One needs to really pour into volumes of legal books and judgment papers to analyze and investigate a case. \u201cJustice delayed is justice denied!\u201d. With time being a big constraint, LAWBO could draw parallelism between cases and guide the lawyers by giving relevant information for the given queries. We use a combination of heuristics applied on data extracted from Supreme Court judgments using in-house developed, state-of-the-art parsers, dynamic memory networks (DMN) and GloVe word representation for Natural Language Processing (NLP).","keywords_author":["Artificial Intelligence","Chatbot","Deep Learning","Dynamic Memory Network","Natural Language Processing","Smart Lawyer"],"keywords_other":["Supreme Court","Chatbot","Chatbots","Word representations","Judicial systems","State of the art","Smart Lawyer","Dynamic memory"],"max_cite":0.0,"pub_year":2018.0,"sources":"['scp']","rawkeys":["dynamic memory network","artificial intelligence","smart lawyer","deep learning","chatbot","natural language processing","judicial systems","state of the art","supreme court","word representations","chatbots","dynamic memory"],"tags":["dynamic memory network","dynamic memory","smart lawyer","chatbot","machine learning","natural language processing","state of the art","supreme court","word representations","judicial systems"]},{"p_id":1914,"title":"Detection of attributes for Bengali phoneme in continuous speech using deep neural network","abstract":"Hidden Markov Model (HMM) has contributed greatly in the area of speech recognition during last two decades. However, in recent days, detection-based, bottom-up speech recognition techniques achieve high success rate. In this detection-based, bottom-up approach of speech recognition, first step is detection of speech attributes like place and manner of articulation of the phonemes. This paper describes about the detection of attributes which leads to identification of place and manner of articulation of Bengali phonemes using Deep Neural Network (DNN).","keywords_author":["speech attribute","place of articulation","manner of articulation","deep neural network","deep belief network"],"keywords_other":["speech recognition","DNN","detection-based bottom-up speech recognition techniques","Speech","Dentistry","speech attribute detection","hidden Markov models","Training","HMM","hidden Markov model","Hidden Markov models","Bengali phoneme","continuous speech","neural nets","Detectors","deep neural network","natural language processing","Accuracy","Speech recognition"],"max_cite":4.0,"pub_year":2015.0,"sources":"['ieee']","rawkeys":["speech recognition","hmm","hidden markov model","detection-based bottom-up speech recognition techniques","manner of articulation","dnn","deep belief network","speech","speech attribute detection","bengali phoneme","training","dentistry","continuous speech","neural nets","accuracy","hidden markov models","detectors","deep neural network","natural language processing","speech attribute","place of articulation"],"tags":["accuracy","hidden markov models","bengali phoneme","detectors","neural networks","training","natural language processing","speech recognition","speech attribute","manner of articulation","dentistry","convolutional neural network","speech","continuous speech","detection-based bottom-up speech recognition techniques","place of articulation","speech attribute detection","deep belief networks"]},{"p_id":1917,"title":"Multitask Learning of Deep Neural Networks for Low-Resource Speech Recognition","abstract":"We propose a multitask learning (MTL) approach to improve low-resource automatic speech recognition using deep neural networks (DNNs) without requiring additional language resources. We first demonstrate that the performance of the phone models of a single low-resource language can be improved by training its grapheme models in parallel under the MTL framework. If multiple low-resource languages are trained together, we investigate learning a set of universal phones (UPS) as an additional task again in the MTL framework to improve the performance of the phone models of all the involved languages. In both cases, the heuristic guideline is to select a task that may exploit extra information from the training data of the primary task(s). In the first method, the extra information is the phone-to-grapheme mappings, whereas in the second method, the UPS helps to implicitly map the phones of the multiple languages among each other. In a series of experiments using three low-resource South African languages in the Lwazi corpus, the proposed MTL methods obtain significant word recognition gains when compared with single-task learning (STL) of the corresponding DNNs or ROVER that combines results from several STL-trained DNNs.","keywords_author":["Deep neural network (DNN)","low-resource speech recognition","multitask learning","universal grapheme set","universal phone set","Deep neural network (DNN)","low-resource speech recognition","multitask learning","universal grapheme set","universal phone set"],"keywords_other":["Lwazi corpus","MTL approach","speech recognition","single low-resource language","Neural networks","DNN","universal phones","Uninterruptible power systems","phone-to-grapheme mappings","South African languages","Speech","HIDDEN MARKOV-MODELS","learning (artificial intelligence)","Training","low-resource speech recognition","Hidden Markov models","neural nets","Data models","deep neural networks","natural language processing","multitask learning approach","LANGUAGE","Acoustics"],"max_cite":14.0,"pub_year":2015.0,"sources":"['ieee', 'wos']","rawkeys":["south african languages","speech recognition","single low-resource language","acoustics","language","universal phones","hidden markov-models","universal phone set","mtl approach","phone-to-grapheme mappings","multitask learning","dnn","speech","deep neural network (dnn)","learning (artificial intelligence)","neural networks","training","universal grapheme set","low-resource speech recognition","lwazi corpus","neural nets","hidden markov models","deep neural networks","natural language processing","multitask learning approach","data models","uninterruptible power systems"],"tags":["south african languages","speech recognition","single low-resource language","convolutional neural network","acoustics","language","universal phones","universal phone set","mtl approach","phone-to-grapheme mappings","machine learning","multitask learning","speech","neural networks","training","universal grapheme set","low-resource speech recognition","lwazi corpus","hidden markov models","natural language processing","multitask learning approach","data models","uninterruptible power systems"]},{"p_id":49025,"title":"A deep learning approach to contract element extraction","abstract":"\u00a9 2017 The authors and IOS Press. We explore how deep learning methods can be used for contract element extraction. We show that a BILSTM operating on word, POS tag, and token-shape embeddings outperforms the linear sliding-window classifiers of our previous work, without any manually written rules. Further improvements are observed by stacking an additional LSTM on top of the BILSTM, or by adding a CRF layer on top of the BILSTM. The stacked BILSTM-LSTM misclassifies fewer tokens, but the BILSTM-CRF combination performs better when methods are evaluated for their ability to extract entire, possibly multi-token contract elements.","keywords_author":["Deep learning","Legal text analytics","Natural language processing"],"keywords_other":["Element extraction","Learning methods","Sliding Window","Learning approach","Embeddings","Legal texts","Multi tokens"],"max_cite":0.0,"pub_year":2017.0,"sources":"['scp']","rawkeys":["embeddings","sliding window","element extraction","deep learning","learning methods","natural language processing","legal text analytics","multi tokens","legal texts","learning approach"],"tags":["embeddings","sliding window","element extraction","learning methods","machine learning","legal text analytics","natural language processing","multi tokens","legal texts","learning approach"]},{"p_id":42882,"title":"Automated human capital management system","abstract":"\u00a9 2018 IEEE. Finding a meaningful and fulfilling work and finding right talent for a given job is a challenging and classical Human Capital Management (HCM) problem. For a long time, HCM was carried out in an offline manner requiring human interventions. As scale of organizations have grown, it becomes relevant to automate these processes. In this paper, we present and detail a stateless scalable architecture for an automated HCM system. For clustering and categorizing job postings and candidate profiles we present algorithms that uses Machine Learning techniques. We present an algorithm that uses Natural Language Processing for feature extraction. We also present a ranking algorithm that uses semantic web technologies for providing accurate recommendations. We conclude this paper presenting results of presented algorithms on real-world job posting and candidate data.","keywords_author":["Automated HCM","Fuzzy String Matching","Hiring Pattern Prediction","Machine Learning","Natural Language Processing","Semantic Web"],"keywords_other":null,"max_cite":0.0,"pub_year":2018.0,"sources":"['scp']","rawkeys":["natural language processing","machine learning","hiring pattern prediction","fuzzy string matching","automated hcm","semantic web"],"tags":["natural language processing","machine learning","hiring pattern prediction","fuzzy string matching","automated hcm","semantic web"]},{"p_id":34696,"title":"Feature selection for twitter classification","abstract":"Twitter-based messages have presented challenges in the identification of features as applied to classification. This paper explores filtering techniques for improved trend detection and information extraction. Starting with a pre-filtered source (Twitter), we will examine the application of both information theory and Natural Language Processing (NLP) based techniques as a means of preprocessing for classification. Results demonstrate that both means allow for improved results in classification among highly idiosyncratic data (Twitter). \u00a9 2014 IEEE.","keywords_author":["Classification","Machine learning","Natural Language Processing"],"keywords_other":["Filtering technique","Trend detection","NAtural language processing"],"max_cite":2.0,"pub_year":2014.0,"sources":"['scp']","rawkeys":["natural language processing","machine learning","filtering technique","classification","trend detection"],"tags":["natural language processing","machine learning","filtering technique","classification","trend detection"]},{"p_id":16268,"title":"Prospective recruitment of patients with congestive heart failure using an ad-hoc binary classifier","abstract":"This paper addresses a very specific problem of identifying patients diagnosed with a specific condition for potential recruitment in a clinical trial or an epidemiological study. We present a simple machine learning method for identifying patients diagnosed with congestive heart failure and other related conditions by automatically classifying clinical notes dictated at Mayo Clinic. This method relies on an automatic classifier trained on comparable amounts of positive and negative samples of clinical notes previously categorized by human experts. The documents are represented as feature vectors, where features are a mix of demographic information as well as single words and concept mappings to MeSH and HICDA classification systems. We compare two simple and efficient classification algorithms (Na\u00efve Bayes and Perceptron) and a baseline term spotting method with respect to their accuracy and recall on positive samples. Depending on the test set, we find that Na\u00efve Bayes yields better recall on positive samples (95 vs. 86%) but worse accuracy than Perceptron (57 vs. 65%). Both algorithms perform better than the baseline with recall on positive samples of 71% and accuracy of 54%. \u00a9 2004 Elsevier Inc. All rights reserved.","keywords_author":["Automatic classification","Congestive heart failure","Machine learning","Medical informatics","Na\u00efve Bayes","Natural language processing","Perceptron"],"keywords_other":["Demographic information","Binary classifiers","Heart failure"],"max_cite":38.0,"pub_year":2005.0,"sources":"['scp']","rawkeys":["na\u00efve bayes","automatic classification","congestive heart failure","machine learning","natural language processing","perceptron","medical informatics","binary classifiers","heart failure","demographic information"],"tags":["automatic classification","machine learning","natural language processing","perceptron","medical informatics","congestive heart failures","binary classifiers","naive bayes","heart failure","demographic information"]},{"p_id":1938,"title":"Deep learning framework with confused sub-set resolution architecture for automatic arabic diacritization","abstract":"The Arabic language belongs to a group of languages that require diacritization over their characters. Modern Standard Arabic (MSA) transcripts omit the diacritics, which are essential for many machine learning tasks like Text-To-Speech (TTS) systems. In this work Arabic diacritics restoration is tackled under a deep learning framework that includes the Confused Sub-set Resolution (CSR) method to improve the classification accuracy, in addition to an Arabic Part-of-Speech (PoS) tagging framework using deep neural nets. Special focus is given to syntactic diacritization, which still suffers low accuracy as indicated in prior works. Evaluation is done versus state-of-the-art systems reported in literature, with quite challenging datasets collected from different domains. Standard datasets like the LDC Arabic Tree Bank are used in addition to custom ones we have made available online to allow other researchers to replicate these results. Results show significant improvement of the proposed techniques over other approaches, reducing the syntactic classification error to 9.9% and morphological classification error to 3% compared to 12.7% and 3.8% of the best reported results in literature, improving the error by 22% over the best reported systems.","keywords_author":["Arabic diacritization","classifier design","deep networks","part-of-speech (PoS) tagging","Arabic diacritization","classifier design","deep networks","part-of-speech (PoS) tagging","Arabic diacritization","classifier design","deep networks","part-of-speech (PoS) tagging"],"keywords_other":["deep neural nets","TTS system","Diacritics restorations","State-of-the-art system","Syntactics","confused subset resolution architecture","Feature extraction","Arabic part-of-speech tagging","machine learning","Context","text-to-speech system","syntactic diacritization","Morphological classifications","PoS tagging","learning (artificial intelligence)","deep learning","Training","Classifier design","modern standard Arabic transcript","Arabic diacritics restoration","Vectors","Text-to-speech system","Classification errors","neural nets","Arabic diacritization","Part of speech tagging","automatic Arabic diacritization","natural language processing","Accuracy","Standards","speech synthesis","Arabic language","Classification accuracy"],"max_cite":9.0,"pub_year":2015.0,"sources":"['wos', 'scp', 'ieee']","rawkeys":["deep neural nets","arabic part-of-speech tagging","modern standard arabic transcript","arabic diacritization","part-of-speech (pos) tagging","confused subset resolution architecture","arabic language","machine learning","vectors","arabic diacritics restoration","classification errors","syntactics","text-to-speech system","syntactic diacritization","automatic arabic diacritization","learning (artificial intelligence)","deep learning","training","state-of-the-art system","morphological classifications","neural nets","pos tagging","accuracy","standards","classification accuracy","natural language processing","part of speech tagging","speech synthesis","tts system","diacritics restorations","feature extraction","context","deep networks","classifier design"],"tags":["deep neural nets","arabic diacritic restoration","arabic part-of-speech tagging","modern standard arabic transcript","arabic diacritization","confused subset resolution architecture","machine learning","tts systems","vectors","classification errors","syntactics","text-to-speech system","arabic languages","syntactic diacritization","automatic arabic diacritization","neural networks","training","state-of-the-art system","morphological classifications","pos tagging","accuracy","standards","classification accuracy","natural language processing","part of speech tagging","speech synthesis","diacritics restorations","feature extraction","context","deep networks","classifier design"]},{"p_id":22419,"title":"Predicting patient acuity from electronic patient records","abstract":"\u00a9 2014 Elsevier Inc. The ability to predict acuity (patients' care needs), would provide a powerful tool for health care managers to allocate resources. Such estimations and predictions for the care process can be produced from the vast amounts of healthcare data using information technology and computational intelligence techniques. Tactical decision-making and resource allocation may also be supported with different mathematical optimization models. Methods: This study was conducted with a data set comprising electronic nursing narratives and the associated Oulu Patient Classification (OPCq) acuity. A mathematical model for the automated assignment of patient acuity scores was utilized and evaluated with the pre-processed data from 23,528 electronic patient records. The methods to predict patient's acuity were based on linguistic pre-processing, vector-space text modeling, and regularized least-squares regression. Results: The experimental results show that it is possible to obtain accurate predictions about patient acuity scores for the coming day based on the assigned scores and nursing notes from the previous day. Making same-day predictions leads to even better results, as access to the nursing notes for the same day boosts the predictive performance. Furthermore, textual nursing notes allow for more accurate predictions than previous acuity scores. The best results are achieved by combining both of these information sources. The developed model achieves a concordance index of 0.821 when predicting the patient acuity scores for the following day, given the scores and text recorded on the previous day. Conclusions: By applying language technology to electronic patient documents it is possible to accurately predict the value of the acuity scores of the coming day based on the previous da\u00fds assigned scores and nursing notes.","keywords_author":["Electronic patient record","Machine learning","Patient acuity","Patient classification system"],"keywords_other":["Algorithms","Electronic Health Records","Classification system","Finland","Nursing Assessment","Patient acuity","Natural Language Processing","Patient Acuity","Health Records, Personal","Artificial Intelligence","Electronic patient record","Nursing Records","Computer Simulation","Data Interpretation, Statistical","Models, Statistical"],"max_cite":12.0,"pub_year":2014.0,"sources":"['scp']","rawkeys":["artificial intelligence","classification system","statistical","patient classification system","computer simulation","health records","machine learning","data interpretation","electronic health records","electronic patient record","finland","natural language processing","nursing records","nursing assessment","patient acuity","models","personal","algorithms"],"tags":["statistics","classification system","model","patient classification system","computer simulation","health records","machine learning","data interpretation","electronic health records","electronic patient record","finland","natural language processing","nursing records","nursing assessment","patient acuity","algorithms","personalizations"]},{"p_id":1939,"title":"Improving acoustic model for English ASR System using deep neural network","abstract":"In this paper, a method based on deep learning is applied to improve acoustic model for English Automatic Speech Recognition (ASR) system using two main approaches of deep neural network (Hybrid and bottleneck feature). Deep neural networks systems are able to achieve significant improvements over a number of last year system. The experiments are carried out on the dataset containing speeches on Technology, Entertainment, and Design (TED) Talks. The results show that applying Deep neural network decrease the relative error rate by 33% compared to the MFCC baseline system.","keywords_author":["Deep Bottleneck Features","Hybrid HMM\/GMM","Automatic Speech Recognition"],"keywords_other":["speech recognition","Neural networks","Technology Entertainment and Design Talks","Speech","mixture models","hybrid feature","hidden Markov models","learning (artificial intelligence)","deep learning","Gaussian processes","Training","HMM","bottleneck feature","Hidden Markov models","MFCC baseline system","GMM","English ASR System","neural nets","acoustic model","English automatic speech recognition system","deep neural network","Gaussian mixture model","natural language processing","Speech recognition","Acoustics","TED Talks","Decoding"],"max_cite":null,"pub_year":2015.0,"sources":"['ieee']","rawkeys":["automatic speech recognition","speech recognition","hmm","english automatic speech recognition system","technology entertainment and design talks","acoustics","mixture models","speech","hybrid feature","english asr system","learning (artificial intelligence)","deep learning","neural networks","training","ted talks","bottleneck feature","hybrid hmm\/gmm","neural nets","gaussian mixture model","hidden markov models","acoustic model","deep neural network","mfcc baseline system","natural language processing","gmm","deep bottleneck features","gaussian processes","decoding"],"tags":["automatic speech recognition","speech recognition","convolutional neural network","english automatic speech recognition system","technology entertainment and design talks","acoustics","bottleneck features","machine learning","mixture models","speech","english asr system","hybrid features","deep bottleneck feature","neural networks","training","ted talks","hybrid hmm\/gmm","gaussian mixture model","hidden markov models","acoustic model","mfcc baseline system","natural language processing","gaussian processes","decoding"]},{"p_id":20373,"title":"Phishing detection and impersonated entity discovery using Conditional Random Field and Latent Dirichlet Allocation","abstract":"Phishing is an attempt to steal users' personal and financial information such as passwords, social security and credit card numbers, via electronic communication such as e-mail and other messaging services. Attackers pretend to be froma legitimate organization and direct users to a fake website that resembles a legitimate website, which is then used to collect users' personal information. In this paper, we propose a novel methodology to detect phishing attacks and to discover the entity\/organization that the attackers impersonate during phishing attacks. The proposed multi-stage methodology employs natural language processing and machine learning. The methodology first discovers (i) named entities, which includes names of people, organizations, and locations; and (ii) hidden topics, using (a) Conditional Random Field (CRF) and (b) Latent Dirichlet Allocation (LDA) operating on both phishing and non-phishing data. Utilizing topics and named entities as features, the next stage classifies each message as phishing or non-phishing using AdaBoost. For messages classified as phishing, the final stage discovers the impersonated entity using CRF. Experimental results show that the phishing classifier detects phishing attacks with no misclassification when the proportion of phishing emails is less than 20%. The F-measure obtained was 100%. Our approach also discovers the impersonated entity from messages that are classified as phishing, with a discovery rate of 88.1%. The automatic discovery of impersonated entity from phishing helps the legitimate organization to take down the offending phishing site. This protects their users from falling for phishing attacks, which in turn leads to satisfied customers. Automatic discovery of an impersonated entity also helps email service providers to collaborate with each other to exchange attack information and protect their customers. \u00a9 2012 Elsevier Ltd. All rights reserved.","keywords_author":["Boosting","Conditional Random Field","Identity theft","Impersonated entity discovery","Latent Dirichlet Allocation","Machine learning","Named entity","Natural language processing","Phishing"],"keywords_other":["Named entities","Phishing","Identity theft","Impersonated entity discovery","Conditional random field","NAtural language processing","Boosting","Latent Dirichlet allocation"],"max_cite":17.0,"pub_year":2013.0,"sources":"['scp']","rawkeys":["named entity","latent dirichlet allocation","conditional random field","machine learning","natural language processing","identity theft","named entities","impersonated entity discovery","boosting","phishing"],"tags":["linear discriminant analysis","conditional random field","machine learning","natural language processing","identity theft","named entities","impersonated entity discovery","boosting","phishing"]},{"p_id":1942,"title":"Unsupervised Generation of Context-Relevant Training-Sets for Visual Object Recognition Employing Multilinguality","abstract":"Image based object classification requires clean training data sets. Gathering such sets is usually done manually by humans, which is time-consuming and laborious. On the other hand, directly using images from search engines creates very noisy data due to ambiguous noun-focused indexing. However, in daily speech nouns and verbs are always coupled. We use this for the automatic generation of clean data sets by the here-presented TRANSCLEAN algorithm, which through the use of multiple languages also solves the problem of polyesters (a single spelling with multiple meanings). Thus, we use the implicit knowledge contained in verbs, e.g. in an imperative such as \"hit the nail\", implicating a metal nail and not the fingernail. One type of reference application where this method can automatically operate is human-robot collaboration based on discourse. A second is the generation of clean image data sets, where tedious manual cleaning can be replaced by the much simpler manual generation of a single relevant verb-noun tuple. Here we show the impact of our improved training sets for several widely used and state-of-the-art classifiers including Multipath Hierarchical Matching Pursuit. All tested classifiers show a substantial boost of about +20% in recognition performance.","keywords_author":null,"keywords_other":["indexing","Fasteners","human-robot collaboration","unsupervised context-relevant training-set generation","hit the nail","Google","visual object recognition","image classification","multipath hierarchical matching pursuit","clean image data sets","iterative methods","Search engines","Context","TRANSCLEAN algorithm","image based object classification","verb-noun tuple","noun-focused indexing","Training","metal nail","Clutter","unsupervised learning","automatic clean data set generation","multilinguality","Nails","fingernail","natural language processing","object recognition","search engines","polysemes"],"max_cite":null,"pub_year":2015.0,"sources":"['ieee']","rawkeys":["indexing","human-robot collaboration","hit the nail","unsupervised context-relevant training-set generation","visual object recognition","nails","image classification","google","multipath hierarchical matching pursuit","clean image data sets","iterative methods","fasteners","image based object classification","verb-noun tuple","noun-focused indexing","training","metal nail","transclean algorithm","unsupervised learning","automatic clean data set generation","multilinguality","fingernail","natural language processing","object recognition","search engines","context","clutter","polysemes"],"tags":["human-robot collaboration","hit the nail","unsupervised context-relevant training-set generation","visual object recognition","nails","image classification","google","multipath hierarchical matching pursuit","clean image data sets","index","iterative methods","fasteners","image based object classification","verb-noun tuple","noun-focused indexing","training","metal nail","transclean algorithm","unsupervised learning","automatic clean data set generation","multilingual","search engine","fingernail","natural language processing","object recognition","context","clutter","polysemes"]},{"p_id":44952,"title":"Scarlet - Artificial Teaching Assistant","abstract":"\u00a9 2017 IEEE. Scarlet an Artificial Teaching Assistant is a personal digital assistant that has been developed with main aim to assist students in their learning process by ensuring fast and efficiently search of documents and learning materials. Scarlet is able to give an adequate response to a specific question based on knowledge gathered by an unique algorithm which enables her to recognize context during file and web page content search. After finding the most appropriate answer Scarlet seeks for student feedback in order to improve future search. The metric proposed is based on the power law which occurs in natural language, that is the Zipfian distribution[1]. It is designed to work for any spoken language although it might work on some better than other depending on the nature of the language, the structure, grammar and semantics. The method uses this metric to derive context from data and then queries the data source looking for the best match. The whole implementation is rounded off by a learning module which gives the system a learning curve based on users (students) scoring how relevant the output is among other parameters.","keywords_author":["artificial intelligence","machine learning","natural language processing","pattern recognition"],"keywords_other":["Student feedback","Learning process","Teaching assistants","Spoken languages","Learning materials","Natural languages","Learning modules","Learning curves"],"max_cite":0.0,"pub_year":2018.0,"sources":"['scp']","rawkeys":["artificial intelligence","teaching assistants","spoken languages","learning process","natural languages","machine learning","natural language processing","student feedback","learning modules","learning materials","learning curves","pattern recognition"],"tags":["teaching assistants","spoken languages","learning process","natural languages","machine learning","natural language processing","student feedback","learning-curve","learning modules","learning materials","pattern recognition"]},{"p_id":22425,"title":"Exploring the Application of Deep Learning Techniques on Medical Text Corpora","abstract":"\u00a9 2014 European Federation for Medical Informatics and IOS Press.With the rapidly growing amount of biomedical literature it becomes increasingly difficult to find relevant information quickly and reliably. In this study we applied the word2vec deep learning toolkit to medical corpora to test its potential for improving the accessibility of medical knowledge. We evaluated the efficiency of word2vec in identifying properties of pharmaceuticals based on mid-sized, unstructured medical text corpora without any additional background knowledge. Properties included relationships to diseases ('may treat') or physiological processes ('has physiological effect'). We evaluated the relationships identified by word2vec through comparison with the National Drug File-Reference Terminology (NDF-RT) ontology. The results of our first evaluation were mixed, but helped us identify further avenues for employing deep learning technologies in medical information retrieval, as well as using them to complement curated knowledge captured in ontologies and taxonomies.","keywords_author":["biomedical literature","Deep learning","evaluation system"],"keywords_other":["Algorithms","Pattern Recognition, Automated","Semantics","Natural Language Processing","Vocabulary, Controlled","Artificial Intelligence","Software","Manuscripts, Medical"],"max_cite":12.0,"pub_year":2014.0,"sources":"['scp', 'wos']","rawkeys":["artificial intelligence","vocabulary","evaluation system","automated","deep learning","semantics","natural language processing","software","manuscripts","controlled","algorithms","pattern recognition","medical","biomedical literature"],"tags":["vocabulary","evaluation system","automated","semantics","control","machine learning","natural language processing","software","manuscripts","algorithms","pattern recognition","medical","biomedical literature"]},{"p_id":10138,"title":"Active deep learning method for semi-supervised sentiment classification","abstract":"In natural language processing community, sentiment classification based on insufficient labeled data is a well-known challenging problem. In this paper, a novel semi-supervised learning algorithm called active deep network (ADN) is proposed to address this problem. First, we propose the semi-supervised learning framework of ADN. ADN is constructed by restricted Boltzmann machines (RBM) with unsupervised learning based on labeled reviews and abundant of unlabeled reviews. Then the constructed structure is fine-tuned by gradient-descent based supervised learning with an exponential loss function. Second, in the semi-supervised learning framework, we apply active learning to identify reviews that should be labeled as training data, then using the selected labeled reviews and all unlabeled reviews to train ADN architecture. Moreover, we combine the information density with ADN, and propose information ADN (IADN) method, which can apply the information density of all unlabeled reviews in choosing the manual labeled reviews. Experiments on five sentiment classification datasets show that ADN and IADN outperform classical semi-supervised learning algorithms, and deep learning techniques applied for sentiment classification. (c) 2013 Elsevier B.V. All rights reserved.","keywords_author":["Active learning","Deep learning","Neural networks","Sentiment classification","Neural networks","Deep learning","Active learning","Sentiment classification"],"keywords_other":["Deep learning","Semi-supervised learning","Restricted boltzmann machine","Exponential loss function","Information density","Active Learning","Sentiment classification","NAtural language processing"],"max_cite":54.0,"pub_year":2013.0,"sources":"['wos', 'scp', 'ieee']","rawkeys":["sentiment classification","deep learning","neural networks","natural language processing","exponential loss function","active learning","semi-supervised learning","information density","restricted boltzmann machine"],"tags":["sentiment classification","neural networks","natural language processing","machine learning","exponential loss function","semi-supervised learning","information density","restricted boltzmann machine"]},{"p_id":36762,"title":"Uncovering the relationships between military community health and affects expressed in social media","abstract":"\u00a9 2017, The Author(s). Military populations present a small, unique community whose mental and physical health impacts the security of the nation. Recent literature has explored social media\u2019s ability to enhance disease surveillance and characterize distinct communities with encouraging results. We present a novel analysis of the relationships between influenza-like illnesses (ILI) clinical data and affects (i.e., emotions and sentiments) extracted from social media around military facilities. Our analyses examine (1) differences in affects expressed by military and control populations, (2) affect changes over time by users, (3) differences in affects expressed during high and low ILI seasons, and (4) correlations and cross-correlations between ILI clinical visits and affects from an unprecedented scale - 171M geo-tagged tweets across 31 global geolocations. Key findings include: Military and control populations differ in the way they express affects in social media over space and time. Control populations express more positive and less negative sentiments and less sadness, fear, disgust, and anger emotions than military. However, affects expressed in social media by both populations within the same area correlate similarly with ILI visits to military health facilities. We have identified potential responsible cofactors leading to location variability, e.g., region or state locale, military service type and\/or the ratio of military to civilian populations. For most locations, ILI proportions positively correlate with sadness and neutral sentiment, which are the affects most often expressed during high ILI season. The ILI proportions negatively correlate with fear, disgust, surprise, and positive sentiment. These results are similar to the low ILI season where anger, surprise, and positive sentiment are highest. Finally, cross-correlation analysis shows that most affects lead ILI clinical visits, i.e. are predictive of ILI data, with affect-ILI leading intervals dependent on geolocation and affect type. Overall, information gained in this study exemplifies a usage of social media data to understand the correlation between psychological behavior and health in the military population and the potential for use of social media affects for prediction of ILI cases.","keywords_author":["biosurveillance","emotion detection","influenza","machine learning","natural language processing","sentiment analysis","social media analytics"],"keywords_other":["Sentiment analysis","influenza","Social media analytics","Biosurveillance","Emotion detection"],"max_cite":1.0,"pub_year":2017.0,"sources":"['scp']","rawkeys":["biosurveillance","natural language processing","machine learning","influenza","emotion detection","social media analytics","sentiment analysis"],"tags":["biosurveillance","natural language processing","machine learning","influenza","emotion detection","social media analytics","sentiment analysis"]},{"p_id":53145,"title":"A special parser for learning english composition - error analysis & learners' model for ILTS","abstract":"By embedding the function of automatically correcting nearly free format English translations of given Japanese sentences, we have developed a new ILTS(intelligent language tutoring system) for improving English Writing Skills in a L2 tutoring environment. In addition to a diagnostic engine capable of identifying grammatical, lexical and word usage errors of students' input translations and returning error contingent feedback, we have developed a simple table lookup parser for displying the grammar trees. The table look-up parser parses a user input, which is always erronous, by simply matching the extended part-of-speech tag sequence of a closing sentence in a template of ILTS to the entries of a look-up-table, in which each extended part-of-speech tag array corresponds to one grammar tree. The complexity of the table lookup parser is O(n), where n denotes the length of a sentence. This shows a marked improvement over the general purpose parser of which the complexity is O(n3). \u00a9 2009 IEEE.","keywords_author":["Intelligent language learning system","Machine learning","Machine translation","Natural language processing"],"keywords_other":["Machine translation","Learning English","User input","Writing skills","General purpose","Tag arrays","Machine learning","Look-up-table","Extended parts","Tutoring system","Diagnostic engines","Intelligent language learning system","Natural language processing"],"max_cite":0.0,"pub_year":2009.0,"sources":"['scp']","rawkeys":["tag arrays","look-up-table","machine learning","machine translation","learning english","natural language processing","user input","general purpose","tutoring system","intelligent language learning system","writing skills","diagnostic engines","extended parts"],"tags":["tag arrays","machine learning","natural language processing","learning english","user input","general purpose","machine translations","tutoring system","intelligent language learning system","writing skills","diagnostic engines","extended parts","look up table"]},{"p_id":30621,"title":"Choosing the best dictionary for Cross-Lingual Word Sense Disambiguation","abstract":"\u00a9 2015 Elsevier B.V. The choice of the dictionary that provides the possible translations a system has to choose when performing Cross-Lingual Word Sense Disambiguation (CLWSD) is one of the most important steps in such a task. In this work, we present a comparison between different dictionaries, in two different frameworks. First of all, a technique for analysing the potential results of an ideal system using those dictionaries is developed. The second framework considers the particular unsupervised CLWSD system CO-Graph, and analyses the results obtained when using different bilingual dictionaries providing the potential translations. Two different CLWSD tasks from the 2010 and 2013 SemEval competitions are used for evaluation, and statistics from the words in the test datasets of those competitions are studied. The conclusions of the analysis of dictionaries on a particular system lead us to a proposal that substantially improves the results obtained in that framework. In this proposal a hybrid system is developed, by combining the results provided by a probabilistic dictionary, and those obtained with a Most Frequent Sense (MFS) approach. The hybrid approach also outperforms the results obtained by other unsupervised systems in the considered competitions.","keywords_author":["Bilingual dictionary","Disambiguation","Natural language processing","Parallel corpus","Sense","Unsupervised systems","Word"],"keywords_other":["NAtural language processing","Bilingual dictionary","Word Sense Disambiguation","Ideal systems","Cross-lingual","Hybrid approach","Parallel corpora","Co-graphs"],"max_cite":4.0,"pub_year":2015.0,"sources":"['scp']","rawkeys":["cross-lingual","parallel corpora","word","sense","natural language processing","co-graphs","hybrid approach","parallel corpus","word sense disambiguation","disambiguation","ideal systems","bilingual dictionary","unsupervised systems"],"tags":["cross-lingual","parallel corpora","unsupervised systems","natural language processing","co-graphs","hybrid approach","parallel corpus","word sense disambiguation","disambiguation","ideal systems","words","bilingual dictionary","sensing"]},{"p_id":49056,"title":"Using psycholinguistic features for the classification of comprehenders from summary speech transcripts","abstract":"\u00a9 2017, The Author(s). In education, some students lack language comprehension, language production and language acquisition skills. In this paper we extracted several psycholinguistics features broadly grouped into lexical and morphological complexity, syntactic complexity, production units, syntactic pattern density, referential cohesion, connectives, amounts of coordination, amounts of subordination, LSA, word information, and readability from students\u2019 summary speech transcripts. Using these Coh-Metrix features, comprehenders are classified into two groups: poor comprehender and proficient comprehender. It is concluded that a computational model can be implemented using a reduced set of features and the results can be used to help poor reading comprehenders for improving their cognitive reading skills.","keywords_author":["Machine learning classification","Natural language processing","Psycholinguistics"],"keywords_other":["Language production","Computational model","Language comprehensions","Language acquisition","Psycholinguistics","Morphological complexity","Syntactic complexity","Machine learning classification"],"max_cite":0.0,"pub_year":2017.0,"sources":"['scp']","rawkeys":["psycholinguistics","language production","syntactic complexity","natural language processing","language comprehensions","morphological complexity","language acquisition","computational model","machine learning classification"],"tags":["computational modeling","psycholinguistics","language production","syntactic complexity","natural language processing","language comprehensions","morphological complexity","language acquisition","machine learning classification"]},{"p_id":20385,"title":"Discriminative reranking for spoken language understanding","abstract":"Spoken language understanding (SLU) is concerned with the extraction of meaning structures from spoken utterances. Recent computational approaches to SLU, e.g., conditional random fields (CRFs), optimize local models by encoding several features, mainly based on simple n-grams. In contrast, recent works have shown that the accuracy of CRF can be significantly improved by modeling long-distance dependency features. In this paper, we propose novel approaches to encode all possible dependencies between features and most importantly among parts of the meaning structure, e.g., concepts and their combination. We rerank hypotheses generated by local models, e.g., stochastic finite state transducers (SFSTs) or CRF, with a global model. The latter encodes a very large number of dependencies (in the form of trees or sequences) by applying kernel methods to the space of all meaning (sub) structures. We performed comparative experiments between SFST, CRF, support vector machines (SVMs), and our proposed discriminative reranking models (DRMs) on representative conversational speech corpora in three different languages: the ATIS (English), the MEDIA (French), and the LUNA (Italian) corpora. These corpora have been collected within three different domain applications of increasing complexity: informational, transactional, and problem-solving tasks, respectively. The results show that our DRMs consistently outperform the state-of-the-art models based on CRF. \u00a9 2011 IEEE.","keywords_author":["Kernel methods","machine learning","natural language processing (NLP)","spoken language understanding (SLU)","stochastic language models","support vector machines (SVMs)"],"keywords_other":["Machine-learning","Spoken language understanding","Stochastic language models","Support vector","Kernel methods","Natural language processing"],"max_cite":17.0,"pub_year":2012.0,"sources":"['scp', 'ieee']","rawkeys":["stochastic language models","support vector machines (svms)","spoken language understanding (slu)","support vector","kernel methods","natural language processing","machine learning","natural language processing (nlp)","machine-learning","spoken language understanding"],"tags":["stochastic language models","support vector","natural language processing","machine learning","kernel methods","spoken language understanding"]},{"p_id":38818,"title":"Performance evaluation of information retrieval models in bug localization on the method level","abstract":"\u00a9 2015 IEEE.This study uses statistical inference to compare the performance of three text models used for bug localization in collaboration systems: Vector Space Model (VSM), Latent Semantic Indexing (LSI), and Latent Dirichlet Analysis (LDA) on the method level. After the three models are compared we confirm that VSM is the superior model. We then, point out which external factors i.e. methods lengths, queries lengths, methods documentation comments, products names and components names mentioned in bug reports affect VSM performance. We conclude that VSM performance is positively correlated with most of the tested factors. We believe our results can be helpful to: (i) text models developers, to understand the strengths and limitations of VSM for future development; (ii) bug localization programmers using classical VSM, to understand improved ways to prepare methods extracted from big data collaboration systems and (iii) bug reporters, to follow the most efficient methods presented in this work in reporting bugs to enhance the information retrieval process.","keywords_author":["discovery, collection, and extraction of information in big data sources","machine learning methods applied to big data analytics","natural language processing methodologies","performance and benchmarking for big data processing and analytics","semantic content extraction and analytics languages and techniques","text categorization and topic recognition"],"keywords_other":["Text categorization","Extraction of information","performance and benchmarking for big data processing and analytics","Semantic content","NAtural language processing","Data analytics"],"max_cite":1.0,"pub_year":2015.0,"sources":"['scp']","rawkeys":["extraction of information","semantic content","machine learning methods applied to big data analytics","discovery","data analytics","natural language processing","collection","text categorization and topic recognition","natural language processing methodologies","performance and benchmarking for big data processing and analytics","text categorization","semantic content extraction and analytics languages and techniques","and extraction of information in big data sources"],"tags":["extraction of information","semantic content","machine learning methods applied to big data analytics","discovery","data analytics","natural language processing","collection","text categorization and topic recognition","natural language processing methodologies","performance and benchmarking for big data processing and analytics","text categorization","semantic content extraction and analytics languages and techniques","and extraction of information in big data sources"]},{"p_id":20391,"title":"Extracting contextualized complex biological events with rich graph-based feature sets","abstract":"We describe a system for extracting complex events among genes and proteins from biomedical literature, developed in context of the BioNLP'09 Shared Task on Event Extraction. For each event, the system extracts its text trigger, class, and arguments. In contrast to the approaches prevailing prior to the shared task, events can be arguments of other events, resulting in a nested structure that better captures the underlying biological statements. We divide the task into independent steps which we approach as machine learning problems. We define a wide array of features and in particular make extensive use of dependency parse graphs. A rule-based postprocessing step is used to refine the output in accordance with the restrictions of the extraction task. In the shared task evaluation, the system achieved an F-score of 51.95% on the primary task, the best performance among the participants. Currently, with modifications and improvements described in this article, the system achieves 52.86% F-score on Task 1, the primary task, improving on its original performance. In addition, we extend the system also to Tasks 2 and 3, gaining F-scores of 51.28% and 50.18%, respectively. The system thus addresses the BioNLP'09 Shared Task in its entirety and achieves the best performance on all three subtasks. \u00a9 2011 Wiley Periodicals, Inc.","keywords_author":["biomedical domain","BioNLP'09 Shared Task","event extraction","machine learning","natural language processing"],"keywords_other":["Event extraction","Machine-learning","BioNLP'09 Shared Task","Biomedical domain","NAtural language processing"],"max_cite":17.0,"pub_year":2011.0,"sources":"['scp']","rawkeys":["machine learning","natural language processing","bionlp'09 shared task","biomedical domain","machine-learning","event extraction"],"tags":["natural language processing","machine learning","bionlp'09 shared task","biomedical domain","event extraction"]},{"p_id":44968,"title":"Twitter sentiment analysis of DKI Jakarta's gubernatorial election 2017 with predictive and descriptive approaches","abstract":"\u00a9 2017 IEEE. The main purpose of this research is to analyze sentiment of DKI Jakarta's gubernatorial election 2017 in social media Twitter with predictive and descriptive approaches. The dataset were collected from Twitter by using each candidate's username as the search query. For the predictive approach, machine learning algorithms such as Multinomial Naive Bayes and Support Vector Machine are used to classify the dataset. As for the descriptive approach, time series graphs and wordclouds are used to get the deeper insights of the dataset and find the connection between Twitter's sentiment and the result of the election itself.","keywords_author":["Data Mining","Machine Learning","Natural Language Processing","Sentiment Analysis"],"keywords_other":["Search queries","Social media","Jakarta","Multinomial naive bayes"],"max_cite":0.0,"pub_year":2018.0,"sources":"['scp']","rawkeys":["data mining","social media","machine learning","natural language processing","multinomial naive bayes","jakarta","sentiment analysis","search queries"],"tags":["data mining","social media","machine learning","natural language processing","multinomial naive bayes","jakarta","sentiment analysis","search queries"]},{"p_id":36780,"title":"Reflective Writing About the Utility Value of Science as a Tool for Increasing STEM Motivation and Retention \u2013 Can AI Help Scale Up?","abstract":"\u00a9 2017, International Artificial Intelligence in Education Society.The integration of subject matter learning with reading and writing skills takes place in multiple ways. Students learn to read, interpret, and write texts in the discipline-relevant genres. However, writing can be used not only for the purposes of practice in professional communication, but also as an opportunity to reflect on the learned material. In this paper, we address a writing intervention \u2013 Utility Value (UV) intervention \u2013 that has been shown to be effective for promoting interest and retention in STEM subjects in laboratory studies and field experiments. We conduct a detailed investigation into the potential of natural language processing technology to support evaluation of such writing at scale: We devise a set of features that characterize UV writing across different genres, present common themes, and evaluate UV scoring models using essays on known and new biology topics. The automated UV scoring results are, we believe, promising, especially for the personal essay genre.","keywords_author":["Automated writing evaluation","Intrapersonal factors","Machine learning","Motivation","Natural language processing","Utility value"],"keywords_other":["Intrapersonal factors","Laboratory studies","Professional communication","Utility values","Automated writing evaluation","Natural languages","Field experiment","Reflective writing"],"max_cite":1.0,"pub_year":2017.0,"sources":"['scp']","rawkeys":["utility value","automated writing evaluation","professional communication","reflective writing","natural languages","field experiment","machine learning","intrapersonal factors","natural language processing","utility values","motivation","laboratory studies"],"tags":["automated writing evaluation","professional communication","reflective writing","natural languages","field experiment","machine learning","intrapersonal factors","natural language processing","utility values","motivation","laboratory studies"]},{"p_id":34735,"title":"A machine learning approach to anaphora resolution in Arabic","abstract":"\u00a9 2014 Praise Worthy Prize S.r.l. All rights reserved Anaphora resolution is a commonly studied research area of Natural Language Processing (NLP). It is crucial for many application areas of Natural Language Processing including information extraction, question answering and text summarization. Most of the earlier work done in the field of anaphora resolution is for English and other European languages. Arabic language is not sufficiently studied with respect to anaphora resolution and rarely being subjected to machine learning experiments. In this paper we present a machine learning approach to resolve the pronominal anaphora in Arabic language. In this work we determine the appropriate features to be used in this task. We consider a number of classifier namely naive Bayes, K-nearest neighbors and linear logistic regression are employed as base-classifiers for each of the feature sets. In this paper, an in-depth study has been conducted on different of feature sets for exploiting effective features and investigating their effect on performance of the Anaphora resolution. Finally, a wide range of comparative experiments on Quranic datasets are conducted, The experimental results on the Arabic Quran training corpus demonstrate that the proposed method is feasible for the pronominal anaphora resolution of Arabic.","keywords_author":["Anaphora resolution","Natural language processing","Supervised machine learning"],"keywords_other":null,"max_cite":2.0,"pub_year":2014.0,"sources":"['scp']","rawkeys":["natural language processing","supervised machine learning","anaphora resolution"],"tags":["natural language processing","supervised machine learning","anaphora resolution"]},{"p_id":42928,"title":"A framework for sentiment analysis with opinion mining of hotel reviews","abstract":"\u00a9 2018 IEEE. The rapid increase in mountains of unstructured textual data accompanied by proliferation of tools to analyse them has opened up great opportunities and challenges for text mining research. The automatic labelling of text data is hard because people often express opinions in complex ways that are sometimes difficult to comprehend. The labelling process involves huge amount of efforts and mislabelled datasets usually lead to incorrect decisions. In this paper, we design a framework for sentiment analysis with opinion mining for the case of hotel customer feedback. Most available datasets of hotel reviews are not labelled which presents a lot of works for researchers as far as text data pre-processing task is concerned. Moreover, sentiment datasets are often highly domain sensitive and hard to create because sentiments are feelings such as emotions, attitudes and opinions that are commonly rife with idioms, onomatopoeias, homophones, phonemes, alliterations and acronyms. The proposed framework is termed sentiment polarity that automatically prepares a sentiment dataset for training and testing to extract unbiased opinions of hotel services from reviews. A comparative analysis was established with Na\u00efve Bayes multinomial, sequential minimal optimization, compliment Na\u00efve Bayes and Composite hypercubes on iterated random projections to discover a suitable machine learning algorithm for the classification component of the framework.","keywords_author":["dataset labelling","machine learning algorithm","natural language processing","opinion mining","sentiment analysis","sentiment polarity"],"keywords_other":["Customer feedback","sentiment polarity","Random projections","Sequential minimal optimization","Training and testing","Automatic labelling","Comparative analysis","Hotel services"],"max_cite":0.0,"pub_year":2018.0,"sources":"['scp']","rawkeys":["sentiment polarity","dataset labelling","customer feedback","natural language processing","training and testing","automatic labelling","sequential minimal optimization","random projections","hotel services","comparative analysis","opinion mining","sentiment analysis","machine learning algorithm"],"tags":["sentiment polarity","dataset labelling","customer feedback","machine learning algorithms","natural language processing","training and testing","sequential minimal optimization","random projections","hotel services","comparative analysis","automatic labeling","opinion mining","sentiment analysis"]},{"p_id":42930,"title":"Detecting malicious powershell commands using deep neural networks","abstract":"\u00a9 2018 Association for Computing Machinery. Microsoft's PowerShell is a command-line shell and scripting language that is installed by default on Windows machines. Based on Microsoft's .NET framework, it includes an interface that allows programmers to access operating system services. While PowerShell can be configured by administrators for restricting access and reducing vulnerabilities, these restrictions can be bypassed. Moreover, PowerShell commands can be easily generated dynamically, executed from memory, encoded and obfuscated, thus making the logging and forensic analysis of code executed by PowerShell challenging. For all these reasons, PowerShell is increasingly used by cybercriminals as part of their attacks' tool chain, mainly for downloading malicious contents and for lateral movement. Indeed, a recent comprehensive technical report by Symantec dedicated to PowerShell's abuse by cybercrimials [52] reported on a sharp increase in the number of malicious PowerShell samples they received and in the number of penetration tools and frameworks that use PowerShell. This highlights the urgent need of developing effective methods for detecting malicious PowerShell commands. In this work, we address this challenge by implementing several novel detectors of malicious PowerShell commands and evaluating their performance. We implemented both \u201ctraditional\u201d natural language processing (NLP) based detectors and detectors based on character-level convolutional neural networks (CNNs). Detectors' performance was evaluated using a large real-world dataset. Our evaluation results show that, although our detectors (and especially the traditional NLP-based ones) individually yield high performance, an ensemble detector that combines an NLP-based classifier with a CNN-based classifier provides the best performance, since the latter classifier is able to detect malicious commands that succeed in evading the former. Our analysis of these evasive commands reveals that some obfuscation patterns automatically detected by the CNN classifier are intrinsically difficult to detect using the NLP techniques we applied. Our detectors provide high recall values while maintaining a very low false positive rate, making us cautiously optimistic that they can be of practical value.","keywords_author":["Deep learning","Malware detection","Natural language processing","Neural networks","PowerShell"],"keywords_other":["Malware detection","False positive rates","Evaluation results","PowerShell","Convolutional neural network","Microsoft's .NET Framework","Forensic analysis","Scripting languages"],"max_cite":0.0,"pub_year":2018.0,"sources":"['scp']","rawkeys":["deep learning","neural networks","false positive rates","natural language processing","forensic analysis","evaluation results","scripting languages","convolutional neural network","microsoft's .net framework","powershell","malware detection"],"tags":["neural networks","false positive rates","machine learning","forensic analysis","natural language processing","evaluation results","scripting languages","convolutional neural network","microsoft's .net framework","powershell","malware detection"]},{"p_id":42933,"title":"Mobile social media networks caching with convolutional neural network","abstract":"\u00a9 2018 IEEE. Nowadays, people use mobile social media networks such as Twitter and Facebook to connect with others. In this work, we discuss the problem of context-aware data caching in the heterogeneous small cell networks (HSCNs) to reduce the service delay for the end users. In the data-caching model, there are three types of cache entities, which are edge caching elements (CAEs), small cell base stations (SBSs), and macro cell base stations (MBS). We propose a deep learning model using the convolutional neural network (CNN) to apply sentence analysis on the data and extract information content in the data from end users. We can predict the data that will most likely to be requested by the end users to reduce service latency by caching the data close to the end users by the interest of the end users. We shows the effectiveness of our proposed algorithm by comparing with other approaches in our simulation.","keywords_author":["Caching","Deep learning","HetNet","Mobile social media network","Natural language processing"],"keywords_other":null,"max_cite":0.0,"pub_year":2018.0,"sources":"['scp']","rawkeys":["deep learning","natural language processing","caching","hetnet","mobile social media network"],"tags":["heterogeneous network","natural language processing","machine learning","caching","mobile social media network"]},{"p_id":22454,"title":"Towards generating a patient's timeline: Extracting temporal relationships from clinical notes","abstract":"Clinical records include both coded and free-text fields that interact to reflect complicated patient stories. The information often covers not only the present medical condition and events experienced by the patient, but also refers to relevant events in the past (such as signs, symptoms, tests or treatments). In order to automatically construct a timeline of these events, we first need to extract the temporal relations between pairs of events or time expressions presented in the clinical notes. We designed separate extraction components for different types of temporal relations, utilizing a novel hybrid system that combines machine learning with a graph-based inference mechanism to extract the temporal links. The temporal graph is a directed graph based on parse tree dependencies of the simplified sentences and frequent pattern clues. We generalized the sentences in order to discover patterns that, given the complexities of natural language, might not be directly discoverable in the original sentences. The proposed hybrid system performance reached an F-measure of 0.63, with precision at 0.76 and recall at 0.54 on the 2012 i2b2 Natural Language Processing corpus for the temporal relation (TLink) extraction task, achieving the highest precision and third highest f-measure among participating teams in the TLink track. \u00a9 2013 Elsevier Inc.","keywords_author":["Automatic patient timeline","Clinical text mining","Machine learning","Natural Language Processing","Temporal graph","Temporal relation extraction"],"keywords_other":["Text mining","Temporal relation","Temporal graphs","NAtural language processing","Automatic patient timeline"],"max_cite":12.0,"pub_year":2013.0,"sources":"['scp']","rawkeys":["text mining","automatic patient timeline","natural language processing","machine learning","clinical text mining","temporal graph","temporal relation","temporal graphs","temporal relation extraction"],"tags":["text mining","automatic patient timeline","natural language processing","machine learning","clinical text mining","temporal relation","temporal graphs","temporal relation extraction"]},{"p_id":51127,"title":"A novel framework for semantic discovery of web services using integrated semantic model","abstract":"Semantic web technology plays a very critical role in the automatic web service discovery by assigning formal semantics to the service descriptions. Practically, it is not feasible to explicitly annotate the formal semantics to millions of existing services. Further, in user context, the request formation for services in semantic web is a complex process as it requires the user to be technically aware of the underlying technologies of the web services, discovery frameworks, description languages and implementation details. In this paper, we propose a semantic framework that enables Web service discovery based on the combination of semantic and syntax information contained in the service profiles. This novel approach for automatic discovery of Web services employs measures of semantic relatedness, Natural Language Processing techniques and information retrieval based statistical models to match a user request. Additionally, we present an efficient semantic matching technique to compute the intra service semantic similarity scores which further facilitates semantic ranking of services. The efficiency of the proposed approach has been demonstrated through experimental evaluations which clearly show that high degree of automation can be achieved with high precision.The results have been further authenticated by providing comparisons with other Information Retrieval based methods.","keywords_author":["Machine learning","Measures of Semantic Relatedness","OWL-S","Semantic Web Service Discovery","Text Mining"],"keywords_other":["Text mining","Experimental evaluation","Semantic web service discovery","Description languages","Measures of semantic relatedness","Semantic Web technology","OWL-S","NAtural language processing"],"max_cite":0.0,"pub_year":2015.0,"sources":"['scp']","rawkeys":["text mining","experimental evaluation","measures of semantic relatedness","semantic web technology","natural language processing","machine learning","semantic web service discovery","owl-s","description languages"],"tags":["text mining","experimental evaluation","measures of semantic relatedness","semantic web technology","natural language processing","machine learning","semantic web service discovery","owl-s","description languages"]},{"p_id":38843,"title":"Machine learning for readability of legislative sentences","abstract":"\u00a9 2015 ACM. Improving the readability of legislation is an important and unresolved problem. Recently, researchers have begun to apply legal informatics to this problem. This paper applies machine learning to predict the readability of sentences from legislation and regulations. A corpus of sentences from the United States Code and US Code of Federal Regulations was created. Each sentence was labelled for language difficulty using results from a large-scale crowdsourced study undertaken during 2014. The corpus was used as training and test data for machine learning. The corpus includes a version tagged using the Stanford parser context free grammar and a version tagged using the Stanford dependency grammar parser. The corpus is described and made available to interested researchers. We investigated whether extending natural language features available as input to machine learning improves the accuracy of prediction. Among features evaluated are those from the context free and dependency grammars. Letter and word ngrams were also studied. We found the addition of such features improves accuracy of prediction on legal language. We also undertake a correlation study of natural language features and language difficulty drawing insights as to the characteristics that may make legal language more difficult. These insights, and those from machine learning, enable us to describe a system for reducing legal language difficulty and to identify a number of suggested heuristics for improving the writing of legislation and regulations.","keywords_author":["Corpus linguistics","Legal informatics","Legislative drafting","Machine learning","Natural language processing","Plain language","Readability","Readability metrics","Supervised learning"],"keywords_other":["Legislative drafting","Corpus linguistics","Readability metrics","Legal informatics","Readability","Plain language","NAtural language processing"],"max_cite":1.0,"pub_year":2015.0,"sources":"['scp']","rawkeys":["readability metrics","supervised learning","legal informatics","readability","natural language processing","machine learning","legislative drafting","plain language","corpus linguistics"],"tags":["readability metrics","supervised learning","legal informatics","readability","natural language processing","machine learning","legislative drafting","plain language","corpus linguistics"]},{"p_id":20414,"title":"Linguistic Structure Prediction","abstract":"Copyright \u00a9 2011 by Morgan & Claypool. A major part of natural language processing now depends on the use of text data to build linguistic analyzers. We consider statistical, computational approaches to modeling linguistic structure. We seek to unify across many approaches and many kinds of linguistic structures. Assuming a basic understanding of natural language processing and\/or machine learning, we seek to bridge the gap between the two fields. Approaches to decoding (i.e., carrying out linguistic structure prediction) and supervised and unsupervised learning of models that predict discrete structures as outputs are the focus. We also survey natural language processing problems to which these methods are being applied, and we address related topics in probabilistic inference, optimization, and experimental methodology.","keywords_author":["computational linguistics","decoding","machine learning","natural language processing","probabilistic inference","statistical modeling","structured prediction","supervised learning","unsupervised learning"],"keywords_other":["Statistical modeling","Structured prediction","Supervised and unsupervised learning","Experimental methodology","Linguistic structure","Probabilistic inference","Discrete structure","Computational approach"],"max_cite":17.0,"pub_year":2011.0,"sources":"['scp', 'wos']","rawkeys":["supervised learning","linguistic structure","statistical modeling","machine learning","natural language processing","unsupervised learning","computational linguistics","computational approach","probabilistic inference","decoding","experimental methodology","structured prediction","discrete structure","supervised and unsupervised learning"],"tags":["supervised learning","structure prediction","linguistic structure","statistical models","machine learning","natural language processing","unsupervised learning","computational linguistics","computational approach","probabilistic inference","decoding","experimental methodology","discrete structure","supervised and unsupervised learning"]},{"p_id":16319,"title":"Structured learning with constrained conditional models","abstract":"Making complex decisions in real world problems often involves assigning values to sets of interdependent variables where an expressive dependency structure among these can influence, or even dictate, what assignments are possible. Commonly used models typically ignore expressive dependencies since the traditional way of incorporating non-local dependencies is inefficient and hence leads to expensive training and inference. The contribution of this paper is two-fold. First, this paper presents Constrained Conditional Models (CCMs), a framework that augments linear models with declarative constraints as a way to support decisions in an expressive output space while maintaining modularity and tractability of training. The paper develops, analyzes and compares novel algorithms for CCMs based on HiddenMarkovModels and Structured Perceptron. The proposed CCM framework is also compared to task-tailored models, such as semi-CRFs. Second, we propose CoDL, a constraint-driven learning algorithm, which makes use of constraints to guide semi-supervised learning.We provide theoretical justification for CoDL along with empirical results which show the advantage of using declarative constraints in the context of semi-supervised training of probabilistic models. \u00a9 The Author(s) 2012.","keywords_author":["Information extraction","Natural language processing","Semi-supervised learning"],"keywords_other":["Conditional models","Semi-supervised","Novel algorithm","Real-world problem","Information Extraction","Complex decision","Perceptron","Structured learning","Probabilistic models","Nonlocal","Semi-supervised learning","Dependency structures","NAtural language processing"],"max_cite":37.0,"pub_year":2012.0,"sources":"['scp']","rawkeys":["probabilistic models","real-world problem","structured learning","information extraction","novel algorithm","complex decision","natural language processing","semi-supervised","perceptron","dependency structures","semi-supervised learning","conditional models","nonlocal"],"tags":["probabilistic models","real-world problem","information extraction","novel algorithm","complex decision","natural language processing","semi-supervised","perceptron","dependency structures","semi-supervised learning","conditional models","nonlocal","structure-learning"]},{"p_id":34751,"title":"Machine learning for knowledge extraction from PHR big data","abstract":"Cloud computing, Internet of things (IOT) and NoSQL database technologies can support a new generation of cloud-based PHR services that contain heterogeneous (unstructured, semi-structured and structured) patient data (health, social and lifestyle) from various sources, including automatically transmitted data from Internet connected devices of patient living space (e.g. medical devices connected to patients at home care). The patient data stored in such PHR systems constitute big data whose analysis with the use of appropriate machine learning algorithms is expected to improve diagnosis and treatment accuracy, to cut healthcare costs and, hence, to improve the overall quality and efficiency of healthcare provided. This paper describes a health data analytics engine which uses machine learning algorithms for analyzing cloud based PHR big health data towards knowledge extraction to support better healthcare delivery as regards disease diagnosis and prognosis. This engine comprises of the data preparation, the model generation and the data analysis modules and runs on the cloud taking advantage from the map\/reduce paradigm provided by Apache Hadoop. \u00a9 2014 The authors and IOS Press. All rights reserved.","keywords_author":["Big data","Cloud computing","Knowledge extraction","Machine learning","PHR"],"keywords_other":["Electronic Health Records","Pattern Recognition, Automated","Natural Language Processing","Knowledge Management","Machine Learning","Health Records, Personal","Data Mining","Cloud Computing","Datasets as Topic"],"max_cite":2.0,"pub_year":2014.0,"sources":"['scp']","rawkeys":["cloud computing","data mining","automated","big data","phr","health records","machine learning","electronic health records","natural language processing","personal","knowledge extraction","knowledge management","pattern recognition","datasets as topic"],"tags":["cloud computing","data mining","automated","big data","phr","health records","machine learning","electronic health records","natural language processing","knowledge extraction","knowledge management","pattern recognition","personalizations","datasets as topic"]},{"p_id":26564,"title":"Identification of subjects with polycystic ovary syndrome using electronic health records","abstract":"\u00a9 2015 Castro et al. Background: Polycystic ovary syndrome (PCOS) is a heterogeneous disorder because of the variable criteria used for diagnosis. Therefore, International Classification of Diseases 9 (ICD-9) codes may not accurately capture the diagnostic criteria necessary for large scale PCOS identification. We hypothesized that use of electronic medical records text and data would more specifically capture PCOS subjects. Methods: Subjects with PCOS were identified in the Partners Healthcare Research Patients Data Registry by searching for the term \"polycystic ovary syndrome\" using natural language processing (n=24,930). A training subset of 199 identified charts was reviewed and categorized based on likelihood of a true Rotterdam PCOS diagnosis, i.e. two out of three of the following: irregular menstrual cycles, hyperandrogenism and\/or polycystic ovary morphology. Data from the history, physical exam, laboratory and radiology results were codified and extracted from notes of definite PCOS subjects. Thirty-two terms were used to build an algorithm for identifying definite PCOS cases and applied to the rest of the dataset. The positive predictive value cutoff was set at 76.8 % to maximize the number of subjects available for study. A true positive predictive value for the algorithm was calculated after review of 100 charts from subjects identified as definite PCOS cases with at least two documented Rotterdam criteria. The positive predictive value was compared to that calculated using 200 charts identified using the ICD-9 code for PCOS (256.4; n=13,670). In addition, a cohort of previously recruited PCOS subjects was submitted for algorithm validation. Results: Chart review demonstrated that 64 % were confirmed as definitely PCOS using the algorithm, with a 9 % false positive rate. 66 % of subjects identified by ICD-9 code for PCOS could be confirmed as definitely PCOS, with an 8.5 % false positive rate. There was no significant difference in the positive predictive values using the two methods (p=0.2). However, the number of charts that had insufficient confirmatory data was lower using the algorithm (5 % vs 11 %; p<0.04). Of 477 subjects with PCOS recruited and examined individually and present in the database as patients, 451 were found within the algorithm dataset. Conclusions: Extraction of text parameters along with codified data improves the confidence in PCOS patient cohorts identified using the electronic medical record. However, the positive predictive value was not significantly different when using ICD-9 codes or the specific algorithm. Further studies are needed to determine the positive predictive value of the two methods in additional electronic medical record datasets.","keywords_author":["Hyperandrogenism","ICD9 code","Natural language processing","Polycystic ovary morphology"],"keywords_other":["Algorithms","Electronic Health Records","Polycystic Ovary Syndrome","Humans","Middle Aged","Adult","Databases, Factual","Female"],"max_cite":7.0,"pub_year":2015.0,"sources":"['scp']","rawkeys":["hyperandrogenism","databases","female","icd9 code","middle aged","electronic health records","humans","natural language processing","polycystic ovary syndrome","algorithms","factual","adult","polycystic ovary morphology"],"tags":["hyperandrogenism","databases","female","icd9 code","middle aged","electronic health records","humans","natural language processing","polycystic ovary syndrome","algorithms","factual","adult","polycystic ovary morphology"]},{"p_id":53190,"title":"Study on Chinese entity mention detection based on conditional random fields","abstract":"Entity Mention Detection (EMD) is based on the Automatic Content Extraction (ACE). We present an approach which adopts Conditional Random Fields to detect entity mention in Chinese texts. The features include words and character, prefix and postfix, thesaurus, dictionaries and semantic features to improve the detective performance of the system. The experimental result on Chinese ACE 2007 training corpus demonstrates that the proposed method is feasible for Chinese entity mention detection. 1553-9105\/ Copyright \u00a9 2009 Binary Information Press.","keywords_author":["Conditional Random Fields","Entity Mention Detection","Machine Learning","Natural Language Processing"],"keywords_other":["Entity Mention Detection","Natural Language Processing","Conditional Random Fields","Training corpus","Machine Learning","Semantic features","Automatic content","Chinese text","Conditional random field"],"max_cite":0.0,"pub_year":2009.0,"sources":"['scp']","rawkeys":["conditional random fields","entity mention detection","chinese text","conditional random field","machine learning","natural language processing","training corpus","semantic features","automatic content"],"tags":["entity mention detection","chinese text","conditional random field","machine learning","natural language processing","training corpus","semantic features","automatic content"]},{"p_id":51143,"title":"A supervised framework for classifying dependency relations from Bengali shallow parsed sentences","abstract":"\u00a9 Springer International Publishing Switzerland 2015. Natural Language Processing, one of the contemporary research area has adopted parsing technologies for various languages across the world for different objectives. In the present task, a new approach has been introduced for classifying the dependency parsed relations for a morphologically rich and free-phrase-ordered Indian language like Bengali. The pair of dependency parsed relations (also referred as kaarakas \u2018cases\u2019) are classified based on different features like vibhaktis (inflections), Part-of-Speech (POS), punctuation, gender, number and post-position. It is observed that the consecutive and non-consecutive occurrences of such relations play a vital role in the classification. We employed three different machine-learning classifiers, namely NaiveBayes, Sequential Minimal Optimization (SMO) and Conditional Random Field (CRF) which obtained the average F-Scores of 0.895, 0.869 and 0.697, respectively for classifying relation pairs of three primary kaarakas and one primary vibhakti relation. We have also conducted the error analysis for such primary relations using confusion matrices.","keywords_author":["Dependency relations","Kaaraka","Machine-learning classifiers","Vibhakti"],"keywords_other":["Dependency relation","Kaaraka","Vibhakti","Parsing technologies","Sequential minimal optimization","Conditional random field","NAtural language processing","Confusion matrices"],"max_cite":0.0,"pub_year":2015.0,"sources":"['scp']","rawkeys":["dependency relations","confusion matrices","conditional random field","natural language processing","kaaraka","sequential minimal optimization","dependency relation","parsing technologies","vibhakti","machine-learning classifiers"],"tags":["confusion matrices","conditional random field","natural language processing","kaaraka","sequential minimal optimization","dependency relation","parsing technologies","vibhakti","multi label classification"]},{"p_id":47051,"title":"A technical reading in statistical and neural machines translation (SMT & NMT)","abstract":"\u00a9 2017 IEEE. Automatic translation of natural languages has been an active body of research in the last decades, especially when it comes to statistical translation which uses machine learning algorithms for translation tasks. Machine translation being a key application in the field of natural language processing, it leads to develop many approaches namely, statistical machine translation and recently neural machine translation. In this paper, we present a survey of the state of the art, where we describe the context of the current research studies by reviewing both the statistical machine translation and neural machine translation, and an overview of the main strengths and limitations of the two approaches.","keywords_author":["Machine learning","Natural Language Processing","Neural Language Model","Neural Networks","Phrase-based SMT","Reccurent Neural Networks"],"keywords_other":["Automatic translation","Statistical machine translation","Language model","State of the art","Research studies","Statistical translation","Natural languages","Machine translations"],"max_cite":0.0,"pub_year":2017.0,"sources":"['scp']","rawkeys":["reccurent neural networks","automatic translation","statistical translation","neural networks","natural languages","natural language processing","machine learning","phrase-based smt","state of the art","research studies","language model","neural language model","machine translations","statistical machine translation"],"tags":["reccurent neural networks","automatic translation","statistical translation","neural networks","natural languages","natural language processing","machine learning","phrase-based smt","state of the art","research studies","language model","neural language model","machine translations","statistical machine translation"]},{"p_id":52501,"title":"General ETL modeling for NLP tasks","abstract":"\u00a9 The Author(s) 2012. In this chapter we present some general entropy guided transformation learning configurations used for natural language processing tasks. We use the same configuration when applying ETL for the four examined tasks. Hence, the ETL modeling phase is performed with little effort. Moreover, the use of a common parameter setting can also provide some insight about the robustness of the learning algorithm. This chapter is organized as follows. In Sect. 4.1, we show how to model NLP tasks as classification tasks. In Sect. 4.2, we present the basic ETL parameter setting. In Sect. 4.3, we present the ETL committee parameter setting. In Sect. 4.4, we detail the performance measures used to assess the system performances on NLP tasks. Finally, in Sect. 4.5, we describe the software and hardware used in our experiments.","keywords_author":["Classification","Decision trees","Entropy guided transformation learning","ETL committee","Machine learning","Natural language processing"],"keywords_other":null,"max_cite":0.0,"pub_year":2012.0,"sources":"['scp']","rawkeys":["machine learning","natural language processing","entropy guided transformation learning","classification","etl committee","decision trees"],"tags":["machine learning","natural language processing","entropy guided transformation learning","classification","etl committee","decision trees"]},{"p_id":2003,"title":"ICFHR2014 Competition on Handwritten Text Recognition on Transcriptorium Datasets (HTRtS)","abstract":"A contest on Handwritten Text Recognition organised in the context of the ICFHR 2014 conference is described. Two tracks with increased freedom on the use of training data were proposed and three research groups participated in these two tracks. The handwritten images for this contest were drawn from an English data set which is currently being considered in the Tran scriptorium project. The goal of this project is to develop innovative, efficient and cost-effective solutions for the transcription of historical handwritten document images, focusing on four languages: English, Spanish, German and Dutch. For the English language, the so-called \"Bentham collection\" is being considered in Tran scriptorium. It encompasses a large set of manuscripts written by the renowned English philosopher and reformer Jeremy Bentham (1748-1832). A small subset of this collection has been chosen for the present HTR competition. The selected subset has been written by several hands (Bentham himself and his secretaries) and entails significant variabilities and difficulties regarding the quality of text images and writing styles. Training and test data were provided in the form of carefully segmented line images, along with the corresponding transcripts. The three participants achieved very good results, with transcription word error rates ranging from 15.0% down to 8.6%.","keywords_author":["Handwritten Text Recognition"],"keywords_other":["Spanish language","Artificial neural networks","Text recognition","HTRtS","handwritten character recognition","segmented line images","ICFHR2014 competition","Dutch language","historical handwritten document image transcription","handwritten text recognition","English language","Training data","Training","tranScriptorium datasets","Adaptive optics","Hidden Markov models","document image processing","image segmentation","German language","natural language processing","Histograms","HTR competition","text analysis","Bentham collection"],"max_cite":11.0,"pub_year":2014.0,"sources":"['ieee']","rawkeys":["spanish language","handwritten character recognition","training data","segmented line images","dutch language","bentham collection","historical handwritten document image transcription","english language","handwritten text recognition","htr competition","training","histograms","icfhr2014 competition","text recognition","document image processing","image segmentation","german language","transcriptorium datasets","hidden markov models","adaptive optics","natural language processing","text analysis","artificial neural networks","htrts"],"tags":["hand-written text recognition","spanish language","handwritten character recognition","training data","segmented line images","dutch language","bentham collection","historical handwritten document image transcription","neural networks","htr competition","training","histograms","icfhr2014 competition","text recognition","document image processing","image segmentation","german language","transcriptorium datasets","hidden markov models","adaptive optics","natural language processing","english languages","text analysis","htrts"]},{"p_id":55251,"title":"Artificial Intelligence in Medical Practice: The Question to the Answer?","abstract":"Computer science advances and ultra-fast computing speeds find artificial intelligence (AI) broadly benefitting modern society-forecasting weather, recognizing faces, detecting fraud, and deciphering genomics. AI's future role in medical practice remains an unanswered question. Machines (computers) learn to detect patterns not decipherable using biostatistics by processing massive datasets (big data) through layered mathematical models (algorithms). Correcting algorithm mistakes (training) adds to AI predictive model confidence. AI is being successfully applied for image analysis in radiology, pathology, and dermatology, with diagnostic speed exceeding, and accuracy paralleling, medical experts. While diagnostic confidence never reaches 100%, combining machines plus physicians reliably enhances system performance. Cognitive programs are impacting medical practice by applying natural language processing to read the rapidly expanding scientific literature and collate years of diverse electronic medical records. In this and other ways, AI may optimize the care trajectory of chronic disease patients, suggest precision therapies for complex illnesses, reduce medical errors, and improve subject enrollment into clinical trials. (c) 2018 Elsevier Inc. All rights reserved.","keywords_author":["Analytics","Artificial intelligence","Big data","Chronic disease","Deep learning","Electronic medical record","Machine learning","Medical imaging","Natural language processing","Neural networks","Precision medicine"],"keywords_other":["LEARNING ALGORITHM","DEEP"],"max_cite":0.0,"pub_year":2018.0,"sources":"['wos']","rawkeys":["artificial intelligence","big data","deep learning","deep","medical imaging","neural networks","machine learning","natural language processing","chronic disease","electronic medical record","learning algorithm","precision medicine","analytics"],"tags":["big data","neural networks","deep","medical imaging","machine learning","natural language processing","chronic disease","electronic medical record","learning algorithm","precision medicine","analytics"]},{"p_id":34777,"title":"Chinese textual entailment recognition based on syntactic tree clipping","abstract":"\u00a9 Springer International Publishing Switzerland 2014. Textual entailment has been proposed as a unifying generic framework for modeling language variability and semantic inference in different Natural Language Processing (NLP) tasks. This paper presents a novel statistical method for recognizing Chinese textual entailment in which lexical, syntactic with semantic matching features are combined together. In order to solve the problems of syntactic tree matching difficulty and tree structure errors caused by Chinese word segmentation, the method firstly clips the syntactic trees into minimum information trees and then computes syntactic matching similarity on them. All features will be used in a voting style under different machine learning methods to predict whether the text sentence can entail the hypothesis sentence in a text-hypothesis pair. The experimental results show that the feature on changing structure of syntactic tree is effective and efficient in Chinese textual entailment.","keywords_author":["Machine learning","Minimum information tree","Syntactic tree clipping","Textual entailment"],"keywords_other":["Syntactic trees","Chinese textual entailments","Textual entailment","Minimum information","Chinese word segmentation","Changing structures","Machine learning methods","NAtural language processing"],"max_cite":2.0,"pub_year":2014.0,"sources":"['scp']","rawkeys":["machine learning methods","syntactic tree clipping","syntactic trees","chinese word segmentation","minimum information tree","minimum information","textual entailment","machine learning","natural language processing","chinese textual entailments","changing structures"],"tags":["machine learning methods","syntactic tree clipping","syntactic trees","chinese word segmentation","minimum information tree","minimum information","textual entailment","machine learning","natural language processing","chinese textual entailments","changing structures"]},{"p_id":38874,"title":"An adaptive approach of Tamil character recognition using deep learning with big data-A survey","abstract":"\u00a9 Springer International Publishing Switzerland 2015. Deep learning is currently an extremely active research area in machine learning and pattern recognition society. It has gained huge successes in a broad area of applications such as speech recognition, computer vision, and natural language processing. With the sheer size of data available today, big data brings big opportunities and transformative potential for various sectors; on the other hand, it also presents unprecedented challenges to harnessing data and information. As the data keeps getting bigger, deep learning is coming to play a key role in providing big data predictive analytics solutions. This paper presents a brief overview of deep learning and highlight how it can be effectively applied for optical character recognition in Tamil language.","keywords_author":["Big Data","Classifier Design and Evaluation","Deep Learning","Feature Representation","GPGPU and Optical Character Recognition","Machine Learning","Neural Nets Models","Parallel Processing"],"keywords_other":["Deep learning","Predictive analytics","Classifier design and evaluation","Data and information","Feature representation","Adaptive approach","NAtural language processing","Parallel processing"],"max_cite":1.0,"pub_year":2015.0,"sources":"['scp']","rawkeys":["parallel processing","big data","deep learning","predictive analytics","machine learning","gpgpu and optical character recognition","natural language processing","classifier design and evaluation","data and information","neural nets models","adaptive approach","feature representation"],"tags":["parallel processing","big data","predictive analytics","machine learning","gpgpu and optical character recognition","natural language processing","classifier design and evaluation","data and information","neural nets models","adaptive approach","feature representation"]},{"p_id":18395,"title":"VarifocalReader - In-depth visual analysis of large text documents","abstract":"\u00a9 2014 IEEE.Interactive visualization provides valuable support for exploring, analyzing, and understanding textual documents. Certain tasks, however, require that insights derived from visual abstractions are verified by a human expert perusing the source text. So far, this problem is typically solved by offering overview+detail techniques, which present different views with different levels of abstractions. This often leads to problems with visual continuity. Focus+context techniques, on the other hand, succeed in accentuating interesting subsections of large text documents but are normally not suited for integrating visual abstractions. With VarifocalReader we present a technique that helps to solve some of these approaches' problems by combining characteristics from both. In particular, our method simplifies working with large and potentially complex text documents by simultaneously offering abstract representations of varying detail, based on the inherent structure of the document, and access to the text itself. In addition, VarifocalReader supports intra-document exploration through advanced navigation concepts and facilitates visual analysis tasks. The approach enables users to apply machine learning techniques and search mechanisms as well as to assess and adapt these techniques. This helps to extract entities, concepts and other artifacts from texts. In combination with the automatic generation of intermediate text levels through topic segmentation for thematic orientation, users can test hypotheses or develop interesting new research questions. To illustrate the advantages of our approach, we provide usage examples from literature studies.","keywords_author":["Distant reading","Document analysis","Literary analysis","Machine learning","Natural language processing","Text mining","Visual analytics"],"keywords_other":["Text mining","Document analysis","Visual analytics","Distant readings","Literary analysis","NAtural language processing"],"max_cite":24.0,"pub_year":2014.0,"sources":"['scp']","rawkeys":["visual analytics","literary analysis","text mining","distant reading","machine learning","natural language processing","document analysis","distant readings"],"tags":["visual analytics","literary analysis","text mining","machine learning","natural language processing","document analysis","distant readings"]},{"p_id":42971,"title":"Explicit Content Detection in Music Lyrics Using Machine Learning","abstract":"\u00a9 2018 IEEE. Music has serious effects on children's development. Music lyrics have become more violent and sexual over the years. However, the system for filtering explicit contents in music often does not work properly, not to mention that it takes a lot of time and effort to do it properly. In this study, we propose several machine learning models that automatically detect explicit contents in Korean lyrics and compare their performances. The proposed Bagging with selective vocabulary model outperformed not only the other competing models we designed, but also the filtering method that used the man-made profanity dictionary, which is a widely-used method to detect explicit contents in the industry. The proposed automated lyrics screening approach makes practical contributions to music industry, helping it significantly save time and effort for censoring harmful contents for the youths. The proposed approach is generalizable to other language settings as long as the same kinds of data used in the study are available.","keywords_author":["Abusive Language","Adolescent Safety","Explicit Contents","Lyrics","Machine Learning","Music","NLP","Parent Advisory Lable"],"keywords_other":["Lyrics","Music","Abusive Language","Explicit Contents","Parent Advisory Lable"],"max_cite":0.0,"pub_year":2018.0,"sources":"['scp']","rawkeys":["explicit contents","nlp","adolescent safety","machine learning","abusive language","parent advisory lable","lyrics","music"],"tags":["explicit contents","adolescent safety","machine learning","natural language processing","abusive language","parent advisory lable","lyrics","music"]},{"p_id":34783,"title":"A machine learning approach to pronominal anaphora resolution in dialogue based intelligent tutoring systems","abstract":"Anaphora resolution is a central topic in dialogue and discourse that deals with finding the referent of a pronoun. It plays a critical role in conversational Intelligent Tutoring Systems (ITSs) as it can increase the accuracy of assessing students' knowledge level, i.e. mental model, based on their natural language inputs. Although anaphora resolution is one of the most studied problems in Natural Language Processing, there are very few studies that focus on anaphora resolution in dialogue based ITSs. To this end, we present Deep Anaphora Resolution Engine++ (DARE++) that adapts and extends existing machine learning solutions to resolve pronouns in ITS dialogues. Experiments showed that DARE++ achieves a F-measure of 88.93%, proving the potential of the proposed method for resolving pronouns in student-tutor dialogues. \u00a9 2014 Springer-Verlag Berlin Heidelberg.","keywords_author":["Anaphora Resolution","Machine Learning","Tutoring System"],"keywords_other":["Intelligent tutoring system (ITSs)","Machine learning approaches","Pronominal anaphora resolution","Tutoring system","Natural languages","Anaphora resolution","Intelligent tutoring system","NAtural language processing"],"max_cite":2.0,"pub_year":2014.0,"sources":"['scp']","rawkeys":["pronominal anaphora resolution","natural languages","machine learning","natural language processing","intelligent tutoring system","tutoring system","intelligent tutoring system (itss)","anaphora resolution","machine learning approaches"],"tags":["pronominal anaphora resolution","natural languages","natural language processing","machine learning","intelligent transportation systems","tutoring system","anaphora resolution","machine learning approaches"]},{"p_id":8160,"title":"Introduction to Arabic natural language processing","abstract":"\u00a9 2010 by Morgan & Claypool All rights reserved. This book provides system developers and researchers in natural language processing and computational linguistics with the necessary background information for working with the Arabic language. The goal is to introduce Arabic linguistic phenomena and review the state-of-the-art in Arabic processing. The book discusses Arabic script, phonology, orthography, morphology, syntax and semantics, with a final chapter on machine translation issues.The chapter sizes correspond more or less to what is linguistically distinctive about Arabic, with morphology getting the lion's share, followed by Arabic script. No previous knowledge of Arabic is needed. This book is designed for computer scientists and linguists alike. The focus of the book is on Modern Standard Arabic; however, notes on practical issues related to Arabic dialects and languages written in the Arabic script are presented in different chapters.","keywords_author":["Arabic","Computational linguistics","Machine translation","Morphology","Natural language processing","Orthography","Phonology","Script","Semantics","Syntax"],"keywords_other":["Orthography","Arabic","Phonology","Syntax","Script","Machine translations"],"max_cite":110.0,"pub_year":2010.0,"sources":"['scp']","rawkeys":["semantics","natural language processing","phonology","machine translation","morphology","syntax","arabic","computational linguistics","machine translations","orthography","script"],"tags":["semantics","natural language processing","phonology","syntax","morphology","scripting","arabic","computational linguistics","machine translations","orthography"]},{"p_id":26593,"title":"Data-mining to build a knowledge representation store for clinical decision support. Studies on curation and validation based on machine performance in multiple choice medical licensing examinations","abstract":"\u00a9 2016 Elsevier Ltd.Extracting medical knowledge by structured data mining of many medical records and from unstructured data mining of natural language source text on the Internet will become increasingly important for clinical decision support. Output from these sources can be transformed into large numbers of elements of knowledge in a Knowledge Representation Store (KRS), here using the notation and to some extent the algebraic principles of the Q-UEL Web-based universal exchange and inference language described previously, rooted in Dirac notation from quantum mechanics and linguistic theory. In a KRS, semantic structures or statements about the world of interest to medicine are analogous to natural language sentences seen as formed from noun phrases separated by verbs, prepositions and other descriptions of relationships. A convenient method of testing and better curating these elements of knowledge is by having the computer use them to take the test of a multiple choice medical licensing examination. It is a venture which perhaps tells us almost as much about the reasoning of students and examiners as it does about the requirements for Artificial Intelligence as employed in clinical decision making. It emphasizes the role of context and of contextual probabilities as opposed to the more familiar intrinsic probabilities, and of a preliminary form of logic that we call presyllogistic reasoning.","keywords_author":["Clinical decision support","Data mining","Knowledge representation","Medical licensing examinations","Natural language processing","Probability","Relevance","USMLE","Web surfing"],"keywords_other":["Licensure, Medical","Relevance","Decision Support Systems, Clinical","Humans","Clinical decision support","USMLE","Data Mining","Data Curation","NAtural language processing","Medical licensing examinations","Web surfing"],"max_cite":7.0,"pub_year":2016.0,"sources":"['scp']","rawkeys":["usmle","data mining","web surfing","licensure","medical licensing examinations","natural language processing","relevance","humans","clinical","clinical decision support","knowledge representation","medical","probability","decision support systems","data curation"],"tags":["usmle","data mining","web surfing","licensure","medical licensing examinations","natural language processing","relevance","humans","clinical","clinical decision support","knowledge representation","medical","probability","decision support systems","data curation"]},{"p_id":34786,"title":"Developing an automated Bangla parts of speech tagged dictionary","abstract":"\u00a9 2014 IEEE. This paper develops an algorithm for making an automated Bangla Parts Of Speech (POS) tagged dictionary. Natural Language Processing is one of the most vigorous research areas of computer science. It enables to communicate and retrieve information form computer based system more effectively and efficiently. Researches on Bangla language processing have started long back. However, this research area still suffers from resource scarcity. A POS tagged corpus is a cardinal element for language processing. POS tagging is the process of categorizing a particular word to a particular part of speech or syntactic category. In Bangla, we do not have any large POS tagged dictionary. In this paper we develop an automated way to make a POS tagged dictionary of Noun, Verb and Adjective. Initially, a suffix (or postfix) list is created manually for Bangla language. Based on this suffix list the POS tagged dictionary is developed. The proposed algorithm is evaluated using a paragraph consisting of manually tagged 10,000 words with 11 tags. We found that POS tagging is obtained more accurately for Verb than Noun and Adjective.","keywords_author":["Bangla Corpus","Bangla Language Processing","Machine Learning","Parts Of Speech Tagging"],"keywords_other":["Parts-of-speech tagging","Language processing","Bangla Corpus","Computer-based system","Parts of speech","Resource scarcity","Part Of Speech","NAtural language processing"],"max_cite":2.0,"pub_year":2014.0,"sources":"['scp']","rawkeys":["bangla corpus","bangla language processing","parts of speech","parts of speech tagging","machine learning","natural language processing","resource scarcity","computer-based system","part of speech","parts-of-speech tagging","language processing"],"tags":["bangla corpus","bangla language processing","machine learning","natural language processing","part of speech tagging","resources scarcity","computer-based system","part of speech","language processing"]},{"p_id":34787,"title":"Bangla word clustering based on N-gram language model","abstract":"\u00a9 2014 IEEE.In this paper, we describe a method for producing Bangla word clusters based on semantic and contextual similarity. Word clustering is important for parts of speech (POS) tagging, word sense disambiguation, text classification, recommender system, spell checker, grammar checker, knowledge discover and for many others Natural Language Processing (NLP) applications. Computerization of Bangla language processing has been started a long ago, but still it is in neophyte stage and suffers from resource scarcity. We propose anunsupervised machine learning technique to develop Bangla word clusters based on their semantic and contextual similarity using N-gram language model. According to N-gram model, a word can be predictedbased on its previous and next words sequence. N-gram model is applied successfully for word clustering in English and some other languages. As word clustering in Bangla is a new dimension in Bangla language processing research, so we think this process is good way to start and our assumption is true as our result is quite decent. We produced 456 clusters using a locally available large Bangla corpus. Subjective score derived from the clusters reveal strong similarity of the words in the same cluster.","keywords_author":["information retrival","machine learning","n-gram model","natural language processing","word cluster"],"keywords_other":["information retrival","Word-clusters","Language processing","Word Sense Disambiguation","N-gram modeling","N-gram language models","NAtural language processing","Machine learning techniques"],"max_cite":2.0,"pub_year":2014.0,"sources":"['scp']","rawkeys":["information retrival","n-gram modeling","n-gram language models","word-clusters","n-gram model","word cluster","machine learning techniques","machine learning","natural language processing","word sense disambiguation","language processing"],"tags":["information retrival","n-gram language models","n-gram models","machine learning techniques","machine learning","natural language processing","word clustering","word sense disambiguation","language processing"]},{"p_id":16356,"title":"Arabic Named Entity Recognition: A Feature-Driven Study","abstract":"The Named Entity Recognition task aims at identifying and classifying Named Entities within an open-domain text. This task has been garnering significant attention recently as it has been shown to help improve the performance of many Natural Language Processing applications. In this paper, we investigate the impact of using different sets of features in three discriminative machine learning frameworks, namely, support vector machines, maximum entropy and conditional random fields for the task of Named Entity Recognition. Our language of interest is Arabic. We explore lexical, contextual and morphological features and nine data-sets of different genres and annotations. We measure the impact of the different features in isolation and incrementally combine them in order to evaluate the robustness to noise of each approach. We achieve the highest performance using a combination of 15 features in conditional random fields using Broadcast News data (F\u00b2=1 = 83.34). \u00a9 2009 IEEE","keywords_author":["Arabic","machine learning comparison","named entity recognition","natural language processing (NLP)"],"keywords_other":null,"max_cite":37.0,"pub_year":2009.0,"sources":"['scp']","rawkeys":["arabic","named entity recognition","natural language processing (nlp)","machine learning comparison"],"tags":["arabic","named entity recognition","natural language processing","machine learning comparison"]},{"p_id":36836,"title":"Learning profiles in duplicate question detection","abstract":"\u00a9 2017 IEEE. This paper presents the results of systematic and comparative experimentation with major types of methodologies for automatic duplicate question detection when these are applied to datasets of progressively larger sizes, thus allowing to study the learning profiles of this task under these different approaches and evaluate their merits. This study was made possible by resorting to the recent release for research purposes, by the Quora online question answering engine, of a new dataset with over 400, 000 pairs labeled with respect to their elements being duplicate interrogative segments.","keywords_author":["Deep learning","Duplicate question detection","Machine learning","Natural language processing","Neural networks","Semantic similarity"],"keywords_other":["Learning profiles","Research purpose","Question Answering","Semantic similarity"],"max_cite":1.0,"pub_year":2017.0,"sources":"['scp']","rawkeys":["question answering","duplicate question detection","deep learning","neural networks","natural language processing","machine learning","semantic similarity","learning profiles","research purpose"],"tags":["duplicate question detection","neural networks","natural language processing","machine learning","information retrieval","semantic similarity","learning profiles","research purpose"]},{"p_id":45024,"title":"Understanding and modeling conversations on microblogs","abstract":"\u00a9 2017 IEEE. Recommender systems have proven to successfully influence user decisions on several applications and websites such as Amazon, eBay, Netflix, Spotify, among others. Largely, these systems rely on collaborative filtering to make useful recommendations but could suffer from cold start issues, i.e., lacking enough information for new users. By harnessing the popularity of social media (e.g. Twitter, Facebook) and other social media platforms, knowledge about users can be extracted, including intent of users that engage in conversations. This can provide additional information to applications using recommender systems in order to overcome issues like cold start. To that end, this research will tackle several challenges of natural language understanding in the context of conversations on social media platforms, such as: clustering, classification and summarization of conversations. Thus, the models developed will allow to extract knowledge from microblogs conversations that could be used in several applications.","keywords_author":["Machine Learning","Natural Language Processing","Social Computing"],"keywords_other":["Social computing","Natural language understanding","Netflix","Cold start","Social media","Facebook","Social media platforms","Microblogs"],"max_cite":0.0,"pub_year":2018.0,"sources":"['scp']","rawkeys":["facebook","microblogs","cold start","netflix","social media","social media platforms","natural language processing","machine learning","social computing","natural language understanding"],"tags":["facebook","cold start","netflix","social media","social media platforms","machine learning","natural language processing","social computing","microblogging","natural language understanding"]},{"p_id":32751,"title":"Generalization errors in estimation of stochastic context-free grammar","abstract":"In sequential data analysis, such as natural language processing and gene analysis, stochastic context-free grammars are commonly used. In spite of the wide-ranged applications and many learning algorithms, the theoretical properties have not been clarified. When the grammar is parametrized, we can regard the production system of words with the grammar as a statistical learning machine, which falls into a singular machine. In this paper, the performance of the system is revealed based on the algebraic geometrical method, which enables us to analyze singular machines. The result helps to estimate the grammar structure.","keywords_author":["Algebraic geometry","Artificial intelligence","Bayesian generalization error","Machine learning","Statistical learning machine","Stochastic context-free grammar"],"keywords_other":["Geometrical methods","Production system","Sequential data analysis","Statistical learning","Stochastic context free grammar","Generalization Error","Algebraic geometry","NAtural language processing"],"max_cite":3.0,"pub_year":2005.0,"sources":"['scp']","rawkeys":["statistical learning machine","artificial intelligence","generalization error","stochastic context free grammar","stochastic context-free grammar","machine learning","geometrical methods","natural language processing","algebraic geometry","statistical learning","bayesian generalization error","sequential data analysis","production system"],"tags":["statistical learning machine","generalization error","stochastic context free grammar","machine learning","geometrical methods","natural language processing","algebraic geometry","statistical learning","bayesian generalization error","sequential data analysis","production system"]},{"p_id":42991,"title":"Contextual-CNN: A Novel Architecture Capturing Unified Meaning for Sentence Classification","abstract":"\u00a9 2018 IEEE. In this paper, we focus on the architecture of the convolutional neural network (CNN) for sentence classification. For understanding natural language, context in the sentence is important information for grasping the word sense. However, traditional CNN's feed-forward architecture is insufficient to reflect this factor. To solve this limitation, we propose a contextual CNN (C-CNN) for better text understanding by adding recurrent connection to the convolutional layer. This architecture helps CCNN units to be modulated over time with their neighboring units, thus the model integrates word meanings with surrounding information within the same layer. We evaluate our model on sentence-level sentiment prediction tasks and question categorization task. The C-CNN achieves state-of-the-art performances on fine-grained sentiment prediction and question categorization.","keywords_author":["convolutional neural network","deep learning","natural language processing","sentence classification"],"keywords_other":["State-of-the-art performance","Prediction tasks","Feed-forward architectures","Novel architecture","Convolutional Neural Networks (CNN)","Sentence classifications","Convolutional neural network","Natural languages"],"max_cite":0.0,"pub_year":2018.0,"sources":"['scp', 'ieee']","rawkeys":["novel architecture","deep learning","feed-forward architectures","prediction tasks","natural languages","natural language processing","sentence classifications","convolutional neural network","sentence classification","state-of-the-art performance","convolutional neural networks (cnn)"],"tags":["novel architecture","feed-forward architectures","prediction tasks","natural languages","natural language processing","machine learning","sentence classifications","convolutional neural network","state-of-the-art performance"]},{"p_id":53233,"title":"Ontology-based examinational students work retrieval","abstract":"Intelligent Virtual Agent (IVA) developments provide a new kind of interaction between the IVA and the human beings in front of the computer. Agents - assistants that present lessons, ask questions about the lesson, and examine the students, are gladly received, with great interest from the students. Therefore we are working on modelling and investigating an IVA intended to help students in their preparation for a particular examination and to check their paper works after the examination. The agent's architecture includes a number of modules, such as: an emotional module; a motivational module; a module, comprising behavioural rules and meta-rules (principles); a module for acquisition and usage of knowledge; modules for visualization of the agent and animated pronunciation of texts. The agent's modules serve different steps: building an ontology of lectures; ontology-based recognition of the topic, on which a student has worked; natural language processing; semiautomatic creation and enrichment of the ontology of each lecture; ontology-based checking of the particular student's work (matching processing); loading up those slides from the lecture, which contain the information, omitted in the student's work; accumulation of statistical data related to the identified matches; change of the agent's condition; visualization of the agent and pronunciation of a text.","keywords_author":["Data mining","Intelligent Virtual Agents","Machine learning","Natural language analysis","Ontology"],"keywords_other":["Motivational modules","NAtural language processing","Meta rules","Natural language analysis","Machine learning","Statistical datas","Ontology-based","Human being","Matching processing"],"max_cite":0.0,"pub_year":2008.0,"sources":"['scp']","rawkeys":["data mining","meta rules","intelligent virtual agents","ontology","machine learning","natural language processing","statistical datas","natural language analysis","ontology-based","human being","motivational modules","matching processing"],"tags":["data mining","meta rules","intelligent virtual agents","machine learning","natural language processing","statistical datas","natural language analysis","ontology-based","matching process","human being","motivational modules"]},{"p_id":2031,"title":"Joint layer based deep learning framework for bilingual machine transliteration","abstract":"Between the growth of Internet or World Wide Web (WWW) and the emersion of the social networking site like Friendster, Myspace etc., information society started facing exhilarating challenges in language technology applications such as Machine Translation (MT) and Information Retrieval (IR). Nevertheless, there were researchers working in Machine Translation that deal with real time information for over 50 years since the first computer has come along. Merely, the need for translating data has become larger than before as the world was getting together through social media. Especially, translating proper nouns and technical terms has become openly challenging task in Machine Translation. The Machine transliteration was emerged as a part of information retrieval and machine translation projects to translate the Named Entities based on phoneme and grapheme, hence, those are not registered in the dictionary. Many researchers have used approaches such as conventional Graphical models and also adopted other machine translation techniques for Machine Transliteration. Machine Transliteration was always looked as a Machine Learning Problem. In this paper, we presented a new area of Machine Learning approach termed as a Deep Learning for improving the bilingual machine transliteration task for Tamil and English languages with limited corpus. This technique precedes Artificial Intelligence. The system is built on Deep Belief Network (DBN), a generative graphical model, which has been proved to work well with other Machine Learning problem. We have obtained 79.46% accuracy for English to Tamil transliteration task and 78.4 % for Tamil to English transliteration.","keywords_author":["Artificial Intelligence","Computational Linguistics","Deep Belief Networks","Deep Learning","Machine Transliteration","Natural Language Processing","Restricted Boltzmann Machine","Natural Language Processing","Computational Linguistics","Machine Transliteration","Artificial Intelligence","Deep Learning","Restricted Boltzmann Machine","Deep Belief Networks"],"keywords_other":["language translation","Tamil-to-English transliteration","Joints","World Wide Web","Machine transliteration","machine translation","grapheme","Friendster","Deep learning","Internet","Computers","English languages","Dictionaries","phoneme","deep belief network","IR","MT","Restricted boltzmann machine","technical terms","generative graphical model","joint layer based deep learning framework","language technology applications","real time information","social media","bilingual machine transliteration","Training","Deep belief networks","information retrieval","Myspace","Vectors","computational linguistics","DBN","belief networks","NAtural language processing","Neurons","information society","Tamil languages","social networking site","natural language processing","proper nouns","machine learning problem","named entities","Support vector machines"],"max_cite":0.0,"pub_year":2014.0,"sources":"['scp', 'ieee']","rawkeys":["language translation","myspace","machine translation","grapheme","tamil languages","internet","dbn","vectors","phoneme","deep belief network","neurons","joints","tamil-to-english transliteration","technical terms","mt","generative graphical model","deep learning","joint layer based deep learning framework","language technology applications","real time information","bilingual machine transliteration","social media","information retrieval","training","machine transliteration","computational linguistics","computers","belief networks","ir","friendster","world wide web","dictionaries","restricted boltzmann machine","information society","artificial intelligence","social networking site","natural language processing","proper nouns","machine learning problem","english languages","support vector machines","named entities","deep belief networks"],"tags":["language translation","real-time information","myspace","graphemes","social networking sites","internet","machine learning","vectors","phoneme","machine translations","neurons","joints","tamil-to-english transliteration","technical terms","mt","generative graphical model","tamil language","neural networks","joint layer based deep learning framework","language technology applications","social media","bilingual machine transliteration","training","information retrieval","machine transliteration","computational linguistics","belief networks","world wide web","friendster","dictionaries","restricted boltzmann machine","information society","natural language processing","proper nouns","machine learning problem","english languages","named entities","deep belief networks"]},{"p_id":28659,"title":"Classification-based filtering of semantic relatedness in hypernymy extraction","abstract":"Manual construction of a wordnet can be facilitated by a system that suggests semantic relations acquired from corpora. Such systems tend to produce many wrong suggestions. We propose a method of filtering a raw list of noun pairs potentially linked by hypernymy, and test it on Polish. The method aims for good recall and sufficient precision. The classifiers work with complex features that give clues on the relation between the nouns. We apply a corpus-based measure of semantic relatedness enhanced with a Rank Weight Function. The evaluation is based on the data in Polish WordNet. The results compare favourably with similar methods applied to English, despite the small size of Polish WordNet. \u00a9 2008 Springer-Verlag Berlin Heidelberg.","keywords_author":["Classifiers","Filtering","Hypernymy extraction","Lexical-semantic relations","Measures of semantic relatedness","Nouns","Polish WordNet","Rank weight function","Supervised machine learning","Wordnet construction"],"keywords_other":["Rank weight function","Polish WordNet","Wordnet construction","Word net","Hypernymy extraction","Nouns","Small size","Filtering","Measures of semantic relatedness","Semantic relatedness","Lexical-semantic relations","International conferences","Classifiers","Weight functions","Semantic relations","Supervised machine learning","NAtural language processing"],"max_cite":5.0,"pub_year":2008.0,"sources":"['scp']","rawkeys":["filtering","semantic relatedness","semantic relations","word net","weight functions","lexical-semantic relations","measures of semantic relatedness","international conferences","natural language processing","hypernymy extraction","classifiers","wordnet construction","small size","nouns","supervised machine learning","polish wordnet","rank weight function"],"tags":["semantic relations","semantic relatedness","wordnet","lexical-semantic relations","weighting functions","measures of semantic relatedness","international conferences","natural language processing","hypernymy extraction","classifier","wordnet construction","small size","filter","nouns","supervised machine learning","polish wordnet","rank weight function"]},{"p_id":14326,"title":"A review of affective computing: From unimodal analysis to multimodal fusion","abstract":"\u00a9 2017 Elsevier B.V. Affective computing is an emerging interdisciplinary research field bringing together researchers and practitioners from various fields, ranging from artificial intelligence, natural language processing, to cognitive and social sciences. With the proliferation of videos posted online (e.g., on YouTube, Facebook, Twitter) for product reviews, movie reviews, political views, and more, affective computing research has increasingly evolved from conventional unimodal analysis to more complex forms of multimodal analysis. This is the primary motivation behind our first of its kind, comprehensive literature review of the diverse field of affective computing. Furthermore, existing literature surveys lack a detailed discussion of state of the art in multimodal affect analysis frameworks, which this review aims to address. Multimodality is defined by the presence of more than one modality or channel, e.g., visual, audio, text, gestures, and eye gage. In this paper, we focus mainly on the use of audio, visual and text information for multimodal affect analysis, since around 90% of the relevant literature appears to cover these three modalities. Following an overview of different techniques for unimodal affect analysis, we outline existing methods for fusing information from different modalities. As part of this review, we carry out an extensive study of different categories of state-of-the-art fusion techniques, followed by a critical analysis of potential performance improvements with multimodal analysis compared to unimodal analysis. A comprehensive overview of these two complementary fields aims to form the building blocks for readers, to better understand this challenging and exciting research field.","keywords_author":["Affective computing","Audio, visual and text information fusion","Multimodal affect analysis","Multimodal fusion","Sentiment analysis","Affective computing","Sentiment analysis","Multimodal affect analysis","Multimodal fusion","Audio, visual and text information fusion"],"keywords_other":["Affective Computing","Multi-modal fusion","FACIAL EXPRESSION RECOGNITION","AUTOMATIC-ANALYSIS","SHAPE TRANSFORMATIONS","Sentiment analysis","SOCIAL DATA-ANALYSIS","ACTION UNITS","Interdisciplinary research","SPEECH EMOTION RECOGNITION","Literature reviews","APPEARANCE MODELS","AUDIOVISUAL AFFECT RECOGNITION","Affect analysis","NEURAL-NETWORK","LEVEL SENTIMENT ANALYSIS","Multimodal analysis","NAtural language processing"],"max_cite":61.0,"pub_year":2017.0,"sources":"['scp', 'wos']","rawkeys":["affective computing","automatic-analysis","literature reviews","visual and text information fusion","sentiment analysis","action units","interdisciplinary research","multi-modal fusion","multimodal fusion","audiovisual affect recognition","neural-network","facial expression recognition","multimodal analysis","shape transformations","appearance models","speech emotion recognition","social data-analysis","audio","level sentiment analysis","natural language processing","affect analysis","multimodal affect analysis"],"tags":["affective computing","literature reviews","visual and text information fusion","sentiment analysis","interdisciplinary research","multi-modal fusion","multimodal fusion","audiovisual affect recognition","facial expression recognition","appearance modeling","multimodal analysis","neural networks","shape transformations","speech emotion recognition","social data-analysis","audio","action unit","level sentiment analysis","natural language processing","affect analysis","automatic analysis","multimodal affect analysis"]},{"p_id":28663,"title":"Study on the Chinese named entity recognition using small scale character tail hints","abstract":"We propose small-scale-hint-character-list (SSHCL) features for location and organization names under the conditional random fields framework. As experiments show, SSHCL features provide significant gains in precision, especially for organization names, showing complementary property to part-of-speech. It also lowers construction and training cost greatly that a common large scale feature set demands. The overall proper nouns F1 measurement of integrated system on simple Chinese 863 program 2004 NER corpora reaches 88.76%, gaining 8.63% improvement over the best system in the evaluation. The performance on SIGHAN 2006 is also remarkable.","keywords_author":["Chinese named entity recognition","Conditional random fields","Machine learning","Natural language processing","Small-scale-tail-hint-character-list feature"],"keywords_other":["Chinese named entity recognition","Conditional random fields","Small-scale-tail-hint-character-list feature","Machine learning","Natural language processing"],"max_cite":5.0,"pub_year":2008.0,"sources":"['scp']","rawkeys":["conditional random fields","natural language processing","machine learning","small-scale-tail-hint-character-list feature","chinese named entity recognition"],"tags":["conditional random field","natural language processing","machine learning","small-scale-tail-hint-character-list feature","chinese named entity recognition"]},{"p_id":28669,"title":"Confusion class discrimination techniques for text classification","abstract":"This paper analyzes confusion class phenomena existing in text classification procedure, and studies further confusion class discrimination techniques to improve the performance of text classification. In this paper, firstly a technique for confusion class recognition based on classification error distribution is proposed to recognize confusion class sets existing in the pre-defined taxonomy. To effectively discriminate confusion classes, this paper proposes an approach to feature selection based on discrimination capability in the procedure of which each candidate feature's discrimination capability for class pair is evaluated. At last, two-stage classifiers are used to integrate baseline classifier and confusion class classifiers, and in which the two output results from two stages are combined into the final output results. The confusion class classifiers in the second stage could be activated only when the output class of the input text assigned by baseline classifier in the first stage belongs to confusion classes, then the confusion class classifiers are used to discriminate the testing text again. In the comparison experiments, Newsgroup and 863 Chinese evaluation data collection are used to evaluate the effectiveness of the techniques proposed in this paper, respectively. Experimental results show that the methods could improve significantly the performance for single-label and multi-class classifier (SMC).","keywords_author":["Classification error distribution","Confusion class discrimination","Feature selection","Machine learning","Natural language processing","Text classification"],"keywords_other":["Single label multiclass classifier (SMC)","Taxonomy","Text classification","Chinese evaluation data","Confusion class classifier","Machine learning","Confusion class discrimination","Newsgroup","Feature selection","Baseline classifier","Classification error distribution"],"max_cite":5.0,"pub_year":2008.0,"sources":"['scp']","rawkeys":["taxonomy","machine learning","natural language processing","confusion class discrimination","baseline classifier","chinese evaluation data","feature selection","single label multiclass classifier (smc)","confusion class classifier","newsgroup","text classification","classification error distribution"],"tags":["newsgroups","taxonomy","machine learning","natural language processing","confusion class discrimination","baseline classifier","chinese evaluation data","feature selection","single label multiclass classifier (smc)","confusion class classifier","text classification","classification error distribution"]}]