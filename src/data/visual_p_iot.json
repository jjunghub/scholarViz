[{"p_id":26629,"title":"ProfilIoT: A machine learning approach for IoT device identification based on network traffic analysis","abstract":"Copyright 2017 ACM. In this work we apply machine learning algorithms on network trafic data for accurate identification of IoT devices connected to a network. To train and evaluate the classifier, we collected and labeled network trafic data from nine distinct IoT devices, and PCs and smartphones. Using supervised learning, we trained a multi-stage meta classifier; in the first stage, the classifier can distinguish between trafic generated by IoT and non-IoT devices. In the second stage, each IoT device is associated a specific IoT device class. The overall IoT classification accuracy of our model is 99.281%.","keywords_author":["Cyber security","Device identification","Internet of Things (IoT)","Machine learning","Network trafic analysis"],"keywords_other":["Device class","Network traffic analysis","Machine learning approaches","Cyber security","Meta-classifiers","Multi stage","Internet of Things (IOT)","Classification accuracy"],"max_cite":6.0,"pub_year":2017.0,"sources":"['scp', 'wos']","rawkeys":["meta-classifiers","device class","network traffic analysis","classification accuracy","machine learning","device identification","multi stage","network trafic analysis","cyber security","machine learning approaches","internet of things (iot)"],"tags":["meta-classifiers","device class","network traffic analysis","classification accuracy","machine learning","multi-stage","device identification","network trafic analysis","cyber security","machine learning approaches","internet of things (iot)"]},{"p_id":49158,"title":"Multi-layered architecture for soil moisture prediction in agriculture 4.0","abstract":"The article defines a multi-layered architecture for data mining in the context of Internet of Things. The conceptual model is based on the data flow and its characteristics (states, bandwidth and bottlenecks) given by the connectivity infrastructure. The complexity of the algorithms increases along with the availability of computational resources and so does their accuracy. The architecture is validated in agriculture and is used daily for predicting the soil moisture of the next day. When the predicted value is out of a certain range, the farmers are alerted by SMS and email.","keywords_author":["Data mining","Internet of Things.","Machine learning","Weather prediction"],"keywords_other":["Computational resources","Soil moisture predictions","Conceptual model","Weather prediction","Data flow","Multi-layered"],"max_cite":0.0,"pub_year":2017.0,"sources":"['scp']","rawkeys":["data mining","weather prediction","conceptual model","internet of things","machine learning","computational resources","soil moisture predictions","data flow","multi-layered"],"tags":["data mining","multi-layered","weather prediction","conceptual model","machine learning","computational resources","soil moisture predictions","data flow","internet of things (iot)"]},{"p_id":59406,"title":"A Survey on Internet of Things From Industrial Market Perspective","abstract":"The Internet of Things (IoT) is a dynamic global information network consisting of Internet connected objects, such as radio frequency identifications, sensors, and actuators, as well as other instruments and smart appliances that are becoming an integral component of the Internet. Over the last few years, we have seen a plethora of IoT solutions making their way into the industry marketplace. Context-aware communications and computing have played a critical role throughout the last few years of ubiquitous computing and are expected to play a significant role in the IoT paradigm as well. In this paper, we examine a variety of popular and innovative IoT solutions in terms of context-aware technology perspectives. More importantly, we evaluate these IoT solutions using a framework that we built around well-known context aware computing theories. This survey is intended to serve as a guideline and a conceptual framework for context-aware product development and research in the IoT paradigm. It also provides a systematic exploration of existing IoT products in the marketplace and highlights a number of potentially significant research directions and trends.","keywords_author":["Internet of Things","industry solutions","context-awareness","product review","IoT marketplace"],"keywords_other":null,"max_cite":103.0,"pub_year":2014.0,"sources":"['ieee', 'wos']","rawkeys":["product review","industry solutions","internet of things","iot marketplace","context-awareness"],"tags":["industry solutions","context-aware","iot marketplace","internet of things (iot)","product reviews"]},{"p_id":45071,"title":"Enabling Machine Learning on Resource Constrained Devices by Source Code Generation of the Learned Models","abstract":"\u00a9 Springer International Publishing AG, part of Springer Nature 2018. Due to the development of IoT solutions, we can observe the constantly growing number of these devices in almost every aspect of our lives. The machine learning may improve increase their intelligence and smartness. Unfortunately, the highly regarded programming libraries consume to much resources to be ported to the embedded processors. Thus, in the paper the concept of source code generation of machine learning models is presented as well as the generation algorithms for commonly used machine learning methods. The concept has been proven in the use cases.","keywords_author":["Edge computing","IoT","Machine learning"],"keywords_other":["Source code generation","Programming library","Machine learning models","Resourceconstrained devices","Machine learning methods","Generation algorithm","Embedded processors"],"max_cite":0.0,"pub_year":2018.0,"sources":"['scp']","rawkeys":["machine learning methods","embedded processors","generation algorithm","machine learning models","machine learning","source code generation","resourceconstrained devices","edge computing","iot","programming library"],"tags":["machine learning methods","embedded processors","generation algorithm","machine learning models","machine learning","source code generation","resourceconstrained devices","edge computing","internet of things (iot)","programming library"]},{"p_id":12304,"title":"Development of Home Intelligent Fall Detection IoT System Based on Feedback Optical Flow Convolutional Neural Network","abstract":"Fall events are important health issues in elderly living environments such as homes. Hence, a confident and real-time video surveillance device that pays attention could better their everyday lives. We proposed an optical flow feedback convolutional neural network according to the video stream in a home environment. Our proposed model uses rule-based filters before an input convolutional layer and the recorded optical flow for supervising the optical flow of variation. Detecting human posture is a key factor, while fall events are like a falling posture. By sequencing frames of action, it is possible to recognize a fall. Our system can clearly detect the normal lying posture and lying after falling. Our proposed method can efficiently detect action motion and recognize the action posture. We compared the performance with other standard benchmark data sets and deployed our model to simulate a real-home situation, and the correct ratio achieved 82.7% and 98% separately.","keywords_author":["IoT","fall detection","convolutional neural network","optical flow"],"keywords_other":["MONITORING-SYSTEM","HUMAN ACTIVITY RECOGNITION","CARE","MODEL","RADAR","BODY","TIME","PREVENTION","SENSORS"],"max_cite":0.0,"pub_year":2018.0,"sources":"['ieee', 'wos']","rawkeys":["prevention","model","sensors","body","human activity recognition","time","care","fall detection","convolutional neural network","optical flow","iot","radar","monitoring-system"],"tags":["prevention","model","monitoring system","sensors","body","human activity recognition","time","care","fall detection","convolutional neural network","optical flow","radar","internet of things (iot)"]},{"p_id":30738,"title":"A methodological approach for assessing amplified reflection distributed denial of service on the internet of things","abstract":"\u00a9 2016 by the authors; licensee MDPI, Basel, Switzerland.Concerns about security on Internet of Things (IoT) cover data privacy and integrity, access control, and availability. IoT abuse in distributed denial of service attacks is a major issue, as typical IoT devices\u2019 limited computing, communications, and power resources are prioritized in implementing functionality rather than security features. Incidents involving attacks have been reported, but without clear characterization and evaluation of threats and impacts. The main purpose of this work is to methodically assess the possible impacts of a specific class\u2013amplified reflection distributed denial of service attacks (AR-DDoS)\u2013against IoT. The novel approach used to empirically examine the threat represented by running the attack over a controlled environment, with IoT devices, considered the perspective of an attacker. The methodology used in tests includes that perspective, and actively prospects vulnerabilities in computer systems. This methodology defines standardized procedures for tool-independent vulnerability assessment based on strategy, and the decision flows during execution of penetration tests (pentests). After validation in different scenarios, the methodology was applied in amplified reflection distributed denial of service (AR-DDoS) attack threat assessment. Results show that, according to attack intensity, AR-DDoS saturates reflector infrastructure. Therefore, concerns about AR-DDoS are founded, but expected impact on abused IoT infrastructure and devices will be possibly as hard as on final victims.","keywords_author":["Amplified reflection","Distributed denial of service","Pentest","Risk management","Vulnerability assessment"],"keywords_other":["Standardized procedure","Controlled environment","Vulnerability assessments","Distributed denial of service attack","Methodological approach","Pentest","Internet of Things (IOT)","Distributed denial of service"],"max_cite":4.0,"pub_year":2016.0,"sources":"['scp', 'wos']","rawkeys":["pentest","controlled environment","distributed denial of service attack","standardized procedure","amplified reflection","methodological approach","vulnerability assessments","distributed denial of service","vulnerability assessment","risk management","internet of things (iot)"],"tags":["pentest","controlled environment","distributed denial of service attack","amplified reflection","methodological approach","vulnerability assessments","distributed denial of service","standard procedures","risk management","internet of things (iot)"]},{"p_id":71702,"title":"A Novel Security Scheme Based on Instant Encrypted Transmission for Internet of Things","abstract":"Internet of Things (IoT) is a research field that has been continuously developed and innovated in recent years and is also an important driving force for the improvement of people's life in the future. There are lots of scenarios in IoT where we need to collaborate through devices to complete tasks; that is, a device sends data to other devices, and other devices operate on the aid of the data. These transmitted data are often users' privacy data, such as medical data and grid data. We propose an instant encrypted transmission based security scheme for such scenarios in IoT. The analysis in this paper indicates that our scheme can guarantee the security of users' data while ensuring rapid transmission and acquisition of instant IoT data.","keywords_author":null,"keywords_other":["IOT","PRIVACY","CHALLENGES"],"max_cite":0.0,"pub_year":2018.0,"sources":"['wos']","rawkeys":["iot","privacy","challenges"],"tags":["internet of things (iot)","privacy","challenges"]},{"p_id":49175,"title":"Towards building a bus travel time prediction model for Metro Manila","abstract":"\u00a9 2016 IEEE.Land Transportation Sector is one of the key sectors in the Philippine economy particularly in Metro Manila. With the rapid urbanization of the Philippines, The urban transport infrastructure is expected to experience pressures posing a major risk of urban transport degradation resulting into longer travel times, economic and productivity losses. In light of this, the Land Transportation Franchising and Regulatory Board (LTFRB) along with DOST-ASTI has initiated a project on implementing a bus management system for Public Utility Vehicles utilizing real time GPS location data. This study takes on establishing a travel time prediction for the buses given a specific route. The travel time estimation was performed using Extremely Randomize Trees, a supervised machine learning algorithm. The resulting prediction set had a correlation of determination score indicative of a good predictive performance for travel time prediction.","keywords_author":["Bus Management","Intelligent Transport System","Internet of Things","Machine Learning"],"keywords_other":["Travel time prediction","Predictive performance","Transportation sector","Bus travel time predictions","Bus management systems","Intelligent transport systems","Travel time estimation","Supervised machine learning"],"max_cite":0.0,"pub_year":2017.0,"sources":"['scp']","rawkeys":["bus travel time predictions","predictive performance","bus management systems","internet of things","machine learning","supervised machine learning","intelligent transport system","transportation sector","travel time estimation","travel time prediction","bus management","intelligent transport systems"],"tags":["bus travel time predictions","bus management systems","machine learning","supervised machine learning","travel-time prediction","intelligent transportation systems","transportation sector","travel time estimation","bus management","prediction performance","internet of things (iot)"]},{"p_id":59416,"title":"Big Data for Context Aware Computing - Perspectives and Challenges","abstract":"Big data has arrived. Myriad applications, systems generate data of humongous volumes, variety and velocity which traditional computing systems and databases are unable to manage. The proliferation of sensors in every possible device is also becoming one of the major generators of Big data. Of particular interest in this article is how context aware computing systems which derive context from data and act accordingly, deal with such huge amounts of data. Big industry players namely Google, Yahoo, and Amazon are already developing context aware applications using user data from emails, chat messages, browsing and shopping histories etc. For instance, Gmail reminds us of our flight schedule by understanding flight booking related content in our emails. Similarly, Amazon understands user preference and recommends items of interest to shop and so on. In this paper, we survey context aware computing systems from a Big data perspective. We first propose a taxonomy of existing work on the basis of sensing platforms and then discuss the latest developments in this field of Big data context aware systems focusingon how such systems deal with various Big data challenges. We conclude the paper with an insight on open research issues involving designing and developing context aware Big data generating systems. (C) 2017 Elsevier Inc. All rights reserved.","keywords_author":["Big data","Context Awareness","Smartphones","Sensors","Internet of Things","Wireless Sensor Networks"],"keywords_other":["HEALTH","THINGS","SMART CITIES","WEARABLE SENSORS","SYSTEMS","SERVICES","MOBILE PHONES","INTERNET"],"max_cite":0.0,"pub_year":2017.0,"sources":"['ieee', 'wos']","rawkeys":["mobile phones","wearable sensors","context awareness","big data","sensors","things","internet of things","smartphones","services","systems","wireless sensor networks","internet","health","smart cities"],"tags":["mobile phones","wearable sensors","context-aware","big data","sensors","things","smartphone","system","services","wireless sensor networks","internet","health","internet of things (iot)","smart cities"]},{"p_id":59426,"title":"Data-Driven Event Triggering for IoT Applications","abstract":"Event-triggering (ET) is an up-and-coming technological paradigm for monitoring, optimization, and control in the Internet of Things (IoT) that achieves improved levels of operational efficiency. This paper first defines the envisioned ET architecture for the IoT domain. It then classifies and reviews the various different ET approaches obtained from the available literature for the three phases of ET, namely behavior modeling, event detection, and event handling. Thereafter, a novel data-driven technique is developed to address all three phases of ET in an efficient and reliable manner. Finally, the applicability of the proposed data-driven technique is showcased in a real-world public transport scenario, demonstrating a substantial improvement in energy and spectrum efficiency compared to existing periodic techniques.","keywords_author":["Event-triggering (ET)","Internet of Things (IoT)","monitoring and control","operational models"],"keywords_other":["THINGS","BIG DATA ANALYTICS","CHALLENGES","INFORMATION","WIRELESS SENSOR NETWORKS","CONVEX-OPTIMIZATION","COMMUNICATION","SERVICES","NETWORKED CONTROL-SYSTEMS","INTERNET"],"max_cite":6.0,"pub_year":2016.0,"sources":"['ieee', 'wos']","rawkeys":["event-triggering (et)","networked control-systems","operational models","things","challenges","monitoring and control","communication","services","information","convex-optimization","wireless sensor networks","internet","big data analytics","internet of things (iot)"],"tags":["networked control-systems","operational models","convex optimization","monitor and control","things","challenges","communication","services","evapotranspiration","information","wireless sensor networks","internet","big data analytics","internet of things (iot)"]},{"p_id":26661,"title":"IoT as a applications: cloud-based building management systems for the internet of things","abstract":"\u00a9 2015, Springer Science+Business Media New York.Recently, excellent by Internet of Things (IoT), the era of connected everything device is coming. However, the devices hardly show the manner to autonomous connectivity on it and the self-cooperation for applied to real-world environments. In this paper, we proposed a smart building on IoT and cloud-based technology that can perform collaboration and efficient operation with various sensing devices in building and facilities. The smart building is very important to reduce on a huge amount of building energy is consumed by the management system of buildings. The proposed system selects an optimum device feature subset from the computing resources and storages by our cloud-based building management system. The performance of our proposed system is tested via experiments which verify that its measures are satisfactory.","keywords_author":["Building sensor data","Cloud","Internet of things","Machine learning","Real-time building energy forecasting"],"keywords_other":["Building management system","Building sensors","Management systems","Computing resource","Sensing devices","Building energy","Internet of Things (IOT)","Real world environments"],"max_cite":6.0,"pub_year":2016.0,"sources":"['scp']","rawkeys":["building sensors","real world environments","computing resource","sensing devices","internet of things","machine learning","real-time building energy forecasting","building energy","building management system","cloud","management systems","building sensor data","internet of things (iot)"],"tags":["building sensors","real world environments","sensing devices","machine learning","real-time building energy forecasting","building energy","building management system","computational resources","cloud","management systems","building sensor data","internet of things (iot)"]},{"p_id":47142,"title":"Impact evaluation and detection of malicious spoofing attacks on BLE based occupancy detection systems","abstract":"\u00a9 2017 ACM. Occupancy detection is beneficial for applications such as emergency management and building energy management, as it provides information on the location of occupants. Internet of Things (IoT) devices such as Bluetooth Low Energy (BLE) beacons installed in a building can benefit the performance of occupancy detection systems, by providing information on an occupant's location. However, BLE beacons operate by broadcasting advertisement messages, and this renders them vulnerable to network attacks. Here, we evaluate the effect of two types of malicious spoofing attacks on a BLE based occupancy detection system, and propose an attack detection method. The building blocks of the system include BLE beacons installed inside the building, a mobile application installed on occupants' phones, and a remote control server where we perform occupancy detection using machine learning. Our real-world experimental results indicate that the attacks can significantly affect the system's performance. Also, our proposed detection method is able to accurately detect an attack by an adversary with physical access, with accuracy ranging from 84% to 91%.","keywords_author":["Bluetooth low energy","IBeacon","Machine learning","Malicious spoofing attacks","Occupancy detection"],"keywords_other":["Spoofing attacks","IBeacon","Bluetooth low energies (BLE)","Emergency management","Occupancy detections","Bluetooth low energies (BTLE)","Building energy managements","Internet of Things (IOT)"],"max_cite":0.0,"pub_year":2017.0,"sources":"['scp']","rawkeys":["occupancy detections","bluetooth low energy","bluetooth low energies (btle)","spoofing attacks","machine learning","emergency management","malicious spoofing attacks","internet of things (iot)","ibeacon","occupancy detection","building energy managements","bluetooth low energies (ble)"],"tags":["occupancy detections","spoofing attacks","machine learning","bluetooth low energies (ble)","emergency management","malicious spoofing attacks","ibeacon","building energy managements","internet of things (iot)"]},{"p_id":59432,"title":"Big Data and Individual Privacy in the Age of the Internet of Things","abstract":"The availability of \"big data\" and \"smart\" products are credited with advancing solutions to complex problems in medicine, transportation, and education, among others. However, with big data comes big responsibility. The collection, storage, sharing, and analysis of data are far outpacing individual privacy protections, whether technological or legislative. The Internet of Things (IoT), with its promise to create networks of networks, will magnify individual data privacy threats. Recent data breaches, exposing the personal information of millions of users, provide insight into the vulnerability of personal data. Although seemingly expansive, there are core individual privacy issues that are central to current big data breaches and anticipated IoT threats. This article examines both big data and the IoT using examples of data privacy breaches to illustrate the impact of individual data loss. Furthermore, the article examines the complexity of tackling technological and legislative challenges in protecting individual privacy. It concludes by summarizing these issues in terms of the future implications of the IoT and the loss of privacy.","keywords_author":["cybersecurity","Internet of Things","IoT","big data","privacy","smart devices","data breaches"],"keywords_other":["MODEL","ISSUES","CHALLENGES"],"max_cite":1.0,"pub_year":2017.0,"sources":"['wos']","rawkeys":["cybersecurity","model","privacy","big data","internet of things","data breaches","challenges","smart devices","issues","iot"],"tags":["privacy","model","big data","data breaches","challenges","smart devices","issues","cyber security","internet of things (iot)"]},{"p_id":49192,"title":"Reinforcement of manufacturing industry competitiveness by utilizing robotics, IoT and AI","abstract":null,"keywords_author":["AI","Automatization","Deep learning","Intelligent robot","IoT","Robotization"],"keywords_other":null,"max_cite":0.0,"pub_year":2017.0,"sources":"['scp']","rawkeys":["intelligent robot","deep learning","robotization","ai","iot","automatization"],"tags":["machine learning","automatic","robotics","intelligent robots","internet of things (iot)"]},{"p_id":36910,"title":"Solar energy prediction for constrained IoT nodes based on public weather forecasts","abstract":"\u00a9 2017 ACM. Solar power is important for many scenarios of the Internet of Things (IoT). Resource-constrained devices depend on limited energy budgets to operate without degrading performance. Predicting solar energy is necessary for an efficient management and utilization of resources. While machine learning is already used to predict solar power for larger power plants, we examine how different machine learning methods can be used in a constrained sensor setting, based on easily available public weather data. The conducted evaluation resorts to commercial IoT hardware, demonstrating the feasibility of the proposed solution in a real deployment. Our results show that predicting solar energy is possible even with limited access to data, progressively improving as the system runs.","keywords_author":["Constrained nodes","Internet of Things","Machine learning","Solar energy","Weather forecasts"],"keywords_other":["Constrained sensors","Internet of thing (IOT)","Energy prediction","Efficient managements","Limited energies","Resourceconstrained devices","Constrained nodes","Machine learning methods"],"max_cite":1.0,"pub_year":2017.0,"sources":"['scp']","rawkeys":["machine learning methods","solar energy","efficient managements","constrained sensors","weather forecasts","internet of thing (iot)","internet of things","machine learning","energy prediction","constrained nodes","resourceconstrained devices","limited energies"],"tags":["machine learning methods","solar energy","efficient managements","weather forecasting","constrained sensors","machine learning","energy prediction","constrained nodes","resourceconstrained devices","limited energies","internet of things (iot)"]},{"p_id":47150,"title":"ADaPT: Optimizing CNN inference on IoT and mobile devices using approximately separable 1-D kernels","abstract":"\u00a9 2017 ACM. Breakthroughs from the field of deep learning are radically changing how sensor data are interpreted to extract important information to help advance healthcare, make our cities smarter, and innovate in smart home technology. Deep convolutional neural networks, which are at the heart of many emerging Internet-of-Things (IoT) applications, achieve remarkable performance in audio and visual recognition tasks, at the expense of high computational complexity in convolutional layers, limiting their deployability. In this paper, we present an easy-to-implement acceleration scheme, named ADaPT, which can be applied to already available pre-trained networks. Our pro\u00ac posed technique exploits redundancy present in the convolutional layers to reduce computation and storage requirements. Additionally, we also decompose each convolution layer into two consecutive one-dimensional stages to make full use of the approximate model. This technique can easily be applied to existing low power processors, GPUs or new accelerators. We evaluated this technique using four diverse and widely used benchmarks, on hardware ranging from embedded CPUs to server GPUs. Our experiments show an average 3-5x speed-up in all deep models and a maximum 8-9x speed-up on many individual convolutional layers. We demonstrate that unlike iterative pruning based methodology, our approximation technique is mathematically well grounded, robust, does not require any time-consuming retrain\u00ac ing, and still achieves speed-ups solely from convolutional layers with no loss in baseline accuracy.","keywords_author":["Convolutional neural networks","Deep learning","Hardware programming","Internet of things (IoT)","Sensors"],"keywords_other":["Approximation techniques","Hardware programming","Storage requirements","Low power processors","Internet of Things (IOT)","Convolutional neural network","Smart Home Technology","Deep convolutional neural networks"],"max_cite":0.0,"pub_year":2017.0,"sources":"['scp']","rawkeys":["approximation techniques","convolutional neural networks","sensors","deep learning","deep convolutional neural networks","smart home technology","low power processors","convolutional neural network","storage requirements","internet of things (iot)","hardware programming"],"tags":["approximation techniques","sensors","smart home technology","machine learning","low power processors","convolutional neural network","storage requirements","internet of things (iot)","hardware programming"]},{"p_id":36915,"title":"Self-Evolving Trading Strategy Integrating Internet of Things and Big Data","abstract":"IEEE In the era of Internet of things and big data, data has increased dramatically. Computers have been used in various fields. Algorithmic trading is beginning to develop rapidly in the trading market, more and more algorithms begin to be used in the transaction market. As a form of machine learning, neural network can fully reveal the complex trading market. Based on the characteristics of commodity futures market, this paper chooses BP neural network to establish price forecasting model. And then, according to the rules of futures market, a self-evolving commodity futures trading strategy is proposed. We also use the data of the Shanghai Futures Exchange and the Dalian Futures Exchange to back-testing the strategy. Finally, we compare the proposed strategies and traditional strategies, and illustrate the evolution of our strategy. Experiments show that our strategies are superior to other compared strategies in the proposed evaluation indicator. Our strategy has a good performance both in yield and risk. It also proves the feasibility of the model and the strategy. The research of this paper is significant to the research of the futures market, and it also provides a new idea for the application of machine learning in algorithmic trading.","keywords_author":["Algorithm design and analysis","Big Data","Big Data.","futures market","Hidden Markov models","Internet of Things","Internet of Things","Machine learning algorithms","neural network","Neural networks","Predictive models","trading strategy"],"keywords_other":["Predictive models","Algorithm design and analysis","Commodity futures","Algorithmic trading","Trading strategies","Evaluation indicators","BP neural networks","Price forecasting"],"max_cite":1.0,"pub_year":2017.0,"sources":"['scp']","rawkeys":["trading strategy","hidden markov models","neural network","bp neural networks","big data","neural networks","predictive models","machine learning algorithms","internet of things","algorithmic trading","price forecasting","trading strategies","commodity futures","evaluation indicators","algorithm design and analysis","futures market"],"tags":["futures markets","hidden markov models","bp neural networks","big data","neural networks","predictive models","machine learning algorithms","algorithmic trading","price forecasting","trading strategies","commodity futures","evaluation indicators","algorithm design and analysis","internet of things (iot)"]},{"p_id":47156,"title":"EXEHDA-RR: Machine learning and MCDA with semantic web in IoT resources classification","abstract":"\u00a9 2017 Association for Computing Machinery.Currently, a lot of resources are connected to the Internet, many simultaneously requesting and providing services. The adequate selection of resources that best meet the demands of users with a broad range of options has been a relevant and current research challenge. Based on the non-functional parameters of QoS play a significant role in the ranking of these resources according to the services they offer. This paper aims to aggregate machine learning in the pre-classification of EXEHDA middleware resources, to reduce the computational cost generated by MCDA algorithms. We presented the proposed software architecture (EXEHDA-RR), and the obtained results with the integration of machine learning in the classification process are promissing, and indicate to the research continuation.","keywords_author":["Internet of things","Machine learning","MCDA","Resource ranking"],"keywords_other":["Research challenges","Classification process","MCDA","Resource ranking","Non-functional","IoT resources","Computational costs"],"max_cite":0.0,"pub_year":2017.0,"sources":"['scp']","rawkeys":["research challenges","iot resources","non-functional","internet of things","machine learning","resource ranking","computational costs","classification process","mcda"],"tags":["research challenges","iot resources","multi-criteria decision analysis","non-functional","machine learning","resource ranking","computational costs","classification process","internet of things (iot)"]},{"p_id":47155,"title":"Self-evolvable knowledge-enhanced IoT data mobility for smart environment","abstract":"\u00a9 2017 Association for Computing Machinery. It has been a long time that the discussions regarding Internet of Things (IoT) have primarily focused on the communicative connectivity and infrastructure, while the data intelligence of IoT has not been paid enough attention to. However, with the growth of heterogeneous devices IoT introduce a pressure of massive amount of heterogeneous data, which makes it very important to explore the methods and tools to strengthen the IoT to intelligently deal with the incremental massive amounts of data. Towards that, this paper presents an IoT edge-based method to enable intelligent IoT entity connectivity for smart data provision, called smart data mobility. The presented method enables the IoT to perceive and learn from the environments, based on which to let the IoT entities interact with each other in a self-evolvable way for data sharing, in responding to the dynamically changing environments. The presented intelligence enablers for IoT can support smart services and digitalized functionalities from different domains and in different purposes, via strengthening the entities' connectivity with self-evolvable interaction relations to support the efficient and smart data exchanging.","keywords_author":["Artificial neural network","Big data","Data sharing","Internet of Things","Machine learning"],"keywords_other":["Data Sharing","Heterogeneous devices","Smart environment","Changing environment","Heterogeneous data","Different domains","Internet of Things (IOT)","Edge-based methods"],"max_cite":0.0,"pub_year":2017.0,"sources":"['scp']","rawkeys":["edge-based methods","big data","heterogeneous devices","changing environment","internet of things","different domains","machine learning","heterogeneous data","data sharing","artificial neural network","smart environment","internet of things (iot)"],"tags":["edge-based methods","big data","neural networks","changing environment","heterogeneous devices","machine learning","different domains","heterogeneous data","data sharing","smart environment","internet of things (iot)"]},{"p_id":26683,"title":"Semantic data extraction over MQTT for IoTcentric wireless sensor networks","abstract":"\u00a9 2016 IEEE. The emergence of the paradigm of Internet of Things (IoT) has necessitated the development of machine-to-machine (M2M) protocols geared towards wireless sensor network interfacing to the Internet and implementing machine learning algorithms over the cloud. This paper discusses the viability of the MQ Telemetry Transport (MQTT) protocol for such applications. This paper introduces MQTT along with its merits and demerits and suitability towards IoT applications. Then it outlines an implementation of a typical IoT application involving ubiquitous sensing, M2M communication, cloud computing and semantic data extraction. The results of this experiment are then analyzed. Finally, the paper looks at future improvements in the proposed architecture for widespread use.","keywords_author":["Data Extraction","Internet of Things","Machine Learning","Wireless Sensor Networks"],"keywords_other":["Semantic data extractions","Machine-to-machine (M2M)","IOT applications","Future improvements","M2m communications","Data extraction","Internet of Things (IOT)","Proposed architectures"],"max_cite":6.0,"pub_year":2016.0,"sources":"['scp']","rawkeys":["machine-to-machine (m2m)","data extraction","m2m communications","proposed architectures","internet of things","machine learning","semantic data extractions","future improvements","iot applications","internet of things (iot)","wireless sensor networks"],"tags":["data extraction","m2m communications","proposed architectures","semantic data extraction","machine learning","machine to machines","future improvements","iot applications","internet of things (iot)","wireless sensor networks"]},{"p_id":30780,"title":"Description and classification for facilitating interoperability of heterogeneous data\/events\/services in the Internet of Things","abstract":"\u00a9 2017 The Internet of Things (IoT) refers to an infrastructure that integrates things over standard wired\/wireless networks and allows them to exchange information with each other. The IoT is a very complex heterogeneous network, enabling seamless integration of these things is a huge challenge. A publish\/subscribe method of integration can be formulated to solve the problems of interconnecting billions of heterogeneous things. In our work, an IoT framework that uses an abstraction layer that decouples an application from the service calls and network interfaces is required to send and receive messages on a particular thing. This paper provides definitions and classifications for heterogeneous data\/events\/services according to the properties of the things in order to integrate them into a framework for description. Based on these definitions and classifications, heterogeneous data\/events\/services in the IoT were integrated via topic description through the Data Distribution Service (DDS) middleware standard for real-time publish\/subscribe. This paper also concludes with general remarks and a discussion of future work.","keywords_author":["Data Distribution Service (DDS)","Description","Internet of Things (IoT)","Interoperability","Topic"],"keywords_other":["Internet of thing (IOT)","Middleware standard","Topic","Method of integration","Description","Seamless integration","Data distribution services","Internet of Things (IOT)"],"max_cite":4.0,"pub_year":2017.0,"sources":"['scp']","rawkeys":["description","middleware standard","method of integration","topic","internet of thing (iot)","data distribution services","seamless integration","data distribution service (dds)","internet of things (iot)","interoperability"],"tags":["description","middleware standard","method of integration","seamless integration","data distribution service (dds)","topics","internet of things (iot)","interoperability"]},{"p_id":30781,"title":"CSF: Crowdsourcing semantic fusion for heterogeneous media big data in the internet of things","abstract":"\u00a9 2017 Elsevier B.V. With the rising popularity of social media in the context of environments based on the Internet of things (IoT), semantic information has emerged as an important bridge to connect human intelligence with heterogeneous media big data. As a critical tool to improve media big data retrieval, semantic fusion encounters a number of challenges: the manual method is inefficient, and the automatic approach is inaccurate. To address these challenges, this paper proposes a solution called CSF (Crowdsourcing Semantic Fusion) that makes full use of the collective wisdom of social users and introduces crowdsourcing computing to semantic fusion. First, the correlation of cross-modal semantics is mined and the semantic objects are normalized for fusion. Second, we employ the dimension reduction and relevance feedback approaches to reduce non-principal components and noise. Finally, we research the storage and distribution mechanism. Experiment results highlight the efficiency and accuracy of the proposed approach. The proposed method is an effective and practical cross-modal semantic fusion and distribution mechanism for heterogeneous social media, provides a novel idea for social media semantic processing, and uses an interactive visualization framework for social media knowledge mining and retrieval to improve semantic knowledge and the effect of representation.","keywords_author":["Big data","Crowdsourcing computing","Internet of things","Semantic fusion","Social media"],"keywords_other":["Distribution mechanism","Internet of thing (IOT)","Principal Components","Automatic approaches","Interactive visualizations","Social media","Semantic fusion","Semantic information"],"max_cite":4.0,"pub_year":2017.0,"sources":"['scp']","rawkeys":["principal components","big data","social media","interactive visualizations","internet of thing (iot)","internet of things","crowdsourcing computing","semantic fusion","semantic information","automatic approaches","distribution mechanism"],"tags":["principal components","big data","social media","interactive visualizations","crowdsourcing computing","semantic fusion","semantic information","automatic approaches","distribution mechanism","internet of things (iot)"]},{"p_id":36926,"title":"On the need of machine learning as a service for the internet of things","abstract":"\u00a9 2017 Copyright held by the owner\/author(s). Publication rights licensed to ACM. In recent years we are witnessing a rapid increase in the diffusion of the Internet of Things (IoT) technology, with a large scale adoption of interconnected heterogeneous devices that are pervasively collecting information through the interaction with humans in their environment. The adoption of Machine Learning (ML) methodolo\u00ac gies can play a fundamental role, allowing smarter IoT applications to continuously adapt to evolving environmental conditions and user's needs. In this context, the time is now ripe for a decisive step forward in the direction of a systematic integration of ML functionalities within the IoT platform. In this paper, we outline the principles that should guide the realization of a ML service for the IoT, proposing a conceptual architecture of such a learning service, integrated within the IoT reference model. Our proposal leverages on the experience of recent successful European initiatives that led to the realization of intelligent sensor networks built on the synergy between resource efficient ML models for temporal data processing and wireless sensor networks. The relevant impact of ML in applicative domains of interest for the IoT is also enucleated through a brief summary of recent results.","keywords_author":["Adaptive IoT applications","Distributed learning service","Intelligent sensor networks","Internet of things","Machine learning service"],"keywords_other":["Environmental conditions","IOT applications","Conceptual architecture","Heterogeneous devices","Diffusion of the internets","Intelligent sensor networks","Distributed learning","Systematic integration"],"max_cite":1.0,"pub_year":2017.0,"sources":"['scp']","rawkeys":["intelligent sensor networks","diffusion of the internets","heterogeneous devices","distributed learning service","internet of things","environmental conditions","conceptual architecture","machine learning service","distributed learning","iot applications","systematic integration","adaptive iot applications"],"tags":["intelligent sensor networks","diffusion of the internets","heterogeneous devices","distributed learning service","environmental conditions","conceptual architecture","internet of things (iot)","machine learning service","distributed learning","iot applications","systematic integration","adaptive iot applications"]},{"p_id":36927,"title":"Optimization of water consumption using dynamic quota based smart water management system","abstract":"\u00a9 2017 IEEE. Water is a vital resource for life and for the economy. Nowadays, one of the most serious challenges to solve is to manage water scarcity. As the importance of water usage optimization in monetary point of view is not that pronounced, we lack the incentive to invest in implementing technologically advanced systems for organized distribution of water. This paper describes the development of a meticulous water distribution system at a city level that will guarantee a continuous supply of water, overcoming some major issues like unaccounted supply to entities and Non-Revenue Water (NRW). A centralized control room equipped with a local computing machine and a Human Machine Interface (HMI) to monitor and control the city's water system is proposed. A smart tariff system should be exercised with an IoT-enabled mobile-friendly web portal developed for accessing various water usage statistics accompanied with an option of paying water bills online. In this volumetric, limit based model, the quota assigned to each entity is decided dynamically based on various supply and demand parameters including the availability of water with changing seasons. Adaptive learning through machine learning algorithms was used for the same. Unbilled, unauthorized consumption, apparent losses (water theft and metering inaccuracies) and transportation losses was curbed by monitoring from a remote location via IoT. Higher degree of theft and leakage was concluded using loss detection technique using the differential flow data. Here, a novel, cost-effective, realtime monitorable and controllable system is proposed with an analysis on a model simulation being performed for optimal water distribution.","keywords_author":["IoT","Machine Learning","NRW","quota","Smart water distribution"],"keywords_other":["Human Machine Interface","Controllable systems","Distribution of water","Monitor and control","Water management systems","Water distributions","Metering inaccuracies","quota"],"max_cite":1.0,"pub_year":2017.0,"sources":"['scp']","rawkeys":["controllable systems","distribution of water","water management systems","monitor and control","metering inaccuracies","machine learning","human machine interface","smart water distribution","iot","nrw","water distributions","quota"],"tags":["distribution of water","water management systems","monitor and control","metering inaccuracies","machine learning","human machine interface","smart water distribution","quota","nrw","water distributions","internet of things (iot)","control systems"]},{"p_id":45118,"title":"Generalization of algorithm recognition in RF side channels between devices","abstract":"\u00a9 2018 SPIE. All digital devices leak information through unintended emissions into analog side channels. The RF side channel enables passive collection of high-bandwidth information about the digital state of the device. We collected these RF emissions with a 500-MHz Riscure probe placed in the nearfield of the device under test (DuT) and applied machine learning to detect what program is running on the processor to identify malware intrusions. We explored the applicability of a generalized algorithm classification infrastructure built from a training set of similar DuTs to a similar device from a different production batch (same model number, different serial number.) We collected RF-SC data for five programs running on 28 distinct Arduino Unos (and 28 MSP430 processors.) We trained program classifiers on RF data from all but one DuT and tested the classifiers on the device withheld from the training set. The high-SNR signal provided by the Riscure probe enabled almost perfect classification results when we trained and tested on the same device. Our classification results remained above 99% when we generalized testing to the new DuT of the same model but a different serial number. The classifier was trained on 27 of the devices and tested to determine its ability to detect deviations from a baseline algorithm on a withheld device. The worst misclassification rate was a mere 0.08%.","keywords_author":["Arduino Uno","ATmega328P","Cross-device Classification","Internet of Everything","Internet of Things","Machine Learning","Side-channel Analysis"],"keywords_other":["Misclassification rates","Side-channel analysis","Classification results","Generalized algorithms","ATmega328P","Arduino","Applied machine learning","Device classifications"],"max_cite":0.0,"pub_year":2018.0,"sources":"['scp']","rawkeys":["internet of everything","generalized algorithms","misclassification rates","internet of things","arduino","arduino uno","applied machine learning","machine learning","atmega328p","side-channel analysis","cross-device classification","classification results","device classifications"],"tags":["generalized algorithms","misclassification rates","machine learning","arduino","arduino uno","applied machine learning","atmega328p","internet of things (iot)","side-channel analysis","cross-device classification","classification results","device classifications"]},{"p_id":6223,"title":"Sensing as a service model for smart cities supported by Internet of Things","abstract":"The world population is growing at a rapid pace. Towns and cities are accommodating half of the world's population thereby creating tremendous pressure on every aspect of urban living. Cities are known to have large concentration of resources and facilities. Such environments attract people from rural areas. However, unprecedented attraction has now become an overwhelming issue for city governance and politics. The enormous pressure towards efficient city management has triggered various Smart City initiatives by both government and private sector businesses to invest in information and communication technologies to find sustainable solutions to the growing issues. The Internet of Things (IoT) has also gained significant attention over the past decade. IoT envisions to connect billions of sensors to the Internet and expects to use them for efficient and effective resource management in Smart Cities. Today, infrastructure, platforms and software applications are offered as services using cloud technologies. In this paper, we explore the concept of sensing as a service and how it fits with the IoT. Our objective is to investigate the concept of sensing as a service model in technological, economical and social perspectives and identify the major open challenges and issues. \u00a9 Copyright 2013 John Wiley & Sons, Ltd.","keywords_author":null,"keywords_other":["Internet of thing (IOT)","Software applications","Information and Communication Technologies","Sustainable solution","Resource management","Cloud technologies","Internet of Things (IOT)","Social perspective"],"max_cite":272.0,"pub_year":2014.0,"sources":"['scp']","rawkeys":["cloud technologies","internet of thing (iot)","sustainable solution","social perspective","information and communication technologies","software applications","resource management","internet of things (iot)"],"tags":["cloud technologies","sustainable solution","social perspective","information and communication technologies","software applications","resource management","internet of things (iot)"]},{"p_id":47185,"title":"A review on IoT healthcare monitoring applications and a vision for transforming sensor data into real-time clinical feedback","abstract":"\u00a9 2017 IEEE. Ageing populations and the increase in chronic diseases all over the world demand efficient healthcare solutions for maintaining well-being of people. One strategy that has drawn significant research attention is a focus on remote health monitoring systems based on Internet of Things (IoT) technology. This concept can help decrease pressure on hospital systems and healthcare providers, reduce healthcare costs, and improve homecare especially for patients with chronic diseases and the elderly. This paper explores the use of IoT-based applications in medical field and proposes an IoT Tiered Architecture (IoTTA) towards an approach for transforming sensor data into real-time clinical feedback. This approach considers a range of aspects including sensing, sending, processing, storing, and mining and learning. Using this approach will help to develop useful and effective solutions for pursuing systems development in IoT healthcare applications. The result of the review found that the growth of IoT applications for healthcare is in areas of self-care, data mining, and machine learning.","keywords_author":["data mining","health monitoring systems","healthcare","Internet of Things","IoT","machine learning","self-care","telecare"],"keywords_other":["Telecare","Healthcare monitoring","Health monitoring system","Health care application","Remote health monitoring","Health care providers","Self-care","Internet of Things (IOT)"],"max_cite":0.0,"pub_year":2017.0,"sources":"['scp']","rawkeys":["data mining","healthcare","health care application","healthcare monitoring","health monitoring system","internet of things","machine learning","health monitoring systems","remote health monitoring","health care providers","iot","self-care","telecare","internet of things (iot)"],"tags":["data mining","healthcare","health care application","healthcare monitoring","health monitoring system","machine learning","remote health monitoring","self care","health care providers","telecare","internet of things (iot)"]},{"p_id":16471,"title":"A Practical Evaluation of Information Processing and Abstraction Techniques for the Internet of Things","abstract":"\u00a9 2015 IEEE. The term Internet of Things (IoT) refers to the interaction and communication between billions of devices that produce and exchange data related to real-world objects (i.e. things). Extracting higher level information from the raw sensory data captured by the devices and representing this data as machine-interpretable or human-understandable information has several interesting applications. Deriving raw data into higher level information representations demands mechanisms to find, extract, and characterize meaningful abstractions from the raw data. This meaningful abstractions then have to be presented in a human and\/or machine-understandable representation. However, the heterogeneity of the data originated from different sensor devices and application scenarios such as e-health, environmental monitoring, and smart home applications, and the dynamic nature of sensor data make it difficult to apply only one particular information processing technique to the underlying data. A considerable amount of methods from machine-learning, the semantic web, as well as pattern and data mining have been used to abstract from sensor observations to information representations. This paper provides a survey of the requirements and solutions and describes challenges in the area of information abstraction and presents an efficient workflow to extract meaningful information from raw sensor data based on the current state-of-the-art in this area. This paper also identifies research directions at the edge of information abstraction for sensor data. To ease the understanding of the abstraction workflow process, we introduce a software toolkit that implements the introduced techniques and motivates to apply them on various data sets.","keywords_author":["Data abstraction","Internet of Things (IoT)","machine-learning","semanticWeb","software tools"],"keywords_other":["Application scenario","Abstraction techniques","Information representation","Information processing technique","Higher-level information","Environmental Monitoring","Data abstraction","Internet of Things (IOT)"],"max_cite":36.0,"pub_year":2015.0,"sources":"['scp']","rawkeys":["higher-level information","semanticweb","information processing technique","application scenario","software tools","information representation","abstraction techniques","environmental monitoring","data abstraction","machine-learning","internet of things (iot)"],"tags":["higher-level information","semanticweb","information processing technique","application scenario","machine learning","software tools","information representation","abstraction techniques","environmental monitoring","data abstraction","internet of things (iot)"]},{"p_id":49239,"title":"Extracting knowledge from stream behavioural patterns","abstract":"Copyright \u00a9 2017 by SCITEPRESS - Science and Technology Publications, Lda. All rights reserved. The increasing number of small, cheap devices full of sensing capabilities lead to an untapped source of information that can be explored to improve and optimize several systems. Yet, as this number grows it becomes increasingly difficult to manage and organize all this new information. The lack of a standard context representation scheme is one of the main difficulties in this research area (Antunes et al., 2016b). With this in mind we propose a stream characterization model which aims to provide the foundations of a new stream similarity metric. Complementing previous work on context organization, we aim to provide an automatic organizational model without enforcing specific representations.","keywords_author":["Context awareness","IoT","M2M","Machine learning","Stream mining"],"keywords_other":["Stream mining","Organizational modeling","Context- awareness","Context representation","Similarity metrics"],"max_cite":0.0,"pub_year":2017.0,"sources":"['scp']","rawkeys":["stream mining","context awareness","context- awareness","context representation","machine learning","organizational modeling","similarity metrics","iot","m2m"],"tags":["stream mining","context-aware","machine learning","context representation","machine to machines","similarity metrics","internet of things (iot)","organizational modeling"]},{"p_id":77920,"title":"A Survey on Data Storage and Information Discovery in the WSANs- Based Edge Computing Systems","abstract":"In the post-Cloud era, the proliferation of Internet of Things (IoT) has pushed the horizon of Edge computing, which is a new computing paradigm with data processed at the edge of the network. As the important systems of Edge computing, wireless sensor and actuator networks (WSANs) play an important role in collecting and processing the sensing data from the surrounding environment as well as taking actions on the events happening in the environment. In WSANs, in-network data storage and information discovery schemes with high energy efficiency, high load balance and low latency are needed because of the limited resources of the sensor nodes and the real-time requirement of some specific applications, such as putting out a big fire in a forest. In this article, the existing schemes of WSANs on data storage and information discovery are surveyed with detailed analysis on their advancements and shortcomings, and possible solutions are proposed on how to achieve high efficiency, good load balance, and perfect real-time performances at the same time, hoping that it can provide a good reference for the future research of the WSANs-based Edge computing systems.","keywords_author":["Internet of Things","Edge computing","WSANs","data storage","information discovery"],"keywords_other":["REPLICATION","EFFICIENT DATA-COLLECTION","ACTOR NETWORKS","FRAMEWORK","MOBILE SINKS","WIRELESS SENSOR NETWORKS","LARGE-SCALE","PERFORMANCE","ACTUATOR NETWORKS","DATA-CENTRIC STORAGE"],"max_cite":1.0,"pub_year":2018.0,"sources":"['wos']","rawkeys":["performance","framework","information discovery","efficient data-collection","internet of things","actuator networks","replication","large-scale","actor networks","wsans","edge computing","data storage","data-centric storage","mobile sinks","wireless sensor networks"],"tags":["performance","framework","information discovery","efficient data-collection","actuator networks","replication","large-scale","actor networks","wsans","edge computing","data storage","data-centric storage","mobile sinks","wireless sensor networks","internet of things (iot)"]},{"p_id":36968,"title":"A wearable health monitoring system for posttraumatic stress disorder","abstract":"\u00a9 2017 Elsevier B.V. Posttraumatic stress disorder or PTSD, a mental disorder that ensues after experiencing a shocking, life-altering, and\/or traumatic event, has commonly afflicted veterans returning from military service. In this paper, we propose a system that monitors for any signs of nightmares, attempts to suppress them, or wakes the patient slowly if unsuccessful, thus improving the quality of life of those who suffer from nightmares related to PTSD. Consisting of a combination of temperature control, aromatherapy, and auditory therapy capabilities, the proposed innovation will be integrated in the form of a wearable device. The Internet of Things resource in the realm of home automation technology enables the affordable use of smart sensors as the foundation for the system. Wearable technology ties the individual into the countermeasure network and monitors for nightmare conditions. The system will have the capability of optimization through machine learning and remote management. Due to widespread diagnosis of PTSD in those exposed to trauma, the system has the potential to positively impact millions of people and reduce rates of depression and suicide.","keywords_author":["Internet of Things","Machine learning","Monitor","Posttraumatic stress disorder","Smart sensors","Suppress","Wake","Wearable"],"keywords_other":["Posttraumatic stress disorder","Wearable","Wearable health-monitoring systems","Remote management","Traumatic events","Suppress","Military services","Wearable devices"],"max_cite":1.0,"pub_year":2017.0,"sources":"['scp']","rawkeys":["wearable devices","monitor","internet of things","machine learning","wearable health-monitoring systems","military services","traumatic events","smart sensors","suppress","posttraumatic stress disorder","wearable","wake","remote management"],"tags":["wakefulness","wearable devices","suppression","machine learning","wearable health-monitoring systems","military services","posttraumatic-stress-disorder","traumatic events","wearables","smart sensors","remote management","internet of things (iot)","monitoring"]},{"p_id":43112,"title":"A semantic mechanism for Internet-of-Things (IoT) to implement intelligent interactions","abstract":"\u00a9 2018 IFIP. Intelligent interactions of IoT mean that devices can perform proper actions and communicate with each other to support specific application scenarios without human intervention. To do this, the biggest challenge is providing IoT objects with common knowledge and abilities to analyze data and to reason. In this paper, we present a semantic mechanism to realize intelligent interactions of IoT devices on account of the semantic knowledge and machine learning technology. Based on this mechanism, a three-layer IoT system named IoT-Book is built, and every action performed by a device is deduced through analyzing the environment data and personalized preferences of the user. Finally, a smart home use case is demonstrated, and our approach is proved to be effective.","keywords_author":["Intelligent interactions","Internet of Things","Machine learning","Semantic"],"keywords_other":["Application scenario","Environment data","Human intervention","Common knowledge","Machine learning technology","Intelligent interactions","Semantic knowledge","Internet of Things (IOT)"],"max_cite":0.0,"pub_year":2018.0,"sources":"['scp']","rawkeys":["environment data","application scenario","internet of things","machine learning","intelligent interactions","common knowledge","machine learning technology","human intervention","semantic","semantic knowledge","internet of things (iot)"],"tags":["environment data","semantics","application scenario","machine learning","intelligent interactions","common knowledge","machine learning technology","human intervention","semantic knowledge","internet of things (iot)"]},{"p_id":22635,"title":"Cloud-Assisted Data Fusion and Sensor Selection for Internet of Things","abstract":"\u00a9 2014 IEEE.The Internet of Things (IoT) is connecting people and smart devices on a scale that was once unimaginable. One major challenge for the IoT is to handle vast amount of sensing data generated from the smart devices that are resource-limited and subject to missing data due to link or node failures. By exploring cloud computing with the IoT, we present a cloud-based solution that takes into account the link quality and spatio-temporal correlation of data to minimize energy consumption by selecting sensors for sampling and relaying data. We propose a multiphase adaptive sensing algorithm with belief propagation (BP) protocol (ASBP), which can provide high data quality and reduce energy consumption by turning on only a small number of nodes in the network. We formulate the sensor selection problem and solve it using both constraint programming (CP) and greedy search. We then use our message passing algorithm (BP) for performing inference to reconstruct the missing sensing data. ASBP is evaluated based on the data collected from real sensors. The results show that while maintaining a satisfactory level of data quality and prediction accuracy, ASBP can provide load balancing among sensors successfully and preserves 80% more energy compared with the case where all sensor nodes are actively involved.","keywords_author":null,"keywords_other":["Constraint programming","Sensor selection problem","Spatiotemporal correlation","Internet of thing (IOT)","Message passing algorithm","Reduce energy consumption","Prediction accuracy","Belief propagation"],"max_cite":12.0,"pub_year":2016.0,"sources":"['scp']","rawkeys":["message passing algorithm","internet of thing (iot)","sensor selection problem","constraint programming","belief propagation","prediction accuracy","spatiotemporal correlation","reduce energy consumption"],"tags":["message passing algorithm","sensor selection problem","constraint programming","backpropagation","prediction accuracy","spatiotemporal correlation","reduce energy consumption","internet of things (iot)"]},{"p_id":43116,"title":"IoT edge computing in quick service restaurants","abstract":"\u00a9 2018 IFIP. Internet of Things (IoT) term has been hyped for years, and takes so many thing's places in everywhere and makes many things to become easier and remotely controllable with smart automations. Human role in such areas are about to be vanished and sensors, actuators, gateways take over the workloads from human-being by generating the values which are desired to be measured, and transferring them within the network. They also take decisions with some preset rules, artificial intelligence or machine learning methods on behalf of humans. Edge computing became a vital as there is a huge amount of requirement of low-latency, extra resources, network restrictions, loose connections, real-time decisions, etc. In quick service restaurants, many waste management and service optimizations are human or paper-based which contains pre-calculated or pre-simulated values. In this paper, we propose an IoT architecture for quick service restaurants and describe various edge computing applications including processing the sensor values, extracting meaningful information, providing data integrity and more importantly learning the data patterns to present predictions, create alerts, or make some intelligent decisions to provide waste minimization and service optimization.","keywords_author":["Edge Computing","Internet of Things (IoT)","Machine Learning","Quick Service Restaurant"],"keywords_other":["Computing applications","Real time decisions","Intelligent decisions","Waste minimization","Machine learning methods","Quick Service Restaurant","Service optimization","Internet of Things (IOT)"],"max_cite":0.0,"pub_year":2018.0,"sources":"['scp']","rawkeys":["machine learning methods","real time decisions","service optimization","waste minimization","computing applications","intelligent decisions","machine learning","edge computing","quick service restaurant","internet of things (iot)"],"tags":["machine learning methods","real time decisions","service optimization","waste minimization","computing applications","intelligent decisions","machine learning","edge computing","quick service restaurant","internet of things (iot)"]},{"p_id":22637,"title":"Towards fog-driven IoT eHealth: Promises and challenges of IoT in medicine and healthcare","abstract":"\u00a9 2017 Elsevier B.V. Internet of Things (IoT) offers a seamless platform to connect people and objects to one another for enriching and making our lives easier. This vision carries us from compute-based centralized schemes to a more distributed environment offering a vast amount of applications such as smart wearables, smart home, smart mobility, and smart cities. In this paper we discuss applicability of IoT in healthcare and medicine by presenting a holistic architecture of IoT eHealth ecosystem. Healthcare is becoming increasingly difficult to manage due to insufficient and less effective healthcare services to meet the increasing demands of rising aging population with chronic diseases. We propose that this requires a transition from the clinic-centric treatment to patient-centric healthcare where each agent such as hospital, patient, and services are seamlessly connected to each other. This patient-centric IoT eHealth ecosystem needs a multi-layer architecture: (1) device, (2) fog computing and (3) cloud to empower handling of complex data in terms of its variety, speed, and latency. This fog-driven IoT architecture is followed by various case examples of services and applications that are implemented on those layers. Those examples range from mobile health, assisted living, e-medicine, implants, early warning systems, to population monitoring in smart cities. We then finally address the challenges of IoT eHealth such as data management, scalability, regulations, interoperability, device\u2013network\u2013human interfaces, security, and privacy.","keywords_author":["Big data","eHealth","Fog Computing","Internet of Things"],"keywords_other":["Multi-layer architectures","Services and applications","Ehealth","Distributed environments","Population monitoring","Early Warning System","Healthcare services","Internet of Things (IOT)"],"max_cite":12.0,"pub_year":2018.0,"sources":"['scp']","rawkeys":["multi-layer architectures","healthcare services","big data","fog computing","services and applications","internet of things","distributed environments","ehealth","early warning system","population monitoring","internet of things (iot)"],"tags":["multi-layer architectures","population monitoring","healthcare services","big data","fog computing","services and applications","distributed environments","early warning system","e-health","internet of things (iot)"]},{"p_id":22641,"title":"Big IoT Data Analytics: Architecture, Opportunities, and Open Research Challenges","abstract":"\u00a9 2013 IEEE. Voluminous amounts of data have been produced, since the past decade as the miniaturization of Internet of things (IoT) devices increases. However, such data are not useful without analytic power. Numerous big data, IoT, and analytics solutions have enabled people to obtain valuable insight into large data generated by IoT devices. However, these solutions are still in their infancy, and the domain lacks a comprehensive survey. This paper investigates the state-of-The-Art research efforts directed toward big IoT data analytics. The relationship between big data analytics and IoT is explained. Moreover, this paper adds value by proposing a new architecture for big IoT data analytics. Furthermore, big IoT data analytic types, methods, and technologies for big data mining are discussed. Numerous notable use cases are also presented. Several opportunities brought by data analytics in IoT paradigm are then discussed. Finally, open research challenges, such as privacy, big data mining, visualization, and integration, are presented as future research directions.","keywords_author":["Big data","data analytics","distributed computing","Internet of Things","smart city"],"keywords_other":["Research challenges","Large data","State of the art","Future research directions","Internet of Things (IOT)","Data analytics"],"max_cite":12.0,"pub_year":2017.0,"sources":"['scp']","rawkeys":["research challenges","distributed computing","big data","data analytics","state of the art","internet of things","large data","future research directions","internet of things (iot)","smart city"],"tags":["research challenges","distributed computing","big data","data analytics","state of the art","large data","future research directions","internet of things (iot)","smart cities"]},{"p_id":32882,"title":"Use of IoT Technology to Drive the Automotive Industry from Connected to Full Autonomous Vehicles","abstract":"\u00a9 2016 The automotive industry has been around for quite some time and it has evolved ever since, but the major transformation that is happening now from vehicles driven by humans to vehicles driven by themselves will have a long term impact on society. Today's cars are already connected and have been connected for some time, since they can link to smartphones, offer emergency roadside assistance, register real-time traffic alerts etc., but this evolution is about to change. The automobile industry is on the brink of a revolution, to move to self-driving automobile industry, and the driving force behind this is the fast developing technology, the Internet of Things (IoT). IoT will transform the automobile industry and at the same time, the automobile industry will provide a big boost to IoT. The potential and the prospects of this technology is astonishing. This paper examines the market and technical trends towards Autonomous Vehicles, evolution stages from early cars to fully autonomous, the importance of IoT in driving this industry ecosystem, advantages and disadvantages of Autonomous Vehincles, key issues and challenges faced by the industry, standards activities around this industry and finally the deployment use cases. The focus of this paper is more based on an industrial push to identify issues and challenges of Autonomous Vehicles and less on any academic research activity. The intention of this paper is to bring these issues and challenges to the attention of IFAC technical committee and trigger some debate on the opportunities for IFAC research in international stability.","keywords_author":["5G","autonomous cars","autonomous vehicles","driverless cars","IoT","self-driving"],"keywords_other":["Internet of thing (IOT)","Autonomous car","Issues and challenges","Standards activities","Driverless cars","Technical committees","Self drivings","Autonomous Vehicles"],"max_cite":3.0,"pub_year":2016.0,"sources":"['scp', 'wos']","rawkeys":["driverless cars","self drivings","self-driving","standards activities","autonomous car","technical committees","internet of thing (iot)","autonomous cars","issues and challenges","iot","5g","autonomous vehicles"],"tags":["driverless cars","self drivings","standards activities","autonomous car","technical committees","issues and challenges","5g","internet of things (iot)","autonomous vehicles"]},{"p_id":32883,"title":"A context-aware recommender system framework for iot based interactive digital signage in urban space","abstract":"\u00a9 2016 ACM.Digital Signage (DS) is one of the popular IoT technologies deployed in the urban space. DS can provide wayfinding and urban information to city dwellers and convey targeted messaging and advertising to people approaching the DS. With the rise of the online-to-offline (O2O) mobile commerce, DS also become an important marketing tool in urban retailing. However, most digital signage systems today lack interactive feature and context-aware recommendation engine. Few interactive digital signage systems available today are also insufficient in engaging anonymous viewers and also not considering temporal interaction between viewer and DS system. To overcome the above challenges, this paper proposes a context-aware recommender system framework with novel temporal interaction scheme for IoT based interactive digital signage deployed in urban space to engage anonymous viewer. The results of experiments indicate that the proposed framework improves the advertising effectiveness for DS system deployed in public in urban space.","keywords_author":["Context-aware","Digital signage","Internet of Things (IoT)","Recommender system"],"keywords_other":["Interactive features","Advertising effectiveness","Context-aware recommender systems","Digital signage","Context-Aware","Interaction schemes","Context-aware recommendations","Internet of Things (IOT)"],"max_cite":3.0,"pub_year":2016.0,"sources":"['scp', 'wos']","rawkeys":["advertising effectiveness","context-aware","digital signage","interactive features","context-aware recommender systems","context-aware recommendations","interaction schemes","recommender system","internet of things (iot)"],"tags":["advertising effectiveness","context-aware","digital signage","interactive features","context-aware recommender systems","context-aware recommendations","interaction schemes","recommender systems","internet of things (iot)"]},{"p_id":47219,"title":"Energy efficient IoT framework for Smart Buildings","abstract":"\u00a9 2017 IEEE. The Internet of Things is a truly state-of-The-Art field that has the potential to significantly improve essential services like power saving [8], security, maintenance and monitoring. The implementation of analytics and corrective feedback processes in the event of anomalous data that needs addressing-such as a sharp temperature increase detected by the sensor in a room that is typically maintained at a low temperature, the air-conditioning running for a long time in a room that has no occupant detected, excessive noise detected by a sound sensor-among other things, are some of several ways in which the Internet of Things can be leveraged to help solve fundamental problems in various walks of life. We provide information on setting up and operating a comprehensive apparatus to accomplish these objectives-starting with setting up the desired test bed that has a resilient Data-Flow Path, retrieving the requisite values through the sensors[9], performing analytics on the data using RStudio to gain further insights on the relationship between the various parameters being measured and help set up a predictive model that could draw a contrast between the expected and actual values, thereby suggesting the extent of wastage of resources. Additionally, the GSM module of the Intel Galileo board is also usefully deployed to send out emergency alerts.","keywords_author":["Analytics","Intel Galileo","Internet of Things","Machine Learning","RStudio","Smart Building"],"keywords_other":["Temperature increase","Energy efficient","Analytics","Essential services","Predictive modeling","RStudio","GALILEO","Corrective feedbacks"],"max_cite":0.0,"pub_year":2017.0,"sources":"['scp']","rawkeys":["essential services","galileo","rstudio","predictive modeling","internet of things","machine learning","temperature increase","smart building","corrective feedbacks","intel galileo","energy efficient","analytics"],"tags":["essential services","smart buildings","galileo","predictive models","rstudio","machine learning","temperature increase","corrective feedbacks","intel galileo","energy efficiency","internet of things (iot)","analytics"]},{"p_id":45186,"title":"IOT wearable device for the safety and security of women and girl child","abstract":"\u00a9 IAEME Publication. The main objective for this work is to create a wearable IOT device for the security and shielding of women, girl children. This is accomplished by the examination of physiological signs in concurrence with body gestures. The signs are analyzed and body temperature is measured by galvanic skin resistance. This work deals with body temperature and stress andskin resistance and relationship between them. By applying the records, activities persons position is analyzed. The device make an analysis of skin resistance and body temperature to analyze the situation of the person.","keywords_author":["Body temperature","GSR","Human activity recognition","IoT","Machine learning"],"keywords_other":null,"max_cite":0.0,"pub_year":2018.0,"sources":"['scp']","rawkeys":["gsr","human activity recognition","machine learning","iot","body temperature"],"tags":["machine learning","human activity recognition","galvanic skin response","internet of things (iot)","body temperature"]},{"p_id":37002,"title":"A gamification framework for sensor data analytics","abstract":"\u00a9 2017 IEEE. The Internet of Things (IoT) enables connected objects to capture, communicate, and collect information over the network through a multitude of sensors, setting the foundation for applications such as smart grids, smart cars, and smart cities. In this context, large scale analytics is needed to extract knowledge and value from the data produced by these sensors. The ability to perform analytics on these data, however, is highly limited by the difficulties of collecting labels. Indeed, the machine learning techniques used to perform analytics rely upon data labels to learn and to validate results. Historically, crowdsourcing platforms have been used to gather labels, yet they cannot be directly used in the IoT because of poor human readability of sensor data. To overcome these limitations, this paper proposes a framework for sensor data analytics which leverages the power of crowdsourcing through gamification to acquire sensor data labels. The framework uses gamification as a socially engaging vehicle and as a way to motivate users to participate in various labelling tasks. To demonstrate the framework proposed, a case study is also presented. Evaluation results show the framework can successfully translate gamification events into sensor data labels.","keywords_author":["Crowdsourcing","Data Analytics","Gamification","Internet of Things","Machine Learning","Sensor Data"],"keywords_other":["Crowdsourcing platforms","Internet of thing (IOT)","Machine learning techniques","Evaluation results","Large-scale analytics","Sensor data","Data analytics","Gamification"],"max_cite":1.0,"pub_year":2017.0,"sources":"['scp']","rawkeys":["gamification","sensor data","data analytics","internet of thing (iot)","internet of things","crowdsourcing","large-scale analytics","machine learning","machine learning techniques","evaluation results","crowdsourcing platforms"],"tags":["gamification","sensor data","data analytics","machine learning techniques","machine learning","crowdsourcing","large-scale analytics","evaluation results","crowdsourcing platforms","internet of things (iot)"]},{"p_id":30859,"title":"Probabilistic Recovery of Incomplete Sensed Data in IoT","abstract":"IEEE Reliable data delivery in the Internet of Things (IoT) is very important in order to provide IoT-based services with the required quality. However, IoT data delivery may not be successful for different reasons, such as connection errors, external attacks, or sensing errors. This results in data incompleteness, which decreases the performance of IoT applications. In particular, the recovery of missing data among the massive sensed data of the IoT is so important that it should be solved. In this paper, we propose a probabilistic method to recover missing (incomplete) data from IoT sensors by utilizing data from related sensors. The main idea of the proposed method is to perform probabilistic matrix factorization (PMF) within the preliminary assigned group of sensors. Unlike previous PMF approaches, the proposed model measures the similarity in data among neighboring sensors and splits them into different clusters with a K-means algorithm. Simulation results show that the proposed PMF model with clustering outperforms support vector machine (SVM) and deep neural network (DNN) algorithms in terms of accuracy and root mean square error. By using normalized datasets, PMF shows faster execution time than SVM, and almost the same execution time as the DNN method. This proposed incomplete data-recovery approach is a promising alternative to traditional DNN and SVM methods for IoT telemetry applications.","keywords_author":["Clustering algorithms","Internet of Things","Internet of Things (IoT)","massive sensed data.","Probabilistic logic","probabilistic matrix factorization","recovery of missing sensor data","Sensor systems","Support vector machines","Wireless sensor networks"],"keywords_other":["massive sensed data","Sensor data","Sensor systems","Probabilistic matrix factorizations","Internet of Things (IOT)"],"max_cite":4.0,"pub_year":2017.0,"sources":"['scp']","rawkeys":["probabilistic matrix factorization","sensor data","probabilistic logic","probabilistic matrix factorizations","internet of things","recovery of missing sensor data","sensor systems","massive sensed data","support vector machines","clustering algorithms","wireless sensor networks","internet of things (iot)"],"tags":["sensor data","probabilistic logic","probabilistic matrix factorizations","sensor systems","recovery of missing sensor data","machine learning","massive sensed data","clustering algorithms","wireless sensor networks","internet of things (iot)"]},{"p_id":49291,"title":"Transparent authentication scheme with adaptive biometrie features for IoT networks","abstract":"\u00a9 2016 IEEE.With the comprehensive evolution of information communication technologies on mobile sensing objects, versatile ubiquitous networks embedded with specific-purpose sensors and intelligent wearable devices have promptly been developed and deployed, called the Internet of Things (IoT). On account of the popularity of IoT, the security issues have been promptly focused due to potential threats from IoT architectures. In consideration of the heterogeneous network property of IoT, in this paper we propose an authentication system which applies machine learning techniques to extract user bio-features as authentication tokens and transparently performs continual or real-time entity verification in the back-ground without the user's notices.","keywords_author":["Internet of Things (IoT)","machine learning","security","support vector machine","transparent authentication"],"keywords_other":["Internet of thing (IOT)","Authentication token","Information communication technology","Transparent authentication","security","Authentication systems","Internet of Things (IOT)","Machine learning techniques"],"max_cite":0.0,"pub_year":2016.0,"sources":"['scp']","rawkeys":["authentication token","information communication technology","internet of thing (iot)","authentication systems","machine learning","machine learning techniques","security","support vector machine","transparent authentication","internet of things (iot)"],"tags":["authentication token","information communication technology","machine learning techniques","machine learning","authentication systems","security","transparent authentication","internet of things (iot)"]},{"p_id":45197,"title":"A geo-based fine granularity air quality prediction using machine learning and internet-of-things","abstract":"\u00a9 ICST Institute for Computer Sciences, Social Informatics and Telecommunications Engineering 2018. As the development of economy and industry, air quality decreases as one of the exchanges of our achievements. Although air pollution has already been considered as a global and critical issue over the past decades, there has not been much innovation on the way people monitor and check the quality. Most of the air quality data today is provided by government or professional sensors set up in cities, which does not provide more detailed status in smaller geo locations with finer granularity, such as specific villages, schools, and shopping malls. In this project, we use machine learning to make a mathematical model which could be used to predict the air quality for small geo locations with accuracy and fine granularity. Through series of experiments and comparisons, the most accuracy mathematical model was found, which had a difference percentage less than 20% with the real data.","keywords_author":["Air quality prediction","Internet-of-Things","Machine learning"],"keywords_other":["Critical issues","Air quality prediction","Fine granularity","Geolocations","Air quality data"],"max_cite":0.0,"pub_year":2018.0,"sources":"['scp']","rawkeys":["air quality prediction","fine granularity","machine learning","critical issues","air quality data","geolocations","internet-of-things"],"tags":["air quality prediction","fine granularity","machine learning","critical issues","air quality data","geolocations","internet of things (iot)"]},{"p_id":4243,"title":"Heterogeneous visual features integration for image recognition optimization in internet of things","abstract":"\u00a9 2016 Elsevier B.V.Recently, a large number of physical devices, together with distributed information systems, deployed in internet of things (IoT), are collecting more and more images. Such collected images recognition poses an important challenge on optimization in internet of things. Specially, most of existing methods only adopt shallow learning models to integrate various features of images for recognition limiting classification accuracy. In this paper, we propose a multimodal deep learning (MMDL) approach to integrate heterogeneous visual features by considering each type of visual feature as one modality for image recognition optimization in internet of things. In our scheme, we extract the high-level abstraction of each modality by a stacked autoencoders. Furthermore, we design a back propagation algorithm with shared weights learned from a softmax layer to update the pretrained parameters of multiple stacked autoencoders simultaneously. The integration is performed by concatenating the last hidden layers of the multimodal stacked autoencoders architecture. Extensive experiments are carried out on three datasets i.e. Animal with Attributes, NUS-WIDE-OBJECT, and Handwritten Numerals, by comparison with SVM, SAE, and AMMSS. Results demonstrate that our scheme has superior performance on heterogeneous visual features integration for image recognition optimization in internet of things.","keywords_author":["Deep learning","Image classification","Internet of things","Multimodal integration optimization","Stacked autoencoders"],"keywords_other":["Deep learning","Handwritten numeral","High-level abstraction","Internet of Things (IOT)","Distributed information systems","Classification accuracy","Multimodal integration","Autoencoders"],"max_cite":0.0,"pub_year":2016.0,"sources":"['scp', 'ieee']","rawkeys":["handwritten numeral","distributed information systems","high-level abstraction","deep learning","classification accuracy","internet of things","multimodal integration optimization","stacked autoencoders","autoencoders","multimodal integration","image classification","internet of things (iot)"],"tags":["handwritten numeral","distributed information systems","high-level abstraction","classification accuracy","auto encoders","machine learning","multimodal integration optimization","stacked autoencoders","multimodal integration","image classification","internet of things (iot)"]},{"p_id":45217,"title":"Design of tensorflow-based proactive smart home managers","abstract":"\u00a9 Springer Nature Singapore Pte Ltd. 2018. In recent years, with IoT(Internet of Things) technology as the main focus, device operation and control technology in smart homes has been attracting considerable attention, and home IoT device management services are being provided by various companies, including communication companies. The smart home manager system manages smart devices used in homes, and it provides only the status value information and control function of the currently registered devices. Thus, unnecessary access procedures occur due to the characteristic of the smart home, which uses a smart device repeatedly for the same purpose. To resolve such shortcomings, in this paper, the Proactive Smart Home Manager has been designed, which can predict and suggest users the next steps to take by user usage pattern analysis and inference via machine learning.","keywords_author":["IoT","Logistic Classification","Machine learning","Smart home manager","TensorFlow"],"keywords_other":["Smart homes","TensorFlow","Smart devices","Control functions","Usage patterns","Device operations","Communication companies","Device management"],"max_cite":0.0,"pub_year":2018.0,"sources":"['scp']","rawkeys":["communication companies","control functions","device management","logistic classification","device operations","smart homes","tensorflow","machine learning","smart devices","usage patterns","iot","smart home manager"],"tags":["communication companies","control functions","device management","logistic classification","device operations","smart homes","tensorflow","machine learning","smart devices","usage patterns","smart home manager","internet of things (iot)"]},{"p_id":162,"title":"Software-Defined Networking for RSU Clouds in Support of the Internet of Vehicles","abstract":"We propose a novel roadside unit (RSU) cloud, a vehicular cloud, as the operational backbone of the vehicle grid in the Internet of Vehicles (IoV). The architecture of the proposed RSU cloud consists of traditional and specialized RSUs employing software-defined networking (SDN) to dynamically instantiate, replicate, and\/or migrate services. We leverage the deep programmability of SDN to dynamically reconfigure the services hosted in the network and their data forwarding information to efficiently serve the underlying demand from the vehicle grid. We then present a detailed reconfiguration overhead analysis to reduce reconfigurations, which are costly for service providers. We use the reconfiguration cost analysis to design and formulate an integer linear programming (ILP) problem to model our novel RSU cloud resource management (CRM). We begin by solving for the Pareto optimal frontier (POF) of nondominated solutions, such that each solution is a configuration that minimizes either the number of service instances or the RSU cloud infrastructure delay, for a given average demand. Then, we design an efficient heuristic to minimize the reconfiguration costs. A fundamental contribution of our heuristic approach is the use of reinforcement learning to select configurations that minimize reconfiguration costs in the network over the long term. We perform reconfiguration cost analysis and compare the results of our CRM formulation and heuristic. We also show the reduction in reconfiguration costs when using reinforcement learning in comparison to a myopic approach. We show significant improvement in the reconfigurations costs and infrastructure delay when compared to purist service installations.","keywords_author":["Internet of Vehicles","Cloud Resource Management","RSU Cloud","Intelligent Transportation Systems","Vehicular Ad hoc Networks","Software Defined Networking"],"keywords_other":["Internet of Vehicles (IoV)","myopic approach","Delays","Cloud computing","integer programming","reinforcement learning","IoV","Resource management","data forwarding information","Cloud resource management (CRM)","Computer architecture","heuristic programming","Vehicles","roadside unit CRM","intelligent transportation systems (ITS)","vehicular ad hoc networks (VANETs)","ILP problem","software defined networking","SDN","RSU cloud resource management","reconfiguration cost analysis","learning (artificial intelligence)","vehicular ad hoc networks","integer linear programming problem","linear programming","reconfiguration cost minimization","minimisation","Pareto optimal frontier","roadside unit (RSU) cloud","Internet of Things","Internet of Vehicles","road vehicles","cloud computing","Pareto optimisation","vehicular ad hoc network","Control systems","software radio","heuristic approach","vehicle grid","POF","software-defined networking (SDN)"],"max_cite":53.0,"pub_year":2014.0,"sources":"['ieee']","rawkeys":["myopic approach","rsu cloud resource management","internet of things","integer programming","intelligent transportation systems (its)","reinforcement learning","iov","ilp problem","data forwarding information","heuristic programming","internet of vehicles (iov)","rsu cloud","software defined networking","roadside unit (rsu) cloud","vehicles","computer architecture","resource management","control systems","reconfiguration cost analysis","software-defined networking (sdn)","vehicle grid","cloud resource management","learning (artificial intelligence)","cloud resource management (crm)","vehicular ad hoc networks","integer linear programming problem","delays","pof","roadside unit crm","reconfiguration cost minimization","minimisation","internet of vehicles","pareto optimal frontier","road vehicles","cloud computing","vehicular ad hoc networks (vanets)","pareto optimisation","intelligent transportation systems","vehicular ad hoc network","software radio","heuristic approach","linear programming","sdn"],"tags":["myopic approach","rsu cloud resource management","integer programming","reinforcement learning","ilp problem","data forwarding information","internet of things (iot)","heuristic programming","machine learning","rsu cloud","vehicles","roadside unit (rsu) cloud","computer architecture","resource management","control systems","reconfiguration cost analysis","vehicle grid","vehicular ad hoc networks","integer linear programming problem","delays","pof","roadside unit crm","reconfiguration cost minimization","minimisation","customer relationship management","internet of vehicles","pareto optimal frontier","road vehicles","cloud computing","software-defined networking","pareto optimisation","intelligent transportation systems","software radio","heuristic approach","linear programming"]},{"p_id":18594,"title":"Big data in manufacturing: a systematic mapping study","abstract":"\u00a9 2015, O\u2019Donovan et al.The manufacturing industry is currently in the midst of a data-driven revolution, which promises to transform traditional manufacturing facilities in to highly optimised smart manufacturing facilities. These smart facilities are focused on creating manufacturing intelligence from real-time data to support accurate and timely decision-making that can have a positive impact across the entire organisation. To realise these efficiencies emerging technologies such as Internet of Things (IoT) and Cyber Physical Systems (CPS) will be embedded in physical processes to measure and monitor real-time data from across the factory, which will ultimately give rise to unprecedented levels of data production. Therefore, manufacturing facilities must be able to manage the demands of exponential increase in data production, as well as possessing the analytical techniques needed to extract meaning from these large datasets. More specifically, organisations must be able to work with big data technologies to meet the demands of smart manufacturing. However, as big data is a relatively new phenomenon and potential applications to manufacturing activities are wide-reaching and diverse, there has been an obvious lack of secondary research undertaken in the area. Without secondary research, it is difficult for researchers to identify gaps in the field, as well as aligning their work with other researchers to develop strong research themes. In this study, we use the formal research methodology of systematic mapping to provide a breadth-first review of big data technologies in manufacturing.","keywords_author":["Big data","Big data analytics","Big data systems","Cyber physical systems","Distributed computing","Engineering informatics","Industry 4.0","Internet of things, loT","Machine learning","Manufacturing","Smart manufacturing"],"keywords_other":null,"max_cite":23.0,"pub_year":2015.0,"sources":"['scp']","rawkeys":["distributed computing","cyber physical systems","big data","engineering informatics","industry 4.0","internet of things","big data systems","machine learning","manufacturing","lot","smart manufacturing","big data analytics"],"tags":["cyber-physical systems","distributed computing","big data system","big data","engineering informatics","industry 4.0","machine learning","manufacturing","lot","smart manufacturing","big data analytics","internet of things (iot)"]},{"p_id":57517,"title":"Cyber-physical-social-thinking modeling and computing for geological information service system","abstract":"The serious geological hazards occurred frequently in the last few years. They have inflicted heavy casualties and property losses. Hence, it is necessary to design a geological information service system to analyze and evaluate geological hazards. With the development of computer and Internet service model, it is now possible to obtain rich data and process the data with some advanced computing techniques under network environment. Then, some technologies, including cyber-physical system, Internet of Things, and cloud computing, have been used in geological information management. Furthermore, the concept of cyber-physical-social-thinking as a broader vision of the Internet of Things was presented through the fusion of those advanced computing technologies. Motivated by it, in this article, a novel modeling and computing method for geological information service system is developed in consideration of the complex data processing requirement of geological service under dynamic environment. Specifically, some key techniques of modeling the information service system and computing geological data via cyber-physical system and Internet of Things are analyzed. Moreover, to show the efficiency of proposed method, two application cases are provided during the cyber-physical-social-thinking modeling and computing for geological information service system.","keywords_author":["Cyber-physical-social-thinking","Internet of Things","geological data processing","cloud computing"],"keywords_other":["NETWORKS","THINGS","INTEGRATION","CPSS"],"max_cite":1.0,"pub_year":2016.0,"sources":"['wos']","rawkeys":["integration","cloud computing","things","cyber-physical-social-thinking","internet of things","geological data processing","networks","cpss"],"tags":["cyber-physical systems","integration","cloud computing","things","cyber-physical-social-thinking","geological data processing","networks","internet of things (iot)"]},{"p_id":37049,"title":"HiCH: Hierarchical fog-assisted computing architecture for healthcare IoT","abstract":"\u00a9 2017 ACM. The Internet of Things (IoT) paradigm holds significant promises for remote health monitoring systems. Due to their life- or mission-critical nature, these systems need to provide a high level of availability and accuracy. On the one hand, centralized cloud-based IoT systems lack reliability, punctuality and availability (e.g., in case of slow or unreliable Internet connection), and on the other hand, fully outsourcing data analytics to the edge of the network can result in diminished level of accuracy and adaptability due to the limited computational capacity in edge nodes. In this paper, we tackle these issues by proposing a hierarchical computing architecture, HiCH, for IoT-based health monitoring systems. The core components of the proposed system are 1) a novel computing architecture suitable for hierarchical partitioning and execution of machine learning based data analytics, 2) a closed-loop management technique capable of autonomous system adjustments with respect to patient's condition. HiCH benefits from the features offered by both fog and cloud computing and introduces a tailored management methodology for healthcare IoT systems. We demonstrate the efficacy of HiCH via a comprehensive performance assessment and evaluation on a continuous remote health monitoring case study focusing on arrhythmia detection for patients suffering from CardioVascular Diseases (CVDs).","keywords_author":["Fog computing","Hierarchical computing","Internet of things","Machine learning","MAPE-K","Remote patient monitoring"],"keywords_other":["Internet of thing (IOT)","Health monitoring system","Remote health monitoring","Management methodologies","Mape","Comprehensive performance assessments","Hierarchical partitioning","Hierarchical computing"],"max_cite":1.0,"pub_year":2017.0,"sources":"['scp']","rawkeys":["mape","fog computing","hierarchical computing","mape-k","health monitoring system","hierarchical partitioning","internet of thing (iot)","internet of things","machine learning","remote health monitoring","remote patient monitoring","comprehensive performance assessments","management methodologies"],"tags":["fog computing","hierarchical computing","mape-k","health monitoring system","hierarchical partitioning","machine learning","management methodologies","mean absolute percentage error","remote health monitoring","remote patient monitoring","comprehensive performance assessments","internet of things (iot)"]},{"p_id":30907,"title":"An Ingestion and Analytics Architecture for IoT Applied to Smart City Use Cases","abstract":"\u00a9 2017 IEEE. As sensors are adopted in almost all fields of life, the Internet of Things (IoT) is triggering a massive influx of data. We need efficient and scalable methods to process this data to gain valuable insight and take timely action. Existing approaches which support both batch processing (suitable for analysis of large historical data sets) and event processing (suitable for real-time analysis) are complex. We propose the hut architecture, a simple but scalable architecture for ingesting and analyzing IoT data, which uses historical data analysis to provide context for real-time analysis. We implement our architecture using open source components optimized for Big Data applications and extend them, where needed. We demonstrate our solution on two real-world smart city use cases in transportation and energy management.","keywords_author":["Big data","complex event processing (CEP)","context-aware","energy management","ingestion","Internet of Things (IoT)","machine learning","smart cities","spark","transportation"],"keywords_other":["Internet of thing (IOT)","Batch production","Big data applications","Scalable architectures","Open-source components","Real time analysis","Context-Aware","Complex event processing"],"max_cite":3.0,"pub_year":2018.0,"sources":"['scp']","rawkeys":["open-source components","energy management","big data","context-aware","smart cities","internet of thing (iot)","machine learning","scalable architectures","ingestion","spark","complex event processing","batch production","big data applications","complex event processing (cep)","transportation","internet of things (iot)","real time analysis"],"tags":["open-source components","energy management","context-aware","big data","smart cities","machine learning","scalable architectures","ingestion","spark","complex event processing","batch production","communication","big data applications","internet of things (iot)","real time analysis"]},{"p_id":30916,"title":"Cyber-physical system enabled nearby traffic flow modelling for autonomous vehicles","abstract":"\u00a9 2017 IEEE. We propose a nearby traffic flow modelling solution based on built-in Cyber-Physical System (CPS) sensors of autonomous vehicles. Our goal is to enhance the offline route planning and driving decision adjustment based on the first-hand traffic information, especially during poor Internet connection moments. Specifically, our model helps to select the optimal speed on a road, the optimal distance for timing to brake, and the safe distance from other vehicles to keep. Moreover, our model can also assist neighboring autonomous vehicles by communicating required information through Ad-Hoc network communications or through a centralized cloud. In detail, we first focus on the unique characteristic of traffic flow (such as traffic rule, avoid collision behaviours), and then build a comprehensive model to handle multiple scenarios. Technically, our model uses density functions of velocities, the differential equation of traffic flows, and the traffic viscosity with information collected from the traffic flow, the distances between vehicles, the amount and density of vehicle, the instant velocity, the speed limit, and the momentum to analysis the the driving scene. We evaluate our model with real traffic data collected by in-vehicle CPS sensors to the proposed nearby traffic flow model. Results show that our work can accurately conduct offline estimation on nearby traffic signal influence, and reveal the correlations among velocity, density and (spatial and temporal) location to adjust route during runtime.","keywords_author":["Autonomous Vehicles","Big Data","Cloud Computing","Cyber-Physical System","Intelligent Transportation","Internet of Things","Machine Learning","Traffic Flow Modeling"],"keywords_other":["Cyber-Physical System (CPS)","Internet connection","Traffic flow modeling","Traffic flow modelling","Intelligent transportation","Comprehensive model","Autonomous Vehicles","Traffic information"],"max_cite":3.0,"pub_year":2018.0,"sources":"['scp']","rawkeys":["cloud computing","big data","traffic information","internet of things","machine learning","comprehensive model","traffic flow modeling","cyber-physical system","cyber-physical system (cps)","traffic flow modelling","internet connection","intelligent transportation","autonomous vehicles"],"tags":["cyber-physical systems","traffic flow models","cloud computing","big data","traffic information","machine learning","comprehensive model","internet connection","intelligent transportation","internet of things (iot)","autonomous vehicles"]},{"p_id":51401,"title":"The internet of things, big data, machine learning, and the lift & Escalator industry","abstract":"New technologies such as the Internet of Things, Big Data, Cloud Computing and Machine Learning have the potential to radically change the Lift and Escalator Industry. This is particularly true in the areas of lift and escalator maintenance, product development, and quality. Lift and escalator maintenance has evolved over the years. The various forms of maintenance have included breakdown maintenance, preventive maintenance, usage based maintenance, condition based maintenance, and task based maintenance. Using the Internet of Things, Cloud Computing, Big Data, and Machine Learning, a new form of maintenance, Data Driven Maintenance, has arrived. Data Driven Maintenance provides benefits to building owners, building managers, lift and escalator passengers, and lift companies. What these new technologies are and how they apply to the lift industry is explained. Additionally, several real world applications of these technologies on lifts are detailed.","keywords_author":["Big data","Cloud computing","Internet of Things (IoT)","Machine learning"],"keywords_other":null,"max_cite":0.0,"pub_year":2015.0,"sources":"['scp']","rawkeys":["machine learning","cloud computing","internet of things (iot)","big data"],"tags":["machine learning","cloud computing","internet of things (iot)","big data"]},{"p_id":37066,"title":"Data mining in IoT data analysis for a new paradigm on the internet","abstract":"\u00a9 2017 ACM. This paper provides an overview on Data Mining (DM) technologies for the Internet of Things (IoT). IoT has become an active area of research, since IoT promises among other to improve quality of live and safety in Smart Cities, to make resource supply and waste management more efficient, and optimize traffic. DM is highly domain specific and depends on what is being mined for. For instance, if IoT is used to optimize traffic in a Smart City to reduce traffic jams and to find parking spaces quicker, different types of data needs to be collected and analysed from an eHealth solution, where IoT is used in a Smart Home to monitor the well being of patients or elderly people. IoT connects things that can collect numeric data from smart sensors, streaming data from cameras or route information on maps. Depending on the type of data, different techniques need to be adopted to analyse them. Also, many IoT applications analyse data from different devices and correlate them to make predictions about possible machine failures in production sites or looming emergency situations in Smart Buildings in a home security application. DM techniques need to handle the heterogeneity of IoT data, the large volumes of data and the speed at which they are produced. This paper explores the state of the art DM techniques for IoT.","keywords_author":["Data mining","Internet of things","Machine learning","Predictive analytics","Smart city"],"keywords_other":["Route information","Internet of thing (IOT)","Production sites","E-health solutions","IOT applications","State of the art","Emergency situation","Domain specific"],"max_cite":1.0,"pub_year":2017.0,"sources":"['scp']","rawkeys":["data mining","e-health solutions","predictive analytics","internet of thing (iot)","internet of things","machine learning","emergency situation","state of the art","domain specific","iot applications","production sites","route information","smart city"],"tags":["data mining","e-health solutions","predictive analytics","state of the art","machine learning","emergency situation","domain specific","route information","iot applications","production sites","internet of things (iot)","smart cities"]},{"p_id":6348,"title":"Next generation 5G wireless networks: A comprehensive survey","abstract":"\u00a9 1998-2012 IEEE.The vision of next generation 5G wireless communications lies in providing very high data rates (typically of Gbps order), extremely low latency, manifold increase in base station capacity, and significant improvement in users' perceived quality of service (QoS), compared to current 4G LTE networks. Ever increasing proliferation of smart devices, introduction of new emerging multimedia applications, together with an exponential rise in wireless data (multimedia) demand and usage is already creating a significant burden on existing cellular networks. 5G wireless systems, with improved data rates, capacity, latency, and QoS are expected to be the panacea of most of the current cellular networks' problems. In this survey, we make an exhaustive review of wireless evolution toward 5G networks. We first discuss the new architectural changes associated with the radio access network (RAN) design, including air interfaces, smart antennas, cloud and heterogeneous RAN. Subsequently, we make an in-depth survey of underlying novel mm-wave physical layer technologies, encompassing new channel model estimation, directional antenna design, beamforming algorithms, and massive MIMO technologies. Next, the details of MAC layer protocols and multiplexing schemes needed to efficiently support this new physical layer are discussed. We also look into the killer applications, considered as the major driving force behind 5G. In order to understand the improved user experience, we provide highlights of new QoS, QoE, and SON features associated with the 5G evolution. For alleviating the increased network energy consumption and operating expenditure, we make a detail review on energy awareness and cost efficiency. As understanding the current status of 5G implementation is important for its eventual commercialization, we also discuss relevant field trials, drive tests, and simulation experiments. Finally, we point out major existing research issues and identify possible future research directions.","keywords_author":["5G","beamforming","C-RAN","channel model","D2D","field trials","HetNets","IDMA","IoT","M2M","massive MIMO","mm-wave","QoE","SDMA","SDN","SON","sustainability"],"keywords_other":["Hetnets","Mm waves","IDMA","Field trial","Channel model"],"max_cite":253.0,"pub_year":2016.0,"sources":"['ieee', 'scp', 'wos']","rawkeys":["channel model","mm-wave","sustainability","hetnets","sdma","mm waves","field trials","son","d2d","c-ran","idma","qoe","massive mimo","iot","5g","field trial","beamforming","sdn","m2m"],"tags":["heterogeneous network","sdma","channel model","field trial","self-organizing network","d2d","software-defined networking","quality of experience (qoe)","beamforming","idma","mm-wave","machine to machines","massive mimo","sustainability","5g","internet of things (iot)","cran"]},{"p_id":45261,"title":"Machine learning for Internet of Things data analysis: a survey","abstract":"\u00a9 2018 Chongqing University of Posts and Telecommunications Rapid developments in hardware, software, and communication technologies have facilitated the emergence of Internet-connected sensory devices that provide observations and data measurements from the physical world. By 2020, it is estimated that the total number of Internet-connected devices being used will be between 25 and 50 billion. As these numbers grow and technologies become more mature, the volume of data being published will increase. The technology of Internet-connected devices, referred to as Internet of Things (IoT), continues to extend the current Internet by providing connectivity and interactions between the physical and cyber worlds. In addition to an increased volume, the IoT generates big data characterized by its velocity in terms of time and location dependency, with a variety of multiple modalities and varying data quality. Intelligent processing and analysis of this big data are the key to developing smart IoT applications. This article assesses the various machine learning methods that deal with the challenges presented by IoT data by considering smart cities as the main use case. The key contribution of this study is the presentation of a taxonomy of machine learning algorithms explaining how different techniques are applied to the data in order to extract higher level information. The potential and challenges of machine learning for IoT data analytics will also be discussed. A use case of applying a Support Vector Machine (SVM) to Aarhus smart city traffic data is presented for a more detailed exploration.","keywords_author":["Internet of Things","Machine learning","Smart City","Smart data"],"keywords_other":null,"max_cite":0.0,"pub_year":2018.0,"sources":"['scp']","rawkeys":["smart data","internet of things","machine learning","smart city"],"tags":["smart data","machine learning","internet of things (iot)","smart cities"]},{"p_id":12492,"title":"CS-CNN: Enabling Robust and Efficient Convolutional Neural Networks Inference for Internet-of-Things Applications","abstract":"Recent advances have shown that convolutional neural networks (CNNs) perform excellent in the tasks of image classification and face recognition when the size of data sets is sufficiently large, i.e., over hundreds of thousands training images. Nevertheless, when public data sets are not suitable for training the model for new application scenarios, it is painful to obtain sufficient training examples, especially when the samples have to be labeled manually. Besides, training and inference using CNNs requires significant resources of energy, computation, and memory usage. Therefore, implanting deep CNN models trained and executed on high performance GPU clusters to resource constrained devices, i.e., Internet of Things (IoT) devices, which have permeated into every aspect of modern life, is not appropriate and impractical. Compression technology is an important and popularly used tool to accelerate the training and inference of the CNN models. In this paper, we aim for a step forward in this area: we propose a new compressedCNNmodel termed CS-CNN for image classification by incorporating the theory of compressive sensing at the input layer of the CNN models to both reduce the resources consumption (evaluated as computation time in this paper) and a required number of training samples. According to our extensive evaluations on the multiple public data sets for deep learning tasks, e.g., MINST and CIFAR-10, using different metrics, we illustrate that the CS-CNN is able to speed up the process of training and inference by a factor of magnitude. Meanwhile, it achieves higher classification accuracy compared with the traditional large CNN models when the size of training database is small.","keywords_author":["Convolutional neural network","compressive sensing","singular value decomposition","IoT","image classification"],"keywords_other":["FACE RECOGNITION","SPARSE REPRESENTATION","CLASSIFICATION","OVERCOMPLETE DICTIONARIES"],"max_cite":0.0,"pub_year":2018.0,"sources":"['ieee', 'wos']","rawkeys":["overcomplete dictionaries","singular value decomposition","sparse representation","face recognition","compressive sensing","classification","convolutional neural network","iot","image classification"],"tags":["singular value decomposition","sparse representation","face recognition","compressive sensing","classification","convolutional neural network","over-complete dictionaries","image classification","internet of things (iot)"]},{"p_id":49361,"title":"Augmented Data Center Infrastructure Management System for Minimizing Energy Consumption","abstract":"\u00a9 2016 IEEE.The significant continuous increase in the power consumption of data centers, brought about by the adoption of cloud computing, is a major social problem. Moreover, the demand for data centers is expected to grow owing to the increasing requirement for data processing related to Internet of Things (IoT) applications. Consequently, reducing the power consumption of data centers has become an urgent and major issue. Modification of the operational settings of equipment residing in a data center leads to fluctuations in the power consumption of other equipment due to their complex interdependencies. Therefore, to reduce the total power consumption of the data center, it is not sufficient to reduce the power consumption of each piece of equipment, but one should consider the dependencies among the data center equipment. In other words, a coordinated control of the operational settings accounting for interdependencies among equipment is required to reduce the total power consumption of data centers. In this study, a data center energy management system focusing on the coordinated control of equipment in a data center is proposed. In the proposed system, the operational settings are determined using power consumption prediction that is based on the machine learning methods. We design a data center control system focusing on the task assignment of servers and air conditioner settings, and this system determines their settings based on a genetic algorithm. The proposed system is evaluated by experiments in our data center testbed and the evaluation results indicate that the proposed system reduces the total power consumption of the data center.","keywords_author":["Coordinated control system","Data center energy management system","Genetic algorithm","Machine learning"],"keywords_other":["Coordinated control system","Infrastructure management system","Evaluation results","Machine learning methods","Co-ordinated control","Data centers","Internet of Things (IOT)","Total power consumption"],"max_cite":0.0,"pub_year":2016.0,"sources":"['scp']","rawkeys":["machine learning methods","infrastructure management system","total power consumption","coordinated control system","machine learning","genetic algorithm","evaluation results","data center energy management system","data centers","internet of things (iot)","co-ordinated control"],"tags":["machine learning methods","infrastructure management system","total power consumption","coordinated control system","machine learning","genetic algorithm","evaluation results","data center energy management system","data centers","internet of things (iot)","co-ordinated control"]},{"p_id":43218,"title":"Artificial intelligence meets large-scale sensing: Using Large-Area Electronics (LAE) to enable intelligent spaces","abstract":"\u00a9 2018 IEEE. The tremendous value artificial intelligence (AI) is showing across a broad range of applications is driving it from cyber-systems to systems pervading every aspect of our lives. But real-world data challenges the efficiency and robustness with which AI systems of today can perform, due to the highly dynamic and noisy scenarios they face. While algorithmic solutions are required, this paper also explores technological solutions based on large-scale sensing. Specifically, Large-Area Electronics (LAE) is a technology that can make large-scale, form-fitting sensors possible for broad deployment in our lives. System-design principles, architectural approaches, supporting circuits, and underlying technological concerns surrounding LAE and its use in emerging systems for intelligent sensing are explored.","keywords_author":["Artificial intelligence","flexible electronics","Internet of Things","large-area electronics","machine learning"],"keywords_other":["Intelligent sensing","Algorithmic solutions","Architectural approach","Large-area electronics","Intelligent spaces","Technological solution","Data challenges","Design Principles"],"max_cite":0.0,"pub_year":2018.0,"sources":"['scp', 'ieee']","rawkeys":["data challenges","artificial intelligence","technological solution","design principles","algorithmic solutions","architectural approach","flexible electronics","internet of things","intelligent spaces","machine learning","intelligent sensing","large-area electronics"],"tags":["data challenges","technological solution","design principles","algorithmic solutions","architectural approach","machine learning","flexible electronics","intelligent spaces","intelligent sensing","large-area electronics","internet of things (iot)"]},{"p_id":43226,"title":"Simultaneously ensuring smartness, security, and energy efficiency in Internet-of-Things sensors","abstract":"\u00a9 2018 IEEE. Internet-of-Things (IoT) sensors have begun generating zettabytes of sensitive data, thus posing significant design challenges: limited bandwidth, insufficient energy, and security flaws. Due to their inherent trade-offs, these design challenges have not yet been simultaneously addressed. We propose a novel way out of this predicament by employing signal compression, machine learning inference, and cryptographic techniques on the IoT sensor node. Our approach not only enables the IoT system to push signal processing and decision-making to the extreme of the edge-side (i.e., the sensor node), but also solves data security and energy efficiency problems simultaneously. Experimental results on six different IoT applications indicate that relative to traditional sense-and-transmit sensors, IoT sensor energy can be reduced by 77.8\u00d7 for electrocardiogram (ECG) sensor based arrhythmia detection, 808.6\u00d7 for freezing of gait detection in the context of Parkinson's disease, 162.8\u00d7 for neural prosthesis spike sorting, 37.6\u00d7 for human activity classification, 368.4\u00d7 for electroencephalogram (EEG) sensor based seizure detection, and 12.9\u00d7 for chemical gas classification.","keywords_author":["Classification","compression","cryptographic techniques","edge-side layer","encryption and hashing","energy efficiency","inference","Internet-of-Things","machine learning","safety","security","sensor node","smartness"],"keywords_other":["smartness","security","inference","Cryptographic techniques","edge-side layer"],"max_cite":0.0,"pub_year":2018.0,"sources":"['scp']","rawkeys":["compression","smartness","machine learning","safety","security","cryptographic techniques","energy efficiency","classification","inference","sensor node","edge-side layer","encryption and hashing","internet-of-things"],"tags":["codes","machine learning","safety","security","cryptographic techniques","energy efficiency","classification","inference","sensor node","edge-side layer","encryption and hashing","smart","internet of things (iot)"]},{"p_id":45277,"title":"Feature selection and machine learning based multilevel stress detection from ECG signals","abstract":"\u00a9 Springer International Publishing AG 2018. Physiological sensor analytics aims at monitoring health as the availability of sensor-enabled portable, wearable, and implantable devices become ubiquitous in the growing Internet of Things (IoT). Physiological multi-sensor studies have been conducted previously to detect stress. In this study, we focus on electrocardiography (ECG) monitoring that can now be performed with minimally invasive wearable patches and sensors, to develop an efficient and robust mechanism for accurate stress identification, for example in automobile drivers. A unique aspect of our research is personalized individual stress analysis including three stress levels: low, medium and high. Using machine learning algorithms from the ECG signals alone, our system achieves up to 100% accuracy and area under ROC curve of 1 depending on the experimental setting in detecting three classes of stress using feature selection from a combination of fiducial points and multiscale entropy as a fine-grained indicator of stress level.","keywords_author":["Data mining","ECG","Machine learning","Sensors","Stress medicine"],"keywords_other":["Physiological sensors","Area under roc curve (AUC)","Multi-scale entropies","Stress detection","Implantable devices","Robust mechanisms","Minimally invasive","Internet of Things (IOT)"],"max_cite":0.0,"pub_year":2018.0,"sources":"['scp']","rawkeys":["stress medicine","data mining","ecg","robust mechanisms","sensors","stress detection","implantable devices","machine learning","physiological sensors","minimally invasive","multi-scale entropies","area under roc curve (auc)","internet of things (iot)"],"tags":["stress medicine","data mining","ecg","robust mechanisms","sensors","stress detection","implantable devices","machine learning","physiological sensors","minimally invasive","multi-scale entropies","area under roc curve (auc)","internet of things (iot)"]},{"p_id":26851,"title":"A comprehensive study of parameters in physical environment that impact students' focus during lecture using Internet of Things","abstract":"\u00a9 2015 Elsevier Ltd.Abstract We describe and analyze the impact of several parameters of the physical environment in a classroom on students' focus, where the term \"focus\" refers to the students' subjective feeling of their ability to concentrate on a lecture at a given moment. The primary goal is to identify those parameters that significantly affect students' focus during the lectures. We had measured several parameters in a real classroom environment using different low-cost smart devices. The research is based on the dataset collected from 14 recorded lectures attended by 197 students. We had measured five parameters of the physical environment and extracted 22 features from the lecturer's voice. After analyzing collected measurements, we had identified eight parameters that have shown to have statistically different values for \"focused\" and \"not focused\" segments. We used obtained dataset to test different classifiers and their ability to correctly classify \"focused\" against \"not focused\" segments of the lectures. We found out that AdaBoost M1 classifier had the best overall recognition accuracy (86.78%). After performing additional series of trials we identified three parameters that could be removed from the original dataset without changing classifier's accuracy, which left us five uncorrelated parameters that have shown to have significant impact on students' focus.","keywords_author":["Classification","Internet of Things","Machine learning","Pattern recognition","Smart classroom","Student's focus"],"keywords_other":["Smart classroom","Three parameters","Five parameters","Smart devices","Low costs","Recognition accuracy","Classroom environment","Physical environments"],"max_cite":6.0,"pub_year":2015.0,"sources":"['scp']","rawkeys":["physical environments","low costs","smart classroom","internet of things","machine learning","smart devices","recognition accuracy","classroom environment","classification","pattern recognition","five parameters","student's focus","three parameters"],"tags":["physical environments","low costs","smart classroom","machine learning","smart devices","recognition accuracy","classroom environment","classification","pattern recognition","five parameters","student's focus","three parameters","internet of things (iot)"]},{"p_id":22766,"title":"Evolving privacy: From sensors to the Internet of Things","abstract":"\u00a9 2017 Elsevier B.V.The Internet of Things (IoT) envisions a world covered with billions of smart, interacting things capable of offering all sorts of services to near and remote entities. The benefits and comfort that the IoT will bring about are undeniable, however, these may come at the cost of an unprecedented loss of privacy. In this paper we look at the privacy problems of one of the key enablers of the IoT, namely wireless sensor networks, and analyse how these problems may evolve with the development of this complex paradigm. We also identify further challenges which are not directly associated with already existing privacy risks but will certainly have a major impact in our lives if not taken into serious consideration.","keywords_author":["Challenges","Internet of Things","Privacy","WSN"],"keywords_other":["Privacy problems","Internet of thing (IOT)","Remote entity","Privacy risks","Challenges"],"max_cite":12.0,"pub_year":2017.0,"sources":"['scp']","rawkeys":["privacy","privacy risks","internet of thing (iot)","internet of things","challenges","remote entity","wsn","privacy problems"],"tags":["privacy","privacy risks","challenges","remote entity","internet of things (iot)","wireless sensor networks","privacy problems"]},{"p_id":24818,"title":"Spendthrift: Machine learning based resource and frequency scaling for ambient energy harvesting nonvolatile processors","abstract":"\u00a9 2017 IEEE.Batteryless energy harvesting systems face a twofold challenge in converting incoming energy into forward progress. Not only must such systems contend with inherently weak and fluctuating power sources, but they have very limited temporal windows for capitalizing on transitory periods of above-average power. To maximize forward progress, such systems should aggressively consume energy when it is available, rather than optimizing for peak averagecase efficiency. However, there are multiple ways that a processor can trade between consumption and performance. In this paper, we examine two approaches, frequency scaling and resource scaling, and develop a predictor-driven scheme for dynamically allocating future power budgets between the two techniques. We show that our solution can achieve forward progress equal to 2.08X of the baseline Out-of-Order (OoO) processor with the best static configuration of frequency and resources. The combined technique outperforms either technique in isolation, with frequency-only and resource-only approaches achieving 1.43X and 1.61X forward progress improvements, respectively.","keywords_author":["Energy harvesting","Internet of Things","Machine learning","Nonvolatile processor","Power-adaptive microarchitecture"],"keywords_other":["Energy harvesting systems","Combined techniques","Incoming energy","Micro architectures","Temporal windows","Non-volatile","Power sources","Frequency-scaling"],"max_cite":8.0,"pub_year":2017.0,"sources":"['scp']","rawkeys":["internet of things","machine learning","power-adaptive microarchitecture","incoming energy","non-volatile","micro architectures","nonvolatile processor","energy harvesting systems","combined techniques","temporal windows","frequency-scaling","power sources","energy harvesting"],"tags":["machine learning","power-adaptive microarchitecture","incoming energy","non-volatile","micro architectures","nonvolatile processor","energy harvesting systems","combined techniques","power sources","temporal windows","frequency-scaling","internet of things (iot)","energy harvesting"]},{"p_id":45299,"title":"Register Hamming distance from side channels","abstract":"\u00a9 2018 SPIE. We applied machine learning to detect changes in state of key registers in digital devices from their analog RF emissions. As digital devices operate, they emit information via analog side channels. We collected the RF side channel with a 500-MHz shielded loop probe from Riscure, placed in the nearfield (<1mm) of the device under test (DuT). We investigated a number of Internet-of-Thing (IoT) DuTs including Arduino Uno and PIC24 processors. Conventional processors implement instructions as a sequence of subtasks. The first subtasks include incrementing the program counter (PC) register and fetching the next instruction from program memory to the instruction register (IR). These two subtasks occur almost every instruction cycle. We ran programs on the DuT and collected the RF emissions. We parsed the object code of the programs to determine the state of key registers including the PC and IR during each instruction cycle and observed that the RF signal of each cycle is strongly correlated with the Hamming Distance (HD) (i.e., the number of bits changing) in the PC and IR registers. Based on this result, we developed classifiers to extract the HD of the PC, IR, as well as the stack pointer (SP). The classification results vary with true HD as some values are rare and have few examples in the training set. The classification accuracy exceeds 99% for the PC and the IR. Due to the relatively few HD in the training set for the SP, its results slightly exceeded 97%.","keywords_author":["Arduino Uno","ATmega328P","Hamming Distance","Instruction Register","Internet of Everything","Internet of Things","Machine Learning","Program Counter","Reverse Engineering","Side-Channel Analysis","Stack Pointer"],"keywords_other":["Side-channel analysis","ATmega328P","Program counters","Arduino","Stack pointers","Instruction register"],"max_cite":0.0,"pub_year":2018.0,"sources":"['scp']","rawkeys":["internet of everything","stack pointers","instruction register","hamming distance","internet of things","arduino","arduino uno","machine learning","program counters","atmega328p","program counter","side-channel analysis","stack pointer","reverse engineering"],"tags":["stack pointers","instruction register","hamming distance","machine learning","arduino","arduino uno","program counters","atmega328p","side-channel analysis","reverse engineering","internet of things (iot)"]},{"p_id":47348,"title":"A 0.42V high bandwidth synthesizable parallel access smart memory fabric for computer vision","abstract":"\u00a9 2017 IEEE. We present a design of a 2 to 12 port scalable multiport compiler with simultaneous read port access and closely packed graphics integration capability specially designed for low power high bandwidth, low latency stream vector processors and machine learning applications. Novel pipe-lined decoder and bitline repeater insertion helps to achieve a fast cycle time. Memory words can be accessed in different ways, serial, parallel or mixed. A wide supply range from 0.4V to 1.1V is supported without any complex write or read assist circuit. Design is non-self-timed and fully testable while timing and power views are generated through a static timing analysis (STA) approach. Layout is based on automatic place and route of standard cells in periphery and full custom standard cell compatible high density memory core. Full custom core is tightly bound with the common graphics processing operations, to enable low latency (< 1\u03bcs), high bandwidth operations at low voltage. Hybrid approach reduces the turn around time to just a few man weeks. Area penalty of a 2W2R 64 Kbit instance is up to 10% in comparison to a logic rule based full custom high speed 1W1R compiler, while doubling the throughput. Compared to complete RTL based synthesis approach, area is just 5% for 64 Kbit. A 2W2R 32\u00d7128 testchip instance in sub-20nm FinFET process, runs up-to 3 GHz on CAD at 1.1 V supply at -40 \u00b0 C. While measured speed of same instance on silicon is 86 MHz (at 0.42 V) for simultaneous access from both the ports and energy consumed is just 5 pJ\/cycle in typical process corner. Architecture is scalable up to 64KB for more parallel architectures (64 cores) as demanded in ultra-high definition real time computational photography [1].","keywords_author":["Cell based design","Chip multi-processor","Data-Path","Energy Efficiency","Internet of Things","machine learning","Neuromorphic Computing","Register File","vector processors"],"keywords_other":["Register files","Data paths","Neuromorphic computing","Vector processors","Cell-based design","Chip multi-processors"],"max_cite":0.0,"pub_year":2017.0,"sources":"['scp']","rawkeys":["data paths","chip multi-processors","cell-based design","cell based design","internet of things","machine learning","data-path","register file","chip multi-processor","register files","vector processors","neuromorphic computing","energy efficiency"],"tags":["cell based design","machine learning","data-path","chip multiprocessor","register files","vector processors","neuromorphic computing","energy efficiency","internet of things (iot)"]},{"p_id":39158,"title":"Virtual Things for machine learning applications","abstract":"Copyright 2014 ACM. Internet-of-Things (IoT) devices, especially sensors are producing large quantities of data that can be used for gathering knowledge. In this field, machine learning technologies are increasingly used to build versatile data-driven models. In this paper, we present a novel architecture able to execute machine learning algorithms within the sensor network, presenting advantages in terms of privacy and data transfer efficiency. We first argument that some classes of machine learning algorithms are compatible with this approach, namely based on the use of generative models that allow a distribution of the computation on a set of nodes. We then detail our architecture proposal, leveraging on the use of Web-of-Things technologies to ease integration into networks. The convergence of machine learning generative models and Web-of-Things paradigms leads us to the concept of virtual things exposing higher level knowledge by exploiting sensor data in the network. Finally, we demonstrate with a real scenario the feasibility and performances of our proposal.","keywords_author":["Machine learning","Sensor network","Web-of-Things"],"keywords_other":["Generative model","Novel architecture","Transfer efficiency","Internet of Things (IOT)","Machine learning technology","Sensor data","Machine learning applications","Data-driven model"],"max_cite":1.0,"pub_year":2014.0,"sources":"['scp']","rawkeys":["novel architecture","generative model","sensor data","sensor network","machine learning applications","data-driven model","machine learning","machine learning technology","web-of-things","transfer efficiency","internet of things (iot)"],"tags":["novel architecture","generative model","sensor data","machine learning applications","data-driven model","machine learning","machine learning technology","web of things","sensor networks","transfer efficiency","internet of things (iot)"]},{"p_id":49399,"title":"A design of IoT device similarity vector based workflow management system","abstract":"\u00a9 2016 IEEE.Nowadays, versatile IoT Devices are connected through the Internet and provide numerous dynamic services. However, until now, these kinds of services are just following established service or application rules. For this reason, an application cannot deal with a condition when rule is not previously set. Additionally, rules should be renewed by reflecting each condition, and software should be re-developed which are costly and time-consuming. To address this issue, we suggest a machine learning and semantic information based IoT devices management system.","keywords_author":["Internet of Things","Machine Learning","Semantic Web"],"keywords_other":["Similarity vectors","Workflow management systems","Dynamic services","Management systems","OR applications","Semantic information"],"max_cite":0.0,"pub_year":2016.0,"sources":"['scp']","rawkeys":["management systems","workflow management systems","dynamic services","similarity vectors","internet of things","machine learning","semantic information","semantic web","or applications"],"tags":["management systems","workflow management systems","dynamic services","similarity vectors","machine learning","semantic information","semantic web","or applications","internet of things (iot)"]},{"p_id":14588,"title":"A Survey on Internet of Things: Architecture, Enabling Technologies, Security and Privacy, and Applications","abstract":"\u00a9 2014 IEEE. Fog\/edge computing has been proposed to be integrated with Internet of Things (IoT) to enable computing services devices deployed at network edge, aiming to improve the user's experience and resilience of the services in case of failures. With the advantage of distributed architecture and close to end-users, fog\/edge computing can provide faster response and greater quality of service for IoT applications. Thus, fog\/edge computing-based IoT becomes future infrastructure on IoT development. To develop fog\/edge computing-based IoT infrastructure, the architecture, enabling techniques, and issues related to IoT should be investigated first, and then the integration of fog\/edge computing and IoT should be explored. To this end, this paper conducts a comprehensive overview of IoT with respect to system architecture, enabling technologies, security and privacy issues, and present the integration of fog\/edge computing and IoT, and applications. Particularly, this paper first explores the relationship between cyber-physical systems and IoT, both of which play important roles in realizing an intelligent cyber-physical world. Then, existing architectures, enabling technologies, and security and privacy issues in IoT are presented to enhance the understanding of the state of the art IoT development. To investigate the fog\/edge computing-based IoT, this paper also investigate the relationship between IoT and fog\/edge computing, and discuss issues in fog\/edge computing-based IoT. Finally, several applications, including the smart grid, smart transportation, and smart cities, are presented to demonstrate how fog\/edge computing-based IoT to be implemented in real-world applications.","keywords_author":["Applications","enabling technologies","fog\/edge computing","internet of Things (IoT)","security and privacy"],"keywords_other":["Enabling techniques","Enabling technologies","Security and privacy issues","System architectures","Distributed architecture","Security and privacy","Existing architectures","Internet of Things (IOT)"],"max_cite":57.0,"pub_year":2017.0,"sources":"['ieee', 'scp', 'wos']","rawkeys":["existing architectures","enabling techniques","system architectures","security and privacy issues","applications","security and privacy","fog\/edge computing","distributed architecture","internet of things (iot)","enabling technologies"],"tags":["existing architectures","enabling techniques","system architectures","security and privacy issues","applications","security and privacy","fog\/edge computing","distributed architecture","internet of things (iot)","enabling technologies"]},{"p_id":33020,"title":"Numerical encoding to Tame SQL injection attacks","abstract":"\u00a9 2016 IEEE. Recent years have seen an astronomical rise in SQL Injection Attacks (SQLIAs) used to compromise the confidentiality, authentication and integrity of organisations' databases. Intruders becoming smarter in obfuscating web requests to evade detection combined with increasing volumes of web traffic from the Internet of Things (IoT), cloud-hosted and on-premise business applications have made it evident that the existing approaches of mostly static signature lack the ability to cope with novel signatures. A SQLIA detection and prevention solution can be achieved through exploring an alternative bio-inspired supervised learning approach that uses input of labelled dataset of numerical attributes in classifying true positives and negatives. We present in this paper a Numerical Encoding to Tame SQLIA (NETsQlIA) that implements a proof of concept for scalable numerical encoding of features to a dataset attributes with labelled class obtained from deep web traffic analysis. In the numerical attributes encoding: the model leverages proxy in the interception and decryption of web traffic. The intercepted web requests are then assembled for front-end SQL parsing and pattern matching by applying traditional Non-Deterministic Finite Automaton (NFA). This paper is intended for a technique of numerical attributes extraction of any size primed as an input dataset to an Artificial Neural Network (ANN) and statistical Machine Learning (ML) algorithms implemented using Two-Class Averaged Perceptron (TCAP) and Two-Class Logistic Regression (TCLR) respectively. This methodology then forms the subject of the empirical evaluation of the suitability of this model in the accurate classification of both legitimate web requests and SQLIA payloads.","keywords_author":["NETSQLIA","numerical attributes encoding","SQL Injection","SQLIA","SQLIA neurons"],"keywords_other":["Numerical attributes","SQL injection","Supervised learning approaches","Internet of thing (IOT)","Statistical machine learning","NETSQLIA","Nondeterministic finite automaton","SQLIA"],"max_cite":3.0,"pub_year":2016.0,"sources":"['ieee', 'scp', 'wos']","rawkeys":["numerical attributes encoding","numerical attributes","nondeterministic finite automaton","internet of thing (iot)","netsqlia","sql injection","sqlia neurons","statistical machine learning","supervised learning approaches","sqlia"],"tags":["numerical attributes encoding","numerical attributes","nondeterministic finite automaton","sqlia neurons","netsqlia","sql injection","statistical machine learning","supervised learning approaches","sqlia","internet of things (iot)"]},{"p_id":37118,"title":"A Machine Learning Decision-Support System Improves the Internet of Things' Smart Meter Operations","abstract":"\u00a9 2014 IEEE. An Internet of Things' (IoT) connected society and system represents a tremendous paradigm shift. We present a framework for a decision-support system (DSS) that operates within the IoT ecosystem. The DSS leverages advanced analytics of electric smart meter (ESM) network communication-quality data to improve cost predictions for smart meter field operations and provide actionable decision recommendations regarding whether to send a technician to a customer location to resolve an ESM issue. The model is empirically evaluated using data sets from a commercial network. We demonstrate the efficiency of our approach with a complete Bayesian network prediction model and compare with three machine learning prediction model classifiers: 1) Na\u00efve Bayes; 2) random forest; and 3) decision tree. Results demonstrate that our approach generates statistically noteworthy estimations and that the DSS will improve the cost efficiency of ESM network operations and maintenance.","keywords_author":["Analytics","Bayesian networks","cyber-physical systems (CPSs)","decision support system","information and communication technologies (ICT)","Internet of Things (IoT)","machine learning (ML)","machine-to-machine (M2M)","operations and maintenance (O&M)","smart cities","smart grid","smart meters","utility"],"keywords_other":["Analytics","Bayes method","Machine-to-machine (M2M)","Predictive models","Information and Communication Technologies","Cyber-physical systems (CPS)","Operations and maintenance","Smart grid","Utility","Internet of Things (IOT)"],"max_cite":1.0,"pub_year":2017.0,"sources":"['scp']","rawkeys":["decision support system","machine-to-machine (m2m)","operations and maintenance","smart meters","machine learning (ml)","smart grid","smart cities","predictive models","cyber-physical systems (cps)","information and communication technologies (ict)","bayes method","cyber-physical systems (cpss)","operations and maintenance (o&m)","information and communication technologies","bayesian networks","utility","internet of things (iot)","analytics"],"tags":["bayes methods","cyber-physical systems","operation and maintenance","smart grid","smart cities","predictive models","machine learning","smart meter","machine to machines","information and communication technologies","bayesian networks","utility","decision support systems","internet of things (iot)","analytics"]},{"p_id":69888,"title":"UTiLearn: A Personalised Ubiquitous Teaching and Learning System for Smart Societies","abstract":"The education industry around the globe is undergoing major transformations. Organizations, such as Coursera are advancing new business models for education. A number of major industries have dropped degrees from the job requirements. While the economics of higher education institutions are under threat in a continuing gloomy global economy, digital and lifelong learners are increasingly demanding new teaching and learning paradigms from educational institutions. There is an urgent need to transform teaching and learning landscape in order to drive global economic growth. The use of distance eTeaching and eLearning (DTL) is on the rise among digital natives alongside our evolution toward smart societies. However, the DTL systems today lack the necessary sophistication due to several challenges including data analysis and management, learner-system interactivity, system cognition, resource planning, agility, and scalability. This paper proposes a personalised Ubiquitous eTeaching & eLearning (UTiLearn) framework that leverages Internet of Things, big data, supercomputing, and deep learning to provide enhanced development, management, and delivery of teaching and learning in smart society settings. A proof of concept UTiLearn system has been developed based on the framework. A detailed design, implementation, and evaluation of the UTiLearn system, including its five components, are provided using 11 widely used datasets.","keywords_author":["Big data","computational and artificial intelligence","distance learning","high performance computing","Internet of Things"],"keywords_other":["THINGS","NETWORKS","ENTERPRISE SYSTEMS","FRAMEWORK","ACTIVITY RECOGNITION","MODEL","IOT","CITIES","LOGISTICS","INTERNET"],"max_cite":5.0,"pub_year":2017.0,"sources":"['ieee', 'wos']","rawkeys":["activity recognition","cities","big data","enterprise systems","framework","model","things","internet of things","computational and artificial intelligence","networks","high performance computing","iot","distance learning","internet","logistics"],"tags":["activity recognition","cities","big data","model","framework","things","computational and artificial intelligence","enterprise system","networks","high performance computing","distance learning","internet","internet of things (iot)","logistics"]},{"p_id":30977,"title":"Predictive analytics for complex IoT data streams","abstract":"\u00a9 2017 IEEE. The requirements of analyzing heterogeneous data streams and detecting complex patterns in near real-time have raised the prospect of complex event processing (CEP) for many Internet of Things (IoT) applications. Although CEP provides a scalable and distributed solution for analyzing complex data streams on the fly, it is designed for reactive applications as CEP acts on near real-time data and does not exploit historical data. In this regard, we propose a proactive architecture which exploits historical data using machine learning for prediction in conjunction with CEP. We propose an adaptive prediction algorithm called adaptive moving window regression for dynamic IoT data and evaluated it using a real-world use case with an accuracy of over 96%. It can perform accurate predictions in near real-time due to reduced complexity and can work along CEP in our architecture. We implemented our proposed architecture using open source components which are optimized for big data applications and validated it on a use-case from intelligent transportation systems. Our proposed architecture is reliable and can be used across different fields in order to predict complex events.","keywords_author":["Complex event processing (CEP)","Data streams","Internet of Things (IoT)","Machine learning (ML)","Predictive analytics (PAs)","Proactive applications","Regression","Time series prediction"],"keywords_other":["Time series prediction","Complex event processing (CEP)","Proactive applications","Regression","Data stream","Internet of Things (IOT)"],"max_cite":3.0,"pub_year":2017.0,"sources":"['ieee', 'scp', 'wos']","rawkeys":["machine learning (ml)","predictive analytics (pas)","proactive applications","time series prediction","data stream","complex event processing (cep)","regression","data streams","internet of things (iot)"],"tags":["proactive applications","time series prediction","data stream","predictive analytics","machine learning","complex event processing","regression","internet of things (iot)"]},{"p_id":33028,"title":"Social relation predictive model of mobile nodes in internet of things","abstract":"With the development of Internet of Things, humans with smart multimedia devices are a new trend of mobile-aware service. This new mode of awareness will inevitable brings some challenges. Among them, the quantification of social relations is the basis of mobile-aware service, and it is an abstract psychological cognitive process, involving space, time and behaviour. Therefore, using social network theory, a novel quantization and predication model with multiple decision factors is proposed, in which multiple decision factors including location, call records, service and feedback factors. These factors are incorporated to reflect the complexity and uncertainty of social relations. Also, the weight distribution is set up by information entropy, and support vector machines optimized based genetic algorithm is used to predict the social relations of mobile nodes, which overcomes the shortage of traditional method, in which the weight is set up by subjective manners and has poor dynamic adaptability. Simulation results show that, cognitive model has better predictive accuracy and dynamic adaptability.","keywords_author":["Decision factors","Internet of Things","Prediction model","Social relations"],"keywords_other":null,"max_cite":3.0,"pub_year":2013.0,"sources":"['scp']","rawkeys":["prediction model","internet of things","decision factors","social relations"],"tags":["predictive models","decision factors","internet of things (iot)","social relations"]},{"p_id":33029,"title":"Towards a dynamic discovery of smart services in the social internet of things","abstract":"\u00a9 2016 Elsevier Ltd The paradigm of the Social Internet of Things (SIoT) boosts a new trend wherein the connectivity and user friendliness benefits of Social Network Services (SNS) are exhibited within the network of connected objects, i.e. the Internet of Things (IoT). The SIoT exceeds the more traditional paradigm of IoT with an enhanced intelligence and context-awareness. In this paper, a novel service framework based on a cognitive reasoning approach for dynamic SIoT services discovery in smart spaces is proposed. That is, reasoning about users\u2019 situational needs, preferences, and other social aspects along with users\u2019 surrounding environment is proposed for generating a list of situation-aware services which matches users\u2019 needs. This reasoning approach is then implemented as a proof-of-concept prototype, namely Airport Dynamic Social, within a smart airport. Finally, an empirical study to evaluate the reasoning approach's efficiency shows improved services adaptability to situational needs compared to common approaches proposed in literature.","keywords_author":["Context-awareness","Internet of Things (IoT)","Semantic reasoning","Service framework","Services discovery","Social Internet of Things (SIoT)"],"keywords_other":["Context- awareness","Service framework","Services discovery","Semantic reasoning","Social Internet of Things (SIoT)","Internet of Things (IOT)"],"max_cite":3.0,"pub_year":2017.0,"sources":"['scp']","rawkeys":["context- awareness","social internet of things (siot)","semantic reasoning","service framework","services discovery","context-awareness","internet of things (iot)"],"tags":["social internet of things","service discovery","context-aware","semantic reasoning","service framework","internet of things (iot)"]},{"p_id":47364,"title":"An intelligent step to effective E-governance in India through e-learning via social networks","abstract":"\u00a9 2016 IEEE. According to an air quality monitoring surveydone by Greenpeace, India has PM2.5 levels, which is arguablyhigher than safety limits. It is essential to impart environmentaleducation to denizens of India. Previous solutions include usageof Mobile Apps to aid in sharing Air Quality Index as a part of E-Governance initiatives. To aid Government in boosting theefficacy of E-Governance service for pollution control andprovide environmental education, we intend to design anintelligent framework that utilizes social networks. In this paper, we have proposed an automated framework for pollution controlE-Governance service which takes the user profiles on socialnetworks and data collected from Internet of Things (IoT) basedair pollution sensors as input. The prototype utilizes Machinelearning algorithm to avoid human intervention and socialnetworks as a platform to aid in outreaching environmentaleducation to large masses of people.","keywords_author":["E-governance","E-learning","Internet of things (IoT)","Machine learning algorithm","Social networks"],"keywords_other":["Human intervention","Pollution sensors","Air quality monitoring","Safety limits","Environmental education","Air quality indices","Internet of Things (IOT)","E-governance"],"max_cite":0.0,"pub_year":2017.0,"sources":"['scp']","rawkeys":["social networks","e-governance","safety limits","human intervention","air quality monitoring","environmental education","machine learning algorithm","air quality indices","pollution sensors","internet of things (iot)","e-learning"],"tags":["machine learning algorithms","e-government","social networks","safety limits","human intervention","air quality monitoring","environmental education","pollution sensors","air quality indices","internet of things (iot)","e-learning"]},{"p_id":10504,"title":"A Deep Learning Approach to on-Node Sensor Data Analytics for Mobile or Wearable Devices","abstract":"The increasing popularity of wearable devices in recent years means that a diverse range of physiological and functional data can now be captured continuously for applications in sports, wellbeing, and healthcare. This wealth of information requires efficient methods of classification and analysis where deep learning is a promising technique for large-scale data analytics. While deep learning has been successful in implementations that utilize high-performance computing platforms, its use on low-power wearable devices is limited by resource constraints. In this paper, we propose a deep learning methodology, which combines features learned from inertial sensor data together with complementary information from a set of shallow features to enable accurate and real-time activity classification. The design of this combined method aims to overcome some of the limitations present in a typical deep learning frame work where on-node computation is required. To optimize the proposed method for real-time on-node computation, spectral domain preprocessing is used before the data are passed onto the deep learning framework. The classification accuracy of our proposed deep learning approach is evaluated against state-of-the-art methods using both laboratory and real world activity datasets. Our results show the validity of the approach on different human activity datasets, outperforming other methods, including the two methods used within our combined pipeline. We also demonstrate that the computation times for the proposed method are consistent with the constraints of real-time on-node processing on smartphones and a wearable sensor platform.","keywords_author":["ActiveMiles","deep learning","Human Activity Recognition (HAR)","Internet-of-Things (IoT)","low-power devices","wearable","ActiveMiles","deep learning","Human Activity Recognition (HAR)","Internet-of-Things (IoT)","low-power devices","wearable"],"keywords_other":["Human Activities","Signal Processing, Computer-Assisted","wearable","Neural Networks (Computer)","Humans","Machine Learning","Human activity recognition","ACCELEROMETER","Monitoring, Ambulatory","ActiveMiles","CLASSIFIERS","Low-power devices","Internet of Things (IOT)","ACTIVITY RECOGNITION"],"max_cite":16.0,"pub_year":2017.0,"sources":"['ieee', 'scp', 'wos']","rawkeys":["ambulatory","classifiers","low-power devices","activemiles","internet of things (iot)","monitoring","machine learning","internet-of-things (iot)","wearable","neural networks (computer)","activity recognition","deep learning","human activities","human activity recognition","humans","signal processing","accelerometer","computer-assisted","human activity recognition (har)"],"tags":["ambulatory","activity recognition","computer-assisted","neural networks","human activities","machine learning","human activity recognition","humans","signal processing","wearables","classifier","internet of things (iot)","low-power devices","activemiles","accelerometer","monitoring"]},{"p_id":69897,"title":"An Emerging Era in the Management of Parkinson's Disease: Wearable Technologies and the Internet of Things","abstract":"Current challenges demand a profound restructuration of the global healthcare system. A more efficient system is required to cope with the growing world population and increased life expectancy, which is associated with a marked prevalence of chronic neurological disorders such as Parkinson's disease (PD). One possible approach to meet this demand is a laterally distributed platform such as the Internet of Things (IoT). Real-time motion metrics in PD could be obtained virtually in any scenario by placing lightweight wearable sensors in the patient's clothes and connecting them to a medical database through mobile devices such as cell phones or tablets. Technologies exist to collect huge amounts of patient data not only during regular medical visits but also at home during activities of daily life. These data could be fed into intelligent algorithms to first discriminate relevant threatening conditions, adjust medications based on online obtained physical deficits, and facilitate strategies to modify disease progression. A major impact of this approach lies in its efficiency, by maximizing resources and drastically improving the patient experience. The patient participates actively in disease management via combined objective device-and self-assessment and by sharing information within both medical and peer groups. Here, we review and discuss the existing wearable technologies and the Internet-of-Things concept applied to PD, with an emphasis on how this technological platform may lead to a shift in paradigm in terms of diagnostics and treatment.","keywords_author":["Internet of things (IoT)","knowledge","parkinson's disease (PD)","patients","wearable technologies"],"keywords_other":["HOME","FEATURES","ARTIFICIAL-INTELLIGENCE","DEEP-BRAIN-STIMULATION","MOVEMENT","AMBULATORY SYSTEM","PROGRESSION","FUTURE CHALLENGES","GAIT","SENSOR DATA"],"max_cite":40.0,"pub_year":2015.0,"sources":"['ieee', 'wos']","rawkeys":["knowledge","patients","sensor data","home","features","deep-brain-stimulation","movement","parkinson's disease (pd)","future challenges","artificial-intelligence","progression","gait","ambulatory system","wearable technologies","internet of things (iot)"],"tags":["knowledge","parkinson's disease","sensor data","home","features","movement","machine learning","wearable technology","future challenges","deep brain stimulation","progression","gait","ambulatory system","internet of things (iot)","patient"]},{"p_id":43272,"title":"Inference of vehicular traffic in smart cities using machine learning with the internet of things","abstract":"\u00a9 2017, Springer-Verlag France. A rapidly increasing world population is not only directly related with increased urban mobility but also traffic delays, losses in fuel, air and noise pollution as well as physiological and psychological problems in the population. Traffic control has been shown to be a means of maintaining road serviceability with a key input being traffic inference. The harvesting of sufficiently large data sets for traffic inference has thus far been a challenge examined by the scientific community. A modelling, interactive design approach to the harvesting and preprocessing of vehicular data can be used to bridge the gap for traffic control to be autonomously implementable in an urban context. With this consideration, this paper explores the implementation of an internet of things multiagent system, in a network and equipped with sensors, distributed along the roadway to collect and share this vehicular data among its nodes and then process the data using a machine learning algorithm for learning and subsequent inference of vehicular types. An interactive design approach is used in the creation of a reduced model of knowledge from each node\u2019s bulk harvested data, which is then easily stored or deployed and which also facilitates post-processing of data. Considering the smart city as a product, the interactive approach to product engineering permits the implementation of these mathematical models as part of an interactive traffic control system, to ensure the city\u2019s adaptability to the ever-changing needs of its inhabitants. In this paper, we present the analysis of the trivial case of the detection of presence and absence of a class A type vehicle. In the introductory section, we examine the problem of transportation in urban expansion. In section two, we examine the solution provided by the Smart Cities with respect to transport services. Further problems which appear within these transport services and their respective solutions in machine learning and the internet of things are discussed in this section on these disruptive technologies. Section three examines the bibliography for successes achieved with similar research and also demonstrates the basis for classification by support vector machines. Sections four to six explain how the experiment is set up to test the hypothesis that over 99% accuracy in vehicular detection can be achieved with the proposed system when cross-referenced with the actual acoustic, magnetic and video presence and absence of a test vehicle.","keywords_author":["Big data","Intelligent traffic systems","Internet of things","Machine learning","Smart cities"],"keywords_other":["Vehicular detection","Product engineering","Intelligent traffic systems","Disruptive technology","Scientific community","Interactive approach","Interactive traffics","Detection of presences"],"max_cite":0.0,"pub_year":2018.0,"sources":"['scp']","rawkeys":["big data","smart cities","internet of things","interactive approach","interactive traffics","disruptive technology","machine learning","scientific community","detection of presences","vehicular detection","intelligent traffic systems","product engineering"],"tags":["big data","machine learning","interactive approach","interactive traffics","disruptive technology","scientific community","detection of presences","production engineering","vehicular detection","intelligent traffic systems","internet of things (iot)","smart cities"]},{"p_id":47372,"title":"Sound analysis in smart cities","abstract":"\u00a9 Springer International Publishing AG 2018. All rights reserved. This chapter introduces the concept of smart cities and discusses the importance of sound as a source of information about urban life. It describes a wide range of applications for the computational analysis of urban sounds and focuses on two high-impact areas, audio surveillance, and noise pollution monitoring, which sit at the intersection of dense sensor networks and machine listening. For sensor networks we focus on the pros and cons of mobile versus static sensing strategies, and the description of a low-cost solution to acoustic sensing that supports distributed machine listening. For sound event detection and classification we focus on the challenges presented by this task, solutions including feature design and learning strategies, and how a combination of convolutional networks and data augmentation result in the current state of the art. We close with a discussion about the potential and challenges of mobile sensing, the limitations imposed by the data currently available for research, and a few areas for future exploration.","keywords_author":["Acoustic sensing","Audio surveillance","Convolutional neural networks","Data augmentation","Deep learning","Internet of things (IOT)","Machine learning","Machine listening","MEMS microphone","Noise monitoring","Sensor network","Smart cities","Sound classification","Sound event detection","Urban sound"],"keywords_other":null,"max_cite":0.0,"pub_year":2017.0,"sources":"['scp', 'ieee']","rawkeys":["urban sound","acoustic sensing","sound classification","sensor network","convolutional neural networks","sound event detection","deep learning","noise monitoring","machine learning","data augmentation","audio surveillance","mems microphone","machine listening","internet of things (iot)","smart cities"],"tags":["urban sound","acoustic sensing","sound classification","sound event detection","noise monitoring","machine learning","data augmentation","audio surveillance","mems microphone","convolutional neural network","machine listening","sensor networks","internet of things (iot)","smart cities"]},{"p_id":43280,"title":"An intelligent improvement of internet-wide scan engine for fast discovery of vulnerable IoT devices","abstract":"\u00a9 2018 by the authors. Since 2016, Mirai and Persirai malware have infected hundreds of thousands of Internet of Things (IoT) devices and created a massive IoT botnet, which caused distributed denial of service (DDoS) attacks. IoT malware targets vulnerable IoT devices, which are vulnerable to security risks. Techniques are needed to prevent IoT devices from being exploited by attackers. However, unlike high-performance PCs, IoT devices are lightweight, low-power, and low-cost, having performance limitations regarding processing and memory, which makes it difficult to install security and anti-malware programs. Recently, several studies have been attempted to quickly search for vulnerable internet-connected devices to solve this real issue. Issues yet to be studied still exist regarding these types of internet-wide scan technologies, such as filtering by security devices and a shortage of collected operating system (OS) information. This paper proposes an intelligent internet-wide scan model that improves IP state scanning with advanced internet protocol (IP) randomization, reactive protocol (port) scanning, and OS fingerprinting scanning, applying k* algorithm in order to find vulnerable IoT devices. Additionally, we describe the experiment's results compared to the existing internet-wide scan technologies, such as ZMap and Shodan. As a result, the proposed model experimentally shows improved performance. Although we improved the ZMap, the throughput per minute (TPM) performance is similar to ZMap without degrading the IP scan throughput and the performance of generating a single IP address is about 118% better than ZMap. In the protocol scan performance experiments, it is about 129% better than the Censys based ZMap, and the performance of OS fingerprinting is better than ZMap, with about 50% accuracy.","keywords_author":["Intelligent security","IoT","Machine learning","Security","Vulnerability"],"keywords_other":null,"max_cite":0.0,"pub_year":2018.0,"sources":"['scp']","rawkeys":["intelligent security","machine learning","security","vulnerability","iot"],"tags":["intelligent security","machine learning","security","vulnerability","internet of things (iot)"]},{"p_id":104722,"title":"Quality Management of Surveillance Multimedia Streams Via Federated SDN Controllers in Fiwi-Iot Integrated Deployment Environments","abstract":"Traditionally, hybrid optical-wireless networks (Fiber-Wireless - FiWi domain) and last-mile Internet of Things edge networks (Edge IoT domain) have been considered independently, with no synergic management solutions. On the one hand, FiWi has primarily focused on high-bandwidth and low-latency access to cellular-equipped nodes. On the other hand, Edge IoT has mainly aimed at effective dispatching of sensor\/actuator data among (possibly opportunistic) nodes, by using direct peer-to-peer and base station (BS)-assisted Internet communications. The paper originally proposes a model and an architecture that loosely federate FiWi and Edge IoT domains based on the interaction of FiWi and Edge IoT software defined networking controllers: the primary idea is that our federated controllers can seldom exchange monitoring data and control hints the one with the other, thus mutually enhancing their capability of end-to-end quality-aware packet management. To show the applicability and the effectiveness of the approach, our original proposal is applied to the notable example of multimedia stream provisioning from surveillance cameras deployed in the Edge IoT domain to both an infrastructure-side server and spontaneously interconnected mobile smartphones; our solution is able to tune the BS behavior of the FiWi domain and to reroute\/prioritize traffic in the Edge IoT domain, with the final goal to reduce latency. In addition, the reported application case shows the capability of our solution of joint and coordinated exploitation of resources in FiWi and Edge IoT domains, with performance results that highlight its benefits in terms of efficiency and responsiveness.","keywords_author":["Fiber wireless (FiWi)","Internet of Things (IoT)","software defined networking (SDN)","quality management","federated SDN controllers"],"keywords_other":["OPTICAL NETWORKS","VEHICULAR NETWORKS","INFORMATION","BANDWIDTH ALLOCATION SCHEMES","ROUTING PROTOCOL","DESIGN","WIRELESS SENSOR NETWORKS","SOFTWARE-DEFINED NETWORKING"],"max_cite":0.0,"pub_year":2018.0,"sources":"['wos']","rawkeys":["design","federated sdn controllers","bandwidth allocation schemes","fiber wireless (fiwi)","software-defined networking","routing protocol","quality management","information","vehicular networks","software defined networking (sdn)","wireless sensor networks","optical networks","internet of things (iot)"],"tags":["design","federated sdn controllers","bandwidth allocation schemes","fiber wireless (fiwi)","routing protocols","software-defined networking","quality management","information","vehicular networks","wireless sensor networks","optical networks","internet of things (iot)"]},{"p_id":43282,"title":"Impact of 5G Technologies on Industry 4.0","abstract":"\u00a9 2018, Springer Science+Business Media, LLC, part of Springer Nature. Manufacturing has evolved over the course of centuries from the days of handmade goods to the adoption of water- and steam-powered machines, the invention of mass production, the introduction of electronic automation, and now beyond. Today, the benchmark for companies to keep up with, is Industry 4.0. Here, Manufacturing systems go beyond simple connection, to also communicate, analyse and use collected information to drive further intelligent actions. It represents an integration of IoT, analytics, additive manufacturing, robotics, artificial intelligence, advanced materials, and augmented reality. The paper looks at the evolution of the Industrial revolution and the technologies that have impacted their growth. The proposed features of 5G technologies are listed and described how these features impact the Industries of the future, leading to Industries 4.0. 5G promises to be a key enabler for Factories of the Future, providing unified communication platform needed to disrupt with new business models and to overcome the shortcomings of current communication technologies.","keywords_author":["5G technologies","Artificial intelligence (AI)","Cyber-physical systems (CPS)","Industrial Internet of Things (IIoT)","Industries 4.0","Information and communication technologies (ICT)","Internet of Things (IoT)","Machine 2 machine (M2M)","Machine learning"],"keywords_other":["Cyber-Physical System (CPS)","Unified communications","Information and Communication Technologies","Industrial internets","Factories of the futures","Industries of the futures","Communication technologies","Internet of Things (IOT)"],"max_cite":0.0,"pub_year":2018.0,"sources":"['scp']","rawkeys":["industrial internet of things (iiot)","industrial internets","5g technologies","machine learning","cyber-physical systems (cps)","information and communication technologies (ict)","industries of the futures","unified communications","cyber-physical system (cps)","factories of the futures","information and communication technologies","artificial intelligence (ai)","industries 4.0","communication technologies","machine 2 machine (m2m)","internet of things (iot)"],"tags":["cyber-physical systems","industrial internets","5g technologies","machine learning","unified communications","industries of the futures","factory of the future","information and communication technologies","communication technologies","machine 2 machine (m2m)","industry 4.0","internet of things (iot)"]},{"p_id":8469,"title":"Cognitive internet of things: A new paradigm beyond connection","abstract":"\u00a9 2014 IEEE.Current research on Internet of Things (IoT) mainly focuses on how to enable general objects to see, hear, and smell the physical world for themselves, and make them connected to share the observations. In this paper, we argue that only connected is not enough, beyond that, general objects should have the capability to learn, think, and understand both physical and social worlds by themselves. This practical need impels us to develop a new paradigm, named cognitive Internet of Things (CIoT), to empower the current IoT with a 'brain' for high-level intelligence. Specifically, we first present a comprehensive definition for CIoT, primarily inspired by the effectiveness of human cognition. Then, we propose an operational framework of CIoT, which mainly characterizes the interactions among five fundamental cognitive tasks: perception-action cycle, massive data analytics, semantic derivation and knowledge discovery, intelligent decision-making, and on-demand service provisioning. Furthermore, we provide a systematic tutorial on key enabling techniques involved in the cognitive tasks. In addition, we also discuss the design of proper performance metrics on evaluating the enabling techniques. Last but not the least, we present the research challenges and open issues ahead. Building on the present work and potentially fruitful future studies, CIoT has the capability to bridge the physical world (with objects, resources, etc.) and the social world (with human demand, social behavior, etc.), and enhance smart resource allocation, automatic network operation, and intelligent service provisioning.","keywords_author":["Cognitive Internet of Things (CIoT)","cognitive radio network (CRN)","decision-making","knowledge discovery","massive data analytics","semantic","service provisioning"],"keywords_other":["Cognitive internets","Intelligent Services","Internet of Things (IOT)","Service provisioning","Cognitive radio network (CRN)","Massive data","Perception-action cycle","Intelligent decision making"],"max_cite":98.0,"pub_year":2014.0,"sources":"['ieee', 'scp', 'wos']","rawkeys":["cognitive internet of things (ciot)","massive data","perception-action cycle","decision-making","cognitive internets","massive data analytics","cognitive radio network (crn)","intelligent decision making","knowledge discovery","service provisioning","semantic","intelligent services","internet of things (iot)"],"tags":["decision making","cognitive internet of things (ciot)","massive data","perception-action cycle","semantics","cognitive internets","massive data analytics","service provisioning","intelligent decision making","knowledge discovery","intelligent services","cognitive radio network","internet of things (iot)"]},{"p_id":26916,"title":"Semantic network of ICT domains and applications","abstract":"Copyright \u00ef\u00bf\u00bd 2014 ACM. In this paper we consider a network of concepts relevant to the new phase of research in the field of ICT. The offered network is the foundation of semantics of a new technological level of e-government. Analysing the flow of publications determined the most popular areas of research. The second part of the article discusses approaches to the creation of applications in the field of medicine and renewable energy, which could considered as a method of transition from e-government to smart government.","keywords_author":["5G","Augmented reality","Big data","Cloud","Cyber-physical systems","E-governance","Geriatric services","ICT domain","Internet of Things","Machine learning","Multilayer intelligent GIS","Renewable energy sources","SDN","Visualization"],"keywords_other":["Intelligent GIS","5G","Cyber physical systems (CPSs)","Renewable energy source","SDN","E-governance","ICT domain"],"max_cite":6.0,"pub_year":2014.0,"sources":"['scp']","rawkeys":["cyber-physical systems","renewable energy sources","big data","cyber physical systems (cpss)","internet of things","machine learning","multilayer intelligent gis","sdn","e-governance","ict domain","geriatric services","augmented reality","cloud","intelligent gis","renewable energy source","5g","visualization"],"tags":["cyber-physical systems","big data","software-defined networking","machine learning","multilayer intelligent gis","e-government","renewable energy source","ict domain","geriatric services","augmented reality","cloud","intelligent gis","5g","visualization","internet of things (iot)"]},{"p_id":43320,"title":"Construction of intelligent traffic information recommendation system based on long short-term memory","abstract":"\u00a9 2018 Elsevier B.V.Traffic information service can improve road utilization and reduce traffic congestion and accidents. In this paper, we design the intelligent traffic information recommendation system based on deep learning. The recommendation system first preprocesses the traffic flow data through Internet of Things (IoT) technology, and then it uses the deep learning network to predict traffic parameters. The traffic congestion duration and spatial diffusion evolution trend are predicted respectively based on long short-term memory (LSTM), which is a typical time-recurrent neural network of deep learning. To the best of our knowledge, it is the first time to construct the intelligent traffic information recommendation system to improve the practicality of traffic information service. The experimental results show that the proposed recommendation system can expand the time horizon of traffic congestion prediction and further improve the reliability and predictability of decision-making basis for traffic managers and travelers.","keywords_author":["Deep learning","Intelligent traffic information","Long short-term memory (LSTM)","Recommendation system"],"keywords_other":["Spatial diffusions","Road utilization","Congestion prediction","Learning network","Traffic parameters","Intelligent traffics","Internet of Things (IOT)","Traffic information service"],"max_cite":0.0,"pub_year":2018.0,"sources":"['scp', 'ieee']","rawkeys":["deep learning","learning network","road utilization","recommendation system","traffic information service","intelligent traffic information","intelligent traffics","traffic parameters","congestion prediction","spatial diffusions","long short-term memory (lstm)","internet of things (iot)"],"tags":["learning network","long short-term memory","road utilization","machine learning","recommender systems","traffic information service","intelligent traffic information","intelligent traffics","traffic parameters","congestion prediction","spatial diffusions","internet of things (iot)"]},{"p_id":24889,"title":"Design of IoT Systems and Analytics in the Context of Smart City Initiatives in India","abstract":"\u00a9 2016 The Authors. Published by Elsevier B.V.The rapid growth of population and industrialization has paved way for the use of technologies like the Internet of Things which gave rise to the concept of smart cities. India as a developing country has a great prospect in developing technologies to make the cities smart. As urbanization occurs the demand for resources and efficient servicing will increase. To achieve this in a smart and efficient way, connected device (IoT) could be used. The possible design of an IoT system based on surveys performed on similar smart solutions implemented has been discussed in this paper. Urbanization and population growth has led to higher demand for resources like water which are of scarce. There is a keen interest from the organizations and government to make proper usage of water. The same can be achieved by proper monitoring and management of water distribution systems. The paper discusses the use of Machine learning techniques to smart city management aspects like smart water management which include water demand forecasting, water quality monitoring and anomaly detection.","keywords_author":["Anomaly Detection","Forecasting","Internet of Things","Machine Learning","Smart City","Smart Water Management"],"keywords_other":["Water demand forecasting","Smart solutions","Anomaly detection","Water quality monitoring","Monitoring and management","Population growth","Smart cities","Machine learning techniques"],"max_cite":8.0,"pub_year":2016.0,"sources":"['scp']","rawkeys":["smart solutions","water quality monitoring","anomaly detection","smart city","machine learning techniques","forecasting","internet of things","machine learning","population growth","water demand forecasting","smart water management","monitoring and management","smart cities"],"tags":["smart solutions","water quality monitoring","anomaly detection","machine learning techniques","forecasting","machine learning","population growth","water demand forecasting","smart water management","monitoring and management","internet of things (iot)","smart cities"]},{"p_id":22842,"title":"Market model and optimal pricing scheme of big data and Internet of Things (IoT)","abstract":"\u00a9 2016 IEEE.Big data has been emerging as a new approach in utilizing large datasets to optimize complex system operations. Big data is fueled with Internet-of-Things (IoT) services that generate immense sensory data from numerous sensors and devices. While most current research focus of big data is on machine learning and resource management design, the economic modeling and analysis have been largely overlooked. This paper thus investigates the big data market model and optimal pricing scheme. We first study the utility of data from the data science perspective, i.e., using the machine learning methods. We then introduce the market model and develop an optimal pricing scheme afterward. The case study shows clearly the suitability of the proposed data utility functions. The numerical examples demonstrate that big data and IoT service provider can achieve the maximum profit through the proposed market model.","keywords_author":["data-as-a-service","Machine learning","market","pricing"],"keywords_other":["Data utilities","Complex system operation","Maximum profits","Optimal pricing","Machine learning methods","Economic modeling","Resource management","Internet of Things (IOT)"],"max_cite":11.0,"pub_year":2016.0,"sources":"['scp']","rawkeys":["economic modeling","machine learning methods","data utilities","complex system operation","resource management","maximum profits","machine learning","pricing","optimal pricing","market","data-as-a-service","internet of things (iot)"],"tags":["economic modeling","machine learning methods","resource management","complex system operation","data utilization","maximum profits","machine learning","marketing","pricing","optimal pricing","data-as-a-service","internet of things (iot)"]},{"p_id":47418,"title":"Risk-based adaptive authentication for internet of things in smart home eHealth","abstract":"\u00a9 2017 ACM.Health care is one of the primary beneficiaries of the technological revolution created by Internet of Things (IoT). In the implementation of health care with IoT, wireless body area network (WBAN) is a suitable communication tool. That being the case security has been one of the major concerns to efficiently utilize the services of WBAN. The diverse nature of the technologies involved in WBAN, the broadcast nature of wireless networks, and the existence of resource constrained devices are the main challenges to implement heavy security protocols for WBAN. In this paper we develop a risk-based adaptive authentication mechanism which continuously monitors the channel characteristics variation, analyzes a potential risk using naive Bayes machine learning algorithm and performs adaptation of the authentication solution. Our solution validates both the authenticity of the user and the device. In addition we evaluate the resource need of the selected authentication solution and provide an offloading functionality in case of scarce resource to perform the selected protocol. The approach is novel because it defines the whole adaptation process and methods required in each phase of the adaptation. The paper also briefly describes the evaluation case study - Smart Home eHealth.","keywords_author":["Adaptive authentication","Ehealth","IoT","Machine learning","Risk-based","Smart home","WBAN"],"keywords_other":["Smart homes","Authentication mechanisms","Ehealth","Resourceconstrained devices","Nature of the technologies","Risk-based","WBAN","Wireless body area network"],"max_cite":0.0,"pub_year":2017.0,"sources":"['scp']","rawkeys":["risk-based","authentication mechanisms","wireless body area network","smart homes","machine learning","resourceconstrained devices","ehealth","nature of the technologies","wban","adaptive authentication","iot","smart home"],"tags":["risk-based","authentication mechanisms","wireless body area network","smart homes","machine learning","resourceconstrained devices","nature of the technologies","adaptive authentication","e-health","internet of things (iot)"]},{"p_id":47440,"title":"Cognitive Acoustic Analytics Service for Internet of Things","abstract":"\u00a9 2017 IEEE. The rapid development of the Internet of Things (IoT) has brought great changes for non-contact and non-destructive sensing and diagnosis. For every inanimate object can tell us something by the sound it makes, acoustic sensor demonstrates great advantages comparing to conventional electronic and mechanic sensors in such cases: overcoming environmental obstacles, mapping to existing use cases of detecting problems with human ears, low cost for deployment, etc. It could be widely applied to various domains, such as predictive maintenance of machinery, robot sensory, elderly and baby care in smart home, etc. Whether we can use the acoustic sensor data to understand what is happening and to predict what will happen relies heavily on the analytics capabilities we apply to the acoustic data, which has to overcome the obstacles of noise, disturbance and errors, and has to meet the requirement of real-time processing of high volume signals with large number of sensors. In this paper, we propose a scalable cognitive acoustics analytics service for IoT that provides the user an incremental learning approach to evolve their analytics capability on non-intuitive and unstructured acoustic data through the combination of acoustic signal processing and machine learning technology. It first performs acoustic signal processing and denoising, enables acoustic signal based abnormal detection based on sound intensity, spectral centroid, etc. Then based on the accumulated abnormal data, a supervised learning method is performed as baseline and a neural network based classifier is used to recognize acoustic events in different scenarios with various volume of sample data and requirement of accuracy. In addition, acoustic sensor arrays processing is supported for localization of moving acoustic source in more complex scenario. In this paper, we designed a hybrid computing structure. Finally, we conduct experiments on acoustic event recognition for machinery diagnosis, and show that the proposed system can achieve high accuracy.","keywords_author":["Acoustic analytics","Acoustic signal processing","Internet of things","Machine learning"],"keywords_other":["Supervised learning methods","Internet of thing (IOT)","Machine learning technology","Incremental learning","Abnormal detection","Realtime processing","Moving acoustic sources","Predictive maintenance"],"max_cite":0.0,"pub_year":2017.0,"sources":"['scp']","rawkeys":["moving acoustic sources","realtime processing","acoustic analytics","supervised learning methods","incremental learning","internet of thing (iot)","internet of things","machine learning","abnormal detection","machine learning technology","predictive maintenance","acoustic signal processing"],"tags":["abnormality detection","moving acoustic sources","realtime processing","acoustic analytics","supervised learning methods","incremental learning","machine learning","machine learning technology","predictive maintenance","acoustic signal processing","internet of things (iot)"]},{"p_id":43348,"title":"Breathing-Based Authentication on Resource-Constrained IoT Devices using Recurrent Neural Networks","abstract":"\u00a9 1970-2012 IEEE. Recurrent neural networks (RNNs) have shown promising results in audio and speech-processing applications. The increasing popularity of Internet of Things (IoT) devices makes a strong case for implementing RNN-based inferences for applications such as acoustics-based authentication and voice commands for smart homes. However, the feasibility and performance of these inferences on resource-constrained devices remain largely unexplored. The authors compare traditional machine-learning models with deep-learning RNN models for an end-to-end authentication system based on breathing acoustics.","keywords_author":["AI","artificial intelligence","authentication","breathing","breathing based authentication","deep learning","Internet of things","IoT","LSTM","mobile and embedded deep learning","performance","recurrent neural networks","RNN","security","wearables"],"keywords_other":["performance","security","wearables","LSTM","breathing"],"max_cite":0.0,"pub_year":2018.0,"sources":"['scp']","rawkeys":["artificial intelligence","lstm","performance","authentication","deep learning","internet of things","security","recurrent neural networks","ai","rnn","wearables","breathing based authentication","iot","mobile and embedded deep learning","breathing"],"tags":["performance","authentication","neural networks","long short-term memory","machine learning","security","wearables","breathing based authentication","mobile and embedded deep learning","breathing","internet of things (iot)"]},{"p_id":22875,"title":"Learning How to Communicate in the Internet of Things: Finite Resources and Heterogeneity","abstract":"\u00a9 2016 IEEE.For a seamless deployment of the Internet of Things (IoT), there is a need for self-organizing solutions to overcome key IoT challenges that include data processing, resource management, coexistence with existing wireless networks, and improved IoT-wide event detection. One of the most promising solutions to address these challenges is via the use of innovative learning frameworks that will enable the IoT devices to operate autonomously in a dynamic environment. However, developing learning mechanisms for the IoT requires coping with unique IoT properties in terms of resource constraints, heterogeneity, and strict quality-of-service requirements. In this paper, a number of emerging learning frameworks suitable for IoT applications are presented. In particular, the advantages, limitations, IoT applications, and key results pertaining to machine learning, sequential learning, and reinforcement learning are studied. For each type of learning, the computational complexity, required information, and learning performance are discussed. Then, to handle the heterogeneity of the IoT, a new framework based on the powerful tools of cognitive hierarchy theory is introduced. This framework is shown to efficiently capture the different IoT device types and varying levels of available resources among the IoT devices. In particular, the different resource capabilities of IoT devices are mapped to different levels of rationality in cognitive hierarchy theory, thus enabling the IoT devices to use different learning frameworks depending on their available resources. Finally, key results on the use of cognitive hierarchy theory in the IoT are presented.","keywords_author":["Internet of things","Learning","machine learning"],"keywords_other":["Sequential learning","Internet of thing (IOT)","Learning","Resource capabilities","Learning performance","Learning frameworks","Resource Constraint","Dynamic environments"],"max_cite":11.0,"pub_year":2016.0,"sources":"['scp']","rawkeys":["sequential learning","internet of thing (iot)","internet of things","learning","machine learning","resource capabilities","resource constraint","learning frameworks","dynamic environments","learning performance"],"tags":["sequential learning","machine learning","resource capabilities","resource constraint","learning frameworks","dynamic environments","internet of things (iot)","learning performance"]},{"p_id":92507,"title":"LSTM-Based Analysis of Industrial IoT Equipment","abstract":"Industrial Internet of Things (IoT) is producing massive data which are valuable for knowing running status of the underlying equipment. However, these data involve various operation events that span some time, which raise questions on how to model long memory of states, and how to predict the running status based on historical data accurately. This paper aims to develop a method of: (1) analyzing equipment working condition based on the sensed data; (2) building a prediction model for working status forecasting and designing a deep neural network model to predict equipment running data; and (3) improving the prediction accuracy by systematic feature engineering and optimal hyperparameter searching. We evaluate our method with real-world monitoring data collected from 33 sensors of a main pump in a power station for three months. The model achieves less root mean square error than that of autoregressive integrated moving average model. Our method is applicable to general IoT equipment for analyzing time series data and forecasting operation status.","keywords_author":["Time series prediction","LSTM model","power equipment","industry Internet of Things"],"keywords_other":["ARMA"],"max_cite":0.0,"pub_year":2018.0,"sources":"['ieee', 'wos']","rawkeys":["time series prediction","arma","lstm model","power equipment","industry internet of things"],"tags":["autoregressive moving average","time series prediction","lstm model","power equipment","internet of things (iot)"]},{"p_id":41311,"title":"Secure IoT Platform for Industrial Control Systems","abstract":"\u00a9 2017 IEEE.Supervisory control and data acquisition (SCADA) systems, are part of industrial control system (ICS), have been playing crucial roles in real-time industrial automation and controls. Through the evolution of 3rd generation, or networks based system, SCADA systems are connected to almost types of networks such as wired, wireless, and cellular and satellite communication, but security is still a big challenge for SCADA system while communicating within. Internet of things (IoT) is a ubiquitous platform, a new advance enhancement, for efficient SCADA system, where billions of network devices, with smart sensing capabilities, are networked over the Internet access. Deployment of smart IoT platform, SCADA system will significantly increase system efficiency, scalability, and reduce cost. Security is still a major issue for both-, as they were initially designed without any priority and requirements of security. This study modeled IoT-SCADA system and deployed a security mechanism, employing of cryptography based algorithm, which provided a secure transmission channel while each time communication occurred, between the field devices in the SCADA system. Proposed security implementation, and computed measurements analyzed as potential security building block against authentication and confidentiality attacks.","keywords_author":["and Advance encryption standard Introduction","Industrial control system","Internet of things","Programmable logical controller","remote terminal units","Supervisory control and data acquisition"],"keywords_other":["Supervisory control and data acquisition","Programmable logical controller","Industrial control systems","Advance encryption standards","Remote terminal units"],"max_cite":1.0,"pub_year":2017.0,"sources":"['scp']","rawkeys":["industrial control system","remote terminal units","industrial control systems","internet of things","supervisory control and data acquisition","programmable logical controller","advance encryption standards","and advance encryption standard introduction"],"tags":["remote terminal units","industrial control systems","scada","advanced encryption standard","programmable logic controllers (plc)","and advance encryption standard introduction","internet of things (iot)"]},{"p_id":45412,"title":"A prototype model for continuous agriculture field monitoring and assessment","abstract":"\u00a9 2018 Authors. Indian farmers are totally dependent on agriculture and livestock for satisfying their basic food and economical needs. Maximum farmers are habitual to take crops continuously with traditional ways without checking the current suitability. Government of India has developed centers to train and provide the information to farmers but everyone don't approach to it. To get the increased yield, usually farmers add fertilizers without understanding requirement which may leads to soil degradation. Proposed solution is an automated system which can monitor major parameters required to estimate suitability for cropping. This system can be made available locally to every farmer. Outcome of this monitoring system can be used to identify particular crop suitability, so that suitable crop can be adopted.","keywords_author":["Agriculture parameter","Iot","Machine learning","Sensor","Suitability level"],"keywords_other":null,"max_cite":0.0,"pub_year":2018.0,"sources":"['scp']","rawkeys":["agriculture parameter","machine learning","sensor","iot","suitability level"],"tags":["agriculture parameter","sensors","machine learning","suitability level","internet of things (iot)"]},{"p_id":29042,"title":"Toward a Gaussian-Mixture Model-Based Detection Scheme Against Data Integrity Attacks in the Smart Grid","abstract":"\u00a9 2017 IEEE. In recent years, the smart grid has been recognized as an important form of the Internet of Things application. In the smart grid, as an energy-based cyber-physical system, the advanced metering infrastructure (AMI) will be developed to monitor and control the power grid by integrating computing and networking components to ensure stable and efficient operation. The AMI is vulnerable to cyber attacks, especially data integrity attacks. There have been a number of research efforts on detecting such attacks. Nonetheless, most of existing schemes either rely on predefined thresholds or require external knowledge. This may lead to low detection accuracy when the thresholds are improperly defined, and where there is a lack of the external knowledge. To address these issues, in this paper, we propose a Gaussian-mixture model-based detection scheme to mitigate data integrity attacks. Not relying upon the predefined thresholds or external knowledge, our developed scheme operates through narrowing the range of normal data, which can be obtained through clustering the historical data and learning minimum and maximum values or distance values to each center of individual clusters. To evaluate the effectiveness of our proposed scheme, we conduct performance simulation based on the ElectricityLoadDiagrams20112014 data set, and then analyze the effectiveness of the proposed scheme with respect to detection accuracy and overhead. The results of our investigation show that our scheme could achieve a higher detection rate, and a lower error rate, in comparison to existing schemes based on the Min-Max model.","keywords_author":["Data integrity attacks","Gaussian-mixture model","Internet of Things (IoT)","smart grid","Threat detection"],"keywords_other":["Threat detection","Data integrity","Smart grid","Internet of Things (IOT)","Gaussian Mixture Model"],"max_cite":5.0,"pub_year":2017.0,"sources":"['ieee', 'scp', 'wos']","rawkeys":["threat detection","gaussian-mixture model","smart grid","data integrity attacks","data integrity","internet of things (iot)","gaussian mixture model"],"tags":["threat detection","smart grid","data integrity attacks","data integration","internet of things (iot)","gaussian mixture model"]},{"p_id":10610,"title":"Intelligent alerting for fruit-melon lesion image based on momentum deep learning","abstract":"Sensors and Internet of things (IoT) have been widely used in the digitalized orchards. Traditional disease-pest recognition and early warning systems, which are based on knowledge rule, expose many defects, discommodities, and it is difficult to meet current production management requirements of the fresh planting environment. On purpose to realize an intelligent unattended alerting for disease-pest of fruit-melon, this paper presents the convolutional neural network (CNN) for recognition of fruit-melon skin lesion image which is real-timely acquired by an infrared video sensor, which network is grounded upon so-called momentum deep learning rule. More specifically, (1) a suite of transformation methods of apple skin lesion image is devised to simulate orientation and light disturbance which always occurs in orchards, then to output a self-contained set of almost all lesion images which might appear in various dynamic sensing environment; and (2) the rule of variable momentum learning is formulated to update the free parameters of CNN. Experimental results demonstrate that the proposed presents a satisfying accuracy and recall rate which are up to 97.5 %, 98.5 % respectively. As compared with some shallow learning algorithms and generally accepted deep learning ones, it also offers a gratifying smoothness, stableness after convergence and a quick converging speed. In addition, the statistics from experiments of different benchmark data-sets suggests it is very effective to recognize image pattern.","keywords_author":["CNN","Deep network","Intelligent alerting","Lesion image","Momentum learning","Lesion image","CNN","Deep network","Momentum learning","Intelligent alerting"],"keywords_other":["BELIEF NETWORKS","Skin lesion images","EXPERT-SYSTEM","APPLE-TREES","DISEASE","Intelligent alerting","CNN","NEURAL-NETWORKS","Transformation methods","RECOGNITION","Early Warning System","Convolutional neural network","Lesion image","Internet of Things (IOT)"],"max_cite":4.0,"pub_year":2016.0,"sources":"['ieee', 'scp', 'wos']","rawkeys":["lesion image","neural-networks","momentum learning","recognition","apple-trees","disease","expert-system","cnn","deep network","skin lesion images","internet of things (iot)","convolutional neural network","transformation methods","belief networks","early warning system","intelligent alerting"],"tags":["lesion image","momentum learning","recognition","neural networks","apple-trees","disease","skin lesion images","internet of things (iot)","convolutional neural network","transformation methods","belief networks","early warning system","deep networks","expert system","intelligent alerting"]},{"p_id":102773,"title":"Robustness Evaluation of Restricted Boltzmann Machine against Memory and Logic Error","abstract":"In an IoT system, neural networks have the potential to perform advanced information processing in various environments. To clarify this, the robustness of a restricted Boltzmann machine (RBM) used for deep neural networks, such as a deep belief network (DBN), was studied in this paper. Even if memory or logic errors occurred in the circuit operating in the RBM while pre-training the DBN, they did not affect the identification rate of the DBN, showing the robustness of the RBM. In addition, robustness against soft errors was evaluated. The soft errors had almost no influence on the RBM unless they were as large as 1012 times or more in the 50-nm CMOS process.","keywords_author":["IoT","neural network","RBM","DBN","soft error"],"keywords_other":null,"max_cite":0.0,"pub_year":2017.0,"sources":"['wos']","rawkeys":["neural network","soft error","rbm","dbn","iot"],"tags":["soft error","neural networks","deep belief networks","restricted boltzmann machine","internet of things (iot)"]},{"p_id":29048,"title":"Towards a social graph approach for modeling risks in big data and internet of things (IoT)","abstract":"\u00a9 2014 IEEE.The discovery and integration of big data and Internet of Things (IoTs) highlight new challenges in the area of risks. This work focuses on the analysis of literature review approaches by presenting a study that includes works for resource discovery and data integration, social search engines, ranking techniques, and social graphs in order to provide a cross comparison and a preliminary evaluation study. The approaches are analyzed in order to define theoretical key requirements that could enable the utilization of social graphs towards the discovery and modeling of interconnected entities in big data and IoT scenarios.","keywords_author":["Big data","Data discovery","Data integration","IoT","Risk analysis","Social graphs"],"keywords_other":["IoT","Data discovery","Resource discovery","Internet of thing (IoTs)","Social graphs","Literature reviews","Discovery and integration","Internet of Things (IOT)"],"max_cite":5.0,"pub_year":2014.0,"sources":"['scp']","rawkeys":["discovery and integration","big data","data discovery","literature reviews","resource discovery","social graphs","data integration","internet of thing (iots)","iot","risk analysis","internet of things (iot)"],"tags":["discovery and integration","big data","data discovery","literature reviews","resource discovery","social graphs","data integration","risk analysis","internet of things (iot)"]},{"p_id":27001,"title":"An Autonomic Approach to Real-Time Predictive Analytics Using Open Data and Internet of Things","abstract":"\u00a9 2014 IEEE.Public datasets are becoming more and more available for organizations. Both public and private data can be used to drive innovations and new solutions to various problems. The Internet of Things (IoT) and Open Data are particularly promising in real time predictive data analytics for effective decision support. The main challenge in this context is the dynamic selection of open data and IoT sources to support predictive analytics. This issue is widely discussed in various domains including economics, market analysis, energy usage, etc. Our case study is the prediction of energy usage of a building using open data and IoT. We propose a two-step solution: (1) data management: collection, filtering and warehousing and (2) data analytics: source selection and prediction. This work has been evaluated in real settings using IoT sensors and open weather data.","keywords_author":["Autonomic System","Energy Management","IoT","Machine Learning","Open Data","Predictive Analytics"],"keywords_other":["Predictive analytics","Internet of thing (IOT)","IoT","Autonomic Systems","Open datum","Source selection","Dynamic selection","Decision supports"],"max_cite":6.0,"pub_year":2014.0,"sources":"['scp']","rawkeys":["energy management","open data","source selection","open datum","predictive analytics","internet of thing (iot)","machine learning","autonomic system","dynamic selection","autonomic systems","iot","decision supports"],"tags":["energy management","open data","source selection","autonomous systems","open datum","predictive analytics","machine learning","dynamic selection","decision supports","internet of things (iot)"]},{"p_id":45436,"title":"Selecting cloud service for healthcare applications: From hardware to cloud across machine learning","abstract":"The paper describes the process of creating the Internet of Things (IoT) healthcare applications and selecting an environment to deploy it. Based on research of healthcare application architecture was proposed selecting of cloud service. It draws our attention to complex architecture with using different sources of medical data like external medical databases, medical equipment and wearable medical and non-medical devices. Much attention is given to using machine learning in the process in the detection of health problems. In addition, the paper describes two levels of machine learning: one for detecting single problems with heals and second for predictions complex reports and providing treatment plan based on data from the first level. The main emphasis in choosing of cloud service is made on scalability and the ability to create multiple neural networks for processing data.","keywords_author":["Cloud","Healthcare","Internet of Things","Machine Learning"],"keywords_other":["Complex architectures","Treatment plans","Cloud services","Internet of thing (IOT)","Health care application","Multiple neural networks","Medical Devices","Medical database"],"max_cite":0.0,"pub_year":2018.0,"sources":"['scp']","rawkeys":["medical devices","multiple neural networks","treatment plans","healthcare","health care application","internet of thing (iot)","internet of things","machine learning","medical database","cloud","cloud services","complex architectures"],"tags":["medical devices","multiple neural networks","treatment planning","healthcare","health care application","machine learning","internet of things (iot)","medical database","cloud","cloud services","complex architectures"]},{"p_id":41341,"title":"Mining repeating pattern in packet arrivals: Metrics, models, and applications","abstract":"\u00a9 2017A substantial portion of the network traffic can be attributed to autonomous network applications that experience repeating networking patterns. This observation is further signified by the emergence of the Internet of Things (IoT) era that features an enormous number of networked, autonomous sensors. Identifying and characterizing repeating patterns therefore become a critical means to Internet measurement and traffic engineering. In this paper, we propose a novel method that can effectively identify and characterize timing-based repeating patterns from network traffic by overcoming three significant practical challenges, including i) time-scale sensitive, ii) transience, and iii) being interleaved by noises. Our method features a novel metric, namely unpredictability index (UPI), to capture repeating patterns by quantifying the predictability of packet arrivals\u2019 temporal structure from the perspective of hierarchical clustering. An online approach is further developed to incrementally compute UPI upon observing a single packet. Extensive experiments based on synthetic and real-world data have demonstrated that our method can effectively conduct repeating pattern mining.","keywords_author":["Hierarchical clustering","Repeating pattern","Temporal structure","Traffic modeling"],"keywords_other":["Internet of thing (IOT)","Internet measurement","Hier-archical clustering","Repeating patterns","Traffic Engineering","Traffic model","Temporal structures","Autonomous networks"],"max_cite":1.0,"pub_year":2017.0,"sources":"['scp']","rawkeys":["temporal structure","hierarchical clustering","traffic model","traffic modeling","internet of thing (iot)","repeating pattern","temporal structures","autonomous networks","traffic engineering","internet measurement","repeating patterns","hier-archical clustering"],"tags":["hierarchical clustering","traffic model","temporal structures","autonomous networks","traffic engineering","internet measurement","repeating patterns","internet of things (iot)","hier-archical clustering"]},{"p_id":31104,"title":"Integrating machine learning in embedded sensor systems for Internet-of-Things applications","abstract":"\u00a9 2016 IEEE.Interpreting sensor data in Internet-of-Things applications is a challenging problem particularly in embedded systems. We consider sensor data analytics where machine learning algorithms can be fully implemented on an embedded processor\/sensor board. We develop an efficient real-time realization of a Gaussian mixture model (GMM) for execution on the NXP FRDM-K64F embedded sensor board. We demonstrate the design of a customized program and data structure that generates real-time sensor features, and we show details and training\/classification results for select IoT applications. The integrated hardware\/software system enables real-time data analytics and continuous training and re-training of the machine learning (ML) algorithm. The real-time ML platform can accommodate several applications with lower sensor data traffic.","keywords_author":["condition monitoring","embedded machine learning","Internet-of-Things","sensor data analytics"],"keywords_other":["Integrating machines","Hardware\/software systems","IOT applications","Sensor data","Embedded processors","Real time sensors","Embedded machines","Gaussian Mixture Model"],"max_cite":3.0,"pub_year":2017.0,"sources":"['scp']","rawkeys":["sensor data","embedded processors","condition monitoring","embedded machine learning","real time sensors","embedded machines","sensor data analytics","iot applications","integrating machines","internet-of-things","hardware\/software systems","gaussian mixture model"],"tags":["sensor data","embedded processors","condition monitoring","embedded machine learning","real time sensors","embedded machines","sensor data analytics","iot applications","integrating machines","hardware\/software systems","internet of things (iot)","gaussian mixture model"]},{"p_id":18819,"title":"An early resource characterization of deep learning on wearables, smartphones and internet-of-things devices","abstract":"\u00a9 2015 ACM. Detecting and reacting to user behavior and ambient context are core elements of many emerging mobile sensing and Internet-of-Things (IoT) applications. However, extracting accurate infer-ences from raw sensor data is challenging within the noisy and complex environments where these systems are deployed. Deep Learning { is one of the most promising approaches for overcom-ing this challenge, and achieving more robust and reliable infer-ence. Techniques developed within this rapidly evolving area of machine learning are now state-of-the-art for many inference tasks (such as, audio sensing and computer vision) commonly needed by IoT and wearable applications. But currently deep learning al-gorithms are seldom used in mobile\/IoT class hardware because they often impose debilitating levels of system overhead (e.g., memory, computation and energy). Efforts to address this bar-rier to deep learning adoption are slowed by our lack of a system-atic understanding of how these algorithms behave at inference time on resource constrained hardware. In this paper, we present the-rst { albeit preliminary { measurement study of common deep learning models (such as Convolutional Neural Networks and Deep Neural Networks) on representative mobile and embed-ded platforms. The aim of this investigation is to begin to build knowledge of the performance characteristics, resource require-ments and the execution bottlenecks for deep learning models when being used to recognize categories of behavior and context. The results and insights of this study, lay an empirical foundation for the development of optimization methods and execution envi-ronments that enable deep learning to be more readily integrated into next-generation IoT, smartphones and wearable systems.","keywords_author":["Deep Learning","Internet-of-Things","Wearables"],"keywords_other":["Deep learning","Empirical foundations","Resource characterizations","Wearable applications","Convolutional neural network","Performance characteristics","Wearables","Internet of Things (IOT)"],"max_cite":22.0,"pub_year":2015.0,"sources":"['wos', 'scp', 'ieee']","rawkeys":["empirical foundations","performance characteristics","deep learning","wearable applications","resource characterizations","wearables","convolutional neural network","internet-of-things","internet of things (iot)"],"tags":["performance characteristics","empirical foundations","wearable applications","machine learning","resource characterizations","wearables","convolutional neural network","internet of things (iot)"]},{"p_id":47493,"title":"A comparison of cloud execution mechanisms: Fog, edge and clone cloud computing","abstract":"\u00a9 2018, Institute of Advanced Engineering and Science. All rights reserved. Cloud computing is a technology that was developed a decade ago to provide uninterrupted, scalable services to users and organizations. Cloud computing also became an attractive feature for mobile users due to the limited features of mobile devices. The combination of cloud technologies with mobile technologies gave a new area of computing called mobile cloud computing. This combined technology is used to augment the resources existing in smart devices. In recent times Fog computing, Edge computing and Clone Cloud computing techniques have become the latest trends after mobile cloud computing, which have all been developed to address the limitations in cloud computing. This paper reviews these recent technologies in detail. This paper also addresses the differences in these technologies and how each of them are effective to organizations and developers.","keywords_author":["Clone cloud","Cloud computing","Crowdsourcing","Distributed computing","Edge computing","Fog computing","Internet of Things","Machine learning"],"keywords_other":null,"max_cite":0.0,"pub_year":2017.0,"sources":"['scp']","rawkeys":["distributed computing","cloud computing","fog computing","internet of things","crowdsourcing","machine learning","edge computing","clone cloud"],"tags":["distributed computing","cloud computing","fog computing","machine learning","crowdsourcing","edge computing","clone cloud","internet of things (iot)"]},{"p_id":45455,"title":"Adaptive edge analytics - A framework to improve performance and prognostics capabilities for dairy IoT sensor","abstract":"\u00a9 Springer International Publishing AG 2018. Edge analytics is an approach to data collection and analysis in which an automated analytical computation is performed on data at a sensor, network switch or other devices instead of waiting for the data to be sent back to a centralized data store. The data collection merits for normal edge operations but limits for the handling of anomaly events and prediction of prognostics conditions. In this paper, we propose an innovative machine learning edge approach that extends Kalman filter for anomaly detection so as to (a) allow the edge to adaptively collect granular data when abnormal or anomaly data markers witnessed for prognostics and (b) relaxes the data collection frequency for normal device operation cycles. In summary, the adaptive edge analytics fine-tunes the data collection and analysis so that overall health and longevity of the device can be improved. The paper presents prototyping dairy IoT sensor solution design as well as its application and certain experimental results.","keywords_author":["Adaptive edge","Bluetooth","Dairy IoT sensor","Decision tree","Edge analytics","Embedded device","Hanumayamma dairy IoT sensor","Humidity sensors","Internet of things (IoT)","Kalman filter","Machine learning","OSA-CBM","Regression analysis"],"keywords_other":["Adaptive edge","Edge analytics","OSA-CBM","Internet of Things (IOT)","Embedded device"],"max_cite":0.0,"pub_year":2018.0,"sources":"['scp']","rawkeys":["hanumayamma dairy iot sensor","dairy iot sensor","adaptive edge","edge analytics","machine learning","regression analysis","kalman filter","bluetooth","decision tree","humidity sensors","osa-cbm","embedded device","internet of things (iot)"],"tags":["hanumayamma dairy iot sensor","dairy iot sensor","adaptive edge","edge analytics","machine learning","regression analysis","kalman filter","bluetooth","humidity sensors","osa-cbm","embedded device","internet of things (iot)","decision trees"]},{"p_id":24976,"title":"Predicting complex events for pro-active IoT applications","abstract":"\u00a9 2015 IEEE. The widespread use of IoT devices has opened the possibilities for many innovative applications. Almost all of these applications involve analyzing complex data streams with low latency requirements. In this regard, pattern recognition methods based on CEP have the potential to provide solutions for analyzing and correlating these complex data streams in order to detect complex events. Most of these solutions are reactive in nature as CEP acts on real-time data and does not exploit historical data. In our work, we have explored a proactive approach by exploiting historical data using machine learning methods for prediction with CEP. We propose an adaptive prediction algorithm called Adaptive Moving Window Regression (AMWR) for dynamic IoT data and evaluated it using a real-world use case. Our proposed architecture is generic and can be used across different fields for predicting complex events.","keywords_author":["Complex event processing","internet of things","machine learning","pattern recognition","proactive applications","regression","time series prediction"],"keywords_other":["Time series prediction","Proactive applications","Proposed architectures","Machine learning methods","Moving window regressions","Complex event processing","regression","Pattern recognition method"],"max_cite":8.0,"pub_year":2015.0,"sources":"['scp']","rawkeys":["machine learning methods","proactive applications","time series prediction","proposed architectures","internet of things","machine learning","pattern recognition method","complex event processing","pattern recognition","regression","moving window regressions"],"tags":["machine learning methods","proactive applications","time series prediction","proposed architectures","machine learning","pattern recognition method","complex event processing","pattern recognition","regression","moving window regressions","internet of things (iot)"]},{"p_id":49555,"title":"Network function computation as a service in future 5G machine type communications","abstract":"\u00a9 2016 IEEE.The 3GPP machine type communications (MTC) service is expected to contribute a dominant share of the IoT traffic via the upcoming fifth generation (5G) mobile cellular systems. MTC has ambition to connect billions of devices to communicate their data to MTC applications for further processing and data analysis. However, for majority of the applications, collecting all the MTC generated data is inefficient as the data is typically fed into application-dependent functions whose outputs determine the application actions. In this paper, we present a novel MTC architecture that, instead of collecting raw large-volume MTC data, offers the network function computation (NFC) as a service. For a given application demand (function to be computed), different modules (atomic nodes) of the communication infrastructure are orchestrated into a (reconfigurable) directed network topology, and each module is assigned an appropriately defined (reconfigurable) atomic function over the input data, such that the desired global network function is evaluated over the MTC data and a requested MTC-NFC service is delivered. We detail practical viability of incorporating MTC-NFC within the existing 3GPP architecture relying on emerging concepts of Network Function Virtualization and Software Defined Networking. Finally, throughout the paper, we point to the theoretical foundations that inspired the presented architecture highlighting challenges and future directions for designing 3GPP MTC-NFC service.","keywords_author":["Big Data","Internet of Things (IoT)","Machine learning","Network Coding","Network Function Computation"],"keywords_other":["Machine type communications","Machinetype communication (MTC)","Directed network topology","Communication infrastructure","Network functions","Theoretical foundations","Mobile cellular systems","Internet of Things (IOT)"],"max_cite":0.0,"pub_year":2016.0,"sources":"['scp']","rawkeys":["machinetype communication (mtc)","theoretical foundations","big data","machine learning","communication infrastructure","machine type communications","network functions","network coding","network function computation","mobile cellular systems","directed network topology","internet of things (iot)"],"tags":["machinetype communication (mtc)","theoretical foundations","big data","machine learning","communication infrastructure","machine type communications","network functions","network coding","network function computation","mobile cellular systems","directed network topology","internet of things (iot)"]},{"p_id":33180,"title":"A deep Recurrent Neural Network based approach for Internet of Things malware threat hunting","abstract":"\u00a9 2018 Elsevier B.V. Internet of Things (IoT) devices are increasingly deployed in different industries and for different purposes (e.g. sensing\/collecting of environmental data in both civilian and military settings). The increasing presence in a broad range of applications, and their increasing computing and processing capabilities make them a valuable attack target, such as malware designed to compromise specific IoT devices. In this paper, we explore the potential of using Recurrent Neural Network (RNN) deep learning in detecting IoT malware. Specifically, our approach uses RNN to analyze ARM-based IoT applications\u2019 execution operation codes (OpCodes). To train our models, we use an IoT application dataset comprising 281 malware and 270 benign ware. Then, we evaluate the trained model using 100 new IoT malware samples (i.e. not previously exposed to the model) with three different Long Short Term Memory (LSTM) configurations. Findings of the 10-fold cross validation analysis show that the second configuration with 2-layer neurons has the highest accuracy (98.18%) in the detection of new malware samples. A comparative summary with other machine learning classifiers also demonstrate that the LSTM approach delivers the best possible outcome.","keywords_author":["ARM-based IoT malware detection","Deep learning threat hunting","IoT malware detection","Long short term memory","Machine learning","OpCodes analysis","ARM-based loT malware detection","loT malware detection","Long short term memory","Machine learning","OpCodes analysis","Deep learning threat hunting"],"keywords_other":["Processing capability","Malware detection","IOT applications","10-fold cross-validation","CLOUD","Recurrent neural network (RNN)","Opcodes","Environmental data","Internet of Things (IOT)"],"max_cite":2.0,"pub_year":2018.0,"sources":"['ieee', 'scp', 'wos']","rawkeys":["10-fold cross-validation","arm-based lot malware detection","processing capability","lot malware detection","machine learning","environmental data","cloud","arm-based iot malware detection","recurrent neural network (rnn)","deep learning threat hunting","iot malware detection","iot applications","opcodes","opcodes analysis","malware detection","internet of things (iot)","long short term memory"],"tags":["10-fold cross-validation","neural networks","arm-based lot malware detection","long short-term memory","machine learning","processing capability","environmental data","cloud","arm-based iot malware detection","deep learning threat hunting","iot malware detection","iot applications","opcodes","opcodes analysis","malware detection","internet of things (iot)","lot malware detection"]},{"p_id":43426,"title":"Machine learning-based automatic reinforcing bar image analysis system in the internet of things","abstract":"\u00a9 2018 Springer Science+Business Media, LLC, part of Springer Nature Research on the analysis of reinforcing bar images has been conducted to count reinforcing bars moving along a conveyor belt at a bar production plant. It is relatively easy to analyze images at the plant, where the environment and light sources can be tightly controlled. At construction sites, the characteristics of images vary greatly depending on the environment, time of image acquisition, and weather conditions. Therefore, a method for correctly segregating the reinforcing bar area is needed. In this paper, we propose an automatic reinforcing bar image analysis system based on machine learning. Our proposed system accurately separates the bar area from the background and counts the number of bars in the image. Compared with existing method, the proposed system performs better on detection of reinforcing bars.","keywords_author":["Image analysis","Internet of things","Machine learning","Quantity management","Reinforcing bar"],"keywords_other":["Bar production","On-machines","Construction sites","Conveyor belts","Image analysis systems","Reinforcing bar","Bar area"],"max_cite":0.0,"pub_year":2018.0,"sources":"['scp']","rawkeys":["on-machines","reinforcing bar","construction sites","bar production","internet of things","bar area","machine learning","conveyor belts","quantity management","image analysis systems","image analysis"],"tags":["on-machines","construction sites","bar production","machine learning","bar area","conveyor belts","quantity management","image analysis systems","reinforcement bar","internet of things (iot)","image analysis"]},{"p_id":31141,"title":"Parking-stall vacancy indicator system, based on deep convolutional neural networks","abstract":"\u00a9 2016 IEEE.Parking-management systems, including services that recognize vacant stalls, can play a valuable role in reducing traffic and energy waste in large cities. Visual methods for detecting vacant parking spots are cost-effective options since they can take advantage of the cameras already available in many parking lots. However, visual-detection methods can be fragile and not easily generalizable. In this paper, we present a robust detection algorithm based on deep convolutional neural networks. We implemented and tested our algorithm on a large baseline dataset, and also tested on video feeds from web-accessible parking-lot cameras. Our detection method improved the state of the art AUC by 8.13%. It also showed robust performance in different testing scenarios including tests on public cameras. We have developed a fully functional system, from server-side image analysis to front-end user interface, to demonstrate the practicality of our method.","keywords_author":["Deep Learning","Internet of Things","Smart Cities","Smart Parking"],"keywords_other":["Functional systems","Smart parking","Visual detection","Robust performance","Convolutional neural network","Detection methods","Robust detection algorithms","Parking management systems"],"max_cite":3.0,"pub_year":2017.0,"sources":"['scp']","rawkeys":["visual detection","deep learning","robust detection algorithms","detection methods","internet of things","smart parking","convolutional neural network","robust performance","functional systems","parking management systems","smart cities"],"tags":["visual detection","robust detection algorithms","detection methods","machine learning","smart parking","convolutional neural network","robust performance","functional systems","parking management systems","internet of things (iot)","smart cities"]},{"p_id":35252,"title":"Integration of cloud computing with internet of things: Challenges and open issues","abstract":"\u00a9 2017 IEEE. The Internet of Things (IoT) is becoming the next Internet-related revolution. It allows billions of devices to be connected and communicate with each other to share information that improves the quality of our daily lives. On the other hand, Cloud Computing provides on-demand, convenient and scalable network access which makes it possible to share computing resources, indeed, this, in turn, enables dynamic data integration from various data sources. There are many issues standing in the way of the successful implementation of both Cloud and IoT. The integration of Cloud Computing with the IoT is the most effective way on which to overcome these issues. The vast number of resources available on the Cloud can be extremely beneficial for the IoT, while the Cloud can gain more publicity to improve its limitations with real world objects in a more dynamic and distributed manner. This paper provides an overview of the integration of the Cloud into the IoT by highlighting the integration benefits and implementation challenges. Discussion will also focus on the architecture of the resultant Cloud-based IoT paradigm and its new applications scenarios. Finally, open issues and future research directions are also suggested.","keywords_author":["Cloud based IoT","Cloud computing","Integration","Internet of things"],"keywords_other":["Internet of thing (IOT)","Data-sources","Future research directions","Computing resource","Real-world objects","Cloud-based","New applications","Scalable networks"],"max_cite":2.0,"pub_year":2018.0,"sources":"['scp']","rawkeys":["integration","cloud computing","scalable networks","computing resource","future research directions","cloud-based","internet of thing (iot)","cloud based iot","internet of things","real-world objects","new applications","data-sources"],"tags":["integration","cloud computing","scalable networks","future research directions","cloud-based","cloud based iot","computational resources","real-world objects","new applications","data-sources","internet of things (iot)"]},{"p_id":37302,"title":"IDispenser-big data enabled intelligent dispenser","abstract":"\u00a9 2017 IEEE.With healthcare-associated infections (HAIs) in the U.S. accounting for an estimated 1.7 million infections and 99,000 deaths annually, reducing and preventing these infections is a top goal for healthcare facilities throughout the country [1]. Not only healthcare facilities, in other captive environments, for instance, ships and cruises, provide environment that may increase risk of infection. According to Minooee and Rickman [2], 'Ships provide an isolated environment that may increase the passenger's risk of infection if exposed to respiratory viruses. High attack rates of influenza, for example, are typically seen in closed settings such as cruises, military vessels, aircraft, and institutions.'The high rates of infection in captive areas are influenced by number of people entering and exiting the place. As per the research summarized by 'Hospital infection control: reducing airborne pathogens'[3], the number of people entering and exiting provides a contaminant source. It's known that the concentration of airborne bacteria is proportional to the number of personnel in the room. The amount of surface contamination is also related to airborne contamination from occupation and activity since these microbes settle continuously.' Fencl [3] suggests that the use of disinfection, to control infectious agents in healthcare settings, is one of its oldest and most cost effective ways to control airborne infection. Nichols [1], importantly, suggest that touchless dispensing solutions as an effective way to help reduce the spread of germs. In our view, with advent of machine learning and Internet of Things, combining automated & intelligent dispensing with touchless systems provides more holistic approach to control infection in healthcare and, more importantly, captive places such as hospitals, cruises, casinos, airports and other places.In this research paper, we propose an innovative approach to prevent spread of airborne diseases through the application of Big Data Technologies and IoT Sensing. Our goal is to cutting down the millions of dollars spent on infectious diseases, intelligent dispenser promises to keep hospitals smelling fresh soothing and disinfecting. The paper presents prototyping solution design as well as its application and certain experimental results.","keywords_author":["Airborne diseases","Android","CEP","Complex Event Processing","Decision Tree","Healthcare-associated infections (HAIs)","Internet Of Things","iOS","IoT","IoT reference architecture","Machine Learning","Regression Analysis","Term Frequency and Inverse","Venue Analytics"],"keywords_other":["Venue Analytics","Android","Term Frequency","Reference architecture","Complex event processing","Airborne disease"],"max_cite":1.0,"pub_year":2017.0,"sources":"['scp']","rawkeys":["reference architecture","term frequency and inverse","iot reference architecture","internet of things","android","healthcare-associated infections (hais)","airborne disease","machine learning","complex event processing","regression analysis","decision tree","term frequency","venue analytics","cep","iot","airborne diseases","ios"],"tags":["iot reference architecture","neural networks","healthcare associated infections","machine learning","regression analysis","airborne disease","complex event processing","venue analytics","term frequency","term frequency and inverse","reference architecture","internet of things (iot)","decision trees"]},{"p_id":37306,"title":"A predictive approach to task scheduling for Big Data in cloud environments using classification algorithms","abstract":"\u00a9 2017 IEEE. There have been many recent developments in integrating the Cloud with the Internet of \u03a4hings (IoT) which comprise of up and coming technologies such as Smart Cities and Smart devices. This federation has resulted in research being directed towards further integration of Big Data with the Cloud, as IoT devices consisting of such technologies generate a continuous stream of sensor data. Thus, in this paper, we seek to present a predictive approach to task scheduling with the aim of reducing the overhead incurred when Big Data is processed on the Cloud. Subsequently, we wish to increase both the efficiency and reliability of the Cloud network while handling Big Data. We present a method of using classification in Machine Learning as a tool for scheduling tasks and assigning them to Virtual Machines (VMs) in the Cloud environment. A comparative study is undertaken to observe which brand of classifiers perform optimally in the given scenario. Particle Swarm Optimization (PSO) is used to generate the dataset which is used to train the classifiers. A number of classification algorithms such as Naive Bayes, Random Forest and \u039a Nearest Neighbor are then used to predict the VM best suited to a task in the test dataset.","keywords_author":["Big Data","Classification","Cloud computing","Internet of Things","Machine Learning","Task Scheduling"],"keywords_other":["Task-scheduling","Nearest neighbors","Random forests","Cloud environments","Efficiency and reliability","Comparative studies","Classification algorithm","Scheduling tasks"],"max_cite":1.0,"pub_year":2017.0,"sources":"['scp']","rawkeys":["cloud computing","big data","task scheduling","cloud environments","comparative studies","internet of things","machine learning","random forests","task-scheduling","classification","efficiency and reliability","scheduling tasks","classification algorithm","nearest neighbors"],"tags":["cloud computing","big data","neural networks","task scheduling","cloud environments","comparative studies","machine learning","random forests","classification","efficiency and reliability","scheduling tasks","classification algorithm","internet of things (iot)"]},{"p_id":33213,"title":"Personal comfort models \u2013 A new paradigm in thermal comfort for occupant-centric environmental control","abstract":"\u00a9 2018 Elsevier Ltd A personal comfort model is a new approach to thermal comfort modeling that predicts an individual's thermal comfort response, instead of the average response of a large population. It leverages the Internet of Things and machine learning to learn individuals\u2019 comfort requirements directly from the data collected in their everyday environment. Its results could be aggregated to predict comfort of a population. To provide guidance on future efforts in this emerging research area, this paper presents a unified framework for personal comfort models. We first define the problem by providing a brief discussion of existing thermal comfort models and their limitations for real-world applications, and then review the current state of research on personal comfort models including a summary of key advances and gaps. We then describe a modeling framework to establish fundamental concepts and methodologies for developing and evaluating personal comfort models, followed by a discussion of how such models can be integrated into indoor environmental controls. Lastly, we discuss the challenges and opportunities for applications of personal comfort models for building design, control, standards, and future research.","keywords_author":["Data-driven modeling","Internet of things","Machine learning","Occupant-centric environmental control","Personal thermal comfort","Smart buildings"],"keywords_other":["Environmental control","Provide guidances","Fundamental concepts","State of research","Unified framework","Comfort response","Thermal comfort models","Data-driven model"],"max_cite":2.0,"pub_year":2018.0,"sources":"['scp']","rawkeys":["occupant-centric environmental control","smart buildings","comfort response","thermal comfort models","environmental control","data-driven model","data-driven modeling","internet of things","machine learning","personal thermal comfort","fundamental concepts","unified framework","state of research","provide guidances"],"tags":["occupant-centric environmental control","smart buildings","comfort response","thermal comfort models","environmental control","data-driven model","machine learning","personal thermal comfort","unified framework","fundamental concepts","state of research","provide guidances","internet of things (iot)"]},{"p_id":49611,"title":"Exploiting data analytics for home automation services","abstract":"\u00a9 2016 IEEE.The Internet of Thing generates data at an unprecedented pace. This is due to the ever increasing number of connected devices being deployed as well as their volubility. There's a huge potential in exploiting this mass of data. In particular collecting, storing this data over time, and analyzing the resulting deposit offline, enables the extraction of information and discovering of patterns, which can then be transformed into executable structures and new services. This approach can apply to a range of application domains including Smart Home, Smart City, Smart Energy services. In this paper we show how Data Analytics (DA) can leverage such data deposit and produce insights which can then be transformed into enhanced services that will bring the user experience to the next level. Our approach has been assessed in the domain of the Smart Home with real data provided by Orange Homelive solution through the implementation of the MACLEOD system.","keywords_author":["Data Analytics","Data Mining","Internet Of Things","Machine Learning","Smart Home"],"keywords_other":["Smart homes","User experience","Extraction of information","Smart energies","Smart cities","New services","Data analytics","Home automation"],"max_cite":0.0,"pub_year":2016.0,"sources":"['scp']","rawkeys":["new services","extraction of information","data mining","smart homes","smart energies","data analytics","home automation","internet of things","machine learning","user experience","smart home","smart cities"],"tags":["new services","extraction of information","data mining","smart homes","smart energies","data analytics","home automation","machine learning","user experience","internet of things (iot)","smart cities"]},{"p_id":43476,"title":"Machine learning for predictive maintenance of industrial machines using IoT sensor data","abstract":"\u00a9 2017 IEEE. The industrial Internet of Things (IIoT) is the use of Internet of Things (IoT) technologies in manufacturing which harnesses the machine data generated by various sensors and applies various analytics on it to gain useful information. The data captured by the machines is usually accompanied by a date time component which proves vital for predictive modelling. This paper explores the use of AutoRegressive Integrated Moving Average (ARIMA) forecasting on the time series data collected from various sensors from a Slitting Machine, to predict the possible failures and quality defects, thus improving the overall manufacturing process. The use of Machine Learning thus proves a vital component in IIoT having use cases in quality management and quality control, lowering the cost of maintenance and improving the overall manufacturing process.","keywords_author":["ARIMA Forecasting","Data Analysis","Machine Learning","Predictive maintenance & Productivity"],"keywords_other":["Predictive modelling","Auto-regressive integrated moving average","Manufacturing process","Cost of maintenance","Industrial internets","Industrial machines","Predictive maintenance","Internet of Things (IOT)"],"max_cite":0.0,"pub_year":2018.0,"sources":"['scp']","rawkeys":["cost of maintenance","arima forecasting","manufacturing process","predictive maintenance & productivity","industrial internets","machine learning","predictive modelling","industrial machines","predictive maintenance","data analysis","internet of things (iot)","auto-regressive integrated moving average"],"tags":["cost of maintenance","arima forecasting","manufacturing process","predictive maintenance & productivity","industrial internets","predictive models","arima","machine learning","industrial machines","predictive maintenance","data analysis","internet of things (iot)"]},{"p_id":49621,"title":"Border crossing data acquisition, analysis, and use","abstract":"\u00a9 2016 IEEE. The centerpiece of the legacy system was the system to capture the identifying information of a traveler and private vehicles, forward to a central processing facility, and obtain the guidance on whether the subject should be admitted. Surveillance cameras, fingerprint scanners, RFID tag readers, and license plate readers did generate additional data but it maintained its own identity. Looking at this operation from an IoT (Internet of Things) lens, and taking account of the latest technology, it seems necessary and desirable to integrate the data sources to make sure that the perspective that decision makers need is based on a complete picture of the situation-not just whether the traveler or a vehicle is on the watch list. This paper looks at some of the issues and opportunities that appear to be worth considering. It is necessarily a cursory look presented in the hope that it will be of use as the thought process for such a project starts. These are strictly personal views of the authors who have no connection with any law-enforcement agency.","keywords_author":["AI","big-data","IoT","machine learning"],"keywords_other":["Fingerprint scanners","Central processing facilities","Additional datum","Surveillance cameras","License plate readers","Latest technology","Law-enforcement agencies","Border crossings"],"max_cite":0.0,"pub_year":2016.0,"sources":"['scp']","rawkeys":["surveillance cameras","law-enforcement agencies","license plate readers","latest technology","machine learning","fingerprint scanners","ai","border crossings","iot","central processing facilities","additional datum","big-data"],"tags":["surveillance cameras","law-enforcement agencies","big data","license plate readers","latest technology","machine learning","fingerprint scanners","border crossings","central processing facilities","additional datum","internet of things (iot)"]},{"p_id":20956,"title":"A laguerre neural network-based ADP learning scheme with its application to tracking control in the Internet of Things","abstract":"\u00a9 2016, Springer-Verlag London.Sensory data have becoming widely available in large volume and variety due to the increasing presence and adoption of the Internet of Things. Such data can be tremendously useful if they are processed properly in a timely fashion. They could play a key role in the coordination of industrial production. It is thus desirable to explore an effective and efficient scheme to support data tracking and monitoring. This paper intends to propose a novel automatic learning scheme to improve the tracking efficiency while maintaining or improving the data tracking accuracy. A core strategy in the proposed scheme is the design of Laguerre neural network (LaNN)-based approximate dynamic programming (ADP). As a traditional optimal learning strategy, ADP is a popular approach for data processing. The action neural network (NN) and the critic NN as two important components in ADP have big impact on the performance of ADP. In this paper, a LaNN is employed as the implementation of the action NN in ADP considering Laguerre polynomials\u2019 approximation capability. In addition, this LaNN-based ADP is integrated into an online parameter-tuning framework to optimize those parameters of characteristic model that is used to trace the data in the tracking control system. Meanwhile, this article provides an associated Lyapunov convergence analysis to guarantee a uniformly ultimately boundedness property for tracking errors in the proposed approach. Furthermore, the proposed LaNN-based ADP optimal online parameter-tuning scheme is validated using a temperature dynamic tracking control task. The simulation results demonstrate that the scheme has satisfactory learning performance over time.","keywords_author":["Approximate dynamic programming (ADP)","Automatic tracking","Characteristic model","Internet of Things","Laguerre neural network","Parameter tuning"],"keywords_other":["Automatic tracking","Approximate dynamic programming","Laguerre","Parameter-tuning","Characteristic model"],"max_cite":16.0,"pub_year":2016.0,"sources":"['ieee', 'scp', 'wos']","rawkeys":["automatic tracking","approximate dynamic programming (adp)","characteristic model","parameter tuning","internet of things","parameter-tuning","laguerre neural network","laguerre","approximate dynamic programming"],"tags":["automatically tracking","characteristic model","parameter-tuning","laguerre neural network","laguerre","approximate dynamic programming","internet of things (iot)"]},{"p_id":14813,"title":"The future of industrial communication: Automation networks in the era of the internet of things and industry 4.0","abstract":"\u00a9 2007-2011 IEEE.With the introduction of the Internet of Things (IoT) and cyberphysical system (CPS) concepts in industrial application scenarios, industrial automation is undergoing a tremendous change. This is made possible in part by recent advances in technology that allow interconnection on a wider and more fine-grained scale. The purpose of this article is to review technological trends and the impact they may have on industrial communication. We will review the impact of IoT and CPSs on industrial automation from an industry 4.0 perspective, give a survey of the current state of work on Ethernet time-sensitive networking (TSN), and shed light on the role of fifth-generation (5G) telecom networks in automation. Moreover, we will point out the need for harmonization beyond networking.","keywords_author":null,"keywords_other":["Fine grained","Internet of thing (IOT)","Cyber-physical systems (CPS)","Telecom networks","Industrial communications","Automation networks","Industrial automation","Technological trends"],"max_cite":53.0,"pub_year":2017.0,"sources":"['scp']","rawkeys":["automation networks","fine grained","internet of thing (iot)","technological trends","industrial automation","cyber-physical systems (cps)","telecom networks","industrial communications"],"tags":["cyber-physical systems","automation networks","fine grained","technological trends","industrial automation","telecom networks","industrial communications","internet of things (iot)"]},{"p_id":29149,"title":"Calibrating chemical multisensory devices for real world applications: An in-depth comparison of quantitative machine learning approaches","abstract":"\u00a9 2017 Elsevier B.V.Chemical multisensor devices need calibration algorithms to estimate gas concentrations. Their possible adoption as indicative air quality measurements devices poses new challenges due to the need to operate in continuous monitoring modes in uncontrolled environments. Several issues, including slow dynamics, continue to affect their real world performances. At the same time, the need for estimating pollutant concentrations on board the devices, especially for wearables and IoT deployments, is becoming highly desirable. In this framework, several calibration approaches have been proposed and tested on a variety of proprietary devices and datasets; still, no thorough comparison is available to researchers. This work attempts a benchmarking of the most promising calibration algorithms according to recent literature with a focus on machine learning approaches. We test the techniques against absolute and dynamic performances, generalization capabilities and computational\/storage needs using three different datasets sharing continuous monitoring operation methodology. Our results can guide researchers and engineers in the choice of optimal strategy. They show that non-linear multivariate techniques yield reproducible results, outperforming linear approaches. Specifically, the Support Vector Regression method consistently shows good performances in all the considered scenarios. We highlight the enhanced suitability of shallow neural networks in a trade-off between performance and computational\/storage needs. We confirm, on a much wider basis, the advantages of dynamic approaches with respect to static ones that only rely on instantaneous sensor array response. The latter have been shown to be best choice whenever prompt and precise response is needed.","keywords_author":["Air quality monitoring","Distributed chemical sensing","Dynamic machine learning","Indicative measurements","Internet of Things","Multisensors calibration algorithms"],"keywords_other":["Generalization capability","Chemical sensing","Calibration algorithm","Air quality measurements","Machine learning approaches","Air quality monitoring","Pollutant concentration","Support vector regression method"],"max_cite":4.0,"pub_year":2018.0,"sources":"['scp']","rawkeys":["generalization capability","indicative measurements","support vector regression method","air quality measurements","calibration algorithm","internet of things","dynamic machine learning","air quality monitoring","pollutant concentration","chemical sensing","distributed chemical sensing","machine learning approaches","multisensors calibration algorithms"],"tags":["generalization capability","indicative measurements","support vector regression method","air quality measurements","calibration algorithm","dynamic machine learning","pollution concentration","air quality monitoring","multisensors calibration algorithms","chemical sensing","distributed chemical sensing","machine learning approaches","internet of things (iot)"]},{"p_id":37342,"title":"A supervisory control loop with Prognostics for human-in-the-loop decision support and control applications","abstract":"\u00a9 2017 IEEE. This paper presents a novel tandem human-machine cognition approach for human-in-the-loop control of complex business-critical and mission-critical systems and processes that are monitored by Internet-of-Things (IoT) sensor networks and where it is of utmost importance to mitigate and avoid cognitive overload situations for the human operators. The approach is based on a decision making supervisory loop for situation awareness and control combined with a machine learning technique that is especially well suited to this control problem. The goal is to achieve a number of functional requirements: (1) ultra-low false alarm probabilities for all monitored transducers, components, machines, systems, and processes; (2) fastest mathematically possible decisions regarding the incipience or onset of anomalies in noisy process metrics; and (3) the ability to unambiguously differentiate between sensor degradation events and degradation in the systems\/processes under surveillance. The novel approach that is presented here does not replace the role of the human in operation of complex engineering systems and processes, but rather augments that role in a manner that minimizes cognitive overload by very rapidly processing, interpreting, and displaying final diagnostic and prognostic information to the human operator in a prioritized format that is readily perceived and comprehended.","keywords_author":["control","decision support","machine learning","Prognostics","sensor networks","situation awareness"],"keywords_other":["Prognostics","Decision supports","Mission critical systems","Situation awareness","Complex engineering system","Internet of Things (IOT)","Human-in-the-loop control","Machine learning techniques"],"max_cite":1.0,"pub_year":2017.0,"sources":"['scp']","rawkeys":["human-in-the-loop control","mission critical systems","prognostics","machine learning techniques","control","machine learning","complex engineering system","internet of things (iot)","decision support","sensor networks","situation awareness","decision supports"],"tags":["complex engineered systems","human-in-the-loop control","mission critical systems","prognostics","machine learning techniques","control","machine learning","situational awareness","decision supports","sensor networks","internet of things (iot)"]},{"p_id":33250,"title":"iSDS: a self-configurable software-defined storage system for enterprise","abstract":"\u00a9 2016 ProphetStor Data Services, Inc. Storage is one of the most important aspects of IT infrastructure for various enterprises. But, enterprises are interested in more than just data storage; they are interested in such things as more reliable data protection, higher performance and reduced resource consumption. Traditional enterprise-grade storage satisfies these requirements at high cost. It is because traditional enterprise-grade storage is usually designed and constructed by customised field-programmable gate array to achieve high-end functionality. However, in this ever-changing environment, enterprises request storage with more flexible deployment and at lower cost. Moreover, the rise of new application fields, such as social media, big data, video streaming service etc., makes operational tasks for administrators more complex. In this article, a new storage system called intelligent software-defined storage (iSDS), based on software-defined storage, is described. More specifically, this approach advocates using software to replace features provided by traditional customised chips. To alleviate the management burden, it also advocates applying machine learning to automatically configure storage to meet dynamic requirements of workloads running on storage. This article focuses on the analysis feature of iSDS cluster by detailing its architecture and design.","keywords_author":["lightweight container","machine learning","neural network","software-defined storage","Storage","Storage","software-defined storage","lightweight container","machine learning","neural network"],"keywords_other":["End-functionality","Video streaming services","TIME-SERIES","Intelligent software","IT infrastructures","Changing environment","Operational tasks","VIRTUALIZATION","Resource consumption","IOT","Self-configurable"],"max_cite":2.0,"pub_year":2018.0,"sources":"['scp', 'wos']","rawkeys":["neural network","it infrastructures","end-functionality","intelligent software","operational tasks","changing environment","self-configurable","time-series","machine learning","storage","software-defined storage","video streaming services","iot","lightweight container","virtualization","resource consumption"],"tags":["end-functionality","intelligent software","it infrastructures","neural networks","changing environment","operational tasks","self configuration","machine learning","storage","video streaming services","resource consumption","software defined storage","lightweight container","virtualization","internet of things (iot)","time series"]},{"p_id":8678,"title":"Internet of Things and Big Data Analytics for Smart and Connected Communities","abstract":"\u00a9 2013 IEEE. This paper promotes the concept of smart and connected communities SCC, which is evolving from the concept of smart cities. SCC are envisioned to address synergistically the needs of remembering the past (preservation and revitalization), the needs of living in the present (livability), and the needs of planning for the future (attainability). Therefore, the vision of SCC is to improve livability, preservation, revitalization, and attainability of a community. The goal of building SCC for a community is to live in the present, plan for the future, and remember the past. We argue that Internet of Things (IoT) has the potential to provide a ubiquitous network of connected devices and smart sensors for SCC, and big data analytics has the potential to enable the move from IoT to real-time control desired for SCC. We highlight mobile crowdsensing and cyber-physical cloud computing as two most important IoT technologies in promoting SCC. As a case study, we present TreSight, which integrates IoT and big data analytics for smart tourism and sustainable cultural heritage in the city of Trento, Italy.","keywords_author":["Big Data Analytics","Internet of Things","Smart and Connected Communities","Smart Cities","Smart Tourism","Sustainable Cultural Heritage"],"keywords_other":["Ubiquitous networks","Internet of Things (IOT)","Community IS","Smart Tourism","Smart cities","Cultural heritages","Cyber physicals","Data analytics"],"max_cite":93.0,"pub_year":2016.0,"sources":"['ieee', 'scp', 'wos']","rawkeys":["smart cities","data analytics","internet of things","smart and connected communities","ubiquitous networks","cultural heritages","smart tourism","cyber physicals","sustainable cultural heritage","big data analytics","internet of things (iot)","community is"],"tags":["smart cities","data analytics","smart and connected communities","ubiquitous networks","cultural heritages","smart tourism","cyber physicals","sustainable cultural heritage","big data analytics","internet of things (iot)","community is"]},{"p_id":29159,"title":"Multi-style learning with denoising autoencoders for acoustic modeling in the internet of things (IoT)","abstract":"\u00a9 2017 Elsevier LtdWe propose a multi-style learning (multi-style training + deep learning) procedure that relies on deep denoising autoencoders (DAEs) to extract and organize the most discriminative information in a training database. Traditionally, multi-style training procedures require either collecting or artificially creating data samples (e.g., by noise injection or data combination) and training a deep neural network (DNN) with all of these different conditions. To expand the applicability of deep learning, the present study instead adopts a DAE to augment the original training set. First, a DAE is utilized to synthesize data that captures useful structure in the input distribution. Next, this synthetic data is combined and mixed within the original training set to exploit the powerful capabilities of DNN classifiers to learn the complex decision boundaries in heterogeneous conditions. By assigning a DAE to synthesize additional examples of representative variations, multi-style learning makes class boundaries less sensitive to corruptions by enforcing back-end DNNs to emphasize on the most discriminative patterns. Moreover, this deep learning technique mitigates the cost and time of data collection and is easy to incorporate into the internet of things (IoT). Results showed these data-mixed DNNs provided consistent performance improvements without even requiring any preprocessing on the test sets.","keywords_author":["Automatic speech recognition","Data combination","Data synthesis","Deep denoising autoencoders","Deep learning","Deep neural networks","Feature compensation","Internet of things (IoT)","Mixed training","Multi-style training","Noise injection theory","Representation learning","Deep learning","Deep neural networks","Multi-style train\"ing","Deep denoising autoencoders","Mixed training","Representation learning","Data combination","Data synthesis","Noise injection theory","Feature compensation","Automatic speech recognition","Internet of things (IoT)"],"keywords_other":["FRONT-END","Data combination","Representation learning","REGULARIZATION","DEEP NEURAL-NETWORKS","Data synthesis","NOISE INJECTION","REPRESENTATIONS","ROBUST SPEECH RECOGNITION","COMPENSATION","Automatic speech recognition","Noise injection","Feature compensation","Internet of Things (IOT)","DISTRIBUTIONS","Autoencoders"],"max_cite":4.0,"pub_year":2017.0,"sources":"['ieee', 'scp', 'wos']","rawkeys":["front-end","automatic speech recognition","robust speech recognition","deep neural-networks","internet of things (iot)","multi-style training","data synthesis","feature compensation","mixed training","regularization","multi-style train\"ing","deep learning","data combination","distributions","representation learning","deep denoising autoencoders","noise injection","noise injection theory","deep neural networks","autoencoders","representations","compensation"],"tags":["automatic speech recognition","robust speech recognition","convolutional neural network","internet of things (iot)","multi-style training","machine learning","data synthesis","feature compensation","mixed training","regularization","multi-style train\"ing","data combination","distributions","representation learning","deep denoising autoencoders","noise injection","front end","noise injection theory","auto encoders","representation","compensation"]},{"p_id":96742,"title":"A Framework of Fog Computing: Architecture, Challenges, and Optimization","abstract":"Fog computing (FC) is an emerging distributed computing platform aimed at bringing computation close to its data sources, which can reduce the latency and cost of delivering data to a remote cloud. This feature and related advantages are desirable for many Internet-of-Things applications, especially latency sensitive and mission intensive services. With comparisons to other computing technologies, the definition and architecture of FC are presented in this paper. The framework of resource allocation for latency reduction combined with reliability, fault tolerance, privacy, and underlying optimization problems are also discussed. We then investigate an application scenario and conduct resource optimization by formulating the optimization problem and solving it via a genetic algorithm. The resulting analysis generates some important insights on the scalability of the FC systems.","keywords_author":["Fog computing","genetic algorithms","Internet of Things","optimization"],"keywords_other":["MOBILE","THINGS","AD HOC NETWORKS","INTERNET"],"max_cite":2.0,"pub_year":2017.0,"sources":"['ieee', 'wos']","rawkeys":["fog computing","things","internet of things","mobile","genetic algorithms","ad hoc networks","optimization","internet"],"tags":["fog computing","things","mobile","genetic algorithm","ad hoc networks","optimization","internet","internet of things (iot)"]},{"p_id":43497,"title":"On condition monitoring of high frequency power GaN converters with adaptive prognostics","abstract":"\u00a9 2018 IEEE. There is no doubt that in the future, a need for higher switching frequency is inevitable to extract the full benefits of reliable Gallium Nitride (GaN) device characteristics. Along with the reliability enhancement for GaN-based power converters, it is essential to monitor a precursor signature identification for diagnostics\/prognostics techniques. With the availability of the most granular information deduced from advanced devices, a new data-driven scheme is proposed for system monitoring and possible lifetime extension of 400W power GaN converters at 100kHz. The approach relies on the real-time Rds(on) data extraction from the power converter, and calibration of an adaptive model using multi-physics co-simulations under thermal cycling. More specifically, the focus is on deploying machine learning algorithms to exploit for the parameter estimation in power electronics engineering reliability.","keywords_author":["Fault prognostics","High frequency dc-dc converter","Internet of things","Machine learning","Metropolis-hasting","Power GaN","Reliability","Wide bandgap semiconductors"],"keywords_other":["Reliability enhancement","High frequency dc-dc converter","Signature identification","Metropolis-hasting","Power GaN","Fault prognostics","On condition monitoring","Device characteristics"],"max_cite":0.0,"pub_year":2018.0,"sources":"['scp']","rawkeys":["device characteristics","on condition monitoring","signature identification","power gan","fault prognostics","internet of things","machine learning","reliability","high frequency dc-dc converter","reliability enhancement","metropolis-hasting","wide bandgap semiconductors"],"tags":["device characteristics","on condition monitoring","signature identification","power gan","fault prognostics","machine learning","reliability","metropolis-hastings","high frequency dc-dc converter","reliability enhancement","wide bandgap semiconductors","internet of things (iot)"]},{"p_id":45547,"title":"Classification of low-SNR side channels","abstract":"\u00a9 2018 SPIE. We use machine learning to characterize the state of digital devices based on their analog emissions. As digital devices operate, they emit internal information into a number of analog side channels. Remote sensing of these unintended signals leads to low signal-to-noise-ratio (SNR) and significant clutter. We developed classifiers to determine which program is executing on a digital device based on analog radio-frequency (RF) emissions collected via a 500-MHz Riscure RF probe. A standard algorithm was developed to serve as a baseline program and intrusions were simulated by introducing minor modifications to this program. We collected a thousand RF traces from each of these modified programs running on ten different devices for thousands of instruction cycles. The ten devices tested are representative of the Internet of Things (IoT) devices including Arduino Unos and PIC24 processors. Our primary approach to mitigating the impact of low SNR is to extend the program execution and signal collection time. Collecting a training set with more traces than samples is not practical. Even after down-sampling the raw data to thirty samples per instruction, the number of samples exceeds the number of traces by orders of magnitude. Such a training set nearly guarantees overlearning. To mitigate this, we present our Whitened Mean Classifier as a method to whiten this sparse training set and avoid overlearning. Classification accuracy exceeded 90% for the modified programs on a subset of the ten devices.","keywords_author":["IoE","IoT","Linear Classifier","Low-SNR Classifier","Machine Learning","Overlearning","Side-channel analysis","Whitening"],"keywords_other":["Side-channel analysis","Whitening","Linear classifiers","Overlearning","Low SNR"],"max_cite":0.0,"pub_year":2018.0,"sources":"['scp']","rawkeys":["linear classifiers","whitening","ioe","machine learning","overlearning","low snr","side-channel analysis","iot","linear classifier","low-snr classifier"],"tags":["linear classifiers","whitening","ioe","machine learning","low snr","over learning","side-channel analysis","low-snr classifier","internet of things (iot)"]},{"p_id":43499,"title":"Implemented IoT based Self-learning Home Management System (SHMS) for Singapore","abstract":"IEEE Internet of things (IoT) makes deployment of smart home concept easy and real. Smart home concept ensures residents to control, monitor and manage their energy consumption without any wastage. This paper presents a self-learning Home Management System (SHMS). In the proposed system, a Home Energy Management System (HEMS), Demand Side Management (DSM) system, and Supply Side Management (SSM) system were developed and integrated for real time operation of a smart home. This integrated system has some capabilities such as Price Forecasting (PF), Price Clustering (PC) and Power Alert System (PAS) which to enhance its functions. These enhancing capabilities were developed and implemented using computational and machine learning technologies. In order to validate the proposed system, real-time power consumption data was collected from a Singapore smart home and a realistic experimental case study was carried out. The case study has shown that the developed system has performed well and created energy awareness to the residents. This proposed system also displays its ability to customize the model for different types of environments compared to traditional smart home models.","keywords_author":["Clustering algorithms","Energy management","Internet of Things","Internet of Things","Logic gates","Machine learning","Machine learning.","Mathematical model","Self-learning Home Management System","Smart homes","Smart homes"],"keywords_other":["Smart homes","Home management","Home energy management system (HEMS)","Machine learning technology","Demand Side Management (DSM)","Supply side managements","Internet of Things (IOT)","Real-time operation"],"max_cite":0.0,"pub_year":2018.0,"sources":"['scp']","rawkeys":["mathematical model","self-learning home management system","energy management","supply side managements","smart homes","demand side management (dsm)","internet of things","machine learning","home management","home energy management system (hems)","machine learning technology","real-time operation","clustering algorithms","logic gates","internet of things (iot)"],"tags":["mathematical model","self-learning home management system","energy management","supply side managements","smart homes","machine learning","home energy management systems","home management","machine learning technology","demand side management","real-time operation","clustering algorithms","logic gates","internet of things (iot)"]},{"p_id":33262,"title":"Reviewing the novel machine learning tools for materials design","abstract":"\u00a9 Springer International Publishing AG 2018. Computational materials design is a rapidly evolving field of challenges and opportunities aiming at development and application of multi-scale methods to simulate, predict and select innovative materials with high accuracy. Today the latest advancements in machine learning, deep learning, internet of things (IoT), big data, and intelligent optimization have highly revolutionized the computational methodologies used for materials design innovation. Such novelties in computation enable the development of problem-specific solvers with vast potential applications in industry and business. This paper reviews the state of the art of technological advancements that machine learning tools, in particular, have brought for materials design innovation. Further via presenting a case study the potential of such novel computational tools are discussed for the virtual design and simulation of innovative materials in modeling the fundamental properties and behavior of a wide range of multi-scale materials design problems.","keywords_author":["Machine learning","Materials design","Optimization"],"keywords_other":["Fundamental properties","Technological advancement","Internet of Things (IOT)","Intelligent optimization","Materials design","Computational materials","Development and applications","Computational methodology"],"max_cite":2.0,"pub_year":2018.0,"sources":"['scp']","rawkeys":["development and applications","fundamental properties","intelligent optimization","technological advancement","machine learning","computational materials","computational methodology","materials design","optimization","internet of things (iot)"],"tags":["development and applications","intelligent optimization","fundamental properties","technological advances","machine learning","computational materials","materials design","optimization","internet of things (iot)","computing methodologies"]},{"p_id":33264,"title":"Supercapacitors in tandem with batteries to prolong the range of UGV systems","abstract":"\u00a9 2017 by the authors. Licensee MDPI, Basel, Switzerland. The purpose of this study was to explore a novel approach to power hybridization in relation to its effectiveness in an unmanned ground vehicle (UGV). This hybridization method is modeled after the power distribution methods found in living organisms, which utilize glycogen stores and adipose tissue to optimize power and energy density strengths and weaknesses. A UGV rover was constructed with an appropriate distribution of power storage elements creating separate power buffers. The primary buffer consisted of a 10Wsolar panel array and a 600 F, 5.4 V supercapacitor bank, and the secondary buffer consisted of a 3.7 V 6 Ah lithium-ion battery pack. The primary buffer provided virtually limitless charge cycles with a superior power density juxtaposed with a secondary buffer that provided superior energy density and volumetric versatility. The design of this rover is presented in this paper; it was tested under manual and autonomous modes. The rover was found to be capable of effectively operating solely on the primary power buffer in high to low luminous conditions while being able to carry out basic extravehicular activities. The rover could travel roughly 22 km without any input power on a full charge of both buffers, and could smoothly switch between its own power buffers during operation, all while transmitting live first person video (FPV) and network data. The introduction of control algorithms on the onboard microcontroller unit (MCU) was also explored in both manual and autonomous configurations. The latter integrated linear regression to intelligently manage power and locomotion based on sensory data from photoresistors.","keywords_author":["Exploration","Hybridization","Internet-of-Things","Lithium-ion","Machine-learning","Perturb-and-observe","Rover","Solar","Supercapacitors","Unmanned-ground-vehicle"],"keywords_other":null,"max_cite":2.0,"pub_year":2018.0,"sources":"['scp']","rawkeys":["unmanned-ground-vehicle","exploration","rover","solar","supercapacitors","perturb-and-observe","machine-learning","hybridization","internet-of-things","lithium-ion"],"tags":["exploration","rover","solar","machine learning","lithium ions","supercapacitors","unmanned ground vehicles","perturb-and-observe","heterogeneity","internet of things (iot)"]},{"p_id":502,"title":"ApDeepSense: Deep Learning Uncertainty Estimation without the Pain for IoT Applications","abstract":"Recent advances in deep-learning-based applications have attracted a growing attention from the IoT community. These highly capable learning models have shown significant improvements in expected accuracy of various sensory inference tasks. One important and yet overlooked direction remains to provide uncertainty estimates in deep learning outputs. Since robustness and reliability of sensory inference results are critical to IoT systems, uncertainty estimates are indispensable for IoT applications. To address this challenge, we develop ApDeepSense, an effective and efficient deep learning uncertainty estimation method for resource-constrained IoT devices. ApDeepSense leverages an implicit Bayesian approximation that links neural networks to deep Gaussian processes, allowing output uncertainty to be quantified. Our approach is shown to significantly reduce the execution time and energy consumption of uncertainty estimation thanks to a novel layer-wise approximation that replaces the traditional computationally intensive sampling-based uncertainty estimation methods. ApDeepSense is designed for neural net-works trained using dropout; one of the most widely used regularization methods in deep learning. No additional training is needed for uncertainty estimation purposes. We evaluate ApDeepSense using four IoT applications on Intel Edison devices. Results show that ApDeepSense can reduce around 88.9% of the execution time and 90.0% of the energy consumption, while producing more accurate uncertainty estimates compared with state-of-the-art methods.","keywords_author":["Deep learning","Internet of Things","Uncertainty estimation","Mobile Computing"],"keywords_other":["Uncertainty","Gaussian processes","Training","Machine learning","Estimation","Neural networks","Bayes methods"],"max_cite":null,"pub_year":2018.0,"sources":"['ieee']","rawkeys":["bayes methods","estimation","deep learning","neural networks","training","uncertainty","internet of things","machine learning","mobile computing","gaussian processes","uncertainty estimation"],"tags":["bayes methods","estimation","neural networks","training","uncertainty","machine learning","mobile computing","gaussian processes","uncertainty estimation","internet of things (iot)"]},{"p_id":33273,"title":"Machine learning on FPGAs to face the IoT revolution","abstract":"\u00a9 2017 IEEE. FPGAs have been rapidly adopted for acceleration of Deep Neural Networks (DNNs) with improved latency and energy efficiency compared to CPU and GPU-based implementations. High-level synthesis (HLS) is an effective design flow for DNNs due to improved productivity, debugging, and design space exploration ability. However, optimizing large neural networks under resource constraints for FPGAs is still a key challenge. In this paper, we present a series of effective design techniques for implementing DNNs on FPGAs with high performance and energy efficiency. These include the use of configurable DNN IPs, performance and resource modeling, resource allocation across DNN layers, and DNN reduction and re-training. We showcase several design solutions including Long-term Recurrent Convolution Network (LRCN) for video captioning, Inception module for FaceNet face recognition, as well as Long Short-Term Memory (LSTM) for sound recognition. These and other similar DNN solutions are ideal implementations to be deployed in vision or sound based IoT applications.","keywords_author":["FPGAs","Internet of Things","Machine Learning"],"keywords_other":["IOT applications","Resource model","Design space exploration","Effective designs","Resource Constraint","Sound recognition","Design solutions","Gpu-based"],"max_cite":2.0,"pub_year":2017.0,"sources":"['wos', 'scp', 'ieee']","rawkeys":["design solutions","sound recognition","design space exploration","fpgas","internet of things","machine learning","resource constraint","resource model","gpu-based","iot applications","effective designs"],"tags":["design solutions","sound recognition","design space exploration","fpgas","machine learning","resource constraint","resource model","gpu-based","iot applications","effective designs","internet of things (iot)"]},{"p_id":20989,"title":"SmaCH: A framework for smart cultural heritage spaces","abstract":"\u00a9 2014 IEEE.Cultural Heritage represents a world wide resource of inestimable value, attracting millions of visitors every year to monuments, museums and art exhibitions. A fundamental aspect of this resource is represented by its fruition and promotion. Indeed, to achieve a fruition of a cultural space that is sustainable, it is necessary to realize smart solutions for visitors' interaction to enrich their visiting experience. In this paper we present a service-oriented framework aimed to transform indoor Cultural Heritage sites in smart environments, which enforces a set of multimedia and communication services to support the changing of these spaces in an indispensable dynamic instrument for knowledge, fruition and growth for all the people. Following the Internet of Things paradigm, the proposed framework relies on the integration of a Wireless Sensor Network (WSN) with Wi-Fi and Bluetooth technologies to identify, locate and support visitors equipped with their own mobile devices.","keywords_author":["Cultural Heritage","Internet of Things","Smart Environments","SOA Architecture"],"keywords_other":["Service oriented frameworks","Smart solutions","Smart environment","SOA architecture","Bluetooth technology","Cultural heritages","Communication service"],"max_cite":16.0,"pub_year":2015.0,"sources":"['scp']","rawkeys":["smart solutions","soa architecture","smart environments","internet of things","service oriented frameworks","cultural heritage","cultural heritages","smart environment","communication service","bluetooth technology"],"tags":["smart solutions","soa architecture","service oriented frameworks","cultural heritages","smart environment","communication service","bluetooth technology","internet of things (iot)"]},{"p_id":33292,"title":"Deep Network Analyzer (DNA): A Big Data Analytics Platform for Cellular Networks","abstract":"\u00a9 2016 IEEE. In this paper, we present deep network analyzer (DNA), a big data analytics platform for anomaly detection (AD) and root cause analysis (RCA) in mobile wireless networks. DNA is motivated by the growing scale and complexity of cellular networks along with the lack of advanced big data analytics tools for effective network management. It abstracts the RCA process into two modules, namely rule (fingerprint) learning and the module of AD and fingerprint matching. We first develop a rare association rule mining method to learn the symptoms of network anomalies and to build a fingerprint knowledge database from the historic data. Then a statistical machine learning approach is employed to identify the anomalies within the incoming dataset collected via various probes in the network and map the fingerprints of the detected anomalies to the rules in the knowledge database. The DNA platform has been tested using the real production data from the field and has been shown to be a highly effective platform for AD and RCA for large-scale cellular systems serving tens of millions of mobile users.","keywords_author":["Big data","fault diagnosis","Internet of Things","machine learning algorithms","wireless networks"],"keywords_other":["Root cause analysis","Anomaly detection","Network anomalies","Statistical machine learning","Fingerprint matching","Knowledge database","Mobile wireless network","Rare association rules"],"max_cite":2.0,"pub_year":2017.0,"sources":"['scp']","rawkeys":["rare association rules","big data","fingerprint matching","anomaly detection","machine learning algorithms","network anomalies","internet of things","mobile wireless network","statistical machine learning","root cause analysis","knowledge database","wireless networks","fault diagnosis"],"tags":["rare association rules","big data","fingerprint matching","anomaly detection","machine learning algorithms","network anomalies","mobile wireless network","statistical machine learning","root cause analysis","knowledge database","wireless networks","fault diagnosis","internet of things (iot)"]},{"p_id":47632,"title":"FiToViz: A Visualisation Approach for Real-time Risk Situation Awareness","abstract":"IEEE People often face risk-prone situations, that range from a mild event to a severe, life-threatening scenario. Risk situations stem from a number of different scenarios: a health condition, a hazard situation due to a natural disaster, a dangerous situation because one is being subject to a crime or physical violence, among others. The lack of a prompt response, calling for assistance, may severely worsen the consequences. In this paper, we propose a novel visualisation method to track and to identify, in real-time, when a person is under a risk-prone situation. Our visualisation model is capable of providing a decision maker a visual description of the physiological behaviour of an individual, or a group thereof; through it, the decision maker may infer whether further assistance is required, if a risky situation is in progress. Our visualisation is leveraged with a traffic light model of a one-class classifier. This combination allows us to train the decision maker into visualising correct and potential risky or abnormal behaviour.","keywords_author":["Biomedical monitoring","Data visualization","IoT","Machine Learning","Monitoring","One-class classification","Personal risk detection","Real-time systems","Visualisation","Visualization","Wearable sensors","Wearable sensors"],"keywords_other":["Biomedical monitoring","Abnormal behaviours","One-class Classification","Natural disasters","Personal risks","Situation awareness","Dangerous situations","One-class classifier"],"max_cite":0.0,"pub_year":2017.0,"sources":"['scp']","rawkeys":["biomedical monitoring","one-class classification","wearable sensors","abnormal behaviours","natural disasters","machine learning","data visualization","one-class classifier","personal risks","visualization","real-time systems","iot","personal risk detection","monitoring","situation awareness","visualisation","dangerous situations"],"tags":["biomedical monitoring","visualisation","one-class classification","wearable sensors","abnormal behaviours","natural disasters","machine learning","data visualization","one-class classifier","personal risks","situational awareness","real-time systems","personal risk detection","monitoring","visualization","internet of things (iot)","dangerous situations"]},{"p_id":37393,"title":"OpenSHS: Open smart home simulator","abstract":"\u00a9 2017 by the authors. Licensee MDPI, Basel, Switzerland. This paper develops a new hybrid, open-source, cross-platform 3D smart home simulator, OpenSHS, for dataset generation. OpenSHS offers an opportunity for researchers in the field of the Internet of Things (IoT) and machine learning to test and evaluate their models. Following a hybrid approach, OpenSHS combines advantages from both interactive and model-based approaches. This approach reduces the time and efforts required to generate simulated smart home datasets. We have designed a replication algorithm for extending and expanding a dataset. A small sample dataset produced, by OpenSHS, can be extended without affecting the logical order of the events. The replication provides a solution for generating large representative smart home datasets. We have built an extensible library of smart devices that facilitates the simulation of current and future smart home environments. Our tool divides the dataset generation process into three distinct phases: first design: the researcher designs the initial virtual environment by building the home, importing smart devices and creating contexts; second, simulation: the participant simulates his\/her context-specific events; and third, aggregation: the researcher applies the replication algorithm to generate the final dataset. We conducted a study to assess the ease of use of our tool on the System Usability Scale (SUS).","keywords_author":["Internet of things","Machine learning","Simulation","Smart home","Visualisation"],"keywords_other":["System Usability Scale (SUS)","Smart homes","Internet of thing (IOT)","Simulation","Replication algorithm","Smart home simulators","Model based approach","Extensible library"],"max_cite":1.0,"pub_year":2017.0,"sources":"['scp']","rawkeys":["model based approach","visualisation","smart home simulators","smart homes","replication algorithm","internet of thing (iot)","internet of things","machine learning","extensible library","system usability scale (sus)","smart home","simulation"],"tags":["model based approach","visualisation","smart home simulators","simulation","smart homes","replication algorithm","machine learning","system usability scale","extensible library","internet of things (iot)"]},{"p_id":33299,"title":"Using big data analytics to extract disease surveillance information from point of care diagnostic machines","abstract":"\u00a9 2017 Elsevier B.V. This paper explains a novel approach for knowledge discovery from data generated by Point of Care (POC) devices. A very important element of this type of knowledge extraction is that the POC generated data would never be identifiable, thereby protecting the rights and the anonymity of the individual, whilst still allowing for vital population-level evidence to be obtained. This paper also reveals a real-world implementation of the novel approach in a big data analytics system. Using Internet of Things (IoT) enabled POC devices and the big data analytics system, the data can be collected, stored, and analyzed in batch and real-time modes to provide a detailed picture of a healthcare system as well to identify high-risk populations and their locations. In addition, the system offers benefits to national health authorities in forms of optimized resource allocation (from allocating consumables to finding the best location for new labs) thus supports efficient and timely decision-making processes.","keywords_author":["Big data analytics","Global health","Internet of Things","Machine generated data","Machine learning","Point of care"],"keywords_other":["Global health","Disease surveillance","Decision making process","Point of care diagnostic","Point of care","Real-world implementation","Internet of Things (IOT)","Data analytics"],"max_cite":2.0,"pub_year":2017.0,"sources":"['scp']","rawkeys":["machine generated data","real-world implementation","point of care diagnostic","data analytics","point of care","internet of things","machine learning","decision making process","disease surveillance","big data analytics","internet of things (iot)","global health"],"tags":["machine generated data","real-world implementation","point of care diagnostic","data analytics","point of care","machine learning","decision making process","disease surveillance","big data analytics","internet of things (iot)","global health"]},{"p_id":37397,"title":"Design of an loT Based autonomous vehicle with the aid of computer vision","abstract":"A web controlled and partially autonomous vehicle system is presented in this paper. It highlights the idea to develop a remote controlled car which can be driven from anywhere using internet over a secured server. This car will also have limited automation features like traffic light detection, obstacle avoidance system and lane detection system so that it can drive itself safely in case of connectivity failure. The main goal here is to minimize the risk of human life and ensure highest safety during driving. At the same time the car will assure comfort and convenience to the controller. A miniature car including the above features has been developed which showed optimum performance in a simulated environment. The system mainly consists of a Raspberry Pi, an Arduino, a Picamera, a sonar module, a web interface and internet modem. The Raspberry Pi was mainly used for the Computer Vision algorithms and for streaming video through internet. The proposed system is very cheap and very efficient in terms of automation.","keywords_author":["Apache web server","Arduino uno","Computer vision","Internet of things(IoT)","Machine learning","OpenCV","Picamera","Python","Raspberry PI","Sonar module"],"keywords_other":["Apache web server","Python","Picamera","Arduino uno","Internet of Things (IOT)","OpenCV"],"max_cite":1.0,"pub_year":2017.0,"sources":"['scp']","rawkeys":["sonar module","machine learning","python","arduino uno","internet of things (iot)","raspberry pi","computer vision","picamera","apache web server","opencv","internet of things(iot)"],"tags":["sonar module","machine learning","python","arduino uno","raspberry pi","computer vision","picamera","apache web server","opencv","internet of things (iot)"]},{"p_id":21025,"title":"Analysis of Eight Data Mining Algorithms for Smarter Internet of Things (IoT)","abstract":"\u00a9 2016 The Authors. Internet of Things (IoT) is set to revolutionize all aspects of our lives. The number of objects connected to IoT is expected to reach 50 billion by 2020, giving rise to an enormous amounts of valuable data. The data collected from the IoT devices will be used to understand and control complex environments around us, enabling better decision making, greater automation, higher efficiencies, productivity, accuracy, and wealth generation. Data mining and other artificial intelligence methods would play a critical role in creating smarter IoTs, albeit with many challenges. In this paper, we examine the applicability of eight well-known data mining algorithms for IoT data. These include, among others, the deep learning artificial neural networks (DLANNs), which build a feed forward multi-layer artificial neural network (ANN) for modelling high-level data abstractions. Our preliminary results on three real IoT datasets show that C4.5 and C5.0 have better accuracy, are memory efficient and have relatively higher processing speeds. ANNs and DLANNs can provide highly accurate results but are computationally expensive.","keywords_author":["Artificial Neural Networks (ANNs)","Big Data","C4.5","C5.0","Deep Learning ANNs (DLANNs)","Internet of Things (IoT)","K-Nearest Neighbours (KNN)","Linear Discriminant Analysis (LDA)","Na\u00efve Bayes (NB)","Smart Cities","Support Vector Machine (SVM)"],"keywords_other":["Deep learning","K nearest neighbours (k-NN)","C5.0","Smart cities","Linear discriminant analysis","Internet of Things (IOT)","C4.5"],"max_cite":15.0,"pub_year":2016.0,"sources":"['scp', 'ieee']","rawkeys":["c5.0","artificial neural networks (anns)","na\u00efve bayes (nb)","linear discriminant analysis","k-nearest neighbours (knn)","c4.5","deep learning","big data","support vector machine (svm)","deep learning anns (dlanns)","k nearest neighbours (k-nn)","linear discriminant analysis (lda)","internet of things (iot)","smart cities"],"tags":["c5.0","linear discriminant analysis","big data","neural networks","smart cities","machine learning","deep learning anns (dlanns)","naive bayes","k-nearest neighbors","internet of things (iot)","decision trees"]},{"p_id":43560,"title":"LSA Based Smart Assessment Methodology for SDN Infrastructure in IoT Environment","abstract":"\u00a9 2018 Springer Science+Business Media, LLC, part of Springer Nature The Software Defined Network (SDN) is merged in the Internet of Things (IoT) to interconnect large and complex networks. It is used in the education system to interconnect students and teacher by heterogenous IoT devices. In this paper, the SDN-based IoT model for students\u2019 Interaction is proposed which interconnects students to a teacher in a smart city environment. The students and teachers are free to move to anywhere, anytime and with any hardware. An architecture model for students\u2019 teacher\u2019s interaction in IoT is proposed which shows the details procedure about the interaction of teacher with students for electronic assessment. The SDN solves the scalability and interoperability issues between their heterogenous IoT devices. A Methodology for Students\u2019 Answer Assessment using Latent Semantic Analysis (LSA) is proposed which calculates the semantic similarity between teacher\u2019s question and students\u2019 answers. The LSA is used to calculate semantic similarity between text documents. It is used to mark the students\u2019 answers automatically by semantics. The Students\u2019 can see results through their IoT devices just after finishing the examination with more accurate marks We have collected fifty (50) undergraduate students\u2019 data from Learning Management System (LMS) of Virtual University (VU) of Pakistan. The experiment is implemented on eighteen (18) students\u2019 answers in R Studio with R version 3.4.2. Teachers are provided with four (4) bins of the mark while the proposed method assigns accurate marks. The experimental results show that the proposed methodology gave accurate results as compared to teacher\u2019s marks.","keywords_author":["Internet of Things","Latent Semantic Analysis","Machine learning","Semantic similarity","Software define network","Technology enhanced assessment"],"keywords_other":["Learning management system","Internet of thing (IOT)","Assessment methodologies","Latent Semantic Analysis","Semantic similarity","Undergraduate students","Architecture modeling","Virtual university"],"max_cite":0.0,"pub_year":2018.0,"sources":"['scp']","rawkeys":["technology enhanced assessment","virtual university","software define network","learning management system","internet of thing (iot)","internet of things","machine learning","semantic similarity","architecture modeling","assessment methodologies","undergraduate students","latent semantic analysis"],"tags":["technology enhanced assessment","virtual university","software-defined networking","machine learning","semantic similarity","language model","internet of things (iot)","architecture modeling","assessment methodologies","undergraduate students","latent semantic analysis"]},{"p_id":27177,"title":"Emotion-Aware Connected Healthcare Big Data Towards 5G","abstract":"IEEE The recent development of big data-oriented wireless technologies in terms of emerging 5G, edge computing, interconnected devices of the Internet of things (IoT), and data analytics, as well as techniques, have enabled connected healthcare services for a happier and healthier life. Although the quality of the healthcare services can be enhanced through big data-oriented wireless technologies, however, the challenges remain for not considering emotional care, especially for children, elderly, and mentally ill people. In this paper, we propose an emotion-aware connected healthcare system using a powerful emotion detection module. Different IoT devices are used to capture speech and image signals of a patient in a smart home scenario. These signals are used as the input to the emotion detection module. Speech and image signals are processed separately, and classification scores using these signals are fused to produce a final score to take a decision about the emotion. If the emotion is detected as pain, caregivers can visit the patient. Several experiments were performed to validate the proposed system, and good accuracies, up to 99.87%, were achieved for emotion detection. The proposed framework would greatly contribute personalized and seamless emotion-aware healthcare services towards 5G.","keywords_author":["5G mobile communication","5G.","Big Data","Big data","Emotion recognition","emotion recognition","Emotion-aware healthcare","Feature extraction","IoT","local binary pattern","Medical services","Speech","Wireless sensor networks"],"keywords_other":["Internet of thing (IOT)","Medical services","Emotion recognition","Connected healthcares","Mobile communications","Wireless technologies","Healthcare services","Local binary patterns"],"max_cite":6.0,"pub_year":2017.0,"sources":"['scp']","rawkeys":["medical services","local binary pattern","healthcare services","big data","connected healthcares","internet of thing (iot)","mobile communications","wireless technologies","emotion recognition","feature extraction","local binary patterns","5g mobile communication","iot","speech","wireless sensor networks","emotion-aware healthcare","5g"],"tags":["medical services","healthcare services","big data","connected healthcares","mobile communications","emotion recognition","feature extraction","local binary patterns","5g mobile communication","speech","wireless sensor networks","emotion-aware healthcare","5g","internet of things (iot)","wireless technology"]},{"p_id":6705,"title":"Edge Computing: Vision and Challenges","abstract":"\u00a9 2014 IEEE. The proliferation of Internet of Things (IoT) and the success of rich cloud services have pushed the horizon of a new computing paradigm, edge computing, which calls for processing the data at the edge of the network. Edge computing has the potential to address the concerns of response time requirement, battery life constraint, bandwidth cost saving, as well as data safety and privacy. In this paper, we introduce the definition of edge computing, followed by several case studies, ranging from cloud offloading to smart home and city, as well as collaborative edge to materialize the concept of edge computing. Finally, we present several challenges and opportunities in the field of edge computing, and hope this paper will gain attention from the community and inspire more research in this direction.","keywords_author":["Edge computing","Internet of Things (IoT)","smart home and city"],"keywords_other":["Smart homes","Cloud services","Data safeties","Edge computing","Computing paradigm","Time requirements","Internet of Things (IOT)","Case-studies"],"max_cite":203.0,"pub_year":2016.0,"sources":"['ieee', 'scp', 'wos']","rawkeys":["time requirements","smart homes","case-studies","smart home and city","edge computing","computing paradigm","cloud services","data safeties","internet of things (iot)"],"tags":["time requirements","smart homes","case studies","smart home and city","edge computing","computing paradigm","cloud services","data safeties","internet of things (iot)"]},{"p_id":37431,"title":"The Sputnik of servgoods: Autonomous vehicles","abstract":"\u00a9 2017, Systems Engineering Society of China and Springer-Verlag Berlin Heidelberg.In an earlier paper (Tien 2015), the author defined the concept of a servgood, which can be thought of as a physical good or product enveloped by a services-oriented layer that makes the good smarter or more adaptable and customizable for a particular use. Adding another layer of physical sensors could then enhance its smartness and intelligence, especially if it were to be connected with each other or with other servgoods through the Internet of Things. Such sensed servgoods are becoming the products of the future. Indeed, autonomous vehicles can be considered the exemplar servgoods of the future; it is about decision informatics and embraces the advanced technologies of sensing (i.e., Big Data), processing (i.e., real-time analytics), reacting (i.e., real-time decision-making), and learning (i.e., deep learning). Since autonomous vehicles constitute a huge quality-of-life disruption, it is also critical to consider its policy impact on privacy and security, regulations and standards, and liability and insurance. Finally, just as the Soviet Union inaugurated the space age on October 4, 1957, with the launch of Sputnik, the first man-made object to orbit the Earth, the U. S. has inaugurated an age of automata or autonomous vehicles that can be considered to be the U. S. Sputnik of servgoods, with the full support of the U. S. government, the U. S. auto industry, the U. S. electronic industry, and the U.S. higher educational enterprise.","keywords_author":["Autonomous vehicles","Big Data","cloud computing","decision informatics","deep learning","goods","insurance","Internet of Things","liability","privacy","real-time analytics","regulations","security","sensors","servgoods","services","Sputnik","standards"],"keywords_other":["Deep learning","regulations","servgoods","goods","security","Sputnik","Informatics","services","Autonomous Vehicles","Real-time analytics"],"max_cite":1.0,"pub_year":2017.0,"sources":"['scp']","rawkeys":["big data","sensors","internet of things","sputnik","privacy","servgoods","insurance","autonomous vehicles","regulations","deep learning","security","informatics","cloud computing","real-time analytics","standards","goods","decision informatics","liability","services"],"tags":["big data","sensors","sputnik","internet of things (iot)","privacy","machine learning","servgoods","insurance","autonomous vehicles","security","regulation","informatics","cloud computing","real-time analytics","standards","goods","decision informatics","liability","services"]},{"p_id":33342,"title":"A New Deep-Q-Learning-Based Transmission Scheduling Mechanism for the Cognitive Internet of Things","abstract":"IEEE Cognitive networks (CNs) are one of the key enablers for the Internet of Things (IoT), where CNs will play an important role in the future internet in several application scenarios, such as healthcare, agriculture, environment monitoring, and smart metering. However, the current low packet transmission efficiency of IoT faces a problem of the crowded spectrum for the rapidly increasing popularities of various wireless applications. Hence, the IoT that uses the advantages of cognitive technology, namely the cognitive radio-based Internet of Things (CIoT), is a promising solution for IoT applications. A major challenge in CIoT is the packet transmission efficiency using CNs. Therefore, a new Q-learning-based transmission scheduling mechanism using deep learning for the CIoT is proposed to solve the problem of how to achieve the appropriate strategy to transmit packets of different buffers through multiple channels to maximize the system throughput. A Markov decision process based model is formulated to describe the state transformation of the system. A relay is used to transmit packets to the sink for the other nodes. To maximize the system utility in different system states, the reinforcement learning method, i.e., the Q learning algorithm, is introduced to help the relay to find the optimal strategy. In addition, the stacked auto-encoders deep learning model is used to establish the mapping between the state and the action to accelerate the solution of the problem. Finally, the experimental results demonstrate that the new action selection method can converge after a certain number of iterations. Compared with other algorithms, the proposed method can better transmit packets with less power consumption and packet loss.","keywords_author":["Bit error rate","cognitive networks","deep learning.","Internet of Things","Internet of Tings","Machine learning","Markov decision process","Q learning","Relays","Scheduling","Throughput","Wireless sensor networks"],"keywords_other":["Transmission scheduling","Internet of thing (IOT)","Markov Decision Processes","Packet transmission efficiencies","Q-learning","Cognitive network","Reinforcement learning method","Relays"],"max_cite":2.0,"pub_year":2017.0,"sources":"['scp', 'wos']","rawkeys":["internet of thing (iot)","internet of things","q-learning","wireless sensor networks","transmission scheduling","packet transmission efficiencies","scheduling","internet of tings","machine learning","markov decision processes","relays","throughput","markov decision process","deep learning","reinforcement learning method","bit error rate","cognitive network","q learning","cognitive networks"],"tags":["packet transmission efficiencies","bit error rate","cognitive network","scheduling","internet of tings","machine learning","markov decision processes","relays","q-learning","reinforcement learning method","throughput","transmission scheduling","wireless sensor networks","internet of things (iot)"]},{"p_id":45630,"title":"Decision Tree and Random Forest Implementations for Fast Filtering of Sensor Data","abstract":"\u00a9 2017 IEEE. With increasing capabilities of energy efficient systems, computational technology can be deployed, virtually everywhere. Machine learning has proven a valuable tool for extracting meaningful information from measured data and forms one of the basic building blocks of ubiquitous computing. In high-throughput applications, measurements are rapidly taken to monitor physical processes. This brings modern communication technologies to its limits. Therefore, only a subset of measurements, the interesting ones, should be further processed and possibly communicated to other devices. In this paper, we investigate architectural characteristics of embedded systems for filtering high-volume sensor data before further processing. In particular, we investigate implementations of decision trees and random forests for the classical von-Neumann computing architecture and custom circuits by the means of field programmable gate arrays.","keywords_author":["decision trees","Field programmable gate arrays (FPGA)","Internet of Things (IoT)","machine learning (ML)","random forest"],"keywords_other":["Random forests","Modern Communication Technologies","Energy efficient systems","Basic building block","Computational technology","Context","Internet of Things (IOT)","Extraterrestrial measurements"],"max_cite":0.0,"pub_year":2018.0,"sources":"['scp', 'ieee']","rawkeys":["modern communication technologies","basic building block","machine learning (ml)","computational technology","field programmable gate arrays (fpga)","random forests","energy efficient systems","context","extraterrestrial measurements","random forest","internet of things (iot)","decision trees"],"tags":["modern communication technologies","basic building block","machine learning","fpga","random forests","energy efficient systems","computer technology","context","extraterrestrial measurements","internet of things (iot)","decision trees"]},{"p_id":576,"title":"Deep learning approach for cyberattack detection","abstract":"With the accelerated growth of internet of things IoT application in recent years, cities have become smarter to optimize resource and improved the quality of life for residents. On the other hand, the IoT face the severe security problem like confidentiality, integrity, privacy, and availability. To prevent the cyberattack irreversible damage, we propose a framework, called DFEL, to detect the internet intrusion in the IoT environment. Through the experimental results, authors present that DFEL not only boosts classifiers' accuracy to predict cyberattack but also significantly reduce the detection time. Furthermore, the paper demonstrates how the DFEL balance the detection performance and speed.","keywords_author":["cyberattack","dimension reduction","deep feature embedding learning","real time intrusion detection"],"keywords_other":["DFEL balance","learning (artificial intelligence)","Feature extraction","classifier accuracy","deep learning","detection time","Training","Cyberattack","cyberattack detection","Conferences","Machine learning","Machine learning algorithms","security of data","Internet intrusion","Internet of Things","pattern classification","Neurons","IoT security"],"max_cite":null,"pub_year":2018.0,"sources":"['ieee']","rawkeys":["machine learning algorithms","internet of things","cyberattack detection","deep feature embedding learning","dfel balance","conferences","real time intrusion detection","detection time","iot security","machine learning","neurons","dimension reduction","learning (artificial intelligence)","cyberattack","deep learning","training","pattern classification","classifier accuracy","security of data","feature extraction","internet intrusion"],"tags":["machine learning algorithms","internet of things (iot)","dimensionality reduction","deep feature embedding learning","dfel balance","conferences","cyber-attacks","real time intrusion detection","detection time","machine learning","iot security","cyber-attack detection","neurons","training","pattern classification","classifier accuracy","security of data","feature extraction","internet intrusion"]},{"p_id":37444,"title":"Augmented ontology by handshaking with machine learning","abstract":"\u00a9 2017 Global IT Research Institute - GiRI.Artificial intelligence products are already around us and will be emerging dramatically a lot in near future. Artificial intelligence is all about data analysis. When it comes to data analysis, there are two representative techniques: machine learning and semantic technology. They stand on the other side from where to begin analysis. Simply speaking, machine learning is based on the data while semantic technology relies on human domain knowledge (human learning). What if collected data are insufficient to reflect whole phenomenon? This is a limitation of machine learning. What if circumstance changes a lot as time goes by? Manual rule updating by experts is not a good solution in that circumstance. Based on these observations, we investigate two approaches and find a good solution which maximizes the advantages of both techniques and mitigates the limitations of them. This paper suggests a novel integration idea to compensate each technology with the other: that is semantic filtering. This paper includes a toy semantic modelling and a machine learning algorithm implementation to realize the proposed concept, semantic filtering.","keywords_author":["Data analysis","Internet of Things","Machine learning","Semantic filtering","Semantic technology"],"keywords_other":["Semantic filtering","Semantic technologies","Semantic modelling","Human learning","Human domain","Manual rules"],"max_cite":1.0,"pub_year":2017.0,"sources":"['scp']","rawkeys":["semantic filtering","semantic modelling","internet of things","machine learning","semantic technologies","human learning","semantic technology","human domain","data analysis","manual rules"],"tags":["semantic filtering","machine learning","semantic model","semantic technologies","manual rules","human learning","human domain","data analysis","internet of things (iot)"]},{"p_id":29255,"title":"Framework and development of fault detection classification using IoT device and cloud environment","abstract":"\u00a9 2017 The Society of Manufacturing EngineersWhile Cyber-physical system (CPS) is considered as a key foundation for cyber manufacturing, many related frameworks and applications have been provided. This research suggests a new and effective CPS architecture for supporting multi-sites and multi-products manufacturing. As target processes, the manufacturing processes for vehicles\u2019 High Intensity Discharge (HID) headlight and cable modules are considered. These modules are manufactured with several multi-manufacturing sites consisting of internal manufacturing tasks and intermediate outsourcing processes. In addition, they produce multiple types of HID cable modules with different components. These issues make it difficult to improve the qualities of the overall processes and to control those considering overall manufacturing plants and processes. In order to overcome these limitations, this research provides an Internet of Things (IoT) embedded cloud control architecture. The mixed flow issues are overcome with the cloud control server with the suggested framework. The developed IoT device detects several system status and transmits the signals. The data is analyzed for the fault detection classification (FDC) mechanism using deep learning based analytics. Then, the cyber manufacturing based simulation is executed using the provided multi-products queueing network model. The estimated simulation results is used for generating dynamic manufacturing decisions reflecting the real-time changes of the production environment. The suggested framework and its implementations can be used for various industrial processes and applications.","keywords_author":["Cloud computing","Cyber-physical system (CPS)","Deep belief network based deep learning (DBN-DL)","Fault detect classification (FDC)","Internet of Things (IoT)"],"keywords_other":["Fault detect","Queueing network model","Manufacturing process","Cyber-physical systems (CPS)","Deep belief networks","Production environments","High intensity discharges","Internet of Things (IOT)"],"max_cite":4.0,"pub_year":2017.0,"sources":"['scp']","rawkeys":["high intensity discharges","cloud computing","manufacturing process","production environments","fault detect classification (fdc)","cyber-physical systems (cps)","cyber-physical system (cps)","internet of things (iot)","fault detect","queueing network model","deep belief networks","deep belief network based deep learning (dbn-dl)"],"tags":["cyber-physical systems","high intensity discharges","cloud computing","manufacturing process","production environments","fault detect classification (fdc)","fault detection","internet of things (iot)","queueing network model","deep belief networks","deep belief network based deep learning (dbn-dl)"]},{"p_id":45643,"title":"Machine Learning: A Convergence of Emerging Technologies in Computing","abstract":"\u00a9 2018, Springer International Publishing AG. Machine Learning (ML) is the convergence of different disciplines in science and technology. While it is conceived, ML is part of computer science however, in its essence, it borrows or utilizes methods from other classic disciplines and mature computing theories and technologies, such as statistics, computational algorithms, optimization, and data mining. In this paper, we explore how these disciplines and technologies work hand in hand to prepare a passionate researcher gains a comprehensive perspective for being an ML expert. We have proposed a roadmap to show how different disciplines and technologies contribute to the ML foundation and we discuss each part of the roadmap separately. Moreover, to apply the proposed roadmap in practical terms, we also present how to use the proposed roadmap in the context of IoT and Fog Computing. The main contribution of this paper is to provide a guideline by developing a roadmap for foundational requirements of being a Machine Learning subject matter expert for the researchers or industry experts.","keywords_author":["Algorithms","Deep Learning","Fog Computing","IoT","Machine Learning","Optimization","Statistics"],"keywords_other":["Emerging technologies","Computational algorithm","Hand in hands","Computing theory","Subject matter experts","Roadmap","Industry experts","Science and Technology"],"max_cite":0.0,"pub_year":2018.0,"sources":"['scp']","rawkeys":["computing theory","industry experts","science and technology","statistics","hand in hands","deep learning","emerging technologies","fog computing","subject matter experts","machine learning","optimization","algorithms","iot","computational algorithm","roadmap"],"tags":["science and technology","industry experts","statistics","hand in hands","fog computing","emerging technologies","subject matter experts","machine learning","computational theories","optimization","algorithms","computational algorithm","roadmap","internet of things (iot)"]},{"p_id":4685,"title":"The Internet of Things: A survey","abstract":"This paper addresses the Internet of Things. Main enabling factor of this promising paradigm is the integration of several technologies and communications solutions. Identification and tracking technologies, wired and wireless sensor and actuator networks, enhanced communication protocols (shared with the Next Generation Internet), and distributed intelligence for smart objects are just the most relevant. As one can easily imagine, any serious contribution to the advance of the Internet of Things must necessarily be the result of synergetic activities conducted in different fields of knowledge, such as telecommunications, informatics, electronics and social science. In such a complex scenario, this survey is directed to those who want to approach this complex discipline and contribute to its development. Different visions of this Internet of Things paradigm are reported and enabling technologies reviewed. What emerges is that still major issues shall be faced by the research community. The most relevant among them are addressed in details. \u00a9 2010 Elsevier B.V. All rights reserved.","keywords_author":["Internet of Things","Pervasive computing","RFID systems"],"keywords_other":["Next generation Internet","Tracking technology","Enabling technologies","Internet of things","Wired and wireless","Research communities","Smart objects","Distributed intelligence","RFID systems","Informatics","Pervasive computing","Communication protocols"],"max_cite":4565.0,"pub_year":2010.0,"sources":"['ieee', 'scp', 'wos']","rawkeys":["informatics","tracking technology","next generation internet","rfid systems","internet of things","smart objects","communication protocols","wired and wireless","research communities","distributed intelligence","pervasive computing","enabling technologies"],"tags":["informatics","tracking technology","next generation internet","rfid systems","smart objects","pervasive computing","communication protocols","wired and wireless","research communities","distributed intelligence","internet of things (iot)","enabling technologies"]},{"p_id":29264,"title":"Medical warning system based on Internet of Things using fog computing","abstract":"\u00a9 2016 IEEE. Remote patient monitoring is essential for many patients that are suffering from acute diseases such as different heart conditions. Continuous health monitoring can provide medical services that consider the current medical state of the patient and to predict or early-detect future potentially critical situations. In this regard, Internet of Things as a multidisciplinary paradigm can provide profound impacts. However, the current IoT-based systems may encounter difficulties to provide continuous and real time patient monitoring due to issues in data analytics. In this paper, we introduce a new IoT-based approach to offer smart medical warning in personalized patient monitoring. The proposed approach consider local computing paradigm enabled by machine learning algorithms and automate management of system components in computing section. The proposed system is evaluated via a case study concerning continuous patient monitoring to early-detect patient deterioration via arrhythmia in ECG signal.","keywords_author":["Autonomic computing","Fog Comouting","Internet of Things","machine learning","Patient monitoring"],"keywords_other":["Autonomic Computing","Medical services","Computing paradigm","System components","Health monitoring","ECG signals","Real time","Data analytics"],"max_cite":4.0,"pub_year":2017.0,"sources":"['scp']","rawkeys":["health monitoring","data analytics","real time","fog comouting","internet of things","machine learning","autonomic computing","ecg signals","computing paradigm","system components","medical services","patient monitoring"],"tags":["health monitoring","data analytics","real time","fog comouting","machine learning","autonomic computing","ecg signals","computing paradigm","system components","medical services","patient monitoring","internet of things (iot)"]},{"p_id":53841,"title":"A simulation study of a smart living IoT solution for remote elderly care","abstract":"\u00a9 2018 IEEE.We report a simulation study of a smart living IoT solution for elderly people living in their own houses. Our study was conducted in the context of BoIT project in Sweden that investigates the use of various IoT devices for remote housing and care-giving services. We focus on a carephone device that enables to establish a voice connection via IP with care givers or relatives. We have developed a simulation model to study the IoT solution for elderly care in the Vaxjo municipality in Sweden. The simulation model can be used to address various issues, such as determining the lack or excess of resources or long waiting times, and study the system behavior when the number of alarms is increased. Simulation results indicate that a 15% increase in the arrivals rate would cause unacceptable long waiting times for patients to receive the care.","keywords_author":["Internet of Things (IoT)","remote elderly care","simulation","smart living"],"keywords_other":null,"max_cite":0.0,"pub_year":2018.0,"sources":"['scp']","rawkeys":["simulation","remote elderly care","internet of things (iot)","smart living"],"tags":["simulation","remote elderly care","internet of things (iot)","smart living"]},{"p_id":10836,"title":"Distributed attack detection scheme using deep learning approach for Internet of Things","abstract":"Cybersecurity continues to be a serious issue for any sector in the cyberspace as the number of security breaches is increasing from time to time. It is known that thousands of zero-day attacks are continuously emerging because of the addition of various protocols mainly from Internet of Things (IoT). Most of these attacks are small variants of previously known cyber-attacks. This indicates that even advanced mechanisms such as traditional machine learning systems face difficulty of detecting these small mutants of attacks over time. On the other hand, the success of deep learning (DL) in various big data fields has drawn several interests in cybersecurity fields. The application of DL has been practical because of the improvement in CPU and neural network algorithms aspects. The use of DL for attack detection in the cyberspace could be a resilient mechanism to small mutations or novel attacks because of its high-level feature extraction capability. The self-taught and compression capabilities of deep learning architectures are key mechanisms for hidden pattern discovery from the training data so that attacks are discriminated from benign traffic. This research is aimed at adopting a new approach, deep learning, to cybersecurity to enable the detection of attacks in social internet of things. The performance of the deep model is compared against traditional machine learning approach, and distributed attack detection is evaluated against the centralized detection system. The experiments have shown that our distributed attack detection system is superior to centralized detection systems using deep learning model. It has also been demonstrated that the deep model is more effective in attack detection than its shallow counter parts. (C) 2017 Elsevier B.V. All rights reserved.","keywords_author":["Cybersecurity","Deep learning","Fog networks","Internet of Things","Smart cities","Cybersecurity","Deep learning","Internet of Things","Fog networks","Smart cities"],"keywords_other":["Machine learning approaches","Cyber security","High-level feature extractions","Learning architectures","Centralized detections","Neural network algorithm","Distributed attack detection","CODE","Internet of Things (IOT)"],"max_cite":5.0,"pub_year":2018.0,"sources":"['ieee', 'scp', 'wos']","rawkeys":["learning architectures","cybersecurity","smart cities","deep learning","high-level feature extractions","neural network algorithm","internet of things","cyber security","fog networks","centralized detections","code","machine learning approaches","internet of things (iot)","distributed attack detection"],"tags":["learning architectures","smart cities","codes","neural network algorithm","machine learning","fog networking","centralized detections","cyber security","high-level feature extraction","machine learning approaches","internet of things (iot)","distributed attack detection"]},{"p_id":47703,"title":"A study on the fast system recovery: Selecting the number of surrogate nodes for fast recovery in industrial IoT environment","abstract":"\u00a9 2017 IEEE.This paper is based on the previous research that selects the proper surrogate nodes for fast recovery mechanism in industrial IoT (Internet of Things) Environment which uses a variety of sensors to collect the data and exchange the collected data in real-time for creating added value. We are going to suggest the way that how to decide the number of surrogate node automatically in different deployed industrial IoT Environment so that minimize the system recovery time when the central server likes IoT gateway is in failure. We are going to use the network simulator to measure the recovery time depending on the number of the selected surrogate nodes according to the sub-devices which are connected to the IoT gateway.","keywords_author":["Fast Recovery","Industrial IoT(IIoT)","Machine Learning"],"keywords_other":["Fast recovery","Central servers","System recovery","Network simulators","Industrial IoT(IIoT)","Recovery time","Fast systems","Added values"],"max_cite":0.0,"pub_year":2017.0,"sources":"['scp']","rawkeys":["central servers","fast systems","added values","network simulators","machine learning","fast recovery","industrial iot(iiot)","recovery time","system recovery"],"tags":["central servers","fast systems","added values","machine learning","fast recovery","recovery time","network simulation","system recovery","internet of things (iot)"]},{"p_id":29279,"title":"Normalization and dropout for stochastic computing-based deep convolutional neural networks","abstract":"\u00a9 2017. Recently, Deep Convolutional Neural Network (DCNN) has been recognized as the most effective model for pattern recognition and classification tasks. With the fast growing Internet of Things (IoTs) and wearable devices, it becomes attractive to implement DCNNs in embedded and portable systems. However, novel computing paradigms are urgently required to deploy DCNNs that have huge power consumptions and complex topologies in systems with limited area and power supply. Recent works have demonstrated that Stochastic Computing (SC) can radically simplify the hardware implementation of arithmetic units and has the potential to bring the success of DCNNs to embedded systems. This paper introduces normalization and dropout, which are essential techniques for the state-of-the-art DCNNs, to the existing SC-based DCNN frameworks. In this work, the feature extraction block of DCNNs is implemented using an approximate parallel counter, a near-max pooling block and an SC-based rectified linear activation unit. A novel SC-based normalization design is proposed, which includes a square and summation unit, an activation unit and a division unit. The dropout technique is integrated into the training phase and the learned weights are adjusted during the hardware implementation. Experimental results on AlexNet with the ImageNet dataset show that the SC-based DCNN with the proposed normalization and dropout techniques achieves 3.26% top-1 accuracy improvement and 3.05% top-5 accuracy improvement compared with the SC-based DCNN without these two essential techniques, confirming the effectiveness of our normalization and dropout designs.","keywords_author":["Deep convolutional neural networks","Deep learning","Dropout","Normalization"],"keywords_other":["Stochastic computing","Hardware implementations","Internet of thing (IoTs)","Accuracy Improvement","Convolutional neural network","Dropout","Pattern recognition and classification","Normalization"],"max_cite":4.0,"pub_year":2017.0,"sources":"['scp', 'ieee']","rawkeys":["dropout","deep learning","hardware implementations","deep convolutional neural networks","normalization","convolutional neural network","internet of thing (iots)","stochastic computing","pattern recognition and classification","accuracy improvement"],"tags":["dropout","hardware implementations","machine learning","normalization","convolutional neural network","stochastic computing","pattern recognition and classification","accuracy improvement","internet of things (iot)"]},{"p_id":45664,"title":"AI infused fragrance systems for creating memorable customer experience and venue brand engagement","abstract":"\u00a9 Springer International Publishing AG 2018. In today\u2019s competitive business environment creating memorable experiences and emotional connections (Creating customer value through service experiences: An empirical study in the hotel industry. Tourism and Hospitality Management 18, no. 1 (2012): 37\u201353) with consumers is critical to win consumer spending and long-term brand loyalty [1]. Brands want their customers to be in pleasing subliminal scented (Robert Klara, \u201cSomething in the air,\u201d http:\/\/www.adweek.com\/brandmarketing\/something-air-138683\/ creation date: March 2012, access date: January 02, 2017) environments because, as research has shown, even a few microparticles of scent can do a lot of marketing\u2019s heavy lifting, from improving consumer perceptions of quality to increasing the number of store visits. Hence, customer venues such as hotels, retail showrooms, casinos, hospitable and other captive audience places employ HVAC (Heating, ventilation and air conditioning) based scent diffusion system that delivers a seamless olfactory [2] experience to connect with consumers on a deeper emotional level, resulting in a more memorable experience. Current scent diffusion systems, however, use power hungry deployments and dispense periodically, without accounting social mood, geographic local etiquettes, venue-patron occupancy ratios and sudden changes in foot traffic numbers. Thus, resulting sub-optimal user experience that might lead to a poor brand engagement and could incur higher operational costs and thus reduce over all return on the investment (ROI). In this research paper, we propose an innovative approach to create artificial intelligence (AI) infused Fragrance Systems that improve venue experience and operational efficiencies through the application of data science, Big Data Technologies, Edge processing, Supervised machine learning and IoT Sensing. Our system combines pragmatic data science and machine learning algorithms with arty social and mood drivers, albeit data science computed, to create adaptive and artistic fragrance system. The amalgamation data science with human mood influencers is our formula to the innovation that we propose and present a prototyping solution design as well as its application and certain experimental results.","keywords_author":["Adaptive edge","CEP","Decision trees","Edge processing","Internet of things","Machine learning","Scent marketing","TF-IDF","Venue analytics"],"keywords_other":["Adaptive edge","TF-IDF","Venue analytics","Operational efficiencies","Emotional connections","Innovative approaches","Supervised machine learning","Ventilation and air conditioning"],"max_cite":0.0,"pub_year":2018.0,"sources":"['scp']","rawkeys":["adaptive edge","tf-idf","internet of things","machine learning","emotional connections","scent marketing","venue analytics","edge processing","cep","innovative approaches","operational efficiencies","supervised machine learning","ventilation and air conditioning","decision trees"],"tags":["adaptive edge","tf-idf","machine learning","emotional connections","complex event processing","scent marketing","venue analytics","edge processing","ventilation and air conditioning","innovative approaches","operational efficiencies","supervised machine learning","internet of things (iot)","decision trees"]},{"p_id":37473,"title":"Analysis of post-harvest losses: An Internet of Things and machine learning approach","abstract":"\u00a9 2016 IEEE. Reduction of post-harvest losses is a critical component of food security. World population is increasing at an alarming rate and thus is the food requirement. Due to limited cultivable land, increasing the food production to meet the needs of people, solely, cannot be the solution. In this paper, we have proposed to build an end-to-end system for farmers and warehouse managers to reduce post-harvest losses. It will consist of a notification-suggestion system which will include data about the current status of farm, suggestions about correct harvesting time and diseases that might affect the crop in its cultivation stages. The system will also include a prediction system for warehouse managers which will suggest the correct dispatch sequence of the stocks and also the optimum temperature and humidity at which one or more crops can be transported so as incur minimum storage and transportation loss. Here, for the prediction-analysis and suggestions, various statistical and probabilistic techniques such as classification and regression are used.","keywords_author":["Android","Embedded Systems","Internet of Things","Machine Learning","Sensor Application"],"keywords_other":["End-to-end systems","Analysis and suggestions","Machine learning approaches","Android","Optimum temperature","Probabilistic technique","Storage and transportations","Sensor applications"],"max_cite":1.0,"pub_year":2017.0,"sources":"['scp']","rawkeys":["sensor applications","sensor application","embedded systems","internet of things","android","end-to-end systems","machine learning","optimum temperature","storage and transportations","analysis and suggestions","probabilistic technique","machine learning approaches"],"tags":["neural networks","embedded systems","machine learning","end-to-end systems","optimum temperature","sensors applications","storage and transportations","analysis and suggestions","probabilistic technique","machine learning approaches","internet of things (iot)"]},{"p_id":53857,"title":"Analysis of Lightweight Encryption Scheme for Fog-to-Things Communication","abstract":"\u00a9 2018 IEEE. The growing concerns in cybersecurity is preventing unknowns which evolve from time to time. Internet of Things (IoT) is one of the emerging fields that have been applied for smart cities and industries. The promises of IoTs could be confronted with the growth in the number and sophistication of cyberattacks. The extension of digital world into physical environment adds new attack surfaces on the existing security threats of traditional Internet. The major challenge brought about by physical connectivity of IoTs is to implement distributed security mechanisms for resource constrain of IoT devices. As an emerging architecture supporting IoT applications, fog computing can be considered to solve the resource and distribution issues in securing fog-to-things communication. Security functions and services, such as cryptography, could be offloaded to fog nodes to reduce computational and storage burdens on IoT devices. The distribution of fog nodes can also solve the scalability of cloud by reducing central processing and communications. On the other hand, lightweight cryptographic functions, such as elliptic curve cryptography, have been proved to be suitable for embedded systems. In this paper, we have analyzed security challenges in terms of cybersecurity principles and proposed a novel encryption scheme for fog-to-things communication.","keywords_author":["Cybersecurity","elliptic curve cryptography","fog computing","Internet of things","lightweight cryptography","Cybersecurity","Internet of things","fog computing","elliptic curve cryptography","lightweight cryptography"],"keywords_other":["CHALLENGES","Cyber security","Elliptic curve cryptography","Cryptographic functions","Emerging architectures","Authorization","Light-weight cryptography","Lightweight encryption","Internet of Things (IOT)","INTERNET"],"max_cite":0.0,"pub_year":2018.0,"sources":"['scp', 'wos']","rawkeys":["light-weight cryptography","cybersecurity","emerging architectures","cryptographic functions","lightweight cryptography","fog computing","authorization","internet of things","challenges","lightweight encryption","cyber security","internet","elliptic curve cryptography","internet of things (iot)"],"tags":["emerging architectures","cryptographic functions","lightweight cryptography","fog computing","authorization","challenges","lightweight encryption","cyber security","internet","elliptic curve cryptography","internet of things (iot)"]},{"p_id":45666,"title":"Pulse: An adaptive intrusion detection for the internet of things","abstract":"\u00a9 2018 Institution of Engineering and Technology. All rights reserved. The number of diverse interconnected Internet of Things (IoT) devices keeps increasing exponentially, introducing new security and privacy challenges. These devices tend to become more pervasive than mobile phones and already have access to very sensitive personal information such as usernames, passwords, etc., making them a target for cyber-attacks. Given that smart devices are vulnerable to a variety of attacks, they can be considered to be the weakest link for breaking into a secure infrastructure. For instance, IoT devices have recently been employed as part of botnets, such as Mirai, and have launched several of the largest Distributed Denial of Service (DDoS) and spam attacks in history. As a result, there is a need to develop an Intrusion Detection System (IDS) dedicated to monitor IoT ecosystems, which will be able to adapt to this heterogeneous environment and detect malicious activity on the network. In this paper, we describe the initial stages of developing Pulse; a novel IDS for the IoT, which employs Machine Learning (ML) methodologies and is capable of successfully identifying network scanning probing and simple forms of Denial of Service (DoS) attacks.","keywords_author":["Internet of things","Intrusion detection","Machine learning"],"keywords_other":["Heterogeneous environments","Denial of Service","Malicious activities","Intrusion Detection Systems","Sensitive personal informations","Security and privacy","Internet of Things (IOT)","Distributed denial of service"],"max_cite":0.0,"pub_year":2018.0,"sources":"['scp']","rawkeys":["intrusion detection","intrusion detection systems","internet of things","machine learning","distributed denial of service","sensitive personal informations","malicious activities","security and privacy","denial of service","internet of things (iot)","heterogeneous environments"],"tags":["intrusion detection systems","machine learning","distributed denial of service","sensitive personal informations","malicious activities","security and privacy","denial of service","internet of things (iot)","heterogeneous environments"]},{"p_id":43617,"title":"Botanical Internet of Things: Toward Smart Indoor Farming by Connecting People, Plant, Data and Clouds","abstract":"\u00a9 2017, Springer Science+Business Media, LLC. With rapid development of a new generation of communication technology, sensor technology as well as big data technology, the application scenes of Internet of Things (IOT) based on these technologies increase constantly in extensive fields, for instance, there are great contributions of Internet of Things in fields such as smart home, intelligent transportation, intelligent healthcare, intelligent monitoring as well as intelligent agriculture. At present, as for intelligent agriculture, the main focus is monitoring agricultural environment with IOT and M2M technology. In the era of population explosion, agricultural resources such as farmland become more and more insufficient, a indoor intelligent agricultural IOT system is designed and implemented by the author in order to attack this conundrum. And the system directs an new trend for agricultural development. With the capability of parallel extension, the system can connect to large-scale indoor farms gradually thus to make these farms combining with each other organically. Finally, information mining shall be achieved based on large amount of sensing data by utilizing the big data technology and machine learning algorithms, and those derived information shall be adopted as critical reference to support indoor agricultural activities.","keywords_author":["Big data","Cloud computing","Internet of things","Machine learning","Smart green house","Smart indoor farming"],"keywords_other":["Agricultural resources","Smart indoor farming","Intelligent transportation","Agricultural development","Agricultural environments","Communication technologies","Internet of Things (IOT)","Agricultural activities"],"max_cite":0.0,"pub_year":2018.0,"sources":"['scp']","rawkeys":["agricultural activities","agricultural resources","cloud computing","big data","smart indoor farming","agricultural development","internet of things","machine learning","smart green house","communication technologies","intelligent transportation","internet of things (iot)","agricultural environments"],"tags":["agricultural activities","agricultural resources","cloud computing","big data","smart indoor farming","agricultural development","machine learning","smart green house","communication technologies","intelligent transportation","internet of things (iot)","agricultural environments"]},{"p_id":47723,"title":"Stacked Auto-Encoder Optimized Emotion Recognition in Multimodal Wearable Biosensor Network","abstract":"\u00a9 2017, Science Press. All right reserved. Emotional health draws great concern with the enhancement of public health consciousness. Emotional health is closely related to the quality of personal life. Even for some special groups of people, like pilots, soldiers, etc., their emotional states will have impacts on the stability of communities. Traditionally, to evaluate emotional states of human beings relies on the doctors or psychologists to communicate with subjects and give scores based on various questionnaires. This approach is not scientific enough and leads to the difficulties in the emotional health monitoring in daily-life. Emotion recognition enables lifeless sensors and computers to measure and interpret human emotions. It is a procedure consisting of emotion-related bio signals recording, features extraction and emotional states classification, providing scientific evidence for emotional health monitoring and primary diagnosis of potential mental diseases. In the related works concerning emotion recognition, the application scenarios are usually restricted in the hospitals or labs and the common-used classifiers are not suitable for the daily emotion recognition data set. This paper develops a multimodal biosensor network to simplify the sensing framework so that it can finish emotion recognition tasks when subjects are participating in daily tasks without so many disturbances. Several wearable biosensor nodes record multimodal bio signals (electroencephalography, pulse and blood pressure) and transmit them to a body station employing Arduino UNO3 and its expansion boards. The body station works as a web client connecting to a web data center on the Internet by wireless routers or personal hotspots. The web data center is established on NI-PXI 1065 with a static public IP address. The recognition algorithm is carried out in the data center and the results are displayed for authorized web terminals with the assistance of web publishing service supported by LabVIEW. The multimodal wearable biosensor network can provide emotion-related bio signals from which typical features are extracted based on the existing theories. In particular, due to the uncertainties in signal acquisition and feature extraction, a stacked auto-encoder (based on the deep learning theory) optimized emotion recognition method is proposed to improve the recognition process. The stacked auto-encoder helps to pre-learn the feature vectors and with the fine tuning it generates a better scheme for emotion classification phase. There are 9 emotional states for classification according to the Valence-Arousal dimensional model. A two-layer stacked neural network with a softmax classifier is designed to finish the final classification tasks. The experiment convinces that the feature vectors pre-learned by stacked auto-encoder are of higher quality both in centrality and distinguishability based on the similarity evaluation theory. The final recognition rate is also improved approximately 5% compared to related works. The main contributions of this paper are the wearable network-based sensing structure, the stacked auto-encoder optimized multimodal emotion recognition algorithm and the quantitative analysis on 71-day experimental data. This is a novel system for daily emotional health monitoring and can provide scientific suggestions for doctors or guardians. However, in the future, large scale of data should be accumulated. Moreover, the dynamic performance and energy efficiency also need improving.","keywords_author":["Deep learning","Emotion recognition","Internet of Things","Multimodal sensing","Sensor networks","Stacked auto-encoder","Wearable biosensor network"],"keywords_other":["Emotion classification","Stacked neural networks","Emotional health monitoring","Multimodal emotion recognition","Recognition algorithm","Emotion recognition","Multi-modal sensing","Auto encoders"],"max_cite":0.0,"pub_year":2017.0,"sources":"['scp']","rawkeys":["emotional health monitoring","emotion classification","wearable biosensor network","deep learning","multimodal emotion recognition","multi-modal sensing","auto encoders","internet of things","emotion recognition","stacked neural networks","multimodal sensing","recognition algorithm","stacked auto-encoder","sensor networks"],"tags":["emotional health monitoring","emotion classification","wearable biosensor network","multimodal emotion recognition","multi-modal sensing","auto encoders","machine learning","stacked autoencoders","emotion recognition","stacked neural networks","multimodal sensing","recognition algorithm","sensor networks","internet of things (iot)"]},{"p_id":14960,"title":"Smart Electricity Meter Data Intelligence for Future Energy Systems: A Survey","abstract":"\u00a9 2005-2012 IEEE. Smart meters have been deployed in many countries across the world since early 2000s. The smart meter as a key element for the smart grid is expected to provide economic, social, and environmental benefits for multiple stakeholders. There has been much debate over the real values of smart meters. One of the key factors that will determine the success of smart meters is smart meter data analytics, which deals with data acquisition, transmission, processing, and interpretation that bring benefits to all stakeholders. This paper presents a comprehensive survey of smart electricity meters and their utilization focusing on key aspects of the metering process, different stakeholder interests, and the technologies used to satisfy stakeholder interests. Furthermore, the paper highlights challenges as well as opportunities arising due to the advent of big data and the increasing popularity of cloud environments.","keywords_author":["Artificial Intelligence","Automated Meter Infrastructure","Big Data","Cloud computing","Data analytics","Internet of Things","Machine learning","Privacy","Smart Grids","Smart meters"],"keywords_other":["Multiple stakeholders","Cloud environments","Automated Meter Infrastructure","Electricity meters","Stakeholder interest","Smart grid","Environmental benefits","Data analytics"],"max_cite":51.0,"pub_year":2016.0,"sources":"['scp']","rawkeys":["artificial intelligence","cloud computing","electricity meters","privacy","big data","smart grid","data analytics","smart meters","automated meter infrastructure","cloud environments","internet of things","machine learning","environmental benefits","multiple stakeholders","stakeholder interest","smart grids"],"tags":["cloud computing","electricity meters","privacy","big data","smart grid","data analytics","automated meter infrastructure","cloud environments","machine learning","smart meter","environmental benefits","multiple stakeholders","stakeholder interest","internet of things (iot)"]},{"p_id":43642,"title":"Solar Power Generation Forecasting With a LASSO-Based Approach","abstract":"\u00a9 2018 IEEE. The smart grid (SG) has emerged as an important form of the Internet of Things. Despite the high promises of renewable energy in the SG, it brings about great challenges to the existing power grid due to its nature of intermittent and uncontrollable generation. In order to fully harvest its potential, accurate forecasting of renewable power generation is indispensable for effective power management. In this paper, we propose a least absolute shrinkage and selection operator (LASSO)-based forecasting model and algorithm for solar power generation forecasting. We compare the proposed scheme with two representative schemes with three real world datasets. We find that the LASSO-based algorithm achieves a considerably higher accuracy comparing to the existing methods, using fewer training data, and being robust to anomaly data points in the training data, and its variable selection capability also offers a convenient tradeoff between complexity and accuracy, which all make the proposed LASSO-based approach a highly competitive solution to forecasting of solar power generation.","keywords_author":["Generation forecasting","Internet of Things (IoT)","least absolute shrinkage and selection operator (LASSO)","machine learning","renewable energy"],"keywords_other":["Internet of thing (IOT)","Training data","Renewable power generation","Renewable energies","Real-world datasets","Forecasting modeling","Least absolute shrinkage and selection operators","Variable selection"],"max_cite":0.0,"pub_year":2018.0,"sources":"['scp']","rawkeys":["training data","variable selection","renewable energies","internet of thing (iot)","least absolute shrinkage and selection operator (lasso)","least absolute shrinkage and selection operators","machine learning","real-world datasets","renewable energy","renewable power generation","forecasting modeling","generation forecasting","internet of things (iot)"],"tags":["training data","variable selection","renewable energies","machine learning","forecasting models","lasso","real-world datasets","renewable power generation","generation forecasting","internet of things (iot)"]},{"p_id":2687,"title":"Cognitive computation and communication: A complement solution to cloud for IoT","abstract":"\u00a9 2016 IEEE.The Internet of Thing (IoT) is experiencing explosive growth in the number of devices and applications. However, the existing cloud-centric architecture of IoT poses serious challenges regarding network latency, privacy, and energy-efficiency. We have presented COGNICOM+ concept, a brain-inspired software-hardware paradigm, to support IoT's future growth and developed 4 research directions - flexible radio, convolutional neural network accelerator, compressed deep learning, and game theory for reasoning and collaboration - within COGNICOM+. The key idea is to bring computing closer to the end-user while focusing on optimal uses of local smart application gateway and cloud computing. COGNICOM+ consists of two key components: Cognitive Engine (CE) and Smart Connectivity (SC). The cognitive engine is powered by deep-learning algorithms integrated with game-Theoretic decision analytics, implemented on a low-power application-specific integrated circuit. It provides cognitive functions to smart objects. The smart connectivity integrates neural network inspired designs of cognitive radio, transceivers, and baseband processors. The SC provides flexible and reliable connections to IoT objects and optimally distributes communication resources.","keywords_author":["CNN accelerator","COGNICOM","Cognitive engine","Compressed CNN","Deep learning","Flexible radio","Game theory","IoT","Smart application gateway","Smart connectivity"],"keywords_other":["Smart applications","Compressed CNN","Cognitive engines","Flexible radio","Smart connectivity","COGNICOM"],"max_cite":1.0,"pub_year":2016.0,"sources":"['scp', 'ieee']","rawkeys":["flexible radio","smart connectivity","cognitive engines","deep learning","cognitive engine","game theory","cnn accelerator","cognicom","iot","compressed cnn","smart application gateway","smart applications"],"tags":["flexible radio","smart connectivity","game theory","machine learning","cnn accelerator","cognicom","smart application gateway","compressed cnn","cognitive engineering","internet of things (iot)","smart applications"]},{"p_id":646,"title":"Deep learning and reconfigurable platforms in the internet of things: Challenges and opportunities in algorithms and hardware","abstract":"As the Internet of Things (IoT) continues its run as one of the most popular technology buzzwords of today, the discussion really turns from how the massive data sets are collected to how value can be derived from them, i.e., how to extract knowledge out of such (big) data. IoT devices are used in an ever-growing number of application domains (see Figure 1), ranging from sports gadgets (e.g., Fitbits and Apple Watches) or more serious medical devices (e.g., pacemakers and biochips) to smart homes, cities, and self-driving cars, to predictive maintenance in missioncritical systems (e.g., in nuclear power plants or airplanes). Such applications introduce endless possibilities for better understanding, learning, and informedly acting (i.e., situational awareness and actionable information in government lingo). Although rapid expansion of devices and sensors brings terrific opportunities for taking advantage of terabytes of machine data, the mind-boggling task of understanding growth of data remains and heavily relies on artificial intelligence and machine learning [1], [2].","keywords_author":null,"keywords_other":["BIG DATA ANALYTICS","SMART HOME","DESIGN","Mission critical systems","Medical Devices","Neural networks","Predictive maintenance","Data mining","OPTIMIZATION ALGORITHM","SYSTEMS","Machine learning","Situational awareness","Smart homes","Self-driving cars","Massive data sets","IMPLEMENTATION","Big Data","Reconfigurable plat-forms","Internet of Things","Smart cities","FPGA","Internet of thing (IOT)","IOT"],"max_cite":0.0,"pub_year":2018.0,"sources":"['scp', 'wos', 'ieee']","rawkeys":["design","big data","smart homes","internet of thing (iot)","internet of things","optimization algorithm","mission critical systems","reconfigurable plat-forms","massive data sets","machine learning","fpga","self-driving cars","situational awareness","data mining","neural networks","predictive maintenance","iot","big data analytics","smart home","medical devices","implementation","systems","smart cities"],"tags":["design","big data","smart homes","internet of things (iot)","mission critical systems","reconfigurable plat-forms","massive data sets","machine learning","fpga","self-driving cars","system","situational awareness","data mining","neural networks","predictive maintenance","big data analytics","medical devices","implementation","optimization algorithms","smart cities"]},{"p_id":45703,"title":"Applying VorEAl for IoT intrusion detection","abstract":"\u00a9 Springer International Publishing AG, part of Springer Nature 2018. Smart connected devices create what has been denominated as the Internet of Things (IoT). The combined and cohesive use of these devices prompts the emergence of Ambient Intelligence (AmI). One of the current key issues in the IoT domain has to do with the detection and prevention of security breaches and intrusions. In this paper, we introduce the use of the Voronoi diagram-based Evolutionary Algorithm (VorEAl) in the context of IoT intrusion detection. In order to cope with the dimensions of the problem, we propose a modification of VorEAl that employs a proxy for the volume that approximates it using a heuristic surrogate. The proxy has linear complexity and, therefore, highly scalable. The experimental studies carried out as part of the paper show that our approach is able to outperform other approaches that have been previously used to address the problem of interest.","keywords_author":["IDS","IoT","Machine learning","Predictive analysis","Time series"],"keywords_other":null,"max_cite":0.0,"pub_year":2018.0,"sources":"['scp']","rawkeys":["ids","predictive analysis","machine learning","iot","time series"],"tags":["predictive analysis","intrusion detection systems","machine learning","internet of things (iot)","time series"]},{"p_id":648,"title":"The Internet technology for defect detection system with deep learning method in smart factory","abstract":"The most obvious difference between the recent smart factory and the traditional automation factory is that the techniques about internet of thing (IoT) are introduced. The smart factory that employs IoT techniques intelligently manages the automated manufacturing equipment and automated defect detection equipment to improve production efficiency and quality significantly. The equipments used in the smart factory are manufacturing equipments, functional testing equipment and defect detection equipment. Defect detection equipment, mainly in the entire product manufacturing and packaging and functional testing process, the establishment of checkpoints, phased detection of semi-finished products to identify defective and defective products selected. As a result, the defective product no longer goes through all of the following processes, thus reducing costs and improving the yield of the final product shipped. In the development of classification algorithm, a huge breakthrough has been made in the deep learning algorithm in recent years. Therefore, in recent years, many studies have tried to apply deep learning algorithm to various fields. However, most of the current studies focus on the performance of the deep learning algorithm in their applications. Fewer studies research the environmental design for the system employing deep learning algorithm. For example, the network architecture for the system employing deep learning algorithm is less discussed by studies. Therefore, this study presents a architecture of smart factory which employing deep learning algorithm in defect detection system. Thereafter, this study presents the network architecture for the proposed smart factory. Finally, the internet technology for defect detection system with deep learning method in smart factory is presented.","keywords_author":["smart factor","internet of thing","defect detection","deep learning algorithm"],"keywords_other":["deep learning algorithm","automated defect detection equipment","Classification algorithms","Internet","learning (artificial intelligence)","Smart manufacturing","defective product","production engineering computing","Logic gates","smart factory","Machine learning","factory automation","Machine learning algorithms","automated manufacturing equipment","Internet of Thing","Internet of Things","Internet technology","fault diagnosis"],"max_cite":null,"pub_year":2018.0,"sources":"['ieee']","rawkeys":["production engineering computing","machine learning algorithms","internet of things","automated manufacturing equipment","logic gates","internet","automated defect detection equipment","internet of thing","defective product","machine learning","classification algorithms","smart factory","factory automation","deep learning algorithm","defect detection","learning (artificial intelligence)","smart factor","internet technology","smart manufacturing","fault diagnosis"],"tags":["deep learning algorithm","automated defect detection equipment","defect detection","production engineering computing","machine learning algorithms","smart factor","machine learning","smart factory","factory automation","smart manufacturing","internet","automated manufacturing equipment","logic gates","internet technology","defective products","classification algorithm","fault diagnosis","internet of things (iot)"]},{"p_id":667,"title":"Indoor Augmented Reality Using Deep Learning for Industry 4.0 Smart Factories","abstract":"This paper proposes to design, develop and implement a fast and markerless mobile augmented reality system to achieve the registration for, the visualization of, and the interaction with machines in indoor smart factories with Industry 4.0 vision. A lightweight deep-learning image detection module based on MobileNets running on mobile devices is used to detect\/recognize different machines and different portions of machines. Internet of Things (IoT) networking allows machines and sensors in machines to report data, such as machine settings and machine states, to the cloud-side server. Thus, augmented information associated with a machine portion can be derived from the server and superimposed with the portion image shown on the device display. Furthermore, interaction methods based on touch gestures and distance calculation are also implemented. A prototype system is developed and tested in a mechanical workshop for the purpose of validation and evaluation. The system is shown to achieve high detection accuracy, intuitive visualization, and unique interaction modes.","keywords_author":["augmented reality, deep learning, indoor positioning, Industry 4.0, Internet of Things, smart factory"],"keywords_other":["Image recognition","Industries","Prototypes","Training","Machine learning","Servers","Software"],"max_cite":null,"pub_year":2018.0,"sources":"['ieee']","rawkeys":["indoor positioning","deep learning","industries","training","internet of things","machine learning","servers","smart factory","prototypes","augmented reality","image recognition","software","industry 4.0"],"tags":["industry","indoor positioning","training","machine learning","smart factory","servers","prototypes","augmented reality","image recognition","software","industry 4.0","internet of things (iot)"]},{"p_id":47778,"title":"Pedestrian detection system for smart communities using deep Convolutional Neural Networks","abstract":"\u00a9 2017 IEEE. Pedestrian recognition is a key problem for a number of application domains namely autonomous driving, search and rescue, surveillance and robotics. Real-time pedestrian recognition entails determining if a pedestrian is in an image frame. State-of-art pedestrian detection convolution neural networks(CNN) such as Fast R-CNN depend on computationally expensive region detection algorithms to hypothesize pedestrian locations. This paper presents a simple, fast and very accurate approach by cascading fast regional detection and deep convolution networks. Convolution networks have been shown to excel at image classification. However, convolution networks are notoriously slow at inference time. In this work, we introduce a fast regional detection cascaded with deep convolution networks that enables real-time pedestrian detection that could be used to alert a driver if a pedestrian is on the roadway. The classification CNN has given an accuracy of 95.7%, with a processing rate of about 15 frames per second on a low performance system without a graphical processing unit (GPU).","keywords_author":["Convolutional Neural Networks","Deep Learning","Internet of Things","Sliding Window","Smart City","Smart Community"],"keywords_other":["Pedestrian recognition","Graphical processing units","Smart community","Pedestrian detection","Sliding Window","Convolutional neural network","Convolution neural network","Pedestrian detection system"],"max_cite":0.0,"pub_year":2017.0,"sources":"['scp']","rawkeys":["pedestrian detection","convolutional neural networks","pedestrian recognition","convolution neural network","deep learning","smart city","internet of things","smart community","convolutional neural network","pedestrian detection system","sliding window","graphical processing units"],"tags":["pedestrian detection","graphics processing units","pedestrian recognition","machine learning","smart community","convolutional neural network","pedestrian detection system","sliding window","internet of things (iot)","smart cities"]},{"p_id":47779,"title":"Voice biometrics: Deep learning-based voiceprint authentication system","abstract":"\u00a9 2017 IEEE.Speaker identification systems are becoming more important in today's world. This is especially true as devices rely on the user to speak commands. In this article, an analysis of how a text-independent voice identification system can be built is presented. Extracting the Mel-Frequency Cepstral Coefficients is evaluated and a support vector machine is trained and tested on two different data sets, one from LibriSpeech and one from in-house recorded audio files. The results show the ability for such systems to be utilized in both speaker identification and speaker verification tasks.","keywords_author":["Authentication System","Biometrics","Deep Learning","Internet of Things","MFCC","Speaker Recognition","SVM","Voiceprint"],"keywords_other":["Speaker identification systems","Speaker identification","Voiceprint","MFCC","Voice identification","Authentication systems","Mel frequency cepstral co-efficient","Speaker recognition"],"max_cite":0.0,"pub_year":2017.0,"sources":"['scp']","rawkeys":["mfcc","voice identification","deep learning","mel frequency cepstral co-efficient","authentication system","internet of things","authentication systems","speaker identification","svm","speaker recognition","voiceprint","biometrics","speaker identification systems"],"tags":["voice identification","authentication systems","machine learning","speaker identification","speaker recognition","mel-frequency cepstral coefficients","voiceprint","biometrics","speaker identification systems","internet of things (iot)"]},{"p_id":47780,"title":"Classification of various daily behaviors using deep learning and smart watch","abstract":"\u00a9 2017 IEEE.In traditional healthcare and therapy, human behavior has been classified into only two categories: specific behavior and active behavior. As internet of things and wearable devices become popular, however, it is necessary to classify human behavior into more various categories for providing useful services. In this paper, we propose a novel classification scheme that classifies human behavior into 11 different categories including active and inactive activities in daily life. We collect data with smart watch and use deep learning model with a neural network for the classification. Extensive evaluation shows that various daily human behavior can be classified with 99.24% accuracy, and that the classification of human behavior can be used for various services.","keywords_author":["Classification","Deep learning","Human behavior","Internet of things","Smart watch"],"keywords_other":["Wearable devices","Daily behaviors","Learning models","Classification scheme","Human behaviors","Active behavior","Daily lives"],"max_cite":0.0,"pub_year":2017.0,"sources":"['scp']","rawkeys":["active behavior","human behaviors","learning models","wearable devices","human behavior","deep learning","daily behaviors","internet of things","smart watch","classification","classification scheme","daily lives"],"tags":["human behaviors","learning models","wearable devices","smartwatch","daily behaviors","machine learning","classification","activity behavior","classification scheme","internet of things (iot)","daily lives"]},{"p_id":4776,"title":"Internet of Things (IoT): A vision, architectural elements, and future directions","abstract":"Ubiquitous sensing enabled by Wireless Sensor Network (WSN) technologies cuts across many areas of modern day living. This offers the ability to measure, infer and understand environmental indicators, from delicate ecologies and natural resources to urban environments. The proliferation of these devices in a communicating-actuating network creates the Internet of Things (IoT), wherein sensors and actuators blend seamlessly with the environment around us, and the information is shared across platforms in order to develop a common operating picture (COP). Fueled by the recent adaptation of a variety of enabling wireless technologies such as RFID tags and embedded sensor and actuator nodes, the IoT has stepped out of its infancy and is the next revolutionary technology in transforming the Internet into a fully integrated Future Internet. As we move from www (static pages web) to web2 (social networking web) to web3 (ubiquitous computing web), the need for data-on-demand using sophisticated intuitive queries increases significantly. This paper presents a Cloud centric vision for worldwide implementation of Internet of Things. The key enabling technologies and application domains that are likely to drive IoT research in the near future are discussed. A Cloud implementation using Aneka, which is based on interaction of private and public Clouds is presented. We conclude our IoT vision by expanding on the need for convergence of WSN, the Internet and distributed computing directed at technological research community. \u00a9 2013 Elsevier B.V. All rights reserved.","keywords_author":["Cloud computing","Internet of Things","RFID Smart environments","Ubiquitous sensing","Wireless sensor networks"],"keywords_other":["Environmental indicators","Internet of thing (IOT)","Technological researches","Revolutionary technology","Smart environment","Ubiquitous sensing","Internet of Things (IOT)","Common operating pictures"],"max_cite":2518.0,"pub_year":2013.0,"sources":"['ieee', 'scp', 'wos']","rawkeys":["rfid smart environments","cloud computing","internet of thing (iot)","internet of things","wireless sensor networks","ubiquitous sensing","smart environment","technological researches","common operating pictures","environmental indicators","internet of things (iot)","revolutionary technology"],"tags":["rfid smart environments","cloud computing","wireless sensor networks","ubiquitous sensing","smart environment","technological researches","common operating pictures","environmental indicators","internet of things (iot)","revolutionary technology"]},{"p_id":17073,"title":"Agricultural production system based on IoT","abstract":"There has been much research and various attempts to apply new IoT technology to agricultural areas. However, IoT for the agriculture should be considered differently against the same areas such as industrial, logistics. This paper presents the IoT-based agricultural production system for stabilizing supply and demand of agricultural products while developing the environment sensors and prediction system for the growth and production amount of crops by gathering its environmental information. Currently, the demand by consumption of agricultural products could be predicted quantitatively, however, the variation of harvest and production by the change of farm's cultivated area, weather change, disease and insect damage etc. could not be predicted, so that the supply and demand of agricultural products has not been controlled properly. To overcome it, this paper designed the IoT-based monitoring system to analyze crop environment, and the method to improve the efficiency of decision making by analyzing harvest statistics. Therefore, this paper developed the decision support system to forecast agricultural production using IoT sensors. This system was also a unified system that supports the processes sowing seeds through selling agricultural products to consumers. 3 Corresponding author The IoT-based agricultural production system through correlation analysis between the crop statistical information and agricultural environment information has enhanced the ability of farmers, researchers, and government officials to analyze current conditions and predict future harvest. Additionally, agricultural products quality can be improved because farmers observe whole cycle from seeding to selling using this IoT-based decision support system. \u00a9 2013 IEEE.","keywords_author":["Agriculture","Decision support","IoT","Monitoring","Statistics"],"keywords_other":["Statistical information","IoT","Environmental information","Agricultural production system","Decision supports","Agricultural productions","Government officials","Agricultural environments"],"max_cite":32.0,"pub_year":2013.0,"sources":"['scp']","rawkeys":["agricultural production system","statistics","statistical information","agricultural productions","environmental information","agriculture","decision support","government officials","iot","monitoring","decision supports","agricultural environments"],"tags":["agricultural production system","statistics","statistical information","agricultural productions","environmental information","agriculture","internet of things (iot)","government officials","monitoring","decision supports","agricultural environments"]},{"p_id":693,"title":"DeepCAS: A Deep Reinforcement Learning Algorithm for Control-Aware Scheduling","abstract":"We consider networked control systems consisting of multiple independent controlled subsystems, operating over a shared communication network. Such systems are ubiquitous in cyber-physical systems, Internet of Things, and large-scale industrial systems. In many large-scale settings, the size of the communication network is smaller than the size of the system. In consequence, scheduling issues arise. The main contribution of this letter is to develop a deep reinforcement learning-based control-aware scheduling (DEEPCAS) algorithm to tackle these issues. We use the following (optimal) design strategy: first, we synthesize an optimal controller for each subsystem; next, we design a learning algorithm that adapts to the chosen subsystems (plants) and controllers. As a consequence of this adaptation, our algorithm finds a schedule that minimizes the control loss. We present empirical results to show that DEEPCAS finds schedules with better performance than periodic ones.","keywords_author":["Deep learning","reinforcement learning","optimal control","networked control systems","scheduling","communication"],"keywords_other":["Schedules","scheduling","Networked control systems","shared communication network","Kalman filters","learning (artificial intelligence)","deep reinforcement learning-based control-aware scheduling algorithm","optimal control","multiple independent controlled subsystems","control engineering computing","optimal controller","cyber-physical systems","Communication networks","Internet of things","large-scale industrial systems","DEEPCAS","Intelligent sensors","Optimal scheduling","networked control systems"],"max_cite":null,"pub_year":2018.0,"sources":"['ieee']","rawkeys":["kalman filters","deepcas","internet of things","reinforcement learning","scheduling","schedules","shared communication network","learning (artificial intelligence)","deep learning","communication","deep reinforcement learning-based control-aware scheduling algorithm","optimal control","multiple independent controlled subsystems","control engineering computing","optimal controller","cyber-physical systems","intelligent sensors","large-scale industrial systems","optimal scheduling","communication networks","networked control systems"],"tags":["cyber-physical systems","intelligent sensors","deepcas","networked control-systems","scheduling","large-scale industrial systems","machine learning","optimal scheduling","reinforcement learning","kalman filter","communication","shared communication network","deep reinforcement learning-based control-aware scheduling algorithm","optimal control","communication networks","multiple independent controlled subsystems","control engineering computing","internet of things (iot)"]},{"p_id":8893,"title":"Security and privacy challenges in industrial Internet of Things","abstract":"\u00a9 2015 ACM. Today, embedded, mobile, and cyberphysical systems are ubiquitous and used in many applications, from industrial control systems, modern vehicles, to critical infrastructure. Current trends and initiatives, such as 'Industrie 4.0' and Internet of Things (IoT), promise innovative business models and novel user experiences through strong connectivity and effective use of next generation of embedded devices. These systems generate, process, and exchange vast amounts of security-critical and privacy-sensitive data, which makes them attractive targets of attacks. Cyberattacks on IoT systems are very critical since they may cause physical damage and even threaten human lives. The complexity of these systems and the potential impact of cyberattacks bring upon new threats. This paper gives an introduction to Industrial IoT systems, the related security and privacy challenges, and an outlook on possible solutions towards a holistic security framework for Industrial IoT systems.","keywords_author":null,"keywords_other":["Strong connectivity","Cyber physical systems (CPSs)","Security-critical","Security and privacy","Potential impacts","Industrial control systems","Security frameworks","Internet of Things (IOT)"],"max_cite":86.0,"pub_year":2015.0,"sources":"['ieee', 'scp', 'wos']","rawkeys":["industrial control systems","security-critical","potential impacts","strong connectivity","cyber physical systems (cpss)","security frameworks","security and privacy","internet of things (iot)"],"tags":["cyber-physical systems","industrial control systems","security-critical","potential impacts","strongly connected","security frameworks","security and privacy","internet of things (iot)"]},{"p_id":45759,"title":"A multiaxial data-based machine learning model for exercise motion recognition","abstract":"\u00a9 2018 SERSC Australia. Wearable devices using only accelerometers cannot correctly recognize the detailed motions of exercise performed by patients and cannot count the correct number of repetitions. Therefore, in this paper, we suggest a method to improve the recognition accuracy of detailed motions using machine learning technology simultaneously using an accelerometer, and magnetometer in a wearable device. We particularly improved the recognition accuracy of detailed exercise motions through machine learning using data measured on 9 axes. To verify its effectiveness, we conducted experiments for the recognition of each motion when the patients performed exercise consisting of nine motions with the 9-axis accelerometer attached to their wrists. Accuracy of 81.8% was recorded when only acceleration data was used, whereas accuracy of 91% was recorded in the 9- axis data obtained using three sensors, thereby improving accuracy by 9.2%.","keywords_author":["Exercise","IoT","Machine learning","Motion sensor","Recognition"],"keywords_other":null,"max_cite":0.0,"pub_year":2018.0,"sources":"['scp']","rawkeys":["recognition","machine learning","motion sensor","iot","exercise"],"tags":["recognition","motion sensors","machine learning","exercise","internet of things (iot)"]},{"p_id":47822,"title":"Advances in big data analytics at the Dow Chemical Company","abstract":"\u00a9 2017 IEEE. Big data analytics is the journey to turn data into insights for more informed business and operational decisions. As the Chemical Engineering community is collecting more data (volume) from different sources (variety), this journey becomes more challenging in terms of using the right data and the right tools (analytics) to make the right decisions in real-time (velocity). This paper highlights recent advancements in the big data analytics journey at The Dow Chemical Company in the areas of Enterprise Manufacturing Intelligence, multivariate analysis, on-line fault detection, inferential sensors, and batch data analytics.","keywords_author":["Big data analytics","Data-driven modeling","Inferential sensors","Internet of things","Machine learning"],"keywords_other":["On-line fault detection","Multi variate analysis","Inferential sensors","Engineering community","Enterprise manufacturing intelligences","Data analytics","Data-driven model","Dow Chemical companies"],"max_cite":0.0,"pub_year":2017.0,"sources":"['scp']","rawkeys":["on-line fault detection","inferential sensors","multi variate analysis","data-driven model","data analytics","data-driven modeling","internet of things","machine learning","enterprise manufacturing intelligences","dow chemical companies","engineering community","big data analytics"],"tags":["on-line fault detection","inferential sensors","multi variate analysis","data-driven model","data analytics","machine learning","enterprise manufacturing intelligences","dow chemical companies","engineering community","big data analytics","internet of things (iot)"]},{"p_id":58064,"title":"Detection of Mass Panic using Internet of Things and Machine Learning","abstract":"The increase of emergency situations that cause mass panic in mass gatherings, such as terrorist attacks, random shooting, stampede, and fires, sheds light on the fact that advancements in technology should contribute in timely detecting and reporting serious crowd abnormal behaviour. The new paradigm of the 'Internet of Things' (IoT) can contribute to that. In this study, a method for real-time detection of abnormal crowd behaviour in mass gatherings is proposed. This system is based on advanced wireless connections, wearable sensors and machine learning technologies. It is a new crowdsourcing approach that considers humans themselves as the surveillance devices that exist everywhere. A sufficient number of the event's attendees are supposed to wear an electronic wristband which contains a heart rate sensor, motion sensors and an assisted-GPS, and has a wireless connection. It detects the abnormal behaviour by detecting heart rate increase and abnormal motion. Due to the unavailability of public bio-dataset on mass panic, dataset of this study was collected from 89 subjects wearing the above-mentioned wristband and generating 1054 data samples. Two types of data collected were: firstly, the data of normal daily activities and secondly, the data of abnormal activities resembling the behaviour of escape panic. Moreover, another abnormal dataset was synthetically generated to simulate panic with limited motion. In our proposed approach, two-phases of data analysis are done. Phase-I is a deep machine learning model that was used to analyze the sensors' collected readings of the wristband and detect if the person has indeed panicked in order to send alerting signals. While phase-II data analysis takes place in the monitoring server that receives the alerting signals to conclude if it is a mass panic incident or a false positive case. Our experiments demonstrate that the proposed system can offer a reliable, accurate, and fast solution for panic detection. This experiment uses the Hajj pilgrimage as a case study.","keywords_author":["Internet of Things","IoT","Mobile Crowd Sensing (MCS)","wearables","mass panic","mass gatherings","accelerometer","Optical Heart Rate (HR) sensor","abnormal crowd behaviour","deep learning","Recurrent Neural Network (RNN)","Long Short Term Memory (LSTM)","Gated Recurrent Unit (GRU)","time series"],"keywords_other":["EMERGENCY","DISASTER","RESPONSES","FREEZE","THREAT","SENSORS"],"max_cite":0.0,"pub_year":2018.0,"sources":"['wos']","rawkeys":["long short term memory (lstm)","optical heart rate (hr) sensor","sensors","internet of things","emergency","freeze","mobile crowd sensing (mcs)","mass panic","mass gatherings","disaster","deep learning","wearables","recurrent neural network (rnn)","iot","abnormal crowd behaviour","accelerometer","time series","gated recurrent unit (gru)","responses","threat"],"tags":["optical heart rate (hr) sensor","freezing","sensors","internet of things (iot)","gated recurrent unit","mass panic","machine learning","multiple classifier systems","disaster","neural networks","long short-term memory","wearables","abnormal crowd behaviour","accelerometer","time series","responses","threat","emergence","mass gathering"]},{"p_id":25301,"title":"Big data analytics in chemical engineering","abstract":"Copyright \u00a9 2017 by Annual Reviews. All rights reserved. Big data analytics is the journey to turn data into insights for more informed business and operational decisions. As the chemical engineering community is collecting more data (volume) from different sources (variety), this journey becomes more challenging in terms of using the right data and the right tools (analytics) to make the right decisions in real time (velocity). This article highlights recent big data advancements in five industries, including chemicals, energy, semiconductors, pharmaceuticals, and food, and then discusses technical, platform, and culture challenges. To reach the next milestone in multiplying successes to the enterprise level, government, academia, and industry need to collaboratively focus on workforce development and innovation.","keywords_author":["Big data analytics","Data-driven modeling","Industry 4.0","Internet of things","Machine learning"],"keywords_other":["Workforce development","Electronics","Humans","Operational decisions","Drug Discovery","Drug Industry","Semiconductors","Machine Learning","Chemical Engineering","Food Industry","Industry needs","Engineering community","Data-driven model","Data Mining","Real time","Data analytics"],"max_cite":8.0,"pub_year":2017.0,"sources":"['scp']","rawkeys":["data analytics","internet of things","chemical engineering","workforce development","food industry","industry 4.0","semiconductors","operational decisions","machine learning","industry needs","data mining","data-driven model","drug industry","drug discovery","humans","big data analytics","electronics","real time","data-driven modeling","engineering community"],"tags":["data analytics","chemical engineering","workforce development","industry 4.0","internet of things (iot)","semiconductors","food industries","operational decisions","machine learning","industry needs","data mining","data-driven model","drug industry","drug discovery","humans","big data analytics","electronics","real time","engineering community"]},{"p_id":41687,"title":"A method of image identification in instrumentation","abstract":"\u00a9 KIPS. Smart city is currently the main direction of development. The automatic management of instrumentation is one task of the smart city. Because there are a lot of old instrumentation in the city that cannot be replaced promptly, how to makes low-cost transformation with Internet of Thing (IoT) becomes a problem. This article gives a low-cost method that can identify code wheel instrument information. This method can effectively identify the information of image as the digital information. Because this method does not require a lot of memory or complicated calculation, it can be deployed on a cheap microcontroller unit (MCU) with low readonly memory (ROM). At the end of this article, test result is given. Using this method to modify the old instrumentation can achieve the automatic management of instrumentation and can help build a smart city.","keywords_author":["Code wheel instrument","Image identification","IoT","Smart city","Template matching"],"keywords_other":["Microcontroller unit","Low costs","Image identification","Method of images","Low cost methods","Digital information","Internet of Things (IOT)","Automatic management"],"max_cite":1.0,"pub_year":2018.0,"sources":"['scp']","rawkeys":["digital information","automatic management","method of images","microcontroller unit","low costs","low cost methods","code wheel instrument","image identification","template matching","iot","internet of things (iot)","smart city"],"tags":["digital information","automatic management","method of images","microcontroller unit","low costs","low cost methods","code wheel instrument","image identification","template matching","internet of things (iot)","smart cities"]},{"p_id":33496,"title":"Cloud, fog and edge: Cooperation for the future?","abstract":"\u00a9 2017 IEEE.Traditional cloud-based infrastructures are not enough for the current demands of Internet of Things (IoT) applications. Two major issues are the limitations in terms of latency and network bandwidth. In recent years, the concepts of fog computing and edge computing were proposed to alleviate these limitations by moving data processing capabilities closer to the network edge. Considering IoT growth and development forecasts, we believe the full potential of IoT can, in many cases, only be unlocked by combining cloud, fog and edge computing. This paper discusses four possible approaches for distributing workload among these levels. We also highlight developments and possibilities as well as consider challenges for implementation in the areas of hardware, machine learning, security, privacy and communication.","keywords_author":["Cloud Computing","Edge Computing","Fog Computing","Hardware","Internet of Things","Machine Learning","Network Security"],"keywords_other":["Network edges","Processing capability","Network bandwidth","Edge computing","Current demands","Growth and development","Cloud-based","Internet of Things (IOT)"],"max_cite":2.0,"pub_year":2017.0,"sources":"['scp']","rawkeys":["cloud computing","growth and development","fog computing","processing capability","cloud-based","internet of things","machine learning","network bandwidth","network edges","edge computing","hardware","network security","internet of things (iot)","current demands"],"tags":["cloud computing","growth and development","fog computing","processing capability","cloud-based","machine learning","network bandwidth","network edges","edge computing","hardware","network security","internet of things (iot)","current demands"]},{"p_id":43740,"title":"An internet of things system for underground mine air quality pollutant prediction based on azure machine learning","abstract":"\u00a9 2018 by the authors. Licensee MDPI, Basel, Switzerland. The implementation of wireless sensor networks (WSNs) for monitoring the complex, dynamic, and harsh environment of underground coal mines (UCMs) is sought around the world to enhance safety. However, previously developed smart systems are limited to monitoring or, in a few cases, can report events. Therefore, this study introduces a reliable, efficient, and cost-effective internet of things (IoT) system for air quality monitoring with newly added features of assessment and pollutant prediction. This system is comprised of sensor modules, communication protocols, and a base station, running Azure Machine Learning (AML) Studio over it. Arduino-based sensor modules with eight different parameters were installed at separate locations of an operational UCM. Based on the sensed data, the proposed system assesses mine air quality in terms of the mine environment index (MEI). Principal component analysis (PCA) identified CH4, CO, SO2, and H2S as the most influencing gases significantly affecting mine air quality. The results of PCA were fed into the ANN model in AML studio, which enabled the prediction of MEI. An optimum number of neurons were determined for both actual input and PCA-based input parameters. The results showed a better performance of the PCA-based ANN for MEI prediction, with R2 and RMSE values of 0.6654 and 0.2104, respectively. Therefore, the proposed Arduino and AML-based system enhances mine environmental safety by quickly assessing and predicting mine air quality.","keywords_author":["Artificial neural network","Azure machine learning","Internet-of-things","Mine environment index","Underground coal mines"],"keywords_other":["Mine environment","Underground coal mine","Underground mine","Air quality monitoring","Wireless sensor network (WSNs)","Internet of Things (IOT)","Harsh environment","Environmental safety"],"max_cite":0.0,"pub_year":2018.0,"sources":"['scp']","rawkeys":["azure machine learning","underground coal mines","environmental safety","mine environment","mine environment index","underground mine","internet-of-things","wireless sensor network (wsns)","underground coal mine","harsh environment","air quality monitoring","artificial neural network","internet of things (iot)"],"tags":["azure machine learning","underground coal mines","neural networks","environmental safety","mine environment","mine environment index","underground mine","harsh environment","air quality monitoring","wireless sensor networks","internet of things (iot)"]},{"p_id":37600,"title":"Namatad: Inferring occupancy from building sensors using machine learning","abstract":"\u00a9 2016 IEEE. Driven by the need to improve efficiency, modern buildings are instrumented with numerous sensors to monitor utilization and regulate environmental conditions. While these sensor systems serve as valuable tools for managing the comfort and health of occupants, there is an increasing need to expand the deployment of sensors to provide additional insights. Because many of these desired insights have high temporal value, such as occupancy during emergency situations, such insights are needed in real time. However, augmenting buildings with new sensors is often expensive and requires a significant capital investment. In this paper, we propose and describe the real-time, streaming system called Namatad that we developed to infer insights from many sensors typical of Internet of Things (IoT) deployments. We evaluate the effectiveness of this platform by leveraging machine learning to infer new insights from environmental sensors within buildings. We describe how we built the components of our system leveraging several open source, streaming frameworks. We also describe how we ingest and aggregate from building sensors and sensing platforms, route data streams to appropriate models, and make predictions using machine learning techniques. Using our system, we have been able to predict the occupancy of rooms within a building on the University of Washington campus over the last three months, in real time, at accuracies of up to 95%.","keywords_author":["Building","IoT","Machine Learning","Sensors","Server"],"keywords_other":["Capital investment","University of Washington","Environmental conditions","Environmental sensor","Emergency situation","Appropriate models","Internet of Things (IOT)","Machine learning techniques"],"max_cite":1.0,"pub_year":2017.0,"sources":"['scp']","rawkeys":["capital investment","environmental sensor","appropriate models","sensors","machine learning techniques","machine learning","emergency situation","university of washington","environmental conditions","server","building","iot","internet of things (iot)"],"tags":["capital investment","environmental sensor","appropriate models","sensors","machine learning techniques","machine learning","connectivity","emergency situation","servers","university of washington","environmental conditions","internet of things (iot)"]},{"p_id":43748,"title":"A dimension reduction model and classifier for anomaly-based intrusion detection in internet of things","abstract":"\u00a9 2017 IEEE. Internet of Things (IoT) devices and services have gained wide spread growth in many commercial and mission critical applications. The devices and services suffer from intrusions, attacks and malicious activities. To protect valuable data transmitted through IoT networks and users'privacy, intrusion detection systems (IDS) should be developed to match with the characteristics of IoT, which requires real-time monitoring. This paper proposes a novel model for intrusion detection which is based on dimensionreduction algorithm and a classifier, which can be used as an online machine learning algorithm. The proposed model uses Principal Component Analysis (PCA) to reduce dimensions of dataset from a large number of features to a small number. To develop a classifier, softmax regression and k-nearestneighbour algorithms are applied and compared. Experimental results using KDD Cup 99 Data Set show that our proposed model performs optimally in labelling benign behaviours and identifying malicious behaviours. Thecomputing complexity and time performance approve that the model can be used to detect intrusions in IoT.","keywords_author":["Classifier","Dimension reduction","Intrusion detection system","IoT","Online machine learning"],"keywords_other":["Mission critical applications","Malicious activities","Intrusion Detection Systems","Anomaly-based intrusion detection","Dimension reduction","Dimension reduction model","Internet of Things (IOT)","On-line machine learning"],"max_cite":0.0,"pub_year":2018.0,"sources":"['scp']","rawkeys":["mission critical applications","intrusion detection system","on-line machine learning","intrusion detection systems","classifier","dimension reduction model","malicious activities","iot","anomaly-based intrusion detection","online machine learning","dimension reduction","internet of things (iot)"],"tags":["mission critical applications","on-line machine learning","intrusion detection systems","classifier","dimension reduction model","malicious activities","dimensionality reduction","anomaly-based intrusion detection","online machine learning","internet of things (iot)"]},{"p_id":742,"title":"Deep Learning for IoT Big Data and Streaming Analytics: A Survey","abstract":"In the era of the Internet of Things (IoT), an enormous amount of sensing devices collect and\/or generate various sensory data over time for a wide range of fields and applications. Based on the nature of the application, these devices will result in big or fast\/real-time data streams. Applying analytics over such data streams to discover new information, predict future insights, and make control decisions is a crucial process that makes IoT a worthy paradigm for businesses and a quality-of-life improving technology. In this paper, we provide a thorough overview on using a class of advanced machine learning techniques, namely Deep Learning (DL), to facilitate the analytics and learning in the IoT domain. We start by articulating IoT data characteristics and identifying two major treatments for IoT data from a machine learning perspective, namely IoT big data analytics and IoT streaming data analytics. We also discuss why DL is a promising approach to achieve the desired analytics in these types of data and applications. The potential of using emerging DL techniques for IoT data analytics are then discussed, and its promises and challenges are introduced. We present a comprehensive background on different DL architectures and algorithms. We also analyze and summarize major reported research attempts that leveraged DL in the IoT domain. The smart IoT devices that have incorporated DL in their intelligence background are also discussed. DL implementation approaches on the fog and cloud centers in support of IoT applications are also surveyed. Finally, we shed light on some challenges and potential directions for future research. At the end of each section, we highlight the lessons learned based on our experiments and review of the recent literature.","keywords_author":["Cloud-based analytics.","Deep Learning","Deep Neural Network","Fast data analytics","Internet of Things","IoT Big Data","On-device Intelligence","Deep Learning","Deep Neural Network","Internet of Things","On-device Intelligence","IoT Big Data","Fast data analytics","Cloud-based analytics."],"keywords_other":["Internet of Things","Data mining","Tutorials","Data analysis","Internet of thing (IOT)","Machine learning techniques","Implementation approach","Big Data","Machine learning","Data analytics","Economics","Data characteristics","Cloud-based","Big Data Analytics","On-device Intelligence"],"max_cite":0.0,"pub_year":2018.0,"sources":"['scp', 'ieee']","rawkeys":["big data","data analytics","internet of thing (iot)","internet of things","machine learning","tutorials","iot big data","data mining","deep learning","fast data analytics","data characteristics","machine learning techniques","big data analytics","economics","deep neural network","cloud-based","on-device intelligence","implementation approach","data analysis","cloud-based analytics"],"tags":["data mining","economics","big data","tutorial","data analytics","data characteristics","cloud-based","machine learning","fast data analytics","iot big data","machine learning techniques","on-device intelligence","implementation approach","convolutional neural network","data analysis","big data analytics","internet of things (iot)","cloud-based analytics"]},{"p_id":27368,"title":"Fusing bluetooth beacon data with Wi-Fi radiomaps for improved indoor localization","abstract":"\u00a9 2017 by the authors. Indoor user localization and tracking are instrumental to a broad range of services and applications in the Internet of Things (IoT) and particularly in Body Sensor Networks (BSN) and Ambient Assisted Living (AAL) scenarios. Due to the widespread availability of IEEE 802.11, many localization platforms have been proposed, based on the Wi-Fi Received Signal Strength (RSS) indicator, using algorithms such as K-Nearest Neighbour (KNN), Maximum A Posteriori (MAP) and Minimum Mean Square Error (MMSE). In this paper, we introduce a hybrid method that combines the simplicity (and low cost) of Bluetooth Low Energy (BLE) and the popular 802.11 infrastructure, to improve the accuracy of indoor localization platforms. Building on KNN, we propose a new positioning algorithm (dubbed i-KNN) which is able to filter the initial fingerprint dataset (i.e., the radiomap), after considering the proximity of RSS fingerprints with respect to the BLE devices. In this way, i-KNN provides an optimised small subset of possible user locations, based on which it finally estimates the user position. The proposed methodology achieves fast positioning estimation due to the utilization of a fragment of the initial fingerprint dataset, while at the same time improves positioning accuracy by minimizing any calculation errors.","keywords_author":["Bluetooth low energy (BLE)","Body Sensor Networks (BSN)","Fingerprint","Indoor localization","Indoor positioning","Internet of Things (IoT)","Positioning algorithms"],"keywords_other":["Positioning algorithms","Bluetooth low energies (BLE)","Indoor positioning","Fingerprint","Indoor localization","Body sensor networks (BSN)","Internet of Things (IOT)"],"max_cite":6.0,"pub_year":2017.0,"sources":"['scp', 'wos']","rawkeys":["body sensor networks (bsn)","bluetooth low energy (ble)","indoor localization","indoor positioning","internet of things (iot)","fingerprint","positioning algorithms","bluetooth low energies (ble)"],"tags":["indoor positioning","fingerprint","internet of things (iot)","indoor localization","positioning algorithms","bluetooth low energies (ble)","body sensor networks"]},{"p_id":746,"title":"Deep Learning for Smart Industry: Efficient Manufacture Inspection System with Fog Computing","abstract":"With the rapid development of Internet of Things (IoT) devices and network infrastructure, there have been a lot of sensors adopted in the industrial productions, resulting in a large size of data. One of the most popular examples is the manufacture inspection, which is to detect the defects of the products. In the paper, in order to implement a robust inspection system with higher accuracy, we propose a deep learning based classification model, in order to find the possible defective products. As there may be many assembly lines in one factory, one huge problem in this scenario is how to process such big data in real time. Therefore, we design our system with the concept of fog computing. By offloading the computation burden from the central server to the fog nodes, the system obtains the ability to deal with extremely large data. There are two obvious advantages in our system. The first one is that we adapt the convolutional neural network (CNN) model into the fog computing environment, which significantly improves its computing efficiency. The other one is we work out an inspection model which can simultaneously indicate the defect type and its degree. The experimental results demonstrate the robustness and efficiency of the proposed method.","keywords_author":["Computational modeling","deep learning","Edge computing","Fog computing","Industries","Informatics","Inspection","Machine learning","manufacture inspection","Production","smart industry","Fog computing","manufacture inspection","smart industry","deep learning"],"keywords_other":["Industries","Computing environments","Inspection","Computational model","Classification models","Edge computing","Convolutional Neural Networks (CNN)","Machine learning","Industrial production","Informatics","Production","Internet of Things (IOT)","Network infrastructure","Computational modeling"],"max_cite":0.0,"pub_year":2018.0,"sources":"['scp', 'ieee']","rawkeys":["computational modeling","informatics","deep learning","fog computing","industries","smart industry","machine learning","inspection","manufacture inspection","computing environments","internet of things (iot)","edge computing","network infrastructure","production","computational model","classification models","industrial production","convolutional neural networks (cnn)"],"tags":["computational modeling","informatics","industry","productivity","fog computing","smart industry","machine learning","inspection","manufacture inspection","computing environments","internet of things (iot)","convolutional neural network","edge computing","network infrastructure","classification models","industrial production"]},{"p_id":41707,"title":"A Survey on Big Data Market: Pricing, Trading and Protection","abstract":"\u00a9 2013 IEEE. Big data is considered to be the key to unlocking the next great waves of growth in productivity. The amount of collected data in our world has been exploding due to a number of new applications and technologies that permeate our daily lives, including mobile and social networking applications, and Internet of Thing-based smart-world systems (smart grid, smart transportation, smart cities, and so on). With the exponential growth of data, how to efficiently utilize the data becomes a critical issue. This calls for the development of a big data market that enables efficient data trading. Via pushing data as a kind of commodity into a digital market, the data owners and consumers are able to connect with each other, sharing and further increasing the utility of data. Nonetheless, to enable such an effective market for data trading, several challenges need to be addressed, such as determining proper pricing for the data to be sold or purchased, designing a trading platform and schemes to enable the maximization of social welfare of trading participants with efficiency and privacy preservation, and protecting the traded data from being resold to maintain the value of the data. In this paper, we conduct a comprehensive survey on the lifecycle of data and data trading. To be specific, we first study a variety of data pricing models, categorize them into different groups, and conduct a comprehensive comparison of the pros and cons of these models. Then, we focus on the design of data trading platforms and schemes, supporting efficient, secure, and privacy-preserving data trading. Finally, we review digital copyright protection mechanisms, including digital copyright identifier, digital rights management, digital encryption, watermarking, and others, and outline challenges in data protection in the data trading lifecycle.","keywords_author":["Big data","data pricing","data trading","data utilization","Internet of Things","privacy and digital copyright protection","Big data","data pricing","privacy and digital copyright protection","data trading","data utilization","Internet of Things"],"keywords_other":["THINGS","WATERMARKING","Data Trading","FULLY HOMOMORPHIC ENCRYPTION","NETWORKS","Big data applications","Data utilization","MODEL","Digital copyright protection","STACKELBERG GAME","AUCTION","PERFORMANCE EVALUATION","STRATEGY","Copyright protections","Data pricing","INTERNET"],"max_cite":1.0,"pub_year":2018.0,"sources":"['ieee', 'scp', 'wos']","rawkeys":["auction","big data","strategy","internet of things","internet","stackelberg game","things","copyright protections","data trading","big data applications","data utilization","watermarking","data pricing","networks","performance evaluation","privacy and digital copyright protection","model","fully homomorphic encryption","digital copyright protection"],"tags":["big data","stackelberg games","internet","internet of things (iot)","auctions","digital copyright protection","things","copyright protections","data trading","big data applications","watermarking","data pricing","strategies","networks","performance evaluation","privacy and digital copyright protection","model","fully homomorphic encryption","data utilization"]},{"p_id":33519,"title":"BONSEYES: Platform for open development of systems of artificial intelligence","abstract":"\u00a9 2017 Copyright held by the owner\/author(s). ACM ISBN 978-1-4503-4487-6\/17\/05. The Bonseyes EU H2020 collaborative project aims to develop a platform consisting of a Data Marketplace, a Deep Learning Toolbox, and Developer Reference Platforms for organizations wanting to adopt Artificial Intelligence. The project will be focused on using artificial intelligence in low power Internet of Things (IoT) devices (\"edge computing\"), embedded computing systems, and data center servers (\"cloud computing\"). It will bring about orders of magnitude improvements in efficiency, performance, reliability, security, and productivity in the design and programming of systems of artificial intelligence that incorporate Smart Cyber-Physical Systems (CPS). In addition, it will solve a causality problem for organizations who lack access to Data and Models. Its open software architecture will facilitate adoption of the whole concept on a wider scale. To evaluate the effectiveness, technical feasibility, and to quantify the real-world improvements in efficiency, security, performance, effort and cost of adding AI to products and services using the Bonseyes platform, four complementary demonstrators will be built. Bonseyes platform capabilities are aimed at being aligned with the European FI-PPP activities and take advantage of its flagship project FIWARE. This paper provides a description of the project motivation, goals and preliminary work.","keywords_author":["Data marketplace","Deep Learning","Internet of things","Smart Cyber-Physical Systems"],"keywords_other":["Open development","Embedded computing system","Cyber-physical systems (CPS)","Collaborative projects","Orders of magnitude","Data marketplaces","Products and services","Internet of Things (IOT)"],"max_cite":2.0,"pub_year":2017.0,"sources":"['scp']","rawkeys":["data marketplace","orders of magnitude","deep learning","smart cyber-physical systems","internet of things","open development","cyber-physical systems (cps)","products and services","internet of things (iot)","data marketplaces","embedded computing system","collaborative projects"],"tags":["cyber-physical systems","data marketplace","orders of magnitude","smart cyber-physical systems","machine learning","open development","products and services","internet of things (iot)","embedded computing system","collaborative projects"]},{"p_id":45809,"title":"Internet of Health Things: Toward intelligent vital signs monitoring in hospital wards","abstract":"\u00a9 2018 Elsevier B.V. Background: Large amounts of patient data are routinely manually collected in hospitals by using standalone medical devices, including vital signs. Such data is sometimes stored in spreadsheets, not forming part of patients\u2019 electronic health records, and is therefore difficult for caregivers to combine and analyze. One possible solution to overcome these limitations is the interconnection of medical devices via the Internet using a distributed platform, namely the Internet of Things. This approach allows data from different sources to be combined in order to better diagnose patient health status and identify possible anticipatory actions. Methods: This work introduces the concept of the Internet of Health Things (IoHT), focusing on surveying the different approaches that could be applied to gather and combine data on vital signs in hospitals. Common heuristic approaches are considered, such as weighted early warning scoring systems, and the possibility of employing intelligent algorithms is analyzed. Results: As a result, this article proposes possible directions for combining patient data in hospital wards to improve efficiency, allow the optimization of resources, and minimize patient health deterioration. Conclusion: It is concluded that a patient-centered approach is critical, and that the IoHT paradigm will continue to provide more optimal solutions for patient management in hospital wards.","keywords_author":["Early Warning Score","Health records","Internet of Things","Machine learning","Vital signs","Wireless sensor networks"],"keywords_other":["Early warning score","Anticipatory actions","Vital sign","Electronic health record","Health records","Intelligent Algorithms","Vital signs monitoring","Distributed platforms"],"max_cite":0.0,"pub_year":2018.0,"sources":"['scp']","rawkeys":["anticipatory actions","vital sign","early warning score","electronic health record","intelligent algorithms","vital signs","health records","internet of things","machine learning","vital signs monitoring","wireless sensor networks","distributed platforms"],"tags":["anticipatory actions","early warning score","intelligent algorithms","vital signs","health records","machine learning","electronic health records","vital sign monitoring","wireless sensor networks","distributed platforms","internet of things (iot)"]},{"p_id":35572,"title":"Internet of things (IoT) platform for structure health monitoring","abstract":"\u00a9 2017 Ahmed Abdelgawad and Kumar Yelamarthi. Increase in the demand for reliable structural health information led to the development of Structural Health Monitoring (SHM). Prediction of upcoming accidents and estimation of useful life span of a structure are facilitated through SHM. While data sensing is the core of any SHM, tracking the data anytime anywhere is a prevailing challenge. With the advancement in information technology, the concept of Internet of Things (IoT) has made it possible to integrate SHM with Internet to track data anytime anywhere. In this paper, a SHM platform embedded with IoT is proposed to detect the size and location of damage in structures. The proposed platform consists of a Wi-Fi module, a Raspberry Pi, an Analog to Digital Converter (ADC), a Digital to Analog Converter (DAC), a buffer, and piezoelectric (PZT) sensors. The piezoelectric sensors are mounted as a pair in the structure. Data collected from the piezoelectric sensors will be used to detect the size and location of damage using a proposed mathematicalmodel. Implemented on a Raspberry Pi, the proposed mathematical model will estimate the size and location of structural damage, if any, and upload the data to Internet. This data will be stored and can be checked remotely from any mobile device. The system has been validated using a real test bed in the lab.","keywords_author":null,"keywords_other":["Structural health","Analog to digital converters","Piezoelectric sensors","Structural health monitoring (SHM)","Structural damages","Structure health monitoring","Data sensing","Internet of Things (IOT)"],"max_cite":2.0,"pub_year":2017.0,"sources":"['scp']","rawkeys":["analog to digital converters","structural health monitoring (shm)","structural health","structure health monitoring","data sensing","piezoelectric sensors","structural damages","internet of things (iot)"],"tags":["analog to digital converters","structural health","data sensing","structural health monitoring","piezoelectric sensors","structural damages","internet of things (iot)"]},{"p_id":43764,"title":"Spatio-temporal multidimensional collective data analysis for providing comfortable living anytime and anywhere","abstract":"Copyright \u00a9 The Authors, 2018 This is an Open Access article, distributed under the terms of the Creative Commons Attribution licence (http:\/\/creativecommons.org\/licenses\/by\/4.0\/), which permits unrestricted re-use, distribution, and reproduction in any medium, provided the original work is properly cited. Machine learning is a promising technology for analyzing diverse types of big data. The Internet of Things era will feature the collection of real-world information linked to time and space (location) from all sorts of sensors. In this paper, we discuss spatio-temporal multidimensional collective data analysis to create innovative services from such spatio-temporal data and describe the core technologies for the analysis. We describe core technologies about smart data collection and spatio-temporal data analysis and prediction as well as a novel approach for real-time, proactive navigation in crowded environments such as event spaces and urban areas. Our challenge is to develop a real-time navigation system that enables movements of entire groups to be efficiently guided without causing congestion by making near-future predictions of people flow. We show the effectiveness of our navigation approach by computer simulation using artificial people-flow data.","keywords_author":["IoT","Machine learning","Proactive navigation","Smart cities","Spatio-temporal data analysis"],"keywords_other":null,"max_cite":0.0,"pub_year":2018.0,"sources":"['scp']","rawkeys":["proactive navigation","machine learning","spatio-temporal data analysis","iot","smart cities"],"tags":["proactive navigation","machine learning","spatio-temporal data analysis","internet of things (iot)","smart cities"]},{"p_id":45815,"title":"How machine learning could detect anomalies on thinger.io platform?","abstract":"\u00a9 2018, Springer International Publishing AG, part of Springer Nature. This research explores the capacity of Machine Learning techniques to detect anomalies and how incorporate this capacity to thinger.io platform. Thinger.io is a IoT opensource platform that allows to create an IoT environment using any hardware available on market. In this paper, several ML techniques are proposed to detect anomalies in the platform.","keywords_author":["IoT","Machine learning","Predictive analysis","Time series"],"keywords_other":["Open-source platforms","Machine learning techniques"],"max_cite":0.0,"pub_year":2018.0,"sources":"['scp']","rawkeys":["predictive analysis","machine learning techniques","machine learning","open-source platforms","iot","time series"],"tags":["predictive analysis","machine learning techniques","machine learning","internet of things (iot)","open source platforms","time series"]},{"p_id":8954,"title":"Fog Computing: Helping the Internet of Things Realize Its Potential","abstract":"\u00a9 2016 IEEE. The Internet of Things (IoT) could enable innovations that enhance the quality of life, but it generates unprecedented amounts of data that are difficult for traditional systems, the cloud, and even edge computing to handle. Fog computing is designed to overcome these limitations.","keywords_author":["cloud computing","Cloud Cover","fog computing","Internet of Things","IoT"],"keywords_other":["Internet of thing (IOT)","Edge computing","Quality of life","Cloud cover","Traditional systems"],"max_cite":84.0,"pub_year":2016.0,"sources":"['scp']","rawkeys":["cloud computing","fog computing","internet of thing (iot)","internet of things","traditional systems","cloud cover","edge computing","iot","quality of life"],"tags":["cloud computing","fog computing","cloud cover","edge computing","traditional systems","quality of life","internet of things (iot)"]},{"p_id":764,"title":"Poster abstract: DeepRT: A predictable deep learning inference framework for IoT devices","abstract":"Recently, deep learning is emerging as a state-of-the-art approach in delivering robust and highly accurate inference in many domains, including Internet-of-Things (IoT). Deep learning is already changing the way computers embedded in IoT devices to make intelligent decisions using sensor feeds in the real world. There have been significant efforts to develop light-weight and highly efficient deep learning inference mechanisms for resource-constrained mobile and IoT devices. Some approaches propose a hardware-based accelerator, and some approaches propose to reduce the amount of computation of deep learning models using various model compression techniques. Even though these efforts have demonstrated significant gains in performance and efficiency, they are not aware of the Quality-of-Service (QoS) requirements of various IoT applications, and, hence manifest unpredictable 'best-effort' performance in terms of inference latency, power consumption, resource usage, etc. In IoT devices with temporal constraints, such unpredictability might result in undesirable effects such as compromising safety. In this work, we present a novel deep learning inference runtime called, DeepRT. Unlike previous inference accelerators, DeepRT focuses on supporting predictable inference performance both temporally and spatially.","keywords_author":["Deep learning","Embedded systems","IoT","Machine learning","QoS","Quality of Service","Real time","deep learning","IoT","embedded systems","real time","QoS","Quality of Service","machine learning"],"keywords_other":["predictable inference performance","data compression","inference mechanisms","robust inference","Intelligent decisions","quality of service","QoS requirements","highly accurate inference","Computational modeling","Internet","Qualityof-service requirement (QoS)","Machine learning","Undesirable effects","DeepRT","learning (artificial intelligence)","Temporal constraints","deep learning inference runtime","Inference mechanism","highly efficient deep learning inference mechanisms","Quality of service","Internet of Things","deep learning models","Task analysis","inference framework","Time factors","Real time","IoT devices","Quality-of-Service","State-of-the-art approach","IoT applications","Performance evaluation","Internet of Things (IOT)"],"max_cite":0.0,"pub_year":2018.0,"sources":"['scp', 'ieee']","rawkeys":["task analysis","predictable inference performance","data compression","inference mechanisms","intelligent decisions","robust inference","embedded systems","internet of things","iot devices","quality of service","iot applications","internet","internet of things (iot)","highly accurate inference","quality-of-service","qos","inference mechanism","machine learning","temporal constraints","computational modeling","learning (artificial intelligence)","deep learning","time factors","deep learning inference runtime","deeprt","qos requirements","highly efficient deep learning inference mechanisms","performance evaluation","iot","deep learning models","inference framework","real time","undesirable effects","state-of-the-art approach","qualityof-service requirement (qos)"],"tags":["task analysis","predictable inference performance","data compression","inference mechanisms","intelligent decisions","robust inference","embedded systems","iot devices","quality of service","iot applications","internet","internet of things (iot)","highly accurate inference","machine learning","temporal constraints","computational modeling","deep learning model","time factors","deep learning inference runtime","deeprt","qos requirements","highly efficient deep learning inference mechanisms","performance evaluation","inference framework","real time","undesirable effects","state-of-the-art approach","qualityof-service requirement (qos)"]},{"p_id":43778,"title":"Profit Maximization Mechanism and Data Management for Data Analytics Services","abstract":"IEEE With the advancement and emergence of new network services such as social network, Internet of Things and crowdsensing, large volume of diverse data is collected, shared and leveraged to develop analytics services. The data analytics service has become a key commodity that can be traded among various economic entities. In this paper, we address the optimal pricing mechanisms and data management for data analytics services and further discuss the perishable services in the time varying environment. We first propose a data market model and define the data utility based on the impact of data size on the performance of data analytics, e.g., prediction and verification accuracy. For perishable services, we study the perishability of data that affects the service quality and provide a quality decay function. The data analytics services are considered as digital goods and uniquely characterized by &#x201C;unlimited supply&#x201D; compared to conventional goods. Therefore, we apply the Bayesian profit maximization mechanism in selling data analytics services, which is truthful, rational and computationally efficient. The optimal service price, data amount and service update interval are obtained to maximize the profit under different customer&#x2019;s valuation distributions. Finally, experimental results on real-world datasets show that our data market model and pricing mechanism effectively solve the profit maximization problem and provide useful strategies for the data analytics service provider.","keywords_author":["Analytical models","Big Data","Cloud computing","Data analysis","data analytics","Data models","deep learning.","digital goods auction","Internet of Things","Internet of Things","optimal pricing","perishable service","Pricing"],"keywords_other":["Real-world datasets","Optimal pricing","Digital goods","Profit maximization","Time-varying environments","Computationally efficient","Data analytics","perishable service"],"max_cite":0.0,"pub_year":2018.0,"sources":"['scp']","rawkeys":["cloud computing","big data","deep learning","data analytics","internet of things","time-varying environments","digital goods","digital goods auction","profit maximization","real-world datasets","data analysis","data models","pricing","analytical models","computationally efficient","optimal pricing","perishable service"],"tags":["cloud computing","big data","data analytics","machine learning","time-varying environments","digital goods","digital goods auction","profit maximization","real-world datasets","data analysis","data models","pricing","analytical models","optimal pricing","computationally efficient","internet of things (iot)","perishable service"]},{"p_id":774,"title":"Deep Learning for the Internet of Things","abstract":"How can the advantages of deep learning be brought to the emerging world of embedded IoT devices? The authors discuss several core challenges in embedded and mobile deep learning, as well as recent solutions demonstrating the feasibility of building IoT applications that are powered by effective, efficient, and reliable deep learning models.","keywords_author":["deep learning","embedded learning","Internet of Things","IoT","machine learning","mobile and embedded deep learning","neural networks","mobile and embedded deep learning","deep learning","Internet of Things","IoT","neural networks","embedded learning","machine learning"],"keywords_other":["Embedded systems","efficient learning model","Neural networks","Computational modeling","Learning models","Feature extraction","mobile computing","Machine learning","mobile deep learning","learning (artificial intelligence)","Embedded learning","Internet oF Things","Internet of Things","Task analysis","embedded learning","embedded IoT devices","effective learning model","IOT applications","reliable deep learning model","IoT applications","Iot devices"],"max_cite":0.0,"pub_year":2018.0,"sources":"['scp', 'wos', 'ieee']","rawkeys":["task analysis","embedded systems","internet of things","iot devices","efficient learning model","iot applications","mobile and embedded deep learning","machine learning","mobile computing","mobile deep learning","embedded iot devices","computational modeling","learning (artificial intelligence)","deep learning","neural networks","iot","embedded learning","effective learning model","learning models","reliable deep learning model","feature extraction"],"tags":["computational modeling","effective learning model","learning models","task analysis","neural networks","embedded systems","machine learning","reliable deep learning model","mobile computing","iot devices","efficient learning model","feature extraction","mobile deep learning","iot applications","mobile and embedded deep learning","embedded learning","internet of things (iot)","embedded iot devices"]},{"p_id":27402,"title":"Towards an IoT big data analytics framework: Smart buildings systems","abstract":"\u00a9 2016 IEEE. There is a growing interest in IoT-enabled smart buildings. However, the storage and analysis of large amount of high-speed real-time smart building data is a challenging task. There are a number of contemporary Big Data management technologies and advanced analytics techniques that can be used to deal with this challenge. There is a need for an integrated IoT Big Data Analytics (IBDA) framework to fill the research gap in the Big Data Analytics domain. This paper presents one such IBDA framework for the storage and analysis of real time data generated from IoT sensors deployed inside the smart building. The initial version of the IBDA framework has been developed by using Python and the Big Data Cloudera platform. The applicability of the framework is demonstrated with the help of a scenario involving the analysis of real-time smart building data for automatically managing the oxygen level, luminosity and smoke\/hazardous gases in different parts of the smart building. The initial results indicate that the proposed framework is fit for the purpose and seems useful for IoT-enabled Big Data Analytics for smart buildings. The key contribution of this paper is the complex integration of Big Data Analytics and IoT for addressing the large volume and velocity challenge of real-time data in the smart building domain. This framework will be further evaluated and extended through its implementation in other domains.","keywords_author":["Apache flume","Apache spark","Big data","Cloudera","Digital-physical ecosystems","IoT","Real time data analytics","Smart building"],"keywords_other":["Complex integrations","Cloudera","Oxygen levels","Apache flume","Real-time data","Management technologies","Data analytics","Large amounts"],"max_cite":6.0,"pub_year":2017.0,"sources":"['scp']","rawkeys":["oxygen levels","apache spark","management technologies","big data","complex integrations","data analytics","real time data analytics","digital-physical ecosystems","smart building","large amounts","apache flume","real-time data","iot","cloudera"],"tags":["oxygen levels","apache spark","smart buildings","management technologies","big data","complex integrations","data analytics","digital-physical ecosystems","apache flume","real-time data","large amounts","real-time data analytics","cloudera","internet of things (iot)"]},{"p_id":41739,"title":"The Social Internet of Thing (S-IOT)-Based Mobile Group Handoff Architecture and Schemes for Proximity Service","abstract":"\u00a9 2013 IEEE. Internet of Things (IOT) is booming and has already been along with us everywhere. The things not only communicate with each other without human intervention but also own some relationships called Social IOT (S-IOT), which is the same as social relationships of human beings. In this work, the S-IOT architecture and scheme called MoGaHo-Prox for a group handoff from a mobile AP to a fixed AP and vice versa in a k-member touring group is proposed, for which members in the group benefit for downloading and sharing of geo touring information, e.g., the contents of Point Of Interests (POIs), in a more efficient way. This work (1) defines an S-IOT architecture and two functional scenarios called m-AP mode and f-AP mode for group touring, (2) proposes two control schemes called the conservative policy and the aggressive policy to handle the group handoff from the m-AP mode to the f-AP mode, and (3) provides control schemes for group handoff from the m-AP mode to the f-AP mode and vice versa. A real system is developed using the Android system and the performance analysis is evaluated from the perspective of expense fee, power consuming, and service time.","keywords_author":["Group handoff","group touring","location-based service (LBS)","proximity service","social internet of things (S-IOT)"],"keywords_other":["Social relationships","Performance analysis","Group handoff","Proximity service","Functional scenarios","social internet of things (S-IOT)","Internet of Things (IOT)","group touring"],"max_cite":1.0,"pub_year":2017.0,"sources":"['scp']","rawkeys":["performance analysis","functional scenarios","proximity service","group handoff","social relationships","group touring","social internet of things (s-iot)","internet of things (iot)","location-based service (lbs)"],"tags":["performance analysis","functional scenarios","social internet of things","proximity service","group handoff","group touring","location-based services","internet of things (iot)","social relationships"]},{"p_id":19212,"title":"A beaconless Opportunistic Routing based on a cross-layer approach for efficient video dissemination in mobile multimedia IoT applications","abstract":"Mobile multimedia networks are enlarging the Internet of Things (IoT) portfolio with a huge number of multimedia services for different applications. Those services run on dynamic topologies due to device mobility or failures and wireless channel impairments, such as mobile robots or Unmanned Aerial Vehicle (UAV) environments for rescue or surveillance missions. In those scenarios, beaconless Opportunistic Routing (OR) allows increasing the robustness of systems for supporting routing decisions in a completely distributed manner. Moreover, the addition of a cross-layer scheme enhances the benefits of a beaconless OR, and also enables multimedia dissemination with Quality of Experience (QoE) support. However, existing beaconless OR approaches do not support a reliable and efficient cross-layer scheme to enable effective multimedia transmission under topology changes, increasing the packet loss rate, and thus reducing the video quality level based on the user's experience. This article proposes a Link quality and Geographical beaconless OR protocol for efficient video dissemination for mobile multimedia IoT, called LinGO. This protocol relies on a beaconless OR approach and uses multiple metrics for routing decisions, including link quality, geographic location, and energy. A QoE\/video-aware optimisation scheme allows increasing the packet delivery rate in presence of links errors, by adding redundant video packets based on the frame importance from the human's point-of-view. Simulation results show that LinGO delivers live video flows with QoE support and robustness in mobile and dynamic topologies, as needed in future IoT environments. \u00a9 2014 Elsevier B.V. All rights reserved.","keywords_author":["Mobile network","Multimedia distribution","Multimedia IoT applications","QoE support"],"keywords_other":["Mobile multimedia networks","Multimedia transmissions","Internet of thing (IOT)","IOT applications","Opportunistic routing","Quality of experience (QoE)","Multimedia distribution","Surveillance missions"],"max_cite":21.0,"pub_year":2014.0,"sources":"['scp']","rawkeys":["mobile multimedia networks","multimedia transmissions","surveillance missions","internet of thing (iot)","mobile network","qoe support","quality of experience (qoe)","multimedia distribution","multimedia iot applications","iot applications","opportunistic routing"],"tags":["mobile multimedia networks","multimedia transmissions","surveillance missions","qoe support","quality of experience (qoe)","multimedia distribution","opportunistic routing","multimedia iot applications","iot applications","mobile networks","internet of things (iot)"]},{"p_id":41740,"title":"Mining Internet of Things for intelligent objects using genetic algorithm","abstract":"\u00a9 2017 Elsevier Ltd The Internet of Things (IoT) is overpopulated by a large number of objects and millions of services and interactions. Therefore, the ability to search for the right object to provide a specific service is important. The merger of the IoT and social networking, the Social Internet of Things (SIoT), has made this possible. The main idea in the SIoT is that every object in the IoT can use its friends\u2019 or friends-of-friends\u2019 relationships to search for a specific service. However, this is usually a slow process because each node (object) is required to manage a large number of friends. This paper addresses the issue of link selection of friends and analyzes five strategies in the literature. Then it proposes a link selection strategy using the Genetic Algorithm (GA) to find the near optimal solution. The results show an improvement over the examined strategies in terms of several parameters.","keywords_author":["Friendship selection","Genetic algorithm","IoT","Link selection","Searching IoT","Social IoT (SIoT)"],"keywords_other":["Internet of thing (IOT)","Near-optimal solutions","Friendship selection","Social IoT (SIoT)","Link selection","Intelligent object","Searching IoT"],"max_cite":1.0,"pub_year":2018.0,"sources":"['scp']","rawkeys":["intelligent object","internet of thing (iot)","friendship selection","near-optimal solutions","genetic algorithm","searching iot","link selection","iot","social iot (siot)"],"tags":["intelligent object","friendship selection","near-optimal solutions","genetic algorithm","searching iot","link selection","social iot (siot)","internet of things (iot)"]},{"p_id":27406,"title":"An extended IoT framework with semantics, big data, and analytics","abstract":"\u00a9 2016 IEEE. Many experts claim that data will be the most valuable commodity in the 21st century. At the same time, two of the most influential components of this era, Big Data and IoT are moving very fast, on a collision course with the methodologies that are associated with conventional data processing and database systems. As a result, new approaches like NoSQL databases, distributed architectures, etc. started appearing on the stage. Meanwhile, another technology, ontology and semantic data processing can be a very convenient catalyzer that might assist in smoothly providing this transformation process. In this paper, we propose a combined framework that brings Big Data, IoT, and semantic web together to build an augmented framework for this new era. We not only list the components of such a system and define the necessary bindings that needs to be integrated together, but also provide a realistic use case that demonstrates how the model can implement the desired functionality and achieve the goals of such a model.","keywords_author":["big data analytics","framework","Internet of things","open system","semantics"],"keywords_other":["Collision course","framework","Nosql database","Transformation process","New approaches","Distributed architecture","Data analytics"],"max_cite":6.0,"pub_year":2016.0,"sources":"['scp']","rawkeys":["new approaches","data analytics","framework","nosql database","internet of things","semantics","transformation process","open system","distributed architecture","big data analytics","collision course"],"tags":["new approaches","data analytics","framework","nosql database","semantics","transformation process","internet of things (iot)","open system","distributed architecture","big data analytics","collision course"]},{"p_id":33552,"title":"Synchronous dynamic view learning: A framework for autonomous training of activity recognition models using wearable sensors","abstract":"\u00a9 2017 ACM. Wearable technologies play a central role in human-centered Internet-of-Things applications. Wearables leverage machine learning algorithms to detect events of interest such as physical activities and medical complications. These algorithms, however, need to be retrained upon any changes in configuration of the system, such as addition\/removal of a sensor to\/from the network or displacement\/misplacement\/mis-orientation of the physical sensors on the body. We challenge this retraining model by stimulating the vision of autonomous learning with the goal of eliminating the labor-intensive, time-consuming, and highly expensive process of collecting labeled training data in dynamic environments. We propose an approach for autonomous retraining of the machine learning algorithms in real-time without need for any new labeled training data. We focus on a dynamic setting where new sensors are added to the system and worn on various body locations. We capture the inherent correlation between observations made by a static sensor view for which trained algorithms exist and the new dynamic sensor views for which an algorithm needs to be developed. By applying our realtime dynamic-view autonomous learning approach, we achieve an average accuracy of 81.1% in activity recognition using three experimental datasets. This amount of accuracy represents more than 13.8% improvement in the accuracy due to the automatic labeling of the sensor data in the newly added sensor. This performance is only 11.2% lower than the experimental upper bound where labeled training data are collected with the new sensor.","keywords_author":["IoT","Machine learning","Signal processing","Wearable sensors"],"keywords_other":["Real-time dynamics","Activity recognition","Autonomous learning","Medical complications","Automatic labeling","Labeled training data","Synchronous dynamics","Dynamic environments"],"max_cite":2.0,"pub_year":2017.0,"sources":"['scp']","rawkeys":["automatic labeling","activity recognition","labeled training data","wearable sensors","synchronous dynamics","machine learning","signal processing","medical complications","dynamic environments","autonomous learning","iot","real-time dynamics"],"tags":["activity recognition","labeled training data","wearable sensors","synchronous dynamics","machine learning","signal processing","medical complications","dynamic environments","autonomous learning","automatic labeling","internet of things (iot)","real-time dynamics"]},{"p_id":41745,"title":"Replacement autoencoder: a privacy-preserving algorithm for sensory data analysis","abstract":"\u00a9 2018 IEEE. An increasing number of sensors on mobile, Internet of things (IoT), and wearable devices generate time-series measurements of physical activities. Though access to the sensory data is critical to the success of many beneficial applications such as health monitoring or activity recognition, a wide range of potentially sensitive information about the individuals can also be discovered through access to sensory data and this cannot easily be protected using traditional privacy approaches. In this paper, we propose a privacy-preserving sensing framework for managing access to time-series data in order to provide utility while protecting individuals' privacy. We introduce Replacement AutoEncoder, a novel feature-learning algorithm which learns how to transform discriminative features of multi-variate time-series that correspond to sensitive inferences, into some features that have been more observed in non-sensitive inferences, to protect users' privacy. This efficiency is achieved by defining a user-customized objective function for deep autoencoders. Replacement will not only eliminate the possibility of recognition sensitive inferences, it also eliminates the possibility of detecting the occurrence of them, that is the main weakness of other approaches such as filtering or randomization. We evaluate the efficacy of the algorithm with an activity recognition task in a multi-sensing environment using extensive experiments on three benchmark datasets. We show that it can retain the recognition accuracy of state-of-the-art techniques while simultaneously preserving the privacy of sensitive information. Finally, we utilize the GANs for detecting the occurrence of replacement, after releasing data, and show that this can be done only if the adversarial network is trained on the users' original data.","keywords_author":["Activity Recognition","Feature Learning","Privacy Protection","Time Series Analysis","Wearable Sensors"],"keywords_other":["Activity recognition","Sensory data analysis","Privacy protection","Sensitive informations","Feature learning","Discriminative features","State-of-the-art techniques","Internet of Things (IOT)"],"max_cite":1.0,"pub_year":2018.0,"sources":"['scp', 'ieee']","rawkeys":["activity recognition","sensory data analysis","wearable sensors","sensitive informations","time series analysis","privacy protection","discriminative features","feature learning","state-of-the-art techniques","internet of things (iot)"],"tags":["activity recognition","sensory data analysis","wearable sensors","sensitive informations","time series analysis","privacy protection","discriminative features","feature learning","state-of-the-art techniques","internet of things (iot)"]},{"p_id":31507,"title":"Enabling Technologies for the Internet of Health Things","abstract":"\u00a9 2013 IEEE. The Internet of Things (IoT) is one of the most promising technologies for the near future. Healthcare and well-being will receive great benefits with the evolution of this technology. This paper presents a review of techniques based on IoT for healthcare and ambient-assisted living, defined as the Internet of Health Things (IoHT), based on the most recent publications and products available in the market from industry for this segment. Also, this paper identifies the technological advances made so far, analyzing the challenges to be overcome and provides an approach of future trends. Through selected works, it is possible to notice that further studies are important to improve current techniques and that novel concept and technologies of IoHT are needed to overcome the identified challenges. The presented results aim to serve as a source of information for healthcare providers, researchers, technology specialists, and the general population to improve the IoHT.","keywords_author":["Ambient assisted living","Internet of Health Things","Internet of Things","mobile health","remote healthcare monitoring","wearable"],"keywords_other":["General population","Internet of thing (IOT)","Enabling technologies","Healthcare monitoring","Wearable","Health care providers","Technological advances","Ambient assisted living"],"max_cite":3.0,"pub_year":2018.0,"sources":"['scp']","rawkeys":["technological advances","ambient assisted living","general population","healthcare monitoring","internet of thing (iot)","internet of health things","internet of things","remote healthcare monitoring","health care providers","mobile health","wearable","enabling technologies"],"tags":["technological advances","ambient assisted living","general population","healthcare monitoring","remote healthcare monitoring","internet of health things","wearables","health care providers","mobile health","internet of things (iot)","enabling technologies"]},{"p_id":791,"title":"Private and Scalable Personal Data Analytics Using Hybrid Edge-to-Cloud Deep Learning","abstract":"Although the ability to collect, collate, and analyze the vast amount of data generated from cyber-physical systems and Internet of Things devices can be beneficial to both users and industry, this process has led to a number of challenges, including privacy and scalability issues. The authors present a hybrid framework where user-centered edge devices and resources can complement the cloud for providing privacy-aware, accurate, and efficient analytics.","keywords_author":["analytics","cloud computing","cyber-physical systems","deep learning","edge computing","Internet of Things","IoT","mobile and embedded deep learning","privacy","security","deep learning","edge computing","security","cloud computing","privacy","Internet of Things","IoT","mobile and embedded deep learning","cyber-physical systems","analytics"],"keywords_other":["hybrid edge-to-cloud deep learning","Cloud computing","Data analytics","Internet of Things devices","privacy","Feature extraction","Machine learning","efficient analytics","Privacy aware","scalability issues","analytics","scalable personal data analytics","Data privacy","data privacy","security","private personal data analytics","Hybrid framework","Internet of Things","user-centered edge devices","Task analysis","User-centered","cyber-physical systems","industry","Data analysis","cloud computing","hybrid framework","Scalability issue","data analysis","users"],"max_cite":0.0,"pub_year":2018.0,"sources":"['scp', 'wos', 'ieee']","rawkeys":["task analysis","hybrid edge-to-cloud deep learning","data analytics","internet of things","mobile and embedded deep learning","user-centered","privacy","scalability issue","machine learning","data analysis","efficient analytics","scalability issues","analytics","scalable personal data analytics","deep learning","data privacy","security","private personal data analytics","edge computing","iot","privacy aware","user-centered edge devices","cyber-physical systems","industry","cloud computing","hybrid framework","feature extraction","internet of things devices","users"],"tags":["task analysis","hybrid edge-to-cloud deep learning","data analytics","mobile and embedded deep learning","internet of things (iot)","user-centered","privacy","scalability issue","machine learning","data analysis","efficient analytics","analytics","scalable personal data analytics","data privacy","security","private personal data analytics","edge computing","privacy aware","user-centered edge devices","cyber-physical systems","industry","cloud computing","hybrid framework","feature extraction","internet of things devices","users"]},{"p_id":58135,"title":"No-Reference Stereoimage Quality Assessment for Multimedia Analysis Towards Internet-of-Things","abstract":"With continuous progress of Internet of Things, multimedia analysis in it has attracted more and more attention. Specially, stereoscopic display technology plays an important role in the multimedia analysis processing. In the Internet of Things system, the quality of stereoscopic image will be reduced in the transmission process. In this mode, it will have a great impact on multimedia analysis to judge whether the quality of stereoscopic image meets the requirements. In this paper, a new no-reference stereoscopic image quality assessment model for multimedia analysis towards Internet of Things is built, which is based on a deep learning model to learn from the class labels and image representations. In our framework, images are represented by natural scene statistics features that are extracted from discrete cosine transform domain, and a regression model is employed to shine upon the quality from the feature vector. The training process of the proposed model contains an unsupervised pretraining phase and a supervised fine-tuning phase, enabling it to generalize over the whole distortion types and severity. The proposed model greatly shows the correlation with subjective assessment as demonstrated by experiments on the LIVE 3-D Image Quality Database and IVC 3-D Image Quality Database.","keywords_author":["Multimedia analysis","internet-of-things","stereoscopic image quality assessment","deep belief networks"],"keywords_other":["SALIENCY","DORSAL","VENTRAL VISUAL PATHWAYS","COMBINATION","STRUCTURAL SIMILARITY","DOMAIN","MODEL","INTEGRATION","STEREOSCOPIC IMAGE QUALITY","SCENE STATISTICS"],"max_cite":0.0,"pub_year":2018.0,"sources":"['ieee', 'wos']","rawkeys":["ventral visual pathways","integration","scene statistics","model","stereoscopic image quality assessment","combination","internet-of-things","stereoscopic image quality","multimedia analysis","domain","saliency","deep belief networks","dorsal","structural similarity"],"tags":["integration","scene statistics","model","stereoscopic image quality assessments","combination","stereoscopic image quality","multimedia analysis","domain","saliency","structural similarity","deep belief networks","dorsal","internet of things (iot)","ventral visual pathway"]},{"p_id":47895,"title":"End-User Development for Interactive Data Analytics: Uncertainty, Correlation and User Confidence","abstract":"Crown This paper investigates End-User Development (EUD) for interactive data-analytic interfaces &#x2013; building upon the ideas of making machine learning transparent. The research is carried out in a business operation environment (water pipe failure prediction in our case) motivated to integrate advanced analytics into decision-making processes of an urban Internet of Things (IoT) concept. We explore effects of revealing uncertainty and correlation on user confidence in a data-driven decision making scenario. It was found that user confidence varied significantly amongst various user groups when different machine learning models were displayed with\/without supplementary information. Galvanic Skin Response (GSR) signals were analyzed and shown as reasonable indices for predicting user confidence levels. Supplementary data visualizations (of inherent uncertainty and correlation in data) contributed to explicability principles while GSR indexing added towards correctibility principles. We recommend transparent machine learning as the key to effective EUD for interactive data analytics.","keywords_author":["Correlation","Correlation","Data analysis","Decision making","decision making","Galvanic Skin Response","Human computer interaction","machine learning","Skin","Uncertainty","uncertainty","user confidence"],"keywords_other":["Uncertainty","Machine learning models","End user development(EUD)","Decision making process","user confidence","Galvanic skin response","Supplementary information","Internet of Things (IOT)"],"max_cite":0.0,"pub_year":2017.0,"sources":"['scp']","rawkeys":["decision making","skin","supplementary information","machine learning models","uncertainty","machine learning","galvanic skin response","human computer interaction","end user development(eud)","correlation","user confidence","data analysis","internet of things (iot)","decision making process"],"tags":["decision making","skin","supplementary information","machine learning models","uncertainty","machine learning","galvanic skin response","human-computer interaction","correlation","end user development","user confidence","data analysis","internet of things (iot)","decision making process"]},{"p_id":811,"title":"An Intelligent Traffic Load Prediction Based Adaptive Channel Assignment Algorithm in SDN-IoT: A Deep Learning Approach","abstract":"Due to the fast increase of sensing data and quick response requirement in the Internet of Things (IoT) delivery network, the high speed transmission has emerged as an important issue. Assigning suitable channels in the wireless IoT delivery network is a basic guarantee of high speed transmission. However, the high dynamics of traffic load make the conventional fixed channel assignment algorithm ineffective. Recently, the Software Defined Networking based IoT (SDN-IoT) is proposed to improve the transmission quality. Besides this, the intelligent technique of deep learning is widely researched in high computational SDN. Hence, we firstly propose a novel deep learning based traffic load prediction algorithm to forecast future traffic load and congestion in network. Then, a Deep Learning based Partially Channel Assignment Algorithm, referred to as DLPOCA, is proposed to intelligently allocate channels to each link in the SDN-IoT network. Finally, we consider a deep learning based prediction and Partially Overlapping Channel Assignment (POCA) to propose a novel intelligent channel assignment algorithm (TP-DLPOCA), which can intelligently avoid potential congestion and quickly assign suitable channels in SDN-IoT. The simulation result demonstrates that our proposal significantly outperforms conventional channel assignment algorithms.","keywords_author":["Channel allocation","Control systems","Deep learning","Internet of Things","Internet of Things (IoT)","Machine learning","Partially Overlapping Channel Assignment (POCA)","Prediction algorithms","Routing","Sensors","Software Defined Network (SDN)","traffic load prediction.","Deep learning","Internet of Things (IoT)","Software Defined Network (SDN)","Partially Overlapping Channel Assignment (POCA)","traffic load prediction."],"keywords_other":["Internet of Things","Channel allocation","Machine learning","Control systems","Channel Assignment","Routing","Prediction algorithms","Traffic loads","Internet of Things (IOT)","Sensors"],"max_cite":0.0,"pub_year":2018.0,"sources":"['scp', 'ieee']","rawkeys":["partially overlapping channel assignment (poca)","sensors","deep learning","routing","channel allocation","software defined network (sdn)","internet of things","machine learning","prediction algorithms","channel assignment","traffic loads","traffic load prediction","internet of things (iot)","control systems"],"tags":["partially overlapping channel assignment (poca)","sensors","routing","channel allocation","software-defined networking","machine learning","prediction algorithms","channel assignment","traffic loads","traffic load prediction","internet of things (iot)","control systems"]},{"p_id":31533,"title":"Learning-based and data-driven TCP design for memory-constrained IoT","abstract":"\u00a9 2016 IEEE. Advances in wireless technology have resulted in pervasive deployment of devices of a high variability in form factors, memory and computational ability. The need for maintaining continuous connections that deliver data with high reliability necessitate re-thinking of conventional design of the transport layer protocol. This paper investigates the use of Q-learning in TCP cwnd adaptation during the congestion avoidance state, wherein the classical alternation of the window is replaced, thereby allowing the protocol to immediately respond to previously seen network conditions. Furthermore, it demonstrates how memory plays a critical role in building the exploration space, and proposes ways to reduce this overhead through function approximation. The superior performance of the learning-based approach over TCP New Reno is demonstrated through a comprehensive simulation study, revealing 33.8% and 12.1% improvement in throughput and delay, respectively, for the evaluated topologies. We also show how function approximation can be used to dramatically reduce the memory requirements of a learning-based protocol while maintaining the same throughput and delay.","keywords_author":["Function approximation","IoT","Kanerva coding","Q-learning","TCP"],"keywords_other":["Function approximation","Learning-based approach","Computational ability","Q-learning","Wireless technologies","Congestion avoidance","Kanerva coding","Transport layer protocols"],"max_cite":3.0,"pub_year":2016.0,"sources":"['scp']","rawkeys":["function approximation","tcp","transport layer protocols","wireless technologies","computational ability","q-learning","congestion avoidance","iot","kanerva coding","learning-based approach"],"tags":["function approximation","computing abilities","tcp","transport layer protocols","q-learning","congestion avoidance","kanerva coding","learning-based approach","internet of things (iot)","wireless technology"]},{"p_id":21297,"title":"Ultra-Dense Networks: Survey of State of the Art and Future Directions","abstract":"\u00a9 2016 IEEE.Within the foreseeable future, the growing number of mobile devices, and their diversity, will challenge the current network architecture. Furthermore, users will expect greater data rates, lower latency, lower packet drop rates, etc. in future wireless networks. Ultra Dense Networks (UDN), considered to be one of the best ways to meet user expectations and support future wireless network deployment, will face multiple significant hurdles, including interference, mobility, and cost. In this paper, we review existing research efforts toward addressing those challenges and present future avenues for research. We first develop a taxonomy to review and describe existing research efforts. Next, we focus on inter-cell interference, handover performance, and energy efficiency as the key techniques to addressing the most pressing challenges. Finally, we present several future research directions, including emergent Internet-of-Things (IoT) applications, security and privacy, modeling and realistic simulations, and relevant techniques.","keywords_author":["Cost management","Interference management","Internet-of-Things (IoT)","Mobility management","Security","Survey","Ultra Dense Network (UDN)"],"keywords_other":["Cost management","Interference management","Dense network","Mobility management","Security","Internet of Things (IOT)"],"max_cite":15.0,"pub_year":2016.0,"sources":"['ieee', 'scp', 'wos']","rawkeys":["interference management","survey","ultra dense network (udn)","mobility management","internet-of-things (iot)","cost management","security","dense network","internet of things (iot)"],"tags":["interference management","survey","mobility management","ultra-dense networks","cost management","security","dense network","internet of things (iot)"]},{"p_id":37686,"title":"Machine learning based estimation of Ozone using spatio-temporal data from air quality monitoring stations","abstract":"\u00a9 2016 IEEE. In this paper, models are created to predict the levels of ground level Ozone at particular locations based on the cross-correlation and spatial-correlation of different air pollutants whose readings are obtained from several different air quality monitoring stations in Gauteng province, South Africa, including the City of Johannesburg which is on the cusp of being one of the world's megacities and is currently the most polluted city in the country. Datasets spanning several years collected from the monitoring stations and transmitted through the Internet-of-Things are used. Big data analytics and cognitive computing is used to get insights on the data and create models that can estimate levels of Ozone without requiring massive computational power or intense numerical analysis.","keywords_author":["analytics","big data","cognitive computing","internet-of-things","machine learning"],"keywords_other":["Computational power","Cognitive Computing","Air quality monitoring stations","Spatio-temporal data","Spatial correlations","Cross correlations","Monitoring stations","analytics"],"max_cite":1.0,"pub_year":2017.0,"sources":"['scp']","rawkeys":["air quality monitoring stations","big data","computational power","machine learning","monitoring stations","cross correlations","spatial correlations","spatio-temporal data","internet-of-things","cognitive computing","analytics"],"tags":["air quality monitoring stations","cloud computing","big data","machine learning","monitoring stations","cross correlations","spatial correlations","spatio-temporal data","computational power","internet of things (iot)","analytics"]},{"p_id":31543,"title":"A delay-aware schedule method for distributed information fusion with elastic and inelastic traffic","abstract":"\u00a9 2016 Elsevier B.V.Information fusion is an efficient way to detect the specified events and extract useful information, especially in the context of big data. As a large-scale data-gathering system, Internet of Things (IoT) has the traffic with the mixed timing characteristics. The real-time observations with various delay constraints and the non-real-time observations are needed in information fusion. In order to guarantee the performance of Distributed Information Fusion (DIF), the paper focuses on the communication mechanism from the perspective of real-time delivery of sensing data. An online scheduling algorithm and its distributed implementation, named Delay-Guaranteed CSMA, are proposed. Both the timing constraints and the historical transmission statistics of sensors are taking into consideration. The simulation results have shown that the proposed policy achieves good delay-guaranteed satisfaction. The goal of real-time data delivery for distributed information fusion is achieved.","keywords_author":["Big data","Distributed information fusion","Internet of things","Online scheduling","Real-time"],"keywords_other":["Online scheduling algorithm","Timing characteristics","Online scheduling","Distributed information fusion","Distributed implementation","Communication mechanisms","Internet of Things (IOT)","Real time"],"max_cite":3.0,"pub_year":2017.0,"sources":"['scp']","rawkeys":["timing characteristics","big data","real time","real-time","internet of things","distributed information fusion","online scheduling algorithm","communication mechanisms","online scheduling","distributed implementation","internet of things (iot)"],"tags":["timing characteristics","big data","real time","distributed information fusion","online scheduling algorithm","communication mechanisms","online scheduling","distributed implementation","internet of things (iot)"]},{"p_id":33592,"title":"SimML framework: Monte Carlo simulation of statistical machine learning algorithms for IoT prognostic applications","abstract":"\u00a9 2016 IEEE. Advanced statistical machine learning (ML) algorithms are being developed, trained, tuned, optimized, and validated for real-time prognostics for internet-of-things (IoT) applications in the fields of manufacturing, transportation, and utilities. For such applications, we have achieved greatest prognostic success with ML algorithms from a class of pattern recognition known as nonlinear, nonparametric regression. To intercompare candidate ML algorithmics to identify the 'best' algorithms for IoT prognostic applications, we use three quantitative performance metrics: false alarm probability (FAP), missed alarm probability (MAP), and overhead compute cost (CC) for real-time surveillance. This paper presents a comprehensive framework, SimML, for systematic parametric evaluation of statistical ML algorithmics for IoT prognostic applications. SimML evaluates quantitative FAP, MAP, and CC performance as a parametric function of input signals' degree of cross-correlation, signal-to-noise ratio, number of input signals, sampling rates for the input signals, and number of training vectors selected for training. Output from SimML is provided in the form of 3D response surfaces for the performance metrics that are essential for comparing candidate ML algorithms in precise, quantitative terms.","keywords_author":["AAKR","IoT Prognostics","Machine Learning","MSET"],"keywords_other":["IoT Prognostics","Real-time surveillance","MSET","Statistical machine learning","AAKR","False alarm probability","Internet of Things (IOT)","Non-parametric regression"],"max_cite":2.0,"pub_year":2017.0,"sources":"['scp']","rawkeys":["mset","iot prognostics","statistical machine learning","machine learning","aakr","false alarm probability","real-time surveillance","internet of things (iot)","non-parametric regression"],"tags":["mset","iot prognostics","statistical machine learning","machine learning","aakr","false alarm probability","real-time surveillance","internet of things (iot)","non-parametric regression"]},{"p_id":824,"title":"Edge Enhanced Deep Learning System for Large-Scale Video Stream Analytics","abstract":"Applying deep learning models to large-scale IoT data is a compute-intensive task and needs significant computational resources. Existing approaches transfer this big data from IoT devices to a central cloud where inference is performed using a machine learning model. However, the network connecting the data capture source and the cloud platform can become a bottleneck. We address this problem by distributing the deep learning pipeline across edge and cloudlet\/fog resources. The basic processing stages and trained models are distributed towards the edge of the network and on in-transit and cloud resources. The proposed approach performs initial processing of the data close to the data source at edge and fog nodes, resulting in significant reduction in the data that is transferred and stored in the cloud. Results on an object recognition scenario show 71\\% efficiency gain in the throughput of the system by employing a combination of edge, in-transit and cloud resources when compared to a cloud-only approach.","keywords_author":null,"keywords_other":["cloudlet-fog resources","Cloud computing","Pipelines","Computer architecture","edge enhanced deep learning system","fog nodes","Cameras","video streaming","data source","Machine learning","large-scale IoT data","data capture source","learning (artificial intelligence)","video signal processing","central cloud","Big Data","Internet of Things","Streaming media","IoT devices","machine learning model","cloud computing","cloud platform","cloud-only approach","Clouds","object recognition","deep learning pipeline","large-scale video stream analytics"],"max_cite":null,"pub_year":2018.0,"sources":"['ieee']","rawkeys":["pipelines","cloudlet-fog resources","big data","internet of things","iot devices","edge enhanced deep learning system","fog nodes","machine learning","clouds","video streaming","data source","cameras","streaming media","data capture source","computer architecture","learning (artificial intelligence)","video signal processing","central cloud","machine learning model","cloud computing","cloud platform","cloud-only approach","object recognition","large-scale iot data","deep learning pipeline","large-scale video stream analytics"],"tags":["pipelines","cloudlet-fog resources","big data","machine learning models","iot devices","data-sources","internet of things (iot)","edge enhanced deep learning system","fog nodes","machine learning","video streaming","cameras","streaming media","data capture source","computer architecture","cloud platforms","video signal processing","central cloud","cloud computing","cloud-only approach","object recognition","large-scale iot data","deep learning pipeline","large-scale video stream analytics","cloud"]},{"p_id":15162,"title":"Internet of multimedia things: Vision and challenges","abstract":"\u00a9 2015 Elsevier B.V. All rights reserved. Internet of Things (IoT) systems cannot successfully realize the notion of ubiquitous connectivity of everything if they are not capable to truly include 'multimedia things'. However, the current research and development activities in the field do not mandate the features of multimedia objects, thus leaving a gap to benefit from multimedia content based services and applications. In this paper, we analyze this issue by contemplating the concept of IoT and drawing an inspiration towards the perspective vision of 'Internet of Multimedia Things' (IoMT). Therein, we introduce IoMT as a novel paradigm in which smart heterogeneous multimedia things can interact and cooperate with one another and with other things connected to the Internet to facilitate multimedia based services and applications that are globally available to the users. Some applications and use-cases for IoMT are presented to reflect the possibilities enabled by this new paradigm. An IoMT architecture is then presented which is segregated into four distinct stages; (i) multimedia sensing, (ii) reporting and addressability, (iii) multimedia-aware cloud, and (iv) multi-agent systems. Instead of proposing specific technical solutions for each individual stage of the presented architecture, we survey the already existing technologies, providing a synthesis for the realization of the vision of IoMT. Subsequently, various requirements and challenges as well as the feasibility of existing solutions for each stage of proposed IoMT architecture are comprehensively discussed.","keywords_author":["Architecture","Internet of multimedia things","Internet of things","Multimedia processing","Wireless communication"],"keywords_other":["Multimedia processing","Services and applications","Research and development","Multimedia object","Technical solutions","Wireless communications","Multimedia contents","Internet of Things (IOT)"],"max_cite":49.0,"pub_year":2015.0,"sources":"['scp', 'wos']","rawkeys":["multimedia processing","research and development","services and applications","internet of things","multimedia object","multimedia contents","wireless communications","technical solutions","wireless communication","internet of multimedia things","architecture","internet of things (iot)"],"tags":["multimedia processing","research and development","services and applications","multimedia object","multimedia contents","wireless communications","technical solutions","internet of multimedia things","architecture","internet of things (iot)"]},{"p_id":11066,"title":"Semisupervised Deep Reinforcement Learning in Support of IoT and Smart City Services","abstract":"Smart services are an important element of the smart cities and the Internet of Things (IoT) ecosystems where the intelligence behind the services is obtained and improved through the sensory data. Providing a large amount of training data is not always feasible; therefore, we need to consider alternative ways that incorporate unlabeled data as well. In recent years, deep reinforcement learning (DRL) has gained great success in several application domains. It is an applicable method for IoT and smart city scenarios where auto-generated data can be partially labeled by users' feedback for training purposes. In this paper, we propose a semisupervised DRL model that fits smart city applications as it consumes both labeled and unlabeled data to improve the performance and accuracy of the learning agent. The model utilizes variational autoencoders as the inference engine for generalizing optimal policies. To the best of our knowledge, the proposed model is the first investigation that extends DRL to the semisupervised paradigm. As a case study of smart city applications, we focus on smart buildings and apply the proposed model to the problem of indoor localization based on Bluetooth low energy signal strength. Indoor localization is the main component of smart city services since people spend significant time in indoor environments. Our model learns the best action policies that lead to a close estimation of the target locations with an improvement of 23% in terms of distance to the target and at least 67% more received rewards compared to the supervised DRL model.","keywords_author":["Bluetooth low energy indoor localization","deep learning","deep reinforcement learning (DRL)","indoor positioning","Internet of Things (IoT)","IoT smart services","reinforcement learning","semisupervised deep reinforcement learning","smart city","Bluetooth low energy indoor localization","deep learning","deep reinforcement learning (DRL)","indoor positioning","Internet of Things (IoT)","IoT smart services","reinforcement learning","semisupervised deep reinforcement learning","smart city"],"keywords_other":["deep reinforcement learning (DRL)","Semi-supervised","Indoor positioning","Smart services","Indoor localization","Internet of Things (IOT)"],"max_cite":3.0,"pub_year":2018.0,"sources":"['ieee', 'scp', 'wos']","rawkeys":["indoor positioning","bluetooth low energy indoor localization","deep learning","semisupervised deep reinforcement learning","semi-supervised","iot smart services","internet of things (iot)","reinforcement learning","indoor localization","smart services","deep reinforcement learning (drl)","smart city"],"tags":["indoor positioning","bluetooth low energy indoor localization","semisupervised deep reinforcement learning","semi-supervised","machine learning","iot smart services","deep reinforcement learning","reinforcement learning","indoor localization","smart services","internet of things (iot)","smart cities"]},{"p_id":47943,"title":"Develop a personalized intelligent music selection system based on heart rate variability and machine learning","abstract":"\u00a9 2016, Springer Science+Business Media New York.Music often plays an important role in people\u2019s daily lives. Because it has the power to affect human emotion, music has gained a place in work environments and in sports training as a way to enhance the performance of particular tasks. Studies have shown that office workers perform certain jobs better and joggers run longer distances when listening to music. However, a personalized music system which can automatically recommend songs according to user\u2019s physiological response remains absent. Therefore, this study aims to establish an intelligent music selection system for individual users to enhance their learning performance. We first created an emotional music database using data analytics classifications. During testing, innovative wearable sensing devices were used to detect heart rate variability (HRV) in experiments, which subsequently guided music selection. User emotions were then analyzed and appropriate songs were selected by using the proposed application software (App). Machine learning was used to record user preference, ensuring accurate and precise classification. Significant results generated through experimental validation indicate that this system generates high satisfaction levels, does not increase mental workload, and improves users\u2019 performance. Under the trend of the Internet of Things (IoT) and the continuing development of wearable devices, the proposed system could stimulate innovative applications for smart factory, home, and health care.","keywords_author":["Heart rate variability (HRV)","Machine learning","Music recommendation","Personalized music system","Wearable device"],"keywords_other":["Physiological response","Internet of thing (IOT)","Experimental validations","Heart rate variability","Continuing development","Personalized music system","Music recommendation","Wearable devices"],"max_cite":0.0,"pub_year":2017.0,"sources":"['scp']","rawkeys":["wearable device","wearable devices","personalized music system","heart rate variability (hrv)","internet of thing (iot)","machine learning","physiological response","experimental validations","continuing development","heart rate variability","music recommendation"],"tags":["wearable devices","personalized music system","machine learning","physiological response","experimental validations","continuous development","heart rate variability","music recommendation","internet of things (iot)"]},{"p_id":29512,"title":"Deep-learning-based security evaluation on authentication systems using arbiter PUF and its variants","abstract":"\u00a9 Springer International Publishing Switzerland 2016. Fake integrated circuit (IC) chips are in circulation on the market, which is considered a serious threat in the era of the Internet of Things (IoTs). A physically unclonable function (PUF) is expected to be a fundamental technique to separate the fake IC chips from genuine ones. Recently, the arbiter PUF (APUF) and its variants are intensively researched aiming at using for a secure authentication system. However, vulnerability of APUFs against machine-learning attacks was reported. Upon the situation, the double arbiter PUF (DAPUF), which has a tolerance against support vector machine (SVM)-based machinelearning attacks, was proposed as another variant of APUF in 2014. In this paper, we perform a security evaluation for authentication systems using APUF and its variants against Deep-learning (DL)-based attacks. DL has attracted attention as a machine-learning method that produces better results than SVM in various research fields. Based on the experimental results, we show that these DAPUFs could be used as a core primitive in a secure authentication system if setting an appropriate threshold to distinguish a legitimate IC tags from fake ones.","keywords_author":["Authentication","Deep learning","Machine-learning attack","Physically unclonable function"],"keywords_other":["Deep learning","Secure authentications","Integrated circuit chips","Physically unclonable functions","Internet of thing (IoTs)","Security evaluation","Machine learning methods","Authentication systems"],"max_cite":4.0,"pub_year":2016.0,"sources":"['scp']","rawkeys":["machine learning methods","integrated circuit chips","authentication","deep learning","physically unclonable functions","machine-learning attack","authentication systems","secure authentications","security evaluation","internet of thing (iots)","physically unclonable function"],"tags":["machine learning methods","integrated circuit chips","authentication","physically unclonable functions","machine learning","authentication systems","security evaluation","machine learning attacks","security authentication","internet of things (iot)"]},{"p_id":49993,"title":"Hot Chips 27 Highlights","abstract":null,"keywords_author":["5G","cloud computing","datacenter","deep learning","field-programmable gate array","global cache coherency","graphics","high-performance computing","Hot Chips","Internet of Things","Moore's law","multimedia","silicon","SoC","social media","Sparc","system on chip","wireless"],"keywords_other":null,"max_cite":0.0,"pub_year":2016.0,"sources":"['scp']","rawkeys":["cloud computing","global cache coherency","wireless","moore's law","deep learning","social media","datacenter","field-programmable gate array","graphics","internet of things","system on chip","high-performance computing","multimedia","soc","sparc","5g","silicon","hot chips"],"tags":["cloud computing","global cache coherency","wireless","moore's law","social media","datacenter","machine learning","fpga","graphics","silicon","system-on-chip","multimedia","high performance computing","sparc","5g","internet of things (iot)","hot chips"]},{"p_id":29525,"title":"Machine learning for stress detection from ECG signals in automobile drivers","abstract":"\u00a9 2015 IEEE. Physiological sensor analytics is becoming an important tool to monitor health as the availability of sensor-enabled portable, wearable, and implantable devices becomes ubiquitous in the growing Internet of Things (IoT). Physiological multi-sensor studies have been conducted previously to detect stress. In this study, we focus on ECG monitoring that can now be performed with minimally invasive wearable patches and sensors, to develop an efficient and robust mechanism for accurate stress identification. A unique aspect of our research is personalized individual stress analysis including three stress levels: low, medium and high. Using machine learning algorithms from the ECG signals alone, we could achieve 88.24% accuracy in detecting the three classes of stress. We also find that high stress can be successfully detected for a person in comparison to his or her rest period with 100% accuracy.","keywords_author":["classification","driver monitoring","driving","ECG signals","machine learning","physiological sensors","precision medicine","stress medicine","time series"],"keywords_other":["ECG signals","Physiological sensors","Driver monitoring","Implantable devices","Robust mechanisms","driving","Minimally invasive","Internet of Things (IOT)"],"max_cite":4.0,"pub_year":2015.0,"sources":"['scp']","rawkeys":["stress medicine","robust mechanisms","implantable devices","machine learning","physiological sensors","driver monitoring","minimally invasive","ecg signals","classification","driving","precision medicine","internet of things (iot)","time series"],"tags":["stress medicine","robust mechanisms","implantable devices","machine learning","physiological sensors","driver monitoring","minimally invasive","ecg signals","classification","driving","precision medicine","internet of things (iot)","time series"]},{"p_id":50008,"title":"A novel pruning model of deep learning for large-scale distributed data processing","abstract":"\u00a9 2015 Asia-Pacific Signal and Information Processing Association. In this paper, we propose a novel pruning model of deep learning for large-scale distributed data processing to simulate a potential application in the geographical neighbor of Internet of Things. We formulate a general model of pruning learning, and we investigate the procedure of pruning learning to satisfy hard constraint and soft constraint. The hard constraint is a class of non-flexible setting without parameter learning to match the structure of distributed data. The soft constraint is a process of adaptive parameter learning to satisfy an inequality without any degradation of accuracy if the size of training data is large enough. Based on the simulation using distributed MNIST image database with large-scale samples, the performance of the proposed pruning model is better than that of a state-of-the-art model of deep learning in case of big data processing.","keywords_author":["big data","cloud computing","deep learning","distributed data","internet of things"],"keywords_other":["Deep learning","Hard constraints","Soft constraint","Distributed data","State of the art","Adaptive parameters","Distributed data processing","Parameter learning"],"max_cite":0.0,"pub_year":2016.0,"sources":"['scp']","rawkeys":["parameter learning","cloud computing","big data","adaptive parameters","deep learning","state of the art","internet of things","distributed data processing","soft constraint","hard constraints","distributed data"],"tags":["parameter learning","cloud computing","big data","adaptive parameters","state of the art","machine learning","distributed data processing","soft constraint","hard constraints","distributed data","internet of things (iot)"]},{"p_id":4953,"title":"Fog computing and its role in the internet of things","abstract":"Fog Computing extends the Cloud Computing paradigm to the edge of the network, thus enabling a new breed of applications and services. Defining characteristics of the Fog are: a) Low latency and location awareness; b) Wide-spread geographical distribution; c) Mobility; d) Very large number of nodes, e) Predominant role of wireless access, f) Strong presence of streaming and real time applications, g) Heterogeneity. In this paper we argue that the above characteristics make the Fog the appropriate platform for a number of critical Internet of Things (IoT) services and applications, namely, Connected Vehicle, Smart Grid, Smart Cities, and, in general, Wireless Sensors and Actuators Networks (WSANs). \u00a9 2012 ACM.","keywords_author":["analytics","cloud computing","fog computing","iot","real time systems","software defined networks","wsan"],"keywords_other":["Real-time application","Services and applications","Wireless sensor","Location awareness","Computing paradigm","Smart grid","Wireless access","iot","wsan","Internet of Things (IOT)","Low latency","analytics"],"max_cite":1215.0,"pub_year":2012.0,"sources":"['wos', 'scp', 'ieee']","rawkeys":["software defined networks","cloud computing","location awareness","smart grid","real time systems","fog computing","real-time application","services and applications","low latency","computing paradigm","wireless sensor","iot","wsan","wireless access","internet of things (iot)","analytics"],"tags":["location-aware","cloud computing","smart grid","services and applications","fog computing","real-time application","software-defined networking","low latency","real-time systems","wsans","computing paradigm","wireless sensor","wireless access","internet of things (iot)","analytics"]},{"p_id":23391,"title":"Fog computing based efficient IoT scheme for the Industry 4.0","abstract":"\u00a9 2017 IEEE. Industry 4.0 aims to dramatically enhance the productivity of manufacturing technologies through the collection and analysis of real-time data. This combines the ubiquity of the IoT with the processing capabilities of cloud computing to generate insights that help to optimize the decision making process. The increasing demand of data and the explosion in the number of sensing devices, which might be highly constrained in terms of communication, battery and computational power, introduce new challenges that need efficient IoT-Cloud architectures. With this in mind, we extend MQTT, the de facto IoT communication protocol, using a fog computing approach that introduces a low complexity computational layer between the Cloud and IoT nodes. In this approach, the MQTT broker, which is in charge of relaying data from publishers to subscribers, is placed at the fog layer. The purpose of introducing an intermediate layer to this particular scenario is to: i) predict future data measurements through prediction techniques; ii) operate as a gateway to upper layers; and iii) provide the capability to offload computationally expensive data processing jobs from the Cloud to the Fog, minimizing additional latency and operational expenses. With this architecture, the transmissions required from IoT devices may be reduced, since the publishers would only need to update the predicted data in case of mismatching. We validate our approach with an energy consumption analysis and simulations of different Machine Learning algorithms on a real dataset, and compare it with the traditional MQTT scheme.","keywords_author":["energy efficiency","fog computing","Industrial IoT","Industry 4.0","Internet of Things","IoT","Machine Learning","MQTT"],"keywords_other":["Processing capability","Cloud architectures","Prediction techniques","MQTT","Energy consumption analysis","Decision making process","Manufacturing technologies","Industrial IoT"],"max_cite":10.0,"pub_year":2017.0,"sources":"['scp']","rawkeys":["cloud architectures","mqtt","prediction techniques","fog computing","energy consumption analysis","processing capability","internet of things","machine learning","industrial iot","manufacturing technologies","iot","energy efficiency","industry 4.0","decision making process"],"tags":["cloud architectures","mqtt","prediction techniques","fog computing","energy consumption analysis","processing capability","machine learning","manufacturing technologies","energy efficiency","industry 4.0","internet of things (iot)","decision making process"]},{"p_id":27489,"title":"Autonomous sensor-context learning in dynamic human-centered internet-of-things environments","abstract":"\u00a9 2016 ACM. Human-centered Internet-of-Things (IoT) applications utilize computational algorithms such as machine learning and signal processing techniques to infer knowledge about important events such as physical activities and medical complications. The inference is typically based on data collected with wearable sensors or those embedded in the environment. A major obstacle in large-scale utilization of these systems is that the computational algorithms cannot be shared between users or reused in contexts different than the setting in which the training data are collected. For example, an activity recognition algorithm trained for a wrist-band sensor cannot be used on a smartphone worn on the waist. We propose an approach for automatic detection of physical sensor-contexts (e.g., on-body sensor location) without need for collecting new labeled training data. Our techniques enable system designers and end-users to share and reuse computational algorithms that are trained under different contexts and data collection settings. We develop a framework to autonomously identify sensor-context. We propose a gating function to automatically activate the most accurate computational algorithm among a set of shared expert models. Our analysis based on real data collected with human subjects while performing 12 physical activities demonstrate that the accuracy of our multi-view learning is only 7.9% less than the experimental upper bound for activity recognition using a dynamic sensor constantly migrating from one on-body location to another. We also compare our approach with several mixture-of-experts models and transfer learning techniques and demonstrate that our approach outperforms algorithms in both categories.","keywords_author":null,"keywords_other":["Activity recognition","Computational algorithm","Medical complications","Signal processing technique","Labeled training data","Mixture-of-experts model","Internet of Things (IOT)","Automatic Detection"],"max_cite":6.0,"pub_year":2016.0,"sources":"['scp']","rawkeys":["activity recognition","labeled training data","signal processing technique","medical complications","mixture-of-experts model","automatic detection","computational algorithm","internet of things (iot)"],"tags":["mixture of experts model","activity recognition","labeled training data","signal processing technique","medical complications","automatic detection","computational algorithm","internet of things (iot)"]},{"p_id":865,"title":"A Survey of Deep Learning: Platforms, Applications and Emerging Research Trends","abstract":"Deep learning has exploded in the public consciousness, primarily as predictive and analytical products suffuse our world, in the form of numerous human-centered smart-world systems, including targeted advertisements, natural language assistants and interpreters, and prototype self-driving vehicle systems. Yet to most, the underlying mechanisms that enable such human-centered smart products remain obscure. In contrast, researchers across disciplines have been incorporating deep learning into their research to solve problems that could not have been approached before. In this paper, we seek to provide a thorough investigation of deep learning in its applications and mechanisms. Specifically, as a categorical collection of state of the art in deep learning research, we hope to provide a broad reference for those seeking a primer on deep learning and its various implementations, platforms, algorithms, and uses in a variety of smart-world systems. Furthermore, we hope to outline recent key advancements in the technology, and provide insight into areas, in which deep learning can improve investigation, as well as highlight new areas of research that have yet to see the application of deep learning, but could nonetheless benefit immensely. We hope this survey provides a valuable reference for new deep learning practitioners, as well as those seeking to innovate in the application of deep learning.","keywords_author":["cyber-physical systems","deep learning","emergent applications","Human-centered smart systems","Internet of Things","networking","neural networks","platform","security","survey","Human-centered smart systems","deep learning","platform","neural networks","emergent applications","Internet of Things","cyber-physical systems","survey","networking","security","Human-centered smart systems","deep learning","platform","neural networks","emergent applications","Internet of Things","cyber-physical systems","survey","networking","security"],"keywords_other":["GO","ALGORITHM","Neural networks","GAME","SENSOR DATA","Computational modeling","Smart System","SYSTEMS","Machine learning","Networking and Security","INTERNET","THINGS","human-centered smart-world systems","learning (artificial intelligence)","Training","Learning systems","RECOGNITION","Task analysis","Neurons","Computational model","deep learning research","CLASSIFICATION","NEURAL-NETWORKS","Platform"],"max_cite":0.0,"pub_year":2018.0,"sources":"['scp', 'wos', 'ieee']","rawkeys":["task analysis","smart system","networking","internet of things","classification","learning systems","internet","neural-networks","game","sensor data","survey","things","machine learning","neurons","computational modeling","algorithm","human-centered smart-world systems","learning (artificial intelligence)","recognition","emergent applications","deep learning","neural networks","training","human-centered smart systems","security","platform","computational model","cyber-physical systems","networking and security","go","deep learning research","systems"],"tags":["task analysis","smart system","emerging applications","classification","learning systems","internet","internet of things (iot)","sensor data","survey","things","machine learning","system","algorithms","neurons","computational modeling","human-centered smart-world systems","recognition","neural networks","training","human-centered smart systems","security","platform","networks","games","cyber-physical systems","networking and security","go","deep learning research"]},{"p_id":37734,"title":"Enrichment of machine learning based activity classification in smart homes using ensemble learning","abstract":"Copyright \u00a9 2016 ACM. Data streams from various Internet-Of-Things (IOT) enabled sensors in smart homes provide an opportunity to develop predictive models to offer actionable insights in form of preventive care to its residence. This becomes particularly relevant for Aging-In-Place (AIP) solutions for the care of the elderly. Over the last decade, diverse stakeholders from practice, industry, education, research, and professional organizations have collaborated to furnish homes with a variety of IOT enabled sensors to record daily activities of individuals. Machine Learning on such streams allows for detection of patterns and prediction of activities which enables preventive care. Behavior patterns that lead to preventive care constitute a series of activities. Accurate labeling of activities is an extremely time-consuming process and the resulting labels are often noisy and error prone. In this paper, we analyze the classification accuracy of various activities within a home using machine learning models. We present that the use of an ensemble model that combines multiple learning models allows to obtain better classification of activities than any of the constituent learning algorithms.","keywords_author":["Aging in place (AIP)","Ensemble learning","Internet of things (IOT)","Machine learning","Smart homes"],"keywords_other":["Smart homes","Professional organization","Activity classifications","Ensemble learning","Machine learning models","Aging in place","Internet of Things (IOT)","Classification of activity"],"max_cite":1.0,"pub_year":2016.0,"sources":"['scp']","rawkeys":["aging in place (aip)","smart homes","machine learning models","machine learning","professional organization","classification of activity","ensemble learning","aging in place","activity classifications","internet of things (iot)"],"tags":["smart homes","machine learning models","machine learning","professional organization","classification of activity","ensemble learning","aging in place","activity classifications","internet of things (iot)"]},{"p_id":9076,"title":"Cloud of Things: Integrating Internet of Things and cloud computing and the issues involved","abstract":"With the trend going on in ubiquitous computing, everything is going to be connected to the Internet and its data will be used for various progressive purposes, creating not only information from it, but also, knowledge and even wisdom. Internet of Things (IoT) becoming so pervasive that it is becoming important to integrate it with cloud computing because of the amount of data IoT's could generate and their requirement to have the privilege of virtual resources utilization and storage capacity, but also, to make it possible to create more usefulness from the data generated by IoT's and develop smart applications for the users. This IoT and cloud computing integration is referred to as Cloud of Things in this paper. IoT's and cloud computing integration is not that simple and bears some key issues. Those key issues along with their respective potential solutions have been highlighted in this paper. \u00a9 2014 IEEE.","keywords_author":["cloud computing","CoT","IoT","IoT and CoT issues"],"keywords_other":["Smart applications","IoT","Virtual resource","Key Issues","IoT and CoT issues","Storage capacity","Internet of Things (IOT)","CoT"],"max_cite":81.0,"pub_year":2014.0,"sources":"['scp']","rawkeys":["cot","cloud computing","iot and cot issues","virtual resource","iot","key issues","storage capacity","internet of things (iot)","smart applications"],"tags":["cot","cloud computing","iot and cot issues","key issues","storage capacity","virtualized resources","internet of things (iot)","smart applications"]},{"p_id":33653,"title":"An IoT system to estimate personal thermal comfort","abstract":"\u00a9 2016 IEEE. Thermal comfort in office buildings is emerging as an important variable that can be used to maximize employee productivity. In this paper we propose a new Internet of Things (IoT) based system that creates a personalized model of thermal comfort. To create this model, our system collects telemetry via an IoT network of sensors and user inputs. This data is then input into machine learning algorithms that continuously calibrate and update a personalized thermal comfort model for the user. To facilitate the individuality of our models, the system combines personal measurements from the Microsoft Band, such as biometric readings and user feedback, with environmental measurements such as temperature, humidity, and air speed. In this work, we evaluate a broad set of classification and regression algorithms. Our experimental results show that using our IoT based system improves the mean squared error of the thermal prediction by about 50% when compared to the industry standard method developed by P.O. Fanger.","keywords_author":["IoT","Machine Learning","Thermal Comfort","Wearable Devices"],"keywords_other":["Industry standards","Environmental measurements","Personalized model","Regression algorithms","Employee productivity","Internet of Things (IOT)","Thermal comfort models","Wearable devices"],"max_cite":2.0,"pub_year":2017.0,"sources":"['scp']","rawkeys":["personalized model","wearable devices","thermal comfort models","employee productivity","machine learning","industry standards","thermal comfort","iot","environmental measurements","regression algorithms","internet of things (iot)"],"tags":["personalized model","wearable devices","thermal comfort models","employee productivity","machine learning","industry standards","thermal comfort","environmental measurements","regression algorithms","internet of things (iot)"]},{"p_id":35707,"title":"A framework of scalable QoE modeling for application explosion in the Internet of Things","abstract":"\u00a9 2016 IEEE. The Internet of Things (IoT) opens a new horizon for network applications by virtue of wireless sensor network technologies and advancement of data analytics. Since various applications are possible in the IoT compared to conventional multimedia applications, quality of experience (QoE), which is a user's subjective satisfaction with the application, becomes more complicated for network operators to perceive. In this paper, we propose a framework of scalable QoE modeling for explosively increasing applications. In the framework, the massive amount of quality metrics in the IoT architecture are defined as physical metrics and organized into four layers; device, network, computing, and user interface. We also introduce metaphysical metrics, which are the summarized quality metrics that users of IoT applications demand. By aggregating the massive amount of physical metrics into certain metaphysical metrics and qualitatively modeling the QoE by the metaphysical metrics according to the characteristics of the applications, scalable modeling of the QoE for continuously emerging applications in the IoT becomes possible.","keywords_author":["quality of experience (QoE)","scalable modeling"],"keywords_other":["Internet of thing (IOT)","Network operator","Network applications","Iot architectures","Emerging applications","Quality of experience (QoE)","Multimedia applications","Scalable model"],"max_cite":2.0,"pub_year":2017.0,"sources":"['scp']","rawkeys":["iot architectures","multimedia applications","internet of thing (iot)","scalable model","quality of experience (qoe)","emerging applications","network operator","scalable modeling","network applications"],"tags":["iot architectures","multimedia applications","scalable model","quality of experience (qoe)","emerging applications","network operator","network applications","internet of things (iot)"]},{"p_id":45964,"title":"Impact of machine learning and internet of things in agriculture: State of the art","abstract":"\u00a9 Springer International Publishing AG 2018. Majority of the population are directly or indirectly dependent on agriculture. In this modern world, agriculture has to be supported with technology to bring the best output. There is a big revolution in agriculture from traditional methods. Recent development in technology has a great impact on agriculture. Evolution of Machine Learning (ML) and Internet of Things (IoT) has helped researchers to apply these techniques in agriculture to help farmers. This in turn helped farmers to increase the productivity, make use of maximum land available, control pest, and so on. This paper highlights the work done in agriculture sector using ML and IoT.","keywords_author":["Agriculture","ANN","IoT","Machine learning","SVM"],"keywords_other":["State of the art","Internet of Things (IOT)","Impact on agriculture","Agriculture sectors"],"max_cite":0.0,"pub_year":2018.0,"sources":"['scp']","rawkeys":["impact on agriculture","agriculture sectors","state of the art","machine learning","agriculture","svm","iot","ann","internet of things (iot)"],"tags":["agriculture sectors","impact on agriculture","neural networks","state of the art","machine learning","agriculture","internet of things (iot)"]},{"p_id":48015,"title":"Fast text classification with Naive Bayes method on Apache Spark Naive Bayes Y\u00f6ntemi ile Apache Spark \u00dczerinde Hizli Metin Siniflandirma","abstract":"\u00a9 2017 IEEE. The increase in the number of devices and users online with the transition of Internet of Things (IoT), increases the amount of large data exponentially. Classification of ascending data, deletion of irrelevant data, and meaning extraction have reached vital importance in today's standards. Analysis can be done in various variations such as Classification of text on text data, analysis of spam, personality analysis. In this study, fast text classification was performed with machine learning on Apache Spark using the Naive Bayes method. Spark architecture uses a distributed in-memory data collection instead of a distributed data structure presented in Hadoop architecture to provide fast storage and analysis of data. Analyzes were made on the interpretation data of the Reddit which is open source social news site by using the Naive Bayes method. The results are presented in tables and graphs.","keywords_author":["Apache Spark","Big data","Classification","Machine learning","Naive Bayes","Text mining"],"keywords_other":["Text mining","Naive bayes","Distributed data structures","Text classification","Data collection","Analysis of data","Open sources","Internet of Things (IOT)"],"max_cite":0.0,"pub_year":2017.0,"sources":"['scp']","rawkeys":["apache spark","text mining","big data","machine learning","open sources","distributed data structures","analysis of data","classification","data collection","naive bayes","text classification","internet of things (iot)"],"tags":["apache spark","text mining","big data","machine learning","open sources","distributed data structures","analysis of data","classification","data collection","naive bayes","text classification","internet of things (iot)"]},{"p_id":93075,"title":"Accurate Location Tracking From CSI-Based Passive Device-Free Probabilistic Fingerprinting","abstract":"The research on indoor localization has received great interest in recent years. This has been fueled by the ubiquitous distribution of electronic devices equipped with a radio frequency (RF) interface. Analyzing the signal fluctuation on the RF-interface can, for instance, solve the still open issue of ubiquitous reliable indoor localization and tracking. Device bound and device free approaches with remarkable accuracy have been reported recently. In this paper, we present an accurate device-free passive (DfP) indoor location tracking system that adopts channel state information (CSI) readings from off-the-shelf WiFi 802.11n wireless cards. The fine-grained subchannel measurements for multiple input multiple output orthogonal frequency-division multiplexing PHY layer parameters are exploited to improve localization and tracking accuracy. To enable precise positioning in the presence of heavy multipath effects in cluttered indoor scenarios, we experimentally validate the unpredictability of CSI measurements and suggest a probabilistic fingerprint-based technique as an accurate solution. Our scheme further boosts the localization efficiency by using principal component analysis to filter the most relevant feature vectors. Furthermore, with Bayesian filtering, we continuously track the trajectory of a moving subject. We have evaluated the performance of our system in four indoor environments and compared it with state-of-the-art indoor localization schemes. Our experimental results demonstrate that this complex channel information enables more accurate localization of nonequipped individuals.","keywords_author":["Pervasive computing","indoor navigation","Internet of Things"],"keywords_other":["WIFI","SYSTEM","INDOOR LOCALIZATION"],"max_cite":0.0,"pub_year":2018.0,"sources":"['wos']","rawkeys":["wifi","internet of things","system","indoor navigation","indoor localization","pervasive computing"],"tags":["wifi","system","internet of things (iot)","in-door navigations","indoor localization","pervasive computing"]},{"p_id":45974,"title":"A Real-Time Data Mining Approach for Interaction Analytics Assessment: IoT Based Student Interaction Framework","abstract":"\u00a9 2017 Springer Science+Business Media, LLC, part of Springer Nature Students\u2019 interaction and collaboration with the fellows and teachers using the Internet of Things (IoT) based interoperable infrastructure is a convenient way. Measuring student attention is an essential part of the educational assessment for students\u2019 interaction. As new learning styles develop, new tools and assessment methods are also needed. The focus in this paper is to develop IoT based interaction framework and analysis of the student experience in electronic learning (eLearning) so that the students can take full advantage of the modern interaction technology and their learning can increase to a high level. This setup has a data collection module, which is implemented using Visual C# programming language and computer vision library. The number of faces, number of eyes, and status of eyes are extracted from the video stream, which is taken from a video camera. The extracted information is saved in a dataset for further analysis. The analysis of the dataset produces interesting results for student learning assessments. Modern learning management systems can integrate the developed tool to consider student-learning behaviors when assessing electronic learning strategies. The tools are also developed for the data collection on both student and teacher ends. Correlation of data and hidden meaning are extracted to make the learning experience and teaching performance better and adaptable. IoT based infrastructure provides the facilities to fellow students about location awareness, fellows\u2019 accessibility, social behavior and helping hand.","keywords_author":["Internet of Things (IoT)","IoT interoperable services","Parallel implementation","Parallel machine learning","Visual attention"],"keywords_other":["Interoperable services","Internet of Things (IOT)","Parallel machine","Parallel implementations","Visual Attention"],"max_cite":0.0,"pub_year":2017.0,"sources":"['scp']","rawkeys":["iot interoperable services","interoperable services","parallel implementations","parallel implementation","visual attention","parallel machine","parallel machine learning","internet of things (iot)"],"tags":["iot interoperable services","interoperable services","parallel implementations","visual attention","parallel machine","parallel machine learning","internet of things (iot)"]},{"p_id":922,"title":"A greedy deep learning method for medical disease analysis","abstract":"This paper proposes a new deep learning method, the greedy deep weighted dictionary learning for mobile multimedia for medical diseases analysis. Based on the traditional dictionary learning methods, which neglects the relationship between the sample and the dictionary atom, we propose the weighted mechanism to connect the sample with the dictionary atom in this paper. Meanwhile, the traditional dictionary learning method is prone to cause over-fitting for patient classification of the limited training data set. Therefore, this paper adopts l 2 -norm regularization constraint, which realizes the limitation of the model space, and enhances the generalization ability of the model and avoids over-fitting to some extent. Compared with the previous shallow dictionary learning, this paper proposed the greedy deep dictionary learning. We adopt the thinking of layer by layer training to increase the hidden layer, so that the local information between the layer and the layer can be trained to maintain their own characteristics, reduce the risk of overfitting and make sure that each layer of the network is convergent, which improves the accuracy of training and learning. With the development of Internet of Things and the soundness of healthcare monitoring system, the method proposed have better reliability in the field of mobile multimedia for healthcare. The results show that the learning method has a good effect on the classification of mobile multimedia for medical diseases, and the accuracy, sensitivity, and specificity of the classification have good performance, which may provide guidance for the diagnosis of disease in wisdom medical.","keywords_author":["Deep learning","Dictionary learning","Machine learning","Medical big data","Mobile multimedia","Patient classification","Medical big data","machine learning","mobile multimedia","deep learning","dictionary learning","patient classification","Medical big data","machine learning","mobile multimedia","deep learning","dictionary learning","patient classification"],"keywords_other":["health care","greedy deep weighted dictionary","ALZHEIMERS-DISEASE","dictionary atom","Medical diagnostic imaging","disease diagnosis","healthcare","mobile multimedia","medical diagnostic computing","FEATURE-SELECTION","Dictionaries","mobile computing","medical diseases analysis","Machine learning","l2-norm regularization constraint","greedy deep dictionary learning","traditional dictionary learning method","learning (artificial intelligence)","Training","greedy algorithms","Big Data","multimedia systems","medical disease analysis","RECOGNITION","DICTIONARY","Internet of Things","Generalization ability","pattern classification","Mobile multimedia","Healthcare monitoring systems","Dictionary learning","Medical services","shallow dictionary learning","patient classification","CLASSIFICATION","Sensitivity and specificity","SPARSE REPRESENTATION","greedy deep learning method","Internet of Things (IOT)","diseases"],"max_cite":0.0,"pub_year":2018.0,"sources":"['scp', 'wos', 'ieee']","rawkeys":["health care","greedy deep weighted dictionary","big data","healthcare monitoring systems","internet of things","feature-selection","dictionary","classification","medical services","internet of things (iot)","dictionary atom","disease diagnosis","healthcare","medical diagnostic computing","mobile multimedia","machine learning","medical diagnostic imaging","medical big data","medical diseases analysis","mobile computing","l2-norm regularization constraint","greedy deep dictionary learning","sensitivity and specificity","traditional dictionary learning method","learning (artificial intelligence)","recognition","deep learning","training","greedy algorithms","sparse representation","multimedia systems","medical disease analysis","dictionary learning","pattern classification","dictionaries","generalization ability","alzheimers-disease","shallow dictionary learning","patient classification","greedy deep learning method","diseases"],"tags":["health care","greedy deep weighted dictionary","big data","healthcare monitoring systems","classification","medical services","dictionary atoms","internet of things (iot)","healthcare","medical diagnostic computing","mobile multimedia","machine learning","medical diagnostic imaging","medical big data","mobile computing","feature selection","greedy deep learning method","l2-norm regularization constraint","greedy deep dictionary learning","sensitivity and specificity","traditional dictionary learning method","recognition","training","disease","greedy algorithms","sparse representation","multimedia systems","medical disease analysis","dictionary learning","pattern classification","dictionaries","generalization ability","alzheimers-disease","shallow dictionary learning","patient classification","disease diagnosis"]},{"p_id":7073,"title":"Design principles for industrie 4.0 scenarios","abstract":"\u00a9 2016 IEEE. The increasing integration of the Internet of Everything into the industrial value chain has built the foundation for the next industrial revolution called Industrie 4.0. Although Industrie 4.0 is currently a top priority for many companies, research centers, and universities, a generally accepted understanding of the term does not exist. As a result, discussing the topic on an academic level is difficult, and so is implementing Industrie 4.0 scenarios. Based on a quantitative text analysis and a qualitative literature review, the paper identifies design principles of Industrie 4.0. Taking into account these principles, academics may be enabled to further investigate on the topic, while practitioners may find assistance in identifying appropriate scenarios. A case study illustrates how the identified design principles support practitioners in identifying Industrie 4.0 scenarios.","keywords_author":["Case Study","Design Principles","Industrie 4.0","Industry 4.0","Internet of Everything","Internet of Things","Smart Factory"],"keywords_other":["Industrial value chains","Text analysis","Literature reviews","Industrie 4.0","Research center","Academic level","Design Principles","Industrial revolutions"],"max_cite":170.0,"pub_year":2016.0,"sources":"['scp']","rawkeys":["internet of everything","industrial value chains","case study","design principles","internet of things","literature reviews","smart factory","research center","academic level","text analysis","industrie 4.0","industrial revolutions","industry 4.0"],"tags":["industrial value chains","design principles","case studies","literature reviews","smart factory","research center","academic level","text analysis","industrial revolutions","industry 4.0","internet of things (iot)"]},{"p_id":31656,"title":"Machine learning methods for big spectrum data processing","abstract":"\u00a9, 2015, Journal of Data Acquisition and Processing. All right reserved.With the rapid development of the mobile Internet and the Internet of Things, the number of personal wireless devices has grown exponentially, resulting in the increase of massive spectrum data. Therefore, the big spectrum data are literally formed. Meanwhile, the spectrum deficit is also increasingly precarious. Effective big spectrum data processing is significant in improving the spectrum utilization. Firstly, from a perspective of wireless communication, a definition of big spectrum data is presented and its characteristics are also analyzed. Then, promising machine learning methods to analyze and utilize the big spectrum data are summarized, such as, the distributed and parallel learning, extreme learning machine, kernel-based learning, deep learning, reinforcement learning, game learning, and transfer learning. Finally, several open issues and research trends are addressed.","keywords_author":["Big data","Big spectrum data","Data mining","Internet of Things","Machine learning","Wireless communication"],"keywords_other":null,"max_cite":3.0,"pub_year":2015.0,"sources":"['scp']","rawkeys":["data mining","big data","internet of things","big spectrum data","machine learning","wireless communication"],"tags":["data mining","big data","machine learning","big spectrum data","wireless communications","internet of things (iot)"]},{"p_id":946,"title":"In-Situ AI: Towards Autonomous and Incremental Deep Learning for IoT Systems","abstract":"Recent years have seen an exploration of data volumes from a myriad of IoT devices, such as various sensors and ubiquitous cameras. The deluge of IoT data creates enormous opportunities for us to explore the physical world, especially with the help of deep learning techniques. Traditionally, the Cloud is the option for deploying deep learning based applications. However, the challenges of Cloud-centric IoT systems are increasing due to significant data movement overhead, escalating energy needs, and privacy issues. Rather than constantly moving a tremendous amount of raw data to the Cloud, it would be beneficial to leverage the emerging powerful IoT devices to perform the inference task. Nevertheless, the statically trained model could not efficiently handle the dynamic data in the real in-situ environments, which leads to low accuracy. Moreover, the big raw IoT data challenges the traditional supervised training method in the Cloud. To tackle the above challenges, we propose In-situ AI, the first Autonomous and Incremental computing framework and architecture for deep learning based IoT applications. We equip deep learning based IoT system with autonomous IoT data diagnosis (minimize data movement), and incremental and unsupervised training method (tackle the big raw IoT data generated in ever-changing in-situ environments). To provide efficient architectural support for this new computing paradigm, we first characterize the two In-situ AI tasks (i.e. inference and diagnosis tasks) on two popular IoT devices (i.e. mobile GPU and FPGA) and explore the design space and tradeoffs. Based on the characterization results, we propose two working modes for the In-situ AI tasks, including Single-running and Co-running modes. Moreover, we craft analytical models for these two modes to guide the best configuration selection. We also develop a novel two-level weight shared In-situ AI architecture to efficiently deploy In-situ tasks to IoT node. Compared with traditional IoT systems, our In-situ AI can reduce data movement by 28-71%, which further yields 1.4X-3.3X speedup on model update and contributes to 30-70% energy saving.","keywords_author":["Architecture","Autonomous","Deep Learning","IoT","IoT","Deep Learning","Architecture","Autonomous"],"keywords_other":["In-situ AI architecture","IoT node","Computing paradigm","raw data","Cloud computing","popular IoT devices","in-situ environments","Architectural support","data movement overhead","incremental training method","Computer architecture","statically trained model","incremental deep learning","Unsupervised training","Supervised trainings","deep learning techniques","inference task","Machine learning","efficient architectural support","diagnosis tasks","data handling","Cloud-centric IoT systems","learning (artificial intelligence)","unsupervised training method","Training","Autonomous","Learning techniques","autonomous IoT data diagnosis","Internet of Things","Task analysis","diagnostic reasoning","supervised training method","Data models","traditional IoT systems","deep learning based applications","IOT applications","Incremental computing","In-situ tasks","data volumes","emerging powerful IoT devices","IoT applications","dynamic data","autonomous learning","big raw IoT data challenges"],"max_cite":1.0,"pub_year":2018.0,"sources":"['ieee', 'scp']","rawkeys":["task analysis","incremental computing","internet of things","raw data","in-situ environments","iot node","iot applications","data movement overhead","architecture","incremental training method","unsupervised training","statically trained model","traditional iot systems","incremental deep learning","machine learning","in-situ ai architecture","deep learning techniques","inference task","efficient architectural support","diagnosis tasks","learning techniques","data handling","computer architecture","emerging powerful iot devices","learning (artificial intelligence)","cloud-centric iot systems","unsupervised training method","deep learning","training","popular iot devices","iot","autonomous iot data diagnosis","diagnostic reasoning","supervised training method","deep learning based applications","cloud computing","autonomous","big raw iot data challenges","data volumes","in-situ tasks","supervised trainings","data models","computing paradigm","dynamic data","autonomous learning","architectural support"],"tags":["task analysis","incremental computing","inference tasks","raw data","in-situ environments","iot node","iot applications","data movement overhead","architecture","incremental training method","internet of things (iot)","unsupervised training","statically trained model","traditional iot systems","deep learning-based applications","incremental deep learning","machine learning","in-situ ai architecture","deep learning techniques","efficient architectural support","diagnosis tasks","learning techniques","data handling","computer architecture","emerging powerful iot devices","cloud-centric iot systems","unsupervised training method","training","popular iot devices","data volume","autonomous iot data diagnosis","diagnostic reasoning","supervised training method","cloud computing","autonomous","big raw iot data challenges","in-situ tasks","supervised trainings","data models","computing paradigm","dynamic data","autonomous learning","architectural support"]},{"p_id":46007,"title":"A robust features based person tracker for overhead views in industrial environment","abstract":"IEEE A top view camera having wide range lens installed overhead of the objects contributes greatly towards resolving the tracking problem and also maintains comprehensive visual access of the environment. Video analytics becoming more important to Internet of Things (IoT) applications including automatic people monitoring and surveillance systems. We followed an approach based on machine learning features based person tracking algorithm in industrial environment. The algorithm implements simple motion detection framework through motion blobs. The algorithm, rHOG uses the history of already imaged\/blobed population with the anticipated blob position of the person observed. We have compared our results, acquired through five varying test sequences, with established algorithms used for object tracking. The results highlight that our algorithm beats others tracking algorithms by greater margins. The accuracy depicted in our results shows 99% of accuracy compared to the last known best algorithm, the Mean Shift algorithm, yielding 48% accuracy in result. Furthermore, unlike other blob based tracking algorithms, our algorithm has additional property to discriminate any blob as a Person or No Person. Our proposed tracking algorithm has the additional advantage of detecting stationary person for a long time, handling occlusion, abrupt change in the environment and keeps performing the tracking by compensating for the gaps in data pertaining to all the frames.","keywords_author":["Algorithm design and analysis","Cameras","Clustering algorithms","Internet of Things","IoT applications","Lenses","Machine learning","Mathematical model","Person Tracking","Tracking","Video analytics","Video surveillance."],"keywords_other":["IOT applications","Person tracking","Algorithm design and analysis","Video analytics","Video surveillance"],"max_cite":0.0,"pub_year":2017.0,"sources":"['scp']","rawkeys":["mathematical model","person tracking","video surveillance","internet of things","machine learning","tracking","cameras","video analytics","lenses","algorithm design and analysis","clustering algorithms","iot applications"],"tags":["mathematical model","person tracking","video surveillance","machine learning","tracking","cameras","video analytics","lenses","algorithm design and analysis","clustering algorithms","iot applications","internet of things (iot)"]},{"p_id":17346,"title":"Secure integration of IoT and Cloud Computing","abstract":"\u00a9 2016 Elsevier B.V. Mobile Cloud Computing is a new technology which refers to an infrastructure where both data storage and data processing operate outside of the mobile device. Another recent technology is Internet of Things. Internet of Things is a new technology which is growing rapidly in the field of telecommunications. More specifically, IoT related with wireless telecommunications. The main goal of the interaction and cooperation between things and objects which sent through the wireless networks is to fulfill the objective set to them as a combined entity. In addition, there is a rapid development of both technologies, Cloud Computing and Internet of Things, regard the field of wireless communications. In this paper, we present a survey of IoT and Cloud Computing with a focus on the security issues of both technologies. Specifically, we combine the two aforementioned technologies (i.e Cloud Computing and IoT) in order to examine the common features, and in order to discover the benefits of their integration. Concluding, we present the contribution of Cloud Computing to the IoT technology. Thus, it shows how the Cloud Computing technology improves the function of the IoT. Finally, we survey the security challenges of the integration of IoT and Cloud Computing.","keywords_author":["Cloud Computing","Internet of Things","Mobile Cloud Computing","Privacy","Security"],"keywords_other":["Common features","Wireless telecommunications","Security issues","Wireless communications","Security","Security challenges","Cloud computing technologies","Data storage"],"max_cite":30.0,"pub_year":2018.0,"sources":"['scp']","rawkeys":["cloud computing","cloud computing technologies","privacy","security challenges","internet of things","security issues","security","wireless communications","mobile cloud computing","common features","data storage","wireless telecommunications"],"tags":["cloud computing","cloud computing technologies","privacy","security challenges","security issues","security","wireless communications","mobile cloud computing","common features","data storage","wireless telecommunications","internet of things (iot)"]},{"p_id":33733,"title":"Generalized activity recognition using accelerometer in wearable devices for IoT applications","abstract":"\u00a9 2016 IEEE. The proliferation of low power and low cost continuous sensing has generated an immense interest in the area of activity recognition. However, the real time detection is still a challenge for several reasons: requirement from the user to specify the type of activity, complex algorithms, and collection of data from multiple devices. In this paper, we describe a generalized activity recognition system, its applications, and the challenges involved in implementing the algorithm in resource-constrained devices. The distinctive aspects of our study include: 1) automatic detection and recognition of different activities (running, walking, crawling, climbing, and pronating), 2) using just one axis from an accelerometer sensor, and 3) simple features and pattern matching algorithm leading to computationally inexpensive and memory efficient system suitable for resource-constrained wearable devices. The activity recognition model was trained using data collected from 52 unique subjects. The model was mapped onto Intel\u00ae Quark\u2122 SE Pattern Matching Engine, and field-Tested using eight additional subjects achieving performance up to 91%.","keywords_author":["Activity Recognition","feature engineering","Internet of Things (IoT)","machine learning","pattern recognition","wearable sensors"],"keywords_other":["Pattern matching algorithms","Activity recognition","Real-time detection","Resourceconstrained devices","Feature engineerings","Accelerometer sensor","Internet of Things (IOT)","Automatic Detection"],"max_cite":2.0,"pub_year":2016.0,"sources":"['scp']","rawkeys":["activity recognition","wearable sensors","feature engineering","machine learning","pattern recognition","resourceconstrained devices","feature engineerings","internet of things (iot)","real-time detection","automatic detection","pattern matching algorithms","accelerometer sensor"],"tags":["activity recognition","wearable sensors","machine learning","pattern recognition","resourceconstrained devices","feature engineerings","real-time detection","automatic detection","pattern matching algorithms","internet of things (iot)","accelerometer sensor"]},{"p_id":965,"title":"End-to-End Learning from Spectrum Data: A Deep Learning Approach for Wireless Signal Identification in Spectrum Monitoring Applications","abstract":"This paper presents end-to-end learning from spectrum data-an umbrella term for new sophisticated wireless signal identification approaches in spectrum monitoring applications based on deep neural networks. End-to-end learning allows to: 1) automatically learn features directly from simple wireless signal representations, without requiring design of hand-crafted expert features like higher order cyclic moments and 2) train wireless signal classifiers in one end-to-end step which eliminates the need for complex multi-stage machine learning processing pipelines. The purpose of this paper is to present the conceptual framework of end-to-end learning for spectrum monitoring and systematically introduce a generic methodology to easily design and implement wireless signal classifiers. Furthermore, we investigate the importance of the choice of wireless data representation to various spectrum monitoring tasks. In particular, two case studies are elaborated: 1) modulation recognition and 2) wireless technology interference detection. For each case study three convolutional neural networks are evaluated for the following wireless signal representations: temporal IQ data, the amplitude\/phase representation, and the frequency domain representation. From our analysis, we prove that the wireless data representation impacts the accuracy depending on the specifics and similarities of the wireless signals that need to be differentiated, with different data representations resulting in accuracy variations of up to 29%. Experimental results show that using the amplitude\/phase representation for recognizing modulation formats can lead to performance improvements up to 2% and 12% for medium to high SNR compared to IQ and frequency domain data, respectively. For the task of detecting interference, frequency domain representation outperformed amplitude\/phase and IQ data representation up to 20%.","keywords_author":["Big spectrum data","convolutional neural networks","deep learning","end-to-end learning","IoT","spectrum monitoring","wireless signal identification","Big spectrum data","spectrum monitoring","end-to-end learning","deep learning","convolutional neural networks","wireless signal identification","IoT","Big spectrum data","spectrum monitoring","end-to-end learning","deep learning","convolutional neural networks","wireless signal identification","IoT"],"keywords_other":["wireless signal identification approaches","wireless signals","spectrum data","Pipelines","Wireless signals","spectrum monitoring tasks","radiofrequency interference","deep learning approach","Wireless communications","wireless technology interference detection","end-to-end step","modulation recognition","radio spectrum management","Monitoring","phase representation","COGNITIVE-RADIO","spectrum monitoring applications","Machine learning","Spectrum monitoring","wireless data representation","feedforward neural nets","INTERNET","THINGS","amplitude representation","learning (artificial intelligence)","convolutional neural networks","simple wireless signal representations","complex multistage machine learning processing pipelines","modulation","End to end","end-to-end learning","signal representation","Modulation","signal classification","telecommunication computing","DIRECTIONS","deep neural networks","wireless signal classifiers","Interference","frequency domain representation","NEURAL-NETWORKS","Wireless communication","Convolutional neural network","Wireless sensor networks","frequency-domain analysis","Big Spectrum data"],"max_cite":0.0,"pub_year":2018.0,"sources":"['scp', 'wos', 'ieee']","rawkeys":["pipelines","wireless signal identification","wireless signal identification approaches","wireless signals","spectrum data","radiofrequency interference","spectrum monitoring tasks","convolutional neural network","deep learning approach","wireless technology interference detection","wireless sensor networks","internet","end-to-end step","modulation recognition","monitoring","neural-networks","radio spectrum management","spectrum monitoring","wireless communication","things","end to end","machine learning","big spectrum data","spectrum monitoring applications","wireless communications","cognitive-radio","wireless data representation","feedforward neural nets","amplitude representation","learning (artificial intelligence)","convolutional neural networks","directions","deep learning","interference","complex multistage machine learning processing pipelines","modulation","simple wireless signal representations","end-to-end learning","signal representation","iot","signal classification","telecommunication computing","deep neural networks","wireless signal classifiers","frequency domain representation","frequency-domain analysis","phase representation"],"tags":["pipelines","wireless signal identification","wireless signal identification approaches","wireless signals","spectrum data","radiofrequency interference","spectrum monitoring tasks","convolutional neural network","frequency-domain representations","deep learning approach","wireless technology interference detection","wireless sensor networks","internet","end-to-end step","modulation recognition","internet of things (iot)","monitoring","radio spectrum management","spectrum monitoring","things","end to end","machine learning","big spectrum data","spectrum monitoring applications","wireless communications","wireless data representation","feedforward neural nets","amplitude representation","directions","interference","neural networks","complex multistage machine learning processing pipelines","cognitive radio","modulation","simple wireless signal representations","end-to-end learning","signal representation","signal classification","telecommunication computing","wireless signal classifiers","frequency-domain analysis","phase representation"]},{"p_id":33735,"title":"CEML: Mixing and moving complex event processing and machine learning to the edge of the network for IoT applications","abstract":"\u00a9 2016 ACM. The Internet of Things (IoT) is a growing field which is expected to generate and collect data everywhere at any time. Highly scalable cloud analytics systems are frequently being used to handle this data explosion. However, the ubiquitous nature of the IoT data imposes new technical and non-Technical requirements which are difficult to address with a cloud deployment. To solve these problems, we need a new set of development technologies such as Distributed Data Mining and Ubiquitous Data Mining targeted and optimized towards IoT applications. In this paper, we present the Complex Event Machine Learning framework which proposes a set of tools for automatic distributed machine learning in (near-) real-Time, automatic continuous evaluation tools, and automatic rules management for deployment of rules. These features are implemented for a deployment at the edge of the network instead of the cloud. We evaluate and validate our approach with a well-known classification problem.","keywords_author":["Complex Event Processing","Edge Computing","Internet of Things","Machine Learning","Stream Mining"],"keywords_other":["Stream mining","Distributed machine learning","Internet of thing (IOT)","Edge computing","Ubiquitous data minings","Development technology","Complex event processing","Distributed data mining"],"max_cite":2.0,"pub_year":2016.0,"sources":"['scp']","rawkeys":["ubiquitous data minings","distributed data mining","stream mining","internet of thing (iot)","internet of things","machine learning","complex event processing","edge computing","development technology","distributed machine learning"],"tags":["ubiquitous data minings","distributed data mining","stream mining","machine learning","complex event processing","edge computing","development technology","distributed machine learning","internet of things (iot)"]},{"p_id":50121,"title":"Smartphone based human activity and postural transition classification with deep stacked autoencoder networks","abstract":"\u00a9 Springer International Publishing Switzerland 2016. Human activity recognition (HAR) is a prominent research area attracting considerable interest in recent years. The use of Body Sensor Networks has enabled researchers to collect user data on which machine learning techniques can be employed for modelling and classification tasks. More recently, smartphones with inertial sensors allow the collection of user activity data. With environments becoming increasingly connected through the Internet of Things (IoT) and the development of smart buildings, data can be meticulously collected to identify patterns of human activity. Health care is an area of great interest focussing on activity based classification research for the promotion of wellbeing, with research centers engaging with individuals in a real world context to improve caregiving [1]. We have conducted experiments to understand how the application of machine learning, specifically Deep Stacked Autoencoder Networks (DSAN), along with traditional models including the Multi-layer Perceptron, Radial Basis Function Neural Network and Support Vector Machine for comparison, perform in presenting a solution to the HAR problem. The research analyses data collected from smartphones in order to classify between motion based activities including walking, running, and transitional actions such as moving from a sitting to standing position [2]. Results show that the DSAN outperforms traditional models with an increase in classification accuracy. A Deep Reinforcement learning approach is considered for future investigation into human behaviour recognition and prediction in smart environments in order to improve classification performance.","keywords_author":["Artificial neural networks","Deep learning","Human activity recognition","Machine learning"],"keywords_other":["Deep learning","Classification performance","Internet of thing (IOT)","Radial basis function neural networks","Reinforcement learning approach","Human activity recognition","Classification accuracy","Machine learning techniques"],"max_cite":0.0,"pub_year":2016.0,"sources":"['scp']","rawkeys":["classification performance","deep learning","reinforcement learning approach","classification accuracy","internet of thing (iot)","human activity recognition","machine learning","machine learning techniques","radial basis function neural networks","artificial neural networks"],"tags":["classification performance","neural networks","reinforcement learning approach","classification accuracy","machine learning techniques","human activity recognition","machine learning","radial basis function neural networks","internet of things (iot)"]},{"p_id":43981,"title":"Wind power generation fault diagnosis based on deep learning model in internet of things (IoT) with clusters","abstract":"\u00a9 2018 Springer Science+Business Media, LLC, part of Springer Nature With the rapid development of wind power capacity and sustained growth in total operation time, the maintenance of wind turbines is becoming increasingly prominent, so we urgently need to develop effective wind turbine fault diagnosis and prediction system. The main fault characteristics of wind turbines are summarized from two aspects of fault diagnosis and fault prediction. Aiming at the difficult problems of fault diagnosis, we analyze and summarize the research status of fault diagnosis methods based on vibration, electrical signal analysis and pattern recognition algorithm. At the same time, we point out the technical characteristics, limitations and future trends of various methods. Based on the characteristics of mechanical structure and electronic system degradation in wind turbines, we summarize the current research progress and propose a fault prediction method based on physical failure model and data driven model fusion. In this paper, we use the deep learning model in the framework of the internet of things to predict and diagnose the faults of wind power generation. The experimental results show that the algorithm proposed in this paper can predict the fault types and make reasonable diagnosis.","keywords_author":["Deep learning","Environment","Fault diagnosis","Internet of things","Model","Prediction model","Wind power generation"],"keywords_other":["Environment","Pattern recognition algorithms","Mechanical structures","Prediction model","Fault characteristics","Fault diagnosis method","Internet of Things (IOT)","Wind power capacity"],"max_cite":0.0,"pub_year":2018.0,"sources":"['scp']","rawkeys":["prediction model","model","deep learning","fault characteristics","mechanical structures","pattern recognition algorithms","internet of things","environment","wind power generation","wind power capacity","fault diagnosis method","fault diagnosis","internet of things (iot)"],"tags":["model","fault characteristics","mechanical structures","pattern recognition algorithms","predictive models","machine learning","environment","wind power generation","wind power capacity","fault diagnosis method","fault diagnosis","internet of things (iot)"]},{"p_id":23502,"title":"A deep learning network for recognizing fruit pathologic images based on flexible momentum","abstract":"\u00a9, 2014, Chinese Society of Agricultural Machinery. All right reserved.Agricultural internet of things (IOT) and sensor technology has been widely used in the informationalized and mechanized orchard. The research aimed at both constructing an automatic-assistant diagnosis and a real-time alerting for plant disease and insect pest. The purpose also covered to realize an unmanned pest-disease monitoring and to release some human interaction in making a diagnosis. A method for pathologic image recognition-diagnosis based on deep learning neural network was designed and an innovative method for updating free parameters of the network was proposed on the basis of analyzing the error propagation of the network, so-called the gradient descendent with flexible momentum. Then, computer recognizing pathologic images of fruit sphere was researched into systematically, where the apple was selected as a subject. Experiment result revealed the method manifested a recall rate at 98.4%. And in parallel with several well-known updating schemes based momentum, the proposal was able to obviously improve the accuracy of learning network with a flatter converging curve, at a cost of short converging time. The test upon the several popular benchmark data-sets also demonstrated it could perform an effective recognition on the image pattern.","keywords_author":["Deep learning network","Flexile momentum","Image recognition","Pathological image","Plant disease and insect pest"],"keywords_other":["Deep learning","Sensor technologies","Disease monitoring","Innovative method","Insect pest","Pathological images","Internet of Things (IOT)","Human interactions"],"max_cite":10.0,"pub_year":2015.0,"sources":"['scp', 'wos']","rawkeys":["deep learning","deep learning network","human interactions","sensor technologies","innovative method","flexile momentum","plant disease and insect pest","image recognition","pathological images","insect pest","disease monitoring","internet of things (iot)","pathological image"],"tags":["deep learning network","machine learning","human interactions","sensor technologies","innovative method","flexile momentum","plant disease and insect pest","image recognition","pathological images","insect pest","disease monitoring","internet of things (iot)"]},{"p_id":66515,"title":"High-order possibilistic c-means algorithms based on tensor decompositions for big data in IoT","abstract":"Internet of Things (IoT) connects the physical world and the cyber world to offer intelligent services by data mining for big data. Each big data sample typically involves a large number of attributes, posing a remarkable challenge on the high-order possibilistic c-means algorithm (HOPCM). Specially, HOPCM requires high-performance servers with a large-scale memory and a powerful computing unit, to cluster big samples, limiting its applicability in IoT systems with low-end devices such as portable computing units and embedded devises which have only limited memory space and computing power. In this paper, we propose two high-order possibilistic c-means algorithms based on the canonical polyadic decomposition (CP-HOPCM) and the tensor-train network (TT-HOPCM) for clustering big data. In detail, we use the canonical polyadic decomposition and the tensor-train network to compress the attributes of each big data sample. To evaluate the performance of our algorithms, we conduct the experiments on two representative big data datasets, i.e., NUS-WIDE-14 and SNAE2, by comparison with the conventional high order possibilistic c-means algorithm in terms of attributes reduction, execution time, memory usage and clustering accuracy. Results imply that CP-HOPCM and TT-HOPCM are potential for big data clustering in IoT systems with low-end devices since they can achieve a high compression rate for heterogeneous samples to save the memory space significantly without a significant clustering accuracy drop. (C) 2017 Published by Elsevier B.V.","keywords_author":["Big data","IoT","Possibilistic c-means clustering","Canonical polyadic decomposition","Tensor-train network"],"keywords_other":null,"max_cite":24.0,"pub_year":2018.0,"sources":"['wos']","rawkeys":["big data","tensor-train network","canonical polyadic decomposition","iot","possibilistic c-means clustering"],"tags":["big data","canonical polyadic decompositions","tensor-train network","internet of things (iot)","possibilistic c-means clustering"]},{"p_id":27614,"title":"A Survey on the Edge Computing for the Internet of Things","abstract":"\u00a9 2013 IEEE. The Internet of Things (IoT) now permeates our daily lives, providing important measurement and collection tools to inform our every decision. Millions of sensors and devices are continuously producing data and exchanging important messages via complex networks supporting machine-to-machine communications and monitoring and controlling critical smart-world infrastructures. As a strategy to mitigate the escalation in resource congestion, edge computing has emerged as a new paradigm to solve IoT and localized computing needs. Compared with the well-known cloud computing, edge computing will migrate data computation or storage to the network 'edge,' near the end users. Thus, a number of computation nodes distributed across the network can offload the computational stress away from the centralized data center, and can significantly reduce the latency in message exchange. In addition, the distributed structure can balance network traffic and avoid the traffic peaks in IoT networks, reducing the transmission latency between edge\/cloudlet servers and end users, as well as reducing response times for real-time IoT applications in comparison with traditional cloud services. Furthermore, by transferring computation and communication overhead from nodes with limited battery supply to nodes with significant power resources, the system can extend the lifetime of the individual nodes. In this paper, we conduct a comprehensive survey, analyzing how edge computing improves the performance of IoT networks. We categorize edge computing into different groups based on architecture, and study their performance by comparing network latency, bandwidth occupation, energy consumption, and overhead. In addition, we consider security issues in edge computing, evaluating the availability, integrity, and the confidentiality of security strategies of each group, and propose a framework for security evaluation of IoT networks with edge computing. Finally, we compare the performance of various IoT applications (smart city, smart grid, smart transportation, and so on) in edge computing and traditional cloud computing architectures.","keywords_author":["Edge computing","Internet of Things","survey"],"keywords_other":["Communication overheads","Internet of thing (IOT)","Edge computing","Security","Monitoring and controlling","Intelligent sensors","Cloud computing architectures","Distributed structures"],"max_cite":6.0,"pub_year":2017.0,"sources":"['scp', 'ieee']","rawkeys":["cloud computing architectures","intelligent sensors","survey","monitoring and controlling","internet of thing (iot)","internet of things","security","communication overheads","edge computing","distributed structures"],"tags":["intelligent sensors","survey","monitor and control","security","communication overheads","cloud computing architecture","edge computing","distributed structures","internet of things (iot)"]},{"p_id":27618,"title":"A source location protection protocol based on dynamic routing in WSNs for the Social Internet of Things","abstract":"\u00a9 2017 Elsevier B.V. With the development of the Internet of Things (IoT), a more humanity-related network called the Social Internet of Things (SIoT) is now evolving. WSNs are also part of the Social Internet of Things (SIoT), a new application of the Internet of Things (IoT). Considering the characteristics of sensor nodes, including limited resource, limited communication capability, and an uncontrollable environment, location privacy protection is a challenging problem for WSNs. In this paper, we propose a source location protection protocol based on dynamic routing to address the source location privacy problem. We introduce a dynamic routing scheme that aims at maximizing paths for data transmission. The proposed scheme first randomly chooses an initial node from the boundary of the network. Every package will travel a greedy route and a subsequent directed route before reaching the sink. Theoretical and experimental results show that our scheme can preserve source location privacy and defeat various privacy disclosure attacks (eavesdropping attack, hop-by-hop trace back attack, and direction-oriented attack) without affecting the network lifetime.","keywords_author":["Cyber attacks","Social internet of things","Source location privacy","Wireless sensor networks"],"keywords_other":["Limited communication","Source-location privacy","Internet of thing (IOT)","Eavesdropping attacks","Location privacy protection","Cyber-attacks","New applications","Privacy disclosures"],"max_cite":6.0,"pub_year":2018.0,"sources":"['scp']","rawkeys":["social internet of things","privacy disclosures","cyber-attacks","source location privacy","internet of thing (iot)","location privacy protection","new applications","source-location privacy","cyber attacks","limited communication","eavesdropping attacks","wireless sensor networks"],"tags":["social internet of things","privacy disclosures","cyber-attacks","location privacy protection","new applications","source-location privacy","limited communication","eavesdropping attacks","internet of things (iot)","wireless sensor networks"]},{"p_id":3052,"title":"DSCNN: Hardware-oriented optimization for Stochastic Computing based Deep Convolutional Neural Networks","abstract":"\u00a9 2016 IEEE. Deep Convolutional Neural Networks (DCNN), a branch of Deep Neural Networks which use the deep graph with multiple processing layers, enables the convolutional model to finely abstract the high-level features behind an image. Large-scale applications using DCNN mainly operate in high-performance server clusters, GPUs or FPGA clusters; it is restricted to extend the applications onto mobile\/wearable devices and Internet-of-Things (IoT) entities due to high power\/energy consumption. Stochastic Computing is a promising method to overcome this shortcoming used in specific hardware-based systems. Many complex arithmetic operations can be implemented with very simple hardware logic in the SC framework, which alleviates the extensive computation complexity. The exploration of network-wise optimization and the revision of network structure with respect to stochastic computing based hardware design have not been discussed in previous work. In this paper, we investigate Deep Stochastic Convolutional Neural Network (DSCNN) for DCNN using stochastic computing. The essential calculation components using SC are designed and evaluated. We propose a joint optimization method to collaborate components guaranteeing a high calculation accuracy in each stage of the network. The structure of original DSCNN is revised to accommodate SC hardware design's simplicity. Experimental Results show that as opposed to software inspired feature extraction block in DSCNN, an optimized hardware oriented feature extraction block achieves as higher as 59.27% calculation precision. And the optimized DSCNN can achieve only 3.48% network test error rate compared to 27.83% for baseline DSCNN using software inspired feature extraction block.","keywords_author":["Deep Convolutional Neural Networks","Deep Learning","Hardware-oriented Co-optimization","Stochastic Computing"],"keywords_other":["Co-optimization","Deep learning","Stochastic computing","Calculation precision","Large-scale applications","Computation complexity","Convolutional neural network","Internet of Things (IOT)"],"max_cite":15.0,"pub_year":2016.0,"sources":"['scp', 'ieee']","rawkeys":["calculation precision","deep learning","large-scale applications","hardware-oriented co-optimization","deep convolutional neural networks","computation complexity","co-optimization","convolutional neural network","stochastic computing","internet of things (iot)"],"tags":["computational complexity","calculation precision","large-scale applications","hardware-oriented co-optimization","machine learning","co-optimization","convolutional neural network","stochastic computing","internet of things (iot)"]},{"p_id":21491,"title":"An IoT Endpoint System-on-Chip for Secure and Energy-Efficient Near-Sensor Analytics","abstract":"\u00a9 2004-2012 IEEE. Near-sensor data analytics is a promising direction for internet-of-things endpoints, as it minimizes energy spent on communication and reduces network load - but it also poses security concerns, as valuable data are stored or sent over the network at various stages of the analytics pipeline. Using encryption to protect sensitive data at the boundary of the on-chip analytics engine is a way to address data security issues. To cope with the combined workload of analytics and encryption in a tight power envelope, we propose Fulmine, a system-on-chip (SoC) based on a tightly-coupled multi-core cluster augmented with specialized blocks for compute-intensive data processing and encryption functions, supporting software programmability for regular computing tasks. The Fulmine SoC, fabricated in 65-nm technology, consumes less than 20mW on average at 0.8V achieving an efficiency of up to 70pJ\/B in encryption, 50pJ\/px in convolution, or up to 25MIPS\/mW in software. As a strong argument for real-life flexible application of our platform, we show experimental results for three secure analytics use cases: secure autonomous aerial surveillance with a state-of-the-art deep convolutional neural network (CNN) consuming 3.16pJ per equivalent reduced instruction set computer operation, local CNN-based face detection with secured remote recognition in 5.74pJ\/op, and seizure detection with encrypted data collection from electroencephalogram within 12.7pJ\/op.","keywords_author":["approximate computing","Computer architecture","encryption","feature extraction","Internet of Things","low-power electronics","neural networks","parallel architectures"],"keywords_other":["Reduced instruction set computers","Seizure detection","Aerial surveillance","Flexible applications","Encryption function","Convolutional neural network","System on chips (SoC)","Multi-core cluster"],"max_cite":14.0,"pub_year":2017.0,"sources":"['ieee', 'scp', 'wos']","rawkeys":["seizure detection","reduced instruction set computers","neural networks","system on chips (soc)","flexible applications","internet of things","multi-core cluster","encryption function","parallel architectures","aerial surveillance","convolutional neural network","approximate computing","feature extraction","encryption","computer architecture","low-power electronics"],"tags":["seizure detection","reduced instruction set computers","neural networks","flexible applications","multi-core cluster","system-on-chip","encryption function","parallel architectures","aerial surveillance","convolutional neural network","approximate computing","feature extraction","encryption","computer architecture","internet of things (iot)","low-power electronics"]},{"p_id":25592,"title":"DIANNE: Distributed artificial neural networks for the internet of things","abstract":"\u00a9 2015 ACM.Nowadays artificial neural networks are widely used to accurately classify and recognize patterns. An interesting application area is the Internet of Things (IoT), where physical things are connected to the Internet, and generate a huge amount of sensor data that can be used for a myriad of new, pervasive applications. Neural networks' ability to comprehend unstructured data make them a useful building block for such IoT applications. As neural networks require a lot of processing power, especially during the training phase, these are most often deployed in a cloud environment, or on specialized servers with dedicated GPU hardware. However, for IoT applications, sending all raw data to a remote back-end might not be feasible, taking into account the high and variable latency to the cloud, or could lead to issues concerning privacy. In this paper the DIANNE middleware framework is presented that is optimized for single sample feed-forward execution and facilitates distributing artificial neural networks across multiple IoT devices. The modular approach enables executing neural network components on a large number of heterogeneous devices, allowing us to exploit the local compute power at hand, and mitigating the need for a large server-side infrastructure at runtime.","keywords_author":["Distributed artificial neural networks","Internet of things","Middleware"],"keywords_other":["Internet of thing (IOT)","Cloud environments","Middleware frameworks","Variable latencies","Heterogeneous devices","Pervasive applications","Unstructured data","Processing power"],"max_cite":8.0,"pub_year":2015.0,"sources":"['scp']","rawkeys":["middleware frameworks","pervasive applications","heterogeneous devices","processing power","middleware","internet of thing (iot)","internet of things","cloud environments","unstructured data","variable latencies","distributed artificial neural networks"],"tags":["middleware frameworks","pervasive applications","heterogeneous devices","processing power","middleware","cloud environments","distributed artificial neural networks","unstructured data","variable latencies","internet of things (iot)"]},{"p_id":15353,"title":"Cloud-Assisted IoT-Based SCADA Systems Security: A Review of the State of the Art and Future Challenges","abstract":"\u00a9 2013 IEEE. Industrial systems always prefer to reduce their operational expenses. To support such reductions, they need solutions that are capable of providing stability, fault tolerance, and flexibility. One such solution for industrial systems is cyber physical system (CPS) integration with the Internet of Things (IoT) utilizing cloud computing services. These CPSs can be considered as smart industrial systems, with their most prevalent applications in smart transportation, smart grids, smart medical and eHealthcare systems, and many more. These industrial CPSs mostly utilize supervisory control and data acquisition (SCADA) systems to control and monitor their critical infrastructure (CI). For example, WebSCADA is an application used for smart medical technologies, making improved patient monitoring and more timely decisions possible. The focus of the study presented in this paper is to highlight the security challenges that the industrial SCADA systems face in an IoT-cloud environment. Classical SCADA systems are already lacking in proper security measures; however, with the integration of complex new architectures for the future Internet based on the concepts of IoT, cloud computing, mobile wireless sensor networks, and so on, there are large issues at stakes in the security and deployment of these classical systems. Therefore, the integration of these future Internet concepts needs more research effort. This paper, along with highlighting the security challenges of these CI's, also provides the existing best practices and recommendations for improving and maintaining security. Finally, this paper briefly describes future research directions to secure these critical CPSs and help the research community in identifying the research gaps in this regard.","keywords_author":["APT","Industrial Control System","Internet of Things (IoT)","NIST","PRECYSE","SOA","Supervisory Control and Data Acquisition"],"keywords_other":["Supervisory control and data acquisition","NIST","PRECYSE","Industrial control systems","Internet of Things (IOT)"],"max_cite":47.0,"pub_year":2016.0,"sources":"['scp']","rawkeys":["industrial control system","industrial control systems","nist","supervisory control and data acquisition","precyse","apt","soa","internet of things (iot)"],"tags":["advanced persistent threat","industrial control systems","nist","scada","precyse","soa","internet of things (iot)"]},{"p_id":23550,"title":"Mobile sensing and network analytics for realizing smart automated systems towards health Internet of Things","abstract":"\u00a9 2015 IEEE. Internet of Things (IoT) provides an unprecedented opportunity to realize smart automated systems such as smart manufacturing, smart city and smart home in the past few years. Pervasive sensing and mobile technology deployed in large-scale IoT systems lead to the accumulation of big data. In particular, wearable biosensing accelerates human-centered computing for smart health management. However, limited work has been done to develop advanced IoT technologies for smart monitoring and control of heart health. There is an urgent need to develop a new IoT technology specific to the heart, namely Internet of Hearts (IOH) that will enable and assist (1) the acquisition of electrocardiogram (ECG) signals pertinent to space-time cardiac dynamics at anytime anywhere; (2) real-time management and compact representation of multi-sensor signals; (3) big data analytics in large-scale IoT contexts. This paper presents a new technology of Mobile and E-Network Smart Health (MESH), which is composed of 4 components as follows: 1) Mobile-based ECG sensing device; 2) Space-time representation of cardiac electrical activity; 3) Optimal model-based representation of ECG signals; 4) Dynamic network embedding for disease pattern recognition. Our preliminary experimental results demonstrated that network analytics is efficient and effective for smart health management in IoT contexts. The MESH technology shows strong potentials to provide an indispensable and enabling tool for realizing smart heart health and wellbeing for the population worldwide.","keywords_author":null,"keywords_other":["Real-time management","Smart manufacturing","Space-time representations","Cardiac electrical activity","Electrocardiogram signal","Internet of Things (IOT)","Compact representation","Human-centered computing"],"max_cite":10.0,"pub_year":2015.0,"sources":"['scp']","rawkeys":["human-centered computing","real-time management","compact representation","cardiac electrical activity","smart manufacturing","space-time representations","electrocardiogram signal","internet of things (iot)"],"tags":["human-centered computing","real-time management","compact representation","cardiac electrical activity","smart manufacturing","space-time representations","electrocardiogram signal","internet of things (iot)"]},{"p_id":46080,"title":"Vehicle traffic and flood monitoring with reroute system using Bayesian networks analysis","abstract":"\u00a9 2017 IEEE. Heavy vehicle traffic and flooded areas are problems experienced on roads because of unimproved road infrastructures and environmental deviations. These factors affect vehicle drivers negatively as they contribute to stress, health problems, and wastefulness of time. This study developed a system called ArRoad that monitors and analyzes vehicle traffic and flooded areas using network of sensors and real-time image processing which then predicts and visualizes possible alternative rerouting paths using machine learning. Water level sensor nodes are used to monitor the flooded areas while real-time video images from cameras are processed to extract the vehicle volume on the streets. A Bayesian Network is generated from the water level sensors and image processing data which provides possible reroute areas to avoid traffic congestion and flooded areas. All data are sent to a cloud platform through the Internet that can be accessed through a mobile user interface. This mobile user application provides information about the condition of the streets and possible reroute maps to users. The accuracy of the system is tested by actual implementation on a specific road. Results showed minimum accessing delay from using the ArRoad to navigate in rerouted paths to prevent impassable roads due to heavy traffic and flood. If effect, it lessens the amount of time experienced by drivers from heavy traffic condition and flooded streets which then improves the quality of life by preventing waste of resources such as time and money.","keywords_author":["Bayesian Network","Image processing","Internet of Things","Machine learning","Sensors","Vehicle traffic"],"keywords_other":["Road infrastructures","Vehicle traffic","Heavy vehicle traffic","Water level sensors","Mobile user applications","Heavy traffic conditions","Mobile user interface","Real-time image processing"],"max_cite":0.0,"pub_year":2017.0,"sources":"['scp']","rawkeys":["road infrastructures","mobile user applications","mobile user interface","image processing","sensors","internet of things","machine learning","vehicle traffic","heavy vehicle traffic","bayesian network","heavy traffic conditions","real-time image processing","water level sensors"],"tags":["road infrastructures","mobile user applications","mobile user interface","image processing","sensors","machine learning","vehicle traffic","heavy vehicle traffic","bayesian networks","heavy traffic conditions","real-time image processing","water level sensors","internet of things (iot)"]},{"p_id":25612,"title":"Detecting crypto-ransomware in IoT networks based on energy consumption footprint","abstract":"\u00a9 2017 The Author(s) An Internet of Things (IoT) architecture generally consists of a wide range of Internet-connected devices or things such as Android devices, and devices that have more computational capabilities (e.g., storage capacities) are likely to be targeted by ransomware authors. In this paper, we present a machine learning based approach to detect ransomware attacks by monitoring power consumption of Android devices. Specifically, our proposed method monitors the energy consumption patterns of different processes to classify ransomware from non-malicious applications. We then demonstrate that our proposed approach outperforms K-Nearest Neighbors, Neural Networks, Support Vector Machine and Random Forest, in terms of accuracy rate, recall rate, precision rate and F-measure.","keywords_author":["Android","Internet of Things security","Machine learning","Malware detection","Power consumption","Ransomware detection"],"keywords_other":["Malware detection","Android","Computational capability","Connected Devices","Internet of Things (IOT)","Precision rates","Storage capacity","K-nearest neighbors"],"max_cite":7.0,"pub_year":2017.0,"sources":"['scp']","rawkeys":["ransomware detection","storage capacity","machine learning","internet of things security","computational capability","android","power consumption","precision rates","connected devices","malware detection","k-nearest neighbors","internet of things (iot)"],"tags":["ransomware detection","storage capacity","neural networks","machine learning","computational capability","internet of things security","power consumption","precision rates","connected devices","malware detection","k-nearest neighbors","internet of things (iot)"]},{"p_id":46093,"title":"A waste city management system for smart cities applications","abstract":"\u00a9 2017 IEEE. This paper presents a new method of smart waste city management which makes the environment of the city clean with a low cost. In this approach, the sensor model detects, measures, and transmits waste volume data over the Internet. The collected data including trash bin's geolocation and the serial number is processed by using regression, classification and graph theory. Thenceforth a new method is proposed to dynamically and efficiently manage the waste collection by predicting waste status, classifying trash bin location, and monitoring the amount of waste. Then, this latter recommends the optimization of the route to manage the garbage truck efficiently. Finally, the simulation results are presented and estimated.","keywords_author":["Big data","IoT","machine learning","optimization","waste management"],"keywords_other":["Serial number","City management","Low costs","Geolocations","Waste collection","Sensor model","Volume data"],"max_cite":0.0,"pub_year":2017.0,"sources":"['scp']","rawkeys":["waste management","big data","low costs","serial number","machine learning","volume data","waste collection","optimization","city management","iot","sensor model","geolocations"],"tags":["waste management","big data","low costs","serial number","machine learning","volume data","waste collection","optimization","city management","sensor model","geolocations","internet of things (iot)"]},{"p_id":70668,"title":"A Distributed Wireless Camera System for the Management of Parking Spaces","abstract":"The importance of detection of parking space availability is still growing, particularly in major cities. This paper deals with the design of a distributed wireless camera system for the management of parking spaces, which can determine occupancy of the parking space based on the information from multiple cameras. The proposed system uses small camera modules based on Raspberry Pi Zero and computationally efficient algorithm for the occupancy detection based on the histogram of oriented gradients (HOG) feature descriptor and support vector machine (SVM) classifier. We have included information about the orientation of the vehicle as a supporting feature, which has enabled us to achieve better accuracy. The described solution can deliver occupancy information at the rate of 10 parking spaces per second with more than 90% accuracy in a wide range of conditions. Reliability of the implemented algorithm is evaluated with three different test sets which altogether contain over 700,000 samples of parking spaces.","keywords_author":["parking space","occupancy","smart city","internet of things","raspbery pi","low-power consumption","histogram of oriented gradients","support vector machine"],"keywords_other":null,"max_cite":0.0,"pub_year":2018.0,"sources":"['wos']","rawkeys":["occupancy","parking space","raspbery pi","internet of things","histogram of oriented gradients","low-power consumption","support vector machine","smart city"],"tags":["occupancy","smart cities","raspbery pi","histogram of oriented gradients","machine learning","low-power consumption","internet of things (iot)","parking spaces"]},{"p_id":15379,"title":"Last-meter smart grid embedded in an internet-of-things platform","abstract":"\u00a9 2014 IEEE. The customer domain of the smart grid naturally blends with smart home and smart building systems, but typical proposed approaches are 'distributor-centric' rather than 'customer-centric,' undermining user acceptance, and are often poorly scalable. To solve this problem, we propose a detailed architecture and an implementation of a 'last-meter' smart grid - the portion of the smart grid on customer premises - embedded in an internet-of-things (IoT) platform. Our approach has four aspects of novelty and advantages with respect to the state of the art: 1) seamless integration of smart grid with smart home applications in the same infrastructure; 2) data gathering from heterogeneous sensor communication protocols; 3) secure and customized data access; and 4) univocal sensor and actuator mapping to a common abstraction layer on which additional concurrent applications can be built. A demonstrator has been built and tested with purposely-developed ZigBee smart meters and gateways, a distributed IoT server, and a flexible user interface.","keywords_author":["Demand side management","internet of things","power meter","smart grid","telemetering"],"keywords_other":["Abstraction layer","Flexible user interfaces","Heterogeneous sensors","Sensor and actuators","Smart grid","Seamless integration","Internet of Things (IOT)","Power meters"],"max_cite":46.0,"pub_year":2015.0,"sources":"['scp']","rawkeys":["telemetering","heterogeneous sensors","smart grid","abstraction layer","seamless integration","internet of things","power meter","demand side management","power meters","sensor and actuators","internet of things (iot)","flexible user interfaces"],"tags":["telemetering","heterogeneous sensors","smart grid","abstraction layer","seamless integration","demand side management","sensors and actuators","power meters","internet of things (iot)","flexible user interfaces"]},{"p_id":13337,"title":"The cascading neural network: building the Internet of Smart Things","abstract":"Most of the research on deep neural networks so far has been focused on obtaining higher accuracy levels by building increasingly large and deep architectures. Training and evaluating these models is only feasible when large amounts of resources such as processing power and memory are available. Typical applications that could benefit from these models are, however, executed on resource-constrained devices. Mobile devices such as smartphones already use deep learning techniques, but they often have to perform all processing on a remote cloud. We propose a new architecture called a cascading network that is capable of distributing a deep neural network between a local device and the cloud while keeping the required communication network traffic to a minimum. The network begins processing on the constrained device, and only relies on the remote part when the local part does not provide an accurate enough result. The cascading network allows for an early-stopping mechanism during the recall phase of the network. We evaluated our approach in an Internet of Things context where a deep neural network adds intelligence to a large amount of heterogeneous connected devices. This technique enables a whole variety of autonomous systems where sensors, actuators and computing nodes can work together. We show that the cascading architecture allows for a substantial improvement in evaluation speed on constrained devices while the loss in accuracy is kept to a minimum.","keywords_author":["Cloud computing","Deep learning","Distributed systems and applications","Internet of Things (IoT)","Mobile systems","Neural networks","Ubiquitous and pervasive computing","Neural networks","Internet of Things (IoT)","Deep learning","Distributed systems and applications","Cloud computing","Mobile systems","Ubiquitous and pervasive computing"],"keywords_other":["DATABASE"],"max_cite":2.0,"pub_year":2017.0,"sources":"['ieee', 'scp', 'wos']","rawkeys":["ubiquitous and pervasive computing","cloud computing","mobile systems","deep learning","neural networks","distributed systems and applications","database","internet of things (iot)"],"tags":["ubiquitous and pervasive computing","cloud computing","mobile systems","databases","neural networks","machine learning","distributed systems and applications","internet of things (iot)"]},{"p_id":44057,"title":"Intelligent agents defending for an IoT world: A review","abstract":"\u00a9 2017 Elsevier Ltd Transition to the Internet of Things (IoT) is progressing without realization. In light of this securing traditional systems is still a challenging role requiring a mixture of solutions which may negatively impact, or simply, not scale to a desired operational level. Rule and signature based intruder detection remains prominent in commercial deployments, while the use of machine learning for anomaly detection has been an active research area. Behavior detection means have also benefited from the widespread use of mobile and wireless applications. For the use of smart defense systems we propose that we must widen our perspective to not only security, but also to the domains of artificial intelligence and the IoT in better understanding the challenges that lie ahead in hope of achieving autonomous defense. We investigate how intruder detection fits within these domains, particularly as intelligent agents. How current approaches of intruder detection fulfill their role as intelligent agents, the needs of autonomous action regarding compromised nodes that are intelligent, distributed and data driven. The requirements of detection agents among IoT security are vulnerabilities, challenges and their applicable methodologies. In answering aforementioned questions, a survey of recent research work is presented in avoiding refitting old solutions into new roles. This survey is aimed toward security researchers or academics, IoT developers and information officers concerned with the covered areas. Contributions made within this review are the review of literature of traditional and distributed approaches to intruder detection, modeled as intelligent agents for an IoT perspective; defining a common reference of key terms between fields of intruder detection, artificial intelligence and the IoT, identification of key defense cycle requirements for defensive agents, relevant manufacturing and security challenges; and considerations to future development. As the turn of the decade draws nearer we anticipate 2020 as the turning point where deployments become common, not merely just a topic of conversation but where the need for collective, intelligent detection agents work across all layers of the IoT becomes a reality.","keywords_author":["Artificial intelligence","Cyber security","Data driven cyber security","Internet of Things","Intrusion detection","Machine learning"],"keywords_other":["Distributed approaches","Internet of thing (IOT)","Intelligent detection","Traditional systems","Cyber security","Commercial deployment","Information officers","Mobile and wireless applications"],"max_cite":0.0,"pub_year":2018.0,"sources":"['scp', 'ieee']","rawkeys":["information officers","artificial intelligence","intelligent detection","commercial deployment","intrusion detection","internet of thing (iot)","internet of things","machine learning","mobile and wireless applications","traditional systems","distributed approaches","cyber security","data driven cyber security"],"tags":["information officers","intelligent detection","commercial deployment","intrusion detection systems","machine learning","mobile and wireless applications","traditional systems","distributed approaches","cyber security","data driven cyber security","internet of things (iot)"]},{"p_id":37920,"title":"IoT and distributed machine learning powered optimal state recommender solution","abstract":"\u00a9 2016 IEEE. Recommender systems add significant benefits to E-commerce in terms of sale conversion, revenues, customer experience, loyalty and lifetime value. But the recommendations from these systems do not change on inputs beyond user and item profile and transaction data. There have been some attempts in the past to optimize on more varied data in recommenders, example of which is the location based recommenders. But location is just one dimension of the state that a user could have shared with GPS\/GLONASS\/BaiDeu sensor available in most Smartphones. With an upcoming era of Smart-wears and pervasive IoTs, there are a lot many other dimensions of a user state which can be utilized to optimize upon the concept of Optimal State Recommender Solutions. This paper suggests upgrading from conventional recommendations that are based on user\/item preferences alone with systems that provide the best recommendation at the most optimal state when the user is most receptive to accept the recommendation, the \"optimal state recommendation solution\" and proposes solutions and architectures to overcome the challenges of dealing with real time, distributed machine learning on IoT scale data in implementing this solution. The paper leverages some of the advance distributed machine learning algorithms like variants of Distributed Kalman Filters, Distributed Alternating Least Square Recommenders, Distributed Mini-Batch Stochastic Gradient Descent(SGD) based Classifiers, and highly scalable distributed computation and machine learning platforms like Apache Spark, (Apache) Spark MLlib, Spark Streaming, Python\/PySpark, R\/SparkR, Apache Kafka in an high performance, distributed, fault tolerant architecture. The solution also aspires to be compliant with upcoming IoT standards and architectures like IEEE P2413 to provide a standard solution for such problems beyond the current scope of this paper.","keywords_author":["Apache Kafka","Apache Spark","Distributed ALS Recommender","Distributed Kalman Filter","Distributed Machine Learning","IEEE P2413","IoT","Mini Batch SGD","MLlib","PySpark","Recommender System","Sensors","Smartwears","SparkR"],"keywords_other":["Mini Batch SGD","Apache Kafka","Distributed machine learning","MLlib","SparkR","PySpark","IEEE P2413","Distributed ALS Recommender","Smartwears","Distributed Kalman filters"],"max_cite":1.0,"pub_year":2016.0,"sources":"['scp']","rawkeys":["distributed kalman filters","apache spark","sensors","smartwears","mllib","ieee p2413","pyspark","sparkr","mini batch sgd","iot","distributed als recommender","distributed machine learning","apache kafka","recommender system","distributed kalman filter"],"tags":["distributed kalman filters","apache spark","sensors","smartwears","mllib","ieee p2413","pyspark","sparkr","mini batch sgd","recommender systems","apache kafka","distributed als recommender","distributed machine learning","internet of things (iot)"]},{"p_id":48164,"title":"Smart dairies-enablement of smart city at gross root level","abstract":"\u00a9 2017 IEEE.Rural and urban areas are linked. A basic definition of rural-urban linkages is that they consist of flows (of goods, people, information, finance, waste, information, social relations) across space, linking rural and urban areas (Cecilia, 2015). Economically, rural and urban areas are linked by the reciprocal exchange of unprocessed and processed products, with both areas acting as mutually reinforcing markets [1]. Perhaps a less descriptive definition is of the functional links between sectors (agriculture, industry, and services). The latter is central to structural change taking place in both rural and urban areas. Additionally, rural and urban economies exhibit symbiotic relationship. Cecilia [2] notes 'in many regions of the world we are witnessing an increase in production, especially of perishable and high-value products such as fruit, vegetables and dairy, responding to urban demand'. This is especially the case in rural areas that are well connected to urban markets by transport links, communications and electricity, and by networks of local traders (Cecilia, 2015). This is especially true with Dairy Industry. The dairy industry exhibits mini ecosystem of rural and urban linkage. The dairy industry plays an important role for both rural and urban dwellers: a) a major source of rural employment (12% to 14% of world population [6]), b) consistent non-seasonal source of income with immediate cash returns, c) major urban consumer staple and d) major contributor of agriculture GDP in developing countries. In many developing countries, dairy industry employees majority of workforce from rural and have direct influence on rural and urban commerce. As per the Food and Agriculture Organization of the United Nations [3], 'more than 6 billion people worldwide consume milk and milk products, the majority of these people live in developing countries' [2]. It's clear from the above, urban and rural areas have symbiotic relationship and in order to make urban areas smart, aka Smart Cities, it is imperative that the linkage of urban, in this case rural areas, needs be Smart entities, aka. Smart Villages. For making Smart Village, according to Viswanadham [7] 'the existing infrastructure and services (such as Power, Water, Buildings, Retail, Health care, etc.) need to be upgraded and in building the new ones. This requires standardization, use of IT and sensor networks'. In this research paper, we propose innovative approach to develop dairy IoT sensor network that enables Smart dairy, making Smart Villages a reality. We offer development of Smart Dairy IoT Sensors that not only identify cattle related health issues but also enable data and information sharing with dairy farmers for better predicting milk production and improvement of productivity. In addition, the data collected from IoT sensor and analytics models play pivotal role in preventing spread of viral flus and cattle health issues. Finally, the data collected from our IoT Dairy sensors and analytics will enable digital transformation at village level thus enabling cities smarter. The paper presents prototyping solution design as well as its application and certain experimental results.","keywords_author":["CEP","Decision Tree","Internet Of Things","IoT","IoT reference architecture","Machine Learning","Regression Analysis","Term Frequency and Inverse Document Frequency"],"keywords_other":["Inverse Document Frequency","Urban and rural areas","Digital transformation","Data and information","Food and agriculture organizations","Symbiotic relationship","Reference architecture","Innovative approaches"],"max_cite":0.0,"pub_year":2017.0,"sources":"['scp']","rawkeys":["inverse document frequency","term frequency and inverse document frequency","iot reference architecture","internet of things","machine learning","regression analysis","urban and rural areas","data and information","decision tree","food and agriculture organizations","cep","innovative approaches","symbiotic relationship","iot","reference architecture","digital transformation"],"tags":["inverse document frequency","term frequency and inverse document frequency","iot reference architecture","machine learning","regression analysis","urban and rural areas","complex event processing","data and information","food and agriculture organizations","innovative approaches","symbiotic relationship","reference architecture","digital transformation","internet of things (iot)","decision trees"]},{"p_id":25640,"title":"A host-based intrusion detection and mitigation framework for smart home IoT using OpenFlow","abstract":"\u00a9 2016 IEEE. Smart devices are gaining popularity in our homes with the promise to make our lives easier and more comfortable. However, the increased deployment of such smart devices brings an increase in potential security risks. In this work, we propose an intrusion detection and mitigation framework, called IoT-IDM, to provide a network-level protection for smart devices deployed in home environments. IoT-IDM monitors the network activities of intended smart devices within the home and investigates whether there is any suspicious or malicious activity. Once an intrusion is detected, it is also capable of blocking the intruder in accessing the victim device on the fly. The modular design of IoT-IDM gives its users the flexibility to employ customized machine learning techniques for detection based on learned signature patterns of known attacks. Software-defined networking technology and its enabling communication protocol, OpenFlow, are used to realise this framework. Finally, a prototype of IoT-IDM is developed and the applicability and efficiency of proposed framework demonstrated through a real IoT device: a smart light bulb.","keywords_author":["Anomaly detection","Attack mitigation","Internet of Things (IoT)","Machine learning","Open-Flow","SDN","Smart-home"],"keywords_other":["Smart homes","Attack mitigation","Anomaly detection","Open flow","Internet of Things (IOT)"],"max_cite":7.0,"pub_year":2016.0,"sources":"['scp']","rawkeys":["smart-home","open-flow","anomaly detection","smart homes","machine learning","attack mitigation","open flow","sdn","internet of things (iot)"],"tags":["openflow","anomaly detection","software-defined networking","smart homes","machine learning","attack mitigation","internet of things (iot)"]},{"p_id":1068,"title":"Pattern Recognition for Automated Healthcare Assessment Using Non-invasive, Ambient Sensors","abstract":"In this paper, a solution for an automated healthcare assessment process is proposed. Non-invasive, ambient sensors are retrieving data from patients being in their home care treatment setups. The type of sensors is limited to the tracking of inertia, motion, and alcohol gas. Low-cost sensor prototypes are developed. They constantly measure the movement and the air around the patients. The Big Data generated in this way is used to retrieve patterns of activities. Different pattern recognition algorithms are tested and compared. The highest accuracy and reliability in assessing the data are support vector machines and feedforward neural networks with a performance of 90 % probability in identifying the correct patients' activities over the test period. In this paper, the setup of the sensor prototypes, the data handling, and the data analytics are discussed.","keywords_author":["Data Analytics","Pattern Recognition","Internet of Things","Wearable Sensors","Inertial Sensors","Automated Healthcare Assessment"],"keywords_other":["data retrieval","health care","Support vector machines","data analytics","Magnetic sensors","home care treatment","Sensor phenomena and characterization","Feature extraction","low-cost sensor prototypes","automated healthcare assessment process","alcohol gas tracking","feedforward neural networks","data handling","feedforward neural nets","inertia tracking","ambient sensors","Big Data","information retrieval","noninvasive sensors","medical computing","Data analysis","motion tracking","support vector machines","pattern recognition","data analysis","pattern recognition algorithms"],"max_cite":null,"pub_year":2017.0,"sources":"['ieee']","rawkeys":["data retrieval","health care","big data","data analytics","internet of things","home care treatment","low-cost sensor prototypes","automated healthcare assessment process","inertial sensors","alcohol gas tracking","feedforward neural networks","data handling","feedforward neural nets","inertia tracking","ambient sensors","wearable sensors","sensor phenomena and characterization","magnetic sensors","automated healthcare assessment","information retrieval","noninvasive sensors","medical computing","motion tracking","feature extraction","support vector machines","pattern recognition","data analysis","pattern recognition algorithms"],"tags":["data retrieval","health care","big data","data analytics","home care treatment","internet of things (iot)","low-cost sensor prototypes","automated healthcare assessment process","machine learning","alcohol gas tracking","data handling","feedforward neural nets","inertia tracking","ambient sensors","wearable sensors","neural networks","sensor phenomena and characterization","magnetic sensors","automated healthcare assessment","information retrieval","medical computing","non-invasive sensors","motion tracking","feature extraction","inertial sensor","pattern recognition","data analysis","pattern recognition algorithms"]},{"p_id":27694,"title":"Big Data Analytic Using Cloud Computing","abstract":"\u00a9 2015 IEEE. Innovations in technology and greater affordability of digital devices with internet made a new global world of data called big data. The continuous increase in the volume and detail of data captured by enterprises, such as the rise of social media, Internet of Things (IoT), and multimedia, has produced an overwhelming flow of data in either structured or unstructured format. It is a fact that data that is too big to process is also too big to transfer anywhere, so it's just the analytical program which needs to be moved - not the data. This is possible with cloud computing, as most of the public data sets such as Facebook, Twitter, financial markets data, weather data, genome datasets and aggregated industry-specific data live in the cloud and it becomes more cost-effective for the enterprise to analysis this data in the cloud itself. This paper discusses various problems related to big data computation and possible solution using cloud computing.","keywords_author":["Big data","cloud computing","cluster computing","data mining","distributed computing","trust"],"keywords_other":["Weather data","Public data","Social media","trust","Data computation","Flow of data","Internet of Things (IOT)","Cost effective"],"max_cite":6.0,"pub_year":2015.0,"sources":"['scp', 'wos']","rawkeys":["flow of data","distributed computing","data computation","cloud computing","data mining","big data","cost effective","social media","trust","weather data","cluster computing","internet of things (iot)","public data"],"tags":["flow of data","distributed computing","data computation","cloud computing","data mining","big data","cost effective","social media","trust","weather data","cluster computing","internet of things (iot)","public data"]},{"p_id":50230,"title":"Using IoT, machine learning & cloud for predictive pollution analysis in a smart city","abstract":"\u00a9 2016 International Science Press. When new age technologies like IoT, Machine learning, and Cloud computing are converged, we can have applications which have far more impact than these individual technologies can. In this paper we will see how we can use these 3 technologies to build a smart predictive pollution analytics systems which can be part of any modern day smart city.","keywords_author":["Cloud Computing","IoT","Machine Learning","Pollution","Smart City"],"keywords_other":null,"max_cite":0.0,"pub_year":2016.0,"sources":"['scp']","rawkeys":["cloud computing","machine learning","pollution","iot","smart city"],"tags":["cloud computing","machine learning","pollution","internet of things (iot)","smart cities"]},{"p_id":46139,"title":"ProFiOt: Abnormal Behavior Profiling (ABP) of IoT devices based on a machine learning approach","abstract":"\u00a9 2017 IEEE. In this paper, we aimed to build the Abnormal Behavior Profiling (ABP) of IoT devices to supplement other studies that focused on the abnormal behavior detection of IoT devices with a high accuracy rate by a wide variety of machine learning algorithms. ABP will integrate all types of abnormal behavior detection and possess a key role for the purpose of IoT security in the future. Our technical motivation was derived from IoT smart sensors, which are equipped with high computing power and communication capability; moreover, it is possible that one sensed data can be modified instead of all sensed data (e.g., a sensor reporting temperature, humidity, light and voltage) for malicious purposes. In this kind of threat, it affects the detection accuracy of abnormal behavior and the degradation of the abnormal behavior detection from both machine learning algorithms, such as the k-Means algorithm and support vector machine (SVM), was observed. The k-Means algorithm and SVM were used to detect one sensed data modification out of 4 possible data points from one sensor and the results demonstrated that the k-Means algorithm (92%) had less affection than the SVM (69.5%) in terms of detection accuracy. The ABP was constructed and proposed on the basis of a k-Means algorithm with 10 clusters. In future work, we will investigate how to improve the detection accuracy of abnormal behavior in an IoT environment so that an improved ABP will be available.","keywords_author":["Classifier","IoT","Machine Learning","Malicious behavior","Profiling"],"keywords_other":["Profiling","Malicious behavior","Machine learning approaches","k-Means algorithm","Communication capabilities","Technical motivations","Abnormal behavior detections","Detection accuracy"],"max_cite":0.0,"pub_year":2017.0,"sources":"['scp']","rawkeys":["communication capabilities","detection accuracy","machine learning","technical motivations","machine learning approaches","profiling","classifier","k-means algorithm","iot","malicious behavior","abnormal behavior detections"],"tags":["communication capabilities","detection accuracy","machine learning","technical motivations","machine learning approaches","classifier","k-means algorithm","malicious behavior","abnormal behavior detections","internet of things (iot)","profiles"]},{"p_id":42047,"title":"3D ranging and tracking using lensless smart sensors","abstract":"Target tracking has a wide range of applications in Internet of Things (IoT), such as smart city sensors, indoor tracking, and gesture recognition. Several studies have been conducted in this area. Most of the published works either use vision sensors or inertial sensors for motion analysis and gesture recognition [1, 2]. Recent works use a combination of depth sensors and inertial sensors for 3D ranging and tracking [3, 4]. This often requires complex hardware and the use of complex embedded algorithms. Stereo cameras or Kinect depth sensors used for high precision ranging are instead expensive and not easy to use. The aim of this work is to track in 3D a hand fitted with a series of precisely positioned IR LEDs using a novel Lensless Smart Sensor (LSS) developed by Rambus, Inc. [5, 6]. In the adopted device, the lens used in conventional cameras is replaced by low-cost ultra-miniaturized diffraction optics attached directly to the image sensor array. The unique diffraction pattern enables more precise position tracking than possible with a lens by capturing more information about the scene.","keywords_author":null,"keywords_other":["Conventional camera","Indoor tracking","Complex hardware","Stereo cameras","Diffraction optics","Inertial sensor","Embedded algorithms","Internet of Things (IOT)"],"max_cite":1.0,"pub_year":2017.0,"sources":"['scp']","rawkeys":["stereo cameras","indoor tracking","diffraction optics","complex hardware","embedded algorithms","inertial sensor","conventional camera","internet of things (iot)"],"tags":["stereo cameras","indoor tracking","diffraction optics","complex hardware","inertial sensor","embedding algorithms","conventional camera","internet of things (iot)"]},{"p_id":21573,"title":"Structural Health Monitoring Framework Based on Internet of Things: A Survey","abstract":"\u00a9 2014 IEEE. Internet of Things (IoT) has recently received a great attention due to its potential and capacity to be integrated into any complex system. As a result of rapid development of sensing technologies such as radio-frequency identification, sensors and the convergence of information technologies such as wireless communication and Internet, IoT is emerging as an important technology for monitoring systems. This paper reviews and introduces a framework for structural health monitoring (SHM) using IoT technologies on intelligent and reliable monitoring. Specifically, technologies involved in IoT and SHM system implementation as well as data routing strategy in IoT environment are presented. As the amount of data generated by sensing devices are voluminous and faster than ever, big data solutions are introduced to deal with the complex and large amount of data collected from sensors installed on structures.","keywords_author":["Internet of Things (IoT)","not only SQL (NoSQL) databases","routing protocol","structural health monitoring (SHM)","wireless sensor network (WSN)"],"keywords_other":["Structural health","Structural health monitoring (SHM)","System implementation","Monitoring system","Sensing devices","Wireless communications","Sensing technology","Internet of Things (IOT)"],"max_cite":14.0,"pub_year":2017.0,"sources":"['scp']","rawkeys":["sensing technology","structural health monitoring (shm)","structural health","monitoring system","system implementation","sensing devices","routing protocol","wireless sensor network (wsn)","not only sql (nosql) databases","wireless communications","internet of things (iot)"],"tags":["sensing technology","structural health","monitoring system","system implementation","routing protocols","sensing devices","not only sql (nosql) databases","wireless communications","wireless sensor networks","structural health monitoring","internet of things (iot)"]},{"p_id":37958,"title":"An NFC-based O2O service model in exhibition-space","abstract":"\u00a9 2016 ACM.We proposed a service model based on the spreading trend of Internet of Things (IoT)-based Online-to-Offline (O2O) services in the commerce industry and the steadfast growth of offline businesses in the exhibition industry. We approached the proposed service model from three perspectives. First, we conducted a case analysis of the O2O service to judge the timeliness of service model which we provide, and to find out the possible issues that may occur. Second, we performed a literature review for materials relevant to the proposed service model and explored the feasibility of applying near-field communication (NFC) technology to the service model by comparing NFC with Beacon technology, which is the most commonly used technology in indoor-space. Finally, we designed an NFC-based O2O service model in exhibition-space.","keywords_author":["Beacon","Business model","Internet of Things (IoT)","Machine learning","Near-Field Communication (NFC)","Online-to-Offline (O2O)"],"keywords_other":["Beacon","Offline","The near field communication (NFC)","Business modeling","Internet of Things (IOT)"],"max_cite":1.0,"pub_year":2016.0,"sources":"['scp']","rawkeys":["business modeling","offline","near-field communication (nfc)","business model","online-to-offline (o2o)","machine learning","the near field communication (nfc)","beacon","internet of things (iot)"],"tags":["offline","online-to-offline (o2o)","near field communications","machine learning","the near field communication (nfc)","beacon","internet of things (iot)","business models"]},{"p_id":19542,"title":"Major requirements for building Smart Homes in Smart Cities based on Internet of Things technologies","abstract":"\u00a9 2016 Elsevier B.V.The recent boom in the Internet of Things (IoT) will turn Smart Cities and Smart Homes (SH) from hype to reality. SH is the major building block for Smart Cities and have long been a dream for decades, hobbyists in the late 1970s made Home Automation (HA) possible when personal computers started invading home spaces. While SH can share most of the IoT technologies, there are unique characteristics that make SH special. From the result of a recent research survey on SH and IoT technologies, this paper defines the major requirements for building SH. Seven unique requirement recommendations are defined and classified according to the specific quality of the SH building blocks.","keywords_author":["Ambient intelligence","Home Automation","Internet of Things","Smart Cities","Smart Home requirements"],"keywords_other":["Smart homes","Recent researches","Internet of thing (IOT)","Building blockes","Home automation","Internet of things technologies"],"max_cite":20.0,"pub_year":2016.0,"sources":"['scp', 'ieee']","rawkeys":["smart home requirements","smart homes","home automation","internet of thing (iot)","internet of things","ambient intelligence","internet of things technologies","recent researches","building blockes","smart cities"],"tags":["building blockes","smart home requirements","smart homes","home automation","ambient intelligence","internet of things technologies","recent researches","internet of things (iot)","smart cities"]},{"p_id":46166,"title":"Apache spark based distributed self-organizing map algorithm for sensor data analysis","abstract":"\u00a9 2017 IEEE. The proliferation of Internets of Things (IoT) technologies in both industrial and non-industrial settings has led to the accumulation of Big Data sets. Analysis of these high-volume, high-velocity datasets require advanced processing techniques that incorporate parallel and distributed computations. In this paper, we present a novel distributed self-adaptive neural-network algorithm, the Distributed Growing Self-Organizing Map (DGSOM) algorithm to address the growing need for unsupervised machine learning of Big Data sets on distributed computing environments. The algorithm was tested on a Big Data set of sensor recordings of human activity collected from wearable devices, 2.8 million records. Results indicate that the distributed algorithm significantly reduces execution time compared to its serial counterpart. Moreover, the self-adaptive nature and controlled growth of the algorithm demonstrates data-driven structure adaptation and multi-granular pattern analysis. Overall, the proposed algorithm addresses the need for pattern discovery and visualization from Big Data sets generated by IoT devices which are increasingly commonplace in industrial scenarios.","keywords_author":["Big Data analysis","Growing Self-Organizing Maps","Industrial IoT","Internet of Things","Resilient Distributed Dataset","Unsupervised Machine Learning"],"keywords_other":["Distributed computations","Self Organizing Map algorithm","Distributed computing environment","Resilient distributed dataset","Unsupervised machine learning","Growing Self Organizing Map","Advanced processing techniques","Industrial IoT"],"max_cite":0.0,"pub_year":2017.0,"sources":"['scp', 'ieee']","rawkeys":["advanced processing techniques","distributed computations","unsupervised machine learning","growing self-organizing maps","growing self organizing map","internet of things","big data analysis","resilient distributed dataset","distributed computing environment","industrial iot","self organizing map algorithm"],"tags":["advanced processing techniques","distributed computing","unsupervised machine learning","growing self-organizing maps","big data analysis","resilient distributed dataset","distributed computing environment","self-organizing map","internet of things (iot)"]},{"p_id":27736,"title":"Understanding and personalising smart city services using machine learning, the Internet-of-Things and Big Data","abstract":"\u00a9 2017 IEEE. This paper explores the potential of Machine Learning (ML) and Artificial Intelligence (AI) to lever Internet of Things (IoT) and Big Data in the development of personalised services in Smart Cities. We do this by studying the performance of four well-known ML classification algorithms (Bayes Network (BN), Na\u00efve Bayesian (NB), J48, and Nearest Neighbour (NN)) in correlating the effects of weather data (especially rainfall and temperature) on short journeys made by cyclists in London. The performance of the algorithms was assessed in terms of accuracy, trustworthy and speed. The data sets were provided by Transport for London (TfL) and the UK MetOffice. We employed a random sample of some 1,800,000 instances, comprising six individual datasets, which we analysed on the WEKA platform. The results revealed that there were a high degree of correlations between weather-based attributes and the Big Data being analysed. Notable observations were that, on average, the decision tree J48 algorithm performed best in terms of accuracy while the kNN IBK algorithm was the fastest to build models. Finally we suggest IoT Smart City applications that may benefit from our work.","keywords_author":["Algorithms","Artificial intelligence","Big Data","Classification","Data Analytics","Data mining","Internet-of-Things","Machine learning","Personalisation","Profiling","Recommendation systems","Smart Cities"],"keywords_other":["Nearest neighbour","Profiling","Personalisation","Classification algorithm","Internet of Things (IOT)","Degree of correlations","Data analytics","Transport for london"],"max_cite":5.0,"pub_year":2017.0,"sources":"['scp', 'wos']","rawkeys":["artificial intelligence","transport for london","data mining","personalisation","big data","data analytics","machine learning","internet-of-things","recommendation systems","classification","algorithms","profiling","nearest neighbour","degree of correlations","classification algorithm","internet of things (iot)","smart cities"],"tags":["transport for london","data mining","personalisation","big data","smart cities","data analytics","machine learning","recommender systems","classification","algorithms","nearest neighbour","degree of correlations","classification algorithm","internet of things (iot)","profiles"]},{"p_id":25689,"title":"Integrating Internet-of-Things with the power of Cloud Computing and the intelligence of Big Data analytics-A three layered approach","abstract":"\u00a9 2015 IEEE.This paper is written through the vision on integrating Internet-of-Things (IoT) with the power of Cloud Computing and the intelligence of Big Data analytics. But integration of all these three cutting edge technologies is complex to understand. In this research we first provide a security centric view of three layered approach for understanding the technology, gaps and security issues. Then with a series of lab experiments on different hardware, we have collected performance data from all these three layers, combined these data together and finally applied modern machine learning algorithms to distinguish 18 different activities and cyber-attacks. From our experiments we find classification algorithm RandomForest can identify 93.9% attacks and activities in this complex environment. From the existing literature, no one has ever attempted similar experiment for cyber-attack detection for IoT neither with performance data nor with a three layered approach.","keywords_author":["Big Data","Cloud Computing","Cyber-attack","Hadoop","Internet of Things","IoT","Machine Learning","Network Security"],"keywords_other":["Layered approaches","Performance data","Cyber-attacks","Complex environments","Classification algorithm","Cutting edge technology","Hadoop","Internet of Things (IOT)"],"max_cite":7.0,"pub_year":2016.0,"sources":"['scp']","rawkeys":["cloud computing","cyber-attacks","hadoop","big data","cyber-attack","internet of things","machine learning","performance data","layered approaches","complex environments","internet of things (iot)","iot","classification algorithm","network security","cutting edge technology"],"tags":["cloud computing","cyber-attacks","hadoop","big data","machine learning","performance data","layered approaches","complex environments","internet of things (iot)","classification algorithm","network security","cutting edge technology"]},{"p_id":44124,"title":"A semantic internet of things framework using machine learning approach based on cloud computing","abstract":"\u00a9 2018 Association for Computing Machinery. A major limitation of existing Semantic Web applications is the lack of automatic generation linked data for personal needs. Internet of Things (IoT) can provide automatic sensing data to improve this limitation. The study addresses this issue by defining a Semantic Internet of Things Framework (SIOTF), which is implemented on Hadoop-based cloud computing ecosystem to provide efficiency in dealing with a mass of sensing data. The SIOTF is composed of four modules: Internet of Things module, Na\u00efve Bayesian Classification module, Open Data Service module, and Semantic Web module. The proposed SIOTF is used to develop a Culture Sharing Cloud Platform (CSCP) that provides customized culture information for personnel needs. To demonstrate the feasibility of CSCP, the experimental results illustrate the efficiency and effectiveness of the proposed approach.","keywords_author":["Cloud Computing","IoT","Machine Learning","Semantic Web"],"keywords_other":["Bayesian classification","Cloud platforms","Open datum","Machine learning approaches","Sensing data","Automatic Generation","Semantic web applications","Internet of Things (IOT)"],"max_cite":0.0,"pub_year":2018.0,"sources":"['scp']","rawkeys":["bayesian classification","cloud platforms","cloud computing","sensing data","open datum","machine learning","semantic web applications","automatic generation","iot","semantic web","machine learning approaches","internet of things (iot)"],"tags":["bayesian classification","cloud platforms","cloud computing","sensing data","open datum","machine learning","semantic web applications","automatically generated","semantic web","machine learning approaches","internet of things (iot)"]},{"p_id":1125,"title":"A 41.3\/26.7 pJ per Neuron Weight RBM Processor Supporting On-Chip Learning\/Inference for IoT Applications","abstract":"An energy-efficient restricted Boltzmann machine (RBM) processor (RBM-P) supporting on-chip learning and inference is proposed for machine learning and Internet of Things (IoT) applications in this paper. To train a neural network (NN) model, the RBM structure is applied to supervised and unsupervised learning, and a multi-layer NN can be constructed and initialized by stacking multiple RBMs. Featuring NN model reduction for external memory bandwidth saving, low power neuron binarizer (LPNB) with dynamic clock gating and area-efficient NN-like activation function calculators for power reduction, user-defined connection map (UDCM) for both computation time and bandwidth saving, and early stopping (ES) mechanism for learning process, the proposed system integrates 32 RBM cores with maximal 4k neurons per layer and 128 candidates per sample for machine learning applications. Implemented in 65nm CMOS technology, the proposed RBM-P chip costs 2.2 M gates and 128 kB SRAM with 8.8 mm2 area. Operated at 1.2 V and 210 MHz, this chip achieves 7.53G neuron weights (NWs) and 11.63G NWs per second with 41.3 and 26.7 pJ per NW for learning and inference, respectively.","keywords_author":["Low-power design","machine learning","memory bandwidth reduction","non-linear functions","restricted Boltzmann machine (RBM)","Low-power design","machine learning","memory bandwidth reduction","non-linear functions","restricted Boltzmann machine (RBM)"],"keywords_other":["early stopping mechanism","NN model reduction","supervised learning","frequency 210 MHz","on-chip learning","learning process","Artificial neural networks","Memory bandwidths","Bandwidth","Low-power design","RBM cores","RBM-P chip","energy-efficient restricted Boltzmann machine processor","energy 41.3 pJ","power reduction","CMOS digital integrated circuits","Computational modeling","on-chip inference","RBM processor","dynamic clock gating","Load modeling","machine learning","Reduced order systems","memory size 128 KByte","SRAM","area-efficient NN-like activation function calculators","size 65 nm","Restricted boltzmann machine","size 8.8 mm","multilayer NN","user-defined connection map","neural network model","unsupervised learning","low power neuron binarizer","voltage 1.2 V","Internet of Things","neural chips","energy 26.7 pJ","SRAM chips","Nonlinear functions","Neurons","Data models","neuron weight","CMOS technology","Computational model","external memory bandwidth saving","IoT applications","Boltzmann machines"],"max_cite":0.0,"pub_year":2017.0,"sources":"['scp', 'ieee']","rawkeys":["early stopping mechanism","supervised learning","cmos digital integrated circuits","on-chip learning","learning process","memory size 128 kbyte","internet of things","memory bandwidth reduction","restricted boltzmann machine (rbm)","multilayer nn","load modeling","sram chips","iot applications","memory bandwidths","power reduction","energy-efficient restricted boltzmann machine processor","bandwidth","reduced order systems","dynamic clock gating","on-chip inference","machine learning","cmos technology","frequency 210 mhz","rbm cores","neurons","computational modeling","size 65 nm","size 8.8 mm","non-linear functions","boltzmann machines","user-defined connection map","low power neuron binarizer","neural network model","unsupervised learning","low-power design","neural chips","computational model","restricted boltzmann machine","voltage 1.2 v","neuron weight","rbm-p chip","rbm processor","external memory bandwidth saving","nonlinear functions","energy 26.7 pj","artificial neural networks","data models","energy 41.3 pj","nn model reduction","sram","area-efficient nn-like activation function calculators"],"tags":["early stopping mechanism","supervised learning","cmos digital integrated circuits","on-chip learning","learning process","memory size 128 kbyte","memory bandwidth reduction","multilayer nn","load modeling","sram chips","iot applications","memory bandwidths","internet of things (iot)","energy-efficient restricted boltzmann machine processor","bandwidth","reduced order systems","dynamic clock gating","on-chip inference","machine learning","cmos technology","frequency 210 mhz","rbm cores","neurons","computational modeling","size 65 nm","size 8.8 mm","neural networks","boltzmann machines","user-defined connection map","low power neuron binarizer","neural network model","unsupervised learning","low-power design","neural chips","voltage 1.2 v","restricted boltzmann machine","neuron weight","rbm-p chip","power reductions","rbm processor","external memory bandwidth saving","nonlinear functions","energy 26.7 pj","data models","sram","energy 41.3 pj","nn model reduction","area-efficient nn-like activation function calculators"]},{"p_id":35942,"title":"Lightweight & secure industrial IoT communications via the MQ telemetry transport protocol","abstract":"\u00a9 2017 IEEE.Massive advancements in computing and communication technologies have enabled the ubiquitous presence of interconnected computing devices in all aspects of modern life, forming what is typically referred to as the 'Internet of Things'. These major changes could not leave the industrial environment unaffected, with 'smart' industrial deployments gradually becoming a reality; a trend that is often referred to as the 4th industrial revolution or Industry 4.0. Nevertheless, the direct interaction of the smart devices with the physical world and their resource constraints, along with the strict performance, security, and reliability requirements of industrial infrastructures, necessitate the adoption of lightweight as well as secure communication mechanisms. Motivated by the above, this paper highlights the Message Queue Telemetry Transport (MQTT) as a lightweight protocol suitable for the industrial domain, presenting a comprehensive evaluation of different security mechanisms that could be used to protect the MQTT-enabled interactions on a real testbed of wireless sensor motes. Moreover, the applicability of the proposed solutions is assessed in the context of a real industrial application, analyzing the network characteristics and requirements of an actual, operating wind park, as a representative use case of industrial networks.","keywords_author":["Authentication","Confidentiality","IIoT","Industrial Internet of Things","Industrial Networks","Information security","MQTT","Secure communication","Wireless Sensor Networks","WSN"],"keywords_other":["Comprehensive evaluation","Reliability requirements","Industrial infrastructure","Industrial networks","IIoT","Communication technologies","MQTT","Confidentiality"],"max_cite":2.0,"pub_year":2017.0,"sources":"['scp']","rawkeys":["mqtt","secure communication","authentication","comprehensive evaluation","industrial networks","confidentiality","information security","industrial infrastructure","reliability requirements","wsn","communication technologies","iiot","industrial internet of things","wireless sensor networks"],"tags":["mqtt","secure communication","authentication","comprehensive evaluation","industrial networks","confidentiality","information security","industrial infrastructure","reliability requirements","communication technologies","internet of things (iot)","wireless sensor networks"]},{"p_id":5224,"title":"Internet of things in industries: A survey","abstract":"\u00a9 2005-2012 IEEE. Internet of Things (IoT) has provided a promising opportunity to build powerful industrial systems and applications by leveraging the growing ubiquity of radio-frequency identification (RFID), and wireless, mobile, and sensor devices. A wide range of industrial IoT applications have been developed and deployed in recent years. In an effort to understand the development of IoT in industries, this paper reviews the current research of IoT, key enabling technologies, major IoT applications in industries, and identifies research trends and challenges. A main contribution of this review paper is that it summarizes the current state-of-the-art IoT in industries systematically.","keywords_author":["Big data analytics","enterprise systems","industrial informatics","information and communications technology (ICT)","internet of things (IoT)","near field communications","radio-frequency identification (RFID)","Wireless sensor networks (WSNs)"],"keywords_other":["Near field communications","Wireless sensor network (WSNs)","Enterprise system","Information and communications technology","Industrial informatics","Internet of Things (IOT)","Data analytics"],"max_cite":725.0,"pub_year":2014.0,"sources":"['scp']","rawkeys":["wireless sensor networks (wsns)","enterprise systems","data analytics","near field communications","radio-frequency identification (rfid)","enterprise system","wireless sensor network (wsns)","information and communications technology (ict)","industrial informatics","big data analytics","internet of things (iot)","information and communications technology"],"tags":["data analytics","near field communications","rfid","enterprise system","information and communication technologies","industrial informatics","wireless sensor networks","big data analytics","internet of things (iot)"]},{"p_id":60524,"title":"Internet of Everything: A Large-Scale Autonomic IoT Gateway","abstract":"Gateways are emerging as a key element of bringing legacy and next generation devices to the Internet of Things (IoT). They integrate protocols for networking, help manage storage and edge analytics on the data, and facilitate data flow securely between edge devices and the cloud. Current IoT gateways solve the communication gap between field control\/sensor nodes and customer cloud, enabling field data to be harnessed for manufacturing process optimization, remote management, and preventive maintenance. However, these gateways do not support fully-automatic configuration of newly added IoT devices. In this paper, we proposed a self-configurable gateway featuring real time detection and configuration of smart things over the wireless networks. This novel gateway's main features are: dynamic discovery of home IoT device(s), automatic updates of hardware changes, connection management of smart things connected over AllJoyn. We use the 'option' field for automatic configuration of IoT devices rather than modify standard format of CoAP protocol. Proposed gateway functionality has been validated over the large-scale IoT testbed.","keywords_author":["IoT","IoT gateway","self-configuration","large scale IoT"],"keywords_other":["OPTIMIZATION","NETWORKING"],"max_cite":2.0,"pub_year":2017.0,"sources":"['wos']","rawkeys":["large scale iot","networking","iot gateway","optimization","iot","self-configuration"],"tags":["large scale iot","self configuration","networks","optimization","iot gateway","internet of things (iot)"]},{"p_id":48240,"title":"Real-time monitoring system for rotating machinery with IoT-based cloud platform","abstract":"\u00a9 2017 The Korean Society of Mechanical Engineers.The objective of this research is to improve the efficiency of data collection from many machine components on smart factory floors using IoT(Internet of things) techniques and cloud platform, and to make it easy to update outdated diagnostic schemes through online deployment methods from cloud resources. The short-term analysis is implemented by a micro-controller, and it includes machine-learning algorithms for inferring snapshot information of the machine components. For long-term analysis, time-series and high-dimension data are used for root cause analysis by combining a cloud platform and multivariate analysis techniques. The diagnostic results are visualized in a webbased display dashboard for an unconstrained user access. The implementation is demonstrated to identify its performance in data acquisition and analysis for rotating machinery.","keywords_author":["Cloud Platform","Internet of Things","Machine Learning","Rotating Machinery","Smart Factory"],"keywords_other":["Cloud platforms","Root cause analysis","Short-term analysis","Deployment methods","Long term analysis","High-dimension data","Real time monitoring system","Multivariate analysis techniques"],"max_cite":0.0,"pub_year":2017.0,"sources":"['scp']","rawkeys":["cloud platforms","cloud platform","deployment methods","multivariate analysis techniques","internet of things","machine learning","smart factory","root cause analysis","real time monitoring system","high-dimension data","rotating machinery","long term analysis","short-term analysis"],"tags":["cloud platforms","deployment methods","multivariate analysis techniques","machine learning","smart factory","root cause analysis","real time monitoring system","internet of things (iot)","high-dimension data","rotating machinery","long term analysis","short-term analysis"]},{"p_id":46197,"title":"Report of the 2017 IEEE cyber science and technology congress","abstract":"\u00a9 2017 by the authors.The modern digitized world has led to the emergence of a new paradigm on global information networks and infrastructures known as Cyberspace and the studies of Cybernetics, which bring seamless integration of physical, social and mental spaces. Cyberspace is becoming an integral part of our daily life from learning and entertainment to business and cultural activities. As expected, this whole concept of Cybernetics brings new challenges that need to be tackled. The 2017 IEEE Cyber Science and Technology Congress (CyberSciTech 2017) provided a forum for researchers to report their research findings and exchange ideas. The congress took place in Orlando, Florida, USA during 6-10 November 2017. Not counting poster papers, the congress accepted over fifty papers that are divided into nine sessions. In this report, we provide an overview of the research contributions of the papers in CyberSciTech 2017.","keywords_author":["Cyber Science","Cybernetics","Cyberspace","Internet of things","Knowledge discovery","Machine learning","Network security","Smart environment","Smart healthcare"],"keywords_other":null,"max_cite":0.0,"pub_year":2017.0,"sources":"['scp']","rawkeys":["smart healthcare","internet of things","knowledge discovery","machine learning","cybernetics","cyber science","smart environment","cyberspace","network security"],"tags":["cybernetics","smart healthcare","machine learning","knowledge discovery","cyberspaces","cyber science","smart environment","network security","internet of things (iot)"]},{"p_id":46200,"title":"Performance of convolutional neural network and recurrent neural network for anticipation of driver's conduct","abstract":"\u00a9 2017 IEEE. With the advancements in Internet of Things (IoT), we could efficiently improve our daily life activities like health care, monitoring, transportation, smart homes etc. Artificial Intelligence along with Machine learning has played a very supportive role to analyze various situations and take decisions accordingly. Maneuver anticipation supplements existing Advance Driver Assistance Systems (ADAS) by anticipating mishaps and giving drivers more opportunity to respond to road circumstances proactively. The capacity to sort the driver conduct is extremely beneficial for advance driver assistance system (ADAS). Deep learning solutions would further be an endeavor of for driving conduct recognition. A technique for distinguishing driver's conduct is imperative to help operative mode transition between the driver and independent vehicles. We propose a novel approach of dissecting driver's conduct by using Convolutional Neural Network (CNN), Recurrent Neural Network(RNN) and a combination of Convolutional Neural Network with Long-Short Term Memory (LSTM) that would give better results in less response time. We are likewise proposing to concentrate high level features and interpretable features depicting complex driving examples by trying CNN, RNN and then CNN with LSTM. We could improve the system accuracy to 95% by combining CNN with LSTM.","keywords_author":["ADAS","Deep learning CNN","ITS","LSTM","RNN"],"keywords_other":["Daily life activities","ADAS","High-level features","Recurrent neural network (RNN)","LSTM","Convolutional neural network","Driver assistance system","Internet of Things (IOT)"],"max_cite":0.0,"pub_year":2017.0,"sources":"['scp', 'ieee']","rawkeys":["high-level features","lstm","deep learning cnn","its","driver assistance system","rnn","convolutional neural network","recurrent neural network (rnn)","daily life activities","internet of things (iot)","adas"],"tags":["high-level features","neural networks","deep learning cnn","long short-term memory","driver assistance system","intelligent transportation systems","convolutional neural network","advanced driver assistance system","daily life activities","internet of things (iot)"]},{"p_id":25721,"title":"PUFs as Promising Tools for Security in Internet of Things","abstract":"The use of physically unclonable functions (PUFs) for providing alternate hardware fingerprints which can be used for lightweight authentication is discussed. The PUFs are extremely promising primitives, in the sense that they are by definition random, physically unclonable, and hence infeasible to be computed by adversaries who do not have possession of the PUF. PUF is the physical embodiment of a function that makes it hard to clone and exhibits an unpredictable challenge-response behavior that is ideally hard to characterize or model, but which is otherwise easily and reliably evaluated. Machine learning attacks are well-known mathematical attacks on PUFs, which are physically unclonable. The root of security of the scheme can be argued as the malware infected system cannot run the PUF function on the MAC addresses which are collected from the ARP tables. Thus, the adversary will not be able to obtain the whitelisted usernames, and will thus not be able to execute unauthorized commands to the IoT masquerading as a legitimate user. However, there are several challenges which need to be considered: starting from making the PUF primitives secured against machine learning but yet lightweight, to develop protocols using them to prevent such machine-learning-based attacks.","keywords_author":["Intel Edison","IoT","lightweight","Machine Learning Attacks","Protocols","PUF"],"keywords_other":["ARP table","lightweight","Physically unclonable functions","Intel Edison","MAC address","Challenge response","Infected systems","Legitimate users"],"max_cite":7.0,"pub_year":2016.0,"sources":"['scp']","rawkeys":["lightweight","physically unclonable functions","puf","legitimate users","challenge response","iot","mac address","machine learning attacks","intel edison","protocols","arp table","infected systems"],"tags":["light weight","physically unclonable functions","legitimate users","challenge response","mac address","protocols","machine learning attacks","intel edison","infected systems","arp table","internet of things (iot)"]},{"p_id":42117,"title":"Random forests for resource allocation in 5G cloud radio access networks based on position information","abstract":"\u00a9 2018, The Author(s). Next generation 5G cellular networks are envisioned to accommodate an unprecedented massive amount of Internet of things (IoT) and user devices while providing high aggregate multi-user sum rates and low latencies. To this end, cloud radio access networks (CRAN), which operate at short radio frames and coordinate dense sets of spatially distributed radio heads, have been proposed. However, coordination of spatially and temporally denser resources for larger sets of user population implies considerable resource allocation complexity and significant system signalling overhead when associated with channel state information (CSI)-based resource allocation (RA) schemes. In this paper, we propose a novel solution that utilizes random forests as supervised machine learning approach to determine the resource allocation in multi-antenna CRAN systems based primarily on the position information of user terminals. Our simulation studies show that the proposed learning based RA scheme performs comparably to a CSI-based scheme in terms of spectral efficiency and is a promising approach to master the complexity in future cellular networks. When taking the system overhead into account, the proposed learning-based RA scheme, which utilizes position information, outperforms legacy CSI-based scheme by up to 100%. The most important factor influencing the performance of the proposed learning-based RA scheme is antenna orientation randomness and position inaccuracies. While the proposed random forests scheme is robust against position inaccuracies and changes in the propagation scenario, we complement our scheme with three approaches that restore most of the original performance when facing random antenna orientations of the user terminal.","keywords_author":["5G","CRAN","Machine learning","Random forests","Resource allocation"],"keywords_other":["Random forests","CRAN","Considerable resources","Future cellular networks","Cloud radio access networks","Supervised machine learning","Internet of Things (IOT)","Spectral efficiencies"],"max_cite":0.0,"pub_year":2018.0,"sources":"['scp']","rawkeys":["future cellular networks","spectral efficiencies","resource allocation","machine learning","supervised machine learning","random forests","internet of things (iot)","cloud radio access networks","5g","considerable resources","cran"],"tags":["future cellular networks","spectral efficiencies","resource allocation","machine learning","supervised machine learning","random forests","internet of things (iot)","cloud radio access networks","5g","considerable resources","cran"]},{"p_id":17545,"title":"Big data: From beginning to future","abstract":"\u00a9 2016 Elsevier LtdBig data is a potential research area receiving considerable attention from academia and IT communities. In the digital world, the amounts of data generated and stored have expanded within a short period of time. Consequently, this fast growing rate of data has created many challenges. In this paper, we use structuralism and functionalism paradigms to analyze the origins of big data applications and its current trends. This paper presents a comprehensive discussion on state-of-the-art big data technologies based on batch and stream data processing. Moreover, strengths and weaknesses of these technologies are analyzed. This study also discusses big data analytics techniques, processing methods, some reported case studies from different vendors, several open research challenges, and the opportunities brought about by big data. The similarities and differences of these techniques and technologies based on important parameters are also investigated. Emerging technologies are recommended as a solution for big data problems.","keywords_author":["Analytics","Big data","Cloud computing","Internet of things","Parallel and distributed computing","Social media"],"keywords_other":["Analytics","Emerging technologies","Parallel and distributed computing","Research challenges","Big data applications","Social media","Stream data processing","Potential researches"],"max_cite":29.0,"pub_year":2016.0,"sources":"['scp']","rawkeys":["research challenges","cloud computing","stream data processing","big data","potential researches","emerging technologies","social media","internet of things","big data applications","parallel and distributed computing","analytics"],"tags":["research challenges","cloud computing","big data","potential researches","emerging technologies","social media","streaming data processing","big data applications","parallel and distributed computing","internet of things (iot)","analytics"]},{"p_id":23690,"title":"Lightweight Lossy Compression of Biometric Patterns via Denoising Autoencoders","abstract":"\u00a9 2015 IEEE. Wearable Internet of Things (IoT) devices permit the massive collection of biosignals (e.g., heart-rate, oxygen level, respiration, blood pressure, photo-plethysmographic signal, etc.) at low cost. These, can be used to help address the individual fitness needs of the users and could be exploited within personalized healthcare plans. In this letter, we are concerned with the design of lightweight and efficient algorithms for the lossy compression of these signals. In fact, we underline that compression is a key functionality to improve the lifetime of IoT devices, which are often energy constrained, allowing the optimization of their internal memory space and the efficient transmission of data over their wireless interface. To this end, we advocate the use of autoencoders as an efficient and computationally lightweight means to compress biometric signals. While the presented techniques can be used with any signal showing a certain degree of periodicity, in this letter we apply them to ECG traces, showing quantitative results in terms of compression ratio, reconstruction error and computational complexity. State of the art solutions are also compared with our approach.","keywords_author":["Autoencoders","biometric patterns","lossy compression","wearable devices"],"keywords_other":["Lossy compressions","Biometric patterns","Personalized healthcare","Transmission of data","Reconstruction error","Internet of Things (IOT)","Wearable devices","Autoencoders"],"max_cite":10.0,"pub_year":2015.0,"sources":"['scp', 'ieee']","rawkeys":["biometric patterns","wearable devices","personalized healthcare","lossy compression","transmission of data","lossy compressions","autoencoders","reconstruction error","internet of things (iot)"],"tags":["biometric patterns","wearable devices","personalized healthcare","lossy compression","transmission of data","auto encoders","reconstruction error","internet of things (iot)"]},{"p_id":48267,"title":"Improving Vehicle Localization in a Smart City with Low Cost Sensor Networks and Support Vector Machines","abstract":"\u00a9 2017 Springer Science+Business Media New York A smart city\u2019s main purpose is to provide intelligent responses to different problems of the rapid urban population growth. For instance, integrating fleet management solutions into intelligent transportation systems (ITS) can efficiently resolve transportation problems relying on each vehicle information. Usually, the position estimate is ensured by the integration of the Global Positioning System (GPS) and Inertial Navigation Systems (INS). For multisensor data fusion, the Extended Kalman Filter (EKF) is generally applied using the sensor\u2019s measures and the GPS position as a helper. However, the INS are expensive and require more complex computing which induces restrictions on their implementation. Furthermore, the EKF performance depends on the vehicle dynamic variations and may quickly diverge because of environmental changes. In this paper, we present a robust low cost approach using EKF and Support Vector Machines (SVM) to reliably estimate the vehicle position by limiting the EKF drawbacks. The sensors used are a GPS augmented by a low cost wireless sensor network. When GPS signals are available, we train SVM on different dynamics and outage times to learn the position errors so we can correct the future EKF predictions during GPS signal outages. We obtain empirically an improvement of up to 94% over the simple EKF predictions in case of GPS failures.","keywords_author":["Global positioning system","Internet of things","Low cost","Machine learning","Smart city","Wireless sensor network"],"keywords_other":["Environmental change","Inertial navigation systems (INS)","Intelligent transportation systems","Transportation problem","Vehicle localization","Low costs","Position estimates","Multisensor data fusion"],"max_cite":0.0,"pub_year":2017.0,"sources":"['scp']","rawkeys":["position estimates","global positioning system","multisensor data fusion","smart city","transportation problem","low costs","low cost","internet of things","machine learning","inertial navigation systems (ins)","wireless sensor network","intelligent transportation systems","vehicle localization","environmental change"],"tags":["multisensor data fusion","smart cities","transportation problem","low costs","machine learning","inertial navigation systems (ins)","intelligent transportation systems","gaussian processes","wireless sensor networks","position estimation","vehicle localization","internet of things (iot)","environmental change"]},{"p_id":29837,"title":"ARIIMA: A real IoT implementation of a machine-learning architecture for reducing energy consumption","abstract":"\u00a9 Springer International Publishing Switzerland 2014.As the inclusion of more devices and appliances within the IoT ecosystem increases, methodologies for lowering their energy consumption impact are appearing. On this field, we contribute with the implementation of a RESTful infrastructure that gives support to Internet-connected appliances to reduce their energy waste in an intelligent fashion. Our work is focused on coffee machines located in common spaces where people usually do not care on saving energy, e.g. the workplace. The proposed approach lets these kind of appliances report their usage patterns and to process their data in the Cloud through ARIMA predictive models. The aim such prediction is that the appliances get back their next-week usage forecast in order to operate autonomously as efficient as possible. The underlying distributed architecture design and implementation rationale is discussed in this paper, together with the strategy followed to get an accurate prediction matching with the real data retrieved by four coffee machines.","keywords_author":["ARIMA Models","Coffee-Maker","Eco-aware Everyday Things","Energy Efficiency","IoT","Machine Learning","RESTful Infrastructure"],"keywords_other":["Reducing energy consumption","ARIMA models","RESTful Infrastructure","IoT","RESTful infrastructure","Eco-aware Everyday Things","Distributed architecture","Accurate prediction","Eco-aware everyday things","Intelligent fashion"],"max_cite":4.0,"pub_year":2014.0,"sources":"['scp']","rawkeys":["reducing energy consumption","eco-aware everyday things","coffee-maker","machine learning","restful infrastructure","intelligent fashion","iot","arima models","energy efficiency","distributed architecture","accurate prediction"],"tags":["eco-aware everyday things","reduce energy consumption","coffee-maker","machine learning","restful infrastructure","intelligent fashion","arima models","energy efficiency","distributed architecture","accurate prediction","internet of things (iot)"]},{"p_id":35987,"title":"Comparison of Collision-Free and Contention-Based Radio Access Protocols for the Internet of Things","abstract":"\u00a9 1972-2012 IEEE.The fifth-generation (5G) cellular networks will face the challenge of integrating the traditional broadband services with the Internet of Things (IoT), which is characterized by sporadic uplink transmissions of small data packets. Indeed, the access procedure of the previous generation cellular network (4G) is not able to support IoT traffic efficiently, because it requires a large amount of signaling for the connection setup before the actual data transmission. In this context, we introduce two innovative radio access protocols for sporadic transmissions of small data packets, which are suitable for 5G networks, because they provide a resource-efficient packet delivery exploiting a connectionless approach. The core of this paper resides in the derivation of an analytical framework to evaluate the performance of all the aforementioned protocols. The final goal is the comparison between 4G and 5G radio access solutions employing both our analytical framework and computer simulations. The performance evaluation results show the benefits of the protocols envisioned for 5G in terms of signaling overhead and access latency.","keywords_author":["5G","Cellular networks","IoT","LTE","M2M","massive radio access","mission critical communications","random access protocols"],"keywords_other":["Cellular network","Internet of thing (IOT)","Mission-critical communication","Evaluation results","Up-link transmissions","Signaling overheads","Radio access","Random access protocol"],"max_cite":2.0,"pub_year":2017.0,"sources":"['scp']","rawkeys":["signaling overheads","cellular networks","random access protocol","mission critical communications","internet of thing (iot)","lte","radio access","mission-critical communication","evaluation results","random access protocols","cellular network","massive radio access","up-link transmissions","iot","5g","m2m"],"tags":["signaling overheads","random access protocol","mission critical communications","lte","radio access","evaluation results","machine to machines","cellular network","massive radio access","up-link transmissions","5g","internet of things (iot)"]},{"p_id":5269,"title":"Big data: A survey","abstract":"In this paper, we review the background and state-of-the-art of big data. We first introduce the general background of big data and review related technologies, such as could computing, Internet of Things, data centers, and Hadoop. We then focus on the four phases of the value chain of big data, i.e., data generation, data acquisition, data storage, and data analysis. For each phase, we introduce the general background, discuss the technical challenges, and review the latest advances. We finally examine the several representative applications of big data, including enterprise management, Internet of Things, online social networks, medial applications, collective intelligence, and smart grid. These discussions aim to provide a comprehensive overview and big-picture to readers of this exciting area. This survey is concluded with a discussion of open problems and future directions. \u00a9 2014 Springer Science+Business Media New York.","keywords_author":["Big data","Big data analysis","Cloud computing","Data center","Hadoop","Internet of things","Smart grid"],"keywords_other":["Hadoop","Smart grid","Data centers","Big datum","Internet of Things (IOT)"],"max_cite":681.0,"pub_year":2014.0,"sources":"['scp']","rawkeys":["cloud computing","hadoop","smart grid","big data","internet of things","big data analysis","data centers","data center","big datum","internet of things (iot)"],"tags":["cloud computing","hadoop","smart grid","big data","big data analysis","data centers","big datum","internet of things (iot)"]},{"p_id":40091,"title":"Osmotic Message-Oriented Middleware for the Internet of Things","abstract":"\u00a9 2014 IEEE. Message-oriented middleware is a key technology in todays Internet of Things (IoT). Centralized message brokers facilitate decoupled device-to-device communication and can transparently scale to handle many millions of messages per second. However, Cloud-based solutions, such as AWS IoT or Azure IoT Hub, cannot satisfy the stringent QoS and privacy requirements of many modern IoT scenarios. Instead, distributed middleware needs to leverage the ever- increasing amount of resources at the edge of the network to provide reliable, ultra-low-latency, and privacy-aware message routing. But the heterogeneity and volatility inherent to Edge resources, and the unpredictability of mobile clients, make it extremely challenging to provide resilient coordination mechanisms and guaranteed message delivery. Applying Osmotic Computing principles to message-oriented middleware opens new opportunities for solving these challenges.","keywords_author":["Cloud Computing","internet of things","iot","message oriented middleware","osmotic computing"],"keywords_other":["Privacy requirements","Message oriented middleware","Device-to-Device communications","osmotic computing","Message delivery","Distributed middleware","Internet of Things (IOT)","Coordination mechanisms"],"max_cite":1.0,"pub_year":2018.0,"sources":"['scp', 'wos']","rawkeys":["cloud computing","internet of things","device-to-device communications","osmotic computing","privacy requirements","message delivery","coordination mechanisms","iot","message oriented middleware","internet of things (iot)","distributed middleware"],"tags":["cloud computing","device-to-device communications","osmotic computing","privacy requirements","message delivery","coordination mechanisms","message oriented middleware","internet of things (iot)","distributed middleware"]},{"p_id":27807,"title":"Wearable environmental sensors and infrastructure for mobile large-scale urban deployment","abstract":"\u00a9 2001-2012 IEEE. We present a platform to allow up to 50000 students to simultaneously collect and learn from their personal activity, transportation, and environmental data. The main goals that we met during the design of our sensor platform were to: 1) be low cost; 2) remain powered for the duration of the data collection campaign; 3) robustly sense a wide range of environmental parameters; and 4) be packaged in a form factor conducive to wide-spread adoption and ease of use. We describe and generalize the design methods we applied on the hardware and firmware. Our sensors employ Wi-Fi communication to move data as well as to localize themselves using a radio-map of Singapore. Our system uses embedded as well as server-based machine learning algorithms to perform on-sensor transportation mode identification and state inference. The testing and validation methods that we applied ensured that over 98% of the deployed sensors successfully met all of their design goals. In addition, we summarize the results of a large-scale deployment of our system for a nation-wide experiment in Singapore in 2015, and describe three sample applications of the collected data. We publish sample data sets and algorithm code for researchers to analyze.","keywords_author":["engineering education","Internet of things","machine learning","state estimation","wearable sensors"],"keywords_other":["Sample applications","Data collection","Wi-Fi communications","Large-scale deployment","Environmental sensor","Transportation mode","Environmental parameter","Environmental data"],"max_cite":5.0,"pub_year":2016.0,"sources":"['scp']","rawkeys":["environmental sensor","large-scale deployment","wearable sensors","environmental parameter","internet of things","machine learning","environmental data","transportation mode","data collection","wi-fi communications","engineering education","sample applications","state estimation"],"tags":["environmental sensor","large-scale deployment","wearable sensors","environmental parameter","machine learning","environmental data","sample applications","transportation mode","data collection","wi-fi communications","engineering education","internet of things (iot)","state estimation"]},{"p_id":7328,"title":"Internet of Things in the 5G Era: Enablers, Architecture, and Business Models","abstract":"\u00a9 1983-2012 IEEE. The IoT paradigm holds the promise to revolutionize the way we live and work by means of a wealth of new services, based on seamless interactions between a large amount of heterogeneous devices. After decades of conceptual inception of the IoT, in recent years a large variety of communication technologies has gradually emerged, reflecting a large diversity of application domains and of communication requirements. Such heterogeneity and fragmentation of the connectivity landscape is currently hampering the full realization of the IoT vision, by posing several complex integration challenges. In this context, the advent of 5G cellular systems, with the availability of a connectivity technology, which is at once truly ubiquitous, reliable, scalable, and cost-efficient, is considered as a potentially key driver for the yet-to emerge global IoT. In the present paper, we analyze in detail the potential of 5G technologies for the IoT, by considering both the technological and standardization aspects. We review the present-day IoT connectivity landscape, as well as the main 5G enablers for the IoT. Last but not least, we illustrate the massive business shifts that a tight link between IoT and 5G may cause in the operator and vendors ecosystem.","keywords_author":["3GPP","5G","Bluetooth Low Energy","cellular","Internet of Things","IoT","Low Power Wide Area","Low-Power Wifi","Machine-Type Communications","MTC","Standardization","Zigbee"],"keywords_other":["3GPP","Machine type communications","Low-Power WiFi","Low Power","cellular","Bluetooth low energies (BTLE)"],"max_cite":153.0,"pub_year":2016.0,"sources":"['scp']","rawkeys":["mtc","bluetooth low energy","cellular","internet of things","machine type communications","3gpp","low power wide area","machine-type communications","zigbee","iot","low power","bluetooth low energies (btle)","5g","low-power wifi","standardization"],"tags":["minimum toe clearance (mtc)","cellular","standards","bluetooth low energies (ble)","machine type communications","3gpp","low power wide area","low-power wifi","zigbee","low power","5g","internet of things (iot)"]},{"p_id":25761,"title":"Privacy-preserving protocols for secure and reliable data aggregation in IoT-enabled Smart Metering systems","abstract":"\u00a9 2017 Elsevier B.V. As the Internet of Things (IoT) gets more pervasive, its areas of usage expands. Smart Metering systems is such an IoT-enabled technology that enables convenient and high frequency data collection compared to existing metering systems. However, such a frequent data collection puts the consumers\u2019 privacy in risk as it helps expose the consumers\u2019 daily habits. Secure in-network data aggregation can be used to both preserve consumers\u2019 privacy and reduce the packet traffic due to high frequency metering data. The privacy can be provided by performing the aggregation on concealed metering data. Fully homomorphic encryption (FHE) and secure multiparty computation (secure MPC) are the systems that enable performing multiple operations on concealed data. However, both FHE and secure MPC systems have some overhead in terms of data size or message complexity. The overhead is compounded in the IoT-enabled networks such as Smart Grid (SG) Advanced Metering Infrastructure (AMI). In this paper, we propose new protocols to adapt FHE and secure MPC to be deployed in SG AMI networks that are formed using wireless mesh networks. The proposed protocols conceal the smart meters\u2019 (SMs) reading data by encrypting it (FHE) or computing its shares on a randomly generated polynomial (secure MPC). The encrypted data\/computed shares are aggregated at some certain aggregator SM(s) up to the gateway of the network in a hierarchical manner without revealing the readings\u2019 actual value. To assess their performance, we conducted extensive experiments using the ns-3 network simulator. The simulation results indicate that the secure MPC-based protocol can be a viable privacy-preserving data aggregation mechanism since it not only reduces the overhead with respect to FHE but also almost matches the performance of the Paillier cryptosystem when it is used within a proper sized AMI network.","keywords_author":["Fully homomorphic encryption","Internet of things","Privacy-preserving protocols","Reliable data aggregation","Secure multiparty computation","Smart Metering systems"],"keywords_other":["Fully homomorphic encryption","Privacy-preserving protocols","Smart metering","Secure multi-party computation","Reliable data aggregation"],"max_cite":7.0,"pub_year":2018.0,"sources":"['scp']","rawkeys":["reliable data aggregation","internet of things","secure multiparty computation","smart metering","smart metering systems","fully homomorphic encryption","privacy-preserving protocols","secure multi-party computation"],"tags":["reliable data aggregation","smart meter","smart metering systems","fully homomorphic encryption","internet of things (iot)","privacy-preserving protocols","secure multi-party computation"]},{"p_id":19622,"title":"IoT-based collaborative reputation system for associating visitors and artworks in a cultural scenario","abstract":"\u00a9 2017 Elsevier Ltd In this paper, starting from a comprehensive mathematical model of a Collaborative Reputation Systems (CRSes), we present a research study within the Cultural Heritage domain. The main goal of this study has been the evaluation and classification of the visitors\u2019 behaviour during a cultural event. By means of mobile technological instruments, opportunely deployed within the environment, it is possible to collect data representing the knowledge to be inferred and give a reliable rate for both visitors and exposed artworks. Discussed results, confirm the reliability and the usefulness of CRSes for deeply understand dynamics related to people visiting styles.","keywords_author":["Classification","Collaborative reputation systems","Cultural heritage","Human behaviours","Internet of Things","Internet of Things","Collaborative reputation systems","Cultural heritage","Classification","Human behaviours"],"keywords_other":["Evaluation and classifications","THINGS","Cultural events","MUSEUM","Human behaviours","Research studies","Reputation systems","HERITAGE","Cultural heritages","INTERNET"],"max_cite":20.0,"pub_year":2017.0,"sources":"['scp', 'wos']","rawkeys":["cultural events","things","evaluation and classifications","internet of things","museum","collaborative reputation systems","cultural heritage","cultural heritages","classification","heritage","human behaviours","research studies","reputation systems","internet"],"tags":["cultural events","things","evaluation and classifications","museum","collaborative reputation systems","research studies","cultural heritages","classification","heritage","human behaviours","reputation systems","internet","internet of things (iot)"]},{"p_id":40106,"title":"Cognitive Computing: Architecture, Technologies and Intelligent Applications","abstract":"\u00a9 2013 IEEE.with the development of network-enabled sensors and artificial intelligence algorithms, various human-centered smart systems are proposed to provide services with higher quality, such as smart healthcare, affective interaction, and autonomous driving. Considering cognitive computing is an indispensable technology to develop these smart systems, this paper proposes human-centered computing assisted by cognitive computing and cloud computing. First, we provide a comprehensive investigation of cognitive computing, including its evolution from knowledge discovery, cognitive science, and big data. Then, the system architecture of cognitive computing is proposed, which consists of three critical technologies, i.e., networking (e.g., Internet of Things), analytics (e.g., reinforcement learning and deep learning), and cloud computing. Finally, it describes the representative applications of human-centered cognitive computing, including robot technology, emotional communication system, and medical cognitive system.","keywords_author":["big data analysis","cloud computing","Cognitive computing","Internet of Things"],"keywords_other":["Cognitive Computing","Artificial intelligence algorithms","Cognitive science","Emotional communications","Cognitive intelligence","Cyberspaces","Cognition","Human-centered computing"],"max_cite":1.0,"pub_year":2018.0,"sources":"['scp', 'ieee']","rawkeys":["cloud computing","cognitive science","emotional communications","human-centered computing","internet of things","big data analysis","cognitive intelligence","artificial intelligence algorithms","cyberspaces","cognition","cognitive computing"],"tags":["cloud computing","cognitive science","emotional communications","human-centered computing","big data analysis","cognitive intelligence","artificial intelligence algorithms","cyberspaces","cognition","internet of things (iot)"]},{"p_id":42163,"title":"Low-power portable devices for metagenomics analysis: Fog computing makes bioinformatics ready for the Internet of Things","abstract":"\u00a9 2018 Elsevier B.V. Portable sequencing machines, such as the Oxford Nanopore MinION, are making the genome sequencing ubiquitous. This can be particularly interesting for identifying specific bacteria in air-filters or waters and for monitoring the microbioma composition in cultivated soils or in different animal samples, using a simple and portable approach. However, a main problem of these portable sequencing devices is that they stream huge amounts of data, which management can be actually challenging. Low-power System-on-Chip architectures represent a feasible way for designing a solution, based on the Fog computing paradigm, for processing locally the raw data, considering both the base calling step and the genome alignment part, and for sending only meaningful results over Internet. Cloud services can be then used to collect and integrate results in a Internet of Things framework, in order to trigger notifications or alarms and, in perspective, for more sophisticated applications based on statistical or machine learning approaches.","keywords_author":["Cloud computing","Environmental genomics","Fog computing","Internet of Things","Machine learning","Metagenomics"],"keywords_other":["Cultivated soils","Genomics","Genome alignment","Metagenomics","Machine learning approaches","Genome sequencing","Computing paradigm","Low-power systems"],"max_cite":0.0,"pub_year":2018.0,"sources":"['scp']","rawkeys":["environmental genomics","cloud computing","low-power systems","fog computing","metagenomics","genome alignment","internet of things","genome sequencing","machine learning","cultivated soils","machine learning approaches","computing paradigm","genomics"],"tags":["environmental genomics","genome sequence","cloud computing","low-power systems","fog computing","metagenomics","genome alignment","machine learning","cultivated soils","machine learning approaches","computing paradigm","genomics","internet of things (iot)"]},{"p_id":42165,"title":"A novel deep learning method for aircraft landing speed prediction based on cloud-based sensor data","abstract":"\u00a9 2018 The combination of artificial intelligence methods and IoT based sensor data will play a critical and crucial role in various environments. Flight landing safety is a research hotspot of aviation field for a long time. Accurately predicting the landing speed is conducive to reducing the landing accidents. In this paper, we proposed an accurate aircraft landing speed prediction model based on the long-short term memory (LSTM) with flight sensor data. Firstly, we analyze and pre-process the dataset with statistical method including randomness tests and stationary tests. Secondly, we design the features by random forest algorithm and reduce the dimensionality of features with principal component analysis. Thirdly, we develop a deep architecture based on long-short term memory to predict the aircraft landing speed. Experiment results prove that it has better performance with higher prediction accuracy compared with the state of the art, indicating that the proposed model is accurate and effective. The findings are expected to be applied into flight operation practice for further preventing of landing accidents and improving the air management for air traffic controllers.","keywords_author":["Deep learning","IoT","Landing speed prediction","QAR data"],"keywords_other":["QAR data","Air traffic controller","Deep architectures","Artificial intelligence methods","Random forest algorithm","Prediction accuracy","Speed prediction","Speed prediction models"],"max_cite":0.0,"pub_year":2018.0,"sources":"['scp']","rawkeys":["deep learning","landing speed prediction","random forest algorithm","deep architectures","speed prediction models","prediction accuracy","speed prediction","qar data","iot","air traffic controller","artificial intelligence methods"],"tags":["air traffic control","random forest algorithm","landing speed prediction","deep architectures","machine learning","speed prediction models","prediction accuracy","speed prediction","qar data","artificial intelligence methods","internet of things (iot)"]},{"p_id":42169,"title":"Improving consumer satisfaction in smart cities using edge computing and caching: A case study of date fruits classification","abstract":"\u00a9 2018 Elsevier B.V. The emergence of 5G technology has enabled a fast development of the wireless communication based on Big Data, Internet of Things (IoT), cloud computing, edge and fog computing. The development has contributed to enhance the lifestyle of the citizens in smart cities. Different applications are provided with 5G technologies to solve problems of the citizens. In this article, we take advantage of the 5G technology to develop a framework of images\u2019 classification to satisfy consumers in smart cities. As a case, study, we develop an automatic date fruits classification system in the framework to satisfy date fruits consumers interest. In the proposed system, a deep learning approach is utilized with fine-tuning pre-trained models. The edge computing and caching are used to provide a low latency and real-time transmission of the date fruits\u2019 images. The experimental results show the viability of the proposed framework.","keywords_author":["Cloud technology","Date fruit classification","Deep learning","Edge caching","Smart cities"],"keywords_other":["Cloud technologies","Consumer satisfactions","Classification system","Date fruits","Real-time transmissions","Wireless communications","Edge caching","Internet of Things (IOT)"],"max_cite":0.0,"pub_year":2018.0,"sources":"['scp', 'ieee']","rawkeys":["classification system","date fruit classification","deep learning","cloud technologies","real-time transmissions","consumer satisfactions","internet of things (iot)","date fruits","wireless communications","edge caching","cloud technology","smart cities"],"tags":["classification system","date fruit classification","cloud technologies","machine learning","real-time transmissions","consumer satisfactions","wireless communications","date fruits","edge caching","internet of things (iot)","smart cities"]},{"p_id":25819,"title":"Context-aware stream processing for distributed IoT applications","abstract":"\u00a9 2015 IEEE. Most of the IoT applications are distributed in nature generating large data streams which have to be analyzed in near real-time. Solutions based on Complex Event Processing (CEP) have the potential to extract high-level knowledge from these data streams but the use of CEP for distributed IoT applications is still in early phase and involves many drawbacks. The manual setting of rules for CEP is one of the major drawback. These rules are based on threshold values and currently there are no automatic methods to find the optimized threshold values. In real-time dynamic IoT environments, the context of the application is always changing and the performance of current CEP solutions are not reliable for such scenarios. In this regard, we propose an automatic and context aware method based on clustering for finding optimized threshold values for CEP rules. We have developed a lightweight CEP called \u03bcCEP to run on low processing hardware which can update the rules on the run. We have demonstrated our approach using a real-world use case of Intelligent Transportation System (ITS) to detect congestion in near real-time.","keywords_author":["Clustering","Complex event processing","context-aware","intelligent transportation system","internet of things","machine learning","real-time"],"keywords_other":["Intelligent transportation systems","Clustering","Context-Aware","Complex event processing","Real time"],"max_cite":7.0,"pub_year":2015.0,"sources":"['scp']","rawkeys":["context-aware","real time","real-time","internet of things","machine learning","intelligent transportation system","clustering","intelligent transportation systems","complex event processing"],"tags":["context-aware","real time","machine learning","clustering","intelligent transportation systems","complex event processing","internet of things (iot)"]},{"p_id":48355,"title":"Cyber security and the role of intelligent systems in addressing its challenges","abstract":null,"keywords_author":["artificial intelligence","autonomous systems","cyber challenges","cyber collaboration","cyber definition","cyber phenomenon","Cyber security","cyber technology trends","intelligent systems","internet of things","machine learning"],"keywords_other":null,"max_cite":0.0,"pub_year":2017.0,"sources":"['scp', 'ieee']","rawkeys":["artificial intelligence","cyber technology trends","autonomous systems","internet of things","machine learning","intelligent systems","cyber security","cyber challenges","cyber phenomenon","cyber definition","cyber collaboration"],"tags":["cyber technology trends","autonomous systems","machine learning","intelligent systems","cyber security","cyber challenges","cyber phenomenon","cyber definition","cyber collaboration","internet of things (iot)"]},{"p_id":21733,"title":"A Smart High-Speed Backbone Path Construction Approach for Energy and Delay Optimization in WSNs","abstract":"\u00a9 2018 IEEE. Quickly and efficiently transmitting data to sink via intelligent routing is an important issue in wireless sensor networks. In previous scenarios, there has existed the phenomenon of 'energy hole,' which results in difficulties in synchronous optimization of energy and delay. Thus, a smart High-Speed Backbone Path (HSBP) construction approach is proposed in this paper. In the HSBP approach, several High-speed Backbone Paths (HBPs) are established at different locations of the network, and the duty cycles of nodes on the HBPs are increased to 1; therefore, the data are forwarded by HBPs without the existence of sleeping delay, which greatly reduces transmission latency. Furthermore, the HBPs are built in regions with adequate residual energy, and they are switched periodically; thus, more nodes can be utilized to equalize the energy consumption. A comprehensive performance analysis demonstrates that the HSBP approach has obvious advantages in improving network performance compared with previous studies; it reduces transmission delay by 48.10% and improves energy utilization by 38.21% while guaranteeing the same network lifetime.","keywords_author":["energy utilization","Internet of Things","smart high-speed backbone path","transmission delay","wireless sensor networks","Internet of Things","wireless sensor networks","smart high-speed backbone path","transmission delay","energy utilization"],"keywords_other":["Delays","High Speed","Network topology","Transmission delays","WIRELESS SENSOR NETWORKS","Routing","LIFETIME","Relays"],"max_cite":14.0,"pub_year":2018.0,"sources":"['scp', 'wos']","rawkeys":["network topology","routing","delays","energy utilization","internet of things","relays","transmission delay","smart high-speed backbone path","lifetime","transmission delays","wireless sensor networks","high speed"],"tags":["network topology","routing","delays","energy utilization","relays","high-speed","smart high-speed backbone path","transmission delays","wireless sensor networks","lifetime","internet of things (iot)"]},{"p_id":17640,"title":"Random forest classification for detecting android malware","abstract":"Internet connected smartphone devices play a crucial role in the application domain of Internet of Things. These devices are being widely used for day-to-day activities such as remotely controlling lighting and heating at homes, paying for parking, and recently for paying for goods using saved credit card information using Near Field Communication (NFC). Android is the most popular smartphone platform today. It is also the choice of malware authors to obtain secure and private data. In this paper we exclusively apply the machine learning ensemble learning algorithm Random Forest supervised classifier on an Android feature dataset of 48919 points of 42 features each. Our goal was to measure the accuracy of Random Forest in classifying Android application behavior to classify applications as malicious or benign. Moreover, we wanted to focus on detection accuracy as the free parameters of the Random Forest algorithm such as the number of trees, depth of each tree and number of random features selected are varied. Our experimental results based on 5-fold cross validation of our dataset shows that Random Forest performs very well with an accuracy of over 99 percent in general, an optimal Out-Of-Bag (OOB) error rate [3] of 0.0002 for forests with 40 trees or more, and a root mean squared error of 0.0171 for 160 trees. \u00a9 2013 IEEE.","keywords_author":["Android","Machine learning","Malware"],"keywords_other":["Random forest classification","Malwares","Near field communications","Android","Random forest algorithm","Root mean squared errors","Internet of Things (IOT)","Ensemble learning algorithm"],"max_cite":28.0,"pub_year":2013.0,"sources":"['scp']","rawkeys":["random forest algorithm","near field communications","machine learning","android","malware","root mean squared errors","malwares","random forest classification","ensemble learning algorithm","internet of things (iot)"],"tags":["neural networks","random forest algorithm","near field communications","machine learning","malware","root mean square errors","random forest classification","ensemble learning algorithm","internet of things (iot)"]},{"p_id":25838,"title":"A context and user aware smart notification system","abstract":"\u00a9 2015 IEEE. Nowadays, notifications are increasingly gaining momentum in our society. New smart devices and appliances are developed everyday with the ability to generate, send and show messages about their status, acquired data and\/or information received from other devices and users. Consequently, the number of notifications received by a user is growing and the tolerance to them could decrease in a short time. This paper presents a smart notification system that uses machine learning algorithms to adequately manage incoming notifications. According to context awareness and user habits, the system decides: a) who should receive an incoming notification; b) what is the best moment to show the notification to the chosen user(s); c) on which device(s) the chosen user(s) should receive the notification; d) which is the best way to notify the incoming notification. After the design of a general architecture, as a first step in building such a system, three different machine learning algorithms were compared in the task of establishing the best device on which the incoming notification should be delivered. The algorithms were applied to a dataset derived from real data provided by the MIT Media Laboratory Reality Mining project, enriched with additional synthetic information.","keywords_author":["Internet of Things","Machine Learning","Notifications"],"keywords_other":["Notification systems","Context- awareness","Reality minings","Smart devices","In-buildings","General architectures","Notifications","Gaining momentum"],"max_cite":7.0,"pub_year":2015.0,"sources":"['scp']","rawkeys":["in-buildings","general architectures","context- awareness","notification systems","internet of things","machine learning","smart devices","notifications","reality minings","gaining momentum"],"tags":["in-buildings","context-aware","general architectures","notification systems","machine learning","smart devices","notifications","reality minings","gaining momentum","internet of things (iot)"]},{"p_id":38130,"title":"Distributed Differentially Private Stochastic Gradient Descent: An Empirical Study","abstract":"\u00a9 2016 IEEE. In fault-prone large-scale distributed environments stochastic gradient descent (SGD) is a popular approach to implement machine learning algorithms. Data privacy is a key concern in such environments, which is often addressed within the framework of differential privacy. The output quality of differentially private SGD implementations as a function of design choices has not yet been thoroughly evaluated. In this study, we examine this problem experimentally. We assume that every data record is stored by an independent node, which is a typical setup in networks of mobile devices or Internet of things (IoT) applications. In this model we identify a set of possible distributed differentially private SGD implementations. In these implementations all the sensitive computations are strictly local, and any public information is protected by differentially private mechanisms. This means that personal information can leak only if the corresponding node is directly compromised. We then perform a set of experiments to evaluate these implementations over several machine learning problems with both logistic regression and support vector machine (SVM) loss functions. Depending on the parameter setting and the choice of the algorithm, the performance of the noise-free algorithm can be closely approximated by differentially private variants.","keywords_author":["distributed differential privacy","machine learning","stochastic gradient descent"],"keywords_other":["Stochastic gradient descent","Machine learning problem","Personal information","Distributed environments","Public information","Logistic regressions","Differential privacies","Internet of Things (IOT)"],"max_cite":1.0,"pub_year":2016.0,"sources":"['scp']","rawkeys":["personal information","public information","machine learning","machine learning problem","differential privacies","distributed environments","logistic regressions","stochastic gradient descent","distributed differential privacy","internet of things (iot)"],"tags":["personal information","public information","machine learning","machine learning problem","differential privacies","distributed environments","logistic regressions","stochastic gradient descent","distributed differential privacy","internet of things (iot)"]},{"p_id":48371,"title":"An Attack-Resilient Middleware Architecture for Grid Integration of Distributed Energy Resources","abstract":"\u00a9 2016 IEEE. In recent years, the increasing penetration of Distributed Energy Resources (DERs) has made an impact on the operation of the electric power systems. In the grid integration of DERs, data acquisition systems and communications infrastructure are crucial technologies to maintain system economic efficiency and reliability. Since most of these generators are relatively small, dedicated communications investments for every generator are capital cost prohibitive. Combining real-time attack-resilient communications middleware with Internet of Things (IoTs) technologies allows for the use of existing infrastructure. In our paper, we propose an intelligent communication middleware that utilizes the Quality of Experience (QoE) metrics to complement the conventional Quality of Service (QoS) evaluation. Furthermore, our middleware employs deep learning techniques to detect and defend against congestion attacks. The simulation results illustrate the efficiency of our proposed communications middleware architecture.","keywords_author":["Attack-Resilient","Deep Learning","Distributed Energy Resources","Internet of Things (IoTs)","Middleware Architecture","Quality of Experience (QoE)","Quality of Service (QoS)","Smart Grid"],"keywords_other":["Distributed Energy Resources","Internet of thing (IoTs)","Middleware architecture","Quality of experience (QoE)","Smart grid","Attack-Resilient"],"max_cite":0.0,"pub_year":2017.0,"sources":"['scp']","rawkeys":["quality of service (qos)","smart grid","attack-resilient","deep learning","middleware architecture","quality of experience (qoe)","internet of things (iots)","internet of thing (iots)","distributed energy resources"],"tags":["smart grid","attack-resilient","middleware architecture","machine learning","quality of experience (qoe)","quality of service","distributed energy resources","internet of things (iot)"]},{"p_id":44279,"title":"Towards a Practical Crowdsensing System for Road Surface Conditions Monitoring","abstract":"IEEE The Internet of Things (IoT) infrastructure, systems, and applications demonstrate potential in serving smart city development. Crowdsensing approaches for road surface conditions monitoring can benefit smart city road information services. Deteriorated roads induce vehicle damage, traffic congestion, and driver discomfort which influence traffic management. In this paper, we propose a framework for monitoring road surface anomalies. We analyze the common road surface types and irregularities as well as their impact on vehicle motion. In addition to the traditional use of sensors available in smart devices, we utilize the vehicle motion sensors (accelerometers and gyroscopes) presently available in most land vehicles. Various land vehicles were used in this research, spanning different sizes, and year model for extensive road experiments. These trajectories were used to collect and build multiple labeled data sets that were used in the system structure. In order to enhance the performance of the sensor measurements, wavelet packet de-noising is used in this study to enable efficient classification of road surface anomalies. We adopt statistical, time domain and frequency domain features to distinguish different road anomalies. The descriptive data sets collected in this study are used to build, train, and test a system classifier through machine learning techniques to detect and categorize multiple road anomalies with different severity levels. Furthermore, we analyze and assess the capabilities of the smart devices and the other vehicle motion sensors to accurately geo-reference the road surface anomalies. Several road test experiments examine the benefits and assess the performance of the proposed architecture.","keywords_author":["Crowdsensing","Global Positioning System","Intelligent sensors","Machine Learning.","Monitoring","Road Information Services","Roads","Signal Processing","Smart cities","Smart City Applications"],"keywords_other":["Internet of thing (IOT)","Wavelet packet de-noising","Crowd sensing","Intelligent sensors","Smart city applications","Road surface condition","Roads","Machine learning techniques"],"max_cite":0.0,"pub_year":2018.0,"sources":"['scp']","rawkeys":["intelligent sensors","wavelet packet de-noising","crowdsensing","global positioning system","road surface condition","smart cities","internet of thing (iot)","machine learning","machine learning techniques","road information services","crowd sensing","roads","signal processing","smart city applications","monitoring"],"tags":["intelligent sensors","wavelet packet de-noising","road surface condition","smart cities","machine learning techniques","machine learning","road information services","roads","signal processing","crowd-sensing","gaussian processes","smart city applications","internet of things (iot)","monitoring"]},{"p_id":27897,"title":"Mind the plug! Laptop-user recognition through power consumption","abstract":"\u00a9 2016 ACM. The Internet of Things (IoT) paradigm, in conjunction with the one of smart cities, is pursuing toward the concept of smart buildings, i.e., \"intelligent\" buildings able to receive data from a network of sensors and thus to adapt the environment. IoT sensors can monitor a wide range of environmental features such as the energy consumption inside a building at fine-grained level (e.g., for a specific wall-socket). Some smart buildings already deploy energy monitoring in order to optimize the energy use for good purposes (e.g., to save money, to reduce pollution). Unfortunately, such measurements raise a significant amount of privacy concerns. In this paper, we investigate the feasibility of recognizing the pair laptop-user (i.e., a user using her own laptop) from the energy traces produced by her laptop. We design MTPlug, a framework that achieves this goal relying on supervised machine learning techniques as pattern recognition in multivariate time series. We present a comprehensive implementation of this system and run a thorough set of experiments. In particular, we collected data by monitoring the energy consumption of two groups of laptop users, some office employees and some intruders, for a total of 27 people. We show that our system is able to build an energy profile for a laptop user with accuracy above 80%, in less than 3.5 hours of laptop usage. To the best of our knowledge, this is the first research that assesses the feasibility of laptop users profiling relying uniquely on fine-grained energy traces collected using wall-socket smart meters.","keywords_author":["Energy consumption","Internet of Things","Intrusion detection","Machine learning","Smart building","Smart meter","User identification"],"keywords_other":["Internet of thing (IOT)","Multivariate time series","Environmental features","Energy monitoring","Network of sensors","Privacy concerns","Supervised machine learning","User identification"],"max_cite":5.0,"pub_year":2016.0,"sources":"['scp']","rawkeys":["energy consumption","network of sensors","intrusion detection","internet of thing (iot)","internet of things","machine learning","smart meter","multivariate time series","smart building","privacy concerns","environmental features","supervised machine learning","user identification","energy monitoring"],"tags":["smart buildings","energy consumption","network of sensors","intrusion detection systems","machine learning","smart meter","multivariate time series","privacy concerns","environmental features","supervised machine learning","user identification","energy monitoring","internet of things (iot)"]},{"p_id":27901,"title":"Integrating mobile and cloud for PPG signal selection to monitor heart rate during intensive physical exercise","abstract":"Heart rate monitoring has become increasingly popular in the industry through mobile phones and wearable devices. However, current determination of heart rate through mobile applications suffers from high corruption of signals during intensive physical exercise. In this paper, we present a novel technique for accurately determining heart rate during intensive motion by classifying PPG signals obtained from smartphones or wearable devices combined with motion data obtained from accelerometer sensors. Our approach utilizes the Internet of Things (IoT) cloud connectivity of smartphones for selection of PPG signals using deep learning. The technique is validated using the TROIKA dataset and is accurately able to predict heart rate with a 10-fold cross validation error margin of 4.88%. Copyright is held by the owner\/author(s).","keywords_author":["Accelerometer","Deep belief networks","Deep learning","Heart rate monitoring","Internet of Things (IoT)","PPG signals"],"keywords_other":["Deep learning","Heart-rate monitoring","Internet of thing (IOT)","Mobile applications","10-fold cross-validation","Deep belief networks","Accelerometer sensor","Internet of Things (IOT)"],"max_cite":5.0,"pub_year":2016.0,"sources":"['ieee', 'scp', 'wos']","rawkeys":["mobile applications","10-fold cross-validation","deep learning","ppg signals","heart-rate monitoring","internet of thing (iot)","heart rate monitoring","internet of things (iot)","deep belief networks","accelerometer","accelerometer sensor"],"tags":["mobile applications","10-fold cross-validation","ppg signals","heart-rate monitoring","machine learning","internet of things (iot)","deep belief networks","accelerometer","accelerometer sensor"]},{"p_id":44288,"title":"Device-Free People Counting in IoT Environments: New Insights, Results and Open Challenges","abstract":"IEEE In the last years multiple Internet of Things (IoT) solutions have been developed to detect, track, count and identify human activity from people that do not carry any device nor participate actively in the detection process. When WiFi radio receivers are employed as sensors for device-free human activity recognition, channel quality measurements are preprocessed in order to extract predictive features towards performing the desired activity recognition via machine learning (ML) models. Despite the variety of predictors in the literature, there is no universally outperforming set of features for all scenarios and applications. However, certain feature combinations could achieve a better average detection performance compared to the use of a thorough feature portfolio. Such predictors are often obtained by feature engineering and selection techniques applied before the learning process. This manuscript elaborates on the feature engineering and selection methodology for counting device-free people by solely resorting to the fluctuation and variation of WiFi signals exchanged by IoT devices. We comprehensively review the feature engineering and ML models employed in the literature from a critical perspective, identifying trends, research niches and open challenges. Furthermore, we present and provide the community with a new open database with WiFi measurements in several indoor environments (i.e. rooms, corridors and stairs) where up to 5 people can be detected. This dataset is used to exhaustively assess the performance of different ML models with and without feature selection, from which insightful conclusions are drawn regarding the predictive potential of different predictors across scenarios of diverse characteristics.","keywords_author":["Activity recognition","Device-free people counting","Feature extraction","feature selection","Internet of Things","IoT architectures.","machine learning","Performance evaluation","Predictive models","Wireless communication","Wireless fidelity"],"keywords_other":["Wireless fidelities","Activity recognition","People counting","Predictive models","Iot architectures","Performance evaluations","Wireless communications"],"max_cite":0.0,"pub_year":2018.0,"sources":"['scp']","rawkeys":["iot architectures","people counting","activity recognition","predictive models","internet of things","machine learning","performance evaluations","feature selection","wireless communications","wireless fidelities","feature extraction","performance evaluation","device-free people counting","wireless fidelity","wireless communication"],"tags":["iot architectures","people counting","activity recognition","predictive models","machine learning","feature selection","wireless communications","wireless fidelities","feature extraction","performance evaluation","device-free people counting","internet of things (iot)"]},{"p_id":91400,"title":"An Incremental CFS Algorithm for Clustering Large Data in Industrial Internet of Things","abstract":"With the rapid advances of sensing technologies and wireless communications, large amounts of dynamic data pertaining to industrial production are being collected from many sensor nodes deployed in the industrial Internet of Things. Analyzing those data effectively can help to improve the industrial services and mitigate the system unprepared breakdowns. As an important technique of data analysis, clustering attempts to find the underlying pattern structures embedded in unlabeled information. Unfortunately, most of the current clustering techniques that could only deal with static data become infeasible to cluster a significant volume of data in the dynamic industrial applications. To tackle this problem, an incremental clustering algorithm by fast finding and searching of density peaks based on k-mediods is proposed in this paper. In the proposed algorithm, two cluster operations, namely cluster creating and cluster merging, are defined to integrate the current pattern into the previous one for the final clustering result, and k-mediods is employed to modify the clustering centers according to the new arriving objects. Finally, experiments are conducted to validate the proposed scheme on three popular UCI datasets and two real datasets collected from industrial Internet of Things in terms of clustering accuracy and computational time.","keywords_author":["CFS clustering","incremental clustering","industrial Internet of Things (IoT)","K-mediods"],"keywords_other":null,"max_cite":26.0,"pub_year":2017.0,"sources":"['ieee', 'wos']","rawkeys":["industrial internet of things (iot)","cfs clustering","incremental clustering","k-mediods"],"tags":["k-mediods","cfs clustering","incremental clustering","internet of things (iot)"]},{"p_id":44297,"title":"Wireless technology identification using deep convolutional neural networks","abstract":"\u00a9 2017 IEEE. With the proliferation of wireless technologies and the ever-increasing growth in Internet of Things (IoT) devices operating the license-free Industrial, Scientific, and Medical (ISM) band, intelligent access systems capable of coexisting in crowded spectrum regions are of vital importance. In this work we study the adaptation of Convolutional Neural Networks (CNNs) to the problem of identifying coexisting wireless devices. We develop a machine learning conduit to facilitate the detection and identification of frequency domain signatures for 802.x standard compliant technologies. Spectrum scans across the entire ISM region (80-MHz) are recorded and a data-driven training process for a wide range of Signal-to-Noise Ratios (SNRs) is completed. Model accuracy is compared to that attained using standard feature based classification methods. Results indicate CNN models outperform their counterpart methods in terms of classification accuracy, connoting them to be highly effective tools for detecting and identifying coexisting devices despite acute overlap and interference presence. The proposed approach aims to advance cognitive wireless awareness by enhancing automatic detection and identification accuracy.","keywords_author":["CNN","Cognitive radio","Machine learning","Neural networks","Spectrum sensing","Wireless identification"],"keywords_other":["Feature-based classification","Spectrum sensing","Wireless identifications","Detection and identifications","Internet of Things (IOT)","Convolutional neural network","Deep convolutional neural networks","Classification accuracy"],"max_cite":0.0,"pub_year":2018.0,"sources":"['scp']","rawkeys":["detection and identifications","neural networks","classification accuracy","deep convolutional neural networks","machine learning","wireless identification","cognitive radio","cnn","convolutional neural network","feature-based classification","wireless identifications","spectrum sensing","internet of things (iot)"],"tags":["detection and identifications","neural networks","classification accuracy","machine learning","cognitive radio","feature-based classification","convolutional neural network","wireless identifications","spectrum sensing","internet of things (iot)"]},{"p_id":42260,"title":"A scalable distributed machine learning approach for attack detection in edge computing environments","abstract":"\u00a9 2018 Elsevier Inc. The ever-increasing number of IoT applications and cyber\u2013physical services is introducing significant challenges associated to their cyber-security. Due to the constrained nature of the involved devices, some heavier computational tasks, such as deep traffic inspection and classification, essential for implementing automatic attack detection systems, are moved on specialized \u201cedge\u201d devices, in order to distribute the processing intelligence near to the data sources. These edge devices are mainly capable of effectively running pre-built classification models but have not enough storage and processing capabilities to build and upgrade such models from huge volumes of field training data, imposing a serious barrier to the deployment of such solutions. This work leverages the flexibility of cloud-based architectures, together with the recent advancements in the area of large-scale machine learning for shifting the more computationally-expensive and storage-demanding operations to the cloud in order to benefit of edge computing capabilities only for effectively performing traffic classification based on sophisticated Extreme Learning Machines models that are pre-built over the cloud.","keywords_author":["Attack detection","Distributed machine learning","Edge computing","Extreme learning machines","IoT"],"keywords_other":["Processing capability","Distributed machine learning","Computing environments","Attack detection","Cloud-based architectures","Traffic classification","Large-scale machine learning","Extreme learning machine"],"max_cite":0.0,"pub_year":2018.0,"sources":"['scp']","rawkeys":["cloud-based architectures","attack detection","processing capability","traffic classification","extreme learning machines","computing environments","large-scale machine learning","edge computing","iot","extreme learning machine","distributed machine learning"],"tags":["cloud-based architectures","attack detection","processing capability","computing environments","large-scale machine learning","edge computing","traffic classification","extreme learning machine","distributed machine learning","internet of things (iot)"]},{"p_id":44308,"title":"Multi-View Stacking Ensemble for Power Consumption Anomaly Detection in the Context of Industrial Internet of Things","abstract":"\u00a9 2013 IEEE. Anomaly detection of power consumption, mainly including electricity stealing and unexpected power energy loss, has been one of the essential routine works in power system management and maintenance. With the help of Industrial Internet of Things technologies, power consumption data was aggregated from distributed various power devices. Hence, the power consumption anomaly was able to be detected by machine learning algorithms. In this paper, a three-stage multi-view stacking ensemble (TMSE) machine learning model based on hierarchical time series feature extraction (HTSF) methods are proposed to solve the anomaly detection problem: HTSF is a novel systematic time series feature engineering method to represent the given data numerically and as input data for machine learning algorithms, while TMSE is designed to ensemble meta-models to archive more accurate performance by using multi-view stacking ensemble method. Performance evaluation in real-world data shows that the proposed method outperforms the existing time series feature extraction means and dramatically decreases the time consumed for ensemble learning process.","keywords_author":["anomaly detection","feature extraction","Internet of Things","machine learning","power consumption","smart grids","time series analysis"],"keywords_other":["Power system management","Power demands","Machine learning models","Anomaly detection","Smart grid","Stacking","Accurate performance","Internet of things technologies"],"max_cite":0.0,"pub_year":2018.0,"sources":"['scp']","rawkeys":["smart grid","anomaly detection","machine learning models","stacking","internet of things","machine learning","power consumption","time series analysis","accurate performance","feature extraction","smart grids","power system management","internet of things technologies","power demands"],"tags":["smart grid","anomaly detection","machine learning models","stacking","machine learning","power consumption","time series analysis","accurate performance","feature extraction","power system management","internet of things technologies","internet of things (iot)","power demands"]},{"p_id":48406,"title":"Demo abstract: Cyber-physical fingerprinting for internet of things authentication","abstract":"\u00a9 2017 ACM.Current security and privacy solutions fail to meet the IoT requirements due to computational restrictions and portable nature of IoT objects. In this demo, a novel authentication framework is proposed that exploits the device-specific information to authenticate each object in the IoT. The framework is shown to effectively track the physical environment effects on objects. The experiment shows a sample IoT environment consisting of multiple Raspberry PI units operating as IoT objects. The proposed framework monitors the changes of the physical environment surrounding objects by examining the changes in the device-specific information of each IoT object. The scenario of an emulation attacker is presented in the demo as a case study. The attacker is capable of replicating all the transmitted messages and security keys of one of the IoT objects. Our proposed framework is able to effectively detect such an attack and improve the authentication accuracy for all IoT objects. Demo Abstract.","keywords_author":["Authentication","Internet of things","Machine learning","Systems security","Transfer learning"],"keywords_other":["Systems security","Transfer learning","Specific information","Security key","Security and privacy","Physical environments","Cyber physicals","Computational restriction"],"max_cite":0.0,"pub_year":2017.0,"sources":"['scp']","rawkeys":["physical environments","security key","specific information","authentication","transfer learning","computational restriction","machine learning","internet of things","systems security","security and privacy","cyber physicals"],"tags":["physical environments","security key","specific information","authentication","transfer learning","computational restriction","machine learning","system security","security and privacy","cyber physicals","internet of things (iot)"]},{"p_id":34079,"title":"Real-time investigation of flight delays based on the internet of things data","abstract":"\u00a9 Springer International Publishing AG 2016. Flight delay is a very important problem resulting in the wasting of billions of dollars each year. Other researchers have studied this problem using historical records of flights. With the emerging paradigm of Internet of things (IoT), it is now possible to analyze sensors data in real-time. We investigate flight delays using real-time data from the IoT. We crawl IoT data and collect the data from various resources including flights, weather and air quality sensors. Our goal is to improve our understanding of the roots and signs of flight delays in order to be able to classify a given flight based on the features from flights and other data sources. We extend the existing works by adding new data sources and considering new factors in the analysis of flight delay. Through the use of real-time data, our goal is to establish a novel service to predict delays in real-time.","keywords_author":["Data mining","Flight delay analysis","Internet of things","Machine learning","Prediction"],"keywords_other":["Data-sources","Historical records","Sensors data","Flight delays","Internet of Things (IOT)","Real time","Real-time data"],"max_cite":2.0,"pub_year":2016.0,"sources":"['scp']","rawkeys":["flight delays","data mining","historical records","real time","prediction","internet of things","machine learning","sensors data","flight delay analysis","real-time data","data-sources","internet of things (iot)"],"tags":["flight delays","sensor data","data mining","historical records","real time","prediction","machine learning","flight delay analysis","real-time data","data-sources","internet of things (iot)"]},{"p_id":38178,"title":"Selection of the most prominent lines of research in ICT domain","abstract":"\u00a9 2015 IEEE.The paper is devoted to selection of the most crucial directions of research in ICT domain that could be implemented in the Republic of Kazakhstan. In the paper we evaluated the dynamics of the annual changes in the number of publications and convergence of ICT sub-domains based on data of Scopus, EBSCO (Information Science & Technology Abstracts, Academic Search Complete) and Google Scholar. To analyze the place of Kazakhstan, we considered indexes shown in the Global Competitiveness Report. As a result, the most rapidly developing areas of research were revealed (big data, machine learning, 5G, augmented reality, and etc.). The semantic network of the most modern concepts of the ICT domain was constructed that visualizes the binary relationship between the components and their relative importance. By using comparative analysis of the number of publications in the leading countries and some other countries including Kazakhstan, we selected some key domains which need to be seriously improved onto the way of development science in RK.","keywords_author":["5G","Augmented Reality","Big Data","Bioinformatics","Cloud computing","Cyber-Physical systems","e-Governance","Embedded systems","Human-machine systems","ICT domain","Information Security","Internet of Things","Machine Learning","Machine to Machine","Mobile computing","Multi agent systems","Neural Networks","Robotics","scientometric databases","SDN","taxonomy","Visualization"],"keywords_other":["Machine to machines","Scientometrics","Cyber physical systems (CPSs)","E-governance","ICT domain"],"max_cite":1.0,"pub_year":2016.0,"sources":"['scp']","rawkeys":["big data","embedded systems","internet of things","visualization","taxonomy","machine learning","multi agent systems","mobile computing","information security","scientometrics","scientometric databases","neural networks","cyber physical systems (cpss)","augmented reality","5g","cyber-physical systems","human-machine systems","cloud computing","e-governance","ict domain","machine to machines","machine to machine","robotics","bioinformatics","sdn"],"tags":["big data","embedded systems","visualization","internet of things (iot)","taxonomy","machine learning","e-government","mobile computing","information security","scientometrics","scientometric databases","neural networks","augmented reality","multi-agent systems","5g","cyber-physical systems","human-machine systems","cloud computing","software-defined networking","ict domain","machine to machines","robotics","bioinformatics"]},{"p_id":38180,"title":"A Distributed Anomaly Detection Method of Operation Energy Consumption Using Smart Meter Data","abstract":"\u00a9 2015 IEEE.Along with the rapid development of communication network construction, the operation energy consumption grows significantly in recent years, and the expensive electricity cost is hard to be ignored. Therefore, it is necessary to develop an operation energy anomaly detection mechanism to enhance the control ability of electricity cost. According to the practical distribution and data characteristic of smart meters, this paper presents a distributed anomaly detection method of operation energy consumption based on deep learning methods. An IOT-based distributed structure is implemented to execute data interaction. Stacked sparse autoencoder is used to extract the high-level representation from massive monitoring data acquired automatically from actual smart meter network. Then softmax is used for classification to detect anomaly and send alarm messages using web technologies. The experimental results show that the proposed method with good prospect for intelligent applications achieves better accuracy and meanwhile decreases computing delay caused by central arithmetic method.","keywords_author":["anomaly detection","deep learning","IOT","operation consumption","smart meter","stacked sparse autoencoder"],"keywords_other":["Deep learning","operation consumption","Anomaly detection","Distributed anomaly detection","Intelligent applications","Auto encoders","Data characteristics","Distributed structures"],"max_cite":1.0,"pub_year":2016.0,"sources":"['scp']","rawkeys":["distributed anomaly detection","operation consumption","anomaly detection","deep learning","intelligent applications","data characteristics","stacked sparse autoencoder","auto encoders","smart meter","distributed structures","iot"],"tags":["distributed anomaly detection","operation consumption","anomaly detection","intelligent applications","stacked sparse autoencoder","data characteristics","auto encoders","machine learning","smart meter","distributed structures","internet of things (iot)"]},{"p_id":50481,"title":"Experience-oriented knowledge management for internet of things","abstract":"\u00a9 Springer International Publishing Switzerland 2016.In this paper, we propose a novel approach for knowledge management in Internet of Things. By utilizing Decisional DNA and deep learning technologies, our approach enables Internet of Things of experiential knowledge discovery, representation, reuse, and sharing among each other. Rather than using traditional machine learning and knowledge discovery methods, this approach focuses on capturing domain\u2019s decisional events via Decisional DNA, and abstracting knowledge through deep learning process based on captured events data. The Decisional DNA is a flexible, domain-independent, and standard experiential knowledge repository solution that allows knowledge to be represented, reused, and easily shared. The main features, architecture, and an initial experiment of this approach are introduced. The presented conceptual approach demonstrates how knowledge can be discovered through its domain\u2019s experiences, and stored and shared as Decisional DNA.","keywords_author":["Decisional DNA","Deep learning","Experience-Oriented Smart Things","Internet of Things","Knowledge representation"],"keywords_other":null,"max_cite":0.0,"pub_year":2016.0,"sources":"['scp']","rawkeys":["decisional dna","deep learning","internet of things","experience-oriented smart things","knowledge representation"],"tags":["decisional dna","machine learning","experience-oriented smart things","knowledge representation","internet of things (iot)"]},{"p_id":27953,"title":"CONDENSE: A Reconfigurable Knowledge Acquisition Architecture for Future 5G IoT","abstract":"\u00ef\u00bf\u00bd 2016 IEEE.In forthcoming years, the Internet of Things (IoT) will connect billions of smart devices generating and uploading a deluge of data to the cloud. If successfully extracted, the knowledge buried in the data can significantly improve the quality of life and foster economic growth. However, a critical bottleneck for realizing the efficient IoT is the pressure it puts on the existing communication infrastructures, requiring transfer of enormous data volumes. Aiming at addressing this problem, we propose a novel architecture dubbed Condense which integrates the IoT-communication infrastructure into the data analysis. This is achieved via the generic concept of network function computation. Instead of merely transferring data from the IoT sources to the cloud, the communication infrastructure should actively participate in the data analysis by carefully designed en-route processing. We define the Condense architecture, its basic layers, and the interactions among its constituent modules. Furthermore, from the implementation side, we describe how Condense can be integrated into the Third Generation Partnership Project (3GPP) machine type communications (MTCs) architecture, as well as the prospects of making it a practically viable technology in a short time frame, relying on network function virtualization and software-defined networking. Finally, from the theoretical side, we survey the relevant literature on computing atomic functions in both analog and digital domains, as well as on function decomposition over networks, highlighting challenges, insights, and future directions for exploiting these techniques within practical 3GPP MTC architecture.","keywords_author":["big data","Internet of things (IoT)","machine learning","network coding","network function computation","Wireless communications"],"keywords_other":["Machine type communications","Internet of thing (IOT)","Third generation partnership project (3GPP)","Communication infrastructure","Network functions","Knowledge acquisition architectures","Wireless communications","Internet of Things (IOT)"],"max_cite":5.0,"pub_year":2016.0,"sources":"['scp']","rawkeys":["big data","knowledge acquisition architectures","internet of thing (iot)","machine learning","communication infrastructure","machine type communications","network functions","wireless communications","network coding","network function computation","third generation partnership project (3gpp)","internet of things (iot)"],"tags":["big data","knowledge acquisition architectures","machine learning","communication infrastructure","machine type communications","network functions","wireless communications","network coding","network function computation","third generation partnership project (3gpp)","internet of things (iot)"]},{"p_id":32049,"title":"IoT service platform enhancement through 'in-situ' machine learning of real-world knowledge","abstract":"With Machine-to-Machine and Internet of Things getting beyond hype, including an ever wider range of connected device types in ever more value-added services, a new era of data (and multimedia) stream-intensive services is emerging. While live data is massively becoming available, turning it into meaningful information that is not only actionable for decision makers, but also can be leveraged as a behavioral service property, or even reused across services, is a challenge that demands a systematic approach. In this paper we propose such systematic approach, towards establishing an Internet of Things service platform architecture that leverages real-world knowledge for faster service creation and more efficient execution. Illustrated by example scenarios, we go further beyond this, proposing a method to systematically leverage machine learning techniques for revising, improving or ultimately semi-automatically extending this real-world knowledge 'in-situ', i.e. during system operation, leveraging real-world observation in-context of requested service execution. \u00a9 2013 IEEE.","keywords_author":["cognitive feedback loop","IoT service platform","machine learning","real world phenomena","service creation"],"keywords_other":["Cognitive feedback","Service creation","Value added service","Machine to machines","Internet of Things (IOT)","real world phenomena","Iot services","Machine learning techniques"],"max_cite":3.0,"pub_year":2013.0,"sources":"['scp']","rawkeys":["cognitive feedback","iot services","iot service platform","machine learning techniques","machine learning","service creation","machine to machines","real world phenomena","cognitive feedback loop","value added service","internet of things (iot)"],"tags":["cognitive feedback","iot services","iot service platform","machine learning techniques","machine learning","service creation","machine to machines","real world phenomena","cognitive feedback loop","value added service","internet of things (iot)"]},{"p_id":38196,"title":"K-Degree layer-wise network for geo-distributed computing between cloud and IoT","abstract":"Copyright \u00a9 2016 The Institute of Electronics, Information and Communication Engineers. In this paper, we propose a novel architecture for a deep learning system, named k-degree layer-wise network, to realize efficient geo-distributed computing between Cloud and Internet of Things (IoT). The geo-distributed computing extends Cloud to the geographical verge of the network in the neighbor of IoT. The basic ideas of the proposal include a k-degree constraint and a layer-wise constraint. The k-degree constraint is defined such that the degree of each vertex on the h-th layer is exactly k(h) to extend the existing deep belief networks and control the communication cost. The layer-wise constraint is defined such that the layer-wise degrees are monotonically decreasing in positive direction to gradually reduce the dimension of data. We prove the k-degree layer-wise network is sparse, while a typical deep neural network is dense. In an evaluation on the Mdistributed MNIST database, the proposal is superior to a state-of-The-Art model in terms of communication cost and learning time with scalability.","keywords_author":["Big Data","Cloud Computing","Deep Learning","Geo-Distributed Computing","Internet Of Things"],"keywords_other":["Deep learning","Degree constraints","Communication cost","Novel architecture","Deep belief networks","Internet of Things (IOT)","State of the art","Deep neural networks"],"max_cite":1.0,"pub_year":2016.0,"sources":"['scp', 'ieee']","rawkeys":["novel architecture","cloud computing","communication cost","deep learning","big data","deep neural networks","state of the art","internet of things","geo-distributed computing","degree constraints","deep belief networks","internet of things (iot)"],"tags":["novel architecture","cloud computing","communication cost","big data","state of the art","machine learning","geo-distributed computing","convolutional neural network","degree constraints","deep belief networks","internet of things (iot)"]},{"p_id":27957,"title":"A multi-agent architecture for quantified fruits: Design and experience","abstract":"The concept of Quantified Self is about connected objects self-monitoring their human owner (e.g., a watch measuring heart rate, etc.). A natural transposition is in self-monitoring arbitrary things, therefore named Quantified Things. In this paper, we present the case of self-monitoring agricultural products. We discuss the rationales for the design of a Quantified Fruit multi-agent architecture for self-monitoring and self-prediction of the maturation of fruits. The architecture includes 6 different types of agents, the 2 more specific ones being respectively, the self-controller equipped with various sensors and the self-prediction module. Our current implementation uses an Arduino microcontroller board with 5 sensors (measuring respectively: temperature, light, humidity, hydrogen and methane). The prediction module uses a neural network. We have implemented the architecture and have conducted various experiments, storing bananas in diverse settings: room, refrigerator, in a box, with other fruits, etc. The paper discusses the architecture, its current implementation, experiments and current results. Future issues (scalability, collaborative prediction, etc.) are also addressed.","keywords_author":["Agent","Architecture","Banana","Design","Fruit","Implementation","Internet of things","Logistics","Machine learning","Maturation","Microcontroller","Monitoring","Multi-agent system","Neural network","Prediction","Quantified self","Quantified things","Software"],"keywords_other":["Quantified self","Banana","Quantified things","Maturation","Implementation"],"max_cite":5.0,"pub_year":2016.0,"sources":"['scp']","rawkeys":["design","neural network","multi-agent system","agent","fruit","internet of things","machine learning","monitoring","maturation","prediction","software","implementation","quantified self","microcontroller","quantified things","banana","architecture","logistics"],"tags":["design","agent","neural networks","fruit","machine learning","prediction","monitoring","maturation","software","implementation","multi-agent systems","quantified self","microcontroller","quantified things","banana","architecture","internet of things (iot)","logistics"]},{"p_id":36152,"title":"Device-free activity recognition using CSI & big data analysis: A survey","abstract":"\u00a9 2017 IEEE. This paper proposes a technique to recognize human motion and distinguish people using CSI, a big data analysis technique. CSI is used to develop MIMO technology. We measure the waveform of the subcarriers that are exchanged during communication between WiFi devices, and create CSI information, which is then used as an input for a big data algorithm and provides device-free based information that can extract characteristic information about repeated human behavior. We propose a technique to recognize human motion and distinguish people using the Deep Learning algorithm for extracted CSI big data.","keywords_author":["Chaneel State Information","Human Behavior","IoT","MIMO","Universal Software Radio Peripheral"],"keywords_other":["Universal Software Radio Peripheral","Activity recognition","Human motions","Data analysis techniques","Human behaviors","MIMO technology","Data algorithm","State information"],"max_cite":2.0,"pub_year":2017.0,"sources":"['wos', 'scp', 'ieee']","rawkeys":["data analysis techniques","human behaviors","data algorithm","human behavior","activity recognition","mimo","mimo technology","human motions","state information","universal software radio peripheral","iot","chaneel state information"],"tags":["data analysis techniques","human behaviors","data algorithm","activity recognition","mimo","mimo technology","human motions","state information","universal software radio peripheral","chaneel state information","internet of things (iot)"]},{"p_id":48451,"title":"Experience-Oriented Intelligence for Internet of Things","abstract":"\u00a9 2017 Taylor & Francis Group, LLC. The Internet of Things (IoT) has gained significant attention from industry as well as academia during the past decade.The main reason behind this interest is the capabilities of the IoT for seamlessly integrating classical networks and networked objects, and hence allowing people to create an intelligent environment based on this powerful integration. However, how to extract useful information from data produced by IoT and facilitate standard knowledge sharing among different IoT systems are still open issues to be addressed. In this paper, we propose a novel approach, the Experience-Oriented Smart Things (EOST), that utilizes deep learning and knowledge representation concept called Decisional DNA to help IoT systems acquire, represent, and store knowledge, as well as share it amid various domains where it can be required to support decisions. Decisional DNA motivation stems from the role of deoxyribonucleic acid (DNA) in storing and sharing information and knowledge. We demonstrate our approach in a set of experiments, in which the IoT systems use knowledge gained from past experience to make decisions and predictions. The presented initial results show that the EOST is a very promising approach for knowledge capture, representation, sharing, and reusing in IoT systems.","keywords_author":["Decisional DNA","deep learning","internet of things","knowledge representation","Decisional DNA","deep learning","internet of things","knowledge representation"],"keywords_other":["Knowledge capture","Decisional DNA","DECISIONAL DNA","Internet of thing (IOT)","KNOWLEDGE STRUCTURE","SETS","REPRESENTATION","ALGORITHM","SYSTEMS","Sharing information","DESIGN","NEURAL-NETWORKS","TECHNOLOGY","Intelligent environment","ONTOLOGY","Knowledge-sharing","Networked objects"],"max_cite":0.0,"pub_year":2017.0,"sources":"['scp', 'wos']","rawkeys":["knowledge structure","design","sharing information","decisional dna","internet of thing (iot)","internet of things","sets","technology","neural-networks","knowledge representation","intelligent environment","algorithm","deep learning","ontology","representation","knowledge-sharing","knowledge capture","networked objects","systems"],"tags":["design","knowledge representation","network objects","sharing information","decisional dna","neural networks","knowledge structures","machine learning","knowledge-sharing","representation","system","knowledge capture","sets","technology","algorithms","internet of things (iot)","intelligent environment"]},{"p_id":36172,"title":"Optimization of non-functional properties in Internet of Things applications","abstract":"\u00a9 2017 Elsevier LtdA major challenge in designing Internet of Things (IoT) systems is to meet various non-functional requirements such as lifetime, reliability, throughput, delay, and so forth. Furthermore, IoT systems tend to have competing requirements, which exacerbate these design challenges. We analyze this problem in detail and propose a model-driven approach to optimize an IoT application regarding to its non-functional requirements. Our approach defines optimizing as finding the best set of adjustable application parameters, which satisfies a given objective function. The relevant parameters are extracted during a simulation process. We apply a source code transformation that updates the source code with the generated adjustable parameter values and executes the compiler to create a new binary image of the application. Our experiment results demonstrate that non-functional requirements such as power consumption and reliability can be improved substantially during the optimization process.","keywords_author":["Internet of Things","Non-functional requirements","Optimization","Sensor networks","Simulation"],"keywords_other":["Application parameters","Simulation","Competing requirements","Source code transformation","Non-functional requirements","Non functional properties","Adjustable parameters","Internet of Things (IOT)"],"max_cite":2.0,"pub_year":2017.0,"sources":"['scp']","rawkeys":["adjustable parameters","simulation","non functional properties","source code transformation","internet of things","application parameters","competing requirements","optimization","non-functional requirements","sensor networks","internet of things (iot)"],"tags":["adjustable parameters","simulation","non functional properties","source code transformation","application parameters","competing requirements","optimization","non-functional requirements","sensor networks","internet of things (iot)"]},{"p_id":42318,"title":"Identification of malicious activities in industrial internet of things based on deep learning models","abstract":"\u00a9 2018 Elsevier Ltd Internet Industrial Control Systems (IICSs) that connect technological appliances and services with physical systems have become a new direction of research as they face different types of cyber-attacks that threaten their success in providing continuous services to organizations. Such threats cause firms to suffer financial and reputational losses and the stealing of important information. Although Network Intrusion Detection Systems (NIDSs) have been proposed to protect against them, they have the difficult task of collecting information for use in developing an intelligent NIDS which can proficiently detect existing and new attacks. In order to address this challenge, this paper proposes an anomaly detection technique for IICSs based on deep learning models that can learn and validate using information collected from TCP\/IP packets. It includes a consecutive training process executed using a deep auto-encoder and deep feedforward neural network architecture which is evaluated using two well-known network datasets, namely, the NSL-KDD and UNSW-NB15. As the experimental results demonstrate that this technique can achieve a higher detection rate and lower false positive rate than eight recently developed techniques, it could be implemented in real IICS environments.","keywords_author":["Auto-encoder","Deep learning","Industrial internet of things (IIoT)","Internet industrial control systems (IICSs)"],"keywords_other":["False positive rates","Continuous services","Anomaly detection","Industrial internets","Auto encoders","Industrial control systems","Network intrusion detection systems","Malicious activities"],"max_cite":0.0,"pub_year":2018.0,"sources":"['scp']","rawkeys":["auto-encoder","industrial control systems","industrial internet of things (iiot)","anomaly detection","deep learning","industrial internets","internet industrial control systems (iicss)","auto encoders","false positive rates","continuous services","malicious activities","network intrusion detection systems"],"tags":["industrial control systems","anomaly detection","industrial internets","internet industrial control systems (iicss)","false positive rates","auto encoders","machine learning","continuous services","malicious activities","network intrusion detection systems","internet of things (iot)"]},{"p_id":19794,"title":"The role of big data analytics in Internet of Things","abstract":"\u00a9 2017 Elsevier B.V. The explosive growth in the number of devices connected to the Internet of Things (IoT) and the exponential increase in data consumption only reflect how the growth of big data perfectly overlaps with that of IoT. The management of big data in a continuously expanding network gives rise to non-trivial concerns regarding data collection efficiency, data processing, analytics, and security. To address these concerns, researchers have examined the challenges associated with the successful deployment of IoT. Despite the large number of studies on big data, analytics, and IoT, the convergence of these areas creates several opportunities for flourishing big data and analytics for IoT systems. In this paper, we explore the recent advances in big data analytics for IoT systems as well as the key requirements for managing big data and for enabling analytics in an IoT environment. We taxonomized the literature based on important parameters. We identify the opportunities resulting from the convergence of big data, analytics, and IoT as well as discuss the role of big data analytics in IoT applications. Finally, several open challenges are presented as future research directions.","keywords_author":["Analytics","Big data","Distributed computing","Internet of Things","Smart city","Internet of Things","Big data","Analytics","Distributed computing","Smart city"],"keywords_other":["Internet of thing (IOT)","Analytics","DATA-MANAGEMENT","IOT applications","CHALLENGES","PERSPECTIVES","Data collection","Explosive growth","FRAMEWORK","MOBILE","IOT","Exponential increase","Future research directions","SMART CITY","DATA PROVENANCE","Data analytics"],"max_cite":19.0,"pub_year":2017.0,"sources":"['ieee', 'scp', 'wos']","rawkeys":["big data","explosive growth","data analytics","internet of thing (iot)","internet of things","challenges","iot applications","future research directions","smart city","perspectives","mobile","exponential increase","analytics","distributed computing","framework","iot","data collection","data provenance","data-management"],"tags":["distributed computing","big data","explosive growth","data analytics","framework","future research directions","mobile","perspective","challenges","smart cities","data collection","data management","iot applications","data provenance","exponential increase","internet of things (iot)","analytics"]},{"p_id":19795,"title":"Big data's role in expanding access to financial services in China","abstract":"\u00a9 2015 Elsevier Ltd. All rights reserved.General consumer and business finance companies have had limited success in serving the needs of economically active low-income families and micro-enterprises cost-effectively and sustainably in emerging economies such as China. Recent advances in computing and telecommunications technology are dramatically transforming this landscape by changing the way the financial industry operates. A key mechanism underlying this transformation concerns the use of big data in assessing, evaluating and refining the creditworthiness of potential borrowers and reducing the transaction costs. While China's internet-only banking industry is currently small and some activities of players in this industry are akin to those in the shadow banking, this industry has potential to cause a major disruption in the Chinese financial market. A main objective of this paper is to examine the role of big data in facilitating the access to financial products for economically active low-income families and micro-enterprises in China. A second objective is to investigate how formal and informal institutions facilitate and constrain the use of big data in the Chinese financial industry and market. The paper also investigates how various inherent characteristics of big data - volume, velocity, variety, variability and complexity - are related to the assessment of the creditworthiness of low-income families and micro-enterprises. Case studies of big data deployment in the Chinese financial industry and market are discussed. The paper also looks at various categories of personal financial and non-financial information that are being used as proxy measures for a potential borrower's identity, ability to repay and willingness to repay. Various business models involving the sources of data (internal vs. external to the big data organization) and providers of credits (big data organization vs. external partners or clients of the big data organization) are investigated. The analysis of the paper indicates that the main reason why low-income families and micro-enterprises in China and other emerging economies lack access to financial services is not because they lack creditworthiness but merely because banks and financial institutions lack data, information and capabilities to access the creditworthiness of and effectively provide financial services to this financial disadvantaged group.","keywords_author":["Big data","China","Creditworthiness","Emerging economies","Information opacity","Internet of things","Shadow banking"],"keywords_other":["Financial institution","Inherent characteristics","China","Non-financial informations","Shadow banking","Telecommunications technologies","Creditworthiness","Emerging economies"],"max_cite":19.0,"pub_year":2016.0,"sources":"['scp']","rawkeys":["big data","non-financial informations","shadow banking","internet of things","financial institution","information opacity","emerging economies","telecommunications technologies","creditworthiness","inherent characteristics","china"],"tags":["big data","non-financial informations","shadow banking","financial institution","internet of things (iot)","information opacity","emerging economies","telecommunications technologies","creditworthiness","inherent characteristics","china"]},{"p_id":50524,"title":"Software design and optimization of ECG signal analysis and diagnosis for embedded IoT devices","abstract":"\u00a9 Springer International Publishing Switzerland 2017. The medical domain is one of the most rapidly expanding application areas of Internet of Things (IoT) technology. For chronic diseases, this technology can be highly useful for the patient, providing constant monitoring and ability for timely intervention of medical staff in case of an emergency. This intended system behavior imposes new requirements to the design and implementation of processing flows implemented on embedded IoT devices which are already constrained by limited computational capabilities and power budget. This work aims at designing and implementing such a bio-medical signal analysis flow based on the case study of arrhythmia detection using electrocardiogram signals and machine learning techniques. Different architectural decisions of the flow are explored at high level and the final optimized version is implemented on a state-of-the-art IoT node. The evaluation of the execution flow on this device provides information on the actual requirements of each sub-component of the flow combined with an analysis of its behavior as computational requirements of the machine learning algorithms scale up.","keywords_author":["Arrhythmia detection","Discrete wavelet transform (DWT)","ECG analysis","Intel galileo IoT node","Machine learning","Support-vector-machine (SVM)"],"keywords_other":["Design and implementations","Electrocardiogram signal","Arrhythmia detection","Internet of Things (IOT)","Computational requirements","GALILEO","ECG analysis","Machine learning techniques"],"max_cite":0.0,"pub_year":2016.0,"sources":"['scp']","rawkeys":["intel galileo iot node","arrhythmia detection","discrete wavelet transform (dwt)","support-vector-machine (svm)","galileo","computational requirements","design and implementations","machine learning techniques","machine learning","ecg analysis","electrocardiogram signal","internet of things (iot)"],"tags":["intel galileo iot node","arrhythmia detection","galileo","computational requirements","design and implementations","machine learning techniques","machine learning","discrete wavelet transform","ecg analysis","electrocardiogram signal","internet of things (iot)"]},{"p_id":42334,"title":"Toy-IoT-Oriented data-driven CDN performance evaluation model with deep learning","abstract":"\u00a9 2018 Elsevier B.V. Content Delivery Network(CDN) is geographically distributed network of cache servers. It can deliver the Internet content based on the users\u2019 geographic position and real-time quality of service(QoS). Now with the rapid development of the Internet of things, IoT also needs CDN acceleration. Therefore, CDN not only needs to serve people, but also needs to serve IoT, such as sensors,toys. Especially for toy computing, CDN need to sink further to support the uplink data transmission. Because the performance of CDN is crucial to the resource management of existing platform and the acceleration of data transmission, CDN providers use different models to exactly describe the performance of CDN. Traditional models are linear models or need to be adjusted manually. These normally existing drawbacks make the methods hardly to be applied stably. Recently, deep learning(DL) has made great breakthroughs in solving many problems. We use the RNN in deep learning to model the CDN performance. Our design can exactly capture the nonlinear relationship between high-dimensional machine data and the CDN performance. And it can also realize the correct prediction of the reach rate which is the CDN\u2019 main performance evaluation index in our design.The experimental results have shown that our model is able to outperform state-of-the-arts models.","keywords_author":["Content delivery network","Data driven","Deep learning","Internet of things","Performance evaluation","Toy computing"],"keywords_other":["Non-linear relationships","Performance evaluations","Content delivery network","Data driven","Performance evaluation models","Real-time quality of services","Performance evaluation index","Toy computing"],"max_cite":0.0,"pub_year":2018.0,"sources":"['scp']","rawkeys":["toy computing","deep learning","internet of things","non-linear relationships","performance evaluation index","performance evaluations","performance evaluation","real-time quality of services","data driven","performance evaluation models","content delivery network"],"tags":["toy computing","machine learning","non-linear relationships","performance evaluation index","performance evaluation","real-time quality of services","data driven","performance evaluation models","content delivery network","internet of things (iot)"]},{"p_id":34154,"title":"A real-time autonomous highway accident detection model based on big data processing and computational intelligence","abstract":"\u00a9 2016 IEEE. Due to increasing urban population and growing number of motor vehicles, traffic congestion is becoming a major problem of the 21st century. One of the main reasons behind traffic congestion is accidents which can not only result in casualties and losses for the participants, but also in wasted and lost time for the others that are stuck behind the wheels. Early detection of an accident can save lives, provides quicker road openings, hence decreases wasted time and resources, and increases efficiency. In this study, we propose a preliminary real-time autonomous accident-detection system based on computational intelligence techniques. Istanbul City traffic-flow data for the year 2015 from various sensor locations are populated using big data processing methodologies. The extracted features are then fed into a nearest neighbor model, a regression tree, and a feed-forward neural network model. For the output, the possibility of an occurrence of an accident is predicted. The results indicate that even though the number of false alarms dominates the real accident cases, the system can still provide useful information that can be used for status verification and early reaction to possible accidents.","keywords_author":["accident detection","big data","computational intelligence","intelligent transportation systems","IoT","machine learning","nearest neighbor","neural networks","regression tree","sensors","Traffic flow"],"keywords_other":["Nearest neighbors","Intelligent transportation systems","Traffic flow","Regression trees","Accident detections"],"max_cite":2.0,"pub_year":2016.0,"sources":"['scp']","rawkeys":["traffic flow","regression tree","accident detections","big data","neural networks","nearest neighbor","sensors","machine learning","accident detection","intelligent transportation systems","computational intelligence","iot","regression trees","nearest neighbors"],"tags":["traffic flow","accident detections","big data","neural networks","sensors","machine learning","intelligent transportation systems","computational intelligence","regression trees","internet of things (iot)"]},{"p_id":42360,"title":"A Scalable, Research Oriented, Generic, Sensor Data Platform","abstract":"OAPA Research interests spanning numerous domains increasingly rely upon computational systems which can store and process a large volume of variable data that is stored at high velocity &#x2013; representing a big data problem. This is particularly notable within the domain of ubiquitous and pervasive computing. This domain increasingly relies on storage and retrieval of sensor data to enable outcomes such as predictive analytics and activity recognition. Several current big data platforms exist; however, they have a range of deficiencies including lack of generic interoperability with agnostic sensors and an absence of features supporting academic research. Due to these deficiencies a custom, research oriented, high performance, big data platform was devised and implemented. This platform is called SensorCentral and is presented within this manuscript. SensorCentral provides a framework which enables interoperability with a large range of agnostic sensor devices whilst simultaneously providing features which support research. Research supporting features include; facility to define experiments, ability to annotate experimental instances via purposebuilt mobile applications, integrated machine learning functionality, facility to export data sets, rule-based classification and an extensible platform. The flagship implementation of this platform has been in operation for over 28 months within a University research group and has been successfully integrated with a range of sensors from a variety of manufacturers. This implementation currently stores over 850 million records and has been central to several research and industrial projects. Future work will integrate this platform into the Open Data Initiative enabling collaboration with the international community of researchers.","keywords_author":["Big Data","Bluetooth","Data analysis","Data storage systems","Database systems","Databases","Internet of Things","LoRa","Machine learning","Memory","Open Data Initiative","Radio frequency","Research tools","Sensor systems","Thermal sensors","Wireless fidelity","Wireless sensor networks"],"keywords_other":["Wireless fidelities","Open datum","Thermal sensors","Research tools","Radio frequencies","LoRa","Data storage systems","Sensor systems"],"max_cite":0.0,"pub_year":2018.0,"sources":"['scp']","rawkeys":["memory","big data","databases","internet of things","lora","research tools","wireless fidelities","wireless sensor networks","machine learning","sensor systems","bluetooth","open data initiative","wireless fidelity","radio frequency","open datum","thermal sensors","database systems","data storage systems","radio frequencies","data analysis"],"tags":["database systems","memory","big data","databases","open datum","lora","machine learning","random forests","research tools","data storage systems","sensor systems","bluetooth","wireless fidelities","open data initiative","thermal sensors","wireless sensor networks","data analysis","internet of things (iot)"]},{"p_id":9593,"title":"Enabling Deep Learning on IoT Devices","abstract":"Deep learning can enable Internet of Things (IoT) devices to interpret unstructured multimedia data and intelligently react to both user and environmental events but has demanding performance and power requirements. The authors explore two ways to successfully integrate deep learning with low-power IoT products.","keywords_author":["cloud computing","deep learning","embedded devices","Internet of Things","IoT","machine learning","The IoT Connection"],"keywords_other":["Two ways","Low Power","Multimedia data","Power requirement","The IoT Connection","Internet of Things (IOT)","Embedded device"],"max_cite":3.0,"pub_year":2017.0,"sources":"['ieee', 'scp', 'wos']","rawkeys":["embedded devices","multimedia data","cloud computing","the iot connection","deep learning","two ways","internet of things","machine learning","power requirement","iot","low power","embedded device","internet of things (iot)"],"tags":["multimedia data","cloud computing","the iot connection","two ways","machine learning","power requirement","low power","embedded device","internet of things (iot)"]},{"p_id":48504,"title":"Knock knock to unlock: A human centered novel authentication method for secure system fluidity","abstract":"When a person gets to a door and wants to get in, what do they do? They knock. In our system, the user's specific knock pattern authenticates their identity, and opens the door for them. The system empowers people's intuitive actions and responses to affect the world around them in a new way. We leverage IOT, and physical computing to make more technology feel like less. From there, the system of a knock based entrance creates affordances in social interaction for shared spaces wherein ownership fluidity and accessibility needs to be balanced with security.","keywords_author":["Automated locking","Human centered design","Hybrid digital and physical","Internet of things","Knocking to unlock","Machine learning","Programmed sound and vibration threshold","Rhythm","Social affordance","User interface becomes environment"],"keywords_other":["Social-affordance","Automated locking","Vibration thresholds","Knocking to unlock","Hybrid digital and physical","Human-centered designs","Rhythm"],"max_cite":0.0,"pub_year":2017.0,"sources":"['scp']","rawkeys":["rhythm","social-affordance","human centered design","programmed sound and vibration threshold","internet of things","machine learning","vibration thresholds","automated locking","human-centered designs","hybrid digital and physical","social affordance","user interface becomes environment","knocking to unlock"],"tags":["rhythm","social-affordance","programmed sound and vibration threshold","machine learning","vibration thresholds","automated locking","human-centered designs","hybrid digital and physical","user interface becomes environment","internet of things (iot)","knocking to unlock"]},{"p_id":48505,"title":"A Study on the Virtuous Circle Self-Learning Methods for Knowledge Enhancement","abstract":"\u00a9 2017 IEEE. Recently, along with the development of ICT technology, there is a significant increase in Internet of Everything (IoE) more advanced than Internet of Things (IoT) is one of the technologies that not only connects people and things, data and services, but also provides users with more intelligent and smart services However, it is difficult to efficiently process data generated from various kinds of things and services in this IoT environment. Also, it is difficult to provide objective analysis and knowledge-based services in an adaptive manner in response to IoE environment changes. In this paper, we propose a virtuous circle based knowledge convergence and extension model to accommodate the above intrinsic requirements. The proposed model exploits 1) the advanced information during preprocessing phase got by analyzing the high volume data of various IoE devices on real-time and 2) gives machine learning based learning models and their results during learning model phase to support the decision making accurately. These self-learning and learning results can be generated, converged, inferenced, and expanded with new knowledge of data processing and learning. Finally, our system has the added advantage of providing knowledge and understanding of physical things, virtual things, domains and services, and analysis information so that the user can easily understand them.","keywords_author":["Deep Learning","Internet of Everything","Knowledge Enhancement","Machine Learning","Virtuous Self-Learning"],"keywords_other":["Preprocessing phase","Knowledge based","Extension models","Advanced informations","Objective analysis","Internet of Things (IOT)","Environment change","Self-learning"],"max_cite":0.0,"pub_year":2017.0,"sources":"['scp', 'ieee']","rawkeys":["knowledge based","preprocessing phase","internet of everything","knowledge enhancement","self-learning","deep learning","extension models","machine learning","environment change","virtuous self-learning","objective analysis","internet of things (iot)","advanced informations"],"tags":["preprocessing phase","self-learning","knowledge enhancement","extension models","machine learning","knowledge base","environment change","virtuous self-learning","objective analysis","internet of things (iot)","advanced informations"]},{"p_id":42365,"title":"ResInNet: A Novel Deep Neural Network with Feature Re-use for Internet of Things","abstract":"IEEE Deep neural networks (DNN) have widely used in various Internet-of-Things (IoT) applications. Pursuing superior performance is always a hot spot in the field of DNN modeling. Recently, feature re-use provides an effective means of achieving favorable nonlinear approximation performance in deep learning. Existing implementations utilizes a multilayer perception (MLP) to act as a functional unit for feature re-use. However, determining connection weight and bias of MLP is a rather intractable problem, since the conventional back-propagation learning approach encounters the limitations of slow convergence and local optimum. To address this issue, this paper develops a novel deep neural network considering a well-behaved alternative called reservoir computing, i.e., reservoir in network (ResInNet). In this structure, the built-in reservoir has two notable functions. First, it behaves as a bridge between any two restricted Boltzmann machines in the feature learning part of ResInNet, performing a feature abstraction once again. Such reservoir-based feature translation provides excellent starting points for the following nonlinear regression. Second, it serves as a nonlinear approximation, trained by a simple linear regression using the most representative (learnt) features. Experimental results over various benchmark datasets show that ResInNet can achieve the superior nonlinear approximation performance in comparison to the baseline models, and produce the excellent dynamic characteristics and memory capacity. Meanwhile, the merits of our approach is further demonstrated in the network traffic prediction related to real-world IoT application.","keywords_author":["Computational modeling","deep belief network","Deep learning","Feature extraction","feature re-use","Internet of Things","Machine learning","Nonhomogeneous media","nonlinear approximation.","reservoir computing","Reservoirs","Training"],"keywords_other":["Nonlinear approximation","Nonhomogeneous media","Computational model","Reservoir Computing","Deep belief networks","feature re-use"],"max_cite":0.0,"pub_year":2018.0,"sources":"['scp']","rawkeys":["computational modeling","reservoirs","deep learning","training","nonlinear approximation","internet of things","machine learning","computational model","feature re-use","deep belief network","feature extraction","reservoir computing","nonhomogeneous media","deep belief networks"],"tags":["computational modeling","reservoirs","training","nonlinear approximation","machine learning","feature re-use","feature extraction","reservoir computing","nonhomogeneous media","deep belief networks","internet of things (iot)"]},{"p_id":23934,"title":"FIoT: An agent-based framework for self-adaptive and self-organizing applications based on the Internet of Things","abstract":"\u00a9 2016 Elsevier Inc. Billions of resources, such as cars, clothes, household appliances and even food are being connected to the Internet forming the Internet of Things (IoT). Subsets of these resources can work together to create new self-regulating IoT applications such as smart health, smart communities and smart homes. However, several challenging issues need to be addressed before this vision of applications based on IoT concepts becomes a reality. Because many IoT applications will be distributed over a large number of interacting devices, centralized control will not be possible and so open problems will need to be solved that relate to building locally operating self-organizing and self-adaptive systems. As an initial step in creating IoT applications with these features, this paper presents a Framework for IoT (FIoT). The approach is based on Multi-Agent Systems (MAS) and Machine Learning Techniques, such as neural networks and evolutionary algorithms. To illustrate the use of FIoT, the paper contains two different IoT applications: (i) Quantified Things and (ii) Smart traffic control. We show how flexible points of our framework are instantiated to generate these IoT application.","keywords_author":["Internet of things (IoT)","Machine learning","Multi-agent system","Quantified things","Self-adaptive","Self-organizing"],"keywords_other":["Internet of thing (IOT)","Agent-based framework","Quantified things","Self-adaptive","Self-organizing","Self-organizing applications","Internet of Things (IOT)","Machine learning techniques"],"max_cite":10.0,"pub_year":2017.0,"sources":"['scp']","rawkeys":["machine learning techniques","internet of thing (iot)","machine learning","self-organizing applications","self-organizing","agent-based framework","self-adaptive","quantified things","internet of things (iot)","multi-agent system"],"tags":["machine learning techniques","machine learning","self-organizing applications","multi-agent systems","agent-based framework","self-adaptive","self-organization","quantified things","internet of things (iot)"]},{"p_id":42374,"title":"HEIF: Highly Efficient Stochastic Computing based Inference Framework for Deep Neural Networks","abstract":"IEEE Deep Convolutional Neural Networks (DCNNs) are one of the most promising types of deep learning technique and have been recognized as the dominant approach for almost all recognition and detection tasks. The computation of DCNNs is highly computational and memory intensive for the large feature maps and neuron connections, and the performance highly depends on the capability of hardware resources. With the recent trend of wearable devices and Internet of Things (IoTs), it becomes attractive to integrate the DCNNs onto embedded and portable devices, which require low power &#x0026;amp; energy consumptions and small hardware footprints. Recent work SC-DCNN asplos demonstrates that Stochastic Computing (SC), as a low-cost substitute to binary-based computing, can radically simplify the hardware implementation of arithmetic units and has the potential to satisfy the stringent power requirements in embedded devices. In SC, many arithmetic operations that are resource-consuming in binary designs can be implemented with very simple hardware logic, alleviating the extensive computational complexity. It offers a colossal design space for integration and optimization due to its reduced area and soft error resiliency. In this paper, we present HEIF, a highly efficient SC-based inference framework of the large-scale DCNNs, with broad applications including (but not limited to) LeNet-5 and AlexNet, that achieves high energy efficiency and low area\/hardware cost. Compared to SC-DCNN asplos, HEIF features with 1) the first (to the best of our knowledge) SC-based Rectified Linear Unit (ReLU) activation function to catch up with the recent advances in software models and mitigate degradation in application-level accuracy; 2) the redesigned Approximate Parallel Counter (APC) and optimized stochastic multiplication using transmission gates and inverse mirror adders; and 3) the new optimization of weight storage using clustering. Most importantly, to achieve maximum energy efficiency while maintaining acceptable accuracy, HEIF considers holistic optimizations on cascade connection of function blocks in DCNN, pipelining technique, and bit-stream length reduction. Experimental results show that in large-scale applications HEIF outperforms previous SC-DCNN by the throughput of 4.1&#x00D7;, by area efficiency of up to 6.5&#x00D7; and achieves up to 5.6&#x00D7; energy improvement.","keywords_author":["ASIC","Convolutional Neural Network","Convolutional neural networks","Deep learning","Energy-efficient","Feature extraction","Hardware","Machine learning","Neurons","Optimization","Optimization.","Stochastic Computing"],"keywords_other":["Energy efficient","Stochastic computing","Hardware implementations","Holistic optimizations","Internet of thing (IoTs)","Large-scale applications","Convolutional neural network","Deep convolutional neural networks"],"max_cite":0.0,"pub_year":2018.0,"sources":"['scp']","rawkeys":["convolutional neural networks","holistic optimizations","deep learning","hardware implementations","large-scale applications","deep convolutional neural networks","asic","machine learning","stochastic computing","convolutional neural network","feature extraction","energy-efficient","hardware","internet of thing (iots)","optimization","neurons","energy efficient"],"tags":["holistic optimizations","large-scale applications","hardware implementations","asic","machine learning","feature extraction","convolutional neural network","optimization","hardware","stochastic computing","energy efficiency","neurons","internet of things (iot)"]},{"p_id":42375,"title":"A Spatial Multi-bit Sub-1V Time-Domain Matrix Multiplier Interface for Approximate Computing in 65nm CMOS","abstract":"IEEE Large scale parallel implementation of matrix multiply and accumulate (MAC) core poses significant energy and area constraints in analog voltage domain under reduced supply voltage. A spatial multi-bit sub-1V time-domain matrix multiplier interface is presented using multi-bit back-gate-driven delay elements as a scalable alternative for various approximate computing applications. A single-chip solution is demonstrated for two application modes: a high throughput digitally-driven mode for acceleration and a low energy analog front-end mode for sensing. In accelerate-mode, the system achieves an aggregate throughput of 21.6 GMAC\/s with 9 TOPS\/W energy efficiency. In sense-mode, the system exhibits an energy efficiency of 55.3 TOPS\/W for classification purpose. The proposed architecture utilizes 16-parallel 6-bit input vectors to perform matrix MAC computations using time-domain signal processing with 3-bit resistive weights at a sub-1V supply of 0.7 V. An integrated speculative time-to-digital converter (TDC) is employed for 6-bit time-domain quantization with an on-chip mismatch calibration scheme. The prototype is fabricated in 65 nm CMOS technology and occupies an active area of 0.04 mm2. The system performs image recognition of handwritten digits using a machine learning scheme and demonstrates an average classification accuracy of 84.3&#x0025; on the MNIST dataset. The resultant energy per MAC computation in the proposed spatial architecture is about 15&#x00D7; lower than a digital CMOS combinational logic based parallel-tree MAC.","keywords_author":["Acceleration","accelerator","Approximate computing","Approximate computing","Computer architecture","DTC","Energy efficiency","IoT","MAC","Machine Learning","Neuromorphic Computing","Sensors","Signal processing","Spatial","TDC","Time-domain analysis","Time-domain signal processing","VTC"],"keywords_other":["Time to digital converters","Approximate computing","Neuromorphic computing","65 nm CMOS technologies","Parallel implementations","Time-domain signal","Spatial","Classification accuracy"],"max_cite":0.0,"pub_year":2018.0,"sources":"['scp']","rawkeys":["sensors","vtc","time-domain analysis","mac","time-domain signal processing","accelerator","tdc","acceleration","machine learning","parallel implementations","time to digital converters","neuromorphic computing","approximate computing","time-domain signal","computer architecture","65 nm cmos technologies","dtc","spatial","signal processing","iot","energy efficiency","classification accuracy"],"tags":["sensors","video-transcoding","time-domain analysis","time-domain signal processing","medium access control","internet of things (iot)","tdc","acceleration","machine learning","parallel implementations","time to digital converters","neuromorphic computing","approximate computing","time-domain signal","computer architecture","65 nm cmos technologies","dtc","spatial","signal processing","energy efficiency","classification accuracy"]},{"p_id":42377,"title":"Comparison of data preprocessing approaches for applying deep learning to human activity recognition in the context of industry 4.0","abstract":"\u00a9 2018 by the authors. According to the Industry 4.0 paradigm, all objects in a factory, including people, are equipped with communication capabilities and integrated into cyber-physical systems (CPS). Human activity recognition (HAR) based on wearable sensors provides a method to connect people to CPS. Deep learning has shown surpassing performance in HAR. Data preprocessing is an important part of deep learning projects and takes up a large part of the whole analytical pipeline. Data segmentation and data transformation are two critical steps of data preprocessing. This study analyzes the impact of segmentation methods on deep learning model performance, and compares four data transformation approaches. An experiment with HAR based on acceleration data from multiple wearable devices was conducted. The multichannel method, which treats the data for the three axes as three overlapped color channels, produced the best performance. The highest overall recognition accuracy achieved was 97.20% for eight daily activities, based on the data from seven wearable sensors, which outperformed most of the other machine learning techniques. Moreover, the multichannel approach was applied to three public datasets and produced satisfying results for multi-source acceleration data. The proposed method can help better analyze workers\u2019 activities and help to integrate people into CPS.","keywords_author":["Data preprocessing","Deep learning","Human Activity Recognition (HAR)","Industry 4.0","Internet of things (IoT)"],"keywords_other":["Cyber-Physical System (CPS)","Communication capabilities","Human activity recognition","Data preprocessing","Recognition accuracy","Segmentation methods","Internet of Things (IOT)","Machine learning techniques"],"max_cite":0.0,"pub_year":2018.0,"sources":"['scp', 'ieee']","rawkeys":["communication capabilities","deep learning","machine learning techniques","human activity recognition","recognition accuracy","cyber-physical system (cps)","segmentation methods","data preprocessing","human activity recognition (har)","industry 4.0","internet of things (iot)"],"tags":["cyber-physical systems","communication capabilities","machine learning techniques","human activity recognition","machine learning","recognition accuracy","segmentation methods","data preprocessing","industry 4.0","internet of things (iot)"]},{"p_id":28054,"title":"Building an IoT framework for connected dairy","abstract":"\u00a9 2015 IEEE.Heat stress (HS) causes cows to produce less milk with the same nutritional input, which effectively increases farmers' production costs. The economic toll due to higher-temperature, heat stress is a $1 billion annual problems. Not only in the United States, but also around the globe heat stress causes an adverse impact on dairy productivity. The opportunities, however, for the dairy industry is to electronically monitor cattle temperature and implement appropriate measures so that the impact of HS can be minimized. The U.S. Department of Agriculture estimates nearly $2.4 billion a year in losses from animal illnesses that lead to death can be prevented by electronically checking on cattle's' vital signs. This research paper recommends the most innovative electronic monitor framework, the 'Smart Connected Objects', aka, 'the Internet of Things (IoT)', that enables dairies to minimize the economic impact of HS and, at the same, capture the higher Return on Assets (ROA) &amp; Return on Investment (ROI) by improving operational efficiencies. Happy Cow, more importantly, means happier, more profitable, dairy industry and richer and creamer dairy products. The proposed framework supports both offline and online dairy IoT. This paper presents a prototyping solution design as well as its application and certain experimental results.","keywords_author":["Android","BLE","Bluetooth Low Energy","CEP","Complex Event Processing","Dairy","Dairy","Decision Tree","Heat Stress","Internet Of Things","iOS","IoT","IoT reference architecture","Machine Learning","Memory Data Cube","Regression Analysis","Sensor Tag","streaming analytics"],"keywords_other":["Sensor tag","Android","Heat stress","Data cube","Bluetooth low energies (BTLE)","Reference architecture","Complex event processing"],"max_cite":5.0,"pub_year":2015.0,"sources":"['scp']","rawkeys":["bluetooth low energy","iot reference architecture","ble","internet of things","regression analysis","complex event processing","reference architecture","heat stress","machine learning","sensor tag","data cube","cep","bluetooth low energies (btle)","memory data cube","android","dairy","iot","streaming analytics","decision tree","ios"],"tags":["dairying","heat stress","reference architecture","iot reference architecture","neural networks","streaming analytics","machine learning","regression analysis","complex event processing","sensor tag","internet of things (iot)","data cube","memory data cube","bluetooth low energies (ble)","decision trees"]},{"p_id":48535,"title":"An overview of Next Generation Technologies and its applications","abstract":"\u00a9 2016 IEEE. The term Next Generation Technology has been often used by IT enthusiasts. Many see next generation technologies as one of the solution vectors for the global challenges of the 21st century. However, a small research has shed light on this term and specified its characteristics and meaning of it. Next generation technologies are critical to solve large problems faced by this world. Therefore, this paper aims to highlight the benefits of Next Generation Technology. The primary aim of this research paper was to highlight the importance of Next Generation Technology, to create awareness about its working principles and advantages and to inform its disadvantages. We have included the following concepts of the Next Generation Technology along with its implementing technology: Augmented Reality and Virtual Reality, Artificial Intelligence and Machine Learning, Autonomous Driving, Internet of Things, Modular Smartphones.","keywords_author":["Artificial intelligence","Augmented reality","Autonomous driving","Internet of things","Machine learning","Modular smartphones","Next generation technology","Virtual reality"],"keywords_other":["Generation technologies","ITS applications","Global challenges","Large problems","Research papers","Autonomous driving","Solution vectors"],"max_cite":0.0,"pub_year":2017.0,"sources":"['scp']","rawkeys":["large problems","artificial intelligence","modular smartphones","virtual reality","its applications","internet of things","machine learning","autonomous driving","solution vectors","generation technologies","augmented reality","research papers","next generation technology","global challenges"],"tags":["large problems","modular smartphones","virtual reality","its applications","machine learning","autonomous driving","internet of things (iot)","generation technologies","augmented reality","research papers","solution vectors","next generation technology","global challenges"]},{"p_id":19882,"title":"A Survey on Security and Privacy Issues in Internet-of-Things","abstract":"\u00a9 2014 IEEE. Internet-of-Things (IoT) are everywhere in our daily life. They are used in our homes, in hospitals, deployed outside to control and report the changes in environment, prevent fires, and many more beneficial functionality. However, all those benefits can come of huge risks of privacy loss and security issues. To secure the IoT devices, many research works have been conducted to countermeasure those problems and find a better way to eliminate those risks, or at least minimize their effects on the user's privacy and security requirements. The survey consists of four segments. The first segment will explore the most relevant limitations of IoT devices and their solutions. The second one will present the classification of IoT attacks. The next segment will focus on the mechanisms and architectures for authentication and access control. The last segment will analyze the security issues in different layers.","keywords_author":["Internet-of-Things (IoT)","privacy","security","survey"],"keywords_other":["Security issues","Security and privacy issues","security","Different layers","Privacy and security","Internet of Things (IOT)","Daily lives"],"max_cite":19.0,"pub_year":2017.0,"sources":"['ieee', 'scp', 'wos']","rawkeys":["privacy","survey","security and privacy issues","security issues","internet-of-things (iot)","security","different layers","privacy and security","internet of things (iot)","daily lives"],"tags":["privacy","survey","security and privacy issues","security issues","security","different layers","privacy and security","internet of things (iot)","daily lives"]},{"p_id":46508,"title":"Prediction of probability of crying of a child and system formation for cry detection and financial viability of the system","abstract":"\u00a9 2017 IEEE. Sometimes parents don't have resources or time to attend to their young ones as they have certain predispositions. This document demonstrates the process of construction of a web-service\/module, defines the algorithm, procedure of construction of the algorithm and the analysis\/results of the procedures performed. The market for this system is the working class nuclear families or single parents that are not present for their babies and have to take help from nannies to keep an eye for them. The algorithm constructed is itself build upon various algorithms that were developed in past and incorporated in the ML studio as modules so a dataset has been generated and utilized these modules to from an algorithm to predict the probability of a child's crying in next few hours based on the previous data that has been collected (randomly generated in this case). A module for creation of automatic machine is stated which augment a basic child cry monitor with Machine Learning and Cognitive services for faster cheaper and more reliable cloud based solution for parents.","keywords_author":["Algorithm","Artificial intelligence","Cloud Computing","Cognitive Services","Internet of Things","Machine learning","Microsoft Machine Learning Studio","Web Application"],"keywords_other":["Financial viability","WEB application","Automatic machines","Cloud-based","Cognitive Services","MicroSoft"],"max_cite":0.0,"pub_year":2017.0,"sources":"['scp']","rawkeys":["algorithm","artificial intelligence","web application","cloud computing","cloud-based","internet of things","machine learning","microsoft","financial viability","microsoft machine learning studio","cognitive services","automatic machines"],"tags":["web application","cloud computing","cloud-based","machine learning","microsoft","financial viability","microsoft machine learning studio","cognitive services","algorithms","automatic machines","internet of things (iot)"]},{"p_id":36279,"title":"A virtual server QoS enhancement method in cloud computing","abstract":"\u00a9 2016 ACM.Virtualization is one of the most promising solutions to create a flexible and powerful technology for cloud computing paradigm. Newly upcoming technologies such as Internet of Things (IoT) and cloud computing have brought so many new cloud-connected devices, resulting in the increased load of cloud computing. In such conditions, system reliability becomes a challenging task for cloud computing. In this paper, we propose a failure prediction and prevention model of hypervisors by using Bayes naive classifier. The advantage of this model is to give higher accuracy of prediction and early failure detection by exploiting real-time sensed data using past experience.","keywords_author":["Cloud computing","Failures prediction","Hypervisor clustering"],"keywords_other":["Prevention models","Naive classifiers","Failures prediction","System reliability","Hypervisor","Early failure detection","Failure prediction","Internet of Things (IOT)"],"max_cite":2.0,"pub_year":2016.0,"sources":"['scp']","rawkeys":["hypervisor","system reliability","cloud computing","failures prediction","hypervisor clustering","prevention models","naive classifiers","early failure detection","failure prediction","internet of things (iot)"],"tags":["hypervisor","system reliability","cloud computing","hypervisor clustering","prevention models","naive classifiers","early failure detection","failure prediction","internet of things (iot)"]},{"p_id":38329,"title":"SmartDeviceLink application to intelligent climate control","abstract":"\u00a9 Copyright 2016 by SCITEPRESS - Science and Technology Publications, Lda. All rights reserved. SmartDeviceLink (SDL) is an open-source software development kit (SDK) that enables a smart-device to connect to the vehicle, providing functions for safe and easy access to the vehicle human-machine interface (HMI) and the ability to programmatically control vehicle functions. This paper discusses a framework for developing intelligent control applications that implement personalized and context-aware features for automotive climate control systems. There is also a discussion of integration of wearables, internet-of-things (IoT) sensors, cloud and mobile machine learning.","keywords_author":["Air Quality","Climate Control","Connected Vehicles","Intelligent Control","Machine Learning"],"keywords_other":["Automotive climate control","Human Machine Interface","Control vehicles","Control applications","Smart devices","Mobile machines","Internet of Things (IOT)","Context-aware features"],"max_cite":1.0,"pub_year":2016.0,"sources":"['scp']","rawkeys":["control vehicles","air quality","context-aware features","control applications","climate control","machine learning","smart devices","connected vehicles","human machine interface","intelligent control","automotive climate control","internet of things (iot)","mobile machines"],"tags":["control vehicles","air quality","context-aware features","control applications","climate control","machine learning","smart devices","connected vehicles","human machine interface","intelligent control","automotive climate control","internet of things (iot)","mobile machines"]},{"p_id":30141,"title":"5G Internet of Things: A survey","abstract":"\u00a9 2018 The existing 4G networks have been widely used in the Internet of Things (IoT) and is continuously evolving to match the needs of the future Internet of Things (IoT) applications. The 5G networks are expected to massive expand today's IoT that can boost cellular operations, IoT security, and network challenges and driving the Internet future to the edge. The existing IoT solutions are facing a number of challenges such as large number of connection of nodes, security, and new standards. This paper reviews the current research state-of-the-art of 5G IoT, key enabling technologies, and main research trends and challenges in 5G IoT.","keywords_author":["5G","Internet of things (IoT)","Wireless communication"],"keywords_other":null,"max_cite":4.0,"pub_year":2018.0,"sources":"['scp']","rawkeys":["5g","internet of things (iot)","wireless communication"],"tags":["wireless communications","5g","internet of things (iot)"]},{"p_id":28099,"title":"Analytics, machine learning, and the internet of things","abstract":"\u00a9 1999-2012 IEEE. Our increasingly connected world, combined with low-cost sensors and distributed intelligence, will have a transformative impact on industry, producing more data than humans will be able to process. How will businesses adapt and evolve quickly enough to maintain their place in the competitive landscape? How will humans make sense of and benefit from these new sources of information and intelligence embedded in our environment?","keywords_author":["big data","data analysis","information technology","Internet of things","machine learning"],"keywords_other":["Distributed intelligence","Low-cost sensors","New sources"],"max_cite":5.0,"pub_year":2015.0,"sources":"['scp']","rawkeys":["big data","internet of things","machine learning","low-cost sensors","new sources","information technology","data analysis","distributed intelligence"],"tags":["big data","machine learning","low-cost sensors","new sources","information technology","data analysis","distributed intelligence","internet of things (iot)"]},{"p_id":44487,"title":"Context-Aware Computing, Learning, and Big Data in Internet of Things: A Survey","abstract":"\u00a9 2014 IEEE. Internet of Things (IoT) has been growing rapidly due to recent advancements in communications and sensor technologies. Meanwhile, with this revolutionary transformation, researchers, implementers, deployers, and users are faced with many challenges. IoT is a complicated, crowded, and complex field; there are various types of devices, protocols, communication channels, architectures, middleware, and more. Standardization efforts are plenty, and this chaos will continue for quite some time. What is clear, on the other hand, is that IoT deployments are increasing with accelerating speed, and this trend will not stop in the near future. As the field grows in numbers and heterogeneity, 'intelligence' becomes a focal point in IoT. Since data now becomes 'big data,' understanding, learning, and reasoning with big data is paramount for the future success of IoT. One of the major problems in the path to intelligent IoT is understanding 'context,' or making sense of the environment, situation, or status using data from sensors, and then acting accordingly in autonomous ways. This is called 'context-aware computing,' and it now requires both sensing and, increasingly, learning, as IoT systems get more data and better learning from this big data. In this survey, we review the field, first, from a historical perspective, covering ubiquitous and pervasive computing, ambient intelligence, and wireless sensor networks, and then, move to context-aware computing studies. Finally, we review learning and big data studies related to IoT. We also identify the open issues and provide an insight for future study areas for IoT researchers.","keywords_author":["Big data in Internet of Things (IoT)","context awareness","data management and analytics","machine learning in IoT","Big data in Internet of Things (IoT)","context awareness","data management and analytics","machine learning in IoT"],"keywords_other":["INFORMATION-CENTRIC NETWORKING","Complex fields","Focal points","PHYSICAL-ACTIVITY","Sensor technologies","AMBIENT-INTELLIGENCE","Context-aware computing","Context- awareness","WEARABLE SENSORS","CHALLENGES","MOBILE","Historical perspective","Ubiquitous and pervasive computing","IOT","MIDDLEWARE","ISSUES","Internet of Things (IOT)","ACTIVITY RECOGNITION"],"max_cite":0.0,"pub_year":2018.0,"sources":"['ieee', 'scp', 'wos']","rawkeys":["focal points","complex fields","challenges","physical-activity","internet of things (iot)","information-centric networking","context awareness","context- awareness","mobile","issues","ambient-intelligence","ubiquitous and pervasive computing","historical perspective","activity recognition","wearable sensors","big data in internet of things (iot)","data management and analytics","iot","machine learning in iot","middleware","sensor technologies","context-aware computing"],"tags":["focal points","complex fields","context-aware","challenges","internet of things (iot)","information-centric networking","physical activity","mobile","ambient intelligence","issues","ubiquitous and pervasive computing","historical perspective","activity recognition","wearable sensors","big data in internet of things (iot)","data management and analytics","machine learning in iot","middleware","sensor technologies","context-aware computing"]},{"p_id":38344,"title":"Using machine learning to secure IoT systems","abstract":"\u00a9 2016 IEEE. The Internet of Things (IoT) is a massive group of devices containing sensors or actuators connected together over wired or wireless networks. With an estimate of over 25 billion devices connected together by 2020, IoT has been rapidly growing over the past decade. During the growth, security has been identified as one of the weakest areas in IoT. When implementing security within an IoT network, there are several challenges including heterogeneity within the system as well as the quantity of devices that need to be addressed. To approach the challenges in securing IoT devices, we propose using machine learning within an IoT gateway to help secure the system. We investigate using Artificial Neural Networks in a gateway to detect anomalies in the data sent from the edge devices. We are convinced that this approach can improve the security of IoT systems.","keywords_author":["Internet of Things","Machine Learning","Security"],"keywords_other":["Security","IOT networks","Internet of thing (IOT)","Sensors or actuators"],"max_cite":1.0,"pub_year":2016.0,"sources":"['scp']","rawkeys":["iot networks","internet of thing (iot)","internet of things","machine learning","sensors or actuators","security"],"tags":["iot networks","sensors or actuators","machine learning","security","internet of things (iot)"]},{"p_id":19913,"title":"State of the art and further development of information and communication systems","abstract":"\ufffd 2016 IEEE. This paper depicts the modern state and directions of the future development in the area of information and telecommunication systems. We analyze the most interesting and promised technologies of the last decade to define the predictable directions of further ICT shifting.","keywords_author":["5G","Cloud RAN","D2D","IoT","Massive MIMO","Mobile Cloud Computing","OBS","SDN"],"keywords_other":["State of the art","nocv1","Information and communication systems"],"max_cite":19.0,"pub_year":2016.0,"sources":"['scp']","rawkeys":["cloud ran","d2d","state of the art","information and communication systems","sdn","massive mimo","mobile cloud computing","obs","iot","5g","nocv1"],"tags":["obs (optical burst switching)","cloud ran","d2d","software-defined networking","state of the art","information and communication systems","massive mimo","mobile cloud computing","5g","internet of things (iot)","nocv1"]},{"p_id":3535,"title":"Autonomous decision making for a driver-less car","abstract":"\u00a9 2017 IEEE.Autonomous driving has been a hot topic with companies like Google, Uber, and Tesla because of the complexity of the problem, seemingly endless applications, and capital gain. The technology's brain child is DARPA's autonomous urban challenge from over a decade ago. Few companies have had some success in applying algorithms to commercial cars. These algorithms range from classical control approaches to Deep Learning. In this paper, we will use Deep Learning techniques and the Tensor flow framework with the goal of navigating a driverless car through an urban environment. The novelty in this system is the use of Deep Learning vs. traditional methods of real-time autonomous operation as well as the application of the Tensorflow framework itself. This paper provides an implementation of AlexNet's Deep Learning model for identifying driving indicators, how to implement them in a real system, and any unforeseen drawbacks to these techniques and how these are minimized and overcome.","keywords_author":["Autonomous Driving","Deep Learning","Internet of Things","Machine Learning","Neural-Network","Vision"],"keywords_other":["Classical control","Autonomous operations","Learning models","Autonomous decision","Autonomous driving","Urban environments","Driverless cars","Learning techniques"],"max_cite":1.0,"pub_year":2017.0,"sources":"['ieee', 'scp']","rawkeys":["driverless cars","learning models","deep learning","internet of things","machine learning","vision","autonomous driving","learning techniques","neural-network","autonomous decision","classical control","autonomous operations","urban environments"],"tags":["driverless cars","learning models","neural networks","machine learning","vision","autonomous driving","learning techniques","autonomous decision","classical control","autonomous operations","urban environments","internet of things (iot)"]},{"p_id":34260,"title":"Urban sensing and smart home energy optimisations: A machine learning approach","abstract":"\u00a9 2015 ACM. Energy effciency for smart home applications is proposed using urban sensing data with machine learning techniques. We exploit Internet of Things (IoTs) enabled environmental and energy panel sensor data, smart home sensing data and opportunistic crowd-sourced data for energy effcient applications in a smart urban home. We present some applications where data from the IoT enabled sensors can be utilised using machine learning techniques. Prediction of small scale renewable energy using solar photovoltaic panels and environmental sensor data is used in energy management such as water heating system. Smart meter data and motion sensor data are used in household appliance monitoring applications with machine learning techniques towards energy savings. Further event detection from environmental and traffc sensor data is proposed in planning and optimising energy usage of smart electric vehicles for a smart urban home. Initial experimental results show the applicability of developing energy effcient applications using machine learning techniques with IoT enabled sensor data.","keywords_author":["Energy efficiency","Machine learning","Urban sensing"],"keywords_other":["Water heating systems","Monitoring applications","Machine learning approaches","Internet of thing (IoTs)","Urban sensing","Renewable energy using","Solar photovoltaic panels","Machine learning techniques"],"max_cite":2.0,"pub_year":2015.0,"sources":"['scp']","rawkeys":["solar photovoltaic panels","urban sensing","renewable energy using","water heating systems","monitoring applications","machine learning techniques","machine learning","internet of thing (iots)","energy efficiency","machine learning approaches"],"tags":["solar photovoltaic panels","urban sensing","renewable energy using","water heating systems","monitoring applications","machine learning techniques","machine learning","energy efficiency","machine learning approaches","internet of things (iot)"]},{"p_id":24026,"title":"Deployment strategies and standardization perspectives for 5G mobile networks","abstract":"\u00a9 2016 National University Lviv Polytechnic. Rapid growing of mobile communications market, driven by continuous development of new technologies, results in significant increasing of traffic demands. New powerful user's equipment became capable of all functions performed mainly by desktop computers. Therefore, mobile Internet access requires data rates comparable with fixed access networks. 5G concept has emerged to meet these growing demands. Recently, many approaches have been proposed as potential 5G technologies. In this paper, we survey main prerequisites for 5G concept and study key 5G technologies. Possible deployment strategies have been analyzed. Pros and cons of key 5G technologies and complementary technologies have been outlined. Future perspectives of 5G standardization have been explained.","keywords_author":["5G","device-To-device","Internet of Things","Massive MIMO","small cells"],"keywords_other":["Traffic demands","Deployment strategy","Small cells","Continuous development","device-To-device","Mobile Internet","Mobile communications","Future perspectives"],"max_cite":10.0,"pub_year":2016.0,"sources":"['scp']","rawkeys":["deployment strategy","future perspectives","mobile communications","internet of things","device-to-device","mobile internet","small cells","continuous development","massive mimo","5g","traffic demands"],"tags":["deployment strategy","future perspectives","mobile communications","device-to-device","mobile internet","small cells","continuous development","massive mimo","5g","internet of things (iot)","traffic demands"]},{"p_id":17883,"title":"Real-Time Locating Systems Using Active RFID for Internet of Things","abstract":"\u00a9 2016 IEEE.The proliferation of the Internet of Things (IoT) has fostered growing attention to real-time locating systems (RTLSs) using radio frequency identification (RFID) for asset management, which can automatically identify and track physical objects within indoor or confined environments. Various RFID indoor locating systems have been proposed. However, most of them are inappropriate for large-scale IoT applications owing to severe radio multipath, diffraction, and reflection. In this paper, we propose a newly fashioned RTLS using active RFID for the IoT, i.e., iLocate, which locates objects at high levels of accuracy up to 30 cm with ultralong distance transmission. To achieve fine-grained localization accuracy, iLocate presents the concept of virtual reference tags. To overcome signal multipath, iLocate employs a frequency-hopping technique to schedule RFID communication. To support large-scale RFID networks, iLocate leverages the ZigBee. We implement all hardware using 2.45-GHz RFID chips so that each active tag can communicate with readers that are around 1000 m away in a free space. Our empirical study and real project deployment show the superiority of the proposed system with respect to the localization accuracy and the data transmission rate for large-scale active RFID networks.","keywords_author":["Frequency hopping","Internet of Things (IoT)","radio-frequency identification (RFID)","real-time locating systems (RTLSs)","tag-tag communication"],"keywords_other":["Data transmission rates","Localization accuracy","Internet of thing (IOT)","Confined environment","RFID communication","Real-Time Locating Systems","Virtual reference tags","Empirical studies"],"max_cite":27.0,"pub_year":2016.0,"sources":"['scp']","rawkeys":["rfid communication","tag-tag communication","empirical studies","internet of thing (iot)","radio-frequency identification (rfid)","real-time locating systems (rtlss)","virtual reference tags","real-time locating systems","frequency hopping","confined environment","localization accuracy","data transmission rates","internet of things (iot)"],"tags":["real time location systems","rfid communication","tag-tag communication","empirical studies","rfid","virtual reference tags","frequency hopping","confined environment","localization accuracy","data transmission rates","internet of things (iot)"]},{"p_id":48610,"title":"A 5.3 pJ\/op approximate TTA VLIW tailored for machine learning","abstract":"\u00a9 2017 Elsevier LtdTo achieve energy efficiency in the Internet-of-Things (IoT), more intelligence is required in the wireless IoT nodes. Otherwise, the energy required by the wireless communication of raw sensor data will prohibit battery lifetime, the backbone of IoT. One option to achive this intelligence is to implement a variety of machine learning algorithms on the IoT sensor instead of the cloud. Shown here is sub-milliwatt machine learning accelerator operating at the Ultra-Low Voltage Minimum-Energy Point. The accelerator is a Transport Triggered Architecture (TTA) Application-Specific Instruction-Set Processor (ASIP) targeted for running various Machine Learning algorithms. The ASIP is implemented in 28 nm FDSOI (Fully Depleted Silicon On Insulator) CMOS process, with an operating voltage of 0.35 V, and is capable of 5.3pJ\/cycle and 1.8nJ\/iteration when performing conventional machine learning algorithms. The ASIP also includes hardware and compiler support for approximate computing. With the machine learning algorithms, computing approximately brings a maximum of 4.7% energy savings.","keywords_author":["Approximate computing","Integrated circuit","Machine learning","Minimum energy point","Processor","Timing error detection"],"keywords_other":["Minimum energy point","Internet of thing (IOT)","Approximate computing","Timing error detection","Processor","Fully depleted silicon-on-insulator","Transport triggered architecture","Application specific instruction set processor"],"max_cite":0.0,"pub_year":2017.0,"sources":"['scp']","rawkeys":["processor","transport triggered architecture","timing error detection","application specific instruction set processor","internet of thing (iot)","machine learning","fully depleted silicon-on-insulator","approximate computing","minimum energy point","integrated circuit"],"tags":["processor","transport triggered architecture","timing error detection","application specific instruction set processor","machine learning","fully depleted silicon-on-insulator","integrated circuits","approximate computing","minimum energy point","internet of things (iot)"]},{"p_id":42469,"title":"DIANNE: a modular framework for designing, training and deploying deep neural networks on heterogeneous distributed infrastructure","abstract":"\u00a9 2018 Elsevier Inc. Deep learning has shown tremendous results on various machine learning tasks, but the nature of the problems being tackled and the size of state-of-the-art deep neural networks often require training and deploying models on distributed infrastructure. DIANNE is a modular framework designed for dynamic (re)distribution of deep learning models and procedures. Besides providing elementary network building blocks as well as various training and evaluation routines, DIANNE focuses on dynamic deployment on heterogeneous distributed infrastructure, abstraction of Internet of Things (IoT) sensors, integration with external systems and graphical user interfaces to build and deploy networks, while retaining the performance of similar deep learning frameworks. In this paper the DIANNE framework is proposed as an all-in-one solution for deep learning, enabling data and model parallelism though a modular design, offloading to local compute power, and the ability to abstract between simulation and real environment.","keywords_author":["Artificial neural networks","Distributed applications","Internet of Things","Machine learning","Artificial neural networks","Distributed applications","Machine learning","Internet of Things"],"keywords_other":["Distributed infrastructure","Distributed applications","Modular framework","Learning frameworks","State of the art","Network building blocks","Real environments","Internet of Things (IOT)"],"max_cite":0.0,"pub_year":2018.0,"sources":"['ieee', 'scp', 'wos']","rawkeys":["distributed infrastructure","modular framework","state of the art","internet of things","machine learning","real environments","learning frameworks","artificial neural networks","network building blocks","distributed applications","internet of things (iot)"],"tags":["distributed infrastructure","modular framework","neural networks","state of the art","machine learning","real environments","learning frameworks","network building blocks","distributed applications","internet of things (iot)"]},{"p_id":38377,"title":"Big IoT and social networking data for smart cities: Algorithmic improvements on big data analysis in the context of RADICAL city applications","abstract":"Copyright \u00a9 2016 by SCITEPRESS-Science and Technology Publications, Lda. All rights reserved.In this paper we present a SOA (Service Oriented Architecture)-based platform, enabling the retrieval and analysis of big datasets stemming from social networking (SN) sites and Internet of Things (IoT) devices, collected by smart city applications and socially-aware data aggregation services. A large set of city applications in the areas of Participating Urbanism, Augmented Reality and Sound-Mapping throughout participating cities is being applied, resulting into produced sets of millions of user-generated events and online SN reports fed into the RADICAL platform. Moreover, we study the application of data analytics such as sentiment analysis to the combined IoT and SN data saved into an SQL database, further investigating algorithmic and configurations to minimize delays in dataset processing and results retrieval.","keywords_author":["Big Data Aggregation and Analysis","Internet of Things","Machine Learning.","Sentiment Analysis","Smart City Applications","Social Networking"],"keywords_other":["Data aggregation","Soa (serviceoriented architecture)","Sentiment analysis","Data aggregation service","User-generated","Smart cities","Internet of Things (IOT)","Data analytics"],"max_cite":1.0,"pub_year":2016.0,"sources":"['scp']","rawkeys":["data aggregation service","user-generated","data analytics","internet of things","machine learning","big data aggregation and analysis","soa (serviceoriented architecture)","internet of things (iot)","smart city applications","social networking","sentiment analysis","data aggregation","smart cities"],"tags":["data aggregation service","user-generated","soa","data analytics","machine learning","big data aggregation and analysis","social networks","internet of things (iot)","smart city applications","sentiment analysis","data aggregation","smart cities"]},{"p_id":3572,"title":"Deep convolutional computation model for feature learning on big data in internet of things","abstract":"Currently, a large number of industrial data, usually referred to big data, are collected from Internet of Things (IoT). Big data are typically heterogeneous, i.e., each object in big datasets is multimodal, posing a challenging issue on the convolutional neural network (CNN) that is one of the most representative deep learning models. In this paper, a deep convolutional computation model (DCCM) is proposed to learn hierarchical features of big data by using the tensor representation model to extend the CNN from the vector space to the tensor space. To make full use of the local features and topologies contained in the big data, a tensor convolution operation is defined to prevent over-fitting and improve the training efficiency. Furthermore, a high-order backpropagation algorithm is proposed to train the parameters of the deep convolutional computational model in the high-order space. Finally, experiments on three datasets, i.e., CUAVE, SNAE2, and STL-10 are carried out to verify the performance of the DCCM. Experimental results show that the deep convolutional computation model can give higher classification accuracy than the deep computation model or the multimodal model for big data in IoT.","keywords_author":["Big data","Convolutional neural network (cnn)","Deep convolutional computation model (dccm)","High-order backpropagation (hbp) algorithm","Internet of things (iot)","Tensor computation","Big data","convolutional neural network (CNN)","deep convolutional computation model (DCCM)","high-order backpropagation (HBP) algorithm","Internet of Things (IoT)","tensor computation"],"keywords_other":["Computation model","Hierarchical features","Computational model","Tensor representation","Convolutional neural network","Training efficiency","Internet of Things (IOT)","Classification accuracy"],"max_cite":10.0,"pub_year":2018.0,"sources":"['ieee', 'scp', 'wos']","rawkeys":["tensor computation","big data","classification accuracy","hierarchical features","training efficiency","convolutional neural network (cnn)","deep convolutional computation model (dccm)","high-order backpropagation (hbp) algorithm","convolutional neural network","computation model","computational model","internet of things (iot)","tensor representation"],"tags":["computational modeling","tensor computation","big data","classification accuracy","hierarchical features","training efficiency","deep convolutional computation model (dccm)","high-order backpropagation algorithm","convolutional neural network","internet of things (iot)","tensor representation"]},{"p_id":36340,"title":"Deep Osmosis: Holistic Distributed Deep Learning in Osmotic Computing","abstract":"\u00a9 2014 IEEE.Emerging availability (and varying complexity and types) of Internet of Things (IoT) devices, along with large data volumes that such devices (can potentially) generate, can have a significant impact on our lives, fuelling the development of critical next-generation services and applications in a variety of application domains (e.g. healthcare, smart grids, finance, disaster management, agriculture, transportation and water management). Deep learning technology, which has in the past been used successfully in computer vision and language modelling is now finding application in new domains driven by availability of diverse and large datasets. One such example is the advances in medical diagnostics and prediction by using Deep Learning technologies to improve human health. However, transferring large data streams (a requirement of Deep Learning technologies for achieving high accuracy) to centralised locations such as Cloud datacentre environments, in a timely and reliable manner, is being seen as a key limitation of expanding the application horizons of such technologies. To this end, various paradigms, including Osmotic Computing, have been proposed that promotes distribution of data analysis tasks across Cloud and Edge computing environments. However, these existing paradigms fail to provide a detailed account of how technologies such as deep learning can be orchestrated and take advantage of the cloud, edge and mobile edge environments in a holistic manner. In other words, the focus of this Blue Skies piece is to analyze the research challenges involved with developing a new class of holistic distributed deep learning algorithms that are 'resource and data aware', and which are able to account for underlying heterogeneous data and data models, resource (cloud vs. edge vs. mobile edge) models and data availability while executing-trading accuracy for execution time, etc.","keywords_author":["cloud IoT","deep learning","distributed","holistic","osmosis"],"keywords_other":["Research challenges","Disaster management","holistic","Learning technology","Next generation services","Medical diagnostics","distributed","Internet of Things (IOT)"],"max_cite":1.0,"pub_year":2018.0,"sources":"['scp']","rawkeys":["next generation services","research challenges","osmosis","learning technology","deep learning","holistic","disaster management","medical diagnostics","distributed","cloud iot","internet of things (iot)"],"tags":["next generation services","research challenges","osmosis","learning technology","holistic","machine learning","distributions","disaster management","medical diagnostics","cloud iot","internet of things (iot)"]},{"p_id":36345,"title":"Improving Quality of Experience in multimedia Internet of Things leveraging machine learning on big data","abstract":"\u00a9 2018 Elsevier B.V. With rapid evolution of the Internet of Things (IoT) applications on multimedia, there is an urgent need to enhance the satisfaction level of Multimedia IoT (MIoT) network users. An important and unsolved problem is automatic optimization of Quality of Experience (QoE) through collecting\/managing\/processing various data from MIoT network. In this paper, we propose an MIoT QoE optimization mechanism leveraging data fusion technology, called QoE optimization via Data Fusion (QoEDF). QoEDF consists of two steps. Firstly, a multimodal data fusion approach is proposed to build a QoE mapping between the uncontrollable user data with the controllable network-related system data. Secondly, an automatic QoE optimization model is built taking fused results, which is different from the traditional way. QoEDF is able to adjust network-related system data automatically so as to achieve optimized user satisfaction. Simulation results show that QoEDF will lead to significant improvements in QoE level as well as be adaptable to dynamic network changes.","keywords_author":["Big data","Data fusion","Machine learning","Multimedia Internet of Things","Neural network","Quality of Experience"],"keywords_other":["Internet of thing (IOT)","Controllable network","Automatic optimization","Optimization modeling","Multimodal data fusion","Quality of experience (QoE)","Multimedia internet","User satisfaction"],"max_cite":1.0,"pub_year":2018.0,"sources":"['scp']","rawkeys":["automatic optimization","neural network","multimedia internet of things","user satisfaction","data fusion","big data","multimedia internet","internet of thing (iot)","machine learning","quality of experience","optimization modeling","multimodal data fusion","quality of experience (qoe)","controllable network"],"tags":["automatic optimization","multimedia internet of things","user satisfaction","data fusion","big data","multimedia internet","neural networks","machine learning","optimization modeling","multimodal data fusion","quality of experience (qoe)","control network","internet of things (iot)"]},{"p_id":32259,"title":"Osmotic Flow: Osmotic Computing + IoT Workflow","abstract":"\u00a9 2014 IEEE. The rapid evolution of Internet of Things (IoT) devices (e.g., sensors and gateways) and the almost ubiquitous connectivity (e.g., 4G, Wi-Fi, RFID\/NFC, Bluetooth, IEEE 802.15.4) are forcing us to radically rethink how to effectively deal with massive volume, velocity, and variety of big data produced by such IoT devices. There are currently 6.4 billion IoT devices in use around the world and their number, capabilities, as well as the scope of their use, keeps growing rapidly.","keywords_author":["blue skies","cloud","data transformation","edge computing","Internet of Things (IoT)","Osmotic Flow"],"keywords_other":["Osmotic flow","Data transformation","blue skies","Edge computing","Internet of Things (IOT)"],"max_cite":3.0,"pub_year":2017.0,"sources":"['scp']","rawkeys":["osmotic flow","blue skies","data transformation","edge computing","cloud","internet of things (iot)"],"tags":["osmotic flow","blue skies","data transformation","edge computing","cloud","internet of things (iot)"]},{"p_id":50691,"title":"Applying an innovative semantic sensor network model in Internet of Things","abstract":"\u00a9 2015 IEEE.It will not take a longtime that the human will apply the networks of sensors in controlling the environment and predict natural disasters. This vast function would create bulks of sensor data to be applied World Wide Web. These bulks of data should be handled and processed a primary manner, which is a difficult task; therefore only the expert in this field can process them. One solution to overcome this drawback is the application of Open Geospatial Consortium (OGC) standard which to the sensor networks would develop the concept of sensor web; hence access to the sensor observations through the web. Data semantic is prerequisite of this process. To overcome this drawback the combination of sensor web and semantic web technology that constitutes the new concept named semantic sensor web should be adopted. The objective of this article is to develop a semantic sensor web model and the challenges involved. Within it is found here that adopting this model is applicable in all sensors in a more rapid manner.","keywords_author":["Internet of Things","Machine learning","Semantic sensor networks","Semantic Sensor Web","Semantic Web","Sensor web"],"keywords_other":["Semantic sensors","Data semantics","Natural disasters","Open geospatial consortium","Sensor web","Semantic sensor webs","Sensor data","Semantic Web technology"],"max_cite":0.0,"pub_year":2015.0,"sources":"['scp']","rawkeys":["semantic sensor webs","sensor data","natural disasters","semantic sensor web","data semantics","semantic web technology","internet of things","machine learning","semantic web","sensor web","semantic sensor networks","semantic sensors","open geospatial consortium"],"tags":["sensor data","natural disasters","semantic sensor web","data semantics","semantic web technology","machine learning","semantic web","sensor web","semantic sensor networks","semantic sensors","internet of things (iot)","open geospatial consortium"]},{"p_id":24072,"title":"A fog computing-based framework for process monitoring and prognosis in cyber-manufacturing","abstract":"\u00a9 2017 The Society of Manufacturing Engineers Small- and medium-sized manufacturers, as well as large original equipment manufacturers (OEMs), have faced an increasing need for the development of intelligent manufacturing machines with affordable sensing technologies and data-driven intelligence. Existing monitoring systems and prognostics approaches are not capable of collecting the large volumes of real-time data or building large-scale predictive models that are essential to achieving significant advances in cyber-manufacturing. The objective of this paper is to introduce a new computational framework that enables remote real-time sensing, monitoring, and scalable high performance computing for diagnosis and prognosis. This framework utilizes wireless sensor networks, cloud computing, and machine learning. A proof-of-concept prototype is developed to demonstrate how the framework can enable manufacturers to monitor machine health conditions and generate predictive analytics. Experimental results are provided to demonstrate capabilities and utility of the framework such as how vibrations and energy consumption of pumps in a power plant and CNC machines in a factory floor can be monitored using a wireless sensor network. In addition, a machine learning algorithm, implemented on a public cloud, is used to predict tool wear in milling operations.","keywords_author":["Cyber-Manufacturing","Fog computing","Industrial internet of things","Machine learning","Prognosis"],"keywords_other":["Prognosis","Intelligent Manufacturing","Diagnosis and prognosis","High performance computing","Monitoring system","Sensing technology","Computational framework","Original equipment manufacturers"],"max_cite":9.0,"pub_year":2017.0,"sources":"['scp']","rawkeys":["sensing technology","cyber-manufacturing","diagnosis and prognosis","monitoring system","fog computing","original equipment manufacturers","machine learning","prognosis","computational framework","high performance computing","intelligent manufacturing","industrial internet of things"],"tags":["sensing technology","monitoring system","diagnosis and prognosis","original equipment manufacturers","fog computing","machine learning","prognosis","computational framework","high performance computing","cyber manufacturing","intelligent manufacturing","internet of things (iot)"]},{"p_id":97801,"title":"Data Fusion-Based Multi-Object Tracking for Unconstrained Visual Sensor Networks","abstract":"Camera node perception capability is one of the crucial issues for visual sensor networks, which belongs to the field of Internet of Things. Multi-object tracking is an important feature in analyzing object trajectories across multiple cameras, thus allowing synthesis of data and security analysis of images in various scenarios. Despite intensive research in the last decades, it remains challenging for tracking systems to perform in real-world situations. We therefore focus on key issues of multi-object state estimation for unconstrained multi-camera systems, e.g., data fusion of multiple sensors and data association. Unlike previous work that rely on camera network topology inference, we construct a graph from 2-D observations of all camera pairs without any assumption of network configuration. We apply the shortest path algorithm to the graph to find fused 3-D observation groups. Our approach is thus able to reject false positive reconstructions automatically, and also minimize computational complexity to guarantee feasible data fusion. Particle filters are applied as the 3-D tracker to form tracklets that integrate local features. These tracklets are modeled by a graph and linked into full tracks incorporating global spatial-temporal features. Experiments on the real-world PETS2009 S2\/L1 sequence show the accuracy of our approach. Analyses of the different components of our approach provide meaningful insights for object tracking using multiple cameras. Evidence is provided for selecting the best view for a visual sensor network.","keywords_author":["Data fusion","graph theory","Internet of Things","particle filters","visual sensor networks"],"keywords_other":["MULTITARGET TRACKING","MULTIPLE TARGETS","DATA ASSOCIATION"],"max_cite":0.0,"pub_year":2018.0,"sources":"['wos']","rawkeys":["multiple targets","data fusion","graph theory","internet of things","particle filters","data association","multitarget tracking","visual sensor networks"],"tags":["multiple targets","data fusion","graph theory","multi-target tracking","data association","particle filter","visual sensor networks","internet of things (iot)"]},{"p_id":32269,"title":"Data ingestion and storage performance of IoT platforms: Study of OpenIoT","abstract":"\u00a9 Springer International Publishing AG 2017. Internet of Things is a very active research area with great commercialisation potential. The number of IoT platforms is already exceeding 300 and still growing. However, performance evaluation and benchmarking of IoT platforms are still in their infancy. As a step towards developing a performance benchmarking approach for IoT platforms, this paper analyses and compares a number of popular IoT platforms from data ingestion and storage capability perspectives. In order to test the proposed approach, we use the widely used open source IoT platform, OpenIoT. The results of the experiments and the lessons learnt are presented and discussed. While having a great research promise and pioneering contribution to semantic interoperability of IoT silos, the experimental results indicate OpenIoT platform needs more development effort to be ready for any substantial deployment in commercial IoT applications.","keywords_author":["Benchmarking","Data management","Evaluation","Ingestion","Internet of Things (IoT)","Platform","Storage"],"keywords_other":["Storage capability","Storage performance","Semantic interoperability","Internet of Things (IOT)","Performance benchmarking","Platform","Evaluation","Performance evaluation and benchmarking"],"max_cite":3.0,"pub_year":2017.0,"sources":"['scp']","rawkeys":["storage performance","performance evaluation and benchmarking","semantic interoperability","storage capability","evaluation","ingestion","platform","performance benchmarking","storage","benchmarking","data management","internet of things (iot)"],"tags":["benchmark","storage performance","performance evaluation and benchmarking","semantic interoperability","storage capability","evaluation","ingestion","platform","performance benchmarking","storage","data management","internet of things (iot)"]},{"p_id":42514,"title":"LaSa: Location Aware Wireless Security Access Control for IoT Systems","abstract":"\u00a9 2018 Springer Science+Business Media, LLC, part of Springer Nature IoT (Internet of Things) security has become a severe yet not well solved problem attracting increasing research attention as well as industrial concerns. Location-based access control approaches, such as Wi-Fi geo-fencing, promise to fulfill the needs of preventing unauthorized access to IoT systems. We propose a crowdsourcing method for location aware security access control, namely LaSa, which is able to confine wireless network access inside certain physical areas only using a single commercial Access Point (AP). Specifically, LaSa detects whether a user enters or exits a room by discovering and recognizing the unique signal patterns. It combines the Received Signal Strength (RSS), Channel State Information (CSI), and coarse Angle of Arrival (AoA) data to improve the accuracy of user classification for accessing the wireless network. Real-world experimental results show that LaSa can achieve a 97.0% accuracy of identification of unauthorized users while maintaining a low false blocking rate of authorized users as low as 3.3%. LaSa is designed to be straightforward for integration with commercial APs and deployment to home and business Wi-Fi environments.","keywords_author":["Access control","Internet of things","Machine learning","User validation"],"keywords_other":["Unauthorized access","Received signal strength","Location-based access controls","Angle of Arrival (AoA)","Wireless network access","Unauthorized users","User validation","User classification"],"max_cite":0.0,"pub_year":2018.0,"sources":"['scp']","rawkeys":["user classification","angle of arrival (aoa)","location-based access controls","internet of things","machine learning","received signal strength","wireless network access","user validation","access control","unauthorized users","unauthorized access"],"tags":["user classification","machine learning","location-based access controls","angle of arrival","recommender systems","wireless network access","user validation","biometrics","unauthorized users","unauthorized access","internet of things (iot)"]},{"p_id":5658,"title":"Research directions for the internet of things","abstract":"\u00a9 2014 IEEE. Many technical communities are vigorously pursuing research topics that contribute to the Internet of Things (IoT). Nowadays, as sensing, actuation, communication, and control become even more sophisticated and ubiquitous, there is a significant overlap in these communities, sometimes from slightly different perspectives. More cooperation between communities is encouraged. To provide a basis for discussing open research problems in IoT, a vision for how IoT could change the world in the distant future is first presented. Then, eight key research topics are enumerated and research problems within these topics are discussed.","keywords_author":["Cyber physical systems","Internet of Things (IoT)","mobile computing","pervasive computing","wireless sensor networks"],"keywords_other":["Technical community","Internet of thing (IOT)","Research problems","Cyber physical systems (CPSs)","Distant futures","Research topics","Internet of Things (IOT)"],"max_cite":427.0,"pub_year":2014.0,"sources":"['ieee', 'scp', 'wos']","rawkeys":["technical community","distant futures","cyber physical systems","research topics","cyber physical systems (cpss)","internet of thing (iot)","mobile computing","pervasive computing","research problems","wireless sensor networks","internet of things (iot)"],"tags":["cyber-physical systems","technical community","distant futures","research topics","mobile computing","internet of things (iot)","research problems","wireless sensor networks","pervasive computing"]},{"p_id":38430,"title":"Perceptive invariance and associative memory between perception and semantic representation USER a universal semantic representation implemented in a system on chip (SoC)","abstract":"\u00a9 Springer International Publishing Switzerland 2016. USER (Universal SEmantic Representation) is a bio-inspired module implemented in a system on a chip (SoC), which builds a link between multichannel perception and semantic representation. The input data are projected into a generic bioinspired higher dimensional nonlinear semantic space with high sparsity. A pooling of these semantic representations (global, dynamic and structural) is done automatically by a set of dynamic attractors embedding spatio-temporal histograms, being drastically more efficient than back-propagation. A supervised learning is used to build the association between the invariant multimodal semantic representations (histogram results) and the labels (\u2018words\u2019). The invariant recognition is achieve thanks to multichannel multiscale dynamic attractors and bilinear representations - imitating brain attentional processes. USER modules can be cascaded, allowing to work at different levels of abstraction (or complexity). Due to its low consumption, small size and minimal price, USER targets deep learning, robotics, and Internet of Things (IoT) applications.","keywords_author":["Associative memory","Deep learning","Dynamic attractor","Embedded system","Invariant representation","IoT","Multi-sensory","Pattern recognition","Semantic representation","SoC","Sparsity","USER"],"keywords_other":["Deep learning","Semantic representation","Associative memory","Sparsity","USER","Invariant representation","Multi-Sensory"],"max_cite":1.0,"pub_year":2016.0,"sources":"['scp']","rawkeys":["associative memory","invariant representation","deep learning","sparsity","pattern recognition","semantic representation","dynamic attractor","embedded system","multi-sensory","soc","iot","user"],"tags":["associative memory","invariant representation","embedded systems","machine learning","semantic representation","system-on-chip","dynamic attractor","sparsity","users","multi-sensory","pattern recognition","internet of things (iot)"]},{"p_id":44574,"title":"SIGMM: A Novel Machine Learning Algorithm for Spammer Identification in Industrial Mobile Cloud Computing","abstract":"IEEE Industrial mobile network is crucial for industrial production in Internet of Things. It guarantees the normal function of machines and the normalization of industrial production. However this characteristic might be utilized by spammers to attack others and influence industrial production. The users who only share spams such as links with virus and advertisement are called spammers. With the growth of mobile network members, spammers are organized in groups for the purpose of benefit maximization, which has caused confusion and heavy loss to industrial production. It is difficult to distinguish spammers from normal users due to characteristics of multidimensional data. To address this problem, this paper proposes a Spammer Identification scheme based on Gaussian Mixture Model (SIGMM) that utilizes machine learning for industrial mobile networks. It works on the intelligent identification of spammers without relying on the flexible and unreliable relationship. SIGMM combines the presentation of data. Each user node is classified to one class in the construction process of model. Finally, we validate SIGMM by comparing it with RMA, HFCM schemes using mobile network dataset from cloud server. Simulation results show that SIGMM outperforms these previous schemes in terms of recall, precision and time complexity.","keywords_author":["Clustering algorithms","Correlation","Data models","Industrial mobile network","intelligent identification","Internet of Things","machine learning","Machine learning algorithms","Mobile communication","Mobile computing","Principal component analysis","spammers"],"keywords_other":["Identification scheme","Intelligent identification","Multidimensional data","Benefit maximization","Industrial production","Spammers","Gaussian Mixture Model","Mobile communications"],"max_cite":0.0,"pub_year":2018.0,"sources":"['scp']","rawkeys":["principal component analysis","spammers","benefit maximization","machine learning algorithms","mobile communications","internet of things","industrial mobile network","intelligent identification","machine learning","mobile communication","mobile computing","identification scheme","data models","clustering algorithms","multidimensional data","correlation","industrial production","gaussian mixture model"],"tags":["principal component analysis","spammers","benefit maximization","machine learning algorithms","mobile communications","machine learning","industrial mobile network","intelligent identification","mobile computing","identification scheme","internet of things (iot)","data models","clustering algorithms","multidimensional data","correlation","industrial production","gaussian mixture model"]},{"p_id":7712,"title":"A Survey on Internet of Things from Industrial Market Perspective","abstract":"\u00a9 2013 IEEE. The Internet of Things (IoT) is a dynamic global information network consisting of Internet-connected objects, such as radio frequency identifications, sensors, and actuators, as well as other instruments and smart appliances that are becoming an integral component of the Internet. Over the last few years, we have seen a plethora of IoT solutions making their way into the industry marketplace. Context-aware communications and computing have played a critical role throughout the last few years of ubiquitous computing and are expected to play a significant role in the IoT paradigm as well. In this paper, we examine a variety of popular and innovative IoT solutions in terms of context-aware technology perspectives. More importantly, we evaluate these IoT solutions using a framework that we built around well-known context-aware computing theories. This survey is intended to serve as a guideline and a conceptual framework for context-aware product development and research in the IoT paradigm. It also provides a systematic exploration of existing IoT products in the marketplace and highlights a number of potentially significant research directions and trends.","keywords_author":["contextawareness","industry solutions","Internet of things","IoT marketplace","product review"],"keywords_other":["Context-aware communications","Internet of thing (IOT)","Conceptual frameworks","Context-aware computing","IoT marketplace","Context-awareness","Systematic exploration","Product reviews"],"max_cite":130.0,"pub_year":2015.0,"sources":"['scp']","rawkeys":["product review","industry solutions","context-aware communications","systematic exploration","internet of thing (iot)","internet of things","iot marketplace","context-aware computing","context-awareness","conceptual frameworks","contextawareness","product reviews"],"tags":["industry solutions","systematic exploration","context-aware","iot marketplace","context-aware computing","product reviews","conceptual frameworks","internet of things (iot)","context-aware communication"]},{"p_id":26147,"title":"Prometheus - Fuzzy information retrieval for semantic homes and environments","abstract":"This paper introduces a novel vision for further enhanced Internet of Things services. Based on a variety of data - such as location data, ontology-backed search queries, in-and outdoor conditions - the Prometheus framework is intended to support users with helpful recommendations and information preceding a search for context-aware data. Adapted from artificial intelligence concepts, Prometheus proposes user-readjusted answers on umpteen conditions. A number of potential Prometheus framework applications are illustrated. Added value and possible future studies are discussed in the conclusion. \u00a9 2010 IEEE.","keywords_author":["Ambient assisted living","Artificial intelligence","Fuzzy expert system","Fuzzy set theory","Information retrieval","Internet of things","Machine learning","Ontology","Semantic environment","Semantic home","Semantic Web","Web squared"],"keywords_other":["Web squared","Internet of things","Machine-learning","Fuzzy expert systems","Ambient assisted living"],"max_cite":7.0,"pub_year":2010.0,"sources":"['scp']","rawkeys":["semantic environment","artificial intelligence","semantic home","fuzzy set theory","ambient assisted living","ontology","internet of things","machine learning","fuzzy expert systems","information retrieval","web squared","fuzzy expert system","machine-learning","semantic web"],"tags":["semantic environment","semantic home","fuzzy set theory","ambient assisted living","machine learning","web squared","information retrieval","fuzzy expert systems","semantic web","internet of things (iot)"]},{"p_id":46632,"title":"Leveraging edge computing through collaborative machine learning","abstract":"\u00a9 2017 IEEE. The Internet of Things (IoT) offers the ability to analyze and predict our surroundings through sensor networks at the network edge. To facilitate this predictive functionality, Edge Computing (EC) applications are developed by considering: power consumption, network lifetime and quality of context inference. Humongous contextual data from sensors provide data scientists better knowledge extraction, albeit coming at the expense of holistic data transfer that threatens the network feasibility and lifetime. To cope with this, collaborative machine learning is applied to EC devices to (i) extract the statistical relationships and (ii) construct regression (predictive) models to maximize communication efficiency. In this paper, we propose a learning methodology that improves the prediction accuracy by quantizing the input space and leveraging the local knowledge of the EC devices.","keywords_author":["collaborative machine learning","edge analytics","edge computing","predictive intelligence"],"keywords_other":["Communication efficiency","Quality of contexts","Internet of thing (IOT)","Knowledge extraction","edge analytics","Statistical relationship","predictive intelligence","Prediction accuracy"],"max_cite":0.0,"pub_year":2017.0,"sources":"['scp']","rawkeys":["communication efficiency","quality of contexts","edge analytics","internet of thing (iot)","knowledge extraction","statistical relationship","prediction accuracy","edge computing","collaborative machine learning","predictive intelligence"],"tags":["communication efficiency","quality of contexts","edge analytics","knowledge extraction","statistical relationship","prediction accuracy","edge computing","collaborative machine learning","predictive intelligence","internet of things (iot)"]},{"p_id":36402,"title":"Integrated method for personal thermal comfort assessment and optimization through users\u2019 feedback, IoT and machine learning: A case study","abstract":"\u00a9 2018 by the authors. Licensee MDPI, Basel, Switzerland. Thermal comfort has become a topic issue in building performance assessment as well as energy efficiency. Three methods are mainly recognized for its assessment. Two of them based on standardized methodologies, face the problem by considering the indoor environment in steady-state conditions (PMV and PPD) and users as active subjects whose thermal perception is influenced by outdoor climatic conditions (adaptive approach). The latter method is the starting point to investigate thermal comfort from an overall perspective by considering endogenous variables besides the traditional physical and environmental ones. Following this perspective, the paper describes the results of an in-field investigation of thermal conditions through the use of nearable and wearable solutions, parametric models and machine learning techniques. The aim of the research is the exploration of the reliability of IoT-based solutions combined with advanced algorithms, in order to create a replicable framework for the assessment and improvement of user thermal satisfaction. For this purpose, an experimental test in real offices was carried out involving eight workers. Parametric models are applied for the assessment of thermal comfort; IoT solutions are used to monitor the environmental variables and the users\u2019 parameters; the machine learning CART method allows to predict the users\u2019 profile and the thermal comfort perception respect to the indoor environment.","keywords_author":["Indoor thermal comfort","IoT","Machine learning","Nearable","Parametric models","Wearable"],"keywords_other":["Environmental variables","Wearable","Assessment and improvement","Nearable","Indoor thermal comfort","Steady-state condition","Parametric models","Machine learning techniques"],"max_cite":1.0,"pub_year":2018.0,"sources":"['scp']","rawkeys":["nearable","environmental variables","machine learning techniques","machine learning","steady-state condition","assessment and improvement","iot","wearable","parametric models","indoor thermal comfort"],"tags":["nearable","environmental variables","machine learning techniques","machine learning","wearables","steady-state condition","assessment and improvement","parametric models","indoor thermal comfort","internet of things (iot)"]},{"p_id":42554,"title":"RF-PUF: Enhancing IoT Security through Authentication of Wireless Nodes using In-situ Machine Learning","abstract":"IEEE Traditional authentication in radio-frequency (RF) systems enable secure data communication within a network through techniques such as digital signatures and hash-based message authentication codes (HMAC), which suffer from keyrecovery attacks. State-of-the-art IoT networks such as Nest also use Open Authentication (OAuth 2.0) protocols that are vulnerable to cross-site-recovery forgery (CSRF), which shows that these techniques may not prevent an adversary from copying or modeling the secret IDs or encryption keys using invasive, side channel, learning or software attacks. Physical unclonable functions (PUF), on the other hand, can exploit manufacturing process variations to uniquely identify silicon chips which makes a PUF-based system extremely robust and secure at low cost, as it is practically impossible to replicate the same silicon characteristics across dies. Taking inspiration from human communication, which utilizes inherent variations in the voice signatures to identify a certain speaker, we present RF-PUF: a deep neural network-based framework that allows real-time authentication of wireless nodes, using the effects of inherent process variation on RF properties of the wireless transmitters (Tx), detected through in-situ machine learning at the receiver (Rx) end. The proposed method utilizes the already-existing asymmetric RF communication framework and does not require any additional circuitry for PUF generation or feature extraction. The burden of device identification is completely shifted to the gateway Rx, similar to the operation of a human listener&#x2019;s brain. Simulation results involving the process variations in a standard 65 nm technology node, and features such as LO offset and I-Q imbalance detected with a neural network having 50 neurons in the hidden layer indicate that the framework can distinguish up to 4800 transmitters with an accuracy of 99.9% (&#x2248; 99% for 10,000 transmitters) under varying channel conditions, and without the need for traditional preambles. The proposed scheme can be used as a stand-alone security feature, or as a part of traditional multifactor authentication.","keywords_author":["Artificial Neural Networks (ANN)","Authentication","Deep Neural Network","Device Signatures.","Internet-of-Things (IoT)","Machine Learning","PUF","Radio Frequency","Security"],"keywords_other":null,"max_cite":0.0,"pub_year":2018.0,"sources":"['scp']","rawkeys":["deep neural network","authentication","artificial neural networks (ann)","device signatures","machine learning","internet-of-things (iot)","security","puf","radio frequency"],"tags":["authentication","neural networks","device signatures","physically unclonable functions","machine learning","random forests","security","convolutional neural network","internet of things (iot)"]},{"p_id":42557,"title":"Deep learning over IoT big data-based ubiquitous parking guidance robot for parking near destination especially hospital","abstract":"\u00a9 2018 Springer-Verlag London Ltd., part of Springer Nature This paper offered a ubiquitous parking guidance robot system used for assisting drivers in selection of parking lots near their destination locations and recommending the parking lot with optimum conditions based on the forecast results by deep learning the big data of parking lots obtained through Internet of Things, which not only decreased the cost and the required time of parking, but also reduced the parking failure by a relatively accurate guidance way, and is important for drivers to save time on parking when drivers hurry to their destination locations such as hospital. The ubiquitous parking guidance robot system can be implemented as parking guidance apps or parking guidance plugins of navigation apps installed in drivers\u2019 mobile phones. Drivers can independently set their filtering criteria of guidance, and then the parking lot with the optimum conditions such as shortest distance, largest number of parking spaces and best environment will be recommended.","keywords_author":["Big data","Deep learning","IoT","Parking guidance","Robot system"],"keywords_other":["Filtering criteria","Required time","Robot system","Destination location","Parking spaces","Accurate guidance","Optimum conditions","Parking guidances"],"max_cite":0.0,"pub_year":2018.0,"sources":"['scp']","rawkeys":["accurate guidance","optimum conditions","big data","deep learning","parking guidance","robot system","filtering criteria","destination location","parking guidances","iot","required time","parking spaces"],"tags":["accurate guidance","optimum conditions","robotic systems","big data","machine learning","filtering criteria","destination location","parking guidances","required time","internet of things (iot)","parking spaces"]},{"p_id":22081,"title":"Data Fusion and IoT for Smart Ubiquitous Environments: A Survey","abstract":"\u00a9 2013 IEEE. The Internet of Things (IoT) is set to become one of the key technological developments of our times provided we are able to realize its full potential. The number of objects connected to IoT is expected to reach 50 billion by 2020 due to the massive influx of diverse objects emerging progressively. IoT, hence, is expected to be a major producer of big data. Sharing and collaboration of data and other resources would be the key for enabling sustainable ubiquitous environments, such as smart cities and societies. A timely fusion and analysis of big data, acquired from IoT and other sources, to enable highly efficient, reliable, and accurate decision making and management of ubiquitous environments would be a grand future challenge. Computational intelligence would play a key role in this challenge. A number of surveys exist on data fusion. However, these are mainly focused on specific application areas or classifications. The aim of this paper is to review literature on data fusion for IoT with a particular focus on mathematical methods (including probabilistic methods, artificial intelligence, and theory of belief) and specific IoT environments (distributed, heterogeneous, nonlinear, and object tracking environments). The opportunities and challenges for each of the mathematical methods and environments are given. Future developments, including emerging areas that would intrinsically benefit from data fusion and IoT, autonomous vehicles, deep learning for data fusion, and smart cities, are discussed.","keywords_author":["big data","computational and artificial intelligence","data fusion","high performance computing","Internet of Things","smart cities","smart societies","ubiquitous environments"],"keywords_other":["Internet of thing (IOT)","Probabilistic methods","smart societies","Technological development","High performance computing","Management of ubiquitous environments","Ubiquitous environments","Computational and artificial intelligences"],"max_cite":13.0,"pub_year":2017.0,"sources":"['scp', 'ieee']","rawkeys":["ubiquitous environments","probabilistic methods","data fusion","smart societies","big data","smart cities","internet of thing (iot)","internet of things","computational and artificial intelligence","technological development","high performance computing","management of ubiquitous environments","computational and artificial intelligences"],"tags":["ubiquitous environments","probabilistic methods","data fusion","smart societies","big data","computational and artificial intelligence","technological development","high performance computing","management of ubiquitous environments","internet of things (iot)","smart cities"]},{"p_id":22082,"title":"Quality of Experience in the Multimedia Internet of Things: Definition and practical use-cases","abstract":"\u00a9 2015 IEEE. In this paper, a first approach for evaluating the Quality of Experience (QoE) for IoT (Internet of Things) applications is presented. Firstly, a layered IoT architecture is analysed to understand which QoE influence factors have to be considered in relevant application scenarios. Secondly, we introduce the concept of Multimedia IoT (MIoT) and define a layered QoE model aimed at evaluating and combining the contributions of each influence factor to estimate the overall QoE in MIoT applications. Finally, we present a vehicular MIoT application that has been used to conduct subjective quality assessments to verify the performance of the proposed approach.","keywords_author":["Internet of Things","Quality of Experience"],"keywords_other":["Application scenario","Practical use","QoE models","Iot architectures","Quality of experience (QoE)","Multimedia internet","Subjective quality assessments"],"max_cite":13.0,"pub_year":2015.0,"sources":"['scp', 'wos']","rawkeys":["iot architectures","multimedia internet","practical use","internet of things","application scenario","quality of experience (qoe)","qoe models","subjective quality assessments","quality of experience"],"tags":["iot architectures","multimedia internet","practical use","application scenario","quality of experience (qoe)","qoe models","subjective quality assessments","internet of things (iot)"]},{"p_id":46658,"title":"Internet of things and machine learning convergence: The E-healthcare revolution","abstract":"\u00a9 2017 Association for Computing Machinery. The relationship between Machine learning and the Internet of things is manifested through many products in many sectors. One of the incarnations of these two technological trends is E-healthcare, which provide real-time data and allow continuous patient monitoring through prediction, anticipation and intelligent decision making. The convergence of Internet of Things IoT and Machine learning has lot of scope for research. In this paper we investigate the convergence of this two technologies in the medical healthcare area. Furthermore we present a survey of related works on IoT and ML in e-healthcare during the last five years in its general architecture and application.","keywords_author":["E-healthcare","IoT","Machine learning"],"keywords_other":["Related works","E-healthcare","General architectures","Intelligent decision making","Real-time data","Technological trends"],"max_cite":0.0,"pub_year":2017.0,"sources":"['scp']","rawkeys":["related works","general architectures","e-healthcare","machine learning","intelligent decision making","technological trends","real-time data","iot"],"tags":["related works","general architectures","e-healthcare","machine learning","intelligent decision making","technological trends","real-time data","internet of things (iot)"]},{"p_id":97862,"title":"Cloud and IoT based disease prediction and diagnosis system for healthcare using Fuzzy neural classifier","abstract":"Recently, the mobile health care (m-healthcare) applications with Internet of Things (IoT) are providing the various dimensionalities and the online services. These applications have provided a new platform to the millions of people for getting benefit over the health tips frequently for living a healthy life. After the introduction of IoT technology and the related devices which are used in medical field, strengthened the various features of these healthcare online applications. The huge volume of big data is generated by IoT devices in healthcare environment. Cloud computing technology is used to handle the large volume of data and also provide the ease of use. In this scenario, cloud based applications are playing major role in this fast world. These medical applications are also used the Cloud Computing technology for secured storage and accessibility. For availing better services to the people over the online healthcare applications, we propose a new Cloud and IoT based Mobile Health care application for monitoring and diagnosing the serious diseases. Here, a new framework is developed for the public. In this work, a new systematic approach is used for the diabetes diseases and the related medical data is generated by using the UCI Repository dataset and the medical sensors for predicting the people who has affected with diabetes severely. In addition, we propose a new classification algorithm called Fuzzy Rule based Neural Classifier for diagnosing the disease and the severity. The experiments have been conducted by the standard UCI Repository dataset and the real health records which are collected from various hospitals. The experimental results show that the performance of the proposed work which outperforms the existing systems for disease prediction. (C) 2018 Elsevier B.V. All rights reserved.","keywords_author":["User Diagnosis Result (UDR)","Smart Student Interactive System (SSIS)","Cloud computing","Internet of Things (IoT)","m-health"],"keywords_other":["MODEL","THINGS","FRAMEWORK","INTERNET"],"max_cite":0.0,"pub_year":2018.0,"sources":"['wos']","rawkeys":["user diagnosis result (udr)","cloud computing","model","things","framework","m-health","smart student interactive system (ssis)","internet","internet of things (iot)"],"tags":["user diagnosis result (udr)","cloud computing","model","things","framework","mobile health","smart student interactive system (ssis)","internet","internet of things (iot)"]},{"p_id":36429,"title":"Effective Features to Classify Big Data Using Social Internet of Things","abstract":"\u00a9 2018 IEEE.Social Internet of Things (SIoT) supports many novel applications and networking services for the IoT in a more powerful and productive way. In this paper, we have introduced a hierarchical framework for feature extraction in SIoT big data using map-reduced framework along with a supervised classifier model. Moreover, a Gabor filter is used to reduce noise and unwanted data from the database, and Hadoop Map Reduce has been used for mapping and reducing big databases, to improve the efficiency of the proposed work. Furthermore, the feature selection has been performed on a filtered data set by using Elephant Herd Optimization. The proposed system architecture has been implemented using Linear Kernel Support Vector Machine-based classifier to classify the data and for predicting the efficiency of the proposed work. From the results, the maximum accuracy, specificity, and sensitivity of our work is 98.2%, 85.88%, and 80%, moreover analyzed time and memory, and these results have been compared with the existing literature.","keywords_author":["big data","feature selection","Internet of Things","machine Learning","social Internet of Things"],"keywords_other":["Map-reduce","Linear kernel","Novel applications","Networking services","System architectures","Supervised classifiers"],"max_cite":1.0,"pub_year":2018.0,"sources":"['scp']","rawkeys":["social internet of things","novel applications","big data","networking services","system architectures","internet of things","machine learning","supervised classifiers","feature selection","map-reduce","linear kernel"],"tags":["social internet of things","novel applications","big data","system architectures","network services","machine learning","supervised classifiers","feature selection","map-reduce","linear kernel","internet of things (iot)"]},{"p_id":48723,"title":"An agricultural monitoring system based on wireless sensor and depth learning algorithm","abstract":"The rise and development of the Internet of Things (IoT) have given birth to the frontier technology of the agricultural IoT, which marks the future trend in agriculture and the IoT. The agricultural IoT can be combined with Zigbee, a short-range wireless network technology for monitoring systems, to solve the excessively large planting area and other defects in agricultural production. Meanwhile, the modernization of planting and harvesting has set the stage for deep learning. Unlike the artificial neural network, the deep learning is an important intelligent algorithm, capable of solving many real-world problems. Therefore, this paper probes into the problems of modern automatic agriculture. First, the routing allocation technology and transmission mode were optimized to solve the energy consumption problem. Second, the classification model based on deep learning algorithm was put forward according to the application of the Wireless Sensor Network (WSN) in continuous monitoring of soil temperature and humidity. Despite the lack of upper soil humidity sensor in agriculture, the model can still classify the soil moisture of the nodes, and derive the main soil moisture content. Finally, a solution was presented based on agricultural ZigBee WSN technology. In addition to cheap cost and low power consumption, the solution has the functions of reminding and recognition due to the adoption of artificial intelligence algorithm. Suffice it to say that the solution is a successful attempt to integrate artificial intelligence and sensor technology into agricultural modernization.","keywords_author":["Deep learning","Modern agriculture","ZigBee"],"keywords_other":["Internet of thing (IOT)","Artificial intelligence algorithms","Modern agricultures","Energy consumption problems","Short-range wireless networks","Agricultural productions","Agricultural monitoring","Agricultural modernizations"],"max_cite":0.0,"pub_year":2017.0,"sources":"['scp']","rawkeys":["deep learning","agricultural productions","internet of thing (iot)","agricultural modernizations","energy consumption problems","short-range wireless networks","zigbee","artificial intelligence algorithms","modern agriculture","modern agricultures","agricultural monitoring"],"tags":["agricultural productions","machine learning","agricultural modernizations","energy consumption problems","short-range wireless networks","internet of things (iot)","zigbee","artificial intelligence algorithms","modern agricultures","agricultural monitoring"]},{"p_id":15963,"title":"Enhanced Fingerprinting and Trajectory Prediction for IoT Localization in Smart Buildings","abstract":"\u00a9 2016 IEEE. Location service is one of the primary services in smart automated systems of Internet of Things (IoT). For various location-based services, accurate localization has become a key issue. Recently, research on IoT localization systems for smart buildings has been attracting increasing attention. In this paper, we propose a novel localization approach that utilizes the neighbor relative received signal strength to build the fingerprint database and adopts a Markov-chain prediction model to assist positioning. The approach is called the novel localization method (LNM) in short. In the proposed LNM scheme, the history data of the pedestrian's locations are analyzed to further lower the unpredictable signal fluctuations in a smart building environment, meanwhile enabling calibration-free positioning for various devices. The performance evaluation conducted in a realistic environment shows that the presented method demonstrates superior localization performance compared with well-known existing schemes, especially when the problems of device heterogeneity and WiFi signals fluctuation exist.","keywords_author":["Fingerprint","Internet of Things (IoT)","Markov chain","mobile positioning","smart building"],"keywords_other":["Localization performance","Received signal strength","Fingerprint database","Device heterogeneities","Realistic environments","Building environment","Internet of Things (IOT)","Trajectory prediction"],"max_cite":40.0,"pub_year":2016.0,"sources":"['scp']","rawkeys":["realistic environments","trajectory prediction","building environment","fingerprint database","received signal strength","device heterogeneities","smart building","localization performance","markov chain","fingerprint","internet of things (iot)","mobile positioning"],"tags":["realistic environments","markov chains","smart buildings","building environment","trajectory prediction","fingerprint database","device heterogeneities","recommender systems","localization performance","fingerprint","internet of things (iot)","mobile positioning"]},{"p_id":44635,"title":"Compression Header Analyzer Intrusion Detection System (CHA - IDS) for 6LoWPAN Communication Protocol","abstract":"\u00a9 2018 IEEE. Prior 6LoWPAN intrusion detection system (IDS) utilized several features to detect various malicious activities. However, these IDS methods only detect specific attack but fails when the attacks are combined. In this paper, we propose an IDS known as compression header analyzer intrusion detection system (CHA-IDS) that analyzes 6LoWPAN compression header data to mitigate the individual and combination routing attacks. CHA-IDS is a multi-agent system framework that capture and manage raw data for data collection, analysis, and system actions. The proposed CHA-IDS utilize best first and greedy stepwise with correlation-based feature selection to determine only significant features needed for the intrusion detection. These features are then tested using six machine learning algorithms to find the best classification method that able to distinguish between an attack and non-attack and then from the best classification method, we devise a rule to be implemented in Tmote Sky. To ensure the reliability of our proposed method, we evaluate the CHA-IDS with three types of combination attacks known as hello flood, sinkhole, and wormhole. We also compare our results in term of accuracy of detection, energy overhead, and memory consumption with the prior 6LoWPAN-IDS implementation such as SVELTE and Pongle's IDS. The results show that CHA-IDS performs better than the aforementioned methods with 99% true positive rate and consumed low energy overhead and memory that fit in constrained device such Tmote Sky.","keywords_author":["6LoWPAN","compression header","Internet of Things","machine learning","routing attack","RPL","security"],"keywords_other":["Classification methods","Intrusion Detection Systems","Security","True positive rates","Routing","6LoWPAN","Routing attacks","Malicious activities"],"max_cite":0.0,"pub_year":2018.0,"sources":"['scp']","rawkeys":["classification methods","routing attack","routing attacks","true positive rates","routing","intrusion detection systems","internet of things","machine learning","rpl","security","malicious activities","6lowpan","compression header"],"tags":["classification methods","routing attacks","true positive rates","routing","intrusion detection systems","machine learning","rpl","security","malicious activities","6lowpan","compression header","internet of things (iot)"]},{"p_id":42595,"title":"DNN Engine: A 28-nm Timing-Error Tolerant Sparse Deep Neural Network Processor for IoT Applications","abstract":"IEEE This paper presents a 28-nm system-on-chip (SoC) for Internet of things (IoT) applications with a programmable accelerator design that implements a powerful fully connected deep neural network (DNN) classifier. To reach the required low energy consumption, we exploit the key properties of neural network algorithms: parallelism, data reuse, small\/sparse data, and noise tolerance. We map the algorithm to a very large scale integration (VLSI) architecture based around an single-instruction, multiple-data data path with hardware support to exploit data sparsity by completely eliding unnecessary computation and data movement. This approach exploits sparsity, without compromising the parallel computation. We also exploit the inherent algorithmic noise-tolerance of neural networks, by introducing circuit-level timing violation detection to allow worst case voltage guard-bands to be minimized. The resulting intermittent timing violations may result in logic errors, which conventionally need to be corrected. However, in lieu of explicit error correction, we cope with this by accentuating the noise tolerance of neural networks. The measured test chip achieves high classification accuracy (98.36&#x0025; for the MNIST test set), while tolerating aggregate timing violation rates &#x003E;10&#x207B;&#x00B9;. The accelerator achieves a minimum energy of 0.36 &#x03BC;J\/inference at 667 MHz; maximum throughput at 1.2 GHz and 0.57 &#x03BC;J\/inference; or a 10&#x0025; margined operating point at 1 GHz and 0.58 &#x03BC;J\/inference.","keywords_author":["Deep neural networks (DNNs)","Hardware","hardware accelerators","Internet of Things","Internet of things (IoT)","machine learning (ML)","Neural networks","Neurons","Random access memory","razor","System-on-chip","system-on-chip (SoC)","Timing","timing error detection and correction","timing error tolerance."],"keywords_other":["razor","Timing","Random access memory","Timing error detection","Timing errors","System on chips (SoC)","Internet of Things (IOT)","Hardware accelerators"],"max_cite":0.0,"pub_year":2018.0,"sources":"['scp']","rawkeys":["hardware accelerators","razor","random access memory","machine learning (ml)","neural networks","system on chips (soc)","timing error detection","timing error detection and correction","internet of things","timing","system-on-chip","timing error tolerance","deep neural networks (dnns)","system-on-chip (soc)","hardware","timing errors","neurons","internet of things (iot)"],"tags":["hardware accelerators","razor","random access memory","neural networks","timing error detection","timing error detection and correction","machine learning","timing error tolerance","system-on-chip","time","convolutional neural network","hardware","timing errors","neurons","internet of things (iot)"]},{"p_id":83558,"title":"A monitoring method of semiconductor manufacturing processes using Internet of Things-based big data analysis","abstract":"This article proposes an intelligent monitoring system for semiconductor manufacturing equipment, which determines spec-in or spec-out for a wafer in process, using Internet of Things-based big data analysis. The proposed system consists of three phases: initialization, learning, and prediction in real time. The initialization sets the weights and the effective steps for all parameters of equipment to be monitored. The learning performs a clustering to assign similar patterns to the same class. The patterns consist of a multiple time-series produced by semiconductor manufacturing equipment and an after clean inspection measured by the corresponding tester. We modify the Line, Buzo, and Gray algorithm for classifying the time-series patterns. The modified Line, Buzo, and Gray algorithm outputs a reference model for every cluster. The prediction compares a time-series entered in real time with the reference model using statistical dynamic time warping to find the best matched pattern and then calculates a predicted after clean inspection by combining the measured after clean inspection, the dissimilarity, and the weights. Finally, it determines spec-in or spec-out for the wafer. We will present experimental results that show how the proposed system is applied on the data acquired from semiconductor etching equipment.","keywords_author":["Monitoring","learning","prediction","matched pattern","Internet of Things","reference model"],"keywords_other":["INFORMATION","CLASSIFICATION","SYSTEM","MODEL","RECOGNITION","SERVICES"],"max_cite":0.0,"pub_year":2017.0,"sources":"['wos']","rawkeys":["matched pattern","recognition","model","prediction","internet of things","learning","system","classification","information","services","reference model","monitoring"],"tags":["recognition","model","prediction","machine learning","system","classification","information","services","reference modeling","matching patterns","internet of things (iot)","monitoring"]},{"p_id":44649,"title":"IOT device code translators using LSTM networks","abstract":"\u00a9 2017 IEEE. Currently, the Internet of Things (IOT) platform used by large buildings to manage the indoor climate uses different controllers and sensors from multiple manufacturers. Communication between these devices requires a human in the loop to translate each devices data to to be compatible with a common integration engine and storage historian. The subject matter expert needs to decipher the non-standard naming convention used for each device and manually translate thousands of codes each time a new device has to be integrated. To aid the human translator, we propose a technique to implement a smart translator using Deep Neural Networks (DNN) by automatically assigning any registers with recognized data patterns to standardized labels.","keywords_author":["Deep learning","LSTM","Seq2seq","Smart translators","Tensorflow","Vector embedding"],"keywords_other":["Code translators","Tensorflow","Internet of thing (IOT)","Large buildings","Subject matter experts","LSTM","Seq2seq","Human-in-the-loop"],"max_cite":0.0,"pub_year":2018.0,"sources":"['scp']","rawkeys":["lstm","deep learning","subject matter experts","large buildings","internet of thing (iot)","tensorflow","code translators","vector embedding","human-in-the-loop","smart translators","seq2seq"],"tags":["tensorflow","subject matter experts","large buildings","long short-term memory","machine learning","code translators","vector embedding","human-in-the-loop","smart translators","internet of things (iot)","seq2seq"]},{"p_id":36465,"title":"A Hierarchical Inference Model for Internet-of-Things","abstract":"IEEE Internet-of-Things (IoT) connects billions of devices that are already collecting zettabytes of data. Current IoT framework suffers from limited sensor energy, communication bandwidth, and server storage. Compact smart sensors can collect data, extract features, derive local inferences, and transmit only inference outcomes. This can dramatically cut down on the amount of sensor data transmitted, and hence its communication energy and network traffic. However, edge or server inference models trained with conventional machine learning approaches only cater to the traditional sense-and-transmit paradigm. This undoes the energy benefits brought by smart sensors. We propose a hierarchical inference model for IoT applications based on hierarchical learning and local inferences. It generalizes sensor-level inference to inference at other edge nodes by exploiting the sensor\/edge-grouped IoT data structure. We verify our approach with seven IoT applications, demonstrating that the model is accurate, efficient, and generally applicable. We derive four edge-level inference models and four server-level inference models for these applications. For four edge-level inference models, we reduce the number of bits transmitted from the sensor by <formula><tex>$3.2 \\times-42.7 \\times$<\/tex><\/formula> while improving the accuracy by 0.3%-6.7%. For four server-level inference models, we reduce the number of edge-to-server bits transmitted by <formula><tex>$17 \\times-60 \\times$<\/tex><\/formula>, with accuracy change in <formula><tex>$-0.4%-+0.1%$<\/tex><\/formula> range.","keywords_author":["Computational modeling","Data models","Energy efficiency","Feature extraction","hierarchical inference","hierarchical learning","Intelligent sensors","Internet-of-Things","machine learning","Robot sensing systems","Servers","smart sensors","Support vector machines"],"keywords_other":["Hierarchical learning","Robot sensing system","Computational model","hierarchical inference","Intelligent sensors"],"max_cite":1.0,"pub_year":2018.0,"sources":"['scp']","rawkeys":["computational modeling","intelligent sensors","hierarchical inference","robot sensing systems","machine learning","servers","internet-of-things","robot sensing system","data models","feature extraction","smart sensors","support vector machines","energy efficiency","computational model","hierarchical learning"],"tags":["computational modeling","intelligent sensors","hierarchical inference","robot sensing systems","machine learning","servers","internet of things (iot)","data models","feature extraction","smart sensors","energy efficiency","hierarchical learning"]},{"p_id":36471,"title":"How to improve fault tolerance in disaster predictions: A case study about flash floods using IoT, ML and real data","abstract":"\u00a9 2018 by the authors. Licensee MDPI, Basel, Switzerland. The rise in the number and intensity of natural disasters is a serious problem that affects the whole world. The consequences of these disasters are significantly worse when they occur in urban districts because of the casualties and extent of the damage to goods and property that is caused. Until now feasible methods of dealing with this have included the use of wireless sensor networks (WSNs) for data collection and machine-learning (ML) techniques for forecasting natural disasters. However, there have recently been some promising new innovations in technology which have supplemented the task of monitoring the environment and carrying out the forecasting. One of these schemes involves adopting IP-based (Internet Protocol) sensor networks, by using emerging patterns for IoT. In light of this, in this study, an attempt has been made to set out and describe the results achieved by SENDI (System for dEtecting and forecasting Natural Disasters based on IoT). SENDI is a fault-tolerant system based on IoT, ML and WSN for the detection and forecasting of natural disasters and the issuing of alerts. The system was modeled by means of ns-3 and data collected by a real-world WSN installed in the town of S\u00e3o Carlos - Brazil, which carries out the data collection from rivers in the region. The fault-tolerance is embedded in the system by anticipating the risk of communication breakdowns and the destruction of the nodes during disasters. It operates by adding intelligence to the nodes to carry out the data distribution and forecasting, even in extreme situations. A case study is also included for flash flood forecasting and this makes use of the ns-3 SENDI model and data collected by WSN.","keywords_author":["Disaster forecast","Fault tolerance","Internet of things","Machine learning","Wireless sensor networks"],"keywords_other":["Natural disasters","Data collection","Communication breakdowns","Data distribution","Wireless sensor network (WSNs)","Emerging patterns","Disaster prediction","Fault tolerant systems"],"max_cite":1.0,"pub_year":2018.0,"sources":"['scp']","rawkeys":["disaster prediction","emerging patterns","natural disasters","communication breakdowns","internet of things","disaster forecast","fault tolerant systems","machine learning","wireless sensor network (wsns)","data collection","data distribution","wireless sensor networks","fault tolerance"],"tags":["disaster prediction","emerging patterns","natural disasters","communication breakdowns","machine learning","disaster forecast","fault tolerant systems","data collection","data distribution","wireless sensor networks","fault tolerance","internet of things (iot)"]},{"p_id":38525,"title":"Generalized hand gesture recognition for wearable devices in iot: Application and implementation challenges","abstract":"\u00a9 Springer International Publishing Switzerland 2016.The proliferation of low power and low cost continuous sensing technology is enabling new and innovative applications in wearables and Internet of Things (IoT). At the same time, new applications are creating challenges to maintain real-time response in a resource-constrained device, while maintaining an acceptable performance. In this paper, we describe an IMU (Inertial Measurement Unit) sensor-based generalized hand gesture recognition system, its applications, and the challenges involved in implementing the algorithm in a resource-constrained device. We have implemented a simple algorithm for gesture spotting that substantially reduces the false positives. The gesture recognition model was built using the data collected from 52 unique subjects. The model was mapped onto Intel\u00ae Quark\u2122 SE Pattern Matching Engine, and fieldtested using 8 additional subjects achieving 92% performance.","keywords_author":["Feature engineering","Gesture recognition","Internet of Things","Machine learning","Pattern recognition","Wearable device"],"keywords_other":["Hand-gesture recognition","Inertial measurement unit","Resourceconstrained devices","Real time response","Feature engineerings","Acceptable performance","Internet of Things (IOT)","Wearable devices"],"max_cite":1.0,"pub_year":2016.0,"sources":"['scp']","rawkeys":["wearable device","wearable devices","feature engineering","gesture recognition","internet of things","machine learning","resourceconstrained devices","feature engineerings","real time response","acceptable performance","hand-gesture recognition","pattern recognition","inertial measurement unit","internet of things (iot)"],"tags":["real-time response","wearable devices","gesture recognition","machine learning","resourceconstrained devices","feature engineerings","acceptable performance","hand-gesture recognition","pattern recognition","inertial measurement unit","internet of things (iot)"]},{"p_id":42622,"title":"DeepNap: Data-Driven Base Station Sleeping Operations through Deep Reinforcement Learning","abstract":"IEEE Base station sleeping is an effective way to reduce the energy consumption of mobile networks. Previous efforts to design sleeping control algorithms mainly rely on stochastic traffic models and analytical derivation. However the tractability of models often conflicts with the complexity of real-world traffic, making it difficult to apply in reality. In this paper, we propose a data-driven algorithm for dynamic sleeping control called DeepNap. This algorithm uses a Deep Q-network (DQN) to learn effective sleeping policies from high-dimensional raw observations or un-quantized systems state vectors. We propose to enhance the original DQN algorithm with action-wise experience replay and adaptive reward scaling to deal with the challenges in non-stationary traffic. We also provide a model-assisted variant of DeepNap through the Dyna framework for inferring and simulating system dynamics. Periodical traffic modeling makes it possible to capture the non-stationarity in real-world traffic and the incorporation with DQN allows for feature learning and generalization from model outputs. Experiments show that both the end-to-end and the model-assisted version of DeepNap outperform table-based Q-learning algorithm and the non-stationarity enhancements improve the stability of vanilla DQN.","keywords_author":["Adaptation models","Analytical models","Base station sleeping","deep Q-network","deep reinforcement learning","Heuristic algorithms","Hidden Markov models","Internet of Things","Machine learning","non-stationary traffic.","Training"],"keywords_other":["Adaptation models","Non-stationary traffics","Experience replay","Stochastic traffic","Sleeping controls","Non-stationarities","Data-driven algorithm","Q-learning algorithms"],"max_cite":0.0,"pub_year":2018.0,"sources":"['scp']","rawkeys":["stochastic traffic","hidden markov models","data-driven algorithm","base station sleeping","heuristic algorithms","non-stationary traffic","sleeping controls","training","internet of things","adaptation models","machine learning","deep reinforcement learning","analytical models","non-stationarities","non-stationary traffics","deep q-network","q-learning algorithms","experience replay"],"tags":["stochastic traffic","hidden markov models","data-driven algorithm","base station sleeping","heuristic algorithms","sleeping controls","training","machine learning","adaptation models","deep reinforcement learning","internet of things (iot)","analytical models","non-stationarities","non-stationary traffics","deep q-network","q-learning algorithms","experience replay"]},{"p_id":50815,"title":"HYFRAMAL: An innovative automation framework for internet of things","abstract":"With the surge of cheaper electronics and newer computing technologies, the dream of an ideal smart automation system is slowly being realized. However, despite many proposals for a smart automation system, a system that balances both complexity and flexibility is not yet presented. In this paper, such a solution is proposed that enables the smart automation system to take complex decisions like machine learning routines despite being based on the lightweight HTTP system. The system also has unique features such as authentication, to ensure security and a flexible structure using RESTful API, so that new functionalities can be easily added. Finally, the system caters to a wide variety of users and developers. Copyright is held by the owner\/author(s). Publication rights licensed to ACM.","keywords_author":["Automation","Framework","Internet of things","K-means algorithm","Machine learning","REST","Smart homes"],"keywords_other":["Smart homes","k-Means algorithm","REST","Complex decision","Automation systems","Framework","Computing technology","Unique features"],"max_cite":0.0,"pub_year":2015.0,"sources":"['scp']","rawkeys":["unique features","smart homes","framework","complex decision","internet of things","machine learning","automation","automation systems","computing technology","k-means algorithm","rest"],"tags":["unique features","automated","smart homes","framework","automated systems","complex decision","machine learning","k-means algorithm","computer technology","rest","internet of things (iot)"]},{"p_id":42623,"title":"UbeHealth: A personalized ubiquitous cloud and edge-enabled networked healthcare system for smart cities","abstract":"\u00a9 2013 IEEE. Smart city advancements are driving massive transformations of healthcare, the largest global industry. The drivers include increasing demands for ubiquitous, preventive, and personalized healthcare, to be provided to the public at reduced risks and costs. Mobile cloud computing could potentially meet the future healthcare demands by enabling anytime, anywhere capture and analyses of patients' data. However, network latency, bandwidth, and reliability are among the many challenges hindering the realization of next-generation healthcare. This paper proposes a ubiquitous healthcare framework, UbeHealth, that leverages edge computing, deep learning, big data, high-performance computing (HPC), and the Internet of Things (IoT) to address the aforementioned challenges. The framework enables an enhanced network quality of service using its three main components and four layers. Deep learning, big data, and HPC are used to predict network traffic, which in turn are used by the Cloudlet and network layers to optimize data rates, data caching, and routing decisions. Application protocols of the traffic flows are classified, enabling the network layer to meet applications' communication requirements better and to detect malicious traffic and anomalous data. Clustering is used to identify the different kinds of data originating from the same application protocols. A proof of concept UbeHealth system has been developed based on the framework. A detailed literature review is used to capture the design requirements for the proposed system. The system is described in detail including the algorithmic implementation of the three components and four layers. Three widely used data sets are used to evaluate the UbeHealth system.","keywords_author":["cloud computing","Cloudlets","deep learning","fog computing","Internet of Things (IoT)","mobile edge computing","mobile healthcare","multimedia applications","preventive healthcare","smart cities","survey","traffic classification","traffic prediction","Cloudlets","deep learning","Internet of Things (IoT)","mobile edge computing","mobile healthcare","preventive healthcare","traffic classification","traffic prediction","survey","fog computing","cloud computing","multimedia applications","smart cities"],"keywords_other":["AREA NETWORKS","CHALLENGES","Medical services","CHRONIC DISEASES","BIG DATA","WIRELESS SENSOR NETWORKS","Traffic classification","Traffic prediction","MOBILE-HEALTH","TECHNOLOGIES","SIGNAL STRENGTH","TRAFFIC CLASSIFICATION","Cloudlets","INTERNET","Multimedia applications","Internet of Things (IOT)","Mobile Edge Computing"],"max_cite":0.0,"pub_year":2018.0,"sources":"['ieee', 'scp', 'wos']","rawkeys":["big data","multimedia applications","challenges","medical services","wireless sensor networks","internet","technologies","internet of things (iot)","chronic diseases","survey","cloudlets","mobile-health","mobile edge computing","mobile healthcare","deep learning","signal strength","area networks","preventive healthcare","cloud computing","fog computing","traffic classification","traffic prediction","smart cities"],"tags":["big data","multimedia applications","challenges","technology","medical services","signal strengths","internet","wireless sensor networks","internet of things (iot)","survey","cloudlets","machine learning","chronic disease","mobile edge computing","mobile healthcare","mobile health","area networks","preventive healthcare","cloud computing","fog computing","traffic classification","traffic prediction","smart cities"]},{"p_id":36486,"title":"A data-driven approach to developing IoT privacy-setting interfaces","abstract":"Copyright \u00a9 2018 ACM. User testing is often used to inform the development of user interfaces (UIs). But what if an interface needs to be developed for a system that does not yet exist? In that case, existing datasets can provide valuable input for UI development. We apply a data-driven approach to the development of a privacy-setting interface for Internet-of-Things (IoT) devices. Applying machine learning techniques to an existing dataset of users' sharing preferences in IoT scenarios, we develop a set of \"smart\" default profiles. Our resulting interface asks users to choose among these profiles, which capture their preferences with an accuracy of 82%-a 14% improvement over a naive default setting and a 12% improvement over a single smart default setting for all users.","keywords_author":["Data-driven design","Internet of things","Machine learning","Privacy settings"],"keywords_other":["Default setting","Data-driven approach","User testing","Data-driven design","Internet of Things (IOT)","Machine learning techniques","Privacy Settings"],"max_cite":1.0,"pub_year":2018.0,"sources":"['scp']","rawkeys":["data-driven design","data-driven approach","machine learning techniques","internet of things","machine learning","user testing","default setting","privacy settings","internet of things (iot)"],"tags":["data-driven design","data-driven approach","machine learning techniques","machine learning","user tests","default setting","privacy settings","internet of things (iot)"]},{"p_id":42632,"title":"Securing the Internet of Things in the Age of Machine Learning and Software-defined Networking","abstract":"IEEE The Internet of Things (IoT) realizes a vision where billions of interconnected devices are deployed just about everywhere, from inside our bodies to the most remote areas of the globe. As the IoT will soon pervade every aspect of our lives and will be accessible from anywhere, addressing critical IoT security threats is now more important than ever. Traditional approaches where security is applied as an afterthought and as a &#x201C;patch&#x201D; against known attacks are insufficient. Indeed, next-generation IoT challenges will require a new secure-by-design vision, where threats are addressed proactively and IoT devices learn to dynamically adapt to different threats. To this end, machine learning and software-defined networking will be key to provide both reconfigurability and intelligence to the IoT devices. In this paper, we first provide a taxonomy and survey the state of the art in IoT security research, and offer a roadmap of concrete research challenges related to the application of machine learning and software-defined networking to address existing and next-generation IoT security threats.","keywords_author":["Authentication","Automobiles","Challenges","Hardware","Internet of Things","Internet of Things","Machine learning","Perspective.","Security","Security","Sensors","Survey","Trust"],"keywords_other":["Concrete research","Traditional approaches","Internet of thing (IOT)","Perspective","Trust","Security","Re-configurability","Challenges"],"max_cite":0.0,"pub_year":2018.0,"sources":"['scp']","rawkeys":["traditional approaches","authentication","sensors","survey","concrete research","internet of thing (iot)","internet of things","machine learning","challenges","perspective","security","trust","re-configurability","hardware","automobiles"],"tags":["traditional approaches","authentication","sensors","reconfigurable","survey","concrete research","machine learning","perspective","challenges","security","trust","hardware","automobiles","internet of things (iot)"]},{"p_id":46729,"title":"IoT at the grassroots - Exploring the use of sensors for livestock monitoring","abstract":"\u00a9 2017 IIMC \/ IST-Africa. In this work we explore the use of cheap sensors to monitor the activities of dairy cattle with the aim of using these sensors as part of a system to detect important events such as when a cow is ill or on heat. This draws on advances in human activity recognition (HAR) where wearable sensors are used to collect data and infer human activity. Our sensor system is based on the Raspberry Pi microprocessor interfaced to an accelerometer sensor. We explore the use of simple machine learning techniques to infer activity from the data we collect and show that our simple system has the potential to detect different animal activities such as walking, standing and feeding. We also test the system on detection of human activities collected under controlled conditions to demonstate the potential use of the system. We envision an internet of things (IoT) system with cows in a herd mounted with appropriate sensors which relay information to servers over the internet. Farmers are then able to access information about their cattle at any time and take appropriate action when events of interest are detected.","keywords_author":["Activity detection","Internet of things","Machine learning","Raspberry Pi"],"keywords_other":["Controlled conditions","Animal activities","Activity detection","Raspberry pi","Human activity recognition","Accelerometer sensor","Internet of Things (IOT)","Machine learning techniques"],"max_cite":0.0,"pub_year":2017.0,"sources":"['scp']","rawkeys":["controlled conditions","machine learning techniques","human activity recognition","internet of things","activity detection","machine learning","internet of things (iot)","raspberry pi","animal activities","accelerometer sensor"],"tags":["controlled conditions","machine learning techniques","human activity recognition","machine learning","activity detection","internet of things (iot)","raspberry pi","animal activities","accelerometer sensor"]},{"p_id":36495,"title":"A load balancing scheme with Loadbot in IoT networks","abstract":"\u00a9 2017, Springer Science+Business Media, LLC. The Internet of Things\u2019 contribution lies in the increased value of information created by the number of interconnections among things and the subsequent transformation of processed information into knowledge for the benefit of society. The Internet of Things\u2019 sensors are deployed to monitor one or more events in an unattended environment. A large number of event data will be generated over a period of time in the Internet of Things. In the future, hundreds of billions of smart sensors and devices will interact with one another, without human intervention. Also, they will generate a large amount of data and resolutions, providing humans with information and the control of events and objects, even in remote physical environments. However, the demands of the Internet of Things cause heavy traffic or bottlenecks on particular nodes or on the paths of Internet of Things networks. Therefore, to resolve this issue, we propose an agent, Loadbot, that measures network loads and processes structural configurations by analyzing a large amount of user data and network loads. Additionally, in order to achieve efficient load balancing in the Internet of Things, we propose applying Deep Learning\u2019s Deep Belief Network method. Finally, using mathematical analysis, we address the key functions of our proposed scheme and simulate the efficiency of our proposed scheme.","keywords_author":["Deep belief network","Deep learning","Internet of Things (IoT)","Load balancing","Smart sensors","Internet of Things (IoT)","Load balancing","Deep learning","Deep belief network","Smart sensors"],"keywords_other":["Mathematical analysis","Structural configurations","Load-balancing schemes","Deep belief networks","Processed information","M2M COMMUNICATIONS","Value of information","Physical environments","Internet of Things (IOT)"],"max_cite":1.0,"pub_year":2018.0,"sources":"['scp', 'wos']","rawkeys":["physical environments","processed information","m2m communications","deep learning","mathematical analysis","structural configurations","deep belief network","load-balancing schemes","load balancing","smart sensors","value of information","deep belief networks","internet of things (iot)"],"tags":["process information","physical environments","m2m communications","mathematical analysis","machine learning","structural configurations","load-balancing schemes","smart sensors","load balancing","value of information","deep belief networks","internet of things (iot)"]},{"p_id":42642,"title":"Data Driven Feature Selection for Machine Learning Algorithms in Computer Vision","abstract":"IEEE Feature selection is a key factor for the performance of machine learning algorithms, as not all data and hence features are related to the various tasks. In this paper, we propose a novel scheme for convolutional feature selection for machine learning algorithms in computer vision. As not all the convolutional features are related to visual tracking, removing the unrelated ones will dramatically reduce the complexity and improve the algorithm performance. However, how to identify and select features related to the visual tracking task is still a challenge for machine learning algorithms. In the proposed scheme, a novel adaptive weights-objective function approach is established to evaluate and select the features. Furthermore, a quadratic programming method is introduced which improves the optimization efficiency. The experimental results demonstrate that our proposed scheme achieves superior performance compared to the state-of-art trackers on the challenging benchmarks in computer vision.","keywords_author":["Computer vision","Computer Vision.","Data Driven Feature Selection","Feature extraction","Internet of Things","Machine Learning","Machine learning algorithms","Target tracking","Task analysis","Visual Tracking","Visualization"],"keywords_other":["Quadratic programming method","Adaptive weights","Objective functions","Visual Tracking","Optimization efficiency","Data driven","Task analysis","Algorithm performance"],"max_cite":0.0,"pub_year":2018.0,"sources":"['scp']","rawkeys":["algorithm performance","task analysis","quadratic programming method","optimization efficiency","machine learning algorithms","adaptive weights","internet of things","machine learning","objective functions","target tracking","visual tracking","data driven feature selection","feature extraction","computer vision","data driven","visualization"],"tags":["algorithm performance","task analysis","quadratic programming method","optimization efficiency","machine learning algorithms","adaptive weights","machine learning","objective functions","target tracking","visual tracking","data driven feature selection","feature extraction","computer vision","data driven","visualization","internet of things (iot)"]},{"p_id":42643,"title":"Self-generation of access control policies","abstract":"\u00a9 2018 Association for Computing Machinery. Access control for information has primarily focused on access statically granted to subjects by administrators usually in the context of a specific system. Even if mechanisms are available for access revocation, revocations must still be executed manually by an administrator. However, as physical devices become increasingly embedded and interconnected, access control needs to become an integral part of the resources being protected and be generated dynamically by the resources depending on the context in which they are being used. In this paper, we discuss a set of scenarios for access control needed in current and future systems and use that to argue that an approach for resources to generate and manage their access control policies dynamically on their own is needed. We discuss some approaches for generating such access control policies that may address the requirements of the scenarios.","keywords_author":["AI","Autonomic access control","Dynamic security","IoT","Machine learning"],"keywords_other":["Integral part","Physical devices","Access control policies","Self-generation","Dynamic security"],"max_cite":0.0,"pub_year":2018.0,"sources":"['scp']","rawkeys":["integral part","autonomic access control","machine learning","dynamic security","access control policies","ai","self-generation","iot","physical devices"],"tags":["integral part","autonomic access control","machine learning","dynamic security","access control policies","self-generation","physical devices","internet of things (iot)"]},{"p_id":42644,"title":"An added value alternative to RAIN RFID items characterization in retail","abstract":"\u00a9 2013 IEEE. This paper deals with the difficulties of detecting UHF passive RFID tags in a RAIN RFID system implemented in a retail environment. The aim of this paper is to evolve from the notion of characterizing a tag to the approach of evaluating a tagged item in a system, in order to better understand the behavior of each item coming to life on the internet of things. To achieve this purpose, real store shelves are mounted and filled with 444 tagged products belonging to 24 different product families. Three automatic inventory systems are installed separately in order to perform two types of evaluation: first, the detectability of each product family and second the clustering of tagged items with similar performances in the same group regardless of their product family. Using inventory systems, this type of evaluation, eliminates the need to invest in characterization equipment and introduces a new way of reliably evaluating tagged items directly in the store. It enables the real-time detecting and targeting low-level tagging and arrangement problems without any prior knowledge of the tagged products present in the reading area and their initial quantities.","keywords_author":["Automatic inventory","clustering","Internet of Things","K-means","machine learning","passive","performance analysis","performances metrics","radiofrequency coverage","RAIN RFID","retail","retail","RFID tags","UHF RFID"],"keywords_other":["UHF RFID","Performances metrics","Passive","K-means","Performance analysis","Clustering","Retail","Radio frequencies","RF-ID tags","Automatic inventory"],"max_cite":0.0,"pub_year":2018.0,"sources":"['scp']","rawkeys":["k-means","performance analysis","passive","retail","internet of things","machine learning","rf-id tags","clustering","automatic inventory","radiofrequency coverage","performances metrics","rain rfid","radio frequencies","rfid tags","uhf rfid"],"tags":["performance analysis","performance metrics","retail","passivity","machine learning","rf-id tags","random forests","clustering","automatic inventory","radiofrequency coverage","rain rfid","kernel methods","uhf rfid","internet of things (iot)"]},{"p_id":34454,"title":"On the application of artificial intelligence techniques to create network intelligence","abstract":"\u00a9 Springer International Publishing Switzerland 2015. Information and Communication Technologies (ICT) growth poses interesting challenges to industry, concerning issues such as scalability, network security, energy management, network monitoring, among others. Several Artificial Intelligence tools can be applied to address many of current ICT challenges. This chapter describes the practical application of Artificial Intelligence (AI) techniques in two main classes of problems: AI for the Internet of Things and Cloud, and usage of AI techniques to manage faults and security issues in traditional Telecommunication networks. Therefore, we will present our research work for the application of AI into different domains, describing for each the current state-of-the-art, and the implemented solution together with the main experimental results. This chapter will demonstrate various benefits achieved from adding an intelligent layer to ICT solutions, in various domains. Finally, we will also address future developments.","keywords_author":["Artificial intelligence","Cloud computing","Internet of things","Machine learning","Telecommunication networks"],"keywords_other":null,"max_cite":2.0,"pub_year":2015.0,"sources":"['scp']","rawkeys":["artificial intelligence","cloud computing","internet of things","machine learning","telecommunication networks"],"tags":["telecommunication networks","machine learning","internet of things (iot)","cloud computing"]},{"p_id":50839,"title":"Executive roundtable series","abstract":"\u00a9 1999-2012 IEEE. Continuing the Executive Roundtable Series hosted by Earley Information Science, this article presents excerpts from the second and third installments of the series. The second roundtable explores the role of analytics in using big data from the Internet of Things (IoT) to monitor, control, and optimize performance, with the ultimate goal of creating autonomous behavior. The full video is available at http:\/\/youtu.be\/moKgBhnRlNg. The third roundtable explores the business potential of machine learning and cognitive computing for customer experience and digital marketing. The full video is available at https:\/\/youtu.be\/rKr6hZ8srzA. An infographic breaking down the entire Executive Roundtable Series is available at https:\/\/s3.amazonaws.com\/ieeecs.cdn.csdl.public\/mags\/it\/2015\/05\/mit2015050062s.pdf.","keywords_author":["big data","cognitive computing","data analytics","Internet of Things","machine learning"],"keywords_other":["Internet of thing (IOT)","Cognitive Computing","Customer experience","Infographic","Digital marketing","Autonomous behaviors","Data analytics"],"max_cite":0.0,"pub_year":2015.0,"sources":"['scp']","rawkeys":["digital marketing","big data","customer experience","data analytics","internet of thing (iot)","internet of things","machine learning","infographic","autonomous behaviors","cognitive computing"],"tags":["digital marketing","cloud computing","big data","customer experience","data analytics","machine learning","infographic","autonomous behaviors","internet of things (iot)"]},{"p_id":9889,"title":"A load balancing scheme based on deep-learning in IoT","abstract":"Extending the current Internet with interconnected objects and devices and their virtual representation has been a growing trend in recent years. The Internet of Things (IoT) contribution is in the increased value of information created by the number of interconnections among things and the transformation of the processed information into knowledge for the benefit of society. Benefit due to the service controlled by communication between objects is now being increased by people who use these services in real life. The sensors are deployed to monitor one or more events in an unattended environment. A large number of the event data will be generated over a period of time in IoT. Hence, the load balancing protocol is critical considerations in the design of IoT. Therefore, we propose an agent Loadbot that measures network load and process structural configuration by analyzing a large amount of user data and network load, and applying Deep Learning's Deep Belief Network method in order to achieve efficient load balancing in IoT. Also, we propose an agent Balancebot that processes a neural load prediction algorithm based on Deep Learning's Q-learning method and neural prior ensemble. We address the key functions for our proposed scheme and simulate the efficiency of our proposed scheme using mathematical analysis.","keywords_author":["Load balancing","Internet of things","Deep belief network","Q-learning"],"keywords_other":null,"max_cite":0.0,"pub_year":2017.0,"sources":"['wos']","rawkeys":["q-learning","internet of things","deep belief network","load balancing"],"tags":["q-learning","deep belief networks","internet of things (iot)","load balancing"]},{"p_id":26277,"title":"Towards a conceptual framework of OSH risk management in smart working environments based on smart PPE, ambient intelligence and the Internet of Things technologies","abstract":"\u00a9 2016 Central Institute for Labour Protection\u2013National Research Institute (CIOP-PIB). Published by Taylor & Francis. Recent developments in domains of ambient intelligence (AmI), Internet of Things, cyber-physical systems (CPS), ubiquitous\/pervasive computing, etc., have led to numerous attempts to apply ICT solutions in the occupational safety and health (OSH) area. A literature review reveals a wide range of examples of smart materials, smart personal protective equipment and other AmI applications that have been developed to improve workers\u2019 safety and health. Because the use of these solutions modifies work methods, increases complexity of production processes and introduces high dynamism into thus created smart working environments (SWE), a new conceptual framework for dynamic OSH management in SWE is called for. A proposed framework is based on a new paradigm of OSH risk management consisting of real-time risk assessment and the capacity to monitor the risk level of each worker individually. A rationale for context-based reasoning in SWE and a respective model of the SWE-dedicated CPS are also proposed.","keywords_author":["ambient intelligence","cyber-physical systems","Internet of Things","occupational safety and health management","real-time risk assessment","smart personal protective equipment","smart working environment","ubiquitous safety"],"keywords_other":["Internet","Risk Management","Personal Protective Equipment","Humans","Occupational Health","Workplace"],"max_cite":7.0,"pub_year":2017.0,"sources":"['scp']","rawkeys":["cyber-physical systems","workplace","smart personal protective equipment","personal protective equipment","risk management","internet of things","humans","ambient intelligence","ubiquitous safety","smart working environment","occupational health","internet","real-time risk assessment","occupational safety and health management"],"tags":["cyber-physical systems","workplace","smart personal protective equipment","personal protective equipment","risk management","humans","ambient intelligence","ubiquitous safety","smart working environment","occupational health","internet","real-time risk assessment","occupational safety and health management","internet of things (iot)"]},{"p_id":46761,"title":"Learning Time-frequency Analysis in Wireless Sensor Networks","abstract":"IEEE Time-frequency analysis is one of essential signal processing tool for wireless sensor signal in internet of things(IoT), but its traditional processing approaches, such as short time-frequency transformation (STFT) and discrete wavelet transformation (DWT), are challenged by the limitation of capability to self-learn from unknown environments and adjust parameters adaptively. To address this problem, we propose to build up the deep learning network to learn time-frequency analysis to instead of traditional STFT and DWT approaches. With using typical neural network layers to remodel STFT and DWT operations, the proposed models consider the efficiency on both training and processing procedures and show their parameter adaptability and capability of deep feature extraction from sensor signals. Moreover, we demonstrate how to integrate learning time-frequency analysis networks into practical IoT applications, signal detection in noisy environment and classifying of various modulated wireless sensor signal, by which their performance are further evaluated in terms of computation complexity and efficiency.","keywords_author":["Convolution","Deep learning","Discrete wavelet transforms","Feature extraction","Signal resolution","Time-frequency analysis","Time-frequency analysis","Wireless sensor network.","Wireless sensor networks"],"keywords_other":["Parameter adaptability","Discrete wavelet transformation","Traditional processing","Time frequency analysis","Time-frequency transformation","Computation complexity","Signal resolution","Internet of Things (IOT)"],"max_cite":0.0,"pub_year":2017.0,"sources":"['scp']","rawkeys":["traditional processing","discrete wavelet transforms","deep learning","convolution","computation complexity","wireless sensor network","time-frequency transformation","time-frequency analysis","feature extraction","time frequency analysis","signal resolution","parameter adaptability","wireless sensor networks","discrete wavelet transformation","internet of things (iot)"],"tags":["traditional processing","computational complexity","convolution","parameters adaptation","machine learning","time-frequency transformation","time-frequency analysis","feature extraction","discrete wavelet transform","signal resolution","wireless sensor networks","internet of things (iot)"]},{"p_id":26283,"title":"An Internet of Things QoE evaluation method based on multiple linear regression analysis","abstract":"\u00a9 2015 IEEE. With the development of the Internet of things(IoT), many various IoT applications continue to emerge. How to evaluate the approbate degree of users on IoT applications is an important problem in IoT research. To address this problem, this paper presents an IoT quality of experience(QoE) evaluation method based on multiple linear regression analysis. Firstly, this paper analyzes the type of IoT applications, determines the quality of service(QoS) parameters of QoE and collects the sample data. Then, it gets the principal components by principal component analysis, associates QoE to the principal components based on multiple regression analysis and builds the regression equation between QoE and principal components. Finally, it gets the functional model between QoE and QoS parameters on the basis of the regression equation. Experimental results show that this evaluation method can reduce the complexity of QoE evaluation and quantify functional relation between QoE and QoS parameters.","keywords_author":["IoT","multiple linear regression analysis","principal component analysis","QoE","qos"],"keywords_other":["Multiple regression analysis","Internet of thing (IOT)","Functional relation","Principal Components","Regression equation","Quality of experience (QoE)","Multiple linear regression analysis","Quality of Service parameters"],"max_cite":7.0,"pub_year":2015.0,"sources":"['scp']","rawkeys":["regression equation","multiple linear regression analysis","principal component analysis","principal components","multiple regression analysis","functional relation","internet of thing (iot)","qos","quality of experience (qoe)","qoe","quality of service parameters","iot"],"tags":["regression equation","multiple linear regression analysis","principal component analysis","principal components","multiple regression analysis","functional relation","quality of experience (qoe)","quality of service","quality of service parameters","internet of things (iot)"]},{"p_id":42672,"title":"Animal monitoring based on IoT technologies","abstract":"\u00a9 2018 IEEE. The placement of grazing animals in vineyards requires additional support to the animal husbandry activities. Such support must include the monitoring and the conditioning of animal's location and behavior, specially their feeding posture. With such a system, it is possible to allow sheep to graze in cultivated areas (e.g. vineyards, orchards) without endangering them. This paper proposes an animal behavior monitoring platform, based on IoT technologies. It includes an IoT local network to gather data from animals and a cloud platform, with processing and storage capabilities, to autonomously shepherd ovine within vineyard areas. The cloud platform also incorporates machine learning features, allowing the extraction of relevant information from the data gathered by the IoT network. Thus, besides the platform description, some results are presented regarding the machine learning platform. Namely, this platform was evaluated for detecting and defining conditions respecting animal's posture, with preliminary promising results. Since several algorithms were tested, this paper includes a comparison of those algorithms.","keywords_author":["Animal Monitoring","Big Data","IoT","Machine Learning","Posture Control"],"keywords_other":["Cloud platforms","Storage capability","Posture control","Grazing animals","Local networks","Animal behavior","IOT networks","Animal husbandry"],"max_cite":0.0,"pub_year":2018.0,"sources":"['scp']","rawkeys":["iot networks","local networks","grazing animals","cloud platforms","big data","storage capability","machine learning","animal behavior","animal husbandry","iot","posture control","animal monitoring"],"tags":["iot networks","grazing animals","cloud platforms","postural control","big data","storage capability","machine learning","animal behavior","animal husbandry","local networks","internet of things (iot)","animal monitoring"]},{"p_id":24244,"title":"Cognitive management framework for Internet of Things:-A prototype implementation","abstract":"In the domain of Internet of Things (IoT), applications are modeled to understand and react based on existing contextual and situational parameters. This work implements a management flow for the abstraction of real world objects and virtual composition of those objects to provide IoT services. We also present a real world knowledge model that aggregates constraints defining a situation, which is then used to detect and anticipate future potential situations. It is implemented based on reasoning and machine learning mechanisms. This work showcases a prototype implementation of the architectural framework in a smart home scenario, targeting two functionalities: actuation and automation based on the imposed constraints and thereby responding to situations and also adapting to the user preferences. It thus provides a productive integration of heterogeneous devices, IoT platforms, and cognitive technologies to improve the services provided to the user. \u00a9 2014 IEEE.","keywords_author":["Cognitive Technologies","Internet of Things","Machine Learning","Smart Home"],"keywords_other":null,"max_cite":9.0,"pub_year":2014.0,"sources":"['scp']","rawkeys":["cognitive technologies","internet of things","smart home","machine learning"],"tags":["cognitive technologies","machine learning","internet of things (iot)","smart homes"]},{"p_id":46786,"title":"Mitigating poisoning atacks on machine learning models: A data provenance based approach","abstract":"\u00a9 2017 Association for Computing Machinery. The use of machine learning models has become ubiquitous. Their predictions are used to make decisions about healthcare, security, investments and many other critical applications. Given this pervasiveness, it is not surprising that adversaries have an incentive to manipulate machine learning models to their advantage. One way of manipulating a model is through a poisoning or causative attack in which the adversary feeds carefully crafted poisonous data points into the training set. Taking advantage of recently developed tamper-free provenance frameworks, we present a methodology that uses contextual information about the origin and transformation of data points in the training set to identify poisonous data, thereby enabling online and regularly re-trained machine learning applications to consume data sources in potentially adversarial environments. To the best of our knowledge, this is the first approach to incorporate provenance information as part of a filtering algorithm to detect causative attacks. We present two variations of the methodology-one tailored to partially trusted data sets and the other to fully untrusted data sets. Finally, we evaluate our methodology against existing methods to detect poison data and show an improvement in the detection rate.","keywords_author":["Adversarial machine learning","Causative attacks","Internet of the Things","IoT","Poisoning attacks","Provenance","Security","Supervised learning"],"keywords_other":["Contextual information","Provenance","Machine learning models","Security","Causative attacks","Poisoning attacks","Adversarial environments","Machine learning applications"],"max_cite":0.0,"pub_year":2017.0,"sources":"['scp']","rawkeys":["provenance","poisoning attacks","supervised learning","adversarial environments","contextual information","machine learning applications","machine learning models","security","adversarial machine learning","internet of the things","causative attacks","iot"],"tags":["provenance","poisoning attacks","supervised learning","adversarial environments","contextual information","machine learning applications","machine learning models","security","adversarial machine learning","internet of the things","internet of things (iot)","causative attack"]},{"p_id":36548,"title":"A Study on Prediction Model of Equipment Failure Through Analysis of Big Data Based on RHadoop","abstract":"\u00a9 2017, Springer Science+Business Media New York. With the development of the internet of things, which is widely applied not only to everyday objects but also to industrial areas, the production of big data is accelerating. To provide intelligent services without human intervention in the internet of things environment, intelligent communication between objects becomes the key, and since the failure of the mechanical equipment attached to the sensor causes malfunction of the object and product failure, big data analysis to predict equipment failure is becoming more important. The purpose of this study is to propose a model for predicting mechanical equipment failure from various sense data collected in the manufacturing process. This study constructed a RHadoop-based big data platform to distribute a large amount of datasets for research, and performed logistic regression modeling to predict the main variables causing the failure from various collected variables. As a result of the study, the main variables in the manufacturing process that cause equipment failure were derived from the collected sense data, and the fitness and performance evaluation for the prediction model were made using the ROC curve. As a result of the performance evaluation of the prediction model, the ROC curve showed a fairly high prediction accuracy with AUC close to 1. The results of this study are expected to be applicable to the prediction of malfunctions, product failure, or abnormal communication between objects due to miscellaneous product faults in our daily lives in the internet of things environment.","keywords_author":["Big data","Internet of things","Machine learning","Prediction model","RHadoop"],"keywords_other":["Intelligent communication","Logistic Regression modeling","Manufacturing process","Intelligent Services","RHadoop","Prediction model","Mechanical equipment","Prediction accuracy"],"max_cite":1.0,"pub_year":2018.0,"sources":"['scp']","rawkeys":["rhadoop","prediction model","mechanical equipment","manufacturing process","intelligent communication","big data","internet of things","machine learning","prediction accuracy","intelligent services","logistic regression modeling"],"tags":["rhadoop","mechanical equipment","manufacturing process","intelligent communication","big data","predictive models","machine learning","internet of things (iot)","prediction accuracy","intelligent services","logistic regression modeling"]},{"p_id":46797,"title":"A secure architecture for IoT with supply chain risk management","abstract":"\u00a9 2017 IEEE. We proposes the development of a cyber-secure, Internet of Things (IoT), supply chain risk management architecture. The proposed architecture is designed to reduce vulnerabilities of malicious supply chain risks by applying machine learning (ML), cryptographic hardware monitoring (CHM), and distributed system coordination (DSC) techniques to mitigate the consequences of unforeseen (including general component failure) threats. In combination, these crosscutting technologies will be integrated into Instrumentation-and-Control\/Operator-in-the-Loop (ICOL) architecture to learn normal and abnormal system behaviors. The detection of absolute or perceived abnormal system-component behaviors will trigger an ICOL alert that will require an operator's manual verification-response action (i.e., that the detected alert is, or is not a viable control system threat). The operator's verification-response will be fed back into the ML systems to recalibrate the perceived normal or abnormal state of the system's behavior.","keywords_author":["IoT","Machine learning","Network attacks","Normal and abnormal behavior","Supply chain risk management","Throttled network"],"keywords_other":["Instrumentation and control","Supply chain risk management","Cryptographic hardware","Network attack","Cross-cutting technology","Internet of Things (IOT)","Proposed architectures","Abnormal behavior"],"max_cite":0.0,"pub_year":2017.0,"sources":"['scp']","rawkeys":["cryptographic hardware","instrumentation and control","proposed architectures","supply chain risk management","machine learning","normal and abnormal behavior","network attack","abnormal behavior","network attacks","iot","throttled network","cross-cutting technology","internet of things (iot)"],"tags":["cryptographic hardware","instrumentation and control","proposed architectures","supply chain risk management","machine learning","normal and abnormal behavior","network attack","abnormal behavior","throttled network","cross-cutting technology","internet of things (iot)"]},{"p_id":44750,"title":"Incomplete dot products for dynamic computation scaling in neural network inference","abstract":"\u00a9 2017 IEEE.We propose the use of incomplete dot products (IDP) to dynamically adjust the number of input channels used in each layer of a convolutional neural network during feedforward inference. IDP adds monotonically non-increasing coefficients, referred to as a 'profile', to the channels during training. The profile orders the contribution of each channel in non-increasing order. At inference time, the number of channels used can be dynamically adjusted to trade off accuracy for lowered power consumption and reduced latency by selecting only a beginning subset of channels. This approach allows for a single network to dynamically scale over a computation range, as opposed to training and deploying multiple networks to support different levels of computation scaling. Additionally, we extend the notion to multiple profiles, each optimized for some specific range of computation scaling. We present experiments on the computation and accuracy trade-offs of IDP for popular image classification models and datasets. We demonstrate that, for MNIST and CIFAR-10, IDP reduces computation significantly, e.g., by 75%, without significantly compromising accuracy. We argue that IDP provides a convenient and effective means for devices to lower computation costs dynamically to reflect the current computation budget of the system. For example, VGG-16 with 50% IDP (using only the first 50% of channels) achieves 70% in accuracy on the CIFAR-10 dataset compared to the standard network which achieves only 35% accuracy when using the reduced channel set.","keywords_author":["Approximate Computing","Convolutional Neural Network","Deep Learning","IoT","Machine Learning"],"keywords_other":["Multiple networks","Approximate computing","Classification models","Increasing coefficients","Computation costs","Convolutional neural network","Dynamic computations","Network inference"],"max_cite":0.0,"pub_year":2018.0,"sources":"['scp']","rawkeys":["dynamic computations","deep learning","increasing coefficients","machine learning","network inference","computation costs","convolutional neural network","approximate computing","iot","multiple networks","classification models"],"tags":["dynamic computations","increasing coefficients","machine learning","network inference","computational costs","convolutional neural network","approximate computing","multiple networks","classification models","internet of things (iot)"]},{"p_id":22224,"title":"IoT Data Compression: Sensor-Agnostic Approach","abstract":"\u00a9 2015 IEEE. Management of bulk sensor data is one of the challenging problems in the development of Internet of Things (IoT) applications. High volume of sensor data induces for optimal implementation of appropriate sensor data compression technique to deal with the problem of energy-efficient transmission, storage space optimization for tiny sensor devices, and cost-effective sensor analytics. The compression performance to realize significant gain in processing high volume sensor data cannot be attained by conventional lossy compression methods, which are less likely to exploit the intrinsic unique contextual characteristics of sensor data. In this paper, we propose SensCompr, a dynamic lossy compression method specific for sensor datasets and it is easily realizable with standard compression methods. Senscompr leverages robust statistical and information theoretic techniques and does not require specific physical modeling. It is an information-centric approach that exhaustively analyzes the inherent properties of sensor data for extracting the embedded useful information content and accordingly adapts the parameters of compression scheme to maximize compression gain while optimizing information loss. Senscompr is successfully applied to compress large sets of heterogeneous real sensor datasets like ECG, EEG, smart meter, accelerometer. To the best of our knowledge, for the first time 'sensor information content'-centric dynamic compression technique is proposed and implemented particularly for IoT-applications and this method is independent to sensor data types.","keywords_author":["compression","information","IoT","loss","outlier","sensor"],"keywords_other":["IoT","Energy efficient transmission","outlier","Lossy compression methods","information","Compression performance","Information theoretic techniques","Internet of Things (IOT)"],"max_cite":13.0,"pub_year":2015.0,"sources":"['scp']","rawkeys":["energy efficient transmission","compression","compression performance","loss","outlier","sensor","information","iot","lossy compression methods","information theoretic techniques","internet of things (iot)"],"tags":["energy efficient transmission","compression performance","sensors","codes","information","losses","outliers","lossy compression methods","information theoretic techniques","internet of things (iot)"]},{"p_id":11991,"title":"Network Traffic Classifier with Convolutional and Recurrent Neural Networks for Internet of Things","abstract":"A network traffic classifier (NTC) is an important part of current network monitoring systems, being its task to infer the network service that is currently used by a communication flow (e.g., HTTP and SIP). The detection is based on a number of features associated with the communication flow, for example, source and destination ports and bytes transmitted per packet. NTC is important, because much information about a current network flow can be learned and anticipated just by knowing its network service (required latency, traffic volume, and possible duration). This is of particular interest for the management and monitoring of Internet of Things (IoT) networks, where NTC will help to segregate traffic and behavior of heterogeneous devices and services. In this paper, we present a new technique for NTC based on a combination of deep learning models that can be used for IoT traffic. We show that a recurrent neural network (RNN) combined with a convolutional neural network (CNN) provides best detection results. The natural domain for a CNN, which is image processing, has been extended to NTC in an easy and natural way. We show that the proposed method provides better detection results than alternative algorithms without requiring any feature engineering, which is usual when applying other models. A complete study is presented on several architectures that integrate a CNN and an RNN, including the impact of the features chosen and the length of the network flows used for training.","keywords_author":["Convolutional neural network","deep learning","network traffic classification","recurrent neural network","Convolutional neural network","deep learning","network traffic classification","recurrent neural network"],"keywords_other":["Network traffic classification","Heterogeneous devices","Recurrent neural network (RNN)","Feature engineerings","Convolutional neural network","Alternative algorithms","Network monitoring systems","Internet of Things (IOT)"],"max_cite":3.0,"pub_year":2017.0,"sources":"['ieee', 'scp', 'wos']","rawkeys":["heterogeneous devices","deep learning","recurrent neural network","feature engineerings","network traffic classification","convolutional neural network","recurrent neural network (rnn)","alternative algorithms","internet of things (iot)","network monitoring systems"],"tags":["heterogeneous devices","neural networks","machine learning","feature engineerings","network traffic classification","convolutional neural network","alternative algorithms","internet of things (iot)","network monitoring systems"]},{"p_id":73442,"title":"Smart Cities: A Survey on Data Management, Security, and Enabling Technologies","abstract":"Integrating the various embedded devices and systems in our environment enables an Internet of Things (IoT) for a smart city. The IoT will generate tremendous amount of data that can be leveraged for safety, efficiency, and infotainment applications and services for city residents. The management of this voluminous data through its lifecycle is fundamental to the realization of smart cities. Therefore, in contrast to existing surveys on smart cities we provide a data-centric perspective, describing the fundamental data management techniques employed to ensure consistency, interoperability, granularity, and reusability of the data generated by the underlying IoT for smart cities. Essentially, the data lifecycle in a smart city is dependent on tightly coupled data management with cross-cutting layers of data security and privacy, and supporting infrastructure. Therefore, we further identify techniques employed for data security and privacy, and discuss the networking and computing technologies that enable smart cities. We highlight the achievements in realizing various aspects of smart cities, present the lessons learned, and identify limitations and research challenges.","keywords_author":["Smart cities","Internet of Things (IoT)","data management","data security","network functions virtualization (NFV)","software-defined networking (SDN)","cloud computing"],"keywords_other":["JAMMING ATTACKS","SOFTWARE-DEFINED NETWORKING","WIRELESS SENSOR NETWORKS","DATA DISSEMINATION","INTRUSION DETECTION","LEARNING ALGORITHMS","COMPREHENSIVE SURVEY","HEALTH-CARE","CITY APPLICATIONS","AD-HOC NETWORKS"],"max_cite":5.0,"pub_year":2017.0,"sources":"['ieee', 'wos']","rawkeys":["software-defined networking (sdn)","cloud computing","ad-hoc networks","network functions virtualization (nfv)","intrusion detection","software-defined networking","health-care","jamming attacks","learning algorithms","data dissemination","data security","wireless sensor networks","comprehensive survey","data management","city applications","internet of things (iot)","smart cities"],"tags":["cloud computing","health care","jamming attack","network function virtualization","software-defined networking","intrusion detection systems","data dissemination","data security","ad hoc networks","comprehensive survey","wireless sensor networks","data management","city applications","learning algorithm","internet of things (iot)","smart cities"]},{"p_id":73445,"title":"Fast Adaptation of Activity Sensing Policies in Mobile Devices","abstract":"With the proliferation of sensors, such as accelerometers, in mobile devices, activity and motion tracking has become a viable technology to understand and create an engaging user experience. This paper proposes a fast adaptation and learning scheme of activity tracking policies when user statistics are unknown a priori, varying with time, and inconsistent for different users. In our stochastic optimization, user activities are required to be synchronized with a backend under a cellular data limit to avoid overcharges from cellular operators. The mobile device is charged intermittently using wireless or wired charging for receiving the required energy for transmission and sensing operations. First, we propose an activity tracking policy by formulating a stochastic optimization as a constrained Markov decision process (CMDP). Second, we prove that the optimal policy of the CMDP has a threshold structure using a Lagrangian relaxation approach and the submodularity concept. We accordingly present a fast Q-learning algorithm by considering the policy structure to improve the convergence speed over that of conventional Q-learning. Finally, simulation examples are presented to support the theoretical findings of this paper.","keywords_author":["Activity tracking","fast adaptation","Internet of things","Markov decision processes","wireless charging"],"keywords_other":["FADING CHANNELS","WEARABLE SENSORS","MIMO TRANSMISSION CONTROL"],"max_cite":0.0,"pub_year":2017.0,"sources":"['ieee', 'wos']","rawkeys":["wireless charging","fading channels","wearable sensors","activity tracking","mimo transmission control","internet of things","markov decision processes","fast adaptation"],"tags":["wireless charging","fading channels","wearable sensors","activity tracking","mimo transmission control","markov decision processes","fast adaptation","internet of things (iot)"]},{"p_id":73446,"title":"Mobile Edge Computing: A Survey","abstract":"Mobile edge computing (MEC) is an emergent architecture where cloud computing services are extended to the edge of networks leveraging mobile base stations. As a promising edge technology, it can be applied to mobile, wireless, and wire-line scenarios, using software and hardware platforms, located at the network edge in the vicinity of end-users. MEC provides seamless integration of multiple application service providers and vendors toward mobile subscribers, enterprises, and other vertical segments. It is an important component in the 5G architecture which supports variety of innovative applications and services where ultralow latency is required. This paper is aimed to present a comprehensive survey of relevant research and technological developments in the area of MEC. It provides the definition of MEC, its advantages, architectures, and application areas; where we in particular highlight related research and future directions. Finally, security and privacy issues and related existing solutions are also discussed.","keywords_author":["Fog computing","Internet of Things (IoT)","mobile cloud computing (MCC)","mobile edge computing (MEC)"],"keywords_other":["NETWORKS","OPEN CHALLENGES","SAVE ENERGY","TAXONOMY","SCENARIOS","PERFORMANCE","MOTIVATION","OPTIMIZATION","ISSUES"],"max_cite":5.0,"pub_year":2018.0,"sources":"['ieee', 'wos']","rawkeys":["scenarios","taxonomy","performance","fog computing","mobile cloud computing (mcc)","save energy","open challenges","networks","mobile edge computing (mec)","issues","motivation","optimization","internet of things (iot)"],"tags":["scenarios","taxonomy","performance","fog computing","save energy","open challenges","networks","motivation","mobile cloud computing","issues","optimization","mobile edge computing","internet of things (iot)"]},{"p_id":36583,"title":"IoT-based personalized NIE content recommendation system","abstract":"\u00a9 2018 Springer Science+Business Media, LLC, part of Springer Nature Recently, the Internet of Things (IoT) has become a popular topic and a dominant trend in various fields, such as healthcare, agriculture, manufacturing, and transportation. In particular, in the field of education, it has become a popular tool to improve learners\u2019 interests and achievements by making them interact with various devices in and out of the classroom. Lessons in newspaper in education (NIE), which uses newspapers as an educational resource, have started to utilize it. For instance, by analyzing the data generated from a learner\u2019s device, such as Raspberry Pi, appropriate news and related multimedia data can be provided to the learners as learning materials to support the lesson. However, as news and multimedia data are scattered in a wide variety of forms, it is very difficult to select appropriate ones for the learner. In this paper, we propose a news and related multimedia recommendation scheme based on IoT for supporting NIE lessons. Specifically, news and related multimedia data are collected from the Web, and they are integrated and stored into the server. After that, the learner can easily browse such contents using a mobile device through personalized visualization, which increase the efficiency of NIE lessons. To show the effectiveness of our scheme, we implemented a prototype system and performed various experiments. We present some of the results.","keywords_author":["Data integration","Deep learning","Internet of things","Multimedia in education","News in education","Semantic web"],"keywords_other":["Newspaper in educations","Internet of thing (IOT)","Multimedia data","Prototype system","Learning materials","Educational resource","Content recommendations","Multimedia in education"],"max_cite":1.0,"pub_year":2018.0,"sources":"['scp']","rawkeys":["multimedia in education","multimedia data","prototype system","deep learning","internet of thing (iot)","internet of things","educational resource","newspaper in educations","news in education","learning materials","data integration","semantic web","content recommendations"],"tags":["multimedia in education","content recommendations","multimedia data","prototype system","machine learning","educational resource","newspaper in educations","news in education","learning materials","data integration","semantic web","internet of things (iot)"]},{"p_id":91883,"title":"Exogenous Coordination for Building Fog-Based Cyber Physical Social Computing and Networking Systems","abstract":"With the proliferation of smart embedded devices, cyber physical social computing and networking systems (CPSCN) are emerging as a next generation of social networks. Unlike traditional social networks that run on cloud-based infrastructure, CPSCN systems usually depend on a large number of distributed, heterogeneous devices, such as mobile phones, smart vehicles, or network access points. These computing resources, which are often referred to as fog computing systems, provide a gateway to the physical world, and thus offer new possibilities for social applications. Unfortunately, building CPSCN systems that leverage fog computing infrastructure is not straightforward. Significant challenges arise from the large scale distribution of computing resources over a wide area, and the dynamic nature of multiple, possibly mobile, hosts. In this paper, we extend our previous work on a distributed dataflow programming model and propose an application platform for realizing CPSCN systems. A key aspect of our work is the development of an exogenous coordination model, which exhibits a separation of concern between computation and communication activities, and helps resolve some of the challenges brought about by the dynamic and large scale nature of CPSCN systems.","keywords_author":["Cyber physical social computing and networking","fog computing","Internet of Things","exogenous coordination","data-flow"],"keywords_other":null,"max_cite":0.0,"pub_year":2018.0,"sources":"['wos']","rawkeys":["data-flow","cyber physical social computing and networking","fog computing","internet of things","exogenous coordination"],"tags":["cyber physical social computing and networking","fog computing","exogenous coordination","data flow","internet of things (iot)"]},{"p_id":46828,"title":"Large scale sensor data processing based on deep stacked autoencoder network","abstract":"\u00a9 2005 \u2013 ongoing JATIT & LLS. Recently, Internet of Things (IoT) extremely populated by massive amounts of connected embedded devices, which are gathering large volumes of real-time heterogeneous data. Hence, IoT becomes an archetypal instance of Big Data. The collected IoT Big Data may not be profitable unless we evaluate and accurately exploit them. Providing mining for large scales of raw sensor data is an open challenge. To cope with this challenge, we proposed a system that operates in two modes, which are preparation and processing. The preparation mode converges on reducing the factors that hinder making efficient processing by focusing on three stages. First, handling missing data by applying interval-valued fuzzy-rough feature selection methodology. It highlights the most important features that contain missing data and gets rid of the others. Then, Maximum Likelihood (ML) approach is used for estimating the missing values. Second, anomalies are detected by initially utilizing K-nearest neighbors (KNN) algorithm then removing the detected ones from the data. Third, the dimensionality of nonlinearly separable data is reduced by exploiting Self-Organizing Map (SOM) network. In the processing mode, we passed the prepared data to a straightforward classifier based on a Deep Learning (DL) approach. We used autoencoder networks in constructing a deep network, which is the Deep Stacked Autoencoder (DSAE). The extracted features by the DSAE are non-handcrafted and task dependent, which gives it the most discriminative power to work as an efficient classifier. We apply the proposed model to PAMAP2 Physical Activity Monitoring data set. The results show that DSAE achieves high accuracy (99.8%) compared to the state-of-the-art classifiers.","keywords_author":["Big data","Deep learning (DL)","Deep stacked autoencoder (DSAE)","Internet of things (IoT)"],"keywords_other":null,"max_cite":0.0,"pub_year":2017.0,"sources":"['scp']","rawkeys":["deep learning (dl)","internet of things (iot)","deep stacked autoencoder (dsae)","big data"],"tags":["deep stacked autoencoder (dsae)","machine learning","internet of things (iot)","big data"]},{"p_id":42747,"title":"System for recognizing lecture quality based on analysis of physical parameters","abstract":"\u00a9 2017 Elsevier Ltd In this paper we have presented a smart classroom system that is able to classify students\u2019 satisfaction with the lecture quality by examining parameters of the physical environment obtained using different smart devices. The system is based on the Random forest classifier, which showed the best accuracy among all machine learning algorithms available in Weka tool, with dataset collected during 28 lectures and evaluated using 10-fold cross validation. The system is implemented using different set of tools (such as Matlab and Weka) and can extract features from the ambient sound and analyze values obtained from different smart devices deployed in the classroom. Based on the extracted and captured data the system provides in real time information about the students\u2019 satisfaction with the lecture quality. For the validation purposes, we recorded 13 more lectures attended by four different student groups where the number of students varied from 5 to 18. The system accuracy was evaluated by comparing system outputs with the students\u2019 feedback and ranged from 70.7% to 83.9%.","keywords_author":["Ambient intelligence","Data mining","IoT","Lecture quality","Machine learning","Smart classroom"],"keywords_other":["Smart classroom","System accuracy","10-fold cross-validation","Physical parameters","Physical environments","Students' satisfaction","Real-time information","Random forest classifier"],"max_cite":0.0,"pub_year":2018.0,"sources":"['scp']","rawkeys":["physical environments","real-time information","data mining","system accuracy","10-fold cross-validation","smart classroom","machine learning","random forest classifier","students' satisfaction","physical parameters","ambient intelligence","iot","lecture quality"],"tags":["physical environments","real-time information","data mining","system accuracy","10-fold cross-validation","smart classroom","machine learning","random forest classifier","students' satisfaction","physical parameters","ambient intelligence","lecture quality","internet of things (iot)"]},{"p_id":42749,"title":"OpCloudSec: Open cloud software defined wireless network security for the Internet of Things","abstract":"\u00a9 2018 Elsevier B.V. Cutting-edge cloud frameworks will require a paradigm shift in regards to how they are built and managed. Traditional management and control platforms face significant challenges in terms of security, reliability, and flexibility that these cutting-edge frameworks must deal with. On the other hand, Distributed Denial of Service (DDoS) attacks have become a weapon of choice for cyber-terrorists, cyber-extortionists, and hackers. Recently, the simplicity of programmability in Software-Defined Networking (SDN) makes it a good platform for the implementation of various initiatives that includes decentralized network management, dynamic topology changes, and application deployment in a multi-tenant data center environment. Motivated by the capabilities of SDN, we are proposing a mitigation architecture for security attacks that incorporates a highly programmable monitoring network so as to make it possible to identify attacks. It has a flexible control structure to quickly define the reaction of attacks and particular side, and we show how SDN can be used as a key application in the cloud IoT. We evaluated the performance of our proposed architecture and compared it with the existing models to obtain various performance measures. The results of our evaluation show that our OpCloudSec architecture model can efficiently and effectively meet the security challenges created by the new network paradigm.","keywords_author":["Deep learning","Internet-of-Things","Security","Software Defined Networking"],"keywords_other":["Decentralized network management","Application deployment","Software defined networking (SDN)","Software-defined wireless networks","Security","Distributed denial of service attack","Traditional management","Proposed architectures"],"max_cite":0.0,"pub_year":2018.0,"sources":"['scp']","rawkeys":["deep learning","proposed architectures","distributed denial of service attack","software-defined wireless networks","security","application deployment","software defined networking","decentralized network management","traditional management","software defined networking (sdn)","internet-of-things"],"tags":["proposed architectures","software-defined networking","distributed denial of service attack","software-defined wireless networks","machine learning","security","application deployment","decentralized network management","traditional management","internet of things (iot)"]},{"p_id":9988,"title":"Deep learning control for complex and large scale cloud systems","abstract":"Deep learning attempts to model high level perceptions in data using deep graph representations and creating models to learn these representations from large-scale unlabeled signals. Efficient unsupervised feature learning is extracted by deep learning algorithms and with multiple processing layers, composed of multiple linear and non-linear transformations. Actual systems become more and more complex with huge numbers of state variables and control of such large and complex systems with chaotic behavior, which needs more information about systems. Deep learning control by discovering continoiusly almost all possible information seems to be a reasonable approach to model and control largescale and complex systems. Recent advancements in machine learning algorithms and platforms are leading to deep learning controllers in real-time applications. The goal of this paper is to describe the concept of deep learning control and explain how cloud fog computing and edge analytics could handle massive amount of real time data streams from Cyber Physical Systems (CPS).","keywords_author":["Cloud Computing","Cyber Physical Systems","Deep Learning Control","Dynamic Systems and Control","Internet of Everything (IOT)","Large Scale Systems","Deep Learning Control","Internet of Everything (IOT)","Dynamic Systems and Control","Large Scale Systems","Cloud Computing","Cyber Physical Systems"],"keywords_other":null,"max_cite":3.0,"pub_year":2017.0,"sources":"['scp', 'wos']","rawkeys":["cloud computing","cyber physical systems","large scale systems","deep learning control","dynamic systems and control","internet of everything (iot)"],"tags":["cyber-physical systems","cloud computing","large-scale systems","deep learning control","dynamic systems and control","internet of things (iot)"]},{"p_id":112397,"title":"Agent-Based Simulation of Smart Beds With Internet-of-Things for Exploring Big Data Analytics","abstract":"Internet-of-Things (IoT) can allow healthcare professionals to remotely monitor patients by analyzing the sensors outputs with big data analytics. Sleeping conditions are one of the most influential factors on health. However, the literature lacks of the appropriate simulation tools to widely support the research on the recognition of sleeping postures. This paper proposes an agent-based simulation framework to simulate sleeper movements on a simulated smart bed with load sensors. This framework allows one to define sleeping posture recognition algorithms and compare their outcomes with the poses adopted by the sleeper. This novel presented ABS-BedIoT simulator allows users to graphically explore the results with starplots, evolution charts, and final visual representations of the states of the bed sensors. This simulator can also generate logs text files with big data for applying offline big data techniques on them. The source code of ABS-BedIoT and some examples of logs are freely available from a public research repository. The current approach is illustrated with an algorithm that properly recognized the simulated sleeping postures with an average accuracy of 98%. This accuracy is higher than the one reported by an existing alternative work in this area.","keywords_author":["Agent-based-simulation","big data","Internet-of-Things","multi-agent systems","smart bed"],"keywords_other":["HOME","SLEEP","ALGORITHM","POSTURE","WIRELESS SENSOR NETWORKS","PROGRAMS","MODEL","EXPERIENCE"],"max_cite":1.0,"pub_year":2018.0,"sources":"['wos']","rawkeys":["algorithm","agent-based-simulation","model","home","big data","posture","programs","sleep","smart bed","multi-agent systems","wireless sensor networks","internet-of-things","experience"],"tags":["program","model","home","big data","posture","sleep","smart bed","wireless sensor networks","multi-agent systems","algorithms","agent based simulation","internet of things (iot)","experience"]},{"p_id":50959,"title":"Towards a trust model for social networks of wireless smart objects work-in-progress","abstract":"\u00a9 Springer International Publishing Switzerland 2015.Smart wireless objects are now pervasive in our lives. As these devices are increasingly used in chain to deliver rich services to users, the next trend will make them evolve in their own social network with the different challenges that it entails. In this paper, we focus on how trust can be modeled and managed in such a network to preserve user privacy and ensure the security of peer-to-peer interactions. We propose to rely on light-weight machine-learning mechanisms to allow these devices, which can be perceived as people\u2019s extensions, to mimic the human behavior of their owners regarding trust.","keywords_author":["IoT","Machine learning","Social network","Trust management","Wireless smart objects"],"keywords_other":["Machine learning mechanism","Trust management","Trust modeling","IoT","Work in progress","Smart objects","Human behaviors","Peer-to-peer interaction"],"max_cite":0.0,"pub_year":2015.0,"sources":"['scp']","rawkeys":["human behaviors","peer-to-peer interaction","trust management","trust modeling","machine learning mechanism","machine learning","smart objects","social network","work in progress","iot","wireless smart objects"],"tags":["human behaviors","peer-to-peer interaction","trust management","trust modeling","machine learning mechanism","machine learning","social networks","smart objects","work in progress","wireless smart objects","internet of things (iot)"]},{"p_id":20240,"title":"Designing a smart multisensor framework based on beaglebone black board","abstract":"\u00a9 Springer-Verlag Berlin Heidelberg 2015. This paper presents an intelligent multisensor framework based on the BeableBone Black platform, a complete open hardware and software embedded computer; the challenge is to create a multi-purpose hierarchical network composed by smart nodes able to gain and manage heterogeneous data and spaces according to the Internet of Things paradigm. Thanks to several expansion modules, the designed nodes can become an instrument for monitoring, preservation and protection of several environments. As a proof of the proposed framework we conducted a first prototypal experimentation within the Cultural Heritage domain; in detail we deployed the system in an art exhibition within the Maschio Angioino Castle, Naples, Italy.","keywords_author":["Internet of Things","Pervasive Computing","Smart Environment"],"keywords_other":["Open hardware","Embedded computers","Multi sensor","Hierarchical network","Smart environment","Heterogeneous data","Multi-purpose","Cultural heritages"],"max_cite":18.0,"pub_year":2015.0,"sources":"['scp']","rawkeys":["embedded computers","open hardware","internet of things","multi sensor","multi-purpose","heterogeneous data","cultural heritages","pervasive computing","smart environment","hierarchical network"],"tags":["embedded computing","open hardware","multi sensor","hierarchical network","multi-purpose","heterogeneous data","cultural heritages","pervasive computing","smart environment","internet of things (iot)"]},{"p_id":55062,"title":"Remote dynamic partial reconfiguration: A threat to Internet-of-Things and embedded security applications","abstract":"The advent of the Internet of Things has motivated the use of Field Programmable Gate Array (FPGA) devices with Dynamic Partial Reconfiguration (DPR) capabilities for dynamic non-invasive modifications to circuits implemented on the FPGA. In particular, the ability to perform DPR over the network is essential in the context of a growing number of Internet of Things (IoT)-based and embedded security applications. However, the use of remote DPR brings with it a number of security threats that could lead to potentially catastrophic consequences in practical scenarios. In this paper, we demonstrate four examples where the remote DPR capability of the FPGA may be exploited by an adversary to launch Hardware Trojan Horse (HTH) attacks on commonly used security applications. We substantiate the threat by demonstrating remotely-launched attacks on Xilinx FPGA-based hardware implementations of a cryptographic algorithm, a true random number generator, and two processor based security applications - namely, a software implementation of a cryptographic algorithm and a cash dispensing scheme. The attacks are launched by on-the-fly transfer of malicious FPGA configuration bitstreams over an Ethernet connection to perform DPR and leak sensitive information. Finally, we comment on plausible countermeasures to prevent such attacks. (C) 2017 Elsevier B.V. All rights reserved.","keywords_author":["Internet of things","Dynamic Partial Reconfiguration","Field Programmable Gate Array","Hardware Trojan Horse","Hardware security"],"keywords_other":["FAULT","ADVANCED ENCRYPTION STANDARD","ATTACKS"],"max_cite":0.0,"pub_year":2017.0,"sources":"['wos']","rawkeys":["fault","field programmable gate array","internet of things","advanced encryption standard","hardware security","attacks","hardware trojan horse","dynamic partial reconfiguration"],"tags":["diagnosis","fpga","advanced encryption standard","hardware security","attacks","hardware trojan horse","dynamic partial reconfiguration","internet of things (iot)"]},{"p_id":34589,"title":"Experience-oriented enhancement of smartness for internet of things","abstract":"\u00a9 Springer International Publishing Switzerland 2015. In this paper, we propose a novel approach, the Experience-Oriented Smart Things that allows experiential knowledge discovery, storage, involving, and sharing for Internet of Things. The main features, architecture, and initial experiments of this approach are introduced. Rather than take all the data produced by Internet of Things, this approach focuses on acquiring only interesting data for its knowledge discovery process. By catching decision events, this approach gathers its own daily operation experience, which is the interesting data, and uses such experience for knowledge discovery. An initial experiment was made at the end of this paper, by applying this approach to a sensors-equipped bicycle, the bicycle is able to learn user\u2019s physical features and recognize its user out of other riders. Customized version of Decisional DNA is used in this approach as the knowledge representation technique. Decisional DNA is a domain-independent, and flexible, and standard experiential knowledge repository solution that allows knowledge to be acquired, reused, evolved and shared easily. The presented conceptual approach demonstrates how knowledge can be discovered through its domain\u2019s experiences and stored as Decisional DNA.","keywords_author":["Decisional DNA","Intelligent systems","Internet of things","Knowledge representation","Machine learning"],"keywords_other":["Decisional DNA","Knowledge discovery process","Physical features","Take-all","Experiential knowledge","Operation experiences","Domain independents","Conceptual approaches"],"max_cite":2.0,"pub_year":2015.0,"sources":"['scp']","rawkeys":["take-all","physical features","experiential knowledge","conceptual approaches","decisional dna","internet of things","domain independents","intelligent systems","machine learning","operation experiences","knowledge representation","knowledge discovery process"],"tags":["take-all","physical features","experiential knowledge","conceptual approaches","decisional dna","machine learning","domain independents","intelligent systems","operational experience","knowledge representation","internet of things (iot)","knowledge discovery process"]},{"p_id":44831,"title":"A deep learning model for air quality prediction in smart cities","abstract":"\u00a9 2017 IEEE. In recent years, Internet of Things (IoT) concept has become a promising research topic in many areas including industry, commerce and education. Smart cities employ IoT based services and applications to create a sustainable urban life. By using information and communication technologies, IoT enables smart cities to make city stakeholders more aware, interactive and efficient. With the increase in number of IoT based smart city applications, the amount of data produced by these applications is increased tremendously. Governments and city stakeholders take early precautions to process these data and predict future effects to ensure sustainable development. In prediction context, deep learning techniques have been used for several forecasting problems in big data. This inspires us to use deep learning methods for prediction of IoT data. Hence, in this paper, a novel deep learning model is proposed for analyzing IoT smart city data. We propose a novel model based on Long Short Term Memory (LSTM) networks to predict future values of air quality in a smart city. The evaluation results of the proposed model are found to be promising and they show that the model can be used in other smart city prediction problems as well.","keywords_author":["Air quality prediction","Deep learning","Internet of things(IoT)","IoT data analytics","Long short term memory(LSTM)","Smart city"],"keywords_other":["Services and applications","Air quality prediction","Information and Communication Technologies","Internet of Things (IOT)","Learning techniques","Smart city applications","Forecasting problems","Data analytics"],"max_cite":0.0,"pub_year":2018.0,"sources":"['scp']","rawkeys":["long short term memory(lstm)","iot data analytics","services and applications","deep learning","data analytics","internet of things(iot)","learning techniques","smart city applications","forecasting problems","information and communication technologies","air quality prediction","internet of things (iot)","smart city"],"tags":["iot data analytics","services and applications","data analytics","long short-term memory","machine learning","learning techniques","smart city applications","forecasting problems","information and communication technologies","air quality prediction","internet of things (iot)","smart cities"]},{"p_id":12068,"title":"A Reconfigurable Streaming Deep Convolutional Neural Network Accelerator for Internet of Things","abstract":"Convolutional neural network (CNN) offers significant accuracy in image detection. To implement image detection using CNN in the Internet of Things (IoT) devices, a streaming hardware accelerator is proposed. The proposed accelerator optimizes the energy efficiency by avoiding unnecessary data movement. With unique filter decomposition technique, the accelerator can support arbitrary convolution window size. In addition, max-pooling function can be computed in parallel with convolution by using separate pooling unit, thus achieving throughput improvement. A prototype accelerator was implemented in TSMC 65-nm technology with a core size of 5 mm(2). The accelerator can support major CNNs and achieve 152GOPS peak throughput and 434GOPS\/W energy efficiency at 350 mW, making it a promising hardware accelerator for intelligent IoT devices.","keywords_author":["Convolution neural network","deep learning","hardware accelerator","IoT","Convolution neural network","deep learning","hardware accelerator","IoT"],"keywords_other":["Streaming hardwares","Internet of thing (IOT)","Convolution windows","Throughput improvement","Convolutional neural network","Convolution neural network","Filter decomposition","Hardware accelerators"],"max_cite":3.0,"pub_year":2018.0,"sources":"['ieee', 'scp', 'wos']","rawkeys":["hardware accelerators","filter decomposition","convolution neural network","deep learning","throughput improvement","convolution windows","internet of thing (iot)","streaming hardwares","convolutional neural network","iot","hardware accelerator"],"tags":["hardware accelerators","filter decomposition","throughput improvement","convolution windows","machine learning","streaming hardwares","convolutional neural network","internet of things (iot)"]},{"p_id":20263,"title":"Recent trends in the Internet of Things","abstract":"\u00a9 2017 IEEE. The Internet of Things (IoT) has become a hot topic in the present tech-driven world. A strong framework of cloud computing, backed up by a seamless blending of sensors and actuators with the environment around us, is making this 'network of networks of autonomous objects' a reality. From smart wearables to smart cities, from domestic life to industries, the IoT is expanding itself to different areas. According to Gartner Inc., the IoT will include 26 billion units installed by 2020. Smart security solutions, smart home automation, smart health care, smart wearables etc. are in-trend applications of IoT, and by the near future we expect to see its application to a city's transportation system or smart power grids. This paper presents a brief overview on different trends of the IoT and also discusses about the effects of the IoT on our day-to-day life. It also discusses the importance of cloud computing, autonomous control, artificial intelligence in the context of the IoT. Lastly, it's concluded with the need of synchronization of the Internet, wireless sensors and actuators and distributed computing for successfully enabling technologies for the IoT.","keywords_author":["Automation","Cloud Computing","Embedded Systems","Machine Learning","Smart devices","The Internet of Things","Wireless Networks"],"keywords_other":["Network of networks","Security solutions","Internet of thing (IOT)","Enabling technologies","Autonomous control","Smart devices","Transportation system","Sensors and actuators"],"max_cite":17.0,"pub_year":2017.0,"sources":"['scp']","rawkeys":["cloud computing","network of networks","embedded systems","internet of thing (iot)","machine learning","security solutions","automation","smart devices","the internet of things","transportation system","wireless networks","sensors and actuators","autonomous control","enabling technologies"],"tags":["cloud computing","network of networks","automated","embedded systems","machine learning","security solutions","smart devices","the internet of things","transportation system","wireless networks","sensors and actuators","autonomous control","internet of things (iot)","enabling technologies"]},{"p_id":38695,"title":"Platform as a service gateway for the Fog of Things","abstract":"\u00a9 2016 Elsevier Ltd Internet of Things (IoT), one of the key research topics in recent years, together with concepts from Fog Computing, brings rapid advancements in Smart City, Monitoring Systems, industrial control, transportation and other fields. These applications require a reconfigurable sensor architecture that can span multiple scenarios, devices and use cases that allow storage, networking and computational resources to be efficiently used on the edge of the network. There are a number of platforms and gateway architectures that have been proposed to manage these components and enable application deployment. These approaches lack horizontal integration between multiple providers as well as higher order functionalities like load balancing and clustering. This is partly due to the strongly coupled nature of the deployed applications, a lack of abstraction of device communication layers as well as a lock-in for communication protocols. This limitation is a major obstacle for the development of a protocol agnostic application environment that allows for single application to be migrated and to work with multiple peripheral devices with varying protocols from different local gateways. This research looks at existing platforms and their shortcomings as well as proposes a messaging based modular gateway platform that enables clustering of gateways and the abstraction of peripheral communication protocol details. These novelties allow applications to send and receive messages regardless of their deployment location and destination device protocol, creating a more uniform development environment. Furthermore, it results in a more streamlined application development and testing while providing more efficient use of the gateway's resources. Our evaluation of a prototype for the system shows the need for the migration of resources and the QoS advantages of such a system. The examined use case scenarios show that clustering proves to be an advantage in certain use cases as well as presenting the deployment of a larger testing and control environment through the platform.","keywords_author":["Cloud manufacturing","Fog Computing","Gateway","Horizontal integration","Internet of Things","MQTT","OSGI"],"keywords_other":["Application development","Horizontal integrations","Development environment","OSGI","Internet of Things (IOT)","MQTT","Cloud Manufacturing","Application environment"],"max_cite":1.0,"pub_year":2017.0,"sources":"['scp']","rawkeys":["mqtt","horizontal integrations","development environment","fog computing","application environment","internet of things","cloud manufacturing","horizontal integration","osgi","application development","gateway","internet of things (iot)"],"tags":["mqtt","horizontal integrations","development environment","fog computing","application environment","application developers","cloud manufacturing","osgi(open service gateway initial)","gateway","internet of things (iot)"]},{"p_id":18218,"title":"Osmotic Computing: A New Paradigm for Edge\/Cloud Integration","abstract":"\u00a9 2014 IEEE. Osmotic computing is a new paradigm to support the efficient execution of Internet of Things (IoT) services and applications at the network edge. This paradigm is founded on the need for a holistic distributed system abstraction enabling the deployment of lightweight microservices on resource-constrained IoT platforms at the network edge, coupled with more complex microservices running on large-scale datacenters. This paradigm is driven by the significant increase in resource capacity\/capability at the network edge, along with support for data transfer protocols that enable such resources to interact more seamlessly with datacenter-based services. This installment of 'Blue Skies' discusses osmotic computing features, challenges, and future directions.","keywords_author":["cloud computing","edge cloud integration","edge computing","Internet of Things"],"keywords_other":["Network edges","Resource capacity","Services and applications","Edge computing","Internet of Things (IOT)","Distributed systems","Data transfer protocols","Edge clouds"],"max_cite":25.0,"pub_year":2016.0,"sources":"['scp', 'wos']","rawkeys":["resource capacity","edge clouds","cloud computing","services and applications","internet of things","network edges","edge cloud integration","distributed systems","edge computing","data transfer protocols","internet of things (iot)"],"tags":["resource capacity","edge clouds","cloud computing","services and applications","network edges","edge cloud integration","distributed systems","edge computing","data transfer protocols","internet of things (iot)"]},{"p_id":22324,"title":"Secure Medical Data Transmission Model for IoT-Based Healthcare Systems","abstract":"\u00a9 2018 IEEE. Due to the significant advancement of the Internet of Things (IoT) in the healthcare sector, the security, and the integrity of the medical data became big challenges for healthcare services applications. This paper proposes a hybrid security model for securing the diagnostic text data in medical images. The proposed model is developed through integrating either 2-D discrete wavelet transform 1 level (2D-DWT-1L) or 2-D discrete wavelet transform 2 level (2D-DWT-2L) steganography technique with a proposed hybrid encryption scheme. The proposed hybrid encryption schema is built using a combination of Advanced Encryption Standard, and Rivest, Shamir, and Adleman algorithms. The proposed model starts by encrypting the secret data; then it hides the result in a cover image using 2D-DWT-1L or 2D-DWT-2L. Both color and gray-scale images are used as cover images to conceal different text sizes. The performance of the proposed system was evaluated based on six statistical parameters; the peak signal-to-noise ratio (PSNR), mean square error (MSE), bit error rate (BER), structural similarity (SSIM), structural content (SC), and correlation. The PSNR values were relatively varied from 50.59 to 57.44 in case of color images and from 50.52 to 56.09 with the gray scale images. The MSE values varied from 0.12 to 0.57 for the color images and from 0.14 to 0.57 for the gray scale images. The BER values were zero for both images, while SSIM, SC, and correlation values were ones for both images. Compared with the state-of-the-art methods, the proposed model proved its ability to hide the confidential patient's data into a transmitted cover image with high imperceptibility, capacity, and minimal deterioration in the received stego-image.","keywords_author":["Cryptography","DWT-1level","DWT-2level","encryption","healthcare services","Internet of Things","medical images","steganography"],"keywords_other":["DWT-2level","Medical diagnostic imaging","Medical services","DWT-1level","Healthcare services"],"max_cite":12.0,"pub_year":2018.0,"sources":"['scp']","rawkeys":["dwt-1level","steganography","healthcare services","internet of things","medical diagnostic imaging","medical images","dwt-2level","medical services","cryptography","encryption"],"tags":["dwt-1level","steganography","healthcare services","medical imaging","medical diagnostic imaging","dwt-2level","medical services","cryptography","encryption","internet of things (iot)"]},{"p_id":48951,"title":"Grounded approach for understanding changes in human emotional states in real time using psychophysiological sensory apparatuses","abstract":"\u00a9 Springer International Publishing AG 2017. This paper discusses the technical and philosophical challenges that researchers and practitioners face when attempting to classify human emotion based upon raw physiological data. It proposes the use of a representational learning approach that adopts techniques from industrial internet of things (IoT) solutions. It applies this approach to the classification of emotional states using functional near infrared spectroscopy (fNIRS) sensor data. The algorithm used first pre-processes the data using a combination of signal processing and vector quantization techniques. Next, it found the optimal number of natural clusters within human emotional states and used these as the target variables for either shallow or for deep learning classification. The deep learning variant used a Restricted Boltzmann Machine (RBM) to form a compressive representation of the input data prior to classification. A final single layer perception model learned the relationship between the input and output states. This approach would be useful for detecting real-time changes in human emotional state. It is able automatically create emotional states that are both highly separable and balanced. It is able to distinguish between low v. high emotional states across all tasks (F1-score of 71.4%) and is better at forming this distinction for tasks intended to elicit higher cognitive load such as the Tetris video game (F1-score of 87.1%) or the Multi Attribute Task Battery (F1-score of 77%).","keywords_author":["Affective computing","Brain computer interfaces","Brain signal processing","Classification","Cognitive computing","Decision support systems","Decision-making","Deep learning","DSS","Machine learning"],"keywords_other":["Affective Computing","Restricted boltzmann machine","Physiological data","Cognitive Computing","Functional near-infrared spectroscopy (fnirs)","Perception Modeling","Internet of Things (IOT)","Brain signal processing"],"max_cite":0.0,"pub_year":2017.0,"sources":"['scp']","rawkeys":["deep learning","affective computing","decision-making","perception modeling","machine learning","dss","brain signal processing","functional near-infrared spectroscopy (fnirs)","internet of things (iot)","classification","physiological data","brain computer interfaces","restricted boltzmann machine","cognitive computing","decision support systems"],"tags":["decision making","brain-computer interfaces","cloud computing","affective computing","perception modeling","machine learning","brain signal processing","classification","physiological data","functional near-infrared spectroscopy","restricted boltzmann machine","decision support systems","internet of things (iot)"]},{"p_id":44857,"title":"Weather data analysis and sensor fault detection using an extended IoT framework with semantics, big data, and machine learning","abstract":"\u00a9 2017 IEEE. In recent years, big data and Internet of Things (IoT) implementations started getting more attention. Researchers focused on developing big data analytics solutions using machine learning models. Machine learning is a rising trend in this field due to its ability to extract hidden features and patterns even in highly complex datasets. In this study, we used our Big Data IoT Framework in a weather data analysis use case. We implemented weather clustering and sensor anomaly detection using a publicly available dataset. We provided the implementation details of each framework layer (acquisition, ETL, data processing, learning and decision) for this particular use case. Our chosen learning model within the library is Scikit-Learn based k-means clustering. The data analysis results indicate that it is possible to extract meaningful information from a relatively complex dataset using our framework.","keywords_author":["anomaly detection","big data analytics","clustering","fault detection","framework","Internet of things","machine learning","weather data analysis"],"keywords_other":["Weather data","framework","Anomaly detection","clustering","Big Data Analytics"],"max_cite":0.0,"pub_year":2018.0,"sources":"['scp']","rawkeys":["anomaly detection","framework","internet of things","machine learning","fault detection","clustering","weather data","weather data analysis","big data analytics"],"tags":["anomaly detection","framework","machine learning","fault detection","clustering","weather data","weather data analysis","big data analytics","internet of things (iot)"]},{"p_id":14139,"title":"From data to actionable knowledge: Big data challenges in the web of things","abstract":"Extending the current Internet and providing connection, communication, and internetworking between devices and physical objects, or 'things,' is a growing trend that's often referred to as the Internet of Things (IoT). Integrating real-world data into the Web, with its large repositories of data, and providing Web-based interactions between humans and IoT resources is what the Web of Things (WoT) stands for. Here, the guest editors describe the Big Data issues in the WoT, discuss the challenges of extracting actionable knowledge and insights from raw sensor data, and introduce the theme articles in this special issue. \u00a9 2013 IEEE.","keywords_author":["Big Data","intelligent systems","Internet of Things","internetworking","IoT","Web of Things","WoT"],"keywords_other":null,"max_cite":64.0,"pub_year":2013.0,"sources":"['scp']","rawkeys":["internetworking","big data","internet of things","intelligent systems","wot","web of things","iot"],"tags":["internetworking","big data","intelligent systems","wot","web of things","internet of things (iot)"]},{"p_id":24396,"title":"Privacy preserving Internet of Things: From privacy techniques to a blueprint architecture and efficient implementation","abstract":"\u00a9 2017 Elsevier B.V.The Internet of Things (IoT) is the latest web evolution that incorporates billions of devices that are owned by different organisations and people who are deploying and using them for their own purposes. IoT-enabled harnessing of the information that is provided by federations of such IoT devices (which are often referred to as IoT things) provides unprecedented opportunities to solve internet-scale problems that have been too big and too difficult to tackle before. Just like other web-based information systems, IoT must also deal with the plethora of Cyber Security and privacy threats that currently disrupt organisations and can potentially hold the data of entire industries and even countries for ransom. To realise its full potential, IoT must deal effectively with such threats and ensure the security and privacy of the information collected and distilled from IoT devices. However, IoT presents several unique challenges that make the application of existing security and privacy techniques difficult. This is because IoT solutions encompass a variety of security and privacy solutions for protecting such IoT data on the move and in store at the device layer, the IoT infrastructure\/platform layer, and the IoT application layer. Therefore, ensuring end-to-end privacy across these three IoT layers is a grand challenge in IoT. In this paper, we tackle the IoT privacy preservation problem. In particular, we propose innovative techniques for privacy preservation of IoT data, introduce a privacy preserving IoT Architecture, and also describe the implementation of an efficient proof of concept system that utilises all these to ensure that IoT data remains private. The proposed privacy preservation techniques utilise multiple IoT cloud data stores to protect the privacy of data collected from IoT. The proposed privacy preserving IoT Architecture and proof of concept implementation are based on extensions of OpenIoT - a widely used open source platform for IoT application development. Experimental evaluations are also provided to validate the efficiency and performance outcomes of the proposed privacy preserving techniques and architecture.","keywords_author":["Internet of Things","IoT privacy and security","Secure IoT platform"],"keywords_other":["Internet of thing (IOT)","Experimental evaluation","Secure IoT platform","Web based information systems","Privacy and security","Efficient implementation","Innovative techniques","Efficiency and performance"],"max_cite":9.0,"pub_year":2017.0,"sources":"['scp', 'wos']","rawkeys":["efficiency and performance","experimental evaluation","internet of thing (iot)","internet of things","privacy and security","secure iot platform","web based information systems","iot privacy and security","efficient implementation","innovative techniques"],"tags":["efficiency and performance","experimental evaluation","privacy and security","secure iot platform","web based information systems","iot privacy and security","efficient implementation","innovative techniques","internet of things (iot)"]},{"p_id":36692,"title":"A Memory-Friendly Multi-modal Emotion Analysis for Smart Toy","abstract":"\u00a9 2017 IEEE. The recent advance in deep learning technologies has provided many opportunities to the industries in developing smarter products, such as smart toys, smart cars and smart homes. Unfortunately, a common practical issue to these deep learning methods is the high requirements in computing power even for the prediction part. As the deep learning models' complexities increase, the memory requirements for these models usually exceed the limit of many low end computing devices, such as mobile IoT devices. It is even worse when a multimodel approach is employed. In this paper, we will demonstrate with the development of multimodal emotion analysis on a smart toy, where the user's speech, facial expression and action are used for the understanding of user's emotion. By trimming down DNN complexity or replaced by other learning approaches, we are able to sequeeze four classifiers in 800MB of memory. Finally, results of these methods are ensembled with a fusion approach using a fully connected neural network to obtain a more accuracy and stable result. Our multimodal approach achieved an improvement of about 20% when comparing any unimodal emotion analysis.","keywords_author":["Deep Learning","Embedded Devices","Emotion","IoT","Machine Learning","Multi-Modal"],"keywords_other":["Emotion","Fully connected neural network","Multi-modal approach","Learning technology","Multi-model approaches","Multi-modal","Memory requirements","Embedded device"],"max_cite":1.0,"pub_year":2017.0,"sources":"['scp']","rawkeys":["embedded devices","multi-modal approach","learning technology","emotion","deep learning","memory requirements","machine learning","fully connected neural network","multi-model approaches","multi-modal","iot","embedded device"],"tags":["multi-modal approach","learning technology","emotion","memory requirements","machine learning","fully connected neural network","multi-model approaches","multi-modal","embedded device","internet of things (iot)"]},{"p_id":36696,"title":"Intelligent IoT Traffic Classification Using Novel Search Strategy for Fast Based-Correlation Feature Selection in Industrial Environments","abstract":"IEEE Internet of Things (IoT) can be combined with Machine Learning in order to provide intelligent applications to the network nodes. Furthermore, IoT expands these advantages and technologies to the industry. In this work, we propose a modification of one of the most popular algorithms for feature selection, Fast Based-Correlation Feature (FCBF). The key idea is to split the feature space in fragments with the same size. By introducing this division, we can improve the correlation and, therefore, the Machine Learning applications that are operating on each node. This kind of IoT applications for industry allows us to separate and prioritize the sensor data from the multimedia-related traffic. With this separation, the sensors are able to detect efficiently emergency situations and avoid both material and human damage. The results show the performance of the three FCBF based algorithms for different problems and different classifiers, confirming the improvements achieved by our approach in terms of model accuracy and execution time.","keywords_author":["Classification algorithms","Correlation based methods","Emergency detection","Feature extraction","Feature Selection","Filter Methods","Filtering algorithms","Industries","Industry","Information filters","Iot","Machine learning algorithms","Machine Learning.","Multimedia traffic"],"keywords_other":["Filter method","Filtering algorithm","Classification algorithm","Emergency detection","Multimedia traffic"],"max_cite":1.0,"pub_year":2017.0,"sources":"['scp']","rawkeys":["multimedia traffic","industry","correlation based methods","industries","machine learning","classification algorithms","machine learning algorithms","emergency detection","filter method","information filters","feature selection","filtering algorithm","filtering algorithms","feature extraction","filter methods","iot","classification algorithm"],"tags":["multimedia traffic","information filtering","industry","correlation based methods","machine learning algorithms","machine learning","emergency detection","filter method","feature selection","filtering algorithm","feature extraction","classification algorithm","internet of things (iot)"]},{"p_id":44888,"title":"'Petroleum Analytics Learning Machine' for optimizing the Internet of Things of today's digital oil field-to-refinery petroleum system","abstract":"\u00a9 2017 IEEE. The Petroleum Analytics Learning Machine (PALM) is a machine-learning-based, 'brutally empirical' analysis system for managing the Internet of Things (IoT) in upstream and midstream oil and gas operations. PALM was developed in the new unconventional shale oil and gas plays of America where the simultaneous analysis of hundreds of IoT attributes from hundreds of horizontal wells with thousands of hydraulic fracture stages must be analyzed in near real-time. PALM was validated in more than 3000 shale oil and gas wells with more than 10,000 hydraulic fracture stages in the Permian Basin, TX, and the Marcellus Basin, PA. PALM comprises Machine Analytics Applications (Apps) that are big-data-centric, using computational machine learning, predictive, and prescriptive analysis techniques to maximize production of natural gas and hydrocarbon liquids while minimizing costs of operations. The PALM predictive and prescriptive technologies utilize Support Vector Machine learning, signatures, and real-time Random Forest and decision trees to steer hydraulic fractures to become high instead of low oil and gas producers as completions of horizontal shale wells progress. PALM also uses Support Vector Regression, logistic regression, Bayesian models, nearest neighbors, neural networks and deep learning networks, uniquely combined as ensemble learning tools, to weigh the importance of hundreds to thousands of geological, geophysical, and engineering attributes, both measured in the field by the IoT and computed from theoretical analyses such as reservoir simulation models and 4D seismic monitoring of production changes over time. PALM is itself an IoT system since each of these methods are written as separate apps, which are then strung together by the operator. Utilizing all oil and gas well attributes to compute Importance Weights and predicted oil, gas, and water production allows the forecasting of accurate Estimated Ultimate Recovery (EUR) over the lifetime of each well.","keywords_author":["Artificial Intelligence","Big Data Analytics","IoT and the Digital Oil Field","Machine Learning","Teaching Energy Industry Professionals"],"keywords_other":["Energy industry","Internet of thing (IOT)","Reservoir simulation model","Support vector regression (SVR)","Digital oil field","Estimated ultimate recoveries","Oil and gas operations","Big Data Analytics"],"max_cite":0.0,"pub_year":2018.0,"sources":"['scp']","rawkeys":["artificial intelligence","digital oil field","estimated ultimate recoveries","oil and gas operations","teaching energy industry professionals","support vector regression (svr)","internet of thing (iot)","machine learning","reservoir simulation model","iot and the digital oil field","big data analytics","energy industry"],"tags":["digital oilfield","teaching energy industry professionals","estimated ultimate recoveries","oil and gas operations","support vector regression (svr)","machine learning","internet of things (iot)","reservoir simulation model","iot and the digital oil field","big data analytics","energy industry"]},{"p_id":3930,"title":"An overview of next-generation architectures for machine learning: Roadmap, opportunities and challenges in the IoT era","abstract":"\u00a9 2018 EDAA. The number of connected Internet of Things (IoT) devices are expected to reach over 20 billion by 2020. These range from basic sensor nodes that log and report the data to the ones that are capable of processing the incoming information and taking an action accordingly. Machine learning, and in particular deep learning, is the de facto processing paradigm for intelligently processing these immense volumes of data. However, the resource inhibited environment of IoT devices, owing to their limited energy budget and low compute capabilities, render them a challenging platform for deployment of desired data analytics. This paper provides an overview of the current and emerging trends in designing highly efficient, reliable, secure and scalable machine learning architectures for such devices. The paper highlights the focal challenges and obstacles being faced by the community in achieving its desired goals. The paper further presents a roadmap that can help in addressing the highlighted challenges and thereby designing scalable, high-performance, and energy efficient architectures for performing machine learning on the edge.","keywords_author":["Convolutional Neural Networks","Deep Learning","IoT","Machine Learning"],"keywords_other":["Iot devices","Energy-efficient architectures","Limited energies","Scalable machine learning","Convolutional neural network","Emerging trends","Internet of Things (IOT)","Desired datum"],"max_cite":0.0,"pub_year":2018.0,"sources":"['scp', 'ieee']","rawkeys":["energy-efficient architectures","convolutional neural networks","deep learning","machine learning","emerging trends","iot devices","desired datum","convolutional neural network","iot","limited energies","scalable machine learning","internet of things (iot)"],"tags":["energy-efficient architectures","machine learning","emerging trends","iot devices","desired datum","convolutional neural network","scalable machine learning","limited energies","internet of things (iot)"]},{"p_id":22363,"title":"Determination of Rule Patterns in Complex Event Processing Using Machine Learning Techniques","abstract":"\u00a9 2015 The Authors. Published by Elsevier B.V.Complex Event Processing (CEP) is a novel and promising methodology that enables the real-time analysis of stream event data. The main purpose of CEP is detection of the complex event patterns from the atomic and semantically low-level events such as sensor, log, or RFID data. Determination of the rule patterns for matching these simple events based on the temporal, semantic, or spatial correlations is the central task of CEP systems. In the current design of the CEP systems, experts provide event rule patterns. Having reached maturity, the Big Data Systems and Internet of Things (IoT) technology require the implementation of advanced machine learning approaches for automation in the CEP domain. The goal of this research is proposing a machine learning model to replace the manual identification of rule patterns. After a pre-processing stage (dealing with missing values, data outliers, etc.), various rule-based machine learning approaches were applied to detect complex events. Promising results with high preciseness were obtained. A comparative analysis of the performance of classifiers is discussed.","keywords_author":["Complex Event Processing","Event Patterns","Machine Learning","Rule Based Classification"],"keywords_other":["Complex event processing (CEP)","Machine learning approaches","Rule-based classification","Performance of classifier","Complex event processing","Internet of Things (IOT)","Event pattern","Machine learning techniques"],"max_cite":12.0,"pub_year":2015.0,"sources":"['scp']","rawkeys":["event patterns","event pattern","machine learning techniques","machine learning","performance of classifier","rule based classification","rule-based classification","complex event processing","complex event processing (cep)","machine learning approaches","internet of things (iot)"],"tags":["machine learning techniques","machine learning","performance of classifier","rule-based classification","complex event processing","event pattern","machine learning approaches","internet of things (iot)"]},{"p_id":51037,"title":"To run or not to run: Predicting resource usage pattern in a smartphone","abstract":"\u00a9 Institute for Computer Sciences, Social Informatics and Telecommunications Engineering 2015.Smart mobile phones are vital to the Mobile Cloud Computing (MCC) paradigm where compute jobs can be offloaded to the devices from the Cloud and vice-versa, or the devices can act as peers to collaboratively perform a task. Recent research in IoT context also points to the use of smartphones as sensor gateways highlighting the importance of data processing at the network edge. In either case, when a smart phone is used as a compute resource or a sensor gateway, the corresponding tasks must be executed in addition to the user\u2019s normal activities on the device without affecting the user experience. In this paper, we propose a framework that can act as an enabler of such features by classifying the availability of system resources like CPU, memory, network usage based on applications running on an Android phone. We show that, such app-based classifications are user-specific and app usage varies with different handsets, leading to different classifications. We further show that irrespective of such variation in classification, distinct patterns exist for all users with available opportunity to schedule external tasks, without affecting user experience. Based on the next to-be-used applications, we output a predicted set of system resources. The resource levels along with handset architecture may be used to estimate worst case execution time for external jobs.","keywords_author":["IoT","Machine learning","Mobile cloud computing","Resource utilisation","Sensor data","Smart phone","Usage prediction"],"keywords_other":["Recent researches","Smart Mobile Phones","IoT","Resource usage patterns","Compute resources","Resource utilisation","Sensor data","Worst-case execution time"],"max_cite":0.0,"pub_year":2015.0,"sources":"['scp']","rawkeys":["sensor data","resource usage patterns","usage prediction","smart phone","machine learning","compute resources","mobile cloud computing","smart mobile phones","iot","recent researches","worst-case execution time","resource utilisation"],"tags":["sensor data","resource usage patterns","usage prediction","machine learning","computational resources","mobile cloud computing","smart mobile phones","worst-case execution time","recent researches","smart phones","internet of things (iot)","resource utilisation"]},{"p_id":46945,"title":"Vehicular dataset for road assessment conditions","abstract":"\u00a9 2017 IEEE. The Internet of Things (IoT) is a very promising concept that by connecting numerous devices to the internet and extracting large sums of information (BigData) can enable the realisation of various futuristic scenarios. In order to develop and assess future applications and services, it is necessary the availability of datasets that can be used to train, test and cross validate. Project SCoT (Smart Cloud of Things) has developed an M2M platform capable of collecting information from heterogeneous devices and collide that information in a large data repository. During its pilot phase, the project made the assessment of the road conditions in the region of Aveiro, Portugal. In this work we make the dataset used on the previous mentioned pilot publicly available. With this dataset our road assessment algorithm reached 80% accuracy in the task of pothole detection, other scenarios (that take into account vehicular speed, position and acceleration) can also be explored. The dataset was not pre-processed in anyway, the only transformation was made to protect the identity of the volunteers.","keywords_author":["Dataset","IoT","M2M","Machine Learning"],"keywords_other":["Pilot Phase","Road condition","Internet of thing (IOT)","Heterogeneous devices","Large data","Future applications","Dataset","Portugal"],"max_cite":0.0,"pub_year":2017.0,"sources":"['scp']","rawkeys":["large data","portugal","dataset","heterogeneous devices","pilot phase","internet of thing (iot)","machine learning","road condition","iot","future applications","m2m"],"tags":["large data","portugal","heterogeneous devices","machine learning","road condition","data sets","machine to machines","pilot phase","future applications","internet of things (iot)"]},{"p_id":71522,"title":"Priority Control in Communication Networks for Accuracy-Freshness Tradeoff in Real-Time Road-Traffic Information Delivery","abstract":"Delivering real-time road-traffic information to the driver is a straightforward solution to the problem of road-traffic congestion. The information is more effective as it is fresh and more accurate. However, real-time road-traffic information delivery has a fundamental problem: an accuracy-freshness tradeoff. Unfortunately, real-time road-traffic information delivery has difficulty satisfying both requirements. To guarantee the freshness, the information needs to be delivered on the basis of the data received by a cloud or edge server before a predetermined deadline. However, only a limited amount of data is received due to bandwidth limitation and processing overhead in communication networks, which results in the poor accuracy of the delivered information. The only way to improve the accuracy is to make the deadline less strict, which results in deteriorating the freshness of information. The proposed system solves this tradeoff. The key idea is that data more \"important\" for the accuracy of information are more prioritized when the data are transferred in communication networks. In the proposed system, \"importance\" is determined by how helpful the data are when the system needs to estimate missing spatial information from a limited amount of received data by using the machine learning technique. In this paper, simulation results verify that the proposed system ensures the accuracy of road-traffic information while satisfying the freshness requirement.","keywords_author":["Internet of Things","road-traffic information","real-time information delivery","priority control","active learning","edge computing."],"keywords_other":["INTERNET","INTELLIGENT TRANSPORTATION","SOFTWARE-DEFINED NETWORKING"],"max_cite":0.0,"pub_year":2017.0,"sources":"['ieee', 'wos']","rawkeys":["software-defined networking","internet of things","active learning","priority control","road-traffic information","edge computing","internet","intelligent transportation","real-time information delivery"],"tags":["software-defined networking","machine learning","priority control","road-traffic information","edge computing","internet","intelligent transportation","internet of things (iot)","real-time information delivery"]},{"p_id":36708,"title":"A smart hydroponics farming system using exact inference in Bayesian network","abstract":"\u00a9 2017 IEEE. Smart farming is seen to be the future of agriculture as it produces higher quality of crops by making farms more intelligent in sensing its controlling parameters. Analyzing massive amount of data can be done by accessing and connecting various devices with the help of Internet of Things (IoT). However, it is not enough to have an Internet support and self-updating readings from the sensors but also to have a self-sustainable agricultural production with the use of analytics for the data to be useful. This study developed a smart hydroponics system that is used in automating the growing process of the crops using exact inference in Bayesian Network (BN). Sensors and actuators are installed in order to monitor and control the physical events such as light intensity, pH, electrical conductivity, water temperature, and relative humidity. The sensor values gathered were used to build the Bayesian Network in order to infer the optimum value for each parameter. A web interface is developed wherein the user can monitor and control the farm remotely via the Internet. Results have shown that the fluctuations in terms of the sensor values were minimized in the automatic control using BN as compared to the manual control. The yielded crop on the automatic control was 66.67% higher than the manual control which implies that the use of exact inference in BN AIDS in producing high-quality crops. In the future, the system can use higher data analytics and longer data gathering to improve the accuracy of inference.","keywords_author":["Actuators","Bayesian Network","Hydroponics","Internet of Things","Machine Learning","Sensors"],"keywords_other":["Hydroponics","Self-sustainable","Water temperatures","Electrical conductivity","Monitor and control","Sensors and actuators","Internet of Things (IOT)","Controlling parameters"],"max_cite":1.0,"pub_year":2017.0,"sources":"['scp']","rawkeys":["electrical conductivity","hydroponics","sensors","monitor and control","internet of things","controlling parameters","actuators","machine learning","self-sustainable","bayesian network","sensors and actuators","water temperatures","internet of things (iot)"],"tags":["electrical conductivity","hydroponics","sensors","monitor and control","machine learning","control parameters","actuators","self-sustainable","water temperature","bayesian networks","sensors and actuators","internet of things (iot)"]},{"p_id":57195,"title":"Improving the Accuracy and Efficiency of PM2.5 Forecast Service Using Cluster-Based Hybrid Neural Network Model","abstract":"Information and communication technologies have been widely used to achieve the objective of smart city development. A smart air quality sensing and forecasting system is an important part of a smart city. One of the major challenges in designing such a forecast system is ensuring high accuracy and an acceptable computation time. In this paper, we show that it is possible to accurately forecast fine particulate matter (PM2.5) concentrations with low computation time by using different clustering techniques. An Internet of Things framework comprising of Airbox devices for PM2.5 monitoring has been used to acquire the data. Our main focus is to achieve high forecasting accuracy with reduced computation time. We use a hybrid model to do the forecast and a grid based system to cluster the monitoring stations based on the geographical distance. The experiments and evaluation is done using Airbox devices data from 557 stations deployed all over Taiwan. We are able to demonstrate that a proper clustering based on geographical distance can reduce the forecasting error rate and also the computation time. Also, in order to further evaluate our system, we have applied wavelet-based clustering to group the monitoring stations. A final comparative analysis is done for different clustering schemes with respect to accuracy and computational time.","keywords_author":["Internet of Things","forecasting","smart cities","neural networks"],"keywords_other":["ARIMA","POLLUTION","CITY","SYSTEM"],"max_cite":0.0,"pub_year":2018.0,"sources":"['wos']","rawkeys":["neural networks","arima","internet of things","forecasting","system","city","pollution","smart cities"],"tags":["cities","neural networks","arima","forecasting","system","pollution","internet of things (iot)","smart cities"]},{"p_id":71533,"title":"Studying real traffic and mobility scenarios for a Smart City using a new monitoring and tracking system","abstract":"This paper presents a novel mobility monitoring system and some of its applications to address problems that would be solved in a Smart City, such as the optimization of traffic flows in terms of trip-time and security (Smart Traffic), and the improvement of security or energetic issues inside buildings. The system tracks the movement of people and vehicles monitoring the radioelectric space, catching the WiFi and Bluetooth signals emitted by personal (smartphones) or on-board (hands-free) devices. A study has been conducted in four different real scenarios, i.e. with real data gathered by the system: two related with people's mobility (a public building and a discotheque); and two focused in traffic tracking (urban and intercity roads). The analysis has consisted on the application of different data mining techniques to extract useful knowledge, traffic forecasting methods to perform accurate predictions, and statistical analyses to model and validate the system reliability (comparing to other real data sources). The obtained results show the viability and utility of the system in all the cases, along with some of its multiple applications for solving different issues in a city. (C) 2016 Elsevier B.V. All rights reserved.","keywords_author":["Smart Traffic","Transit indicators","Traffic forecast","Mobility analysis","Smart City","Internet of Things"],"keywords_other":["PREDICTION","PEOPLE","MODEL","TERM","NETWORK"],"max_cite":3.0,"pub_year":2017.0,"sources":"['wos']","rawkeys":["mobility analysis","network","traffic forecast","model","prediction","internet of things","term","transit indicators","smart traffic","people","smart city"],"tags":["mobility analysis","traffic forecasting","model","prediction","term","networks","transit indicators","smart traffic","people","internet of things (iot)","smart cities"]},{"p_id":42867,"title":"Pushing intelligence to the network edge","abstract":"\u00a9 2018 IEEE. Networking has already entered a new era where it is evolving into the Internet of Things (IoT). The future network consists of tens of billions of connected devices. These things bear different security vulnerabilities and can be abused to perform diverse attacks. Due to the heterogeneous nature of IoT and its massive deployment, managing the IoT security requires automated techniques to deal with the emerging security challenges. To address this problem, we propose a model where intelligence is integrated at the network edge to extract specific statistical features per network flow. These features are passed to a controller which implements a classification algorithm to determine the device type generating the observed flow. A monitoring module is responsible for analyzing the network traffic behavior. If abnormal traffic is detected from a certain device, the controller takes the adequate action (e.g. blocking the traffic from this device). After collecting data from IoT devices, we evaluate our proposed classification model. Results show that our approach can achieve up to 99.6% accuracy for device type identification and up to 99.9% for traffic type classification.","keywords_author":["Fog Computing","Internet of Things","Internet Traffic Classification","Machine Learning","SDN"],"keywords_other":null,"max_cite":0.0,"pub_year":2018.0,"sources":"['scp']","rawkeys":["fog computing","internet of things","machine learning","internet traffic classification","sdn"],"tags":["fog computing","software-defined networking","machine learning","internet traffic classification","internet of things (iot)"]},{"p_id":46972,"title":"Towards a big data analytics framework for smart cities","abstract":"\u00a9 2017 Association for Computing Machinery. Due to the emergence of the Internet of Things, social network and the mobile applications, the data produced is very important and diversified. These data are very useful to design a smart city based on machine learning and statistical algorithms used by real time analytics or predictive analytics. This paper presents a framework headlines that helps cities to take profit from data to improve the life quality of citizens in several areas: mobility and transportation, economy, environment and energy, public safety, health, education and others. This framework describe different data sources (IoT, software applications, sensors,..) used to generate the information. It also describes, the processing steps that make data usable, the different machine learning statistical algorithms and data mining techniques used to extract the insights. This framework, combine two complementary approaches: Techno-centric approach based on a city populated by sensors that collect a massive data to drive all the urban services, and the collaborative approach that addresses citizens more directly.","keywords_author":["Analytics","Big data","Internet of things (IoT)","Machine learning","Smart cities"],"keywords_other":["Environment and energies","Analytics","Software applications","Mobile applications","Statistical algorithm","Collaborative approach","Real-time analytics","Internet of Things (IOT)"],"max_cite":0.0,"pub_year":2017.0,"sources":"['scp']","rawkeys":["collaborative approach","mobile applications","big data","real-time analytics","smart cities","machine learning","software applications","environment and energies","statistical algorithm","internet of things (iot)","analytics"],"tags":["collaborative approach","mobile applications","big data","real-time analytics","smart cities","machine learning","software applications","environment and energies","statistical algorithm","internet of things (iot)","analytics"]},{"p_id":8062,"title":"Health Monitoring and Management Using Internet-of-Things (IoT) Sensing with Cloud-Based Processing: Opportunities and Challenges","abstract":"\u00a9 2015 IEEE.Among the panoply of applications enabled by the Internet of Things (IoT), smart and connected health care is a particularly important one. Networked sensors, either worn on the body or embedded in our living environments, make possible the gathering of rich information indicative of our physical and mental health. Captured on a continual basis, aggregated, and effectively mined, such information can bring about a positive transformative change in the health care landscape. In particular, the availability of data at hitherto unimagined scales and temporal longitudes coupled with a new generation of intelligent processing algorithms can: (a) facilitate an evolution in the practice of medicine, from the current post facto diagnose-and-treat reactive paradigm, to a proactive framework for prognosis of diseases at an incipient stage, coupled with prevention, cure, and overall management of health instead of disease, (b) enable personalization of treatment and management options targeted particularly to the specific circumstances and needs of the individual, and (c) help reduce the cost of health care while simultaneously improving outcomes. In this paper, we highlight the opportunities and challenges for IoT in realizing this vision of the future of health care.","keywords_author":["Analytics","IoT","Remote health monitoring","Visualization"],"keywords_other":["Management options","Internet of thing (IOT)","Analytics","IoT","Living environment","Intelligent processing","Remote health monitoring","Internet of Things (IOT)"],"max_cite":114.0,"pub_year":2015.0,"sources":"['scp', 'wos']","rawkeys":["remote health monitoring","internet of thing (iot)","living environment","management options","iot","intelligent processing","visualization","internet of things (iot)","analytics"],"tags":["remote health monitoring","living environment","management options","intelligent processing","visualization","internet of things (iot)","analytics"]},{"p_id":38782,"title":"Graph Analysis of Fog Computing Systems for Industry 4.0","abstract":"\u00a9 2017 IEEE. Increased adoption of Fog Computing concepts into Cyber Physical Systems (CPS) is a driving force for implementing Industry 4.0. The modern industrial environment focuses on providing a flexible factory floor that suits the needs of modern manufacturing through the reduction of downtimes, reconfiguration times, adoption of new technologies and the increase of its production capabilities and rates. Fog Computing through CPS aims to provide a flexible orchestration and management platform that can meet the needs of this emerging industry model. Proposals on Fog Computing platform and Software Defined Networks (SDN) for Industry allow for resource virtualization and access throughout the system enabling large composite application systems to be deployed on multiple nodes. The increase of reliability, redundancy and runtime parameters as well as the reduction of costs in such systems are of key interest to Industry and researchers as well. The development of optimization algorithms and methods is made difficult by the complexity of such systems and the lack of real-world data on fog systems resulting in algorithms that are not being designed for real world scenarios. We propose a set of use-case scenarios based on our Industrial partner that we analyze to determine the graph based parameters of the system that allows us to scale and generate a more realistic testing scenario for future optimization attempts as well as determine the nature of such systems in comparison to other networks types. To show the differences between these scenarios and our real-world use-case we have selected a set of key graph characteristics based on which we analyze and compare the resulting graphs from the systems.","keywords_author":["CPS","Fog Computing","Fog of Things","Graph Analysis","Industry 4.0","Internet of Things"],"keywords_other":["Composite applications","Cyber-physical systems (CPS)","Industrial environments","Optimization algorithms","Production capabilities","Resource Virtualization","Management platforms","Graph analysis"],"max_cite":1.0,"pub_year":2017.0,"sources":"['scp']","rawkeys":["fog of things","fog computing","production capabilities","graph analysis","internet of things","cyber-physical systems (cps)","composite applications","optimization algorithms","resource virtualization","management platforms","industrial environments","industry 4.0","cps"],"tags":["cyber-physical systems","fog of things","fog computing","production capabilities","graph analysis","composite applications","optimization algorithms","resource virtualization","management platforms","industrial environments","industry 4.0","internet of things (iot)"]},{"p_id":36734,"title":"How 5G wireless (and Concomitant Technologies) will revolutionize healthcare?","abstract":"\u00a9 2017 by the authors. The need to have equitable access to quality healthcare is enshrined in the United Nations (UN) Sustainable Development Goals (SDGs), which defines the developmental agenda of the UN for the next 15 years. In particular, the third SDG focuses on the need to \"ensure healthy lives and promote well-being for all at all ages\". In this paper, we build the case that 5G wireless technology, along with concomitant emerging technologies (such as IoT, big data, artificial intelligence and machine learning), will transform global healthcare systems in the near future. Our optimism around 5G-enabled healthcare stems from a confluence of significant technical pushes that are already at play: apart from the availability of high-throughput low-latency wireless connectivity, other significant factors include the democratization of computing through cloud computing; the democratization of Artificial Intelligence (AI) and cognitive computing (e.g., IBMWatson); and the commoditization of data through crowdsourcing and digital exhaust. These technologies together can finally crack a dysfunctional healthcare system that has largely been impervious to technological innovations. We highlight the persistent deficiencies of the current healthcare system and then demonstrate how the 5G-enabled healthcare revolution can fix these deficiencies. We also highlight open technical research challenges, and potential pitfalls, that may hinder the development of such a 5G-enabled health revolution.","keywords_author":["5G","Artificial intelligence and machine learning","Big data analytics","Healthcare","Internet of Things"],"keywords_other":["Emerging technologies","Cognitive Computing","Technical research","Low-latency wireless connectivity","Wireless technologies","Data analytics","Health-care system","Technological innovation"],"max_cite":1.0,"pub_year":2017.0,"sources":"['scp']","rawkeys":["artificial intelligence and machine learning","technical research","technological innovation","healthcare","data analytics","emerging technologies","internet of things","wireless technologies","big data analytics","low-latency wireless connectivity","5g","cognitive computing","health-care system"],"tags":["technical research","technological innovation","cloud computing","healthcare","data analytics","emerging technologies","machine learning","internet of things (iot)","low-latency wireless connectivity","5g","big data analytics","health-care system","wireless technology"]},{"p_id":32642,"title":"Multimedia IoT systems and applications","abstract":"\u00a9 2017 IEEE. A wide spectrum of applications based on Internet of things (IoTs) are recently emerging and have drawn considerable attention. IoT-based multimedia (IoTMM) applications have received relatively less attention. This article provides a review of the field of mobile IoTMM systems; it assesses the state of the art and then discusses some active areas of ongoing research, particularly issues related to video traffic prioritization, virtualization of network elements, mobility management, and security issues. A priority-based resource allocation strategy for IoTMM is also introduced. Overall, IoTMM presents challenges and opportunities for the stakeholders.","keywords_author":null,"keywords_other":["Security issues","Resource allocation strategies","Prioritization","Priority-based","Mobility management","Internet of thing (IoTs)","State of the art","Network element"],"max_cite":3.0,"pub_year":2017.0,"sources":"['scp']","rawkeys":["prioritization","state of the art","network element","mobility management","security issues","resource allocation strategies","priority-based","internet of thing (iots)"],"tags":["prioritization","state of the art","network element","mobility management","security issues","resource allocation strategies","priority-based","internet of things (iot)"]},{"p_id":42886,"title":"Energy and area efficiency in neuromorphic computing for resource constrained devices","abstract":"\u00a9 2018 Association for Computing Machinery. Resource constrained devices are the building blocks of the internet of things (IoT) era. Since the idea behind IoT is to develop an interconnected environment where the devices are tiny enough to operate with limited resources, several control systems have been built to maintain low energy and area consumption while operating as IoT edge devices. Several researchers have begun work on implementing control systems built from resource constrained devices using machine learning. However, there are many ways such devices can achieve lower power consumption and area utilization while maximizing application efficiency. Spiky neuromorphic computing (SNC) is an emerging paradigm that can be leveraged in resource constrained devices for several emerging applications. While delivering the benefits of machine learning, SNC also helps minimize power consumption. For example, low energy memory devices (memristors) are often used to achieve low power operation and also help in reducing system area. In total, we anticipate SNC will provide computational efficiency approaching that of deep learning while using low power, resource constrained devices.","keywords_author":["Internet of things","Machine learning","Memristor","Neuromorphic"],"keywords_other":["Internet of thing (IOT)","Lower-power consumption","Neuromorphic","Memristor","Emerging applications","Resourceconstrained devices","Neuromorphic computing","Low-power operation"],"max_cite":0.0,"pub_year":2018.0,"sources":"['scp']","rawkeys":["lower-power consumption","internet of thing (iot)","internet of things","machine learning","resourceconstrained devices","emerging applications","neuromorphic","neuromorphic computing","memristor","low-power operation"],"tags":["lower-power consumption","machine learning","resourceconstrained devices","emerging applications","neuromorphic","neuromorphic computing","resistive switching memory","internet of things (iot)","low-power operation"]},{"p_id":18319,"title":"Fog Computing for the Internet of Things: Security and Privacy Issues","abstract":"\u00a9 1997-2012 IEEE. The inherent characteristics of Internet of Things (IoT) devices, such as limited storage and computational power, require a new platform to efficiently process data. The concept of fog computing has been introduced as a technology to bridge the gap between remote data centers and IoT devices. Fog computing enables a wide range of benefits, including enhanced security, decreased bandwidth, and reduced latency. These benefits make the fog an appropriate paradigm for many IoT services in various applications such as connected vehicles and smart grids. Nevertheless, fog devices (located at the edge of the Internet) obviously face many security and privacy threats, much the same as those faced by traditional data centers. In this article, the authors discuss the security and privacy issues in IoT environments and propose a mechanism that employs fog to improve the distribution of certificate revocation information among IoT devices for security enhancement. They also present potential research directions aimed at using fog computing to enhance the security and privacy issues in IoT environments.","keywords_author":["certificate revocation","fog computing","Internet of Things","Internet\/Web technologies","IoT","privacy","security"],"keywords_other":["Certificate revocation informations","Security and privacy issues","Inherent characteristics","security","Security enhancements","Certificate revocation","Traditional data centers","Internet of Things (IOT)"],"max_cite":25.0,"pub_year":2017.0,"sources":"['scp', 'wos']","rawkeys":["certificate revocation informations","privacy","internet\/web technologies","security and privacy issues","fog computing","traditional data centers","internet of things","security","security enhancements","internet of things (iot)","iot","certificate revocation","inherent characteristics"],"tags":["certificate revocation informations","privacy","internet\/web technologies","security and privacy issues","fog computing","traditional data centers","security","security enhancements","inherent characteristics","certificate revocation","internet of things (iot)"]},{"p_id":30617,"title":"An artificial immune-based distributed intrusion detection model for the internet of things","abstract":"Traditional detection technology for network attacks is difficult to adapt the complicated and changeful environment of the Internet of Things (IoT). In the interest of resolving the distributed intrusion detection problem of IoT, this paper proposes an artificial immune-based theory model for distributed intrusion detection in IoT. Artificial immune principles are used to solve the problem of IoT intrusion detection. Antigen, self, non-self and detector in the IoT environment are defined. Good immune mechanisms are simulated. Detector is evolved dynamically to make the proposed model have self-learning and self-adaptation. The outstanding detectors which have accepted training are shared in the whole IoT to adapt the local IoT environment and improve the ability of global intrusion detection in IoT. The proposed model is expected to realize detecting intrusion of IoT in distribution and parallelity. \u00a9 (2012) Trans Tech Publications.","keywords_author":["Artificial immune system","Distribution","Internet of things","Intrusion detection"],"keywords_other":["Theory model","Internet of things","Parallelity","Artificial Immune System","Self adaptation","Distribution","Network attack","Detection technology","Artificial immune principle","Immune mechanism","Distributed intrusion detection","Self-learning"],"max_cite":4.0,"pub_year":2012.0,"sources":"['scp']","rawkeys":["immune mechanism","self-learning","artificial immune system","distribution","intrusion detection","artificial immune principle","theory model","internet of things","distributed intrusion detection","parallelity","detection technology","self adaptation","network attack"],"tags":["immune mechanism","self-learning","artificial immune principle","intrusion detection systems","machine learning","theory model","parallelizations","distributed intrusion detection","distributions","detection technology","self-adaptive","network attack","internet of things (iot)"]},{"p_id":26525,"title":"Tariff agent: Interacting with a future smart energy system at home","abstract":"\u00a9 2016 ACM.Smart systems are becoming increasingly ubiquitous and consequently transforming our lives. The level of system autonomy plays a vital role in the development of smart systems as it profoundly affects how people and these systems interact with each other. However, to date, there are very few studies on human interaction with such systems. This paper presents findings from two field studies where two different prototypes for automating energy tariff-switching were developed and evaluated in the wild. Both prototypes offer flexible autonomy by which users can shift the system's level of autonomy among three options: suggestion-only, semi-autonomy, and full autonomy, whenever they like. Our findings based on thematic analysis show that flexible autonomy is a promising way to sustain users' engagement with smart systems, despite their occasional mistakes. The findings also suggest that users take responsibility for the undesired outcomes of automated actions when delegation of autonomy can be adjusted flexibly.","keywords_author":["Algorithms","Design","Field study","Flexible autonomy","H.5.2 [information interfaces and presentation]: user interfaces","Human Factors","Human-agent interaction","Interactive intelligent systems","Internet of things","Smart grid"],"keywords_other":["Human agent interactions","Field studies","Flexible autonomy","Interactive intelligent systems","Smart grid","H.5.2 [Information Interfaces and Presentation]: User Interfaces"],"max_cite":7.0,"pub_year":2016.0,"sources":"['scp']","rawkeys":["flexible autonomy","design","field studies","smart grid","human agent interactions","internet of things","field study","h.5.2 [information interfaces and presentation]: user interfaces","human-agent interaction","interactive intelligent systems","algorithms","human factors"],"tags":["flexible autonomy","design","field studies","smart grid","human agent interactions","h.5.2 [information interfaces and presentation]: user interfaces","interactive intelligent systems","algorithms","human factors","internet of things (iot)"]},{"p_id":10146,"title":"MalDozer: Automatic framework for android malware detection using deep learning","abstract":"Android OS experiences a blazing popularity since the last few years. This predominant platform has established itself not only in the mobile world but also in the Internet of Things (IoT) devices. This popularity, however, comes at the expense of security, as it has become a tempting target of malicious apps. Hence, there is an increasing need for sophisticated, automatic, and portable malware detection solutions. In this paper, we propose MalDozer, an automatic Android malware detection and family attribution framework that relies on sequences classification using deep learning techniques. Starting from the raw sequence of the app's API method calls, MalDozer automatically extracts and learns the malicious and the benign patterns from the actual samples to detect Android malware. MalDozer can serve as a ubiquitous malware detection system that is not only deployed on servers, but also on mobile and even IoT devices. We evaluate MalDozer on multiple Android malware datasets ranging from 1 K to 33 K malware apps, and 38 K benign apps. The results show that MalDozer can correctly detect malware and attribute them to their actual families with an F1-Score of 96%-99% and a false positive rate of 0.06%-2%, under all tested datasets and settings. (c) 2018 The Author(s). Published by Elsevier Ltd on behalf of DFRWS.","keywords_author":["Android","Deep learning","IoT","Malware","Mobile","Mobile","Android","Malware","IoT","Deep learning"],"keywords_other":["Malware detection","Internet of thing (IOT)","Android ossa","False positive rates","Android","Learning techniques","Android malware","Mobile"],"max_cite":0.0,"pub_year":2018.0,"sources":"['scp', 'wos']","rawkeys":["android malware","deep learning","internet of thing (iot)","false positive rates","android ossa","android","malware","mobile","learning techniques","iot","malware detection"],"tags":["android malware","neural networks","machine learning","false positive rates","android ossa","malware","mobile","learning techniques","malware detection","internet of things (iot)"]},{"p_id":6061,"title":"Integration of Cloud computing and Internet of Things: A survey","abstract":"\u00a9 2015 Elsevier B.V.Cloud computing and Internet of Things (IoT) are two very different technologies that are both already part of our life. Their adoption and use are expected to be more and more pervasive, making them important components of the Future Internet. A novel paradigm where Cloud and IoT are merged together is foreseen as disruptive and as an enabler of a large number of application scenarios. In this paper, we focus our attention on the integration of Cloud and IoT, which is what we call the CloudIoT paradigm. Many works in literature have surveyed Cloud and IoT separately and, more precisely, their main properties, features, underlying technologies, and open issues. However, to the best of our knowledge, these works lack a detailed analysis of the new CloudIoT paradigm, which involves completely new applications, challenges, and research issues. To bridge this gap, in this paper we provide a literature survey on the integration of Cloud and IoT. Starting by analyzing the basics of both IoT and Cloud Computing, we discuss their complementarity, detailing what is currently driving to their integration. Thanks to the adoption of the CloudIoT paradigm a number of applications are gaining momentum: we provide an up-to-date picture of CloudIoT applications in literature, with a focus on their specific research challenges. These challenges are then analyzed in details to show where the main body of research is currently heading. We also discuss what is already available in terms of platforms-both proprietary and open source-and projects implementing the CloudIoT paradigm. Finally, we identify open issues and future directions in this field, which we expect to play a leading role in the landscape of the Future Internet.","keywords_author":["Cloud computing","Cloud of things","Internet of Things","Pervasive applications","Smart city","Ubiquitous networks"],"keywords_other":["Research challenges","Literature survey","Pervasive applications","Smart cities","New applications","Internet of Things (IOT)","Ubiquitous networks","Gaining momentum"],"max_cite":309.0,"pub_year":2016.0,"sources":"['scp']","rawkeys":["research challenges","literature survey","cloud computing","pervasive applications","smart city","internet of things","ubiquitous networks","cloud of things","new applications","gaining momentum","internet of things (iot)","smart cities"],"tags":["research challenges","literature survey","cloud computing","pervasive applications","ubiquitous networks","cloud of things","new applications","gaining momentum","internet of things (iot)","smart cities"]},{"p_id":42925,"title":"Indoor navigation systems based on data mining techniques in internet of things: a survey","abstract":"\u00a9 2018 Springer Science+Business Media, LLC, part of Springer NatureInternet of Things (IoT) is turning into an essential part of daily life, and numerous IoT-based scenarios will be seen in future of modern cities ranging from small indoor situations to huge outdoor environments. In this era, navigation continues to be a crucial element in both outdoor and indoor environments, and many solutions have been provided in both cases. On the other side, recent smart objects have produced a substantial amount of various data which demands sophisticated data mining solutions to cope with them. This paper presents a detailed review of previous studies on using data mining techniques in indoor navigation systems for the loT scenarios. We aim to understand what type of navigation problems exist in different IoT scenarios with a focus on indoor environments and later on we investigate how data mining solutions can provide solutions on those challenges.","keywords_author":["Data mining","Indoor navigation system","Indoor positioning","IoT","Machine learning"],"keywords_other":["Navigation problem","Indoor positioning","Smart objects","Indoor navigation system","Indoor environment","Internet of Things (IOT)","Daily lives","Outdoor environment"],"max_cite":0.0,"pub_year":2018.0,"sources":"['scp']","rawkeys":["outdoor environment","indoor navigation system","data mining","indoor positioning","machine learning","smart objects","navigation problem","iot","indoor environment","internet of things (iot)","daily lives"],"tags":["outdoor environment","indoor navigation system","data mining","indoor positioning","machine learning","smart objects","navigation problem","indoor environment","internet of things (iot)","daily lives"]},{"p_id":42927,"title":"Traffic accident detection using random forest classifier","abstract":"\u00a9 2018 IEEE.The Internet of Things (IoT) has been growing in recent years with the improvements in several different applications in the military, marine, intelligent transportation, smart health, smart grid, smart home and smart city domains. Although IoT brings significant advantages over traditional information and communication (ICT) technologies for Intelligent Transportation Systems (ITS), these applications are still very rare. Although there is a continuous improvement in road and vehicle safety, as well as improvements in IoT, the road traffic accidents have been increasing over the last decades. Therefore, it is necessary to find an effective way to reduce the frequency and severity of traffic accidents. Hence, this paper presents an intelligent traffic accident detection system in which vehicles exchange their microscopic vehicle variables with each other. The proposed system uses simulated data collected from vehicular ad-hoc networks (VANETs) based on the speeds and coordinates of the vehicles and then, it sends traffic alerts to the drivers. Furthermore, it shows how machine learning methods can be exploited to detect accidents on freeways in ITS. It is shown that if position and velocity values of every vehicle are given, vehicles' behavior could be analyzed and accidents can be detected easily. Supervised machine learning algorithms such as Artificial Neural Networks (ANN), Support Vector Machine (SVM), and Random Forests (RF) are implemented on traffic data to develop a model to distinguish accident cases from normal cases. The performance of RF algorithm, in terms of its accuracy, was found superior to ANN and SVM algorithms. RF algorithm has showed better performance with 91.56% accuracy than SVM with 88.71% and ANN with 90.02% accuracy.","keywords_author":["Accident Detection","Artificial Neural Networks (ANN)","Internet of Things (IoT)","Machine Learning","Random Forests","Support Vector Machines (SVM)","Traffic Simulation","vehicular ad-hoc networks (VANETs)"],"keywords_other":["Traffic simulations","Random forests","Vehicular Adhoc Networks (VANETs)","Internet of Things (IOT)","Accident detections"],"max_cite":0.0,"pub_year":2018.0,"sources":"['scp']","rawkeys":["support vector machines (svm)","accident detections","traffic simulation","artificial neural networks (ann)","machine learning","random forests","accident detection","vehicular adhoc networks (vanets)","vehicular ad-hoc networks (vanets)","traffic simulations","internet of things (iot)"],"tags":["accident detections","vehicular ad hoc networks","neural networks","machine learning","random forests","traffic simulations","internet of things (iot)"]},{"p_id":24501,"title":"Smart City and IoT","abstract":"\u00a9 2017The new Internet of Things (IoT) applications are enabling Smart City initiatives worldwide. It provides the ability to remotely monitor, manage and control devices, and to create new insights and actionable information from massive streams of real-time data. The main features of a smart city include a high degree of information technology integration and a comprehensive application of information resources. The essential components of urban development for a smart city should include smart technology, smart industry, smart services, smart management and smart life. The Internet of Things is about installing sensors (RFID, IR, GPS, laser scanners, etc.) for everything, and connecting them to the internet through specific protocols for information exchange and communications, in order to achieve intelligent recognition, location, tracking, monitoring and management. With the technical support from IoT, smart city need to have three features of being instrumented, interconnected and intelligent. Only then a Smart City can be formed by integrating all these intelligent features at its advanced stage of IOT development. The explosive growth of Smart City and Internet of Things applications creates many scientific and engineering challenges that call for ingenious research efforts from both academia and industry, especially for the development of efficient, scalable, and reliable Smart City based on IoT. New protocols, architectures, and services are in dire needs to respond for these challenges. The goal of the special issue is to bring together scholars, professors, researchers, engineers and administrators resorting to the state-of-the-art technologies and ideas to significantly improve the field of Smart City based on IoT.","keywords_author":["IoT","Sensors networks","Smart city","Smart home","Ubiquitous computing","Urban development"],"keywords_other":null,"max_cite":9.0,"pub_year":2017.0,"sources":"['scp']","rawkeys":["sensors networks","urban development","ubiquitous computing","iot","smart home","smart city"],"tags":["urban development","smart homes","ubiquitous computing","sensor networks","internet of things (iot)","smart cities"]},{"p_id":47040,"title":"Towards cognitive device management: A testbed to explore autonomy for constrained IoT devices","abstract":"\u00a9 2017 Copyright held by the owner\/author(s). Providing constrained IoT devices with more intelligence is important to make them work optimally with regard to energy consumption and quality of data. To overcome the constraints of the sensors, we place cognition, i.e., learning and planning, in the cloud. In this demonstration paper, we present a testbed for exploring autonomy for constrained sensor nodes.","keywords_author":["Autonomous device management","Internet of Things","Machine learning","Solar energy harvesting"],"keywords_other":["Constrained sensors","Autonomous devices","Quality of data","Device management"],"max_cite":0.0,"pub_year":2017.0,"sources":"['scp']","rawkeys":["device management","constrained sensors","autonomous devices","internet of things","machine learning","quality of data","autonomous device management","solar energy harvesting"],"tags":["device management","constrained sensors","autonomous devices","machine learning","quality of data","autonomous device management","solar energy harvesting","internet of things (iot)"]},{"p_id":36805,"title":"Applications of deep neural networks for ultra low power IoT","abstract":"\u00a9 2017 IEEE. IoT devices are increasing in prevalence and popularity, becoming an indispensable part of daily life. Despite the stringent energy and computational constraints of IoT systems, specialized hardware can enable energy-efficient sensor-data classification in an increasingly diverse range of IoT applications. This paper demonstrates seven different IoT applications using a fully-connected deep neural network (FC-NN) accelerator on 28nm CMOS. The applications include audio keyword spotting, face recognition, and human activity recognition. For each application, a FC-NN model was trained from a preprocessed dataset and mapped to the accelerator. Experimental results indicate the models retained their state-of-the-art accuracy on the accelerator across a broad range of frequencies and voltages. Real-time energy results for the applications were found to be on the order of 100nJ per inference or lower.","keywords_author":["Deep learning","Face recognition","Hardware accelerator","Human activity recognition","Internet of things","Low-power","Machine learning","Neural networks","Spotting"],"keywords_other":["Specialized hardware","Computational constraints","Low Power","State of the art","Human activity recognition","Sensor data classifications","Spotting","Hardware accelerators"],"max_cite":1.0,"pub_year":2017.0,"sources":"['scp']","rawkeys":["hardware accelerators","specialized hardware","deep learning","neural networks","state of the art","human activity recognition","internet of things","machine learning","spotting","face recognition","computational constraints","low power","hardware accelerator","low-power","sensor data classifications"],"tags":["hardware accelerators","specialized hardware","neural networks","state of the art","human activity recognition","machine learning","spotting","face recognition","computational constraints","low power","internet of things (iot)","sensor data classifications"]},{"p_id":44997,"title":"A smart campus internet of things framework","abstract":"\u00a9 2017 IEEE. Smart cities and communities (S&CC) are experiencing transformational change across the nation and globally with the proliferation deployment of Internet of Things (IoT). Smart campus is an important part of S&CC. We built a smart campus IoT framework to address functionality, energy consumption, security and safety in a user-centric perspective. To achieve these, machine learning algorithms are employed in the hardware-software co-design process. During experiment, we focused on energy consumption management using support vector machine to learn the classroom schedule, so that the system can respond to dynamic change of the schedules. The experimental results show that our proposed framework can recognize the classroom regular events and irregular events with high accuracy.","keywords_author":["Energy consumption","Internet of things","Machine learning","Smart campus"],"keywords_other":["Smart campus","User-centric","Iot frameworks","Dynamic changes","High-accuracy","Co-designs","Internet of Things (IOT)"],"max_cite":0.0,"pub_year":2018.0,"sources":"['scp']","rawkeys":["iot frameworks","smart campus","energy consumption","dynamic changes","internet of things","machine learning","co-designs","user-centric","high-accuracy","internet of things (iot)"],"tags":["iot frameworks","smart campus","energy consumption","dynamic changes","machine learning","co-designs","user-centric","high-accuracy","internet of things (iot)"]},{"p_id":45003,"title":"PulsePrint: Single-arm-ECG biometric human identification using deep learning","abstract":"\u00a9 2017 IEEE.Focusing on the privacy and security challenges brought by emerging\/promising smart health applications, we propose a single-arm-ECG biometric human identification system, with two major contributions. Firstly, to replace the traditional inconvenient\/uncomfortable ECG leads like the chest and two-wrist lead configurations, we propose a highly wearable single-arm-ECG lead configuration. Secondly, to prevent time-consuming and information-missing feature engineering work, we introduce advanced deep learning techniques to automatically learn from the raw ECG data highly level features. To achieve this goal, the 1D ECG time series is transform to a new domain, where a 2D ECG representation is obtained. Afterwards, a convolutional neural network is applied to the 2D ECG data and learn the hidden patterns for user identification purpose. The proposed system is validated on a single-arm-ECG dataset. This study demonstrates the feasibility of this highly wearable deep learning-empowered human identification system.","keywords_author":["Convolutional neural network","Deep learning","ECG","Internet of Things","Machine learning","Smart health","Wearable computer"],"keywords_other":["Information missing","ECG data","Learning techniques","Privacy and security","Convolutional neural network","Human identification","Hidden patterns","User identification"],"max_cite":0.0,"pub_year":2018.0,"sources":"['scp']","rawkeys":["ecg data","ecg","deep learning","internet of things","machine learning","smart health","wearable computer","privacy and security","learning techniques","convolutional neural network","information missing","hidden patterns","user identification","human identification"],"tags":["wearable computing","ecg data","ecg","machine learning","smart health","privacy and security","learning techniques","convolutional neural network","information missing","hidden patterns","user identification","internet of things (iot)","human identification"]},{"p_id":36812,"title":"Improve IoT\/M2M data organization based on stream patterns","abstract":"\u00a9 2017 IEEE. The increasing number of small, cheap devices full of sensing capabilities lead to an untapped source of information that can be explored to improve and optimize several systems. Yet, as this number grows it becomes increasingly difficult to manage and organize all this new information. The lack of a standard context representation scheme is one of the main difficulties in this research area. With this in mind we propose a tailored generative stream model, with two main uses: Stream similarity and generation. Sensor data can be organized based on pattern similarity, that can be estimated using the proposed model. The proposed stream model will be used in conjunction with our context organization model, in which we aim to provide an automatic organizational model without enforcing specific representations. Moreover, the model can be used to generate streams in a controlled environment. Useful for validating, evaluating and testing any platform that deals with IoT\/M2M devices.","keywords_author":["Context awareness","IoT","M2M","Machine learning","Stream Mining"],"keywords_other":["Stream mining","Organizational modeling","Context- awareness","Context representation","Controlled environment","Data organization","Pattern similarity","Organization model"],"max_cite":1.0,"pub_year":2017.0,"sources":"['scp']","rawkeys":["data organization","context awareness","stream mining","pattern similarity","context- awareness","controlled environment","context representation","machine learning","organizational modeling","organization model","iot","m2m"],"tags":["data organization","context-aware","stream mining","pattern similarity","controlled environment","context representation","machine learning","machine to machines","organization model","internet of things (iot)","organizational modeling"]},{"p_id":63442,"title":"IoT-Based Prognostics and Systems Health Management for Industrial Applications","abstract":"Prognostics and systems health management (PHM) is an enabling discipline that uses sensors to assess the health of systems, diagnoses anomalous behavior, and predicts the remaining useful performance over the life of the asset. The advent of the Internet of Things (IoT) enables PHM to be applied to all types of assets across all sectors, thereby creating a paradigm shift that is opening up significant new business opportunities. This paper introduces the concepts of PHM and discusses the opportunities provided by the IoT. Developments are illustrated with examples of innovations from manufacturing, consumer products, and infrastructure. From this review, a number of challenges that result from the rapid adoption of IoT-based PHM are identified. These include appropriate analytics, security, IoT platforms, sensor energy harvesting, IoT business models, and licensing approaches.","keywords_author":["Internet of things","maintenance","prognostics and systems health management","reliability","remaining useful life"],"keywords_other":["PREDICTION","DIAGNOSIS","MACHINERY","TRANSPORT","FRAMEWORK","CONDITION-BASED MAINTENANCE","LIFE","ELECTRONIC PRODUCTS","FUSION PROGNOSTICS","PROPORTIONAL-HAZARDS MODEL"],"max_cite":12.0,"pub_year":2016.0,"sources":"['ieee', 'wos']","rawkeys":["diagnosis","proportional-hazards model","prognostics and systems health management","maintenance","framework","prediction","fusion prognostics","internet of things","condition-based maintenance","life","reliability","remaining useful life","electronic products","machinery","transport"],"tags":["diagnosis","prognostics and systems health management","maintenance","framework","proportional hazards models","prediction","electronic product","fusion prognostics","life","remaining useful life","reliability","communication","condition based maintenance","machinery","internet of things (iot)"]},{"p_id":45012,"title":"Hear the heart: Daily cardiac health monitoring using Ear-ECG and Machine Learning","abstract":"\u00a9 2017 IEEE.Daily cardiac health monitoring is of high importance for effective heart disease prediction\/management. In this study, we propose a novel ear-worn system for long-term continuous ECG QRS duration tracking, to overcome challenges of current wearable ECG systems such as the uncomfortableness and inconvenience. Specifically, we place all the ECG electrodes behind the ear to enhance the wearability, and weak\/noisy ear-ECG is obtained. Then, we use a support vector machine classifier for heartbeat identification, apply an unsupervised learning approach for heartbeat purification, and a regression model to derive the standard chest-ECG QRS durations from the ear-ECG QRS durations. We have evaluated the proof-of-concept system using an ear-ECG dataset acquired by a semi-customized wearable prototype, and demonstrated the effectiveness of the proposed system. To the best of our knowledge, it is the first study on an ear-worn system for ECG QRS duration estimation, which can be used in daily cardiac health monitoring applications.","keywords_author":["ECG","Internet of things","Machine learning","Pattern recognition","Smart health","Wearable computer"],"keywords_other":["Regression model","Heart disease","Wearable prototypes","Wearable ECG","Cardiac health","Support vector machine classifiers","ECG electrodes","Proof of concept"],"max_cite":0.0,"pub_year":2018.0,"sources":"['scp']","rawkeys":["support vector machine classifiers","heart disease","ecg","internet of things","machine learning","smart health","wearable computer","ecg electrodes","proof of concept","regression model","pattern recognition","cardiac health","wearable prototypes","wearable ecg"],"tags":["support vector machine classifiers","heart disease","ecg","machine learning","smart health","ecg electrodes","wearable ecg","proof of concept","regression model","pattern recognition","cardiac health","wearable prototypes","internet of things (iot)","wearable computing"]},{"p_id":63449,"title":"Advanced internet of things for personalised healthcare systems: A survey","abstract":"As a new revolution of the Internet, Internet of Things (IoT) is rapidly gaining ground as a new research topic in many academic and industrial disciplines, especially in healthcare. Remarkably, due to the rapid proliferation of wearable devices and smartphone, the Internet of Things enabled technology is evolving healthcare from conventional hub based system to more personalised healthcare systems (PHS). However, empowering the utility of advanced IoT technology in PHS is still significantly challenging in the area considering many issues, like shortage of cost-effective and accurate smart medical sensors, unstandardised IoT system architectures, heterogeneity of connected wearable devices, multi-dimensionality of data generated and high demand for interoperability. In an effect to understand advance of IoT technologies in PHS, this paper will give a systematic review on advanced IoT enabled PHS. It will review the current research of IoT enabled PHS, and key enabling technologies, major IoT enabled applications and successful case studies in healthcare, and finally point out future research trends and challenges. (C) 2017 Elsevier B.V. All rights reserved.","keywords_author":["Internet of things","Personalised","Healthcare system","Survey"],"keywords_other":["ACCELERATION DATA","HIDDEN MARKOV-MODELS","DECISION-SUPPORT-SYSTEM","TRIAXIAL ACCELEROMETER","WEARABLE SENSORS","PHYSICAL-ACTIVITY RECOGNITION","CASE-MANAGEMENT","BAROMETRIC-PRESSURE","NEURAL-NETWORKS","CLINICAL DECISIONS"],"max_cite":9.0,"pub_year":2017.0,"sources":"['wos']","rawkeys":["case-management","neural-networks","personalised","wearable sensors","barometric-pressure","survey","healthcare system","physical-activity recognition","internet of things","triaxial accelerometer","decision-support-system","hidden markov-models","clinical decisions","acceleration data"],"tags":["hidden markov models","wearable sensors","barometric-pressure","personalisation","healthcare system","neural networks","survey","physical activity recognition","case management","internet of things (iot)","triaxial accelerometer","clinical decision","decision support systems","acceleration data"]},{"p_id":18396,"title":"Toward cyber-enhanced working dogs for search and rescue","abstract":"\u00a9 2014 IEEE.The authors introduce the fundamental building blocks for a cyber-enabled, computer-mediated communication platform to connect human and canine intelligence to achieve a new generation of Cyber-Enhanced Working Dog (CEWD). The use of monitoring technologies provides handlers with real-time information about the behavior and emotional state of their CEWDs and the environments they're working in for a more intelligent canine-human collaboration. From handler to dog, haptic feedback and auditory cues are integrated to provide remote command and feedback delivery. From dog to handler, multiple inertial measurement units strategically located on a harness are used to accurately detect posture and behavior, and concurrent noninvasive photoplethysmogram and electrocardiogram for physiological monitoring. The authors also discuss how CEWDs would be incorporated with a variety of other robotic and autonomous technologies to create next-generation intelligent emergency response systems. Using cyber-physical systems to supplement and augment the two-way information exchange between human handlers and dogs would amplify the remarkable sensory capacities of search and rescue dogs and help them save more lives.","keywords_author":["canine machine interfaces","inertial measurement","intelligent systems","Internet of Things","machine learning","physiological sensing"],"keywords_other":["Computer-mediated communication","Cyber physical systems (CPSs)","Inertial measurement unit","Machine interfaces","Physiological sensing","Emergency response systems","Inertial measurements","Fundamental building blocks"],"max_cite":24.0,"pub_year":2014.0,"sources":"['scp']","rawkeys":["machine interfaces","fundamental building blocks","canine machine interfaces","cyber physical systems (cpss)","internet of things","machine learning","intelligent systems","physiological sensing","inertial measurement","computer-mediated communication","inertial measurements","emergency response systems","inertial measurement unit"],"tags":["cyber-physical systems","machine interfaces","fundamental building blocks","canine machine interfaces","machine learning","intelligent systems","physiological sensing","computer-mediated communication","inertial measurements","emergency response systems","inertial measurement unit","internet of things (iot)"]},{"p_id":51167,"title":"Smart cities: An architectural approach","abstract":"Copyright \u00a9 2015 SCITEPRESS - Science and Technology Publications.Smart cities are usually defined as modern cities with smooth information processes, facilitation mechanisms for creativity and innovativeness, and smart and sustainable solutions promoted through service platforms. With the objective of improving citizen's quality of life and quickly and efficiently make informed decisions, authorities try to monitor all information of city systems. Smart cities provide the integration of all systems in the city via a centralized command centre, which provides a holistic view of it. As smart cities emerge, old systems already in place are trying to evolve to become smarter, although these systems have many specific needs that need to be attended. With the intent to suit the needs of specific systems the focus of this work is to gather viable information that leads to analyse and, present solutions to address their current shortcomings. In order to understand the most scalable, adaptable and interoperable architecture for the problem, existing architectures will be analysed as well as the algorithms that make them work. To this end, we propose a new architecture to smart cities.","keywords_author":["Internet of things (IoT)","Machine learning","Machine to machine (M2M)","Smart cities"],"keywords_other":["Machine-to-machine (M2M)","Architectural approach","Sustainable solution","Informed decision","Smart cities","Information process","Existing architectures","Internet of Things (IOT)"],"max_cite":0.0,"pub_year":2015.0,"sources":"['scp']","rawkeys":["existing architectures","machine-to-machine (m2m)","information process","smart cities","architectural approach","machine learning","informed decision","sustainable solution","internet of things (iot)","machine to machine (m2m)"],"tags":["existing architectures","information processing","architectural approach","machine learning","informed decision","sustainable solution","machine to machines","internet of things (iot)","smart cities"]},{"p_id":69611,"title":"Smart Home Based on WiFi Sensing: A Survey","abstract":"Conventional sensing methodologies for smart home are known to be labor-intensive and complicated for practical deployment. Thus, researchers are resorting to alternative sensing mechanisms. Wi-Fi is one of the key technologies that enable connectivity for smart home services. Apart from its primary use for communication, Wi-Fi signal has now been widely leveraged for various sensing tasks, such as gesture recognition and fall detection, due to its sensitivity to environmental dynamics. Building smart home based on Wi-Fi sensing is cost-effective, non-invasive, and enjoys convenient deployment. In this paper, we survey the recent advances in the smart home systems based on the Wi-Fi sensing, mainly in such areas as health monitoring, gesture recognition, contextual information acquisition, and authentication.","keywords_author":["IoT","smart home","WiFi sensing"],"keywords_other":["CHANNEL STATE INFORMATION","SLEEP-APNEA"],"max_cite":0.0,"pub_year":2018.0,"sources":"['wos']","rawkeys":["channel state information","wifi sensing","iot","sleep-apnea","smart home"],"tags":["channel state information","smart homes","wifi sensing","sleep apnea","internet of things (iot)"]},{"p_id":20468,"title":"Big data applications in operations\/supply-chain management: A literature review","abstract":"\u00a9 2016 Elsevier LtdPurpose Big data is increasingly becoming a major organizational enterprise force to reckon with in this global era for all sizes of industries. It is a trending new enterprise system or platform which seemingly offers more features for acquiring, storing and analysing voluminous generated data from various sources to obtain value-additions. However, current research reveals that there is limited agreement regarding the performance of \u201cbig data.\u201d Therefore, this paper attempts to thoroughly investigate \u201cbig data,\u201d its application and analysis in operations or supply-chain management, as well as the trends and perspectives in this research area. This paper is organized in the form of a literature review, discussing the main issues of \u201cbig data\u201d and its extension into \u201cbig data II\u201d\/IoT\u2013value-adding perspectives by proposing a value-adding framework. Methodology\/research approach The research approach employed is a comprehensive literature review. About 100 or more peer-reviewed journal articles\/conference proceedings as well as industrial white papers are reviewed. Harzing Publish or Perish software was employed to investigate and critically analyse the trends and perspectives of \u201cbig data\u201d applications between 2010 and 2015. Findings\/results The four main attributes or factors identified with \u201cbig data\u201d include \u2013 big data development sources (Variety \u2013 V1), big data acquisition (Velocity \u2013 V2), big data storage (Volume \u2013 V3), and finally big data analysis (Veracity \u2013 V4). However, the study of \u201cbig data\u201d has evolved and expanded a lot based on its application and implementation processes in specific industries in order to create value (Value-adding \u2013 V5) \u2013 \u201cBig Data cloud computing perspective\/Internet of Things (IoT)\u201d. Hence, the four Vs of \u201cbig data\u201d is now expanded into five Vs. Originality\/value of research This paper presents original literature review research discussing \u201cbig data\u201d issues, trends and perspectives in operations\/supply-chain management in order to propose \u201cBig data II\u201d (IoT \u2013 Value-adding) framework. This proposed framework is supposed or assumed to be an extension of \u201cbig data\u201d in a value-adding perspective, thus proposing that \u201cbig data\u201d be explored thoroughly in order to enable industrial managers and businesses executives to make pre-informed strategic operational and management decisions for increased return-on-investment (ROI). It could also empower organizations with a value-adding stream of information to have a competitive edge over their competitors.","keywords_author":["Big data \u2013 applications and analysis","Cloud computing","Internet of Things (IoT)","Master database management","Operations\/supply-chain management"],"keywords_other":["MASTER database","Implementation process","Big data applications","Industrial managers","Literature reviews","Research approach","Management decisions","Internet of Things (IOT)"],"max_cite":17.0,"pub_year":2016.0,"sources":"['scp']","rawkeys":["cloud computing","master database","master database management","operations\/supply-chain management","literature reviews","management decisions","research approach","big data \u2013 applications and analysis","industrial managers","big data applications","implementation process","internet of things (iot)"],"tags":["cloud computing","master database","master database management","operations\/supply-chain management","literature reviews","management decisions","research approach","big data \u2013 applications and analysis","industrial managers","big data applications","implementation process","internet of things (iot)"]},{"p_id":43004,"title":"Poster abstract: Comparison of classifiers for prediction of human actions in a smart home","abstract":"\u00a9 2018 IEEE. There is a strong interest in IoT-based systems that monitor and control smart home environments by accurately predicting the needs of the human occupants. Past research has focused on the accuracy of prediction of a user's future action. However, much of that work uses synthetic datasets which do not always reflect the real-world interactions that occur between an individual and the home environment. In addition, a focus on prediction accuracy often comes at the cost of slower processing time. This paper focuses on the prediction of future human actions in an intelligent environment with the goal of achieving both high accuracy and real-time performance. We performed experiments using the MavPad dataset, which was gathered from a fully-instrumented home environment, and compared several different machine learning algorithms that included both single and ensemble classifiers. The results show that using a Support Vector Machine approach achieved the best results when using a group of sensors within a local zone around the user and the Random Forest classifier achieved higher performance when using sensors that are distributed across the entire home environment.","keywords_author":["Ensemble Classifier","IoT","Machine Learning Algorithms","Prediction System","Smart Home System"],"keywords_other":["Ensemble classifiers","Prediction systems","Smart-home system","Random forest classifier","Real time performance","Intelligent environment","Monitor and control","Comparison of classifiers"],"max_cite":0.0,"pub_year":2018.0,"sources":"['scp']","rawkeys":["ensemble classifier","prediction systems","monitor and control","machine learning algorithms","prediction system","smart home system","random forest classifier","smart-home system","comparison of classifiers","ensemble classifiers","real time performance","iot","intelligent environment"],"tags":["prediction systems","monitor and control","machine learning algorithms","smart-home system","random forest classifier","ensemble classifiers","real time performance","comparison of classifiers","internet of things (iot)","intelligent environment"]}]